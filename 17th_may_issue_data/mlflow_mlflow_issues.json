[
  {
    "issue_number": 16232,
    "title": "[FR] Showing partial response for OpenAI LengthFinishReasonError when using structured output",
    "author": "ai-learner-00",
    "state": "open",
    "created_at": "2025-06-12T17:08:09Z",
    "updated_at": "2025-06-17T13:28:40Z",
    "labels": [
      "enhancement",
      "area/tracking"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nWhen the max completion tokens is reached, the openai library (which I think langchain relies on) returns a `LengthFinishReasonError`. The error has a `completion` field which isn't logged by mlflow. It would be nice if the output was logged as usual, currently it doesn't appear anywhere.\n\n![Image](https://github.com/user-attachments/assets/f000bcf2-2896-42a3-8470-41eb5e48ce52)\n\n<img width=\"535\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9ada889f-8923-44a9-b3fb-2d92561fcd8c\" />\n\n### Motivation\n\n> #### What is the use case for this feature?\nSeing where LLM cuts off when using OpenAI structured output (not sure how it's handled by other LLM providers)\n\n> #### Why is this use case valuable to support for MLflow users in general?\nTo determine the source of the problem and find ways to mitigate it\n\n> #### Why is this use case valuable to support for your project(s) or organization?\nHandling large documents and json responses\n\n> #### Why is it currently difficult to achieve this use case?\nI tried to emit a `llm_end` event to see if it would be logged before the error, but I didn't figure it out.\n\n```\nfrom typing import Any\nfrom uuid import UUID\n\nimport tiktoken\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain_core.outputs import LLMResult\nfrom langchain_openai import AzureChatOpenAI\nfrom openai._exceptions import LengthFinishReasonError\nfrom tiktoken.core import Encoding\n\nfrom logging_info import logger\n\n\nclass LogPartialCompletionOnError(BaseCallbackHandler):\n    \"\"\"Log partial completion in mlflow to investigate what went wrong\"\"\"\n\n    def __init__(self, llm: AzureChatOpenAI) -> None:\n        super().__init__()\n        self.llm = llm\n\n    def on_llm_error(\n        self, error: BaseException, *, run_id: UUID, parent_run_id: UUID | None = None, **kwargs: Any\n    ) -> Any:\n        if isinstance(error, LengthFinishReasonError):\n            # llm_result = self.llm._combine_llm_outputs(llm_outputs=[error.completion.model_dump()])\n            # self.on_llm_end(response=llm_result, run_id=run_id, parent_run_id=parent_run_id)\n\n    # def on_llm_end(self, response: LLMResult, *, run_id: UUID, parent_run_id: UUID | None = None, **kwargs: Any) -> Any:\n    #     return super().on_llm_end(response, run_id=run_id, parent_run_id=parent_run_id, **kwargs)\n\n```\n```\nllm = AzureChatOpenAI(..., max_tokens=10)\nllm.callbacks = [LogPartialCompletionOnError(llm=llm)]\n```\n\n### Details\n\nWhen making a call to the OpenAI API directly, it returns a response, but with a `\"finish_reason\": \"length\"`. In that sense, the error is after the llm output when trying to parse it (the error is emitted by the openai library, not the service).\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "Thanks for the FR! This sounds reasonable, though implementation wise i guess the reason it's not logged at the moment is that it's not technically an output, and MLflow Tracing follows the [OpenTelemetry schema](https://opentelemetry.io/docs/specs/otel/trace/exceptions/) which only defines these three fields for exceptions.\n\nHowever I agree it would be nice to have more info about the exception. Perhaps we could stringify the completion and append it into the `exception.message` field. cc @B-Step62 for visibility"
      },
      {
        "user": "B-Step62",
        "body": "Commented in the discussion, but wondering enabling OpenAI autologging together gives the visibility for the partial response. https://github.com/mlflow/mlflow/discussions/16211"
      },
      {
        "user": "ai-learner-00",
        "body": "@B-Step62 It doesn't unfortunately, and they don't seem to emit an event before raising the error: \n\nhttps://github.com/openai/openai-python/blob/8ade764fc124bee145990ad59d0d7c4bbe27a754/src/openai/lib/streaming/chat/_completions.py#L428\n```\n                if has_parseable_input(response_format=self._response_format, input_tools=self._input_tools):\n                    if choice.finish_reason == \"length\":\n                        # at the time of writing, `.usage` will always be `None` but\n                        # we include it here in case that is changed in the future\n                        raise LengthFinishReasonError(completion=completion_snapshot)\n```\n\nI am guessing that other LLMS such as [Gemini](https://ai.google.dev/gemini-api/docs/structured-output) will have a similar issue, but I haven't tried them."
      }
    ]
  },
  {
    "issue_number": 15291,
    "title": "[FR] Increase prompt template length limit",
    "author": "gvelimir",
    "state": "open",
    "created_at": "2025-04-11T07:10:03Z",
    "updated_at": "2025-06-17T05:33:11Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\n\n### Proposal Summary\n\nAs we have recently decided to use the prompt registry, we noticed that the prompt template for the version is limited to 5000 characters. This creates a problem because some prompts with few shot learning can be really long. Can we please increase the limit to at least 10000?\n\nHere are the lines that probably have to be changed:\nhttps://github.com/mlflow/mlflow/blob/11a934f937097b8af6a89011b013e85298bbef0c/mlflow/utils/validation.py#L57\nhttps://github.com/mlflow/mlflow/blob/11a934f937097b8af6a89011b013e85298bbef0c/tests/db/schemas/mssql.sql#L163\nhttps://github.com/mlflow/mlflow/blob/11a934f937097b8af6a89011b013e85298bbef0c/tests/db/schemas/mysql.sql#L170\nhttps://github.com/mlflow/mlflow/blob/11a934f937097b8af6a89011b013e85298bbef0c/tests/db/schemas/postgresql.sql#L168\nhttps://github.com/mlflow/mlflow/blob/11a934f937097b8af6a89011b013e85298bbef0c/tests/db/schemas/sqlite.sql#L171\nhttps://github.com/mlflow/mlflow/blob/11a934f937097b8af6a89011b013e85298bbef0c/mlflow/store/model_registry/dbmodels/models.py#L153\n\n\n### Motivation\n\n> #### What is the use case for this feature?\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\n> #### Why is it currently difficult to achieve this use case?\n\n\n### Details\n\n![Image](https://github.com/user-attachments/assets/940d8414-c4b1-48ec-9af7-fd07beadef0e)\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "this feature request makes sense. But the SQL server database string max length is 8000 bytes. If we want to support SQL server, the max length can be up to 8000."
      },
      {
        "user": "gvelimir",
        "body": "For SQL server ok, but for the other SQL DBMSes you can increase it more. I tried with MySQL for instance, can go more than 10k."
      },
      {
        "user": "WeichenXu123",
        "body": "We'd better make the same limit for all database backends. \nfor SQL server, we can use https://learn.microsoft.com/en-us/sql/relational-databases/blob/binary-large-object-blob-data-sql-server?view=sql-server-ver16 type."
      }
    ]
  },
  {
    "issue_number": 16291,
    "title": "Prompt 404 Error to prevent retry when `traces.json` file is missing.",
    "author": "soonjune",
    "state": "open",
    "created_at": "2025-06-17T04:17:36Z",
    "updated_at": "2025-06-17T04:17:36Z",
    "labels": [],
    "body": "https://github.com/mlflow/mlflow/blob/f3fe293e619705b0d25d66ecd04de458d3ce01bf/mlflow/store/artifact/artifact_repo.py#L302\n\nIt takes time to search traces via REST API when some of `traces.json` files are deleted from the backend store. This is mainly due to `FileNotFoundError` returning 500 error to the client.\nI suggest this to return 404 error instead.",
    "comments": []
  },
  {
    "issue_number": 16289,
    "title": "[FR] Add OpenTelemetry support for MLflow tracking server",
    "author": "kimminw00",
    "state": "open",
    "created_at": "2025-06-17T03:09:14Z",
    "updated_at": "2025-06-17T03:09:14Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nAdd OpenTelemetry support for MLflow tracking server\n\n### Motivation\n\nOpenTelemetry has been considered as the next standard specification for telemetry data (metrics, traces, and logs) and has been gaining popularity and support from many users and monitoring tool vendors.\n\nBy adopting OpenTelemetry, MLflow can enhance its compatibility with a wide range of monitoring tools, including Prometheus, Datadog, Tanzu Observability, Dynatrace, and New Relic. \n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": []
  },
  {
    "issue_number": 16288,
    "title": "[BUG] Inconsistent Alias Behavior When Loading Multiple Prompt Versions in Same Process",
    "author": "tingjun-cs",
    "state": "open",
    "created_at": "2025-06-17T02:59:25Z",
    "updated_at": "2025-06-17T03:08:55Z",
    "labels": [
      "bug"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\nmlflow                             3.1.0\n\n### System information\n\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 12\n- **Python version**: 3.13.3\n\n\n### Describe the problem\n\n\n**Description:**  \n\nI created aliases for different versions of the same prompt using `mlflow.genai.set_prompt_alias()`, assigning the **same alias name to two different versions**. According to the documentation, only one version should retain this alias globally, which the MLflow UI correctly reflects.  \n\n![Image](https://github.com/user-attachments/assets/d31c4975-9a05-4441-86eb-d7637ee3e621)\n\n![Image](https://github.com/user-attachments/assets/56660fba-fa7e-4db5-8e7d-bea1517311fb)\n\nHowever, when programmatically loading all prompt versions using `mlflow.genai.load_prompt()` in sequence within the **same process**, **both versions unexpectedly retain the alias**. \n\n![Image](https://github.com/user-attachments/assets/76e7f46d-aa01-4ca9-beb5-8f670227f92d)\n\nBelow is the relevant code:  \n\n```python\nfor i, model_version in enumerate(model_versions):\n    prompt_version = self.load_prompt(model_version.name, model_version.version)\n    if prompt_version is not None:\n        prompt_versions.append(prompt_version)\n\ndef load_prompt(self, name: str, version: str = None, alias: str = None) -> PromptVersion:\n    # Determine version to load\n    if version is not None:\n        source_uri = f\"prompts:/{name}/{version}\"\n    elif alias is not None:\n        source_uri = f\"prompts:/{name}@{alias}\"\n    else:\n        source_uri = f\"prompts:/{name}/latest\"\n\n    prompt = mlflow.genai.load_prompt(source_uri)\n    return prompt\n```\n\n**Unexpected Behavior:**  \n- During runtime in a single process, both versions appear to hold the alias when loaded individually.  \n- After restarting the program, only one version correctly retains the alias (as expected).  \n\n**Question:**  \nIs this inconsistent behavior within the same process indicative of a bug in the alias management system?  \n\n\n\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": []
  },
  {
    "issue_number": 16287,
    "title": "[FR]INVALID_PARAMETER_VALUE: Prompt text exceeds max length of 5000 characters.",
    "author": "tingjun-cs",
    "state": "open",
    "created_at": "2025-06-17T02:04:50Z",
    "updated_at": "2025-06-17T02:04:50Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\n### Proposal Summary\n\n\nI tried to register a prompt using the following code, but encountered an error:  \n`INVALID_PARAMETER_VALUE: Prompt text exceeds max length of 5000 characters.`  \n\nIt appears MLflow has a length restriction for prompts. How can I change this length limit? I have sufficient storage space available.\n\nI've tried both the local file system and MySQL as the `backend_store`, but the same error occurs.  \n\n```python\ndef register_prompt(self, name: str, prompt_text: str, alias: str = None) -> str:\n\n    prompt = mlflow.genai.register_prompt(\n        name=name,\n        template=prompt_text,\n    )\n```\n\n### Motivation\n\n> #### What is the use case for this feature?\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\n> #### Why is it currently difficult to achieve this use case?\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": []
  },
  {
    "issue_number": 16017,
    "title": "Fix Violation.json to use correct attributes in linter.py",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-02T13:32:48Z",
    "updated_at": "2025-06-17T00:48:19Z",
    "labels": [
      "has-closing-pr",
      "Copilot"
    ],
    "body": "In `dev/clint/src/clint/linter.py`, the `Violation.json` method uses non-existing attributes such as `self.lineno`. This should be updated to use the correct attributes from the `Violation` and `Location` classes. Please review and fix the attribute usage to prevent runtime errors and ensure the method returns accurate information.",
    "comments": [
      {
        "user": "harupy",
        "body": "Fixed by #16088."
      }
    ]
  },
  {
    "issue_number": 16139,
    "title": "CrewAI dev cross version test",
    "author": "TomeHirata",
    "state": "open",
    "created_at": "2025-06-08T20:43:33Z",
    "updated_at": "2025-06-17T00:09:04Z",
    "labels": [
      "Copilot"
    ],
    "body": "The cross version test for CrewAI dev is failing because they changed the number of chat attributes from 5 to 4 again. We should update the test code again while maintaining the validation logic for version > 0.114.\nhttps://github.com/mlflow/dev/actions/runs/15518806598/job/43695586022",
    "comments": []
  },
  {
    "issue_number": 16056,
    "title": "Correct supported type of Feedback value",
    "author": "B-Step62",
    "state": "open",
    "created_at": "2025-06-04T03:54:35Z",
    "updated_at": "2025-06-17T00:08:59Z",
    "labels": [
      "Copilot"
    ],
    "body": "Currently, `log_feedback` does not support `dict` type and only primitives are valid. However, the type hint and API docstring says it support dictionary. We should fix them.\n\nhttps://github.com/mlflow/mlflow/blob/7685fbde34f7795878dcf9ca95e35da25e739748/mlflow/tracing/assessment.py#L278\nhttps://github.com/mlflow/mlflow/blob/7685fbde34f7795878dcf9ca95e35da25e739748/mlflow/entities/assessment.py#L28\nhttps://github.com/mlflow/mlflow/blob/7685fbde34f7795878dcf9ca95e35da25e739748/mlflow/entities/assessment.py#L187",
    "comments": []
  },
  {
    "issue_number": 16075,
    "title": "[FR] Restore full chat completion response in OpenAI streaming autolog",
    "author": "B-Step62",
    "state": "open",
    "created_at": "2025-06-05T03:10:26Z",
    "updated_at": "2025-06-17T00:08:57Z",
    "labels": [
      "Copilot"
    ],
    "body": "When MLflow traces OpenAI chat completion in streaming mode, the trace output will store the concatenated string from chunks.\n\n```\nimport mlflow\nimport openai\n\nmlflow.openai.autolog()\n\nstream = client.chat.completions.create(\n        messages=[{\"role\": \"user\", \"content\": \"test\"}],\n        model=\"gpt-4o-mini\",\n        stream=True,\n)\nfor chunk in stream:\n    pass\n```\n\nThe span output will only store a string, and lacks other metadata such as id, token usage. The aggregation is done in this line: https://github.com/mlflow/mlflow/blob/master/mlflow/openai/autolog.py#L345\n\n\nWe want to recover the full completion response [here](https://github.com/openai/openai-python/blob/56540b32873df335aca9270715a839c2a9770639/src/openai/types/chat/chat_completion.py#L43) instead. Basically, you should create an empty completion object first, then fill fields based on the info in chunks. And the message content itself should be the concatenated string.",
    "comments": []
  },
  {
    "issue_number": 15329,
    "title": "[BUG] Unable to register prompts in Azure ML workspace registry",
    "author": "edgBR",
    "state": "open",
    "created_at": "2025-04-15T10:06:29Z",
    "updated_at": "2025-06-17T00:08:54Z",
    "labels": [
      "bug",
      "area/model-registry",
      "area/models",
      "integrations/azure",
      "Copilot"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nAzure Machine Learning\n\n### MLflow version\n\nmlflow, version 2.21.3\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 24.10.18\n- **Python version**: 3.10.14=h955ad1f_1\n- **yarn version, if running the dev UI**:\n\n\n### Describe the problem\n\nI am trying to register a prompt in AzureML model registry. It is not possible.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nSystem information: Linux #82~20.04.1-Ubuntu SMP Tue Sep 3 12:27:43 UTC 2024\nPython version: 3.10.14\nMLflow version: 2.21.3\nMLflow module location: /anaconda/envs/scoring_env/lib/python3.10/site-packages/mlflow/__init__.py\nTracking URI: azureml://YYYYYYYYYYYYYYYYYY.workspace.OOOOO.api.azureml.ms/mlflow/v1.0/subscriptions/XXXXXXXXXXXXXXXXXXXXXXXX/resourceGroups/ZZZZZZ/providers/Microsoft.MachineLearningServices/workspaces/IIIIIIII\nRegistry URI: azureml://YYYYYYYYYYYYYYYYYY.workspace.OOOOO.api.azureml.ms/mlflow/v1.0/subscriptions/XXXXXXXXXXXXXXXXXXXXXXXX/resourceGroups/ZZZZZZ/providers/Microsoft.MachineLearningServices/workspaces/IIIIIIII\nActive experiment ID: 1316b985-46bb-4f38-82ce-4a92105381c4\nActive run ID: 9RRRRRR\nActive run artifact URI: azureml://YYYYYYYYYYYYYYYYYY.workspace.OOOOO.api.azureml.ms/mlflow/v2.0/subscriptions/XXXXXXXXXXXXXXXXXXXXXXXX/resourceGroups/ZZZZZZ/providers/Microsoft.MachineLearningServices/workspaces/IIIIIIII/experiments/1316b985-46bb-4f38-82ce-4a92105381c4/runs/9RRRRRR/artifacts\nMLflow environment variables: \n  MLFLOW_EXPERIMENT_ID: 1316b985-46bb-4f38-82ce-4a92105381c4\n  MLFLOW_TRACKING_URI: azureml://YYYYYYYYYYYYYYYYYY.workspace.OOOOO.api.azureml.ms/mlflow/v1.0/subscriptions/XXXXXXXXXXXXXXXXXXXXXXXX/resourceGroups/ZZZZZZ/providers/Microsoft.MachineLearningServices/workspaces/IIIIIIII\nMLflow dependencies: \n  Flask: 3.1.0\n  Jinja2: 3.1.6\n  aiohttp: 3.11.16\n  alembic: 1.15.2\n  azure-storage-file-datalake: 12.20.0\n  docker: 7.1.0\n  fastapi: 0.115.12\n  graphene: 3.4.3\n  gunicorn: 23.0.0\n  langchain: 0.3.22\n  markdown: 3.7\n  matplotlib: 3.10.1\n  mlflow-skinny: 2.21.3\n  numpy: 1.26.4\n  pandas: 2.2.3\n  pyarrow: 18.1.0\n  scikit-learn: 1.6.1\n  scipy: 1.15.2\n  sqlalchemy: 2.0.40\n  tiktoken: 0.9.0\n  uvicorn: 0.34.0\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```python\nimport argparse  # noqa: D100\nimport os\n\nimport mlflow\nimport tiktoken\nfrom utils.logger import Logger\nfrom utils.sdk_utils import AMLManager\n\nlogging = Logger().getLogger(__name__)\n\nclass PromptRegistrator(AMLManager):\n    \"\"\"Registration of prompt.\n\n    The class log and register the prompt as a model in shared registry.\n\n    Attributes\n    ----------\n    cluster_identity_id : str\n        A string containing the cluster identity id.\n    subscription_id : str\n        A string containing the subscription id.\n    resource_group : str\n        A string containing the resource group.\n    workspace_name : str\n        A string containing the workspace name.\n    registry_name : str\n        A string containing the registry name.\n    registry_region : str\n        A string containing the registry region.\n    logger : class\n        A custom logger initialized in a separate class according to the format\n        standards of company.\n    args : Namespace\n        Execution arguments that can change at runtime.\n    encoding : class\n        A class that contains the tokens encoding for the LLM model.\n    \"\"\"\n\n    def __init__(self, args) -> None:\n        \"\"\"__init__ method.\n\n        Parameters\n        ----------\n        args : Namespace\n            Command-line arguments.\n        \"\"\"\n        ## Resource variables\n        self.cluster_identity_id = os.getenv(\"CLUSTER_IDENTITY_ID\", \"XXXXXXXXXXX\")\n        self.subscription_id = os.getenv(\"SUBSCRIPTION_ID\", \"XXXXXXXXXXXXXXXXXX\")\n        self.resource_group = os.getenv(\"RESOURCE_GROUP\", \"XXXXXXXXXXXXXX\")\n        self.workspace_name = os.getenv(\"WORKSPACE_NAME\", \"XXXXXXXXXXXXXX\")\n        self.registry_name = os.getenv(\"REGISTRY_NAME\", \"XXXXXXXXXXXX\")\n        self.registry_region = os.getenv(\"REGISTRY_REGION\", \"XXXXXXXXXXXXXXXXXX\")\n        self.azure_openai_endpoint = os.getenv(\n            \"AZURE_OPENAI_ENDPOINT\", \"XXXXXXXXXXXXXXXXXXXX\"\n        )\n        self.azure_openai_api_version = os.getenv(\"AZURE_OPENAI_VERSION\", \"XXXXXXXXXXXXXX\")\n\n        ## Super class initiation and clients\n        super().__init__(self.cluster_identity_id)\n        self.setup_client_workspace(\n            subscription_id=self.subscription_id,\n            resource_group=self.resource_group,\n            workspace_name=self.workspace_name,\n        )\n\n        # Config\n        self.logger = logging\n        self.args = args\n\n        # Tokens encoding class\n        self.encoding = tiktoken.encoding_for_model(self.args.llm_model)\n\n    def register_prompt_as_model(self) -> None:\n        \"\"\"Method that register the prompt as model using code-based approach.\"\"\"\n        try:\n            # Setting experiment\n            ws = self.ml_client_workspace.workspaces.get(self.ml_client_workspace.workspace_name)\n            mlflow.set_tracking_uri(ws.mlflow_tracking_uri)\n            mlflow.set_experiment(\"prompt_registration\")\n\n            # Model details\n\n            with mlflow.start_run():\n\n                initial_template = \"\"\"\\\n                You work in a aaaaaaaaaaaaa\n                You can read the conversation between X and Z: {{transcription}}.\n                Please note that the transcription might be not fully accurate. Some words might be missing or incorrect.\n\n                Please collect some important informations:\n\n                valid_conversation:\n                * Indicate if the conversation is valid and important details can be obtained from it.\n                * Possible values: Yes, No\n                * Return Yes if any of the following conditions are met:\n                * A\n                * B\n                * C\n                * D\n                * E\n                * If any {{keyword}} is mentioned.\n                * Return No if:\n                * Z\n                * X\n                * O\n\n                Please return the output in format:\n                {\n                    \"valid_conversation\": value,\n                }\n\n                Ensure that the output is valid JSON format.\n\n                \"\"\"\n                token_number = len(self.encoding.encode(initial_template))\n                model_name = self.args.project_name + \"_\" + self.args.prompt_name\n                model_tags = {\n                    \"project_name\": self.args.project_name,\n                    \"stage\": os.getenv(\"ENV\", \"dev\"),\n                    \"type\": \"prompt\",\n                    \"country_code\": \"XXX\",\n                    \"prompt_tokens\": token_number,\n                    \"llm_model\": self.args.llm_model,\n                }\n                import ipdb\n                ipdb.set_trace()\n\n                # Register a new prompt\n                prompt = mlflow.register_prompt(\n                    name=\"call_validation_prompt\",\n                    template=initial_template,\n                    # Optional: Provide a commit message to describe the changes\n                    commit_message=\"Initial commit\",\n                    # Optional: Specify any additional metadata about the prompt version\n                    version_metadata={\n                        \"author\": \"yyyyyyyyyyyy\",\n                    },\n                    # Optional: Set tags applies to the prompt (across versions)\n                    tags=model_tags,\n                )\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\n*** mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Response: {'Error': {'Code': 'ValidationError', 'Severity': None, 'Message': 'Model source from file must be in the following format: azureml://artifacts/<origin>/<container>/<artifact_prefix>. Provided model source: dummy-source', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': 'd864cb07a415628229a45ea569eb3bbc', 'request': '424a29e7bf3b8e4f'}, 'Environment': 'westeurope', 'Location': 'westeurope', 'Time': '2025-04-15T07:50:36.6186825+00:00', 'ComponentName': 'mlflow', 'statusCode': 400, 'error_code': 'INVALID_PARAMETER_VALUE'}\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [x] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [x] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "justkr",
        "body": "Hello,\nThis bug bother me a lot, please help."
      },
      {
        "user": "edgBR",
        "body": "Hi, anyone who could take a look on this?\n\n"
      },
      {
        "user": "serena-ruan",
        "body": "MLflow is only responsible for prompt registry in OSS MLflow. For feature request about prompt registry in AML, please open a ticket with AML team instead :) cc @akshaya-a for awareness"
      }
    ]
  },
  {
    "issue_number": 16236,
    "title": "Self-referencing \"Next\" link on Tracing overview page",
    "author": "shu8hamrajput",
    "state": "open",
    "created_at": "2025-06-13T03:20:11Z",
    "updated_at": "2025-06-16T19:38:08Z",
    "labels": [
      "good first issue"
    ],
    "body": "### Summary\n\nThe \"Tracing\" overview page in the MLflow documentation has a self-referencing \"Next\" link. This can be found at:\n\nðŸ“„ URL: https://mlflow.org/docs/latest/genai/getting-started/tracing/\n\nðŸ”— Issue:\nAt the bottom of the page, the \"Next\" navigation link points to the same page (https://mlflow.org/docs/latest/genai/getting-started/tracing/) instead of directing to the next intended section in the documentation.\n\nExpected Behavior:\nThe \"Next\" link should point to the next logical section or topic in the documentation sequence, not back to the current page.\n\nImpact:\nThis can confuse users navigating the documentation and disrupt the expected flow.\n\nSuggested Fix:\nUpdate the \"Next\" link to point to the actual next page in the documentation sequence following the Tracing overview.\n",
    "comments": [
      {
        "user": "saishreyakumar",
        "body": "Hi! I would like to work on this issue, can you assign this to me?"
      },
      {
        "user": "iatharvmore",
        "body": "hello ! could you assign this issue to me ? I would like to resolve this.\n"
      }
    ]
  },
  {
    "issue_number": 16282,
    "title": "[DOC-FIX] Models documentation is a 403",
    "author": "lendle",
    "state": "open",
    "created_at": "2025-06-16T18:07:04Z",
    "updated_at": "2025-06-16T18:07:04Z",
    "labels": [
      "area/docs"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute a documentation fix at this time.\n\n### URL(s) with the issue\n\nhttps://mlflow.org/docs/latest/api_reference/models.html returns a 403 error. \n\n### Description of proposal (what needs changing)\n\nTrying to access https://mlflow.org/docs/latest/api_reference/models.html returns a 403 error. \n\nIt's linked to from [here](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.models.html) for example.\n\nProposed change: Remove links to this page if it doesn't exist, or allow access if it does. If it doesn't exist, it should probably 404 though, not 403. ",
    "comments": []
  },
  {
    "issue_number": 16183,
    "title": "[BUG] Mlflow keras log_model function does not create the necessary directory in tmp folder when input_example is not None.",
    "author": "adi250491",
    "state": "open",
    "created_at": "2025-06-10T18:32:19Z",
    "updated_at": "2025-06-16T17:57:48Z",
    "labels": [
      "bug",
      "area/artifacts",
      "area/examples",
      "area/models"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.22.0\n- Tracking server: 2.21.3\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 22.04 WSL on Windows 11\n- **Python version**: 3.10.12\n- **yarn version, if running the dev UI**: NA\n\n\n### Describe the problem\n\nWhen using `mlflow.keras.log_model(model, artifact_path, input_example=input_example)` the appropriate temporary directory structure is not created. The code crashes with error `FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmptzvmv1ij/model/input_example.json'`\n\nAdding this line `os.makedirs(path)` in the `mlflow.keras.save.py` file at line number 187 fixes the issue\n\nBefore:\n`    if input_example is not None:\n        _save_example(mlflow_model, input_example, path)`\n\nAfter fix:\n`    if input_example is not None:\n          os.makedirs(path)\n          _save_example(mlflow_model, input_example, path)`\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nimport mlflow\nimport mlflow.keras\n\n#Load data\ndf = pd.read_csv('dummy_data.csv')\n\n#Traing a keras model\nmodel=train_keras_model(df)\n\n#log to mlflow\nartifact_path='models/dummymodel'\ninput_example=df.head(10)\nmlflow.keras.log_model(model, artifact_path, input_example=input_example)\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\n\nFile /mnt/c/Users/ak14097/Documents/Projects/Forecasting/Production/training/utils/mlflow_logger.py:111, in MLFlowLogger.log_keras_model(self, model, artifact_path, input_example)\n    109     print(\"MLFlow logging is disabled.\")\n    110     return\n--> 111 mlflow.keras.log_model(model, artifact_path, input_example=input_example)\n\nFile ~/Documents/Projects/Forecasting/.forecasting/lib/python3.10/site-packages/mlflow/keras/save.py:339, in log_model(model, artifact_path, save_exported_model, conda_env, signature, input_example, registered_model_name, await_registration_for, pip_requirements, extra_pip_requirements, save_model_kwargs, metadata)\n    277 @experimental\n    278 @format_docstring(LOG_MODEL_PARAM_DOCS.format(package_name=FLAVOR_NAME))\n    279 def log_model(\n   (...)\n    291     metadata=None,\n    292 ):\n    293     \"\"\"\n    294     Log a Keras model along with metadata to MLflow.\n    295 \n   (...)\n    337             mlflow.keras.log_model(model, \"model\")\n    338     \"\"\"\n--> 339     return Model.log(\n    340         artifact_path=artifact_path,\n    341         flavor=mlflow.keras,\n    342         model=model,\n    343         conda_env=conda_env,\n    344         registered_model_name=registered_model_name,\n    345         signature=signature,\n    346         input_example=input_example,\n    347         await_registration_for=await_registration_for,\n    348         pip_requirements=pip_requirements,\n    349         extra_pip_requirements=extra_pip_requirements,\n    350         save_model_kwargs=save_model_kwargs,\n    351         save_exported_model=save_exported_model,\n    352         metadata=metadata,\n    353     )\n\nFile ~/Documents/Projects/Forecasting/.forecasting/lib/python3.10/site-packages/mlflow/models/model.py:840, in Model.log(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, **kwargs)\n    831     prompts = [pr.uri if isinstance(pr, Prompt) else pr for pr in prompts]\n    832 mlflow_model = cls(\n    833     artifact_path=artifact_path,\n    834     run_id=run_id,\n   (...)\n    838     prompts=prompts,\n    839 )\n--> 840 flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n    841 # `save_model` calls `load_model` to infer the model requirements, which may result in\n    842 # __pycache__ directories being created in the model directory.\n    843 for pycache in Path(local_path).rglob(\"__pycache__\"):\n\nFile ~/Documents/Projects/Forecasting/.forecasting/lib/python3.10/site-packages/mlflow/keras/save.py:187, in save_model(model, path, save_exported_model, conda_env, mlflow_model, signature, input_example, pip_requirements, extra_pip_requirements, save_model_kwargs, metadata)\n    185     mlflow_model.signature = signature\n    186 if input_example is not None:\n--> 187     _save_example(mlflow_model, input_example, path)\n    188 if metadata is not None:\n    189     mlflow_model.metadata = metadata\n\nFile ~/Documents/Projects/Forecasting/.forecasting/lib/python3.10/site-packages/mlflow/models/utils.py:528, in _save_example(mlflow_model, input_example, path, no_conversion)\n    519     warnings.warn(\n    520         \"The `example_no_conversion` parameter is deprecated since mlflow 2.16.0 and will be \"\n    521         \"removed in a future release. This parameter is no longer used and safe to be removed, \"\n   (...)\n    524         stacklevel=2,\n    525     )\n    527 example = _Example(input_example)\n--> 528 example.save(path)\n    529 mlflow_model.saved_input_example_info = example.info\n    530 return example\n\nFile ~/Documents/Projects/Forecasting/.forecasting/lib/python3.10/site-packages/mlflow/models/utils.py:431, in _Example.save(self, parent_dir_path)\n    426 def save(self, parent_dir_path: str):\n    427     \"\"\"\n    428     Save the example as json at ``parent_dir_path``/`self.info['artifact_path']`.\n    429     Save serving input as json at ``parent_dir_path``/`self.info['serving_input_path']`.\n    430     \"\"\"\n--> 431     with open(os.path.join(parent_dir_path, self.info[INPUT_EXAMPLE_PATH]), \"w\") as f:\n    432         f.write(self.json_input_example)\n    433     if self.json_serving_input:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmptzvmv1ij/model/input_example.json'\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [x] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "thanks for the report @adi250491, since you have the fix ready, would you like to make the PR for it? if you don't have time one of the maintainers can take it over as well ðŸ˜„ "
      },
      {
        "user": "daniellok-db",
        "body": "note that we do have a util for this, it looks like it just wasn't called for this flavor. if you'd like to make the PR, please follow the pattern in (should be a one-liner):\n\nhttps://github.com/mlflow/mlflow/blob/master/mlflow/dspy/save.py#L145-L146"
      },
      {
        "user": "adi250491",
        "body": "Great! I will make the PR for this."
      }
    ]
  },
  {
    "issue_number": 9805,
    "title": "[FR] Train/test metrics (tagged metric values)",
    "author": "rsundqvist",
    "state": "open",
    "created_at": "2023-10-03T13:03:33Z",
    "updated_at": "2025-06-16T15:08:29Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Willingness to contribute\r\n\r\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n\r\n### Proposal Summary\r\n\r\nAllow metrics to be logged with optional tags. This could be used both to extend existing MLflow functionality, and would make it easier for MLflow users to implement their own custom metric-related features.\r\n\r\n## Simplified example\r\n```python\r\nmlflow.log_metric(\"r2\", 0.99, label=\"train\")\r\nmlflow.log_metric(\"r2\", 0.59, label=\"test\")\r\n```\r\nUnder the hood. this would probably log the metric using `tags={\"mlflow.data.context\": dataset}`; see the next example.\r\n\r\n## Example\r\nUsing tags.\r\n\r\n```python\r\nMLFLOW_DATASET_CONTEXT = \"mlflow.data.context\"\r\nMLFLOW_DATASET_NAME = \"mlflow.data.name\"\r\n\r\ntraining_data = Dataset(\"input/train/all-markets.csv\")\r\nmodel = AmazingModel().fit(training_data)\r\n\r\nmetric_tags = {MLFLOW_DATASET_CONTEXT: \"train\", MLFLOW_DATASET_NAME: training_data.name}\r\nmlflow.evaluate(model, training_data, metric_tags=metric_tags)\r\n\r\ntest_data = Dataset(\"input/test/all-markets.csv\")\r\nmetric_tags = {MLFLOW_DATASET_CONTEXT: \"test\", MLFLOW_DATASET_NAME: test_data.name}\r\nfor market, df in test_data.groupby(\"market\"):\r\n  metric_tags[\"market\"] = market\r\n  mlflow.evaluate(model, test_data, metric_tags=metric_tags)\r\n```\r\n\r\nSince datasets are already part of the MLflow UI, it would make sense if metrics logged using the `MLFLOW_DATASET_CONTEXT` and/or `MLFLOW_DATASET_NAME` keys were displayed there somehow, assuming no other keys are present on the metric.\r\n\r\n\r\n### Motivation\r\n\r\n> #### What is the use case for this feature?\r\nThe same metric (R2, AUC, etc) is often computed multiple times for a given model (usually for different datasets, e.g. #1921). Another common use case is to compute and store metrics based on business domain, for example per country.\r\n\r\nUsing tags (as opposed to something like an enum column) makes it easier for users and maintainers alike to build new functionality that involves metrics.\r\n\r\n> #### Why is this use case valuable to support for MLflow users in general?\r\nI believe computing metrics per dataset, at the very least, is such a common use case that it should be natively supported without relying on ad-hoc naming conventions or subruns. This could further lead into additional features, such as detecting overfitting.\r\n\r\n> #### Why is this use case valuable to support for your project(s) or organization?\r\nThe models we use at Livewrapped are responsible for making hundreds of millions of decisions per day. As such, we need insight into how the models treat different regions and markets. We currently rely on our own methods to keep track of this, but would like it if MLflow had better support for tracking \"variants\" of the same metric.\r\n\r\nJust having something we can easily query would go a long way, but the ideal would of course be to have the graphs be tag-aware in the UI as well.\r\n\r\nFurthermore, we like to log things such as estimator fitting time, as well as the time consumed by related processes. Training big models is expensive and we like knowing where our AWS bills are coming from :).\r\n\r\nAnd the train/test use case applies to us as well, of course.\r\n\r\n> #### Why is it currently difficult to achieve this use case?\r\nRelying on a naming paradigms to keep track of train/test metric value is cumbersome, and clutters up the run page. Adding per-customer metrics as well would make the UI unusable, so we use different methods to store and analyze more fine-grained metrics.\r\n\r\n\r\n### Details\r\n\r\n# Random thoughts\r\n## Local storage\r\nTo maintain backward compatibility, I believe we'd have to split `mlflow.store.tracking.file_store.FileStore` metrics logging into tagged and untagged metrics. I've got a naive version of this working on https://github.com/Livewrapped-official/mlflow/commit/6866e9a0d053f1ce6cf6848f76efe4245bca3a94.\r\n\r\n## Metrics are often `dict`\r\nThere are multiple places where metrics are assumed to be plain dicts. In this case, we might have to split dicts into tagged/untagged as well, or apply an \"official\" naming paradigm that includes characters that aren't usually available when logging metrics (e.g. `\"r2:mlflow.data.context=test+mlflow.data.name=se\"`.\r\n\r\n## Visualization\r\nIf something like `MLFLOW_DATASET_CONTEXT` were to be included as a \"reserved\" tag, I think it would be very nice if metric graphs could show train/test values over time in the UI. I'm not much of a frontend dev unfortunately, but something simple might be doable.\r\n\r\n### What component(s) does this bug affect?\r\n\r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/gateway`: AI Gateway service, Gateway client APIs, third-party Gateway integrations\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n### What interface(s) does this bug affect?\r\n\r\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\n### What language(s) does this bug affect?\r\n\r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\n### What integration(s) does this bug affect?\r\n\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "@rsundqvist Can we use `mlflow.set_tags` like this?\r\n\r\n```python\r\nwith mlflow.start_run():\r\n    mlflow.evaluate(model, training_data)\r\n    mlflow.set_tags(metric_tags)\r\n```"
      },
      {
        "user": "rsundqvist",
        "body": "> @rsundqvist Can we call `mlflow.set_tags` like this?\r\n> \r\n> ```python\r\n> with mlflow.start_run():\r\n>     mlflow.evaluate(model, training_data)\r\n>     mlflow.set_tags(metric_tags)\r\n> ```\r\n\r\nThis sets the tags on the run itself, which isn't really what we want. Though you could of course do something similar with nested runs + tags:\r\n\r\n```python\r\nwith mlflow.start_run(\"main-run\"):\r\n    training_data = Dataset(\"input/train/all-markets.csv\")\r\n    model = AmazingModel().fit(training_data)\r\n\r\n    metric_tags = {MLFLOW_DATASET_CONTEXT: \"train\", MLFLOW_DATASET_NAME: training_data.name}\r\n    mlflow.evaluate(\"model\", training_data)\r\n    mlflow.set_tags(metric_tags)\r\n    \r\n    test_data = Dataset(\"input/test/all-markets.csv\")\r\n    metric_tags = {MLFLOW_DATASET_CONTEXT: \"test\", MLFLOW_DATASET_NAME: test_data.name}\r\n    for market, df in test_data.groupby(\"market\"):\r\n        metric_tags[\"market\"] = market\r\n        with mlflow.start_run(run_name=f\"test metrics for {market}\", nested=True):\r\n            mlflow.evaluate(\"model\", df)\r\n            mlflow.set_tags(metric_tags)\r\n```\r\nThis might serve as some kind of grouping/specialization. \r\n\r\nHowever, at least as far as I know, there's no good way to query and group metrics with the same name based on parent runs. Subruns are treated just like parent runs in the compare view, for example, which isn't what we want. It would also make the number of runs explode.\r\n\r\nIf subruns played a part in how metrics are grouped and presented it might make more sense for us, but I don't think this is what subruns are intended to do. With that in mind, I don't think using subruns (or `mlflow.set_tags`) make sense as a solution to the use case described above."
      },
      {
        "user": "BenWilson2",
        "body": "It's an interesting idea @rsundqvist :) We're going to discuss this more amongst the maintainers along with some additional other topics around plotting that are somewhat related to the interesting point that you bring up about categorical grouping."
      }
    ]
  },
  {
    "issue_number": 16259,
    "title": "[FR] Add support for chat-style prompts with structured output and configurable model metadata in prompt object",
    "author": "harshilprajapati96",
    "state": "open",
    "created_at": "2025-06-15T20:15:42Z",
    "updated_at": "2025-06-16T14:34:25Z",
    "labels": [
      "enhancement",
      "area/artifacts",
      "area/model-registry",
      "area/models",
      "area/tracking"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\n\n### Proposal Summary\n\nCurrently, MLflowâ€™s prompt object supports only plain text prompts without structured support for chat-style prompts or associated metadata like response formats and model parameters. This proposal aims to extend the prompt object to support multi-message chat prompts with structured outputs by adding fields for `prompt_type` and `response_format`, and`config` field to encapsulate model-specific information such as `model_parameters`, `model_name`, and `model_provider`. This will enable users to define and version chat-based prompts natively within MLflow, facilitating easier integration for structured chat completions and improved prompt management.\n\n### Motivation\n\n> #### What is the use case for this feature?\nThe use case is to enable MLflow users to define, store, and manage chat-style prompts that consist of multiple messages with distinct roles (such as system, user, and assistant), and to handle structured response outputs (e.g., JSON) along with model-specific configuration like parameters, model name, and provider metadata.\n\n> #### Why is this use case valuable to support for MLflow users in general?\nAs large language models increasingly operate in chat formats with structured outputs, MLflow users need native support for these complex prompt types to track, version, and reproduce prompt-driven workflows effectively. The ability to manage both prompt content and its associated configuration within MLflow improves usability, experiment reproducibility, and integration with downstream AI services.\n\n> #### Why is this use case valuable to support for your project(s) or organization?\nOur projects rely heavily on conversational AI that requires multi-turn chat prompts and structured JSON-like outputs. Without native support in MLflow, we face manual overhead in managing disparate configurations and risk inconsistencies that slow down development and deployment cycles. Enhanced prompt capabilities will streamline workflows and reduce errors.\n\n> #### Why is it currently difficult to achieve this use case?\nThe current MLflow prompt object only supports plain text templates; it cannot represent chat-style message lists or store structured response format schemas. Model parameters and provider information are stored loosely in tags, which lack validation and clear semantics. Thus, users must rely on external files or tag-based workarounds that complicate prompt management, reduce reproducibility, and limit tooling support.\n\n\n### Details\n\nTo enhance the `PromptVersion` class in MLflow to support chat-style prompts with structured outputs and configurable model metadata, the following updates are proposed:\n\n**Update `PromptVersion` Class:**\n\n- **Add New Fields:**\n  - `prompt_type`: Enum to specify the type of prompt (`\"text\"` or `\"chat\"`).\n  - `response_format`: JSON schema or structured definition of the expected output.\n  - `config`: Dictionary to encapsulate model-specific configurations, including:\n    - `model_name`: Name of the model (e.g., `\"gpt-4o-mini\"`).\n    - `model_provider`: Provider of the model (e.g., `\"openai\"`).\n    - `model_parameters`: Dictionary of model parameters (e.g., `{\"temperature\": 0.2}`).\n\n- **Modify Existing Fields:**\n  - `template`: Behavior changes based on `prompt_type`:\n    - For `\"text\"`: Remains a string template with placeholders.\n    - For `\"chat\"`: Becomes a list of dictionaries, each containing `role` (e.g., `\"system\"`, `\"user\"`, `\"assistant\"`) and `content` (string with placeholders).\n\nWith the enhanced `PromptVersion`, integrating with OpenAI's chat completions becomes straightforward. Here's an example of how to use it:\n\n```python\nimport openai\n\n# Define a chat prompt\nchat_prompt = PromptVersion(\n    name=\"SupportBot\",\n    version=1,\n    template=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"How can I reset my password?\"},\n    ],\n    prompt_type=PromptType.CHAT,\n    response_format={\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\"}}},\n    config={\n        \"model_name\": \"gpt-4o-mini\",\n        \"model_provider\": \"openai\",\n        \"model_parameters\": {\"temperature\": 0.2, \"max_tokens\": 500},\n    },\n)\n\n# Prepare the messages for OpenAI API\nmessages = chat_prompt.template\n\n# Extract model configuration\nmodel_name = chat_prompt.config.get(\"model_name\", \"gpt-4o-mini\")\nmodel_parameters = chat_prompt.config.get(\"model_parameters\", {})\n\n# Call OpenAI's API\nresponse = openai.ChatCompletion.create(\n    model=model_name,\n    messages=messages,\n    **model_parameters\n)\n\n# Process the response\nprint(response.choices[0].message[\"content\"])\n```\n\n**Explanation:**\n\n- **Defining the Prompt:** A `PromptVersion` instance is created with a chat-style template, specifying roles and content for each message.\n- **Preparing Messages:** The `template` is directly used as the `messages` parameter for OpenAI's API.\n- **Extracting Configuration:** Model name and parameters are retrieved from the `config` field.\n- **API Call:** OpenAI's `ChatCompletion.create` method is called with the prepared messages and model configurations.\n- **Processing Response:** The assistant's reply is extracted and printed.\n\nThis approach ensures that prompts are managed and versioned within MLflow, while seamlessly integrating with OpenAI's chat completion API.\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [x] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "Thank you for the feature request, @harshilprajapati96! Yes, chat-style support is on our roadmap for prompt registry. cc: @rohitarun-db "
      },
      {
        "user": "harshilprajapati96",
        "body": "will it also have `response_format`? I think all the llms are moving into structured output direction, it would be really nice to have. "
      }
    ]
  },
  {
    "issue_number": 3849,
    "title": "[FR] Add Pandas category dtype to mlflow.types.schema",
    "author": "henriqueluzz",
    "state": "open",
    "created_at": "2020-12-15T13:21:16Z",
    "updated_at": "2025-06-16T13:30:45Z",
    "labels": [
      "enhancement",
      "area/model-registry",
      "area/models"
    ],
    "body": "## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [X] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\nAdd Pandas category dtype, to mlflow.types.schema().\r\n\r\nI currently have a ML model based on LightGBM running on production environment that contains categorical variables. Instead of label-encoding or OHE the categorical variables, I'm using Pandas dataframe with categorical columns set to be of the categorical dtype.\r\n\r\n` df['some_categorical_column'] = df['some_categorical_column'].astype('category') .`\r\n\r\nI'm trying to deploy this model on Databricks Serve Model that uses MLFlow as backend, but currently this category dtype is not supported for model signature according to the available dtypes on [https://www.mlflow.org/docs/latest/_modules/mlflow/types/schema.html](https://www.mlflow.org/docs/latest/_modules/mlflow/types/schema.html), so I'm unable to correctly cast the prediction Dataframe accordingly to the training phase.\r\n\r\nIt makes me think that deploying models that were trained with LightGBM using Pandas category dtypes, cannot be correctly deployed since this dtype is not availabe, so it would occur some casting error during the prediction leading to incorrect scores/predictions. \r\n\r\nI could of course OHE the data and outline the problem, but I'm trying to deploy the current model into MLFlow.\r\nI'm not sure if this is a FR or something else.\r\n\r\n![bug2](https://user-images.githubusercontent.com/3441278/102220152-2f68b900-3ebf-11eb-9417-871fd027cca8.PNG)\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [X] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [X] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: Local serving, model deployment tools, spark UDFs\r\n- [ ] `area/server-infra`: MLflow server, JavaScript dev server\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n",
    "comments": [
      {
        "user": "dmatrix",
        "body": "@tomasatdatabricks @dbczumar is this something that should be supported?"
      },
      {
        "user": "tomasatdatabricks",
        "body": "Possibly yes, but it needs a bit of work. \r\n\r\nThe tricky part about categoricals is that the dictionary can be different between training and inference data. So you need to store the mapping seen during training and make sure you map the data the same way at inference time. You also need to handle categories unseen during training. We would also need to figure out where to store these additional dictionaries - probably as additional artifacts. It can be done, but it is not clear how often would people use this instead of using something like scikit pipeline. "
      },
      {
        "user": "olbapjose",
        "body": "Any advances on this? I think it would be useful as there are some algorithms which support null values directly (e.g. XGBoost, which also supports categorical variables in a straightforward way from now on)."
      }
    ]
  },
  {
    "issue_number": 6274,
    "title": "[BUG] OSError: [Errno 30] Read-only file system",
    "author": "zstern",
    "state": "closed",
    "created_at": "2022-07-19T22:23:01Z",
    "updated_at": "2025-06-16T11:41:10Z",
    "labels": [
      "bug",
      "area/artifacts"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\n\n### MLflow version\n\n1.27.0\n\n### System information\n\nNAME=\"Ubuntu\"\r\nVERSION=\"20.04.4 LTS (Focal Fossa)\"\r\nas underlying OS\r\n\r\nRunning Docker version 20.10.12, build 20.10.12-0ubuntu2~20.04.1\r\ndocker-compose version 1.25.0\r\n\r\n```\r\nFROM python:3-slim\r\nARG MLFLOW_VERSION=1.27.0\r\n\r\nWORKDIR /mlflow/\r\n\r\nCOPY ./requirements.txt ./requirements.txt\r\n\r\nRUN pip install -r requirements.txt\r\n\r\nEXPOSE 5000\r\n\r\nCMD mlflow server --backend-store-uri ${BACKEND_URI} --default-artifact-root ${ARTIFACT_ROOT} \\ \r\n    --host 0.0.0.0 --port 5000 \r\n```\r\n```\r\nversion: '3'\r\n\r\nservices:\r\n  mlflow:\r\n    image: 'atcommons/mlflow-server'\r\n    build: .\r\n    ports:\r\n      - \"8080:5000\"\r\n    volumes:\r\n      - ./data:/mlflow\r\n      - ./data/mlartifacts:/mlflow/mlartifacts:rw\r\n    environment:\r\n      BACKEND_URI: sqlite:////mlflow/mlflow.db\r\n      ARTIFACT_ROOT: /mlflow/mlartifacts\r\n```\r\n\r\nrequirements.txt:\r\n\r\nmlflow==1.27.0\r\npsycopg2-binary\r\nboto3\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n\n### Describe the problem\n\nIf I try to log an experiment and add artifacts using the following:\r\n\r\n```\r\nimport uuid\r\nimport mlflow\r\nmodel = onnx.load(f'{path}/model.onnx')\r\nmlflow.set_experiment(uuid.uuid4().hex)\r\n\r\nwith mlflow.start_run(run_name=name):\r\n    mlflow.get_artifact_uri()\r\n    mlflow.log_artifact(f'{path}/MLmodel', artifact_path=name)\r\n    mlflow.log_artifact(f'{path}/config.yml', artifact_path=name)\r\n    mlflow.log_artifact(f'{path}/conda.yaml', artifact_path=name)\r\n    mlflow.log_artifact(f'{path}/model_config.yaml', artifact_path=name)\r\n    mlflow.log_artifact(f'{path}/processing.py', artifact_path=name)\r\n    mlflow.onnx.log_model(model, artifact_path=name)\r\n\r\n```\r\n\r\nI get the error:\r\n`OSError: [Errno 30] Read-only file system: '/mlflow'`\r\n\r\nIf I ssh into the container I can go to the mounted file that is at `/mlflow/mlartifacts` and create a file manually, exit the container and see the file on the local system all without issue.  \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n\n### Tracking information\n\n_No response_\n\n### Code to reproduce issue\n\nsee Describe the problem above\n\n### Other info / logs\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [X] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/pipelines`: Pipelines, Pipeline APIs, Pipeline configs, Pipeline Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "@zstern \r\n\r\n> OSError: [Errno 30] Read-only file system: '/mlflow'\r\n\r\nCan you share the full stack trace?"
      },
      {
        "user": "zstern",
        "body": "```\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n/var/folders/ph/bmdywq8d3gvcn2xps9_389bc0000gp/T/ipykernel_1952/282632136.py in <module>\r\n      2                       experiment_id='1') as run:\r\n      3 \r\n----> 4                       mlflow.log_artifact(f'{path}/MLmodel', artifact_path=name)\r\n      5                       mlflow.log_artifact(f'{path}/config.yml', artifact_path=name)\r\n      6                       mlflow.log_artifact(f'{path}/conda.yaml', artifact_path=name)\r\n\r\n~/opt/anaconda3/lib/python3.9/site-packages/mlflow/tracking/fluent.py in log_artifact(local_path, artifact_path)\r\n    750     \"\"\"\r\n    751     run_id = _get_or_start_run().info.run_id\r\n--> 752     MlflowClient().log_artifact(run_id, local_path, artifact_path)\r\n    753 \r\n    754 \r\n\r\n~/opt/anaconda3/lib/python3.9/site-packages/mlflow/tracking/client.py in log_artifact(self, run_id, local_path, artifact_path)\r\n    953             is_dir: False\r\n    954         \"\"\"\r\n--> 955         self._tracking_client.log_artifact(run_id, local_path, artifact_path)\r\n    956 \r\n    957     def log_artifacts(\r\n\r\n~/opt/anaconda3/lib/python3.9/site-packages/mlflow/tracking/_tracking_service/client.py in log_artifact(self, run_id, local_path, artifact_path)\r\n    363             artifact_repo.log_artifacts(local_path, path_name)\r\n...\r\n--> 225         mkdir(name, mode)\r\n    226     except OSError:\r\n    227         # Cannot rely on checking for EEXIST, since the operating system\r\n\r\nOSError: [Errno 30] Read-only file system: '/mlflow'\r\n```"
      },
      {
        "user": "harupy",
        "body": "Looks like it's truncated. Do you have the `...` part?"
      }
    ]
  },
  {
    "issue_number": 16132,
    "title": "Need help recovering mlflow tracking server",
    "author": "TheLurps",
    "state": "closed",
    "created_at": "2025-06-07T10:18:47Z",
    "updated_at": "2025-06-16T07:36:48Z",
    "labels": [],
    "body": "I ran mlflow tracking server as quadlet on my local machine using podman:\n```\n[Container]\nContainerName=mlflow-server\nImage=ghcr.io/mlflow/mlflow:latest\nVolume=mlruns:/mlruns:Z\nVolume=mlartifacts:/mlartifacts:Z\nExec=mlflow server --backend-store-uri sqlite:///mlruns/backend.sqlite --registry-store-uri sqlite:///mlruns/registry.sqlite --serve-artifacts --artifacts-destination file:///mlartifacts --host 0.0.0.0\nPull=newer\nPublishPort=127.0.0.1:5000:5000\nAutoUpdate=registry\n\n[Service]\nTimeoutStartSec=300\nTimeoutStopSec=300\n\n[Install]\nWantedBy=default.target\n```\n\nUnfortunately I did not specify an image tag in the quadlet file. So I guess the db migration failed, because I get this error while starting the mlflow container:\n```\nJun 07 12:09:56 ws108961 mlflow-server[129349]: \nJun 07 12:09:56 ws108961 mlflow-server[129349]: The above exception was the direct cause of the following exception:\nJun 07 12:09:56 ws108961 mlflow-server[129349]: \nJun 07 12:09:56 ws108961 mlflow-server[129349]: Traceback (most recent call last):\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/mlflow/cli.py\", line 425, in server\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     initialize_backend_stores(backend_store_uri, registry_store_uri, default_artifact_root)\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/mlflow/server/handlers.py\", line 370, in initialize_backend_stores\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     _get_tracking_store(backend_store_uri, default_artifact_root)\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/mlflow/server/handlers.py\", line 347, in _get_tracking_store\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/registry.py\", line 45, in get_store\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri)\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/registry.py\", line 56, in _get_store_with_resolved_uri\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri)\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/mlflow/server/handlers.py\", line 168, in _get_sqlalchemy_store\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     return SqlAlchemyStore(store_uri, artifact_uri)\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py\", line 176, in __init__\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     mlflow.store.db.utils._initialize_tables(self.engine)\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/mlflow/store/db/utils.py\", line 105, in _initialize_tables\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     _upgrade_db(engine)\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/mlflow/store/db/utils.py\", line 230, in _upgrade_db\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     command.upgrade(config, \"heads\")\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/alembic/command.py\", line 483, in upgrade\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     script.run_env()\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/alembic/script/base.py\", line 551, in run_env\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     util.load_python_file(self.dir, \"env.py\")\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/alembic/util/pyfiles.py\", line 114, in load_python_file\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     module = load_module_py(module_id, path)\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/alembic/util/pyfiles.py\", line 134, in load_module_py\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     spec.loader.exec_module(module)  # type: ignore\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/mlflow/store/db_migrations/env.py\", line 84, in <module>\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     run_migrations_online()\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/mlflow/store/db_migrations/env.py\", line 78, in run_migrations_online\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     context.run_migrations()\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"<string>\", line 8, in run_migrations\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/alembic/runtime/environment.py\", line 946, in run_migrations\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     self.get_context().run_migrations(**kw)\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/alembic/runtime/migration.py\", line 611, in run_migrations\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     for step in self._migrations_fn(heads, self):\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/alembic/command.py\", line 472, in upgrade\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     return script._upgrade_revs(revision, rev)\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/alembic/script/base.py\", line 415, in _upgrade_revs\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     with self._catch_revision_errors(\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/contextlib.py\", line 153, in __exit__\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     self.gen.throw(typ, value, traceback)\nJun 07 12:09:56 ws108961 mlflow-server[129349]:   File \"/usr/local/lib/python3.10/site-packages/alembic/script/base.py\", line 251, in _catch_revision_errors\nJun 07 12:09:56 ws108961 mlflow-server[129349]:     raise util.CommandError(resolution) from re\nJun 07 12:09:56 ws108961 mlflow-server[129349]: alembic.util.exc.CommandError: Can't locate revision identified by '6953534de441'\nJun 07 12:09:56 ws108961 podman[129441]: 2025-06-07 12:09:56.404327311 +0200 CEST m=+0.012482491 container died 8a22443569b1ae75c79a3d6c6fd6434f6d2e495f89400aaaaf5a2c706fb09878 (image=ghcr.io/mlflow/mlflow:late>\nJun 07 12:09:56 ws108961 podman[129441]: 2025-06-07 12:09:56.624885109 +0200 CEST m=+0.233040268 container remove 8a22443569b1ae75c79a3d6c6fd6434f6d2e495f89400aaaaf5a2c706fb09878 (image=ghcr.io/mlflow/mlflow:la>\nJun 07 12:09:56 ws108961 systemd[3206]: mlflow-server.service: Main process exited, code=exited, status=1/FAILURE\n```\n\nI already tried running `mlflow db upgrade`, starting the container with various mlflow versions (v2.16.2-2.22.1) and [recovering-from-a-failed-migration](https://github.com/mlflow/mlflow/blob/master/mlflow/store/db_migrations/README.md#recovering-from-a-failed-migration), without any success.\n\nCan someone help me recovering my experiment results and getting mlflow running again?",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "@TheLurps  I believe this DB migration was added in MLflow 3.0 (perhaps initially it was pulling the RC version which has been out for a couple weeks). You mentioned you tried rerunning with MLflow 2.16-2.22, have you tried it with 3.0 / 3.1? We just released the stable version of these yesterday so you should be able to upgrade if you wish"
      },
      {
        "user": "Su3h7aM",
        "body": "Update to latest version fixed it for me"
      },
      {
        "user": "TheLurps",
        "body": "> [@TheLurps](https://github.com/TheLurps) I believe this DB migration was added in MLflow 3.0 (perhaps initially it was pulling the RC version which has been out for a couple weeks). You mentioned you tried rerunning with MLflow 2.16-2.22, have you tried it with 3.0 / 3.1? We just released the stable version of these yesterday so you should be able to upgrade if you wish\n\nI upgraded to 3.0, DB seems fine now, experiments are shown and after restoring the db backup experiment runs appeared as well. Thank you!"
      }
    ]
  },
  {
    "issue_number": 16233,
    "title": "[FR] Support token counting for integrated LLM/GenAI libraries",
    "author": "B-Step62",
    "state": "open",
    "created_at": "2025-06-12T20:30:16Z",
    "updated_at": "2025-06-16T07:32:03Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "body": "## Overview\n\nMLflow Tracing now support token count tracking since 3.1.0.\n\nIn the initial release, we only have a few libraries covered\n* [OpenAI](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/openai#token-usage)\n* [LangChain](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/langchain#token-usage-tracking)\n* [LangGraph](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/langgraph#token-usage-tracking)\n\nWe would like to expand the coverage for all 20+ tracing libraries in coming months. If you are interested in helping with one or more of them, please don't hesitate to comment on this issue!ðŸ™Œ\n\n## Tracker\n\n|Flavor|Assignee|PR|\n|:--:|:--:|:--:|\n|Anthropic|@joelrobin18|In-progress|\n|Bedrock|||\n|LlamaIndex|@sanatb187|In-progress|\n|DSPy|||\n|Gemini|@joelrobin18|In-progress|\n|Pydantic AI|@joelrobin18|In-progress|\n|AutoGen|||\n|AG2|||\n|CrewAI|||\n|Groq|@joelrobin18|https://github.com/mlflow/mlflow/pull/16258|\n|Mistral|||\n|SmolAgent|||\n|....|\n\n## References\n* General tracing contribution guide: https://mlflow.org/docs/latest/genai/tracing/integrations/contribute (Ignore step 4-5)\n* Sample implementation: https://github.com/mlflow/mlflow/blob/master/mlflow/openai/autolog.py#L331\n\n## Notes\n* Typically libraries/frameworks support various inference mode e.g. streaming, async, batch. No need to scope everything in the first path, please consult with maintainers about minimal requirements.\n* UI component for visualizing token counts are being built now. Stay tuned!",
    "comments": [
      {
        "user": "joelrobin18",
        "body": "Hi @B-Step62 Can I work on this FR?"
      },
      {
        "user": "B-Step62",
        "body": "@joelrobin18 Sure! Would you pick one or more frameworks from https://mlflow.org/docs/latest/genai/tracing/integrations?"
      },
      {
        "user": "sanatb187",
        "body": "Hi @B-Step62 I'd like to work on this FR for LlamaIndex!"
      }
    ]
  },
  {
    "issue_number": 16272,
    "title": "Update ruff version to 0.11.13 (latest version)",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-16T05:55:08Z",
    "updated_at": "2025-06-16T06:29:16Z",
    "labels": [],
    "body": "## Description\n\nUpdate ruff to the latest version (0.11.13) across the codebase to ensure we're using the most recent linting capabilities and bug fixes.\n\n## Files to Update\n\nThe following files need to be updated to use ruff version 0.11.13:\n\n1. `requirements/lint-requirements.txt` - Update the ruff dependency version\n2. `pyproject.toml` - Update the `required-version` field in the `[tool.ruff]` section\n\n## Current Status\n\n- `requirements/lint-requirements.txt`: Currently at `ruff==0.11.13` \n- `pyproject.toml`: Currently has `required-version = \"0.11.13\"`\n\n## Tasks\n\n- [ ] Update `requirements/lint-requirements.txt` to `ruff==0.11.13`\n- [ ] Update `pyproject.toml` required-version field to `\"0.11.13\"`\n- [ ] Test that linting still works correctly with the new version\n- [ ] Update any relevant documentation if needed\n\n## Acceptance Criteria\n\n- Both files are updated to use ruff version 0.11.13\n- All existing linting rules continue to work as expected\n- No breaking changes are introduced by the version update",
    "comments": []
  },
  {
    "issue_number": 16089,
    "title": "[FR] Add support for uv as an environment manager in `mlflow run`",
    "author": "isuyyy",
    "state": "open",
    "created_at": "2025-06-05T08:17:27Z",
    "updated_at": "2025-06-16T06:06:22Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nIt would be great to add uv as one of the options for the --env-manager flag when running `mlflow run`.\n\nWhen --env-manager=uv is specified, MLflow should use uv to create and manage the virtual environment.\n\n### Motivation\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\nThis would allow users to benefit from uvâ€™s faster dependency resolution and installation performance compared to pip or conda.\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "@isuyyy The feature totally makes sense, looking forward to the PR!"
      },
      {
        "user": "Anshumank399",
        "body": "I am new to the the repo and would like to help  if @isuyyy are not already working on it. "
      },
      {
        "user": "isuyyy",
        "body": "> I am new to the the repo and would like to help if [@isuyyy](https://github.com/isuyyy) are not already working on it.\n\nHi @Anshumank399, Iâ€™ve opened a PR for this. Since itâ€™s my first time contributing to the MLflow repo, Iâ€™d really appreciate it if you could take a look and let me know if your approach or thoughts differ from mine."
      }
    ]
  },
  {
    "issue_number": 16269,
    "title": "Update .github/copilot-instructions.md to enforce pull request template compliance",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-16T03:40:59Z",
    "updated_at": "2025-06-16T03:47:16Z",
    "labels": [],
    "body": "## Description\n\nThe current `.github/copilot-instructions.md` file provides basic guidelines for Copilot behavior but doesn't instruct Copilot to follow the repository's pull request template when creating PRs. This can lead to inconsistent PR descriptions and missing required information.\n\n## Proposed Changes\n\nUpdate the `.github/copilot-instructions.md` file to include instructions for Copilot to:\n\n1. **Follow the PR template structure**: Ensure all PRs created by Copilot include the required sections from `.github/pull_request_template.md`\n2. **Fill out mandatory sections**: Provide guidance on completing sections like:\n   - Related Issues/PRs\n   - What changes are proposed in this pull request?\n   - How is this PR tested?\n   - Documentation update requirements\n   - Release notes classification\n3. **Use appropriate checkboxes**: Guide Copilot to select relevant component areas, interfaces, languages, and integrations\n4. **Release classification**: Help Copilot determine appropriate release note categories\n\n## Expected Benefits\n\n- **Consistency**: All PRs will follow the same format and include required information\n- **Maintainer efficiency**: Reviewers won't need to ask for missing information\n- **Better tracking**: Proper categorization will improve release note generation\n- **Quality assurance**: Ensures testing and documentation considerations are addressed\n\n## Implementation Details\n\nThe instructions should be added as a new section in the copilot-instructions.md file, providing clear guidance on:\n- When to use each section of the PR template\n- How to determine appropriate component/area labels\n- Guidelines for release note classification\n- Best practices for linking related issues/PRs\n\nThis enhancement will improve the overall quality and consistency of PRs created by GitHub Copilot in the MLflow repository.",
    "comments": []
  },
  {
    "issue_number": 16267,
    "title": "Remove unused file: .github/workflows/sync.md",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-16T03:28:35Z",
    "updated_at": "2025-06-16T03:36:38Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "## Description\n\nThe file `.github/workflows/sync.md` appears to be no longer in use and should be removed from the repository to reduce clutter and avoid confusion.\n\n## Details\n\nThe file contains instructions for manually syncing the `mlflow-3` branch with the `master` branch, but this process may no longer be relevant or may have been automated/replaced with other workflows.\n\n## Proposed Solution\n\nRemove the file `.github/workflows/sync.md` from the repository.\n\n## Additional Context\n\nThis cleanup will help maintain a cleaner repository structure by removing outdated documentation.",
    "comments": []
  },
  {
    "issue_number": 16264,
    "title": "Remove v3 filter from patch.js after MLflow 3.0.0 release",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-16T01:21:46Z",
    "updated_at": "2025-06-16T01:42:12Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "## Description\n\nThere's a TODO in `.github/workflows/patch.js` (line 56) that needs to be addressed after MLflow 3.0.0 is released.\n\n## Current Code\n\n```javascript\n// TODO: Remove this line once MLflow 3.0.0 is released\nconst latest = releases.data.find(({ tag_name }) => !tag_name.startsWith(\"v3\"));\n```\n\n## Expected Behavior\n\nOnce MLflow 3.0.0 is officially released, this line should be removed and replaced with the standard logic to get the latest release:\n\n```javascript\nconst latest = releases.data[0]; // or similar standard approach\n```\n\n## Context\n\nThe current code filters out releases that start with \"v3\" to avoid selecting pre-release or beta versions of MLflow 3.0.0. Once 3.0.0 is stable and released, this special handling should be removed.\n\n## Location\n\nFile: `.github/workflows/patch.js`\nLine: 56",
    "comments": []
  },
  {
    "issue_number": 16252,
    "title": "[FR] Support for image-text-to-text transformers pipeline",
    "author": "dmvieira",
    "state": "open",
    "created_at": "2025-06-14T14:16:31Z",
    "updated_at": "2025-06-15T13:05:43Z",
    "labels": [
      "enhancement",
      "area/windows",
      "language/r",
      "area/uiux",
      "language/java",
      "area/model-registry",
      "area/docker",
      "integrations/azure",
      "integrations/sagemaker",
      "integrations/databricks",
      "language/new",
      "area/deployments"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nNow only supports [text-generation](https://mlflow.org/docs/latest/ml/deep-learning/transformers/task) pipeline for Chat Completion, but some models like Gemma 3 support Image-text-to-text pipelines for image and text understanding using the same Chat Completion API\n\n### Motivation\n\n> #### What is the use case for this feature?\n\nuse same Chat Completion API for models with image and texts support\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\nthere are a lot of multimodal models right now\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\nMy org is using detabricks and I need more models in serving endpoint for some projects\n\n> #### Why is it currently difficult to achieve this use case?\n\nThere is no support for Image-text-to-text transformer pipeline\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [x] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [x] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [x] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [x] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [x] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [x] `language/r`: R APIs and clients\n- [x] `language/java`: Java APIs and clients\n- [x] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [x] `integrations/azure`: Azure and Azure ML integrations\n- [x] `integrations/sagemaker`: SageMaker integrations\n- [x] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "medihack",
        "body": "I also agree that VLMs should be better supported. I just [opened a discussion yesterday](https://github.com/mlflow/mlflow/discussions/16257) about filtering out those base64 images sent to models, which currently seems impossible with automatic tracing. But a deeper integration would even make more sense."
      }
    ]
  },
  {
    "issue_number": 14745,
    "title": "[FR] Add LLM model endpoint management UI to Prompt Engineering UI",
    "author": "kimminw00",
    "state": "open",
    "created_at": "2025-02-26T07:21:14Z",
    "updated_at": "2025-06-15T11:30:17Z",
    "labels": [
      "enhancement",
      "area/tracking"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nThe current requirement to deploy `AI Gateway` before using the `Prompt Engineering UI` creates a barrier to entry. This dependency not only complicates the initial setup but also introduces additional operational overhead (e.g., maintaining the AI gateway). In contrast, applications like `Cline` allow users to leverage LLM capabilities without requiring separate gateway deployments.\n\nBy integrating endpoint management directly into the Prompt Engineering UI, users could:\nAvoid operational complexity from managing `AI Gateway`\n\n### Motivation\n\n> #### What is the use case for this feature?\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\n> #### Why is it currently difficult to achieve this use case?\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "Thank you for the report. Let us discuss internally."
      },
      {
        "user": "ThankaBharathi",
        "body": "Hi  Iâ€™d like to contribute to add endpoint management UI and backend support for LLM endpoints. Can you please point me to relevant frontend/backend modules?"
      }
    ]
  },
  {
    "issue_number": 16241,
    "title": "[DOC-FIX] mlflow.log_artifact does not accept single files",
    "author": "adrianlzt",
    "state": "open",
    "created_at": "2025-06-13T08:42:54Z",
    "updated_at": "2025-06-15T07:01:43Z",
    "labels": [
      "area/docs",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute a documentation fix independently.\n\n### URL(s) with the issue\n\nhttps://mlflow.org/docs/latest/ml/tracking/tracking-api#:~:text=Log%20single%20file/directory\n\n### Description of proposal (what needs changing)\n\nThe function cleary says that it accepts a local_dir path:\n\n```python\ndef log_artifacts(\n    local_dir: str, artifact_path: Optional[str] = None, run_id: Optional[str] = None\n) -> None:\n    \"\"\"\n    Log all the contents of a local directory as artifacts of the run. If no run is active,\n    this method will create a new active run.\n\n    Args:\n        local_dir: Path to the directory of files to write.\n        artifact_path: If provided, the directory in ``artifact_uri`` to write to.\n        run_id: If specified, log the artifacts to the specified run. If not specified, log the\n            artifacts to the currently active run.\n```\n\nPassing a file will return an empty list [in this loop of the http_artifact_repo.py](https://github.com/mlflow/mlflow/blob/5f0333c663c85c115980a17632e657ec3a87ca1f/mlflow/store/artifact/http_artifact_repo.py#L70C35-L70C53)\n```\n(Pdb) p list(os.walk(\"/etc/hosts\"))\n[]\n```",
    "comments": [
      {
        "user": "ArjunJagdale",
        "body": "_Hi ðŸ‘‹, Iâ€™d like to help fix this documentation issue. Iâ€™ll submit a PR clarifying that mlflow.log_artifacts accepts only directories, not single files._"
      }
    ]
  },
  {
    "issue_number": 13928,
    "title": "[FR] Allow us to specify tiktoken encoding for `token_count` metric",
    "author": "NevoleMarek",
    "state": "open",
    "created_at": "2024-11-29T14:15:34Z",
    "updated_at": "2025-06-14T16:57:57Z",
    "labels": [
      "enhancement",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\r\n\r\nYes. I can contribute this feature independently.\r\n\r\n### Proposal Summary\r\n\r\n[`mlflow.metric.token_count`](https://github.com/mlflow/mlflow/blob/64721e7f9da641eb9609cd49e775b13f5a0adc55/mlflow/metrics/__init__.py#L52C1-L61C6) currently uses `cl100k_base` encoding of tiktoken library. This is no longer the most up to date as that would be `o200k_base` for `gpt-4o` and `gpt-4o-mini`.\r\n\r\nI propose to make the metric to allows us to specify the tiktoken encoding.\r\n\r\n### Motivation\r\n\r\n---\r\n\r\n### Details\r\n\r\n_No response_\r\n\r\n### What component(s) does this bug affect?\r\n\r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n### What interface(s) does this bug affect?\r\n\r\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\n### What language(s) does this bug affect?\r\n\r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\n### What integration(s) does this bug affect?\r\n\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "@NevoleMarek The proposal totally makes sense! Would you go ahead and make a PR? I think we can add a new argument to `metrics.token_count` and move `_token_count_eval_fn` to an inner function."
      }
    ]
  },
  {
    "issue_number": 15452,
    "title": "[FR] log_figure with run_id",
    "author": "MarkBusschers",
    "state": "open",
    "created_at": "2025-04-23T13:09:57Z",
    "updated_at": "2025-06-14T08:09:22Z",
    "labels": [
      "enhancement",
      "area/tracking",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nIs it possible to manually provide a run_id when logging a figure (log_figure, same holds for log_image), just as in log_metrics, log_params: \nrun_id = run_id or _get_or_start_run().info.run_id\n\n### Motivation\n\n> #### What is the use case for this feature?\nMore freedom in the moment of logging a figure\n> #### Why is this use case valuable to support for MLflow users in general?\nWhen logging is separated from fitting/predicting, the model run ends\n> #### Why is this use case valuable to support for your project(s) or organization?\n\n> #### Why is it currently difficult to achieve this use case?\n\n\n### Details\n\nchange: run_id = _get_or_start_run().info.run_id\ninto: run_id = run_id or _get_or_start_run().info.run_id\n\nadd run_id as an optional argument;  run_id: Optional[str] = None\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "@MarkBusschers You can use [MlflowClient().log_figure()](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.client.html#mlflow.client.MlflowClient.log_figure) to log an image with a specific Run.\n\n```\nclient = mlflow.MlflowClient()\nclient.log_figure(run_id=..., )\n```\n\nConventionally `mlflow.log_xyz` API (so called \"fluent\" API) focuses on logging things for an **active Run**. The two APIs `log_metric ` and `log_text ` are exceptions and we actually should have not added `run_id` parameter there.\n\n> Log a metric under the current run. If no run is active, this method will create a new active run.\n\nThe client API is responsible for this use case instead.\n\n> The mlflow.client module provides a Python CRUD interface to MLflow Experiments, Runs, Model Versions, and Registered Models. This is a lower level API that directly translates to MLflow [REST API](https://mlflow.org/docs/latest/api_reference/rest-api.html) calls. For a higher level API for managing an â€œactive runâ€, use the [mlflow](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.html#module-mlflow) module.\n\nThat said, another option is to add `run_id` to every fluent API and remove this convention, as @joelrobin18 does in the PR.   Curious what @BenWilson2 @harupy think here."
      }
    ]
  },
  {
    "issue_number": 15111,
    "title": "[BUG] MLFlow logger raising `invalid artifact path`",
    "author": "marinegor",
    "state": "open",
    "created_at": "2025-03-25T16:39:40Z",
    "updated_at": "2025-06-14T03:30:11Z",
    "labels": [
      "bug",
      "has-closing-pr"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.20.0\n- Tracking server: 2.20.0\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\n- **Python version**:\n- **yarn version, if running the dev UI**:\n\n\n### Describe the problem\n\n\nI'm running training witht pytorch lightning, important part of my config:\n\n```\n  callbacks:\n    - class_path: lightning.pytorch.callbacks.ModelCheckpoint\n      init_args:\n        dirpath: \"checkpoints\"\n        monitor: \"training/loss\"\n        filename: \"plm-{epoch:02d}-{training/loss:.4f}\"\n        auto_insert_metric_name: false\n        save_weights_only: true\n        mode: min\n        save_last: link\n        save_top_k: 1\n```\n\nRunning training results in:\n\n```\n[rank0]: mlflow.exceptions.MlflowException: Invalid artifact path: 'plm-00-3.1752-v2'. Names may be treated as files in certain cases, and must not re\nsolve to other names when treated as such. This name would resolve to 'plm-00-3.1752-v2'                                                              \n```\n\nhowever, entering under debugger shows that conditions in `path_not_unique` are following:\n\n\n```\n-------------------- norm='plm-00-3.1752-v2' --------------------                                                                                     \n-------------------- name=PosixPath('plm-00-3.1752-v2') --------------------                                                                          \n-------------------- conditions={'norm != name': True, \"norm == '.'\": False, \"norm.startswith('..')\": False, \"norm.startswith('/')\": False} ----------\n----------                                                                                                                                            \n```\n\nHence, I propose changing these conditions to `norm != str(name)` since `name` isn't always `str`.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "Thanks for the report and the fix! it makes sense to me"
      },
      {
        "user": "daniellok-db",
        "body": "cc @marinegor could you make that PR to `mlflow/mlflow`? it looks like it's currently targeting your fork"
      },
      {
        "user": "marinegor",
        "body": "@daniellok-db my bad, fixed it now. "
      }
    ]
  },
  {
    "issue_number": 16157,
    "title": "[BUG] mlflow.log_metric failing for asynchronous logging on distributed training",
    "author": "ZhiliangWu",
    "state": "open",
    "created_at": "2025-06-09T12:46:06Z",
    "updated_at": "2025-06-13T19:47:04Z",
    "labels": [
      "bug",
      "area/tracking",
      "integrations/azure"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nAzure Machine Learning\n\n### MLflow version\n\n- Client\n\n```console\n# mlflow --version\nmlflow, version 2.22.0\n# pip freeze | grep mlflow\nazureml-mlflow==1.60.0.post1\nmlflow==2.22.0\nmlflow-skinny==2.22.0\n```\n- Tracking server: \n\nWe used the tracking on Azure Machine Learning workspace pre-defined with \"MLflow tracking URI\"\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**\n\n```console\n# lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 22.04.5 LTS\nRelease:        22.04\nCodename:       jammy\n```\n\n- **Python version**: `3.12.10`\n\n\n### Describe the problem\n\nWe followed the [`Log metrics asynchronously` tutorial ](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-log-view-metrics?view=azureml-api-2&tabs=interactive#log-metrics-asynchronously) from Azure Machine Learning to enable the asynchronous metrics logging.  To be concrete, we injected the following code to log metrics asynchronously in distributed training \n\n```python\n mlflow.log_metric(\"loss\", loss.item(), synchronous=False)\n```\n\nHowever, we found the logging got blocked when we use this in a distributed training. \n\nIn a training epoch where we enable this metric logging on all 16 ranks/processes(/GPUs), we get the following tracing information on the main rank/process.  We see mlflow tracking took a lot of time for the `log_metric` function and most of it is `sleep` \n\n<img width=\"2088\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8e03ecd7-1a25-44ee-a6f4-c2a5124fca66\" />\n\nFor comparison, in a training epoch where we enable this metric logging ONLY on the main rank/process, we have the following tracing information, where we don't see any `sleep` (also note the scale of the time axis being two seconds in comparison with the 20 seconds above)\n\n<img width=\"2248\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4be10469-6e14-466b-bf3e-75b8a8a40352\" />\n\nAs a results, the time for training one epoch with logging on all ranks takes 40-50 seconds, whereas the time with logging ONLY on the main rank took 8 seconds. Such bug was highly unexpected, took us a long time to find as it was originally used in a more complicated setups than the toy example we share here.\n\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\nThe training code we run was mostly taken from the huggingface accelerate examples to train a BERT model at https://github.com/huggingface/accelerate/blob/main/examples/nlp_example.py. We added the metrics logging with `mlflow` and the [profiling utilities](https://docs.pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#using-profiler-to-analyze-long-running-jobs) to generate the tracing file pasted above. The code is run in Azure Machine Learning workspaces with a distributed training setup according to [the respective tutorial ](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-distributed-gpu?view=azureml-api-2) and to the [accelerate discussion](https://github.com/huggingface/accelerate/issues/3494). More details on the Azure setup I will add in the next sections. Following are the complete script we run to reproduce the issue I described above\n\n\n```python\n# Modified from: https://github.com/huggingface/accelerate/blob/main/examples/nlp_example.py\n# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport argparse\nimport mlflow\nimport time\nfrom pathlib import Path\nimport os\n\nimport evaluate\nimport torch\nfrom datasets import load_dataset\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom torch.profiler import ProfilerActivity\nfrom torch.profiler import profile\nfrom torch.profiler import schedule\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n\nfrom accelerate import Accelerator, DistributedType\n\n\n########################################################################\n# This is a fully working simple example to use Accelerate\n#\n# This example trains a Bert base model on GLUE MRPC\n# in any of the following settings (with the same script):\n#   - single CPU or single GPU\n#   - multi GPUS (using PyTorch distributed mode)\n#   - (multi) TPUs\n#   - fp16 (mixed-precision) or fp32 (normal precision)\n#\n# To run it in each of these various modes, follow the instructions\n# in the readme for examples:\n# https://github.com/huggingface/accelerate/tree/main/examples\n#\n########################################################################\n\n\nMAX_GPU_BATCH_SIZE = 16\nEVAL_BATCH_SIZE = 32\n\n\ndef get_dataloaders(accelerator: Accelerator, batch_size: int = 16):\n    \"\"\"\n    Creates a set of `DataLoader`s for the `glue` dataset,\n    using \"bert-base-cased\" as the tokenizer.\n\n    Args:\n        accelerator (`Accelerator`):\n            An `Accelerator` object\n        batch_size (`int`, *optional*):\n            The batch size for the train and validation DataLoaders.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n    datasets = load_dataset(\"glue\", \"mrpc\")\n\n    def tokenize_function(examples):\n        # max_length=None => use the model max length (it's actually the default)\n        outputs = tokenizer(\n            examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, max_length=None)\n        return outputs\n\n    # Apply the method we just defined to all the examples in all the splits of the dataset\n    # starting with the main process first:\n    # with accelerator.main_process_first():\n    tokenized_datasets = datasets.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\n    )\n\n    # We also rename the 'label' column to 'labels' which is the expected name for labels by the models of the\n    # transformers library\n    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n\n    def collate_fn(examples):\n        # For Torchxla, it's best to pad everything to the same length or training will be very slow.\n        max_length = 128 if accelerator.distributed_type == DistributedType.XLA else None\n        # When using mixed precision we want round multiples of 8/16\n        if accelerator.mixed_precision == \"fp8\":\n            pad_to_multiple_of = 16\n        elif accelerator.mixed_precision != \"no\":\n            pad_to_multiple_of = 8\n        else:\n            pad_to_multiple_of = None\n\n        return tokenizer.pad(\n            examples,\n            padding=\"longest\",\n            max_length=max_length,\n            pad_to_multiple_of=pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n\n    # Instantiate dataloaders.\n    train_dataloader = DataLoader(\n        tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size, drop_last=True\n    )\n    eval_dataloader = DataLoader(\n        tokenized_datasets[\"validation\"],\n        shuffle=False,\n        collate_fn=collate_fn,\n        batch_size=EVAL_BATCH_SIZE,\n        drop_last=(accelerator.mixed_precision == \"fp8\"),\n    )\n\n    return train_dataloader, eval_dataloader\n\n\ndef training_function(config, args):\n    # Initialize accelerator\n    accelerator = Accelerator(\n        cpu=args.cpu, mixed_precision=args.mixed_precision)\n    # Sample hyper-parameters for learning rate, batch size, seed and a few other HPs\n    lr = config[\"lr\"]\n    num_epochs = int(config[\"num_epochs\"])\n    seed = int(config[\"seed\"])\n    batch_size = int(config[\"batch_size\"])\n\n    metric = evaluate.load(\"glue\", \"mrpc\")\n\n    # If the batch size is too big we use gradient accumulation\n    gradient_accumulation_steps = 1\n    if batch_size > MAX_GPU_BATCH_SIZE and accelerator.distributed_type != DistributedType.XLA:\n        gradient_accumulation_steps = batch_size // MAX_GPU_BATCH_SIZE\n        batch_size = MAX_GPU_BATCH_SIZE\n\n    set_seed(seed)\n    train_dataloader, eval_dataloader = get_dataloaders(\n        accelerator, batch_size)\n    # Instantiate the model (we build the model here so that the seed also control new weights initialization)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"bert-base-cased\", return_dict=True)\n\n    # We could avoid this line since the accelerator is set with `device_placement=True` (default value).\n    # Note that if you are placing tensors on devices manually, this line absolutely needs to be before the optimizer\n    # creation otherwise training will not work on TPU (`accelerate` will kindly throw an error to make us aware of that).\n    model = model.to(accelerator.device)\n    # Instantiate optimizer\n    optimizer = AdamW(params=model.parameters(), lr=lr)\n\n    # Instantiate scheduler\n    lr_scheduler = get_linear_schedule_with_warmup(\n        optimizer=optimizer,\n        num_warmup_steps=100,\n        num_training_steps=(len(train_dataloader) *\n                            num_epochs) // gradient_accumulation_steps,\n    )\n\n    # Prepare everything\n    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n    # prepare method.\n\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    dir_name = f\"{args.profiler_dir}/rank_{os.environ['RANK']}/\"\n    Path(dir_name).mkdir(parents=True, exist_ok=True)\n    profiler = profile(\n        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n        schedule=schedule(wait=1, warmup=1, active=1, repeat=1),\n        # During warmup profiler is disabled, to reduce the overhead at the beginning of profiling\n        # That could skew the profiling result. During active steps, the profiler works and records events.\n        record_shapes=True,\n        profile_memory=True,\n        with_stack=True,  # this requires 3.12.10\n        with_flops=False,\n        on_trace_ready=lambda p: p.export_chrome_trace(\n            dir_name + f\"step_{p.step_num}.pt.trace.json.gz\"),\n    )\n    mlflow.config.enable_async_logging()\n    mlflow.start_run()\n    profiler.start()\n\n    # Now we train the model\n    for epoch in range(num_epochs):\n        start_time = time.time()\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            # We could avoid this line since we set the accelerator with `device_placement=True`.\n            batch.to(accelerator.device)\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss / gradient_accumulation_steps\n            accelerator.backward(loss)\n            if step % gradient_accumulation_steps == 0:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n            if args.mlflow_on_main.lower() == \"true\":\n                # print(\"Log on the main rank\")\n                if accelerator.is_main_process:\n                    mlflow.log_metric(\"loss\", loss.item(), synchronous=False)\n            else:\n                # print(\"Log on all ranks\")\n                mlflow.log_metric(\"loss\", loss.item(), synchronous=False)\n\n        model.eval()\n        for step, batch in enumerate(eval_dataloader):\n            # We could avoid this line since we set the accelerator with `device_placement=True`.\n            batch.to(accelerator.device)\n            with torch.no_grad():\n                outputs = model(**batch)\n            predictions = outputs.logits.argmax(dim=-1)\n            predictions, references = accelerator.gather_for_metrics(\n                (predictions, batch[\"labels\"]))\n            metric.add_batch(\n                predictions=predictions,\n                references=references,\n            )\n\n        eval_metric = metric.compute()\n        # Use accelerator.print to print only on the main process.\n        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n\n        end_time = time.time()\n        elapsed_time_seconds = end_time - start_time\n        print(f\"One epoch training time: {elapsed_time_seconds:.2f} seconds\")\n\n        profiler.step()\n\n    accelerator.end_training()\n\n    profiler.stop()\n\n    mlflow.end_run()\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Simple example of training script.\")\n    parser.add_argument(\n        \"--mixed_precision\",\n        type=str,\n        default=None,\n        choices=[\"no\", \"fp16\", \"bf16\", \"fp8\"],\n        help=\"Whether to use mixed precision. Choose\"\n        \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n        \"and an Nvidia Ampere GPU.\",\n    )\n    parser.add_argument(\n        \"--mlflow_on_main\",\n        type=str,\n        default=\"False\")\n    parser.add_argument(\n        \"--profiler_dir\",\n        type=str)\n    parser.add_argument(\"--cpu\", action=\"store_true\",\n                        help=\"If passed, will train on the CPU.\")\n    args = parser.parse_args()\n    config = {\"lr\": 2e-5, \"num_epochs\": 3, \"seed\": 42, \"batch_size\": 16}\n    training_function(config, args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Other info / logs\n\nFollowing is the Azure ML [command job YAML](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-job-command?view=azureml-api-2) file we used to run the script above. The compute we used is [NCads H100 v5-Serie](https://learn.microsoft.com/de-de/azure/virtual-machines/ncads-h100-v5)\n\n```yaml\n# https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-job-command?view=azureml-api-2#yaml-distributed-pytorch\n$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json\ncode: ../src\ninputs:\n  mlflow_on_main: false  # or true\noutputs:\n  output_dir:\n    type: uri_folder\n    mode: rw_mount\ncommand: >-\n  python nlp_example_mlflow.py --mlflow_on_main ${{inputs.mlflow_on_main}} --profiler_dir ${{outputs.output_dir}} \nenvironment:\n    image: # an environment build on top of nvidia/cuda:12.8.1-devel-ubuntu22.04 with other python dependencies\ncompute: # our compute cluster name\ndistribution:\n  type: pytorch\n  process_count_per_instance: 2\nresources:\n  instance_count: 8\ndisplay_name: Simple-NLP-example-Multi-Node-Single-Process\nexperiment_name: Simple-NLP-example\ndescription: The nlp_example.py script is a simple example to train a Bert model on a classification task (GLUE's MRPC).\n```\n\nFor the Docker file where we built environment, we have \n\n```Dockerfile\nFROM nvidia/cuda:12.8.1-devel-ubuntu22.04\n\nENV DEBIAN_FRONTEND=noninteractive \\\n    PYTHON_VERSION=3.12 \\\n    POETRY_VERSION=2.1.1 \\\n    # Set path for default python interpreter and poetry\n    PATH=\"/.venv/bin:/root/.local/bin:$PATH\" \\\n    POETRY_VIRTUALENVS_IN_PROJECT=1 \\\n    POETRY_CACHE_DIR=/tmp/poetry_cache\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    software-properties-common \\\n    && add-apt-repository \"ppa:deadsnakes/ppa\" -y \\\n    && apt-get update && apt-get install -y --no-install-recommends \\\n    curl \\\n    python${PYTHON_VERSION} \\\n    python${PYTHON_VERSION}-venv \\\n    python${PYTHON_VERSION}-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 120 \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN curl -sSL https://install.python-poetry.org | python3 -\n\nCOPY pyproject.toml .\nCOPY poetry.lock .\n\nRUN poetry install --no-root  \\\n    && rm -rf /tmp/poetry_cache\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [x] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "cc @chenmoneygithub do you have any tips on how to go about debugging this?"
      },
      {
        "user": "chenmoneygithub",
        "body": "It's very hard to 100% reproduce a distributed training workloads with the infra we have access to, but we can try spinning up multiple processes (macbook pro should be sufficient) and log to the same run aggressively with async logging. \n\n@ZhiliangWu To resolve your issue, you may want to consider adding batching to the metrics, for example: https://github.com/mosaicml/composer/blob/0eec49da42e7f617329f035853800211f0a54ca3/composer/loggers/mlflow_logger.py#L194, which groups the metrics at the client side, and make network calls to log to MLflow server periodically, so your network I/O is reduced."
      },
      {
        "user": "ZhiliangWu",
        "body": "@daniellok-db @chenmoneygithub Thanks for looking at this! ðŸ™‡ðŸ¼ \n\n@chenmoneygithub The linked code refers to `os.environ['MLFLOW_ASYNC_LOGGING_BUFFERING_SECONDS']`. How does it relate to the batching operations to the metrics? Could you elaborate which changes are you recommending to make here?\n\nIn addition, if you think you need me to add any debugging/logging code in the attached script and rerun it, I am also happy to contribute like that. \n"
      }
    ]
  },
  {
    "issue_number": 16110,
    "title": "LangGraph cross version test failure",
    "author": "TomeHirata",
    "state": "closed",
    "created_at": "2025-06-06T03:00:49Z",
    "updated_at": "2025-06-13T19:14:54Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "Several LangGraph related cross version tests are failing due to an inexistent module `langgraph.graph.graph`. Need to check https://github.com/langchain-ai/langgraph to see what the correct module and class name are.\n\nhttps://github.com/mlflow/dev/actions/runs/15468025398/job/43568503191\n\n```\nFAILED | MEM 0.8/15.6 GB | DISK 52.1/71.6 GB tests/langgraph/test_langgraph_autolog.py::test_langgraph_save_as_code - mlflow.exceptions.MlflowException: Failed to import code model from /home/runner/work/dev/dev/tests/langgraph/sample_code/langgraph_prebuilt.py. Error: No module named 'langgraph.graph.graph'\nFAILED | MEM 0.8/15.6 GB | DISK 52.1/71.6 GB tests/langgraph/test_langgraph_autolog.py::test_langgraph_tracing_prebuilt - mlflow.exceptions.MlflowException: Failed to import code model from /home/runner/work/dev/dev/tests/langgraph/sample_code/langgraph_prebuilt.py. Error: No module named 'langgraph.graph.graph'\nFAILED | MEM 0.8/15.6 GB | DISK 52.1/71.6 GB tests/langgraph/test_langgraph_autolog.py::test_langgraph_tracing_with_custom_span - mlflow.exceptions.MlflowException: Failed to import code model from /home/runner/work/dev/dev/tests/langgraph/sample_code/langgraph_with_custom_span.py. Error: No module named 'langgraph.graph.graph'\nFAILED | MEM 0.8/15.6 GB | DISK 52.1/71.6 GB tests/langgraph/test_langgraph_autolog.py::test_langgraph_chat_agent_trace - mlflow.exceptions.MlflowException: Failed to import code model from /home/runner/work/dev/dev/tests/langgraph/sample_code/langgraph_chat_agent.py. Error: No module named 'langgraph.graph.graph'\n```",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "@harupy @B-Step62 can you assign this ticket to Copilot?"
      }
    ]
  },
  {
    "issue_number": 16244,
    "title": "[FR] Display in-progress traces",
    "author": "fschuh",
    "state": "open",
    "created_at": "2025-06-13T13:57:43Z",
    "updated_at": "2025-06-13T14:00:44Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nmlflow is currently unable to display a trace that is in progress. \nIt displays the following error message instead:\n```\nTrace data not available\nTrace data is not available for in-progress traces. Please wait for the trace to complete.\n```\n\nFor longer running actions such as those in agentic applications, this is an issue. It's important to be able to track down what the agent is doing before it's fully completed its task.\n\nMost other AI-focused observability tools (i.e. Langfuse, Langsmith, Arize Phoenix, etc) don't have this problem and can display traces that haven't yet completed.\n\nAny particular reason this limitation exists? Are there plans to support this in mlflow? \n\n### Motivation\n\n> #### What is the use case for this feature?\nBeing able to visualize longer running actions before they complete.\n \n",
    "comments": []
  },
  {
    "issue_number": 16242,
    "title": "Add CircleCI step to show docs/build/latest directory size",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-13T09:11:04Z",
    "updated_at": "2025-06-13T12:41:44Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "## Description\n\nAdd a step in the CircleCI configuration (`.circleci/config.yml`) to display the size of the `docs/build/latest` directory. This will help monitor the documentation build size and identify potential issues with documentation generation.\n\n## Proposed Changes\n\nAdd a new step in the appropriate CircleCI job that runs after the documentation is built to show:\n- The total size of the `docs/build/latest` directory\n- Optionally, a breakdown of the largest files/subdirectories\n\n## Benefits\n\n- Better visibility into documentation build output size\n- Easier debugging of documentation build issues\n- Monitoring for unexpected size increases\n\n## Implementation Notes\n\nThe step should use standard Unix tools like `du` to calculate and display directory sizes in a human-readable format.",
    "comments": []
  },
  {
    "issue_number": 16230,
    "title": "Lint Rule to Prevent Unnamed `threading.Thread`",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-12T15:06:21Z",
    "updated_at": "2025-06-13T08:18:20Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "# Proposal: Lint Rule to Prevent Unnamed `threading.Thread`\n\n## Summary\n\nThis proposal suggests implementing a lint rule to prevent the creation of unnamed `threading.Thread` instances in Python code. Unnamed threads make debugging and monitoring significantly more difficult, especially in complex applications.\n\n## Problem\n\nWhen creating threads without explicit names using `threading.Thread()`, Python assigns generic names like \"Thread-1\", \"Thread-2\", etc. This creates several issues:\n\n1. **Debugging Difficulty**: Stack traces and logs show generic thread names, making it hard to identify which part of the code spawned a problematic thread\n2. **Monitoring Challenges**: Thread monitoring tools cannot provide meaningful insights when all threads have generic names\n3. **Code Maintainability**: Developers cannot easily understand the purpose of threads when reviewing code or investigating issues\n\n## Examples\n\n### Bad (should be flagged):\n\n```python\nimport threading\n\n# Unnamed thread - difficult to debug\nthread = threading.Thread(target=my_function)\nthread.start()\n\n# Also bad - using positional args without name\nthread = threading.Thread(my_function, (arg1, arg2))\nthread.start()\n```\n\n### Good (should pass):\n\n```python\nimport threading\nimport uuid\n\n# Named thread - easy to identify in debugging\nthread = threading.Thread(target=my_function, name=\"data_processor\")\nthread.start()\n\n# Also good - using keyword args with name\nthread = threading.Thread(target=my_function, args=(arg1, arg2), name=\"background_worker\")\nthread.start()\n\n# Unique names for multiple similar threads\nthread = threading.Thread(target=my_function, name=f\"worker_{uuid.uuid4().hex[:8]}\")\nthread.start()\n```\n\n## Proposed Rule\n\nThe lint rule should:\n\n1. **Flag** any `threading.Thread()` instantiation that doesn't include a `name` parameter\n2. **Suggest** adding a descriptive `name` parameter\n3. **Allow** exemptions via comments (e.g., `# clint: disable=unnamed-thread`) for rare cases where unnamed threads are intentional\n\n## Implementation Considerations\n\n- **Scope**: Apply to direct `threading.Thread` instantiation\n- **Severity**: Warning level (not error) to allow gradual adoption\n- **Framework Compatibility**: Consider exemptions for testing frameworks that may create temporary threads\n- **Performance**: Rule should have minimal impact on linting performance\n\n## Benefits\n\n1. **Improved Debugging**: Thread names appear in stack traces and debugging tools\n2. **Better Monitoring**: APM and monitoring tools can track threads by meaningful names\n3. **Enhanced Code Quality**: Forces developers to think about thread purpose when creating them\n4. **Easier Maintenance**: Code reviews and troubleshooting become more efficient\n\n## Migration Path\n\nFor existing codebases:\n\n1. Start with warnings only\n2. Provide automated fixes where possible\n3. Allow gradual migration with disable comments\n4. Eventually promote to error level after adoption period\n\n## Implementation in MLflow's Custom Linter (Clint)\n\nMLflow has a custom linter called **Clint** located in `dev/clint/` that enforces rules not covered by ruff. This threading rule would fit perfectly into this framework.\n\n### Code Structure References\n\n- **Main linter logic**: `dev/clint/src/clint/linter.py:273-689` (Linter class)\n- **Rule definitions**: `dev/clint/src/clint/rules.py` (where new rule should be added)\n- **Configuration**: `dev/clint/src/clint/config.py`\n- **Usage**: Via `clint file.py` command or integrated with VS Code\n\n### Implementation Steps\n\n1. **Add rule class** in `dev/clint/src/clint/rules.py`:\n\n   ```python\n   class UnnamedThread(Rule):\n       def _id(self) -> str:\n           return \"MLF0024\"  # Next available ID\n\n       def _message(self) -> str:\n           return \"threading.Thread() calls should include a 'name' parameter for easier debugging\"\n\n       @staticmethod\n       def check(node: ast.Call) -> bool:\n           \"\"\"\n           Returns True if the call is threading.Thread() without a name parameter.\n           \"\"\"\n           # Check if it's a threading.Thread call\n           if not UnnamedThread._is_threading_thread_call(node):\n               return False\n\n           # Check if name parameter is provided\n           return not UnnamedThread._has_name_parameter(node)\n\n       @staticmethod\n       def _is_threading_thread_call(node: ast.Call) -> bool:\n           \"\"\"Check if this is a threading.Thread() call.\"\"\"\n           if isinstance(node.func, ast.Attribute):\n               return (\n                   isinstance(node.func.value, ast.Name)\n                   and node.func.value.id == \"threading\"\n                   and node.func.attr == \"Thread\"\n               )\n           elif isinstance(node.func, ast.Name):\n               return node.func.id == \"Thread\"  # Direct import case\n           return False\n\n       @staticmethod\n       def _has_name_parameter(node: ast.Call) -> bool:\n           \"\"\"Check if the call includes a name parameter.\"\"\"\n           # Check keyword arguments\n           for keyword in node.keywords:\n               if keyword.arg == \"name\":\n                   return True\n           return False\n   ```\n\n2. **Add detection logic** in `dev/clint/src/clint/linter.py`:\n\n   In the `visit_Call()` method around line 608, add:\n\n   ```python\n   if rules.UnnamedThread.check(node):\n       self._check(Location.from_node(node), rules.UnnamedThread())\n   ```\n\n3. **Add tests** in `tests/dev/clint/`:\n   - Create test cases for both violation and passing scenarios\n   - Test both `threading.Thread()` and direct `Thread()` import patterns\n   - Verify the disable comment functionality works\n\n## Acceptance Criteria\n\n- [ ] Rule correctly identifies unnamed `threading.Thread()` calls\n- [ ] Rule handles both `threading.Thread()` and `from threading import Thread` patterns\n- [ ] Rule can be disabled with `# clint: disable=unnamed-thread` comments\n- [ ] Tests cover all edge cases and scenarios\n- [ ] Documentation is updated if needed\n",
    "comments": []
  },
  {
    "issue_number": 5931,
    "title": "[FR] `log_text` support for arbitrary plain text file extensions",
    "author": "edemattos",
    "state": "open",
    "created_at": "2022-05-22T22:05:48Z",
    "updated_at": "2025-06-13T08:07:00Z",
    "labels": [
      "enhancement",
      "area/uiux",
      "help wanted",
      "area/artifacts",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\n\n### Proposal Summary\n\nThe UI does not render plain text for artifacts logged with a custom file extension. `MLflow.log_text()` will write the file to the `mlruns` directory and appear in the UI, but the file contents are not displayed. Just the message: \"Select a file to preview. Supported formats: image, text, html, pdf, geojson files.\"\n\n### Motivation\n\n> #### What is the use case for this feature? Why is this use case valuable to support for MLflow users in general?\r\n\r\nThis would enhance the existing feature to accommodate any plain text file not limited to `.txt` or `.log`.\r\n\r\n> #### Why is this use case valuable to support for your project(s) or organization?\r\n\r\nWe use a few custom file extensions to identify certain output types, and would like to log all outputs in a single place.\r\n\r\n> #### Why is it currently difficult to achieve this use case?\r\n\r\nIf I simply change our custom extensions to `.txt` or `.log` when logging, then the contents are properly shown in the UI. But this necessitates writing the file twice to comply with our internal tools while also being able to easily view them in the UI alongside the rest of the model outputs.\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [X] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [X] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "dbczumar",
        "body": "@sunishsheth2009 @xanderwebs Do you have any thoughts here about rendering of arbitrary artifacts? Is it safe to attempt to render them as text?"
      },
      {
        "user": "xanderwebs",
        "body": "Seems okay as long as we're making sure we escape the text properly to avoid code injections"
      },
      {
        "user": "sunishsheth2009",
        "body": "Agree with Alex, we need to make sure the text is HTML safe using something like html_sanitize. "
      }
    ]
  },
  {
    "issue_number": 16235,
    "title": "[FR] How to change the length limit of prompt? (INVALID_PARAMETER_VALUE: Prompt text exceeds max length of 5000 characters.)",
    "author": "tingjun-cs",
    "state": "open",
    "created_at": "2025-06-13T02:35:32Z",
    "updated_at": "2025-06-13T03:02:43Z",
    "labels": [
      "enhancement"
    ],
    "body": "\n\n### Proposal Summary\n\n\nI tried to register a prompt using the following code, but encountered an error:  \n`INVALID_PARAMETER_VALUE: Prompt text exceeds max length of 5000 characters.`  \n\nIt appears MLflow has a length restriction for prompts. How can I change this length limit? I have sufficient storage space available.\n\nI've tried both the local file system and MySQL as the `backend_store`, but the same error occurs.  \n\n```python\ndef register_prompt(self, name: str, prompt_text: str, alias: str = None) -> str:\n\n    prompt = mlflow.genai.register_prompt(\n        name=name,\n        template=prompt_text,\n    )\n```\n\n",
    "comments": []
  },
  {
    "issue_number": 16123,
    "title": "[FR] Add \"ranker\" model_type support to mlflow.evaluate()",
    "author": "kingdavescott",
    "state": "open",
    "created_at": "2025-06-06T15:55:30Z",
    "updated_at": "2025-06-13T02:06:50Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\n\n### Proposal Summary\n\nI'd like to propose adding support for a new `model_type` called \"ranker\" to mlflow.evaluate() for ML problems where the evaluation set is broken up into groups of \"queries\" with scored \"items\" that need to be ranked _within each group_. This feature would allow MLflow to natively support the evaluation of ranking and recommendation models by providing essential metrics like NDCG, MAP@K, and MRR. Currently, users need external tools for these evaluations, but integrating \"ranker\" would streamline workflows and make MLflow a more comprehensive platform for the entire machine learning lifecycle specifically for ranking problems.\n\n### Motivation\n\n> #### What is the use case for this feature?\n\nThis feature primarily addresses the need for comprehensive and standardized evaluation of ranking models within consumer-facing applications. For products like e-commerce search, personalized content feeds, or streaming service recommendations, the quality of a ranked list directly impacts user engagement and satisfaction. Data scientists and ML engineers currently lack a native, integrated method within MLflow to evaluate these models using critical ranking-specific metrics (e.g., NDCG, MAP@K, MRR), leading to fragmented workflows and hindering rapid iteration on critical user-facing features.\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\nSupporting this use case is valuable for MLflow users in general because it significantly expands MLflow's utility to a major segment of the ML ecosystem and promotes best practices for MLOps.\n\n1. Broadens MLflow's Scope: Ranking and recommendation systems are ubiquitous in modern applications. By supporting a \"ranker\" model_type, MLflow becomes a more complete and versatile platform, enabling a much wider array of ML practitioners to manage their entire ML lifecycle within a single tool. This attracts new users who previously found MLflow lacking in this area.\n2. Standardizes MLOps for Ranking: Currently, evaluating ranking models often involves disparate tools. This leads to inconsistent evaluation methodologies, difficulty in comparing experiments, and increased operational overhead. Integrating ranking evaluation into mlflow.evaluate() would provide a standardized, opinionated approach, promoting reproducibility and reducing cognitive load for users.\n4. Accelerates Experimentation and Deployment: When evaluation is integrated, users can more quickly iterate on model improvements, compare different algorithms (e.g., neural ranking models vs. tree-based models), and confidently select the best-performing model for deployment. This directly translates to faster innovation cycles and quicker delivery of improved user experiences.\n6. Enhances Model Observability and Auditability: By logging ranking metrics directly within MLflow Tracking using mlflow.evaluate() natively, users gain a centralized view of their model's performance over time and across different runs. This improves observability, makes it easier to audit model behavior, and aids in debugging and performance regression analysis.\n7. Reduces Tool Sprawl: MLflow's value proposition is to provide a comprehensive platform. By absorbing a common evaluation need currently met by external tools, it helps users consolidate their MLOps stack, reducing complexity and maintenance burden.\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\nFor our organization, which has many ML ranking applications powering our personalization systems, the inability to use mlflow.evaluate() for these specific problems creates a critical gap in our otherwise standardized ML lifecycle.\n\nHere's why supporting this use case is critically valuable for us:\n\n1. Bridging the mlflow.evaluate() Gap for Ranking: We extensively use MLflow for all other aspects of our model lifecycle â€“ from experiment tracking and parameter logging to model registry and deployment. However, when it comes to evaluating our ranking models, we hit a wall. mlflow.evaluate() currently does not support the necessary ranking metrics or data structures, forcing us out of the MLflow ecosystem precisely at the evaluation stage. This feature would close this critical gap, allowing us to leverage the convenience and automation of mlflow.evaluate() for all our ML problems, including ranking.\n3. Driving True End-to-End MLflow Standardization: The lack of native ranking evaluation means our ML project pipelines are not fully standardized across the organization. For ranking problems, we are presently forced to leverage external tools like Evidently to compute metrics and create custom visualizations. This creates a dual workflow: one streamlined within MLflow for classification/regression, and another fragmented one for ranking. Implementing \"ranker\" model_type in mlflow.evaluate() would drive genuine end-to-end standardization of our ML project pipelines across our entire organization.\n5. Significant Reduction in ML Practitioner Cognitive Load and Overhead: Our data scientists and ML engineers experience considerable friction due to this missing capability. They must:\n- Learn and maintain separate, non-MLflow specific evaluation code for ranking.\n- Perform manual data transformations to fit external tool requirements.\n- Explicitly write code to re-log metrics and artifacts from these external tools back into MLflow, if they want them tracked. This \"context switching\" and manual work increase cognitive load and development overhead. By enabling mlflow.evaluate() for ranking, we can drastically reduce this burden, allowing our practitioners to focus on model development rather than evaluation pipeline plumbing.\n10. Enhanced Comparability and Reproducibility for Critical Ranking Models: With a standardized mlflow.evaluate() process for ranking, we would gain immediate benefits in comparing different ranking model iterations side-by-side in the MLflow UI, ensuring consistent evaluation criteria. This is paramount for our ranking applications where small improvements in ranking quality can lead to significant business impact.\n\n> #### Why is it currently difficult to achieve this use case?\n\nIt is currently difficult to achieve this use case primarily because MLflow's existing mlflow.evaluate() capabilities are designed for classification and regression problems among others, and inherently lack the specialized understanding of group-based ranking as far as I can tell.\n\nHere's why related MLflow features are insufficient:\n\n1. mlflow.evaluate()'s model_type limitations:\n- \"classifier\": This model_type expects binary or multi-class labels and produces metrics like accuracy, precision, recall, F1-score, ROC AUC, etc. While some ranking problems can be reframed as pairwise classification (e.g., item A is better than item B), this doesn't directly give overall list-wise ranking metrics like NDCG or MAP@K, nor does it inherently understand the concept of a \"query group.\"\n- \"regressor\": This model_type expects continuous target values and outputs metrics like RMSE, MAE, R-squared. While a ranker might predict a relevance score, simply evaluating it as a regressor doesn't capture the ordering quality within a group of items, nor does it provide standard ranking metrics. A low RMSE on scores doesn't guarantee a good ranking order.\n2. Absence of Grouping/Query Context: Ranking metrics are fundamentally calculated within a \"query\" or \"group\" context (e.g., \"for search query 'red shoes', how well did the items rank?\"). MLflow's current evaluation functions do not have a built-in mechanism to specify or understand these groups, making it impossible to correctly compute metrics like NDCG@K, MAP@K, or MRR, which require knowledge of the true relevance labels within that specific group.\n3. No Dedicated Ranking Visualizations: While MLflow provides visualizations for classification (e.g., Confusion Matrix, ROC curve) and regression (e.g., Residuals plot), there are no analogous built-in visualizations tailored for ranking performance (e.g., plots showing NDCG distribution across queries, or precision-recall curves for ranking).\n4. Manual Metric Logging for Ranking: To track ranking metrics, users are forced to write custom Python code to:\n- Perform the ranking evaluation using a separate library (e.g., trec_eval, rank_eval, pytorch_lightning.metrics.retrieval).\n- Manually calculate the desired metrics (NDCG, MAP, MRR, etc.).\nThen explicitly log each metric using mlflow.log_metric(). This is repetitive, error-prone, and doesn't leverage the automated, standardized mlflow.evaluate() interface.\n\n### Details\n\nWhile mlflow.evaluate() does support some ranking metrics with the retriever `model_type` as far as I can tell that is specific for LLM evaluation and does not satisfy the requirements for the structure of the ranking problems I'm referring to.\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "cc @BenWilson2 / @B-Step62, not sure what our plans are for expanding the classic `mlflow.evaluate()` functionality, maybe we can provide guidance as to how to implement new metrics / model type"
      }
    ]
  },
  {
    "issue_number": 16215,
    "title": "[BUG] regression issue downloading artifacts: 3.1 client, 2.x server",
    "author": "mazer-ai",
    "state": "open",
    "created_at": "2025-06-11T23:28:29Z",
    "updated_at": "2025-06-13T01:51:37Z",
    "labels": [
      "bug",
      "area/artifacts",
      "area/tracking"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 3.1.0\n- Tracking server: 2.22.0\n- \n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 24.l04 LTS (both server and client)\n- **Python version**: 3.12.3 (both)\n- **yarn version, if running the dev UI**: N/A\n\n\n### Describe the problem\n\nUsing a 3.1 client to download artifacts from a server running 2.22.0 fails.\n\nWhen you use `mlflow.artifacts.download_artifacts()` to pull down an existing artifact, it appears to try to download a model now, perhaps from the registry, as part of the download process, resulting in a fatal error.\n\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nSystem information: Linux #61-Ubuntu SMP PREEMPT_DYNAMIC Fri Apr 11 23:16:11 UTC 2025\nPython version: 3.12.3\nMLflow version: 3.1.0\nMLflow module location: /home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/__init__.py\nTracking URI: file:///home/mazer/src/taut/notebooks/jamie/mlflow-tsts/mlruns\nRegistry URI: file:///home/mazer/src/taut/notebooks/jamie/mlflow-tsts/mlruns\nMLflow dependencies: \n  Flask: 3.1.1\n  alembic: 1.16.1\n  docker: 7.1.0\n  fastapi: 0.115.12\n  graphene: 3.4.3\n  gunicorn: 23.0.0\n  matplotlib: 3.10.3\n  mlflow-skinny: 3.1.0\n  numpy: 2.3.0\n  pandas: 2.2.3\n  pyarrow: 19.0.1\n  scikit-learn: 1.7.0\n  scipy: 1.15.3\n  sqlalchemy: 2.0.41\n  uvicorn: 0.34.3\n  virtualenv: 20.31.2\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nimport mlflow\n\n# change following to suit your test environment\nserver = 'http://tinybee:5000'\nexper_name = 'jamie-test'\nrun_name = 'testrun'\n\nclient = mlflow.MlflowClient(server)\nmlflow.set_tracking_uri(server)\nxids = [x.experiment_id for x in client.search_experiments() if x.name == exper_name]\nif len(xids) > 0:\n    xid = xids[0]\nelse:\n    xid = client.create_experiment(exper_name)\n\nrs = client.search_runs(experiment_ids=[xid], filter_string=f'run_name = \"{run_name}\"')\nif len(rs) > 0:\n    rid = rs[0].info.run_id\nelse:\n    r = client.create_run(xid, run_name=run_name)\n    rid = r.info.run_id\n\n    # create an artifact if it doesn't already exist\nartifacts = mlflow.artifacts.list_artifacts(run_id=rid)\nif len(artifacts) == 0:\n    open('foo.txt', 'w').write('hello world!\\n')\n    client.log_artifact(rid, 'foo.txt', '')\n\n# show that it exists now\nprint([a.path for a in mlflow.artifacts.list_artifacts(run_id=rid)])\n\n# try to download\n# this works in 2.22.1, but fails in 3.1.0\nmlflow.artifacts.download_artifacts(run_id=rid, artifact_path='foo.txt', dst_path='/tmp')\n\n# verify it downloaded correctly\ncheck = open('/tmp/foo.txt', 'r').read()\nassert check == 'hello world!\\n'\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nTraceback (most recent call last):\n  File \"/home/mazer/src/taut/notebooks/jamie/mlflow-tsts/artifact_fail.py\", line 33, in <module>\n    mlflow.artifacts.download_artifacts(run_id=rid, artifact_path='foo.txt', dst_path='/tmp')\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/artifacts/__init__.py\", line 86, in download_artifacts\n    return _download_artifact_from_uri(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/tracking/artifact_utils.py\", line 117, in _download_artifact_from_uri\n    return repo.download_artifacts(artifact_path=artifact_path, dst_path=output_path)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/store/artifact/runs_artifact_repo.py\", line 209, in download_artifacts\n    model_out_path = self._download_model_artifacts(artifact_path, dst_path=dst_path)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/store/artifact/runs_artifact_repo.py\", line 223, in _download_model_artifacts\n    if repo := self._get_logged_model_artifact_repo(run_id=run_id, name=model_name):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/store/artifact/runs_artifact_repo.py\", line 153, in _get_logged_model_artifact_repo\n    if matched := next((m for m in iter_models() if m.source_run_id == run_id), None):\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/store/artifact/runs_artifact_repo.py\", line 153, in <genexpr>\n    if matched := next((m for m in iter_models() if m.source_run_id == run_id), None):\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/store/artifact/runs_artifact_repo.py\", line 142, in iter_models\n    page = client.search_logged_models(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/tracking/client.py\", line 5569, in search_logged_models\n    return self._tracking_client.search_logged_models(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/client.py\", line 881, in search_logged_models\n    return self.store.search_logged_models(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/store/tracking/rest_store.py\", line 1057, in search_logged_models\n    response_proto = self._call_endpoint(SearchLoggedModels, req_body)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/store/tracking/rest_store.py\", line 135, in _call_endpoint\n    return call_endpoint(\n           ^^^^^^^^^^^^^^\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/utils/rest_utils.py\", line 590, in call_endpoint\n    response = verify_rest_response(response, endpoint)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mazer/src/taut/venv12/lib/python3.12/site-packages/mlflow/utils/rest_utils.py\", line 310, in verify_rest_response\n    raise MlflowException(\nmlflow.exceptions.MlflowException: API request to endpoint /api/2.0/mlflow/logged-models/search failed with error code 404 != 200. Response body: '<!doctype html>\n<html lang=en>\n<title>404 Not Found</title>\n<h1>Not Found</h1>\n<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\n'\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "szho42",
        "body": "I had a similar issue after upgrading both client and tracking server from 2.22 to 3.1. Once the tracking server is restarted it all works in my case."
      },
      {
        "user": "daniellok-db",
        "body": "@mazer-ai let us know again if upgrading your tracking server doesn't work! cc @harupy for visibilityâ€”is the MLflow 3 client meant to be able to interact with MLflow 2 servers? If it's an easy fix it would be good to support that compatibility but given the amount of changes I think it's understandable if it wasn't in scope"
      },
      {
        "user": "harupy",
        "body": "> is the MLflow 3 client meant to be able to interact with MLflow 2 servers?\n\nNo, it's meant to be used with 3.x server.\n\nhttps://mlflow.org/docs/latest/genai/mlflow-3/#compatibility-with-mlflow-2x"
      }
    ]
  },
  {
    "issue_number": 13485,
    "title": "[FR] enable customizing the name of 'Step' in log_metric. Display x axis with negative value in mlflow ui",
    "author": "guanboch",
    "state": "open",
    "created_at": "2024-10-19T22:55:23Z",
    "updated_at": "2025-06-12T15:00:48Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Willingness to contribute\r\n\r\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n\r\n### Proposal Summary\r\n\r\nIn mlflow UI, for the log_metric function, I would like to have the X axis to be a different parameter instead of Step. For example, the X axis can be the current 'SNR' metric of the iteration,  while the Y axis can be the validation loss. The value of the 'SNR' on X axis can be negative and it can be correctly displayed on mlflow UI\r\n\r\n### Motivation\r\n\r\n> #### What is the use case for this feature?\r\nEnable log_metric to have a customizable x axis instead of Step. The value of the x axis can be negative, and it will display correctly in the MLflow ui\r\n> #### Why is this use case valuable to support for MLflow users in general?\r\nDuring experiment, the logged metric might not only associate with the step but also associate with other important metric at that step. I would like to plot  (SNR, val_loss) in the mlflow UI. The 'SNR' metric was in the place of the Step. \r\n> #### Why is this use case valuable to support for your project(s) or organization?\r\nIt will add lots of flexibility to log the metrics and display them against an x-axis which is more relevant than Steps. \r\n> #### Why is it currently difficult to achieve this use case?\r\nWe just need to add this feature. Thank you!\r\n\r\n\r\n### Details\r\n\r\n_No response_\r\n\r\n### What component(s) does this bug affect?\r\n\r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n### What interface(s) does this bug affect?\r\n\r\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\n### What language(s) does this bug affect?\r\n\r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\n### What integration(s) does this bug affect?\r\n\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "The proposal makes sense to me. I will raise the idea in our team meeting and see opinion from other folks  :)"
      },
      {
        "user": "WeichenXu123",
        "body": "Our TL @BenWilson2 also agrees with your idea. I assign this to you  :) Appreciate your contribution!"
      },
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      }
    ]
  },
  {
    "issue_number": 16225,
    "title": "Improve test_databricks_sdk_retry_backoff_calculation to use pytest.raises",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-12T09:18:14Z",
    "updated_at": "2025-06-12T10:31:58Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "## Description\n\nThe test `test_databricks_sdk_retry_backoff_calculation` in `tests/utils/test_rest_utils.py` uses a try/except pattern to handle expected exceptions, but it should use `pytest.raises` for better test clarity and to ensure the correct exception is raised.\n\n## Current Code\n\nThe test currently uses:\n```python\ntry:\n    _retry_databricks_sdk_call_with_exponential_backoff(\n        call_func=mock_failing_call,\n        retry_codes=_TRANSIENT_FAILURE_RESPONSE_CODES,\n        retry_timeout_seconds=10,\n        backoff_factor=1,\n        backoff_jitter=0,\n        max_retries=3,\n    )\nexcept Exception:\n    pass  # Expected to fail\n```\n\n## Proposed Improvement\n\nReplace the try/except pattern with `pytest.raises` to:\n1. Ensure the correct exception type is raised\n2. Improve test readability and maintainability\n3. Follow pytest best practices\n\nThe test should use:\n```python\nwith pytest.raises(DatabricksError):\n    _retry_databricks_sdk_call_with_exponential_backoff(\n        call_func=mock_failing_call,\n        retry_codes=_TRANSIENT_FAILURE_RESPONSE_CODES,\n        retry_timeout_seconds=10,\n        backoff_factor=1,\n        backoff_jitter=0,\n        max_retries=3,\n    )\n```\n\n## Benefits\n\n- More explicit about what exception is expected\n- Better error reporting if the wrong exception is raised\n- Follows pytest conventions\n- Improves code maintainability\n\n## Location\n\nFile: `tests/utils/test_rest_utils.py`\nFunction: `test_databricks_sdk_retry_backoff_calculation`\nLines: ~800-830",
    "comments": []
  },
  {
    "issue_number": 16133,
    "title": "[BUG] \"file:\" URI treated as remote host when using \"--default-artifact-root\"",
    "author": "AndhikaWB",
    "state": "closed",
    "created_at": "2025-06-07T12:09:58Z",
    "updated_at": "2025-06-12T09:54:06Z",
    "labels": [
      "bug",
      "area/artifacts"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.22.0\n- Tracking server: 2.22.0\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 11 26100\n- **Python version**: 3.12.9\n- **yarn version, if running the dev UI**: -\n\n\n### Describe the problem\n\nI'm using Makefile on Git Bash. When running MLFlow using:\n```\nmlflow:\n\tconda activate pytorch &&\n\tmlflow ui -h 127.0.0.1 -p 5000 \\\n\t\t--backend-store-uri \"file://$(CURDIR)/mlflow/mlruns\" \\\n\t\t--default-artifact-root \"file://${CURDIR}/mlflow/mlartifacts\"\n```\n\nI got this error as if it's a remote URL:\n\n```\nFile \"C:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\utils\\uri.py\", line 57, in is_local_uri\n    raise MlflowException(\nmlflow.exceptions.MlflowException: file://C:/Users/Dhika/Documents/Projects/Proto/Python/mlops_dtc/ml/mlflow/mlartifacts is not a valid remote uri. For remote access on windows, please consider using a different scheme such as SMB (e.g. smb://<hostname>/<path>).\n```\n\nThis error comes from MLFlow CLI itself, not from my Python script/notebook. In other words, the MLFlow UI failed to run. However, if I use this directly:\n```\nmlflow:\n\tconda activate pytorch &&\n\tmlflow ui -h 127.0.0.1 -p 5000 \\\n\t\t--backend-store-uri \"file://$(CURDIR)/mlflow/mlruns\" \\\n\t\t--default-artifact-root \"${CURDIR}/mlflow/mlartifacts\"\n```\n\nThe UI will run successfully, but when I call `mlflow.pytorch.log_model()` on the notebook it will return this error:\n\n```\nFile c:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\pytorch\\__init__.py:296, in log_model(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, requirements_file, extra_files, pip_requirements, extra_pip_requirements, metadata, **kwargs)\nMlflowException: Could not find a registered artifact repository for: c:/Users/Dhika/Documents/Projects/Proto/Python/mlops_dtc/ml/mlflow/mlartifacts/570908566307805170/93070c70081b4839a30768264332f33c/artifacts. Currently registered schemes are: ['', 'file', 's3', 'r2', 'gs', 'wasbs', 'ftp', 'sftp', 'dbfs', 'hdfs', 'viewfs', 'runs', 'models', 'http', 'https', 'mlflow-artifacts', 'abfss']\n```\n\nThis make it impossible to change the artifact path unless I use S3 or other remote solution. And it seems that the bug affects `--artifacts-destination` as well, because I originally want to use it like this to workaround issue #3144 :\n\n```\nmlflow:\n\tconda activate pytorch &&\n\tmlflow ui -h 127.0.0.1 -p 5000 \\\n\t\t--backend-store-uri \"file://$(CURDIR)/mlflow/mlruns\" \\\n\t\t--default-artifact-root \"mlflow-artifacts:/\" \\\n\t\t--artifacts-destination \"file://${CURDIR}/mlflow/mlartifacts\" \\\n\t\t--serve-artifacts\n```\n\nBased on the info from `mlflow ui --help` and this [StackOverflow question](https://stackoverflow.com/questions/75057477).\n\n### Tracking information\n\n```\n2025/06/07 19:04:48 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\nMLflow version: 2.22.0\nTracking URI: http://localhost:5000/\nArtifact URI: [c:/Users/Dhika/Documents/Projects/Proto/Python/mlops_dtc/ml/mlflow/mlartifacts/570908566307805170/36e64883979d4195a823eb0232f7a7e8/artifacts](file:///C:/Users/Dhika/Documents/Projects/Proto/Python/mlops_dtc/ml/mlflow/mlartifacts/570908566307805170/36e64883979d4195a823eb0232f7a7e8/artifacts)\nSystem information: Windows 10.0.26100\nPython version: 3.12.9\nMLflow version: 2.22.0\nMLflow module location: [c:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\__init__.py](file:///C:/Users/Dhika/Apps/CommonFiles/Python/envs/pytorch/Lib/site-packages/mlflow/__init__.py)\nTracking URI: http://localhost:5000/\nRegistry URI: http://localhost:5000/\nActive experiment ID: 570908566307805170\nActive run ID: 36e64883979d4195a823eb0232f7a7e8\nActive run artifact URI: [c:/Users/Dhika/Documents/Projects/Proto/Python/mlops_dtc/ml/mlflow/mlartifacts/570908566307805170/36e64883979d4195a823eb0232f7a7e8/artifacts](file:///C:/Users/Dhika/Documents/Projects/Proto/Python/mlops_dtc/ml/mlflow/mlartifacts/570908566307805170/36e64883979d4195a823eb0232f7a7e8/artifacts)\nMLflow environment variables: \n  MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING: True\n  MLFLOW_EXPERIMENT_ID: 570908566307805170\n  MLFLOW_TRACKING_URI: http://localhost:5000/\nMLflow dependencies: \n  Flask: 3.1.1\n  Jinja2: 3.1.4\n  aiohttp: 3.12.6\n  alembic: 1.16.1\n  docker: 7.1.0\n  fastapi: 0.115.12\n  graphene: 3.4.3\n  markdown: 3.8\n  matplotlib: 3.10.3\n  mlflow-skinny: 2.22.0\n  numpy: 2.1.2\n  pandas: 2.2.3\n  pyarrow: 19.0.1\n  scikit-learn: 1.6.1\n  scipy: 1.15.3\n  sqlalchemy: 2.0.41\n  uvicorn: 0.34.3\n  waitress: 3.0.2\n```\n\n\n### Code to reproduce issue\n\nI use PyTorch model so the code is too long to paste here, but the default Iris example will reproduce this error as well (I already tested it):\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nimport mlflow\n\nX, y = load_iris(return_X_y=True)\nmodel = LogisticRegression().fit(X, y)\nwith mlflow.start_run():\n    mlflow.sklearn.log_model(model, \"model\")\n```\n\n### Stack trace\n\nWhen using `--default-artifact-root \"file://${CURDIR}/mlflow/mlartifacts\"` (the error comes from MLFlow UI CLI):\n\n```\n2025/06/07 18:53:04 ERROR mlflow.cli: Error initializing backend store\n2025/06/07 18:53:04 ERROR mlflow.cli: file://C:/Users/Dhika/Documents/Projects/Proto/Python/mlops_dtc/ml/mlflow/mlartifacts is not a valid remote uri. For remote access on windows, please consider using a different scheme such as SMB (e.g. smb://<hostname>/<path>).\nTraceback (most recent call last):\n  File \"C:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\cli.py\", line 425, in server\n    initialize_backend_stores(backend_store_uri, registry_store_uri, default_artifact_root)\n  File \"C:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\server\\handlers.py\", line 370, in initialize_backend_stores\n    _get_tracking_store(backend_store_uri, default_artifact_root)\n  File \"C:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\server\\handlers.py\", line 347, in _get_tracking_store\n    _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\registry.py\", line 45, in get_store\n    return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\registry.py\", line 56, in _get_store_with_resolved_uri\n    return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\server\\handlers.py\", line 162, in _get_file_store\n    return FileStore(store_uri, artifact_uri)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 189, in __init__\n    self.artifact_root_uri = resolve_uri_if_local(artifact_root_uri)\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\utils\\uri.py\", line 425, in resolve_uri_if_local\n    if local_uri is not None and is_local_uri(local_uri):\n                                 ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\utils\\uri.py\", line 57, in is_local_uri\n    raise MlflowException(\nmlflow.exceptions.MlflowException: file://C:/Users/Dhika/Documents/Projects/Proto/Python/mlops_dtc/ml/mlflow/mlartifacts is not a valid remote uri. For remote access on windows, please consider using a different scheme such as SMB (e.g. smb://<hostname>/<path>).\nmake: *** [Makefile:11: mlflow] Error 1\n```\n\nWhen using `--default-artifact-root \"${CURDIR}/mlflow/mlartifacts\"` (the error comes from `log_model`):\n\n```\n---------------------------------------------------------------------------\nMlflowException                           Traceback (most recent call last)\nCell In[13], line 77\n     74 if cb['early_stop'].best_epoch == epoch:\n     75     print('Saving best model so far...')\n---> 77     mlflow.pytorch.log_model(\n     78         model,\n     79         artifact_path = cfg['model_dir'],\n     80         conda_env = 'conda.yaml',\n     81         signature = signature\n     82     )\n\nFile c:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\pytorch\\__init__.py:296, in log_model(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, requirements_file, extra_files, pip_requirements, extra_pip_requirements, metadata, **kwargs)\n    156 \"\"\"\n    157 Log a PyTorch model as an MLflow artifact for the current run.\n    158 \n   (...)    293     PyTorch logged models\n    294 \"\"\"\n    295 pickle_module = pickle_module or mlflow_pytorch_pickle_module\n--> 296 return Model.log(\n    297     artifact_path=artifact_path,\n    298     flavor=mlflow.pytorch,\n    299     pytorch_model=pytorch_model,\n    300     conda_env=conda_env,\n    301     code_paths=code_paths,\n    302     pickle_module=pickle_module,\n    303     registered_model_name=registered_model_name,\n    304     signature=signature,\n    305     input_example=input_example,\n    306     await_registration_for=await_registration_for,\n    307     requirements_file=requirements_file,\n    308     extra_files=extra_files,\n    309     pip_requirements=pip_requirements,\n    310     extra_pip_requirements=extra_pip_requirements,\n    311     metadata=metadata,\n    312     **kwargs,\n    313 )\n\nFile c:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\models\\model.py:921, in Model.log(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, **kwargs)\n    918     for prompt in prompts:\n    919         client.log_prompt(run_id, prompt)\n--> 921 mlflow.tracking.fluent.log_artifacts(local_path, mlflow_model.artifact_path, run_id)\n    923 # if the model_config kwarg is passed in, then log the model config as an params\n    924 if model_config := kwargs.get(\"model_config\"):\n\nFile c:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\tracking\\fluent.py:1219, in log_artifacts(local_dir, artifact_path, run_id)\n   1185 \"\"\"\n   1186 Log all the contents of a local directory as artifacts of the run. If no run is active,\n   1187 this method will create a new active run.\n   (...)   1216             mlflow.log_artifacts(tmp_dir, artifact_path=\"states\")\n   1217 \"\"\"\n   1218 run_id = run_id or _get_or_start_run().info.run_id\n-> 1219 MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n\nFile c:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\tracking\\client.py:2428, in MlflowClient.log_artifacts(self, run_id, local_dir, artifact_path)\n   2381 def log_artifacts(\n   2382     self, run_id: str, local_dir: str, artifact_path: Optional[str] = None\n   2383 ) -> None:\n   2384     \"\"\"Write a directory of files to the remote ``artifact_uri``.\n   2385 \n   2386     Args:\n   (...)   2426 \n   2427     \"\"\"\n-> 2428     self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n\nFile c:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:964, in TrackingServiceClient.log_artifacts(self, run_id, local_dir, artifact_path)\n    955 def log_artifacts(self, run_id, local_dir, artifact_path=None):\n    956     \"\"\"Write a directory of files to the remote ``artifact_uri``.\n    957 \n    958     Args:\n   (...)    962 \n    963     \"\"\"\n--> 964     self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n\nFile c:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:906, in TrackingServiceClient._get_artifact_repo(self, run_id)\n    902 run = self.get_run(run_id)\n    903 artifact_uri = add_databricks_profile_info_to_artifact_uri(\n    904     run.info.artifact_uri, self.tracking_uri\n    905 )\n--> 906 artifact_repo = get_artifact_repository(artifact_uri)\n    907 # Cache the artifact repo to avoid a future network call, removing the oldest\n    908 # entry in the cache if there are too many elements\n    909 if len(utils._artifact_repos_cache) > 1024:\n\nFile c:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\store\\artifact\\artifact_repository_registry.py:133, in get_artifact_repository(artifact_uri)\n    120 def get_artifact_repository(artifact_uri: str) -> ArtifactRepository:\n    121     \"\"\"\n    122     Get an artifact repository from the registry based on the scheme of artifact_uri\n    123 \n   (...)    131         requirements.\n    132     \"\"\"\n--> 133     return _artifact_repository_registry.get_artifact_repository(artifact_uri)\n\nFile c:\\Users\\Dhika\\Apps\\CommonFiles\\Python\\envs\\pytorch\\Lib\\site-packages\\mlflow\\store\\artifact\\artifact_repository_registry.py:73, in ArtifactRepositoryRegistry.get_artifact_repository(self, artifact_uri)\n     71 repository = self._registry.get(scheme)\n     72 if repository is None:\n---> 73     raise MlflowException(\n     74         f\"Could not find a registered artifact repository for: {artifact_uri}. \"\n     75         f\"Currently registered schemes are: {list(self._registry.keys())}\"\n     76     )\n     77 return repository(artifact_uri)\n\nMlflowException: Could not find a registered artifact repository for: c:/Users/Dhika/Documents/Projects/Proto/Python/mlops_dtc/ml/mlflow/mlartifacts/570908566307805170/93070c70081b4839a30768264332f33c/artifacts. Currently registered schemes are: ['', 'file', 's3', 'r2', 'gs', 'wasbs', 'ftp', 'sftp', 'dbfs', 'hdfs', 'viewfs', 'runs', 'models', 'http', 'https', 'mlflow-artifacts', 'abfss']\n```\n\n### Other info / logs\n\nFrom MLFlow UI CLI there's only this output when the error occur on the notebook:\n\n```\nINFO:waitress:Serving on http://127.0.0.1:5000\n\n```\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "@AndhikaWB seems like a similar problem to #5489, could you try adding 3 slashes in front of `file`, e.g. `file:///${CURDIR}`? it seems that's the correct way to specify the URI on windows"
      },
      {
        "user": "AndhikaWB",
        "body": "Thanks, it's fixed! I thought triple slash is only needed because Linux directory starts with slash, turns out Windows uses triple slash too."
      }
    ]
  },
  {
    "issue_number": 16220,
    "title": "Fix typo: \"asynchnorous\" should be \"asynchronous\" in anthropic/__init__.py",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-12T06:23:34Z",
    "updated_at": "2025-06-12T09:34:31Z",
    "labels": [],
    "body": "## Description\n\nThere is a typo in the file `mlflow/anthropic/__init__.py` on line 17. The word \"asynchnorous\" should be \"asynchronous\".\n\n## Location\n\nFile: `mlflow/anthropic/__init__.py`\nLine: 17\nCurrent text: `Only synchronous calls and asynchnorous APIs are supported.`\nShould be: `Only synchronous calls and asynchronous APIs are supported.`\n\n## Steps to Fix\n\n1. Open `mlflow/anthropic/__init__.py`\n2. Locate line 17 in the docstring\n3. Change \"asynchnorous\" to \"asynchronous\"\n\nThis is a simple spelling correction that will improve the documentation quality.",
    "comments": []
  },
  {
    "issue_number": 16096,
    "title": "âš ï¸ [Security] Address CVE-2025-0453 - DoS Vulnerability via /graphql Endpoint in MLflow >= 2.21.1",
    "author": "salmankadaya",
    "state": "open",
    "created_at": "2025-06-05T13:24:47Z",
    "updated_at": "2025-06-12T09:21:53Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "A high-severity security vulnerability (CVE-2025-0453) has been identified in MLflow versions 2.21.1 and above. The /graphql endpoint is susceptible to a Denial of Service (DoS) attack. An attacker can exploit this by submitting large batches of GraphQL queries that request all runs from an experiment, leading to uncontrolled resource consumption.",
    "comments": [
      {
        "user": "salmankadaya",
        "body": "Hey @jonas @d18s @tmielika @vguerra , Any update on this issue?"
      }
    ]
  },
  {
    "issue_number": 16203,
    "title": "[BUG] Security Vulnerability",
    "author": "salmankadaya",
    "state": "open",
    "created_at": "2025-06-11T09:17:41Z",
    "updated_at": "2025-06-12T08:45:19Z",
    "labels": [],
    "body": "A high-severity security vulnerability (https://github.com/advisories/GHSA-49m6-vrr9-2cqm) has been identified in MLflow versions 2.21.1 and above. The /graphql endpoint is susceptible to a Denial of Service (DoS) attack. An attacker can exploit this by submitting large batches of GraphQL queries that request all runs from an experiment, leading to uncontrolled resource consumption.",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "Thanks for reporting, this seems to generally relate to rate limiting, which is also tracked in other issues (e.g. #16028)"
      }
    ]
  },
  {
    "issue_number": 16222,
    "title": "[DOC-FIX] Add default helm value for databaseMigration: false as breaking change",
    "author": "kappa8219",
    "state": "closed",
    "created_at": "2025-06-12T06:27:53Z",
    "updated_at": "2025-06-12T06:30:50Z",
    "labels": [
      "area/docs"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute a documentation fix independently.\n\n### URL(s) with the issue\n\nhttps://github.com/mlflow/mlflow/releases/tag/v3.1.0\n\n### Description of proposal (what needs changing)\n\nAdd point that you need to set \"databaseMigration: true\" to have successfull upgrade.\n\n[here](https://artifacthub.io/packages/helm/community-charts/mlflow?modal=values)",
    "comments": [
      {
        "user": "kappa8219",
        "body": "Sorry, I missed that helm chart is not official. Will contact them instead."
      }
    ]
  },
  {
    "issue_number": 15937,
    "title": "[BUG] Security Vulnerability",
    "author": "x03gh0st",
    "state": "open",
    "created_at": "2025-05-28T11:56:08Z",
    "updated_at": "2025-06-11T15:28:44Z",
    "labels": [],
    "body": "Hello, I found 3 vulnerabilities in Mlflow",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "I will handle them shortly."
      },
      {
        "user": "x03gh0st",
        "body": "Thanks, I will send details via email in 1-2 days - after I finish describing the findings. \n\n/email sent  "
      },
      {
        "user": "x03gh0st",
        "body": "Following discussions with the internal team, there is a suspicion that some vulnerabilities may be the result of customizing the application to fit the organizationâ€™s needs. Please review each scenario. Thanks ! "
      }
    ]
  },
  {
    "issue_number": 11724,
    "title": "[BUG] SSL issue when uploading artifact via Python interface",
    "author": "kasuteru",
    "state": "open",
    "created_at": "2024-04-16T13:24:23Z",
    "updated_at": "2025-06-11T14:22:07Z",
    "labels": [
      "bug",
      "area/windows",
      "area/artifacts",
      "area/docker",
      "integrations/azure",
      "integrations/databricks"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### Willingness to contribute\n\nYes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\n\n### MLflow version\n\n- Client: 2.11.3\r\n- Tracking server: Azure Databricks\r\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows\r\n- **Python version**: 3.12.1\r\n\r\n\n\n### Describe the problem\n\nShort summary:\r\nIt seems that currently, artifact upload to Azure Databricks does not offer an option to ignore self-signed certificate errors.\r\n\r\nIn-Depth:\r\nWe are running mlflow \"locally\" inside a company network and are trying to log runs into Azure Databricks. This is working fine - no SSL certificate erros / firewall problems. However, we cannot log artifacts. These will time out due to â `self-signed certificate in certificate chain `. \r\n\r\n**What we tried**:\r\n - insecure = true in databrickscfg file\r\n - MLFLOW_TRACKING_INSECURE_TLS=True\r\n - MLFLOW_S3_IGNORE_TLS=True\r\n\r\nAll of these did not work. Looking at the function that fails, it also seems like there is not really a way to pass a parameter telling the function to ignore sslcert problems. \r\n\r\nThis function works fine if called from outside the company network (because then there is no self-signed certificate in the chain), but unfortunately, neither this nor providing one is an option at this point.\r\n\r\nAny feedback on what else to try is appreciated - currently though, this looks like a bug for me because the insecure flag does not seem to work for Azure Databricks artifact uploads.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```shell\r\nREPLACE_ME\r\n```\r\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nimport mlflow\r\nimport os\r\n\r\n# Set the MLFlow tracking URI to the address of your Azure Databricks workspace\r\ntracking_uri = os.environ.get(\"MLFLOW_TRACKING_URI\")\r\nmlflow.set_tracking_uri(tracking_uri)\r\n\r\n# Set the MLFlow experiment name\r\nexperiment_name = \"/Users/~censored~\"\r\nmlflow.set_experiment(experiment_name)\r\n\r\n\r\n# Start a new MLFlow run\r\nwith mlflow.start_run(run_name=\"Test-12\"):\r\n    # Your code here\r\n    # Log metrics, parameters, artifacts, etc.\r\n    mlflow.log_metric(\"accuracy\", 0.731434)\r\n    mlflow.log_param(\"anonymize\", \"True\")\r\n    mlflow.log_artifact(\"results.md\")  # <- This fails due to cert error\r\n```\r\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nMlflowException                           Traceback (most recent call last)\r\nFile c:\\NAME_LOCAL\\code\\FOGenAITicketResolutionService\\scripts\\log_mlflow.py:27\r\n     25 mlflow.log_param(\"train_file_name\", \"train_rdy.csv\")\r\n     26 mlflow.log_param(\"anonymize\", \"False\")\r\n---> 27 mlflow.log_artifact(r\"C:\\NAME_LOCAL\\code\\FOGenAITicketResolutionService\\results.md\")\r\n\r\nFile c:\\Users\\LOCALUSER\\Miniconda3\\envs\\FOGenAI_ENV\\Lib\\site-packages\\mlflow\\tracking\\fluent.py:1057, in log_artifact(local_path, artifact_path, run_id)\r\n   1029 \"\"\"\r\n   1030 Log a local file or directory as an artifact of the currently active run. If no run is\r\n   1031 active, this method will create a new active run.\r\n   (...)\r\n   1054             mlflow.log_artifact(path)\r\n   1055 \"\"\"\r\n   1056 run_id = run_id or _get_or_start_run().info.run_id\r\n-> 1057 MlflowClient().log_artifact(run_id, local_path, artifact_path)\r\n\r\nFile c:\\Users\\LOCALUSER\\Miniconda3\\envs\\FOGenAI_ENV\\Lib\\site-packages\\mlflow\\tracking\\client.py:1189, in MlflowClient.log_artifact(self, run_id, local_path, artifact_path)\r\n   1150 def log_artifact(self, run_id, local_path, artifact_path=None) -> None:\r\n   1151     \"\"\"Write a local file or directory to the remote ``artifact_uri``.\r\n   1152 \r\n   1153     Args:\r\n   (...)\r\n   1187 \r\n   1188     \"\"\"\r\n-> 1189     self._tracking_client.log_artifact(run_id, local_path, artifact_path)\r\n\r\nFile c:\\Users\\LOCALUSER\\Miniconda3\\envs\\FOGenAI_ENV\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:560, in TrackingServiceClient.log_artifact(self, run_id, local_path, artifact_path)\r\n    558     artifact_repo.log_artifacts(local_path, path_name)\r\n    559 else:\r\n--> 560     artifact_repo.log_artifact(local_path, artifact_path)\r\n\r\nFile c:\\Users\\LOCALUSER\\Miniconda3\\envs\\FOGenAI_ENV\\Lib\\site-packages\\mlflow\\store\\artifact\\databricks_artifact_repo.py:574, in DatabricksArtifactRepository.log_artifact(self, local_file, artifact_path)\r\n    572 artifact_file_path = posixpath.join(artifact_path or \"\", src_file_name)\r\n    573 write_credential_info = self._get_write_credential_infos([artifact_file_path])[0]\r\n--> 574 self._upload_to_cloud(\r\n    575     cloud_credential_info=write_credential_info,\r\n    576     src_file_path=local_file,\r\n    577     artifact_file_path=artifact_file_path,\r\n    578 )\r\n\r\nFile c:\\Users\\LOCALUSER\\Miniconda3\\envs\\FOGenAI_ENV\\Lib\\site-packages\\mlflow\\store\\artifact\\databricks_artifact_repo.py:401, in DatabricksArtifactRepository._upload_to_cloud(self, cloud_credential_info, src_file_path, artifact_file_path)\r\n    389 \"\"\"\r\n    390 Upload a local file to the cloud. Note that in this artifact repository, files are uploaded\r\n    391 to run-relative artifact file paths in the artifact repository.\r\n   (...)\r\n    398 \r\n    399 \"\"\"\r\n    400 if cloud_credential_info.type == ArtifactCredentialType.AZURE_SAS_URI:\r\n--> 401     self._azure_upload_file(cloud_credential_info, src_file_path, artifact_file_path)\r\n    402 elif cloud_credential_info.type == ArtifactCredentialType.AZURE_ADLS_GEN2_SAS_URI:\r\n    403     self._azure_adls_gen2_upload_file(\r\n    404         cloud_credential_info, src_file_path, artifact_file_path\r\n    405     )\r\n\r\nFile c:\\Users\\LOCALUSER\\Miniconda3\\envs\\FOGenAI_ENV\\Lib\\site-packages\\mlflow\\store\\artifact\\databricks_artifact_repo.py:297, in DatabricksArtifactRepository._azure_upload_file(self, credentials, local_file, artifact_file_path)\r\n    295             raise e\r\n    296 except Exception as err:\r\n--> 297     raise MlflowException(err)\r\n\r\nMlflowException: Failed to upload at least one part of C:\\NAME_LOCAL\\code\\FOGenAITicketResolutionService\\results.md. Errors: {0: 'SSLError(MaxRetryError(\"HTTPSConnectionPool(host=\\'dbstoragevugh5qqn2oolw.blob.core.windows.net\\', port=443): Max retries exceeded with url: - URL CENSORED - (Caused by SSLError(SSLCertVerificationError(1, \\'[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1000)\\')))\"))'}\r\n```\r\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nREPLACE_ME\r\n```\r\n\n\n### What component(s) does this bug affect?\n\n- [X] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [X] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [X] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [X] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [X] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "@kasuteru Could you try the latest MLflow and see if it works? If it doesn't could you update the stacktrace?"
      },
      {
        "user": "kasuteru",
        "body": "@serena-ruan of course. I tried again with v2.12.1, I have updated stack trace and version info accordingly.\r\n\r\nFor me to better understand: What would be the correct method to prevent this error in theory? `insecure = true` in .databrickscfg file? Or the environment variable `MLFLOW_TRACKING_INSECURE_TLS=True`?"
      },
      {
        "user": "serena-ruan",
        "body": "Could you try adding `insecure=true` in the .databrickscfg file, if that doesn't work try adding `DATABRICKS_INSECURE=true` in environment variable and see if it works? "
      }
    ]
  },
  {
    "issue_number": 8539,
    "title": "[BUG] Timeout errors while uploading large models to mlflow server",
    "author": "rohanicad",
    "state": "open",
    "created_at": "2023-05-26T15:00:15Z",
    "updated_at": "2025-06-11T09:18:12Z",
    "labels": [
      "bug",
      "area/models",
      "area/projects",
      "area/tracking"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Willingness to contribute\n\nYes. I can contribute a fix for this bug independently.\n\n### MLflow version\n\n- Client: 2.2.1\r\n\r\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10, Version 22H2\r\n- **Python version**: 3.9.9\r\n- **yarn version, if running the dev UI**:\r\n\n\n### Describe the problem\n\nI am trying to upload  a PCA model (trained using sklearn) to a custom ec2 server. The size of the model is 1.5Gb. I am receiving timeout errors despite increasing  the timeout to 900. I referred to #2401 and tried to increase server timeout but it doesnt help. \r\n\r\nThe server configurations during the code run were\r\n```\r\n[Unit]\r\nDescription=Mlflow server configuration\r\n[Service]\r\nType=simple\r\nUser=ubuntu\r\nExecStart=/bin/bash -c 'PATH=/home/ubuntu/.local/bin/:$PATH exec mlflow server --host 0.0.0.0 --port 8883 --artifacts-destination s3://icad-mlflow-artifacts --serve-artifacts --backend-store-uri /home/ubuntu/mlruns --gunicorn-opts \"--log-level debug --timeout 900 --graceful-timeout 120\"'\r\nWorkingDirectory=/home/ubuntu\r\nRestart=always\r\n[Install]\r\nWantedBy=multi-user.target\r\n```\r\n\r\nAs highlighted the timeout was set to 900 as recommended in #2401 but the upload was not successful. Additionally, the internet speed should not be an issue and both the upload and download are greater than 10Mbps\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```shell\r\nSystem information: Windows 10.0.19045\r\nPython version: 3.9.9\r\nMLflow version: 2.2.1\r\nMLflow module location: C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlflow\\__init__.py\r\nTracking URI: <omitted>\r\nRegistry URI: <omitted>\r\nActive experiment ID: 586086446372044084\r\nActive run ID: 4a42167d9b324b3eab64db95e79bc882\r\nActive run artifact URI: mlflow-artifacts:/586086446372044084/4a42167d9b324b3eab64db95e79bc882/artifacts\r\nMLflow dependencies:\r\n  Flask: 2.2.3\r\n  Jinja2: 3.1.2\r\n  alembic: 1.10.2\r\n  click: 8.1.3\r\n  cloudpickle: 2.2.1\r\n  databricks-cli: 0.17.4\r\n  docker: 6.0.1\r\n  entrypoints: 0.4\r\n  gitpython: 3.1.31\r\n  importlib-metadata: 6.0.0\r\n  markdown: 3.4.1\r\n  matplotlib: 3.7.1\r\n  numpy: 1.23.5\r\n  packaging: 23.0\r\n  pandas: 1.5.3\r\n  protobuf: 4.22.1\r\n  pyarrow: 11.0.0\r\n  pytz: 2022.7.1\r\n  pyyaml: 6.0\r\n  querystring-parser: 1.2.4\r\n  requests: 2.28.2\r\n  scikit-learn: 1.2.1\r\n  scipy: 1.9.1\r\n  shap: 0.41.0\r\n  sqlalchemy: 2.0.5.post1\r\n  sqlparse: 0.4.3\r\n  waitress: 2.1.2\r\n```\r\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport json\r\nimport mlflow.pytorch\r\nimport numpy as np\r\nfrom sklearn.decomposition import PCA\r\nimport matplotlib.pyplot as plt\r\nimport pickle as pk\r\n\r\nfolder = 'enter the path of folder'\r\nimages = []\r\nimage_names = []\r\ncount =0 \r\nfor filename in os.listdir(folder):\r\n    img = cv2.imread(os.path.join(folder,filename))\r\n    img = np.resize(img,(90, 360))\r\n    # print(img.shape)\r\n    if img is not None:\r\n        # print(len(img.flatten()/255))\r\n        images.append(img.flatten()/255)\r\n        image_names.append(filename)\r\n\r\nmlflow.set_tracking_uri(\"mention your tracking uri\") \r\nmlflow.set_experiment(experiment_name=\"your experiment name\")\r\nexp = 'some random name'\r\n\r\nwith mlflow.start_run(run_name=exp) as run:\r\n    print(\"run started\")\r\n    pca_2000 = PCA(n_components=7000)\r\n    pca_2000_reduced = pca_2000.fit_transform(images)\r\n    pk.dump(pca_2000, open(\"pca_test_cat.pkl\",\"wb\"))\r\n    print(\"started upload\")\r\n    model_info = mlflow.sklearn.log_model(sk_model=pca_2000, artifact_path=\"pca_cat_dog\",registered_model_name= \"pca_model_test_notuseful\")\r\n```\r\n\r\nFor the purpose of testing I had used cat dog dataset available on [Kaggle](https://www.kaggle.com/competitions/dogs-vs-cats/data), but one can use any dataset as long as the size of PCA model >= 1.5Gb.  \r\n\r\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\n\r\n2023/05/25 19:22:47 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen\r\n    httplib_response = self._make_request(\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\", line 398, in _make_request\r\n    conn.request(method, url, **httplib_request_kw)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py\", line 239, in request\r\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 1285, in request\r\n    self._send_request(method, url, body, headers, encode_chunked)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 1331, in _send_request\r\n    self.endheaders(body, encode_chunked=encode_chunked)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 1280, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 1079, in _send_output\r\n    self.send(chunk)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 1001, in send\r\n    self.sock.sendall(data)\r\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\adapters.py\", line 489, in send\r\n    resp = conn.urlopen(\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\", line 815, in urlopen\r\n    return self.urlopen(\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\", line 815, in urlopen\r\n    return self.urlopen(\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\", line 815, in urlopen\r\n    return self.urlopen(\r\n  [Previous line repeated 2 more times]\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\r\n    retries = retries.increment(\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\util\\retry.py\", line 592, in increment\r\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='<server url>', port=8883): Max retries exceeded with url: /api/2.0/mlflow-artifacts/artifacts/586086446372044084/031b643c33c54003a364e1a26369ff7b/artifacts/pca_cat_dog/model.pkl (Caused by ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\rest_utils.py\", line 167, in http_request\r\n    return _get_http_response_with_retries(\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\rest_utils.py\", line 98, in _get_http_response_with_retries\r\n    return session.request(method, url, **kwargs)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py\", line 587, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py\", line 701, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\adapters.py\", line 565, in send\r\n    raise ConnectionError(e, request=request)\r\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='<server url>', port=8883): Max retries exceeded with url: /api/2.0/mlflow-artifacts/artifacts/586086446372044084/031b643c33c54003a364e1a26369ff7b/artifacts/pca_cat_dog/model.pkl (Caused by ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\someting\\OneDrive - iCAD Dental\\Desktop\\git_igps\\clustering\\training_code_pca\\pca_mnist.py\", line 39, in <module>\r\n    model_info = mlflow.sklearn.log_model(sk_model=pca_2000, artifact_path=\"pca_cat_dog\",registered_model_name= \"pca_model_test_notuseful\")\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 424, in log_model\r\n    return Model.log(\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlflow\\models\\model.py\", line 552, in log\r\n    mlflow.tracking.fluent.log_artifacts(local_path, mlflow_model.artifact_path)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\fluent.py\", line 817, in log_artifacts\r\n    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\client.py\", line 1069, in log_artifacts\r\n    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py\", line 448, in log_artifacts\r\n    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlflow\\store\\artifact\\http_artifact_repo.py\", line 40, in log_artifacts\r\n    self.log_artifact(os.path.join(root, f), artifact_dir)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlflow\\store\\artifact\\http_artifact_repo.py\", line 25, in log_artifact\r\n    resp = http_request(self._host_creds, endpoint, \"PUT\", data=f)\r\n  File \"C:\\Users\\something\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlflow\\utils\\rest_utils.py\", line 185, in http_request\r\n    raise MlflowException(f\"API request to {url} failed with exception {e}\")\r\nmlflow.exceptions.MlflowException: API request to <server url>:8883/api/2.0/mlflow-artifacts/artifacts/586086446372044084/031b643c33c54003a364e1a26369ff7b/artifacts/pca_cat_dog/model.pkl failed with exception HTTPConnectionPool(host='<server url>', port=8883): Max retries exceeded with url: /api/2.0/mlflow-artifacts/artifacts/586086446372044084/031b643c33c54003a364e1a26369ff7b/artifacts/pca_cat_dog/model.pkl (Caused by ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)))\r\n```\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nREPLACE_ME\r\n```\r\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [X] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [X] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [X] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "@rohanicad Can you try setting `export MLFLOW_HTTP_REQUEST_TIMEOUT=900`?"
      },
      {
        "user": "rohanicad",
        "body": "@harupy yes I have tried that. The solution was mentioned in #2401, but it doesnt seem to work for my case "
      },
      {
        "user": "mlflow-automation",
        "body": "<!-- assign-maintainer -->\n@BenWilson2 @dbczumar @harupy @WeichenXu123 Please assign a maintainer and start triaging this issue."
      }
    ]
  },
  {
    "issue_number": 16195,
    "title": "Update validate-author action to allow mlflow-app bot user",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-11T05:23:02Z",
    "updated_at": "2025-06-11T05:33:01Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "## Description\n\nThe `validate-author` GitHub Action currently validates permissions for users who comment on PRs, but it doesn't allow the `mlflow-app[bot]` user to trigger workflows.\n\n## Problem\n\nThe current `isAllowed` function in `.github/actions/validate-author/index.js` only allows:\n- Users with `owner`, `member`, or `collaborator` repository permissions\n- The `copilot` bot user\n\nHowever, it doesn't include the `mlflow-app[bot]` user, which may need to trigger certain workflows.\n\n## Proposed Solution\n\nUpdate the `isAllowed` function to include `mlflow-app[bot]` in the list of allowed bot users:\n\n```javascript\nfunction isAllowed({ author_association, user }) {\n  return (\n    [\"owner\", \"member\", \"collaborator\"].includes(author_association.toLowerCase()) ||\n    // Allow Copilot and mlflow-app bot to run this workflow\n    (user && user.type.toLowerCase() === \"bot\" && \n     [\"copilot\", \"mlflow-app[bot]\"].includes(user.login.toLowerCase()))\n  );\n}\n```\n\n## Acceptance Criteria\n\n- [ ] The `isAllowed` function allows `mlflow-app[bot]` user to pass validation\n- [ ] Existing functionality for other users remains unchanged\n- [ ] Code is properly commented to reflect the change",
    "comments": []
  },
  {
    "issue_number": 16190,
    "title": "Remove `rc2` once `databricks-agents==1.0.0` is released",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-11T00:12:31Z",
    "updated_at": "2025-06-11T04:46:18Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "Address this TODO: https://github.com/mlflow/mlflow/blob/c1c0e26441e3f5c7f27b8fd58cc51568dbdb9d9e/dev/pyproject.py#L184",
    "comments": []
  },
  {
    "issue_number": 16138,
    "title": "AutoGen cross version test 0.6.1",
    "author": "TomeHirata",
    "state": "closed",
    "created_at": "2025-06-08T20:40:10Z",
    "updated_at": "2025-06-11T04:43:49Z",
    "labels": [],
    "body": "The cross-version test for autogen 0.6.1 is failing due to new fields added to the agent response objects. We should fix the failure while keeping the backward compatibility for old versions\nhttps://github.com/mlflow/dev/actions/runs/15518806598/job/43695586021",
    "comments": []
  },
  {
    "issue_number": 16066,
    "title": "[BUG]MLFlow cannot visualize large JSON file",
    "author": "gioargyr",
    "state": "open",
    "created_at": "2025-06-04T10:32:07Z",
    "updated_at": "2025-06-10T15:02:03Z",
    "labels": [
      "bug",
      "area/artifacts",
      "area/docker"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.19.0\n- Tracking server: 2.19.0\n- gunicorn 23.0.0\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MLFlow runs inside a docker container. The docker image that is used is a combination of the official MLFLow docker image as a base image + installation of packages `psycopg2-binary`, and  `boto3`. So, the IS is `Debian 11`.\n- **Python version**: 3.10.16\n\nI am the administrator of this MLFlow installation.\nThis MLFlow installation is supported by Object Storage (a MinIO bucket) and a PostgreSQL instance that holds a dedicated database for it.\n\n\n### Describe the problem\n\nGenerally things are fine and most artifacts are being visualized in the UI without issues.\nThe problem is with \"big\" JSON files that are stored as artifacts. \"big\" = 2 MB and more\n- JSON files that have a size of ~ 600 KB are being visualized fine\n- JSON files that have a size of >2 MB, I get the error \"Something went wrong\"\n\nI do not know what is the exact limit of MLFlow for visualizing such files/artifacts, but I hope you get a good idea from what I describe.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nmlflow server --backend-store-uri postgresql://PG_USER_REDACTED:PG_PWD_REDACTED@PG_HOST:5432/mlflow --host 0.0.0.0 --serve-artifacts --artifacts-destination s3://MLFLOW_BUCKET\n```\n`AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are defined as environment variables.\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\nIn order to reproduce the issue, you need to install MLFlow the same way that is described above.\nOn the other hand, one approach would be to install MLFlow any way you like and store a big JSON as artifact.\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\nThere is no stack trace\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\nNo logs\n\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [x] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "@gioargyr Have you checked tracking server logs? Can you also open the network tab in devtools on your browser and check if there are any failed requests?"
      },
      {
        "user": "kimminw00",
        "body": "I'm experiencing UI freezes when handling JSON files larger than 2MiB. The UI becomes temporarily unresponsive but recovers after a brief wait(1~2 min). Based on this behavior, I suspect the issue arises from attempting to load and render the entire JSON at once, which may cause performance bottlenecks in the visualization/rendering process."
      }
    ]
  },
  {
    "issue_number": 16071,
    "title": "[BUG] ERROR mlflow.server: Exception on /graphql when trying to open a run if auth is enabled.",
    "author": "John-Pywell",
    "state": "closed",
    "created_at": "2025-06-04T21:49:01Z",
    "updated_at": "2025-06-10T11:18:44Z",
    "labels": [
      "bug",
      "area/uiux",
      "area/tracking",
      "area/server-infra",
      "has-closing-pr"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: mlflow[auth] == 3.1.0rc3\n- Tracking server: mlflow[auth] == 3.1.0rc3\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 24.04.2 LTS\n- **Python version**: Python 3.12.9\n- **yarn version, if running the dev UI**: \n\n\n### Describe the problem\n\nThe error does not occur unless the launch option `-app-name basic-auth` is used.\n\nWhen navigating to a run page in the UI, e.g. `localhost:5000/#/experiments/614506273212328900/runs/bdc753d2173a4218b03b533ca1b2bd80`, the endpoint POST localhost:5000/graphql fails with a 500 code. In the tracking server's console, the following stack trace appears:\n\n```\n2025/06/04 15:39:49 ERROR mlflow.server: Exception on /graphql [POST]\nTraceback (most recent call last):\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1511, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 920, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 941, in finalize_request\n    response = self.process_response(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1319, in process_response\n    response = self.ensure_sync(func)(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/auth/__init__.py\", line 849, in _after_request\n    handler(resp)\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\nTypeError: _graphql() takes 0 positional arguments but 1 was given\n```\n\n### Tracking information\n\n\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nexport MLFLOW_FLASK_SERVER_SECRET_KEY=\"(insert auth secret)\" \nmlflow server --host 0.0.0.0 --port 5005 --app-name basic-auth\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nTraceback (most recent call last):\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1511, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 920, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 941, in finalize_request\n    response = self.process_response(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1319, in process_response\n    response = self.ensure_sync(func)(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/auth/__init__.py\", line 849, in _after_request\n    handler(resp)\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\nTypeError: _graphql() takes 0 positional arguments but 1 was given\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\n(mlp-proto) ppp596@mymachine:~/ml_pipeline$ ./run-mlflow.sh\n[2025-06-04 15:39:26 -0600] [2765489] [INFO] Starting gunicorn 23.0.0\n[2025-06-04 15:39:26 -0600] [2765489] [INFO] Listening at: http://0.0.0.0:5005 (2765489)\n[2025-06-04 15:39:26 -0600] [2765489] [INFO] Using worker: sync\n[2025-06-04 15:39:26 -0600] [2765490] [INFO] Booting worker with pid: 2765490\n[2025-06-04 15:39:26 -0600] [2765491] [INFO] Booting worker with pid: 2765491\n[2025-06-04 15:39:26 -0600] [2765492] [INFO] Booting worker with pid: 2765492\n[2025-06-04 15:39:26 -0600] [2765493] [INFO] Booting worker with pid: 2765493\n2025/06/04 15:39:29 WARNING mlflow.server.auth: This feature is still experimental and may change in a future release without warning\n2025/06/04 15:39:29 WARNING mlflow.server.auth: This feature is still experimental and may change in a future release without warning\n2025/06/04 15:39:29 WARNING mlflow.server.auth: This feature is still experimental and may change in a future release without warning\n2025/06/04 15:39:29 WARNING mlflow.server.auth: This feature is still experimental and may change in a future release without warning\n2025/06/04 15:39:49 ERROR mlflow.server: Exception on /graphql [POST]\nTraceback (most recent call last):\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1511, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 920, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 941, in finalize_request\n    response = self.process_response(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1319, in process_response\n    response = self.ensure_sync(func)(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/auth/__init__.py\", line 849, in _after_request\n    handler(resp)\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\nTypeError: _graphql() takes 0 positional arguments but 1 was given\n2025/06/04 15:39:50 ERROR mlflow.server: Exception on /graphql [POST]\nTraceback (most recent call last):\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1511, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 920, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 941, in finalize_request\n    response = self.process_response(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1319, in process_response\n    response = self.ensure_sync(func)(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/auth/__init__.py\", line 849, in _after_request\n    handler(resp)\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\nTypeError: _graphql() takes 0 positional arguments but 1 was given\n2025/06/04 15:39:51 ERROR mlflow.server: Exception on /graphql [POST]\nTraceback (most recent call last):\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1511, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 920, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 941, in finalize_request\n    response = self.process_response(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1319, in process_response\n    response = self.ensure_sync(func)(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/auth/__init__.py\", line 849, in _after_request\n    handler(resp)\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\nTypeError: _graphql() takes 0 positional arguments but 1 was given\n2025/06/04 15:39:54 ERROR mlflow.server: Exception on /graphql [POST]\nTraceback (most recent call last):\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1511, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 920, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 941, in finalize_request\n    response = self.process_response(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1319, in process_response\n    response = self.ensure_sync(func)(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/auth/__init__.py\", line 849, in _after_request\n    handler(resp)\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\nTypeError: _graphql() takes 0 positional arguments but 1 was given\n2025/06/04 15:39:55 ERROR mlflow.server: Exception on /graphql [POST]\nTraceback (most recent call last):\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1511, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 920, in full_dispatch_request\n    return self.finalize_request(rv)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 941, in finalize_request\n    response = self.process_response(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/flask/app.py\", line 1319, in process_response\n    response = self.ensure_sync(func)(response)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/auth/__init__.py\", line 849, in _after_request\n    handler(resp)\n  File \"/home/ppp596/pg/miniconda3/envs/mlp-proto/lib/python3.12/site-packages/mlflow/server/handlers.py\", line 591, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\nTypeError: _graphql() takes 0 positional arguments but 1 was given\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [x] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [x] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": []
  },
  {
    "issue_number": 11764,
    "title": "[FR] PyTorch Dataset/DataLoader support for log_input",
    "author": "daviddwlee84",
    "state": "open",
    "created_at": "2024-04-19T07:24:35Z",
    "updated_at": "2025-06-09T20:59:18Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\n\n### Proposal Summary\n\nCurrently, we don't have something like `mlflow.data.torch_dataset.PyTorchDataset` or `mlflow.data.from_torch`.\r\n\r\n- [mlflow.data â€” MLflow 2.12.1 documentation](https://mlflow.org/docs/latest/python_api/mlflow.data.html#mlflow.data.dataset.Dataset)\r\n- [MLflow Dataset Tracking Tutorial â€” MLflow 2.12.1 documentation](https://mlflow.org/docs/latest/tracking/data-api.html)\r\n\r\nThus it's not possible to use [`mlflow.log_input`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_input) with PyTorch easily (compares to other machine learning frameworks).\r\n\r\nAt least I found when using PyTorch Lightning ([pytorch autolog example](https://github.com/mlflow/mlflow/blob/master/examples/pytorch/MNIST/mnist_autolog_example.py)), the [`mlflow.pytorch.autolog`](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.autolog) is not able to capture either `torch.utils.data.DataLoader` or `lightning.LightningDataModule` we used in Lightning Trainer (not found in the MLflow UI Dataset column).\r\n\r\n- [Automatic Logging with MLflow Tracking â€” MLflow 2.12.1 documentation](https://mlflow.org/docs/latest/tracking/autolog.html#autolog-pytorch)\r\n\n\n### Motivation\n\n> #### What is the use case for this feature?\r\n\r\nFor recording the PyTorch Dataset / DataLoader used in an experiment. (Ideally include information of batch_size, etc.)\r\n\r\n> #### Why is this use case valuable to support for MLflow users in general?\r\n\r\nThis can make using `mlflow.pytorch.autolog` with PyTorch Lightning more intuitive.\r\n\r\n> #### Why is this use case valuable to support for your project(s) or organization?\r\n\r\nI would like to track the dataset I used in different experiment runs.\r\n\r\n> #### Why is it currently difficult to achieve this use case?\r\n\r\nMaybe the workaround is to create a `mlflow.data.Dataset` and `mlflow.data.DatasetSource` manually.\r\nAnd maybe is because a [`torch.utils.data.Dataset` or `torch.utils.data.DataLoader`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) is too flexible to be parsed versus a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)?\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "BenWilson2",
        "body": "Hi @daviddwlee84 please let us know if you have any questions when submitting the PR. We'll happily review your implementation! Thanks!"
      },
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      },
      {
        "user": "daviddwlee84",
        "body": "After some investigation, I might submit a draft pull request in a few days.\r\n\r\nThere are some random notes of how I might implement this.\r\n\r\nI will implement a `PyTorchDataset` and `PyTorchDatasetSource`.\r\nMaybe with `LightningDataModuleDataset` and `LightningDataModuleDatasetSource`.\r\n(https://github.com/mlflow/mlflow/blob/master/mlflow/data/dataset.py)\r\n(https://github.com/mlflow/mlflow/blob/master/mlflow/data/dataset_source.py)\r\n\r\nThe `PyTorchDataset` is a `Dataset` while the `LightningDataModuleDataset` is a `Dataset` + `PyFuncConvertibleDatasetMixin` (able to get the evaluation set).\r\n\r\nThere might be some issue that, PyTorch Dataset is quite flexible, that is, it can return basically anything.\r\nIn the draft version, I might assume it returns `torch.Tensor` or a tuple of it, then support all common return in the end.\r\n\r\nAnother issue is, that the DataSource `to_dict`, and `from_json` stuff might be tricky.\r\nWe need to somehow preserve the PyTorch Dataset class name to be able to reconstruct it from the JSON string.\r\nBut the Dataset class is obviously not serializable into a string. Need to find a workaround.\r\n\r\nFor the schema, the currently built-in `_infer_schema` doesn't support `torch.Tensor` yet. Even though we can convert it into a Numpy array, it is still not able to \"schema\" a tuple of `torch.Tensor` or simply concatenate a different shape Numpy array together.\r\nhttps://github.com/mlflow/mlflow/blob/acfa074d5798196a387fb5de96d13e7cbde603ed/mlflow/types/utils.py#L203-L238\r\n\r\nSo extending `_infer_schema` to support `torch.Tensor` and tuple of the array is required. And construct `Schema` from scratch to handling possible returns from a PyTorch Dataset\r\nhttps://github.com/mlflow/mlflow/blob/e34729a9e1cc8b2daf61232acfac5b18e7b56a09/mlflow/types/schema.py#L868\r\n\r\nWe also need to calculate digest from the sample of it.\r\nIt can be using the convert-to-numpy trick for now.\r\nhttps://github.com/mlflow/mlflow/blob/acfa074d5798196a387fb5de96d13e7cbde603ed/mlflow/data/digest_utils.py#L46\r\n\r\nAdditionally, we might somehow be able store some manual metadata. The user can used to describe how they construct the dataset. Or the meaning of each dimension.\r\n(For example, if PyTorch dataset is loading from multiple Pandas DataFrame (even though during training we treated it as a `torch.Tensor`, we might still want to record each column's schema like PandasDataset does)"
      }
    ]
  },
  {
    "issue_number": 15102,
    "title": "AttributeError: module 'mlflow' has no attribute 'trace'",
    "author": "saketh395",
    "state": "open",
    "created_at": "2025-03-25T06:27:04Z",
    "updated_at": "2025-06-09T15:13:15Z",
    "labels": [],
    "body": "using databricks-langchain package for my work and it has a dependency of mlflow\n\n**Got this error while executing:** \n\nAttributeError: module 'mlflow' has no attribute 'trace'\nFile <command-3937375196672>, line 10\n      8 from databricks.sdk import WorkspaceClient\n      9 from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n---> 10 from databricks_langchain import ChatDatabricks\n     12 graph_builder = StateGraph(MessagesState)\n     14 llm = ChatDatabricks(\n     15   endpoint=\"databricks-meta-llama-3-3-70b-instruct\"\n     16 )\n\n**Solutions tried:**\n\n1. using the latest mllfow version 2.21.0\n2. downgrading the version of mlflow\n3. used latest and older versions of databricks-langchain\n\nBut nothing helped.\n\n**_Need help in resolving this_**",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "Could you share the full stacktrace?"
      },
      {
        "user": "daniellok-db",
        "body": "also just a hunch but do you have a folder or file named \"mlflow\" in the directory you're running this in? this can mess with python imports. "
      },
      {
        "user": "CYBki",
        "body": "### Possible Cause: File or Folder Named mlflow in Project Directory\n\nThis kind of error is fairly common in Python:\n\nAttributeError: module 'mlflow' has no attribute 'trace'\nIf there is a file named mlflow.py or a folder named mlflow/ in your working directory, Python may import that instead of the actual mlflow package. This can cause missing attribute errors like the one above.\n\nHow to Check:\nFrom your terminal:\n\n`ls mlflow*`\nOr within Python:\n\n`import mlflow\nprint(mlflow.__file__)`\n\nIf the output looks like:\n\n/path/to/your/project/mlflow.py\n# or\n/path/to/your/project/mlflow/__init__.py\n\nthen Python is importing the wrong module.\n\nHow to Fix It:\nRename the conflicting file or folder. For example:\n\nmlflow.py â†’ my_mlflow_script.py\nClear Python cache files:\n\nfind . -name \"*.pyc\" -delete\nfind . -name \"__pycache__\" -delete\nRe-run your application.\n\n\n"
      }
    ]
  },
  {
    "issue_number": 10933,
    "title": "[BUG] POST search bug",
    "author": "moghadas76",
    "state": "open",
    "created_at": "2024-01-29T11:07:38Z",
    "updated_at": "2025-06-09T06:50:40Z",
    "labels": [
      "bug",
      "area/artifacts",
      "area/build",
      "area/deployments"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### Willingness to contribute\n\nYes. I can contribute a fix for this bug independently.\n\n### MLflow version\n\n- Client: 1.x.y\r\n- Tracking server: 1.x.y\r\n\n\n### System information\n\n-- Ubuntu 22.04\r\n-- MLFLOW 2.10\r\n\n\n### Describe the problem\n\n500 Internal Server Error Internal Server Error The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.\n\n### Tracking information\n\n\r\n```python\r\n         ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/flask/app.py\", line 852, in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/server/handlers.py\", line 494, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/server/handlers.py\", line 535, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/server/handlers.py\", line 916, in _search_runs\r\n    run_entities = _get_tracking_store().search_runs(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/store/tracking/abstract_store.py\", line 327, in search_runs\r\n    runs, token = self._search_runs(\r\n                  ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py\", line 915, in _search_runs\r\n    run_infos = self._list_run_infos(experiment_id, run_view_type)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py\", line 883, in _list_run_infos\r\n    run_info = self._get_run_info_from_dir(r_dir)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py\", line 695, in _get_run_info_from_dir\r\n    return _read_persisted_run_info_dict(meta)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py\", line 131, in _read_persisted_run_info_dict\r\n    dict_copy = run_info_dict.copy()\r\n                ^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'NoneType' object has no attribute 'copy'\r\n```\r\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nREPLACE_ME\r\n```\r\n\n\n### Stack trace\n\n\r\n```python\r\n^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/flask/app.py\", line 852, in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/server/handlers.py\", line 494, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/server/handlers.py\", line 535, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/server/handlers.py\", line 916, in _search_runs\r\n    run_entities = _get_tracking_store().search_runs(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/store/tracking/abstract_store.py\", line 327, in search_runs\r\n    runs, token = self._search_runs(\r\n                  ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py\", line 915, in _search_runs\r\n    run_infos = self._list_run_infos(experiment_id, run_view_type)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py\", line 883, in _list_run_infos\r\n    run_info = self._get_run_info_from_dir(r_dir)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py\", line 695, in _get_run_info_from_dir\r\n    return _read_persisted_run_info_dict(meta)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/seyed/miniconda3/envs/env/lib/python3.11/site-packages/mlflow/store/tracking/file_store.py\", line 131, in _read_persisted_run_info_dict\r\n    dict_copy = run_info_dict.copy()\r\n                ^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'NoneType' object has no attribute 'copy'\r\n```\r\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nREPLACE_ME\r\n```\r\n\n\n### What component(s) does this bug affect?\n\n- [X] `area/artifacts`: Artifact stores and artifact logging\n- [X] `area/build`: Build and test infrastructure for MLflow\n- [X] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "@moghadas76 Please provide code to repro"
      },
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      },
      {
        "user": "wassimrkik",
        "body": "following for the same issue"
      }
    ]
  },
  {
    "issue_number": 12909,
    "title": "[DOC-FIX] Incorrect documentation on building sagemaker docker container",
    "author": "MartinBarus",
    "state": "open",
    "created_at": "2024-08-08T12:48:06Z",
    "updated_at": "2025-06-09T04:55:25Z",
    "labels": [
      "area/docs"
    ],
    "body": "### Willingness to contribute\r\n\r\nNo. I cannot contribute a documentation fix at this time.\r\n\r\n### URL(s) with the issue\r\n\r\nhttps://mlflow.org/docs/latest/deployment/deploy-model-to-sagemaker.html#step-2-build-a-docker-image-and-push-to-ecr\r\n\r\nIncorrect documentation in step `Step 2: Build a Docker Image and Push to ECR`\r\n\r\nfollowing suggested code \r\n```\r\nmlflow sagemaker build-and-push-container  -m runs:/<run_id>/model\r\n```\r\n\r\nDoes not work, since  `mlflow sagemaker build-and-push-container ` does not support `-m` parameter.\r\n\r\n\r\nRunning only `mlflow sagemaker build-and-push-container` results in an incorrect image (in my case incorrect pyhton version) and then using this image going forward makes the docker startup time very slow, since it installs the correct python version first on startup, instead of creating a proper docker container with proper python + libs.\r\n\r\n\r\n### Description of proposal (what needs changing)\r\n\r\nAt least change the documentation so that the code can run successfully, so change line\r\n```\r\nmlflow sagemaker build-and-push-container  -m runs:/<run_id>/model\r\n```\r\n\r\nto \r\n\r\n```\r\nmlflow sagemaker build-and-push-container\r\n```\r\n\r\nIdeally document how to create a proper image with proper python and libs baked in them, based on the mlflow model.\r\n\r\nI can firstly create the image like this:\r\n```\r\nmlflow models build-docker -m runs:/<run_id>/model\r\n```\r\n\r\nbut I am not sure if it then can be uploaded like this\r\n```\r\nmlflow sagemaker build-and-push-container -c <image_name> --no-build\r\n```\r\n\r\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      },
      {
        "user": "ahmedalsiba",
        "body": "Facing the same issue. @MartinBarus have you found a work around?"
      },
      {
        "user": "MartinBarus",
        "body": "@ahmedalsiba I created the docker image like this `mlflow models build-docker -m runs:/<run_id>/model`\r\n\r\nand then followed the instructions in AWS ECR on how to push the image there manually. You can have a look here https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html"
      }
    ]
  },
  {
    "issue_number": 16121,
    "title": "[docs] Unused Markdown directive :::tips in docs/prompts/run-and-model.mdx causes Docusaurus warning",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-06T11:25:36Z",
    "updated_at": "2025-06-09T02:17:22Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "When running `yarn build` in the docs, the following warning appears:\n\n```\n[WARNING] Docusaurus found 1 unused Markdown directives in file \"docs/prompts/run-and-model.mdx\"\n\n:::tips (123:1)\nYour content might render in an unexpected way. Visit https://github.com/facebook/docusaurus/pull/9394 to find out why and how to fix it.\n```\n\nThis is due to an unused or unrecognized Markdown directive (`:::tips`) in `docs/prompts/run-and-model.mdx`. Please review the file and update or remove the directive to resolve the warning.\n\nReference: https://github.com/facebook/docusaurus/pull/9394\n\nSteps to reproduce:\n1. Run `yarn build` in the `docs` directory.\n2. Observe the warning about the unused Markdown directive.\n\nExpected behavior:\n- No warnings about unused Markdown directives during the build process.\n\nActual behavior:\n- Warning about `:::tips` directive appears during build.\n\nFile: `docs/prompts/run-and-model.mdx`\nLine: 123\n\n---\nThis issue was filed automatically based on a user report.",
    "comments": []
  },
  {
    "issue_number": 16120,
    "title": "Performance issue while Using MS SQL Server",
    "author": "deepupv",
    "state": "open",
    "created_at": "2025-06-06T09:07:14Z",
    "updated_at": "2025-06-08T16:03:48Z",
    "labels": [
      "bug"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nOther\n\n### MLflow version\n\n- Client: 2.20.1\n- Tracking server: 2.20.1\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 11\n- **Python version**: 3.11\n- **yarn version, if running the dev UI**:\n\n\n### Describe the problem\n\nI hosted mlflow server in K8s with MS SQL server as the backend. When the python code does search on mlflow its generates queries like below in SQL Server. The below query is causing serious performance issue when the volume of data is high.\n\nThe data type of column run_uuid in database is varchar. But the query is doing a nvarchar conversion like N'116f1f2e97ed4145a68af2a5667d5d7a' which is not required.\n\nThe current query that takes more than 30 seconds to respond.\n\nSELECT params.run_uuid AS params_run_uuid, \n       params.[key] AS params_key, \n                           params.value AS params_value  \n  FROM params  \nWHERE params.run_uuid IN (N'116f1f2e97ed4145a68af2a5667d5d7a', \n                           N'1342e2d2e03842189c5580219342e25a',                                                                                                                                                    N'a40debfeebdc49dfb667633a0cfb1ca7')\nGO\n\nSolution:  Avoid the explicit nvarchar type casting. One the casting is removed, the query returns result in 1 second.\n\nSELECT params.run_uuid AS params_run_uuid, \n       params.[key] AS params_key, \n                           params.value AS params_value  \n  FROM params  \nWHERE params.run_uuid IN ('116f1f2e97ed4145a68af2a5667d5d7a', \n                           '1342e2d2e03842189c5580219342e25a',                                                                                                                                                    'a40debfeebdc49dfb667633a0cfb1ca7')\nGO\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "@deepupv We use SQLAlchemy to generate SQL and it seems there is [a similar issue](https://github.com/sqlalchemy/sqlalchemy/discussions/10524) opened in their repository, which I suspect is relevant. Will investigate how to prevent this behavior."
      }
    ]
  },
  {
    "issue_number": 16094,
    "title": "[BUG] mlflow.set_tracking_uri() fails with relative local paths like \"mlruns\"",
    "author": "Harsh4962",
    "state": "open",
    "created_at": "2025-06-05T13:04:05Z",
    "updated_at": "2025-06-08T15:55:20Z",
    "labels": [
      "bug",
      "area/windows",
      "area/artifacts",
      "area/tracking",
      "integrations/azure"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- MLflow version : 2 . 22 . 0\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 11\n- **Python version**: 3.11.9\n- **yarn version, if running the dev UI**: -\n\n\n\n### Describe the problem\n\nWhen importing MLflow and calling `mlflow.get_tracking_uri()`, the default tracking URI returned is a local file URI like: file:///c:/users/...\n\nHowever, if we try to set the tracking URI to this default value or other local paths without the proper `file://` scheme (e.g., `\"mlruns\"`), MLflow throws an error expecting only `http://`, `https://`, or `file://` schemes.\n\nTo avoid the error, users have to manually set the tracking URI with a proper URI prefix, for example:\n\n```python\nmlflow.set_tracking_uri(\"file:///c:/users/...\")\n(or)\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n\nThis behavior is confusing because the default local tracking URI is returned as a file:// URI, but setting a relative path without the scheme causes failure. It would be better if MLflow:\n\n1) Automatically accepted relative local paths and internally converted them to valid file:// URIs,\n2) Or improved the error message to clearly instruct users on the correct URI format.\n\n\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nimport mlflow\n\n# Setting tracking URI to a relative local path without scheme causes error\nmlflow.set_tracking_uri(\"mlruns\")\n\n# MlflowException: InvalidTrackingUriException: URI 'mlruns' must use scheme [http, https, file]\n\n\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nTraceback (most recent call last):\n  File \"C:\\Users\\harsh\\OneDrive\\Desktop\\MLOps-Learn\\MLOps-MLFlow\\src\\file1.py\", line 50, in <module>\n    mlflow.log_artifact(\"Confusion-matrix.png\")\n  File \"C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlflow\\tracking\\fluent.py\", line 1179, in log_artifact\n    MlflowClient().log_artifact(run_id, local_path, artifact_path)\n  File \"C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlflow\\tracking\\client.py\", line 2379, in log_artifact\n    self._tracking_client.log_artifact(run_id, local_path, artifact_path)\n  File \"C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py\", line 923, in log_artifact\n    artifact_repo = self._get_artifact_repo(run_id)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py\", line 906, in _get_artifact_repo\n    artifact_repo = get_artifact_repository(artifact_uri)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlflow\\store\\artifact\\artifact_repository_registry.py\", line 133, in get_artifact_repository\n    return _artifact_repository_registry.get_artifact_repository(artifact_uri)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlflow\\store\\artifact\\artifact_repository_registry.py\", line 77, in get_artifact_repository\n    return repository(artifact_uri)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlflow\\store\\artifact\\mlflow_artifacts_repo.py\", line 51, in __init__\n    super().__init__(self.resolve_uri(artifact_uri, get_tracking_uri()))\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlflow\\store\\artifact\\mlflow_artifacts_repo.py\", line 65, in resolve_uri\n    _validate_uri_scheme(track_parse)\n  File \"C:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mlflow\\store\\artifact\\mlflow_artifacts_repo.py\", line 35, in _validate_uri_scheme\n    raise MlflowException(\nmlflow.exceptions.MlflowException: When an mlflow-artifacts URI was supplied, the tracking URI must be a valid http or https URI, but it was currently set to file:///C:/Users/harsh/OneDrive/Desktop/MLOps-Learn/MLOps-MLFlow/mlruns. Perhaps you forgot to set the tracking URI to the running MLflow server. To set the tracking URI, use either of the following methods:    \n1. Set the MLFLOW_TRACKING_URI environment variable to the desired tracking URI. `export MLFLOW_TRACKING_URI=http://localhost:5000`\n2. Set the tracking URI programmatically by calling `mlflow.set_tracking_uri`. `mlflow.set_tracking_uri('http://localhost:5000')`\nPS C:\\Users\\harsh\\OneDrive\\Desktop\\MLOps-Learn\\MLOps-MLFlow>\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [x] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "@Harsh4962 I could not reproduce the issue. Actually we don't have `InvalidTrackingUriException` in the code base, could you check if you have mlflow installed correctly?"
      }
    ]
  },
  {
    "issue_number": 16092,
    "title": "[BUG] mlflow.evaluate: confusion_matrix crashes when y_true or y_pred contains only one class",
    "author": "StefanieSwz",
    "state": "open",
    "created_at": "2025-06-05T08:54:18Z",
    "updated_at": "2025-06-08T15:42:46Z",
    "labels": [
      "bug",
      "area/models"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.20.3 (local env)\n- Tracking server: Managed by Databricks\n\n\n\n### System information\n\n- OS: Ubuntu 24.04.2 LTS (WSL2)\n- Kernel: Linux 5.15.167.4-microsoft-standard-WSL2\n- Architecture: x86-64\n- Virtualization: WSL2\n- Python 3.12.9\n- scikit-learn Version: 1.6.1\n\n\n\n### Describe the problem\n\n\n###  **Bug Report: `mlflow.evaluate()` crashes on binary classification with single-class input despite `label_list` being passed**\n\n---\n\n#### Inital function call\n\nI'm using `mlflow.evaluate()` in a classification setting with `model=None` and precomputed predictions. My call includes a **binary `label_list`**, but when the union of `y_true` and `y_pred` contains only one class, MLflow raises a `ValueError`.\n\n```python\nresults: EvaluationResult = mlflow.evaluate(\n    model=None,\n    data=data,\n    targets=label_col_name,\n    predictions=prediction_col_name,\n    model_type=\"classifier\",\n    evaluator_config={\n        \"default\": {\n            \"metric_prefix\": metric_prefix,\n            \"label_list\": [\n                Response.TRUE.value,\n                Response.FALSE.value,\n            ],\n            \"pos_label\": Response.FALSE.value, \n        },\n    },\n)\n```\n\nDespite passing `label_list`, the evaluation fails when only one class is present in the data.\n\n---\n\n####  **Error Observed**\n\nThe traceback points to the `mlflow.models.evaluation.evaluators.classifier._get_binary_classifier_metrics` function:\n\n```text\nValueError: not enough values to unpack (expected 4, got 1)\n```\n\nTriggered by:\n\n```python\ntn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n```\n\nThis occurs because `confusion_matrix` returns a 1x1 matrix when only one class is present, and `ravel()` fails to unpack four values.\n\n---\n\n#### Proposed Workaround\n\nTo unblock my workflow, I applied the following monkey patch to override `_get_binary_classifier_metrics` and ensure that `labels` are explicitly passed to `confusion_matrix`:\n\n```python\ndef custom_get_binary_classifier_metrics(\n    *, y_true, y_pred, y_proba=None, labels=None, pos_label=1, sample_weights=None\n):\n    \"\"\"\n    Computes binary classification metrics, including confusion matrix components and common\n    evaluation metrics such as precision, recall, and F1 score.\n\n    This function monkey-patches MLflow's internal `_get_binary_classifier_metrics` function\n    to handle edge cases where only a single class is present in either `y_true` or `y_pred`.\n    It explicitly passes the `labels` argument to `confusion_matrix` to ensure consistent\n    label alignment, avoiding crashes due to missing classes in small or imbalanced datasets.\n\n    \"\"\"\n    with mlflow_classifier_module._suppress_class_imbalance_errors(ValueError):\n        if labels is not None and len(labels) == 2:\n            cm = confusion_matrix(y_true, y_pred, labels=labels) # fix\n        else:\n            cm = confusion_matrix(y_true, y_pred)\n        tn, fp, fn, tp = cm.ravel()\n\n        return {\n            \"true_negatives\": tn,\n            \"false_positives\": fp,\n            \"false_negatives\": fn,\n            \"true_positives\": tp,\n            **mlflow_classifier_module._get_common_classifier_metrics(\n                y_true=y_true,\n                y_pred=y_pred,\n                y_proba=y_proba,\n                labels=labels,\n                average=\"binary\",\n                pos_label=pos_label,\n                sample_weights=sample_weights,\n            ),\n        }\n```\n\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nimport mlflow\nimport pandas as pd\n\n# NOTE: This is not running, as it needs and active experiment\n# Simulated data with only one class in y_true and y_pred\ndata = pd.DataFrame(\n    {\n        \"y_true\": [\"true\", \"true\", \"true\", \"true\", \"true\"],  # only one class\n        \"y_pred\": [\"true\", \"true\", \"true\", \"true\", \"true\"],  # same\n    }\n)\n\n# Will crash due to single-class input\nmlflow.evaluate(\n    model=None,\n    data=data,\n    targets=\"y_true\",\n    predictions=\"y_pred\",\n    model_type=\"classifier\",\n    evaluator_config={\n        \"default\": {\n            \"label_list\": [\"true\", \"false\"],  # explicitly defining both labels\n            \"pos_label\": \"false\",  # define positive class\n        }\n    },\n)\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nValueError was raised: not enough values to unpack (expected 4, got 1)\n\nTraceback (most recent call last):\n  File \"/home/stschwarz/Project_x/devtools/evaluate_mlflow.py\", line 205, in evaluate_run\n    evaluate_assessment(\n  File \"/home/stschwarz/Project_x/devtools/evaluate_mlflow.py\", line 349, in evaluate_assessment\n    assessment_results: EvaluationResult = mlflow.evaluate(\n                                           ^^^^^^^^^^^^^^^^\n  File \"/home/stschwarz/anaconda3/envs/my-env/lib/python3.12/site-packages/mlflow/models/evaluation/base.py\", line 1758, in evaluate\n    evaluate_result = _evaluate(\n                      ^^^^^^^^^^\n  File \"/home/stschwarz/anaconda3/envs/my-env/lib/python3.12/site-packages/mlflow/models/evaluation/base.py\", line 1025, in _evaluate\n    eval_result = eval_.evaluator.evaluate(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/stschwarz/anaconda3/envs/my-env/lib/python3.12/site-packages/mlflow/models/evaluation/default_evaluator.py\", line 957, in evaluate\n    return self._evaluate(model, extra_metrics, custom_artifacts)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/stschwarz/anaconda3/envs/my-env/lib/python3.12/site-packages/mlflow/models/evaluation/evaluators/classifier.py\", line 73, in _evaluate\n    self._compute_builtin_metrics(model)\n  File \"/home/stschwarz/anaconda3/envs/my-env/lib/python3.12/site-packages/mlflow/models/evaluation/evaluators/classifier.py\", line 144, in _compute_builtin_metrics\n    metrics = _get_binary_classifier_metrics(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/stschwarz/anaconda3/envs/my-env/lib/python3.12/site-packages/mlflow/models/evaluation/evaluators/classifier.py\", line 477, in _get_binary_classifier_metrics\n    tn, fp, fn, tp = sk_metrics.confusion_matrix(y_true, y_pred).ravel()\n    ^^^^^^^^^^^^^^\nValueError: not enough values to unpack (expected 4, got 1)\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "heidekrueger",
        "body": "I'm also affected by this bug.\n\nSome additional context:\nA similar issue was previously described in #12790 and _partially_ addressed in #12825 / 82771d8d825af070dd8682aea4848d203d8af241 \nbut the test in that PR only tests the case where `\"_MLFLOW_EVALUATE_SUPPRESS_CLASSIFICATION_ERRORS\"=True`. In that case, the process does not fail/crash, but the error from sklean is _not_ mitigated properly and expected metrics are not logged."
      },
      {
        "user": "B-Step62",
        "body": "Thank you for bringing the issue to our attention. We will investigate on the issue."
      }
    ]
  },
  {
    "issue_number": 2183,
    "title": "[FR] Allow configuring the number of runs loaded at a time in the experiment view",
    "author": "smurching",
    "state": "closed",
    "created_at": "2019-12-10T00:40:15Z",
    "updated_at": "2025-06-07T17:03:31Z",
    "labels": [
      "enhancement",
      "area/uiux",
      "area/tracking",
      "priority/backlog"
    ],
    "body": "## Describe the proposal\r\nAs described in [this post](http://feedback.databricks.com/forums/263785-product-feedback/suggestions/39092506-for-an-mlflow-experiment-you-were-able-to-see-all), we should enable we should make the number of runs that appear per \"page\" load configurable, so that users can view more than 100 runs at a time without clicking \"load more\" a bunch of times. \r\n\r\n### Motivation\r\nSee above - we should make the number of runs that appear per \"page\" load configurable, so that users can view more than 100 runs at a time without clicking \"load more\" a bunch of times\r\n\r\n### Proposed Changes\r\nTo implement this proposal, we can expose the [max_results SearchRuns API parameter](https://github.com/mlflow/mlflow/blob/23df0e111a9802181d4e0ea1e85934805514e3d0/mlflow/protos/service.proto#L926) via the search UI rendered by [ExperimentView](https://github.com/mlflow/mlflow/blob/master/mlflow/server/js/src/components/ExperimentView.js#L364) & likely updating logic [here](https://github.com/mlflow/mlflow/blob/23df0e111a9802181d4e0ea1e85934805514e3d0/mlflow/server/js/src/components/ExperimentPage.js#L145) & [here](https://github.com/mlflow/mlflow/blob/23df0e111a9802181d4e0ea1e85934805514e3d0/mlflow/server/js/src/components/ExperimentPage.js#L76) to pass the number of results to the REST API call. We should also add unit tests [here](https://github.com/mlflow/mlflow/blob/23df0e111a9802181d4e0ea1e85934805514e3d0/mlflow/server/js/src/components/ExperimentPage.test.js#L173)\r\n",
    "comments": [
      {
        "user": "mparkhe",
        "body": "@gioa for design\r\n@Zangr for thoughts on proposed UI changes"
      },
      {
        "user": "chenmoneygithub",
        "body": "Since there hasn't been any activity for 4 years I am closing this issue. We can reopen on demand. "
      },
      {
        "user": "vandit2209",
        "body": "Please implement this feature. Due to increasing number of hyper-parameters the number of trials have increased and loading just 100 per button is annoying..."
      }
    ]
  },
  {
    "issue_number": 16019,
    "title": "[BUG] Security Vulnerability",
    "author": "zdi-disclosures",
    "state": "closed",
    "created_at": "2025-06-02T14:37:47Z",
    "updated_at": "2025-06-07T08:58:08Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "We have submitted a new vulnerability report \"ZDI-CAN-26921\" via email to [mlflow-oss-maintainers@googlegroups.com](mailto:mlflow-oss-maintainers@googlegroups.com), please let us know if you have any questions",
    "comments": [
      {
        "user": "harupy",
        "body": "Thanks for reaching out. Replied to your email."
      }
    ]
  },
  {
    "issue_number": 12140,
    "title": "Artifact files are not removed from tmp/ folder",
    "author": "LarsdeRuiter",
    "state": "open",
    "created_at": "2024-05-27T14:15:57Z",
    "updated_at": "2025-06-06T17:48:20Z",
    "labels": [
      "bug",
      "area/artifacts",
      "area/docker",
      "area/server-infra"
    ],
    "body": "### Issues Policy acknowledgement\r\n\r\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\r\n\r\n### Where did you encounter this bug?\r\n\r\nLocal machine\r\n\r\n### Willingness to contribute\r\n\r\nYes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n\r\n### MLflow version\r\n\r\n- Client: not sure and probably irrelevant.\r\n- Tracking server: 2.11.3\r\n\r\n### System information\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 6.1.20-2~bpo11+1 (2023-04-23) x86_64 GNU/Linux\r\n- **Python version**: 3.10.11\r\n\r\n\r\n### Describe the problem\r\n\r\nI have been using a self-hosted MLFlow setup succesfully for a while now. Since yesterday I ran into a problem. The container in which my client is running is **piling up artifacts in a temporary files folder /tmp**. Today it crashed since it ran out of disk space.\r\n\r\nSome details on my setup:\r\nI am running MLFlow on a VM. \r\n- My colleagues run locally installed clients.\r\n- The tracking server runs on a Docker container in Linux.\r\n- Backend is a Postgres database.\r\n- Artifact store is Azure Blob storage.\r\n\r\nI was under the impression that no files were actually being stored on the container in which the MLFlow tracking server is running, since it is storing them in Azure Blob storage, but clearly I was wrong. I assumed that /tmp was actually a temporary drive, so would be removed, e.g. after a successful upload to Azure.\r\n\r\nIs this intended behavior? Or is the temporary drive supposed to get rid of the files automatically? If not, is there an existing solution to make sure my tracking server doesn't blow up over time? I tried to look for any settings within the MLFlow docs to make sure the /tmp files are deleted after upload, but cannot find anything. I'd also rather not delete and re-execute my container every month because of this issue.\r\n\r\n\r\n\r\nDisclaimer: This is my first issue on GitHub ever, so apologies if my question is unclear. I realize this question is very broad and probably unrelated to my specific setup. I tried to read about this in the documentation but could not find anything, hence I try it here.\r\n\r\n\r\n\r\n### Tracking information\r\n\r\nN/A\r\n\r\n### Code to reproduce issue\r\n\r\nN/A\r\n\r\n### Stack trace\r\n\r\nN/A\r\n\r\n### Other info / logs\r\n\r\nN/A\r\n\r\n\r\n### What component(s) does this bug affect?\r\n\r\n- [X] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [X] `area/server-infra`: MLflow Tracking server backend\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n### What interface(s) does this bug affect?\r\n\r\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [X] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\n### What language(s) does this bug affect?\r\n\r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\n### What integration(s) does this bug affect?\r\n\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "Thanks for the report, and the issue make sense! Could you provide a little more informationâ€”for example, what seems to be taking up the most space? This will help us prioritize what to fix first."
      },
      {
        "user": "LarsdeRuiter",
        "body": "I looked into the issue a bit more myself and learned that is does not have to do with the logging of artifacts to Azure. It happens when using the MLFlow webapp to look at the graphs. \r\n\r\nWhen artifacts are loaded, the MLFlow server will download the required artifact from the Azure blob backend. It stores them in /tmp, each artifact in a separate folder. My colleagues are logging Plotly graphs (and checking them out in the UI), which are .html files of 4MB each. This piled up pretty quickly.\r\n\r\nI guess that the server should be cleaning its /tmp directory when the user stops looking at the graph. This however doesn't seem to be happening.\r\n\r\nLet me know if you need more info!\r\n\r\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      }
    ]
  },
  {
    "issue_number": 16095,
    "title": "[BUG] mlflow 2.20.x cannot build docker container because of pandas 2.3.0",
    "author": "harmonic-jamie",
    "state": "closed",
    "created_at": "2025-06-05T13:07:00Z",
    "updated_at": "2025-06-06T10:48:03Z",
    "labels": [
      "bug",
      "area/models",
      "area/docker",
      "area/deployments"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n2.20.3\nReproducible on 2.20.4 as well\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 24.04\n- **Python version**: 3.11.11\n- **yarn version, if running the dev UI**: N/A\n\n\n### Describe the problem\n\n`mlflow models build-docker` fails when the mlflow version is 2.20.x because it specifies a pandas dependency of \"<3\".\n\nPandas 2.3.0 has just been released and doesn't seem to have a wheel that is compatible with the `docker.io/library/ubuntu:20.04` base image that the docker build is based upon. As such, it downloads pandas source and attempts to build it and fails.\n\nSee this docker build log.\n\n[docker-build.log](https://github.com/user-attachments/files/20610968/docker-build.log)\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nimport mlflow\n\nX, y = load_iris(return_X_y=True)\nmodel = LogisticRegression().fit(X, y)\n\nmlflow.set_experiment(\"example-model\")\nwith mlflow.start_run():\n    mlflow.sklearn.log_model(model, \"model\")\n\n# e.g. if the model uri is then: \"runs:/54a89f415b5943f78859f5c7cbe2abc6/model\"\n# this command in the shell will attempt to build the docker image and fail\n# BUILDKIT_PROGRESS=plain MLFLOW_TRACKING_URI=http://tracking-uri.example.com:5000 mlflow models build-docker \\\n# --model-uri \"runs:/54a89f415b5943f78859f5c7cbe2abc6/model\" \\\n# --name \"example-model\"\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nDownloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 34.04it/s]\n2025/06/05 14:05:01 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\nDownloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 106.59it/s]\n2025/06/05 14:05:01 INFO mlflow.pyfunc.backend: Building docker image with name example-model\n#0 building with \"default\" instance using docker driver\n\n#1 [internal] load build definition from Dockerfile\n#1 transferring dockerfile: 2.52kB done\n#1 DONE 0.1s\n\n#2 [internal] load metadata for docker.io/library/ubuntu:20.04\n#2 DONE 0.2s\n\n#3 [internal] load .dockerignore\n#3 transferring context: 2B done\n#3 DONE 0.0s\n\n#4 [ 1/17] FROM docker.io/library/ubuntu:20.04@sha256:8feb4d8ca5354def3d8fce243717141ce31e2c428701f6682bd2fafe15388214\n#4 DONE 0.0s\n\n#5 [ 2/17] RUN apt-get -y update && DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get install -y --no-install-recommends wget curl nginx ca-certificates bzip2 build-essential cmake git-core\n#5 CACHED\n\n#6 [ 5/17] RUN apt install -y python3.9 python3.9-distutils     && ln -s -f $(which python3.9) /usr/bin/python     && wget https://bootstrap.pypa.io/get-pip.py -O /tmp/get-pip.py     && python /tmp/get-pip.py\n#6 CACHED\n\n#7 [ 4/17] RUN git clone     --depth 1     --branch $(git ls-remote --tags --sort=v:refname https://github.com/pyenv/pyenv.git | grep -o -E 'v[1-9]+(\\.[1-9]+)+$' | tail -1)     https://github.com/pyenv/pyenv.git /root/.pyenv\n#7 CACHED\n\n#8 [ 6/17] RUN pip install virtualenv\n#8 CACHED\n\n#9 [ 7/17] RUN apt-get install -y --no-install-recommends openjdk-11-jdk maven\n#9 CACHED\n\n#10 [ 3/17] RUN DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get -y install tzdata     libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm     libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\n#10 CACHED\n\n#11 [ 8/17] WORKDIR /opt/mlflow\n#11 CACHED\n\n#12 [internal] load build context\n#12 transferring context: 2.12kB done\n#12 DONE 0.1s\n\n#13 [ 9/17] RUN pip install mlflow==2.20.3\n#13 0.525 Collecting mlflow==2.20.3\n#13 0.558   Downloading mlflow-2.20.3-py3-none-any.whl.metadata (30 kB)\n#13 0.601 Collecting mlflow-skinny==2.20.3 (from mlflow==2.20.3)\n#13 0.614   Downloading mlflow_skinny-2.20.3-py3-none-any.whl.metadata (31 kB)\n#13 0.645 Collecting Flask<4 (from mlflow==2.20.3)\n#13 0.655   Downloading flask-3.1.1-py3-none-any.whl.metadata (3.0 kB)\n#13 0.679 Collecting Jinja2<4,>=2.11 (from mlflow==2.20.3)\n#13 0.690   Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n#13 0.729 Collecting alembic!=1.10.0,<2 (from mlflow==2.20.3)\n#13 0.737   Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n#13 0.758 Collecting docker<8,>=4.0.0 (from mlflow==2.20.3)\n#13 0.767   Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n#13 0.794 Collecting graphene<4 (from mlflow==2.20.3)\n#13 0.802   Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n#13 0.826 Collecting gunicorn<24 (from mlflow==2.20.3)\n#13 0.834   Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n#13 0.854 Collecting markdown<4,>=3.3 (from mlflow==2.20.3)\n#13 0.861   Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n#13 0.955 Collecting matplotlib<4 (from mlflow==2.20.3)\n#13 0.966   Downloading matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n#13 1.084 Collecting numpy<3 (from mlflow==2.20.3)\n#13 1.093   Downloading numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n#13 1.196 Collecting pandas<3 (from mlflow==2.20.3)\n#13 1.206   Downloading pandas-2.3.0.tar.gz (4.5 MB)\n#13 1.377      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4.5/4.5 MB 26.0 MB/s eta 0:00:00\n#13 1.839   Installing build dependencies: started\n#13 3.969   Installing build dependencies: finished with status 'done'\n#13 3.971   Getting requirements to build wheel: started\n#13 4.071   Getting requirements to build wheel: finished with status 'done'\n#13 4.075   Installing backend dependencies: started\n#13 4.551   Installing backend dependencies: finished with status 'done'\n#13 4.552   Preparing metadata (pyproject.toml): started\n#13 6.621   Preparing metadata (pyproject.toml): finished with status 'error'\n#13 6.625   error: subprocess-exited-with-error\n#13 6.625   \n#13 6.625   Ã— Preparing metadata (pyproject.toml) did not run successfully.\n#13 6.625   â”‚ exit code: 1\n#13 6.625   â•°â”€> [22 lines of output]\n#13 6.625       + meson setup /tmp/pip-install-_5klhxl8/pandas_60a2f95091fe45d3a541ee16f727daa8 /tmp/pip-install-_5klhxl8/pandas_60a2f95091fe45d3a541ee16f727daa8/.mesonpy-_l8zinuy -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=/tmp/pip-install-_5klhxl8/pandas_60a2f95091fe45d3a541ee16f727daa8/.mesonpy-_l8zinuy/meson-python-native-file.ini\n#13 6.625       The Meson build system\n#13 6.625       Version: 1.8.1\n#13 6.625       Source dir: /tmp/pip-install-_5klhxl8/pandas_60a2f95091fe45d3a541ee16f727daa8\n#13 6.625       Build dir: /tmp/pip-install-_5klhxl8/pandas_60a2f95091fe45d3a541ee16f727daa8/.mesonpy-_l8zinuy\n#13 6.625       Build type: native build\n#13 6.625       Project name: pandas\n#13 6.625       Project version: 2.3.0\n#13 6.625       C compiler for the host machine: cc (gcc 9.4.0 \"cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\")\n#13 6.625       C linker for the host machine: cc ld.bfd 2.34\n#13 6.625       C++ compiler for the host machine: c++ (gcc 9.4.0 \"c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\")\n#13 6.625       C++ linker for the host machine: c++ ld.bfd 2.34\n#13 6.625       Cython compiler for the host machine: cython (cython 3.1.1)\n#13 6.625       Host machine cpu family: x86_64\n#13 6.625       Host machine cpu: x86_64\n#13 6.625       Program python found: YES (/usr/bin/python)\n#13 6.625       Found pkg-config: YES (/usr/bin/pkg-config) 0.29.1\n#13 6.625       Run-time dependency python found: NO (tried pkgconfig, pkgconfig and sysconfig)\n#13 6.625       \n#13 6.625       ../pandas/_libs/tslibs/meson.build:32:7: ERROR: Python dependency not found\n#13 6.625       \n#13 6.625       A full log can be found at /tmp/pip-install-_5klhxl8/pandas_60a2f95091fe45d3a541ee16f727daa8/.mesonpy-_l8zinuy/meson-logs/meson-log.txt\n#13 6.625       [end of output]\n#13 6.625   \n#13 6.625   note: This error originates from a subprocess, and is likely not a problem with pip.\n#13 6.628 error: metadata-generation-failed\n#13 6.628 \n#13 6.628 Ã— Encountered error while generating package metadata.\n#13 6.628 â•°â”€> See above for output.\n#13 6.628 \n#13 6.628 note: This is an issue with the package mentioned above, not pip.\n#13 6.628 hint: See above for details.\n#13 ERROR: process \"/bin/sh -c pip install mlflow==2.20.3\" did not complete successfully: exit code: 1\n------\n > [ 9/17] RUN pip install mlflow==2.20.3:\n6.625       [end of output]\n6.625   \n6.625   note: This error originates from a subprocess, and is likely not a problem with pip.\n6.628 error: metadata-generation-failed\n6.628 \n6.628 Ã— Encountered error while generating package metadata.\n6.628 â•°â”€> See above for output.\n6.628 \n6.628 note: This is an issue with the package mentioned above, not pip.\n6.628 hint: See above for details.\n------\nDockerfile:30\n--------------------\n  28 |     \n  29 |     # Install MLflow\n  30 | >>> RUN pip install mlflow==2.20.3\n  31 |     \n  32 |     # Install Java mlflow-scoring from Maven Central\n--------------------\nERROR: failed to solve: process \"/bin/sh -c pip install mlflow==2.20.3\" did not complete successfully: exit code: 1\nTraceback (most recent call last):\n  File \"/home/jcockrill/code/ml/models/file-financial-projections/.venv/bin/mlflow\", line 10, in <module>\n    sys.exit(cli())\n             ^^^^^\n  File \"/home/jcockrill/code/ml/models/file-financial-projections/.venv/lib/python3.11/site-packages/click/core.py\", line 1442, in __call__\n    return self.main(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jcockrill/code/ml/models/file-financial-projections/.venv/lib/python3.11/site-packages/click/core.py\", line 1363, in main\n    rv = self.invoke(ctx)\n         ^^^^^^^^^^^^^^^^\n  File \"/home/jcockrill/code/ml/models/file-financial-projections/.venv/lib/python3.11/site-packages/click/core.py\", line 1830, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jcockrill/code/ml/models/file-financial-projections/.venv/lib/python3.11/site-packages/click/core.py\", line 1830, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jcockrill/code/ml/models/file-financial-projections/.venv/lib/python3.11/site-packages/click/core.py\", line 1226, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jcockrill/code/ml/models/file-financial-projections/.venv/lib/python3.11/site-packages/click/core.py\", line 794, in invoke\n    return callback(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/jcockrill/code/ml/models/file-financial-projections/.venv/lib/python3.11/site-packages/mlflow/models/cli.py\", line 309, in build_docker\n    python_api.build_docker(**kwargs)\n  File \"/home/jcockrill/code/ml/models/file-financial-projections/.venv/lib/python3.11/site-packages/mlflow/models/python_api.py\", line 88, in build_docker\n    get_flavor_backend(model_uri, docker_build=True, env_manager=env_manager).build_image(\n  File \"/home/jcockrill/code/ml/models/file-financial-projections/.venv/lib/python3.11/site-packages/mlflow/pyfunc/backend.py\", line 388, in build_image\n    docker_utils.build_image_from_context(context_dir=cwd, image_name=image_name)\n  File \"/home/jcockrill/code/ml/models/file-financial-projections/.venv/lib/python3.11/site-packages/mlflow/models/docker_utils.py\", line 230, in build_image_from_context\n    raise RuntimeError(\"Docker build failed.\")\nRuntimeError: Docker build failed.\n```\n\n\n### Other info / logs\n\nNot relevant\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [x] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [x] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "jayshrivastava0",
        "body": "Hello @harmonic-jamie !\n\nI was able to look into this, and I can confirm the issue is due to the new `pandas==2.3.0` release. This version was just released yesterday and now requires Python 3.10+.\n\nThe key issue, as seen in the build logs, is that the `mlflow models build-docker` command uses a base image with **Python 3.9**, which is incompatible with the latest pandas.\n\nHowever, Excellent news: it looks like the MLflow maintainers have already addressed this in the development branch. **This commit by @harupy** avoids `pandas==2.3.0` and will resolve the issue in a future release: [https://github.com/mlflow/mlflow/commit/c4e5d9730bd78b7261755a6515c49ff1873a551b](https://github.com/mlflow/mlflow/commit/c4e5d9730bd78b7261755a6515c49ff1873a551b)\n\nFor anyone on MLflow `2.20.x` who needs an immediate fix, the best workaround is to **manually downgrade pandas** before building the Docker image. You can do this by running:\n\n```bash\npip install \"pandas<2.3.0\"\n```\n\nAfter doing that, the `mlflow models build-docker` command should complete successfully.\n\nHope this helps everyone who runs into this"
      },
      {
        "user": "harupy",
        "body": "pandas uploaded wheels for python 3.9. pandas 2.3.0 should be installable now in python 3.9\n\nhttps://github.com/pandas-dev/pandas/issues/61563#issuecomment-2946600916"
      },
      {
        "user": "harmonic-jamie",
        "body": "Thank you ðŸ‘ "
      }
    ]
  },
  {
    "issue_number": 16113,
    "title": "Update .github/workflows/advice.yml to trigger on ready-for-review event",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-06T06:37:25Z",
    "updated_at": "2025-06-06T07:09:50Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "Please update the workflow file `.github/workflows/advice.yml` so that it is triggered on the `ready-for-review` event. This will ensure the workflow runs when a pull request is marked as ready for review. No labels are needed for this issue.",
    "comments": []
  },
  {
    "issue_number": 14915,
    "title": "[FR] automatically update artifact view in UI",
    "author": "mazer-ai",
    "state": "open",
    "created_at": "2025-03-08T18:51:09Z",
    "updated_at": "2025-06-06T01:14:58Z",
    "labels": [
      "enhancement",
      "area/uiux"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nWe often use a `.txt` artifact as a log and append to it over the course of a run. If would be nice if the artifact display was able to refresh the view when the the artifact changes, like the plot windows to.\n\nIdeally it would be nice if this were automatic, but even a button to refresh would be better than the current setup -- if you refresh the entire page, the currently selected artifact becomes unselected and then you have to reselect it with the mouse to get the updated view.\n\n### Motivation\n\n> #### What is the use case for this feature?\n\nThis allows users to easily monitor text logging messages for long training runs.\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\nI think this is a feature that would be generally useful -- it's generally useful to be able to monitor text output from training runs in attition to plots etc..\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\nSee above\n\n> #### Why is it currently difficult to achieve this use case?\n\nI don't think there's any way to currently do this without modifying the UI/front end code.\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [x] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "joelrobin18",
        "body": "Can i work on this @harupy ? I was able to reproduce the issue"
      },
      {
        "user": "mazer-ai",
        "body": "@joelrobin18 Is the new \"Auto-update\" checkbox in response to this issue?\n\n<img width=\"265\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fb59d860-05bd-48a6-b494-a37e7aa32677\" />\n\nIf so, this does update the view, but would be nice if it scrolled to the end as well. For a text artifact (like a growing logfile), the updates are not visible, because the viewport into the artifact remains fixed and the new data appears out of view. I actually just realized after a month that it was really adding to the display, but I just never noticed the scrollbar changing until today ðŸ˜„ \n\nI just looked at your screen capture in the PR and for you it seems to scroll to the bottom automatically.. perhaps this is chrome issue or something.. what browser were you using?"
      }
    ]
  },
  {
    "issue_number": 15182,
    "title": "[BUG] windows-only non-deterministic behavior of `mlflow.Mlflowclient.search_runs()`",
    "author": "mazer-ai",
    "state": "open",
    "created_at": "2025-04-01T01:06:26Z",
    "updated_at": "2025-06-06T01:07:14Z",
    "labels": [
      "bug",
      "area/windows",
      "area/tracking"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.21.2\n- Tracking server: 2.21.2\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 11\n- **Python version**: 3.12.3\n- **yarn version, if running the dev UI**: N/A\n\n\n### Describe the problem\n\nUsing the client API, the `search_runs()` method sometimes returns `[]`, even though there are definitely runs present in the experiment. The behavior seems to be completely random -- running the exact same script multiple times leads to different, unpredictable behavior.\n\nThis happens when the client is a windows 11 machine and connecting to either a linux-based server over the network, or a locally running server on the same windows 11 machine using the loopback interface (127.0.0.1).\n\nExact same code (below) always runs properly for linux and osx clients running same version of python, mlflow etc.\n\n### Tracking information\n\n**Please note that the tracking and aritfact repo info in this report are not relevant to the script below that reproduces the bug since the script exclusively uses the client API to specify the server locations.**\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nMLflow version: 2.21.2\nTracking URI: file:///C:/Users/jvonb/Documents/Work/taut/GitHub/taut/notebooks/james/mlruns\nArtifact URI: mlflow-artifacts:/0/0e83795ae3914ff1a95930a0e60ec8f9/artifacts\nSystem information: Windows 10.0.26100\nPython version: 3.12.3\nMLflow version: 2.21.2\nMLflow module location: C:\\Users\\jvonb\\Documents\\Work\\taut\\GitHub\\taut\\venv\\Lib\\site-packages\\mlflow\\__init__.py\nTracking URI: file:///C:/Users/jvonb/Documents/Work/taut/GitHub/taut/notebooks/james/mlruns\nRegistry URI: file:///C:/Users/jvonb/Documents/Work/taut/GitHub/taut/notebooks/james/mlruns\nActive experiment ID: 0\nActive run ID: 0e83795ae3914ff1a95930a0e60ec8f9\nActive run artifact URI: mlflow-artifacts:/0/0e83795ae3914ff1a95930a0e60ec8f9/artifacts\nMLflow dependencies:\n  Flask: 3.0.3\n  Jinja2: 3.1.4\n  alembic: 1.13.3\n  docker: 7.1.0\n  fastapi: 0.115.12\n  graphene: 3.3\n  markdown: 3.7\n  matplotlib: 3.9.2\n  mlflow-skinny: 2.21.2\n  numpy: 1.26.4\n  pandas: 2.2.2\n  pyarrow: 17.0.0\n  scikit-learn: 1.5.1\n  scipy: 1.14.1\n  sqlalchemy: 2.0.35\n  uvicorn: 0.34.0\n  virtualenv: 20.26.4\n  waitress: 3.0.0\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nimport mlflow\n\nserver = 'INSERT-SERVER-NAME-HERE'\n# server = '127.0.0.1'\nexper = 'jamie-test'\n\nclient = mlflow.MlflowClient(f'http://{server}:5000')\nxids = [x.experiment_id for x in client.search_experiments() if x.name == exper]\nif len(xids) > 0:\n    xid = xids[0]\nelse:\n    # first time through, create some runs\n    client.create_experiment(exper)\n    print(f'creating experiment: {server}:{exper}')\n    xids = [x.experiment_id for x in client.search_experiments() if x.name == exper]\n    xid = xids[0]\n    client.create_run(xids[0], run_name='run1')\n    client.create_run(xids[0], run_name='run2')\n\nruns = client.search_runs(experiment_ids=[xid], order_by=['end_time']).to_list()\nrun_names = [r.data.tags['mlflow.runName'] for r in runs]\nassert len(run_names) > 0\nprint('ok')\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nNot relevant -- doesn't crash -- just doesn't return correct result.\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nHappens with multiple servers -- both linux and windows. No errors show up in logs, even with `--dev` option.\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [x] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "Hi, @mazer-ai. Thank you for the report. Is this happening only for `search_runs`? Or you don't see the runs created in the mlflow server UI either?"
      },
      {
        "user": "mazer-ai",
        "body": "Hi @TomeHirata - The runs are visible on UI when this happens. But timing is everything -- if you immediately retry the `search_runs()` call after it fails (returns empty), it **usually** succeeds and properly returns the list of two.\n\nIt's been really hard to pin this down.. and it's not sufficient to retry every time the call returns empty, since somethings it can return empty 2x, but 3rd try it succeeds (rarely). It appears to be completely random..\n\nUnfortunately, I don't have access to another windows 11 machine, so can't test it on another machine locally... Any of your devs running win 11?\n\nOh, I should say, I have tried this with 3 different servers -- two ubuntu systems and the windows box that's failing as a client. All three generate the failure.. No failures when the client is a linux or osx system."
      },
      {
        "user": "TomeHirata",
        "body": "Understood. Then, it seems this is a delay in the run creation. I wonder if this is caused by the cache response in Windows. Can you verify if the request reaches the tracking server when the empty response happens? You can enable the access log of the tracking server by `mlflow server --gunicorn-opts=\"--access-logfile -\"`. Please give us some time to reproduce this with Windows 11 as we usually use mac os."
      }
    ]
  },
  {
    "issue_number": 15997,
    "title": "[BUG] Models (Experimental) ignores config for prefix",
    "author": "itestyoy",
    "state": "closed",
    "created_at": "2025-06-01T07:34:51Z",
    "updated_at": "2025-06-05T18:49:42Z",
    "labels": [
      "bug",
      "area/uiux",
      "has-closing-pr"
    ],
    "body": "### MLflow version\n\n3.0.0rc2\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\n- **Python version**: 3.9\n\n\n### Describe the problem\n\nModels (Experimental) ignores configs for prefix  \n```\n   --serve-artifacts\n   --static-prefix /services/mlflow\n```\n\n<img width=\"538\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/40c5714f-006c-4f23-a350-fdca68be676a\" />\n\n### Steps to reproduce the bug\n\n1. Run Mlflow on custom prefix\n\n### Code to generate data required to reproduce the bug\n\n_No response_\n\n### Is the console panel in DevTools showing errors relevant to the bug?\n\n_No response_\n\n### Does the network panel in DevTools contain failed requests relevant to the bug?\n\n_No response_",
    "comments": [
      {
        "user": "harupy",
        "body": "@itestyoy Thanks for reporting this. Filed https://github.com/mlflow/mlflow/pull/16002."
      },
      {
        "user": "itestyoy",
        "body": "Hi! @harupy Thanks for this fix, and this fixes one error, but another one is still present.\nProbably it is somehow connected with the GraphQL API call without the path prefix. \n\nCould you pleas help with this one?\n\nWe use main brunch to test this fix\n\n<img width=\"781\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3b245899-935e-4fed-8839-7cc3a1988544\" />"
      },
      {
        "user": "harupy",
        "body": "@itestyoy can you check the request URL and the response?"
      }
    ]
  },
  {
    "issue_number": 16078,
    "title": "Enhance check-vcs-permalinks pre-commit hook configuration",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-05T04:38:09Z",
    "updated_at": "2025-06-05T16:11:23Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "## Issue Description\n\nThe `check-vcs-permalinks` pre-commit hook is currently configured in our `.pre-commit-config.yaml` file but only applies to Python files (`.py`). This hook is valuable for ensuring that VCS permalinks in our codebase are properly formatted and up-to-date.\n\n## Current Configuration\n```yaml\n- id: check-vcs-permalinks\n  files: \\.(py)$\n  require_serial: true\n```\n\n## Proposed Enhancement\n\nConsider expanding the scope of `check-vcs-permalinks` to include additional file types where VCS permalinks might be present, such as:\n- Markdown files (`.md`, `.mdx`)\n- reStructuredText files (`.rst`)\n- Documentation files\n- Configuration files\n\n## Benefits\n\n1. **Consistency**: Ensures all VCS permalinks across the codebase follow the same format\n2. **Maintenance**: Helps identify and fix outdated or broken permalinks\n3. **Documentation Quality**: Improves the reliability of links in documentation\n\n## Acceptance Criteria\n\n- [ ] Review current usage of VCS permalinks across different file types\n- [ ] Determine appropriate file patterns for the hook\n- [ ] Update `.pre-commit-config.yaml` with expanded file coverage\n- [ ] Test the updated configuration\n- [ ] Document any breaking changes or migration steps if needed",
    "comments": []
  },
  {
    "issue_number": 15359,
    "title": "[DOC-FIX] Link for 3.0.0rc0 is broken",
    "author": "lucamattiazzi",
    "state": "closed",
    "created_at": "2025-04-17T09:15:26Z",
    "updated_at": "2025-06-05T11:50:03Z",
    "labels": [
      "area/docs"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute a documentation fix at this time.\n\n### URL(s) with the issue\n\nThe link present in the description of v3.0.0rc0 (https://mlflow.org/docs/latest/mlflow-3/) leads to a page that returns an `access denied` message.\nThe same, by the way, happens for similar pages about the same version (e.g. https://mlflow.org/docs/latest/mlflow-3/deep-learning).\n\nI had to put No for \"Willingness to contribute\" since that's something over which I don't have any agency!\n\nThank you!\n\n### Description of proposal (what needs changing)\n\nEither update the release description or fix the website",
    "comments": [
      {
        "user": "harupy",
        "body": "@lucamattiazzi Thanks for reporting this. Yesterday, we deployed the mlflow 2.22.0.rc0. Looks like that broke the link. We'll fix this soon."
      },
      {
        "user": "harupy",
        "body": "@lucamattiazzi We've fixed the https://mlflow.org/docs/latest/mlflow-3 page :)"
      },
      {
        "user": "lucamattiazzi",
        "body": "thank you!"
      }
    ]
  },
  {
    "issue_number": 16087,
    "title": "Add custom lint rule to detect @pytest.mark.repeat decorator",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-05T07:33:58Z",
    "updated_at": "2025-06-05T09:26:32Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "## Problem\n\nWe use `@pytest.mark.repeat` to repeat tests and check whether they're flaky, but this decorator should not be committed to the repository as it's only meant for local testing.\n\n## Proposed Solution\n\nAdd a custom lint rule in the `dev/clint` linter to detect usage of `@pytest.mark.repeat` decorators in test files and flag them as errors.\n\n## Implementation Details\n\nThe rule should:\n1. Scan Python test files for `@pytest.mark.repeat` decorators\n2. Report an error when found, with a clear message explaining that this decorator should not be committed\n3. Be integrated into the existing clint linter framework alongside other custom rules\n\n## Example\n\n```python\n# This should be flagged by the linter:\n@pytest.mark.repeat(10)\ndef test_something():\n    pass\n```\n\n## Benefits\n\n- Prevents accidental commits of test repetition markers\n- Maintains clean test suite without debugging artifacts\n- Automated detection as part of the existing linting pipeline",
    "comments": []
  },
  {
    "issue_number": 16068,
    "title": "[FR] Improve plotting of multiple metrics",
    "author": "coldhearti",
    "state": "open",
    "created_at": "2025-06-04T12:09:25Z",
    "updated_at": "2025-06-05T06:47:05Z",
    "labels": [
      "enhancement",
      "area/uiux",
      "area/tracking",
      "area/server-infra"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nWhen plotting multiple metrics in the UI, the differentiation of the lines is done automatically with different line types. This can create extremely messy looking plots. I suggest that either the lines can be customized in the UI, or that color will be used instead of line type by default.\n\n![Image](https://github.com/user-attachments/assets/117e87d2-2f1a-4f06-b751-447bae971834)\n\n### Motivation\n\n> #### What is the use case for this feature?\n\nCreate better metric plots.\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\nIn order to create more meaningful and easy to interpret reports of experiments, this feature is vital.\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\nWhen I communicate results of training/evaluation to customers and other stakeholders, I may want to plot some metrics in a single plot to make comparison easier.\n\n> #### Why is it currently difficult to achieve this use case?\n\nThe messy plots make interpretation of the metrics difficult, and harder to interpret.\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [x] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [x] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "@coldhearti Thanks for reaching out. How can we reproduce this? Can you provide a script to log data that can reproduce this issue?"
      },
      {
        "user": "coldhearti",
        "body": "@harupy I don't have a snippet to provide right now, but the basic principle is:\n\n- log multiple metrics to mlflow using log_metric\n- create a new chart in the UI\n- add all of the logged metrics to the new chart"
      }
    ]
  },
  {
    "issue_number": 16027,
    "title": "[FR] Support for multimodal AI tracing",
    "author": "kimminw00",
    "state": "open",
    "created_at": "2025-06-03T02:08:30Z",
    "updated_at": "2025-06-05T05:19:35Z",
    "labels": [
      "enhancement",
      "area/tracking"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nExtend MLflow's tracing and logging capabilities to support multi-modal AI workloads, including image, audio, and video data. This would involve the ability to log, visualize, and compare inputs and outputs of various modalities directly within MLflow runs.\n\n### Motivation\n\n**What is the use case for this feature?**\nMany modern Gen AI applications are multi-modal, involving models that process images, audio, or videoâ€”either independently or jointly. Common examples include:\n\n* Image classification, segmentation, or generation (e.g., using CLIP, DALL-E)\n* Audio transcription, classification, or synthesis (e.g., Whisper, TTS)\n* Video understanding or captioning (e.g., action recognition, video-to-text)\n* Multi-modal tasks such as VQA (Visual Question Answering) and image captioning\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "Appreciate the FR! Itâ€™s on our roadmap. Stay tuned :)"
      }
    ]
  },
  {
    "issue_number": 16052,
    "title": "Fix type in log_feedback",
    "author": "B-Step62",
    "state": "closed",
    "created_at": "2025-06-04T03:32:05Z",
    "updated_at": "2025-06-05T00:36:44Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "The type hint should be union of exception object and AssessmentError, not Expectation\n\nhttps://github.com/mlflow/mlflow/blob/master/mlflow/tracing/assessment.py#L256",
    "comments": []
  },
  {
    "issue_number": 9411,
    "title": "[BUG] Default evaluator binary classifier metrics SQLAlchemy type conflict",
    "author": "jasonharris438",
    "state": "closed",
    "created_at": "2023-08-22T13:22:16Z",
    "updated_at": "2025-06-04T23:25:00Z",
    "labels": [
      "bug",
      "area/artifacts",
      "area/models",
      "area/scoring",
      "area/docker",
      "area/sqlalchemy",
      "has-closing-pr"
    ],
    "body": "### Issues Policy acknowledgement\r\n\r\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\r\n\r\n### Willingness to contribute\r\n\r\nYes. I can contribute a fix for this bug independently.\r\n\r\n### MLflow version\r\n\r\n- Client: 2.6.0\r\n- Tracking server: 2.6.0\r\n\r\n\r\n### System information\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 10 (buster)\r\n- **Python version**: 3.10.9\r\n- **yarn version, if running the dev UI**:\r\n\r\n\r\n### Describe the problem\r\n\r\nI can see that when calling `mlflow.evaluate` for a binary model, there is a database insertion error due to `numpy.int64` values being added to the SQLAlchemy session. This is due to the `log_batch` function in `SQLAlchemyStore` storing model run metrics in the `SQLMetric` type that assigns `value` to a `sa.types.Float`. This fails when the values that make up the precision/recall curve (fp, tp, fn, tn) are inserted.\r\n\r\nI have only encountered this when using binary classifiers. Multiclass classifiers work fine. This makes sense as per your docs for `evaluate()`. You could cast integer values to a floating point just ahead of insertion to guarantee that this error doesn't get thrown as your `evaluate` function calls the `scikit-learn` evaluation routine to calculate confusion matrix scores. [Numeric](https://docs.sqlalchemy.org/en/20/core/type_basics.html#sqlalchemy.types.Numeric) is for non-integer types only.\r\n\r\n### Tracking information\r\n\r\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n\r\n\r\n### Code to reproduce issue\r\n\r\nThis is specific to my use case and not to what is produced by `load_iris`, however I will try to provide you context of how I am using `evaluate()`. It is very standard. I simply fit a binary XGBClassifier and evaluate the model once it is trained as per:\r\n```\r\nfrom xgboost import XGBClassifier\r\nfrom pandas import concat, DataFrame, Series\r\nfrom mlflow import start_run\r\n\r\nmodel = XGBClassifier()\r\n# ... Run mlflow training run with start_run, etc, etc.\r\n# Input features are simply floating point values as shown below.\r\n# Targets are binary integer values.\r\n\r\nevaluate(\r\n    model=\"/usr/src/app/mlruns/f2782bd7e59047dcbe4e3cc151ef79ee/artifacts/model\",\r\n    data=concat(\r\n        objs=[\r\n            DataFrame(\r\n                [\r\n                    [62.4566, 77.7149, 70.5662],\r\n                    [60.9133, 89.7368, 75.2636],\r\n                    [63.254, 92.8988, 79.2051],\r\n                    [61.9181, 82.0553, 74.6155],\r\n                    [60.8766, 73.754, 71.0428],\r\n                ]\r\n            ),\r\n            Series(\r\n                [1, 0, 0, 0, 1],\r\n                index=[\"f1\", \"f2\", \"f3\"],\r\n                name=\"targets\",\r\n            ),\r\n        ],\r\n        axis=1,\r\n    ),\r\n    targets=\"targets\",\r\n    model_type=\"classifier\",\r\n    evaluators=[\"default\"],\r\n)\r\n```\r\n\r\n\r\n### Stack trace\r\n\r\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 2108, in _exec_insertmany_context\r\n    dialect.do_execute(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py\", line 921, in do_execute\r\n    cursor.execute(statement, parameters)\r\npsycopg2.ProgrammingError: can't adapt type 'numpy.int64'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/src/app/dependencies/mlflow/store/db/utils.py\", line 143, in make_managed_session\r\n    yield session\r\n  File \"/usr/src/app/dependencies/mlflow/store/tracking/sqlalchemy_store.py\", line 834, in _log_metrics\r\n    _insert_metrics(metric_instances)\r\n  File \"/usr/src/app/dependencies/mlflow/store/tracking/sqlalchemy_store.py\", line 828, in _insert_metrics\r\n    self._update_latest_metrics_if_necessary(\r\n  File \"/usr/src/app/dependencies/mlflow/store/tracking/sqlalchemy_store.py\", line 922, in _update_latest_metrics_if_necessary\r\n    .all()\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/query.py\", line 2688, in all\r\n    return self._iter().all()  # type: ignore\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/query.py\", line 2842, in _iter\r\n    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py\", line 2262, in execute\r\n    return self._execute_internal(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py\", line 2123, in _execute_internal\r\n    ) = compile_state_cls.orm_pre_session_exec(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/context.py\", line 551, in orm_pre_session_exec\r\n    session._autoflush()\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py\", line 2939, in _autoflush\r\n    raise e.with_traceback(sys.exc_info()[2])\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py\", line 2928, in _autoflush\r\n    self.flush()\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py\", line 4179, in flush\r\n    self._flush(objects)\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py\", line 4314, in _flush\r\n    with util.safe_reraise():\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py\", line 147, in __exit__\r\n    raise exc_value.with_traceback(exc_tb)\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/session.py\", line 4275, in _flush\r\n    flush_context.execute()\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py\", line 466, in execute\r\n    rec.execute(self)\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py\", line 642, in execute\r\n    util.preloaded.orm_persistence.save_obj(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py\", line 93, in save_obj\r\n    _emit_insert_statements(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py\", line 1043, in _emit_insert_statements\r\n    result = connection.execute(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1412, in execute\r\n    return meth(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/sql/elements.py\", line 515, in _execute_on_connection\r\n    return connection._execute_clauseelement(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1635, in _execute_clauseelement\r\n    ret = self._execute_context(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1839, in _execute_context\r\n    return self._exec_insertmany_context(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 2116, in _exec_insertmany_context\r\n    self._handle_dbapi_exception(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 2339, in _handle_dbapi_exception\r\n    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 2108, in _exec_insertmany_context\r\n    dialect.do_execute(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py\", line 921, in do_execute\r\n    cursor.execute(statement, parameters)\r\nsqlalchemy.exc.ProgrammingError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\r\n(psycopg2.ProgrammingError) can't adapt type 'numpy.int64'\r\n[SQL: INSERT INTO metrics (key, value, timestamp, step, is_nan, run_uuid) VALUES (%(key__0)s, %(value__0)s, %(timestamp__0)s, %(step__0)s, %(is_nan__0)s, %(run_uuid__0)s), (%(key__1)s, %(value__1)s, %(timestamp__1)s, %(step__1)s, %(is_nan__1)s, %(run_uuid_ ... 542 characters truncated ... uid__7)s), (%(key__8)s, %(value__8)s, %(timestamp__8)s, %(step__8)s, %(is_nan__8)s, %(run_uuid__8)s)]\r\n[parameters: {'run_uuid__0': '58c077b2ae614e649c23b6dcfcafafb8', 'step__0': 0, 'value__0': 29, 'timestamp__0': 1692708076077, 'key__0': 'true_negatives', 'is_nan__0': False, 'run_uuid__1': '58c077b2ae614e649c23b6dcfcafafb8', 'step__1': 0, 'value__1': 8, 'timestamp__1': 1692708076077, 'key__1': 'false_positives', 'is_nan__1': False, 'run_uuid__2': '58c077b2ae614e649c23b6dcfcafafb8', 'step__2': 0, 'value__2': 5, 'timestamp__2': 1692708076077, 'key__2': 'false_negatives', 'is_nan__2': False, 'run_uuid__3': '58c077b2ae614e649c23b6dcfcafafb8', 'step__3': 0, 'value__3': 1, 'timestamp__3': 1692708076077, 'key__3': 'true_positives', 'is_nan__3': False, 'run_uuid__4': '58c077b2ae614e649c23b6dcfcafafb8', 'step__4': 0, 'value__4': 43, 'timestamp__4': 1692708076077, 'key__4': 'example_count', 'is_nan__4': False, 'run_uuid__5': '58c077b2ae614e649c23b6dcfcafafb8', 'step__5': 0, 'value__5': 0.6976744186046512, 'timestamp__5': 1692708076077, 'key__5': 'accuracy_score', 'is_nan__5': False, 'run_uuid__6': '58c077b2ae614e649c23b6dcfcafafb8', 'step__6': 0, 'value__6': 0.16666666666666666, 'timestamp__6': 1692708076077, 'key__6': 'recall_score', 'is_nan__6': False, 'run_uuid__7': '58c077b2ae614e649c23b6dcfcafafb8', 'step__7': 0, 'value__7': 0.1111111111111111, 'timestamp__7': 1692708076077, 'key__7': 'precision_score', 'is_nan__7': False, 'run_uuid__8': '58c077b2ae614e649c23b6dcfcafafb8', 'step__8': 0, 'value__8': 0.13333333333333333, 'timestamp__8': 1692708076077, 'key__8': 'f1_score', 'is_nan__8': False}]\r\n(Background on this error at: https://sqlalche.me/e/20/f405)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/src/app/src/api/routes/training.py\", line 59, in train_models\r\n    model_evaluation(\r\n  File \"/usr/src/app/src/api/pipeline/training.py\", line 392, in model_evaluation\r\n    mlflow_run(\r\n  File \"/usr/src/app/src/api/pipeline/training.py\", line 271, in mlflow_run\r\n    model = evaluate_model(\r\n  File \"/usr/src/app/src/api/pipeline/training.py\", line 144, in evaluate_model\r\n    evaluate(\r\n  File \"/usr/src/app/dependencies/mlflow/models/evaluation/base.py\", line 1563, in evaluate\r\n    evaluate_result = _evaluate(\r\n  File \"/usr/src/app/dependencies/mlflow/models/evaluation/base.py\", line 1000, in _evaluate\r\n    eval_result = evaluator.evaluate(\r\n  File \"/usr/src/app/dependencies/mlflow/models/evaluation/default_evaluator.py\", line 1396, in evaluate\r\n    evaluation_result = self._evaluate(model, is_baseline_model=False)\r\n  File \"/usr/src/app/dependencies/mlflow/models/evaluation/default_evaluator.py\", line 1355, in _evaluate\r\n    self._log_metrics()\r\n  File \"/usr/src/app/dependencies/mlflow/models/evaluation/default_evaluator.py\", line 557, in _log_metrics\r\n    self.client.log_batch(\r\n  File \"/usr/src/app/dependencies/mlflow/tracking/client.py\", line 1037, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"/usr/src/app/dependencies/mlflow/tracking/_tracking_service/client.py\", line 396, in log_batch\r\n    self.store.log_batch(run_id=run_id, metrics=metrics_batch, params=[], tags=[])\r\n  File \"/usr/src/app/dependencies/mlflow/store/tracking/sqlalchemy_store.py\", line 1492, in log_batch\r\n    raise e\r\n  File \"/usr/src/app/dependencies/mlflow/store/tracking/sqlalchemy_store.py\", line 1489, in log_batch\r\n    self._log_metrics(run_id, metrics)\r\n  File \"/usr/src/app/dependencies/mlflow/store/tracking/sqlalchemy_store.py\", line 822, in _log_metrics\r\n    with self.ManagedSessionMaker() as session:\r\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/usr/src/app/dependencies/mlflow/store/db/utils.py\", line 157, in make_managed_session\r\n    raise MlflowException(message=e, error_code=BAD_REQUEST)\r\nmlflow.exceptions.MlflowException: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\r\n(psycopg2.ProgrammingError) can't adapt type 'numpy.int64'\r\n[SQL: INSERT INTO metrics (key, value, timestamp, step, is_nan, run_uuid) VALUES (%(key__0)s, %(value__0)s, %(timestamp__0)s, %(step__0)s, %(is_nan__0)s, %(run_uuid__0)s), (%(key__1)s, %(value__1)s, %(timestamp__1)s, %(step__1)s, %(is_nan__1)s, %(run_uuid_ ... 542 characters truncated ... uid__7)s), (%(key__8)s, %(value__8)s, %(timestamp__8)s, %(step__8)s, %(is_nan__8)s, %(run_uuid__8)s)]\r\n[parameters: {'run_uuid__0': '58c077b2ae614e649c23b6dcfcafafb8', 'step__0': 0, 'value__0': 29, 'timestamp__0': 1692708076077, 'key__0': 'true_negatives', 'is_nan__0': False, 'run_uuid__1': '58c077b2ae614e649c23b6dcfcafafb8', 'step__1': 0, 'value__1': 8, 'timestamp__1': 1692708076077, 'key__1': 'false_positives', 'is_nan__1': False, 'run_uuid__2': '58c077b2ae614e649c23b6dcfcafafb8', 'step__2': 0, 'value__2': 5, 'timestamp__2': 1692708076077, 'key__2': 'false_negatives', 'is_nan__2': False, 'run_uuid__3': '58c077b2ae614e649c23b6dcfcafafb8', 'step__3': 0, 'value__3': 1, 'timestamp__3': 1692708076077, 'key__3': 'true_positives', 'is_nan__3': False, 'run_uuid__4': '58c077b2ae614e649c23b6dcfcafafb8', 'step__4': 0, 'value__4': 43, 'timestamp__4': 1692708076077, 'key__4': 'example_count', 'is_nan__4': False, 'run_uuid__5': '58c077b2ae614e649c23b6dcfcafafb8', 'step__5': 0, 'value__5': 0.6976744186046512, 'timestamp__5': 1692708076077, 'key__5': 'accuracy_score', 'is_nan__5': False, 'run_uuid__6': '58c077b2ae614e649c23b6dcfcafafb8', 'step__6': 0, 'value__6': 0.16666666666666666, 'timestamp__6': 1692708076077, 'key__6': 'recall_score', 'is_nan__6': False, 'run_uuid__7': '58c077b2ae614e649c23b6dcfcafafb8', 'step__7': 0, 'value__7': 0.1111111111111111, 'timestamp__7': 1692708076077, 'key__7': 'precision_score', 'is_nan__7': False, 'run_uuid__8': '58c077b2ae614e649c23b6dcfcafafb8', 'step__8': 0, 'value__8': 0.13333333333333333, 'timestamp__8': 1692708076077, 'key__8': 'f1_score', 'is_nan__8': False}]\r\n(Background on this error at: https://sqlalche.me/e/20/f405)\r\n```\r\n\r\n\r\n### Other info / logs\r\nWas using `sqlalchemy==2.0.20`, but this issue will persist (with a slightly different trace) with older versions. I originally found on `.1.4.46`.\r\n\r\n### What component(s) does this bug affect?\r\n\r\n- [X] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/gateway`: AI Gateway service, Gateway client APIs, third-party Gateway integrations\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [X] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [X] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n### What interface(s) does this bug affect?\r\n\r\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [X] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [X] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\n### What language(s) does this bug affect?\r\n\r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\n### What integration(s) does this bug affect?\r\n\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "BenWilson2",
        "body": "Hi @jasonharris438 thanks for reporting! This is the PostgreSQL incompatibility with numpy types issue, right? \r\nWe appreciate you volunteering for a fix! Please tag this Issue with your filed PR when you're ready to submit!"
      },
      {
        "user": "jasonharris438",
        "body": "@BenWilson2 thanks, I've raised a very simple PR. I'll check back later to see which tests have failed/passed. Let me know whether you have some conventions that you need me to follow. I tried to keep this as contained as possible."
      },
      {
        "user": "mlflow-automation",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      }
    ]
  },
  {
    "issue_number": 12576,
    "title": "[FR] Raise an exception if `mlflow.get_artifact_uri()` used outside of an active run",
    "author": "mohammedsalah-ai",
    "state": "closed",
    "created_at": "2024-07-04T10:55:39Z",
    "updated_at": "2025-06-04T18:31:32Z",
    "labels": [
      "enhancement",
      "good first issue",
      "help wanted",
      "area/artifacts",
      "area/model-registry",
      "area/models",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nIt's just to avoid new users having the problem that I went through which is detailed in [issue #12564](https://github.com/mlflow/mlflow/issues/12564)\n\n### Motivation\n\n> #### What is the use case for this feature?\r\n\r\njust to avoid confusion\r\n\r\n> #### Why is this use case valuable to support for MLflow users in general?\r\n\r\nensuring control over mlflow so it won't create runs unexpectedly\r\n\n\n### Details\n\nagain all details to understand the issue it outlined here [issue #12564](https://github.com/mlflow/mlflow/issues/12564)\n\n### What component(s) does this bug affect?\n\n- [X] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [X] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [X] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "karoel2",
        "body": "Hi,\r\nI'm here because of the \"good first issue\" tag. After reviewing the code for a while, I believe the proposed solution of raising an exception will not work.\r\n\r\nWhen using `mlflow.get_artifact_uri` with `artifact_path` as argument (which is the case in mentioned issue https://github.com/mlflow/mlflow/issues/12564 ) , the intended behavior of this function is to create a new run if there is no active run:\r\n\r\n` If no run is active, this method will create a new active run.`\r\nReference: [mlflow/tracking/fluent.py#L1794-L1847](https://github.com/mlflow/mlflow/blob/2bbf9e6d3826a7ea153477b609cfa2a8b513a3a7/mlflow/tracking/fluent.py#L1794-L1847)\r\n\r\nTo solve this bug, either the intended behavior of this function needs to be changed, or it needs to be debugged to determine why it is not working when creating a new run."
      },
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      },
      {
        "user": "mohammedsalah-ai",
        "body": "If that was the intended procedure. What about adding an additional argument to `.get_artifact_uri()` maybe something like \"complain\", \"panic\", or whatever to control this behavior? Like by default. It'll complain and raise an error if called outside a run, when set to false it'll just carry on with default behavior.\r\n\r\nso for example,\r\n```python\r\nwith mlflow.start_run(..):\r\n    # a block of code.\r\n\r\nmlflow.get_artiface_uri(...) # error because by default complain is True.\r\nmlflow.get_artifact_uri(..., complain=False) # here we can get the normal behavior.\r\n```\r\n\r\nI think it's the best for it to be that way as most of the time I think this method is used within runs as I'm the first to use it outside to create a uri for a non-existing artifact ðŸ˜‡"
      }
    ]
  },
  {
    "issue_number": 5894,
    "title": "[BUG] Nested runs not displayed properly",
    "author": "Nika-St",
    "state": "closed",
    "created_at": "2022-05-18T11:35:45Z",
    "updated_at": "2025-06-04T12:52:02Z",
    "labels": [
      "bug"
    ],
    "body": "### Willingness to contribute\r\n\r\nNo. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **MLflow installed from (source or binary)**: binary\r\n- **MLflow version (run ``mlflow --version``)**: 1.22.0\r\n- **Python version**: 3.9.10\r\n- **yarn version, if running the dev UI**:\r\n\r\n### Describe the problem\r\n\r\n![2022-05-18](https://user-images.githubusercontent.com/29167311/169028297-278c82d1-4144-4124-94c5-43d56f877ec0.png)\r\n\r\nInstead of being properly grouped under the parent run, nested runs are scrambled (apparently, sorted by date). The expectation is that standalone and parent runs would be sorted by date but _nested_ runs would be shown _nested_ under parent ones.\r\n\r\n### Tracking information\r\n\r\n_No response_\r\n\r\n### Code to reproduce issue\r\n\r\n```python\r\nwith mlflow.start_run(run_name='some_name') as parent_run:\r\n    for x in range(20):\r\n        with mlflow.start_run(run_name='some_other_name' + x, nested=True) as child_run:\r\n            mlflow.log_param('some_param', x)\r\n```\r\n\r\n### Other info / logs\r\n\r\n_No response_\r\n\r\n### What component(s) does this bug affect?\r\n\r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n### What interface(s) does this bug affect?\r\n\r\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\n### What language(s) does this bug affect?\r\n\r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\n### What integration(s) does this bug affect?\r\n\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "adamreeve",
        "body": "Hi @Nika-St, you're using quite an old version of mlflow. Can you reproduce this with 1.26.0? 1.26.0 included some changes I made to the runs grid, so this issue caught my eye, but I think nested runs were already working correctly before 1.26.0."
      },
      {
        "user": "Nika-St",
        "body": "Thanks @adamreeve ! I requested an update from the admin."
      },
      {
        "user": "dbczumar",
        "body": "As indicated by @adamreeve , this has been resolved in recent versions of MLflow. Thank you for using MLflow!"
      }
    ]
  },
  {
    "issue_number": 2585,
    "title": "[FR] Why can't the backend store uri be an s3 location?",
    "author": "ksanjeevan",
    "state": "closed",
    "created_at": "2020-03-15T17:57:20Z",
    "updated_at": "2025-06-04T11:34:49Z",
    "labels": [
      "enhancement"
    ],
    "body": "## Describe the proposal\r\nAllow for `backend-store-uri` to be an s3 location. That way parameters/metrics are in the same location as the artifacts (same as when working in local mode).\r\n\r\n### Motivation\r\nMakes sense to have the params/metrics in a SQL db to be able to query and such, but since we can also interact with them through the `mlflow ui`, it would be more convenient to allow it all to point to a single s3 location and have `mlflow server` run only against it.\r\n\r\n### Proposed Changes\r\nAllow for `backend-store-uri` to be an s3 location, essentially having the same as local mode but instead of a single local file path have it be s3.",
    "comments": [
      {
        "user": "tomasatdatabricks",
        "body": "Hi @ksanjeevan , this is an interesting idea, thanks for posting! Unfortunately, this would require a new type of tracking store backend and we are not planning to add a new type of store at this time nor can we support it at this time.\r\n\r\n"
      },
      {
        "user": "oren-hecht",
        "body": "Any chance this can considered again as an option?\n\nMost people have an S3 bucket already as they need it for artifact storage. Why make them handle additional DB infrastructure, if there isn't a very important reason?\n\nI'm sure this can be a really useful feature for many!\n(also, this was marked closed as `completed` but it should have been marked as `not-planned`, no?)"
      }
    ]
  },
  {
    "issue_number": 15831,
    "title": "[FR] Support local/s3 model paths in copy_model_version command",
    "author": "sniafas",
    "state": "open",
    "created_at": "2025-05-21T22:04:28Z",
    "updated_at": "2025-06-04T08:52:14Z",
    "labels": [
      "enhancement",
      "area/artifacts",
      "area/model-registry",
      "area/tracking"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\n\n### Proposal Summary\n\n[`Copy_model_version`](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.client.html#mlflow.client.MlflowClient) command can copy models only as entities, but real model source (local or s3) is not maintained since only `models:/` scheme is the allowed format.\nAt the end of the day, the new copied model has limited effectiveness.\n\n```\n<RegisteredModel: ... name='src_model', run_id='f25e78a09fb941c5bdb9af7846f064b6', source='/app/mlruns/1/f25e78a09fb941c5bdb9af7846f064b6/artifacts/model', version='1'>], name='src_model', tags={}>\n\n<RegisteredModel: ... name='target_model', run_id='f25e78a09fb941c5bdb9af7846f064b6', source='models:/src_model/1', version='1'>], name='target_model', tags={}>\n```\n\n[source](https://mlflow.org/docs/latest/api_reference/_modules/mlflow/tracking/client.html#MlflowClient.copy_model_version)\n```\nif urllib.parse.urlparse(src_model_uri).scheme != \"models\":\n  raise MlflowException(\n     f\"Unsupported source model URI: '{src_model_uri}'. The `copy_model_version` API \"\n      \"only copies models stored in the 'models:/' scheme.\"\n   )\n```\n\n### Motivation\n\n> #### What is the use case for this feature?\n\n- The current function's behavior, only copies the model in model registry and creates a new model source path. It expects only \"models:/\" scheme which is an artificial uri, comparing to real model's source uri (local/s3)\n\n- The expected `copy_model_version` behavior would be to copy the referenced model's source uri -> to the dst model's source.\n\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\nBy design `copy_model_version` function does not have the expected functionality and will help MLflow user to actual copy and reuse the copied model.\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\nAn additional logic is needed to maintain the source model version for the target model, which cannot scale when multiple copies are existing.\n\nRefModelA -> DstModelA\nRefModelA -> DstModelB\nRefModelB -> DstModelC\n...\n\n> #### Why is it currently difficult to achieve this use case?\n\nTarget's model source path is not the real path of the copied model\n\n\n### Details\n\nPossible additions in\n\n`store/model_registry/abstract_store.py`\n```\ntry:\n    mv_copy = self.create_model_version(\n        name=dst_name,\n        source=f\"models:/{src_mv.name}/{src_mv.version}\",\n        source=src_mv.name,\n        version=src_mv.version,\n        run_id=src_mv.run_id,\n        tags=[ModelVersionTag(k, v) for k, v in src_mv.tags.items()],\n        run_link=src_mv.run_link,\n```\n\n`server/handlers.py`\n```\n model_version = _get_model_registry_store().create_model_version(\n        name=request_message.name,\n        source=request_message.source,\n        version=request_message.version,\n        run_id=request_message.run_id,\n        run_link=request_message.run_link,\n        tags=request_message.tags,\n```\n\n`store/_unity_catalog/registry/rest_store.py`\n```\nCreateModelVersionRequest(\n    name=full_name,\n    source=source,\n    version=version,\n    run_id=run_id,\n    description=description,\n    tags=uc_model_version_tag_from_mlflow_tags(tags),\n```\n\n`store/model_registry/rest_store`\n```\nCreateModelVersion(\n    name=name,\n    source=source,\n    version=version,\n    run_id=run_id,\n    run_link=run_link,\n    tags=proto_tags,\n```\n\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [x] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "This proposal makes sense, we can update code to:\n* Copy the real model source in [Copy_model_version](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.client.html#mlflow.client.MlflowClient)\n* We can support other model URI format like `runs:/<run-id>/model`"
      },
      {
        "user": "WeichenXu123",
        "body": "One question:\n\nWhat if the source model's source is also a string like 'models:/...' ?\n\nDo you want to trace the real model source recursively ? \n\n"
      },
      {
        "user": "sniafas",
        "body": "Thanks @WeichenXu123, I agree on the first part. On the second, good question. Lets take it a step back. Supposedly the model source is \"models:/\", then imho this can only be effective with _mlflow model serve_ command.\nI suggest that saved model uri should be always a traceable real path, local or remote (s3 etc). "
      }
    ]
  },
  {
    "issue_number": 15855,
    "title": "Add `databricks-agents>=1.0` as dependency of `mlflow[databricks]` extra",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-23T06:58:55Z",
    "updated_at": "2025-06-04T04:08:46Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "`dev/pyproject.py` is a script to generate `pyproject.toml`. Let's update this file to include `databricks-agents>=1.0` in the `databricks` extra. Once updated, run it to update `pyproject.toml` files.",
    "comments": []
  },
  {
    "issue_number": 16038,
    "title": "Improve `dev/update_changelog.py` performance by batch-fetching PRs with GraphQL API",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-03T08:37:52Z",
    "updated_at": "2025-06-03T14:41:55Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "Currently, `dev/update_changelog.py` fetches PRs one by one, which is slow. Instead, we can batch-fetch PRs with GraphQL API.",
    "comments": []
  },
  {
    "issue_number": 15814,
    "title": "[BUG] search_runs APIs: Confusion between mlflow.search_runs and MlflowClient().search_runs, especially client's silent 1000-run limit, can mislead users",
    "author": "JLC827",
    "state": "open",
    "created_at": "2025-05-20T13:05:55Z",
    "updated_at": "2025-06-03T14:06:46Z",
    "labels": [
      "bug",
      "area/tracking"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.14.3\n- Tracking server: 2.14.2\n\n### System information\n\nMicrosoft Windows 11 Education 10.0.22631 Build 22631\nPython 3.10.0\n\n### Describe the problem\n\n\nMLflow offers two primary methods for searching runs: the fluentÂ mlflow.search_runs()Â and the client APIÂ MlflowClient().search_runs(). Their similar names, coupled with a critical difference in default behavior regarding result limitsâ€”specifically,Â MlflowClient().search_runs()Â silently defaulting to 1000 results for its direct callsâ€”creates a confusing situation where users can easily and unknowingly receive truncated datasets.\n\n**Core Issues Leading to Confusion and Unexpected Outcomes:**\n\n1. **MlflowClient().search_runs()Â Silently Limits to 1000 Results by Default:**\n    \n    - MlflowClient().search_runs()Â has aÂ max_resultsÂ parameter that defaults toÂ SEARCH_MAX_RESULTS_DEFAULTÂ (1000).\n        \n    - When a user calls this method directly without overriding this default, and more than 1000 runs match the query,Â **only the first 1000 runs are returned in the initialÂ PagedListÂ object.**\n        \n    - There isÂ **no explicit warning or immediate, obvious indication**Â that the returned list is partial. Users not specifically looking for theÂ page_tokenÂ can easily assume they have received all matching runs.\n        \n2. **Confusion Arising from Two Similarly Named APIs with Different Default Maxima & Intentions:**\n    \n    - The fluentÂ mlflow.search_runs() works by callingÂ multiple MlflowClient().search_runs() internally. mlflow.search_runs() has a default limit of 100,000 runs.\n    \n    - This difference in default behavior and intended scope (single page vs. many pages) for methods with such similar names is confusing.\n        \n\n**Impact:**  \nThis combination leads to:\n\n- Users unknowingly working with incomplete datasets when directly usingÂ MlflowClient().search_runs()Â with its default settings.\n\n\n**Suggested Improvements:**\n\n1. **Safer Default forÂ MlflowClient().search_runs()Â through Conditional Warning:**\n    \n    - WhenÂ MlflowClient().search_runs()Â is called directly by a user (i.e., not as part of an internal pagination loop byÂ mlflow.search_runs()):\n        \n        - If itsÂ max_resultsÂ parameter is at its default value (1000), AND\n            \n        - The number of runs returned equals this default, AND\n            \n        - AÂ page_tokenÂ is present (indicating more results are available),\n            \n        - Then, a clearÂ UserWarningÂ should be logged. This warning would inform the user that results are limited by the defaultÂ max_results, that more runs are available, and that they should handle pagination or adjustÂ max_resultsÂ for their direct calls.\n            \n    - To prevent this warning from flooding logs whenÂ mlflow.search_runs()Â uses the client API internally for correct pagination,Â MlflowClient().search_runs()Â could accept an internal flag (e.g.,Â _internal_call=TrueÂ orÂ suppress_default_limit_warning=True) passed byÂ mlflow.search_runs()Â to suppress this specific warning.\n\n**Longer-Term Considerations (for API Design Discussion):**\n\n- To further reduce ambiguity, consider ifÂ MlflowClient().search_runs()Â should be renamed (e.g., toÂ search_runs_page()) or marked with a leading underscore (e.g.,Â _search_runs()) in a future major version to more clearly indicate its primary role as a lower-level, page-oriented function, distinct from the comprehensive fluentÂ mlflow.search_runs(). This would be a breaking change but could improve API intuitiveness.\n\n### Tracking information\n\n_No response_\n\n### Code to reproduce issue\n\n``` python\nfrom mlflow import MlflowClient\n\nclient = MlflowClient()\n# Assume >1000 runs exist for experiment '0'\nruns_page = client.search_runs(experiment_ids=[\"0\"])\n\n# len(runs_page) will be 1000, even if 5000 total runs match.\n# No warning is issued. A user might unknowingly proceed with incomplete data.\nprint(f\"Number of runs fetched by client: {len(runs_page)}\")\n```\n\n### Stack trace\n\nn/a\n\n### Other info / logs\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "We have clear documentation in the Python API about the default value of `max_results` as well as the docstring: https://github.com/mlflow/mlflow/blob/90dc80c029be2756b5eda6648db394dfc5909ed9/mlflow/tracking/client.py#L3402\nhttps://mlflow.org/docs/latest/api_reference/python_api/mlflow.client.html#mlflow.client.MlflowClient.search_runs\nPlease read the API doc for the usage, if there're any confusion on the docs then we'll improve them.\nThe fluent API is not designed to be exactly the same as the client API, you could choose either one based on your use case."
      },
      {
        "user": "JLC827",
        "body": "Hi @serena-ruan,\n\nThanks for the response and for pointing to the documentation. I do see that the max_results default is documented.\n\nMy main concern is the silent nature of this default when using MlflowClient().search_runs() directly. This, coupled with the similar naming to the fluent mlflow.search_runs() can create confusion.\n\nThis touches on a broader point; I find the distinction between the fluent API and the MlflowClient, and their distinct use cases, to be somewhat confusing. This might just be a me problem, however.\n\nI just wanted to highlight an area of potential developer experience friction. I don't intend to pursue this further, so please feel free to close this issue if you believe the current documentation and behavior are sufficient.\nThanks!"
      }
    ]
  },
  {
    "issue_number": 15825,
    "title": "[BUG] \"Create prompt\" button freezes UI when no prompts have been logged",
    "author": "marmal88",
    "state": "open",
    "created_at": "2025-05-21T09:40:01Z",
    "updated_at": "2025-06-03T04:34:25Z",
    "labels": [
      "bug",
      "area/uiux"
    ],
    "body": "### MLflow version\n\n2.22.0\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 22.04\n- **Python version**: 3.10.16\n\n\n### Describe the problem\n\nIn the UI under \"Prompts\" clicking on the \"create prompt\" button freezes the webpage, when there are no prompts logged. This keeps happening until new prompts are logged (in my case using the python SDK).\n\nIn the case when there are no prompts the timeout error returns (previously created prompts deleted till nothing is left). Unsure if this is due schema not yet being created in the DB.\n\nError showing timeout\n![Image](https://github.com/user-attachments/assets/1028b767-761a-4d65-b486-3e6710ccf0f7)\n\nUI works when there are prompts already logged\n![Image](https://github.com/user-attachments/assets/a3c1a574-c116-415f-88c4-4e806761e304)\n\n### Steps to reproduce the bug\n\n1.  pull image ghcr.io/mlflow/mlflow:v2.22.0\n2. run \"mlflow server --host 0.0.0.0 --port 8080\"\n\n### Code to generate data required to reproduce the bug\n\nno code required, error created via UI\n\n### Is the console panel in DevTools showing errors relevant to the bug?\n\nthere are no relevent errors showing \n\n### Does the network panel in DevTools contain failed requests relevant to the bug?\n\nthere are no failed requests",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "Thanks @marmal88 I can verify this bug.\ncc @daniellok-db Could you help debug on this? I don't see any special service logs when clicking on the button, it freezes ðŸ¤” \n```\n[2025-05-21 19:16:34 +0800] [31451] [CRITICAL] WORKER TIMEOUT (pid:31474)\n[2025-05-21 19:16:34 +0800] [31474] [ERROR] Error handling request (no URI read)\n```"
      },
      {
        "user": "Searcherr",
        "body": "Thank you @marmal88 for reporting this issue.\nSame here."
      },
      {
        "user": "marmal88",
        "body": "Hi @serena-ruan and @daniellok-db,\nIt seems that the issue arises as the openModal function tries to pre-fill the information from the latest version, when there are no prompts. \nIf you could review my [proposed changes](https://github.com/mlflow/mlflow/pull/15858) that would be really helpful "
      }
    ]
  },
  {
    "issue_number": 16028,
    "title": "[FR] Support for rate limiting in MLflow tracking server",
    "author": "kimminw00",
    "state": "open",
    "created_at": "2025-06-03T02:41:00Z",
    "updated_at": "2025-06-03T03:54:07Z",
    "labels": [
      "enhancement",
      "area/server-infra"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nAdd support for rate limiting in the MLflow tracking server to control and mitigate high volumes of incoming requests from clients, prevent abuse, and improve system reliability and scalability.\n\n### Motivation\n\nOrganizations deploying MLflow often need to ensure fair usage across users and prevent any single client from overwhelming the tracking server. For example, an automated retraining pipeline might inadvertently send too many tracking requests, degrading performance for other users.\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [x] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "Thanks for the FR. https://flask-limiter.readthedocs.io/en/stable/ looks nice. We could use it to support rate limiting."
      },
      {
        "user": "Ajay-Satish-01",
        "body": "@harupy Can I try this?\n\nI would like to know on which components/services I should implement rate limiting using the flask-limiter package."
      }
    ]
  },
  {
    "issue_number": 16022,
    "title": "[BUG] Error fetching response: [object Object]",
    "author": "heyalexchoi",
    "state": "closed",
    "created_at": "2025-06-02T19:27:51Z",
    "updated_at": "2025-06-03T00:54:04Z",
    "labels": [
      "bug",
      "area/uiux"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.22.0\n- Tracking server: 2.22.0\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\n- Debian GNU/Linux 12 (bookworm)\n- **Python version**:\nPython 3.11.11\n- **yarn version, if running the dev UI**:\n\n\n### Describe the problem\n\ncannot view trace details. I see list view, but clicking on item gives \"Error fetching response: [object Object] \" error message.\nconsole shows 404s:\n```\nGET http://localhost:5000/ajax-api/2.0/mlflow/get-trace-artifact?request_id=490bdaca12ce435db0c881907de5fbcc 404 (NOT FOUND)\n```\n\n![Image](https://github.com/user-attachments/assets/96acd1be-0ce6-4999-b630-0ac01589441e)\n\nUsing`mlflow.dspy.autolog()` w/ sqlite `MLFLOW_TRACKING_URI=sqlite:////app/data/mlflow.db`.\nw/ UI: `mlflow ui --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:////app/data/mlflow.db`\n\nI go to `http://localhost:5000/#/experiments/1/runs/51201bd061a149c3bf4f9447a9894888/traces` by clicking to my run trace in my browser.\n\nthe odd thing is that I can see snippets in the UI but clicking in errors.\n\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nâžœ  Exo git:(dspy) âœ— docker compose logs mlflow\nmlflow-1  | [2025-06-02 19:03:22 +0000] [35] [INFO] Starting gunicorn 23.0.0\nmlflow-1  | [2025-06-02 19:03:22 +0000] [35] [INFO] Listening at: http://0.0.0.0:5000 (35)\nmlflow-1  | [2025-06-02 19:03:22 +0000] [35] [INFO] Using worker: sync\nmlflow-1  | [2025-06-02 19:03:22 +0000] [36] [INFO] Booting worker with pid: 36\nmlflow-1  | [2025-06-02 19:03:22 +0000] [37] [INFO] Booting worker with pid: 37\nmlflow-1  | [2025-06-02 19:03:22 +0000] [38] [INFO] Booting worker with pid: 38\nmlflow-1  | [2025-06-02 19:03:23 +0000] [39] [INFO] Booting worker with pid: 39\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [x] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "heyalexchoi",
        "body": "for anyone who has a similar issue:\n\nartifacts and logs are separate. logs have little snippets of traces in them, which can be confusing, but the full trace is in a separate json file.\n\nmy problem was that I had mismatches between the  `default-artifact-root ` for the logging process vs the UI server.\n\neasiest thing, IMO, is set up a server which consolidates the configurations for the logging and UI processes:\n```\n# example from my docker compose:\nmlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:////app/data/mlflow.db --default-artifact-root file:////app/data/mlruns\n\n# and then in your environment (mine is docker compose):\nMLFLOW_TRACKING_URI=http://mlflow:5000\n\n# otherwise might be something more like `MLFLOW_TRACKING_URI=http://localhost:5000`\n\n```\n\nwhich configures the logs and artifact store for both the writing and reading (ui) processes."
      }
    ]
  },
  {
    "issue_number": 15969,
    "title": "usage of deprecated rlang functions in R package",
    "author": "lschneiderbauer",
    "state": "closed",
    "created_at": "2025-05-30T06:47:19Z",
    "updated_at": "2025-06-03T00:39:00Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "Not necessarily a bug, but might become one in the future:\n\nWhen using the R package mlflow I regularly get deprecation warnings when using `mlflow::mlflow_log_metric()`, because of deprecated rlang functions:\n```R\nWarnmeldungen:\n1: `as_integer()` is deprecated as of rlang 0.4.0\nPlease use `vctrs::vec_cast()` instead.\n```",
    "comments": []
  },
  {
    "issue_number": 16008,
    "title": "Remove `dev/check-notebooks.sh` and use `clint` instead",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-02T07:11:16Z",
    "updated_at": "2025-06-03T00:32:28Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "`clint` is a custom linter used in this repository to check python scripts/notebooks. We should use it to check notebooks instead of the `dev/check-notebooks.sh` script.\n\nInstructions:\n\n1. Add a new rule in `dev/clint/src/clint/rules.py`.\n2. Update `_lint_cell` in `dev/clint/src/clint/linter.py` to use the new rule.\n3. Remove the `check-notebooks` hook in `.pre-commit-config.yaml`.\n4. Remove `dev/check-notebooks.sh`.\n\nTesting:\n\nCreate a dummy notebook that violates the new rule, and ensure `clint` catches it.",
    "comments": []
  },
  {
    "issue_number": 12290,
    "title": "[FR] ML Flow Deployment Swagger: Serve static CSS, JS scripts files locally to avoid the use of external CDNs to render the documentation.",
    "author": "sturiot",
    "state": "open",
    "created_at": "2024-06-07T08:52:49Z",
    "updated_at": "2025-06-02T20:21:12Z",
    "labels": [
      "enhancement",
      "area/uiux",
      "has-closing-pr",
      "area/deployments"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nProposed changes:\r\n\r\nProvide through the ML Flow Deployment configuration, options to build the FastAPI App.\r\n\r\n- usage of external CDNs (by default)\r\n- usage of internal CDNs\r\n- usage of local static files (self-hosting)\r\n\r\n[Documentation on Fast API to use custom resources/assets](https://fastapi.tiangolo.com/how-to/custom-docs-ui-assets/)\n\n### Motivation\n\nMost companies will not authorized access to external CDNs.\r\nThe Swagger documentation cannot be rendered.\r\nBootstrapping and serving the required static files will avoid the issue.\r\n\n\n### Details\n\nProposed changes:\r\n\r\nProvide through the ML Flow Deployment configuration, options to build the FastAPI App.\r\n\r\n- usage of external CDNs (by default)\r\n- usage of internal CDNs\r\n- usage of local static files (self-hosting)\r\n\r\n[Documentation on Fast API to use custom resources/assets](https://fastapi.tiangolo.com/how-to/custom-docs-ui-assets/)\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [X] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [X] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      },
      {
        "user": "sturiot",
        "body": "No updates on this.\r\nIs still something we should consider?"
      },
      {
        "user": "OliverLeighC",
        "body": "I think all we would need to do is update the GatewayConfig to include something like \n\n```\nclass SwaggerConfig(AliasedConfigModel):\n    swagger_js_url: Optional[str]\n    swagger_css_url: Optional[str]\n\nclass GatewayConfig(AliasedConfigModel):\n    routes: list[RouteConfig] = Field(alias=\"endpoints\")\n    swagger_config: Optional[SwaggerConfig]\n\n```\n\nand then modify the docs endpoint (`mlflow/gateway/app.py`) to something like this: \n\n```\n    @app.get(\"/docs\", include_in_schema=False)\n    async def docs():\n        return get_swagger_ui_html(\n            openapi_url=\"/openapi.json\",\n            title=\"MLflow AI Gateway\",\n            swagger_favicon_url=\"/favicon.ico\",\n            **(config.swagger_config.model_dump(exclude_none=True,exclude_unset=True) or {})\n        )\n```\n\nhttps://github.com/OliverLeighC/mlflow/tree/FR-12290/static-swagger-config this could potentially work, though it doesn't cover mounting static files"
      }
    ]
  },
  {
    "issue_number": 2838,
    "title": "[FR] Better visualization of nested params",
    "author": "johny-c",
    "state": "closed",
    "created_at": "2020-05-15T15:37:04Z",
    "updated_at": "2025-06-02T20:10:48Z",
    "labels": [
      "enhancement",
      "area/uiux",
      "area/tracking",
      "priority/backlog",
      "has-closing-pr"
    ],
    "body": "## Proposal Summary\r\n\r\nIn my work, I compare runs with large and **nested** configurations stored as python dictionaries. This results in the Run comparison page looking like this:\r\n![mlflow_comparison_nested_configs](https://user-images.githubusercontent.com/7363758/82066536-db4ee980-96cf-11ea-8f66-8de7663efa41.png)\r\n\r\nAs shown, the parameters that are themselves dictionaries are displayed in a single row of the table. I'm guessing this is because they are retrieved as plain strings when the page is loaded.\r\n\r\n\r\nThe visualization would be much more useful if each level of a nested configuration was displayed indented and in a new row, much like in a .yaml file. Maybe this can be done with a (recurrent) check if a param can be read as dictionary.\r\n\r\nIn case of deep nesting, there could be a fixed upper limit (e.g. 3) supported by mlflow.\r\nIdeally there would be a button in the UI, so the user can control the depth of nested params they want to see by expanding/contracting these params.\r\n\r\nMoreover, since nested are not read as dictionaries, they are not searchable in order to filter runs.  \r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [x] `area/uiux`: Front-end, user experience, JavaScript, plotting\r\n",
    "comments": [
      {
        "user": "smurching",
        "body": "Hi @johny-c, thanks for filing this! Do you need to log the params as a dictionary, or would it be feasible to log each key-value pair in the dictionary as a separate param to work around the current limitations of the UI?"
      },
      {
        "user": "johny-c",
        "body": "Hi @smurching, I think it makes more sence to log the params as a dictionary (a single json/yaml file). Flattening the dictionary would leave you with the question of a level separator that can only work for string keys. In any case, indexing/being able to search the config is what's important. Maybe now that databricks acquired redash, JSON makes even more sense, as I guess you can get some parsing, indexing, visualization, etc. for free."
      },
      {
        "user": "piclem",
        "body": "Hi @johny-c , have you found a nice way of logging nested dictionaries parameters ? Have you thought of a UI element to \"browse\" such parameters (e.g. wrapping/unwrapping a tree of parameters...) ?\r\n\r\nSince this is still not managed, I am thinking of using a dictionary \"flattening\" option (e.g. [this SO question](https://stackoverflow.com/questions/6027558/flatten-nested-dictionaries-compressing-keys)), that converts nested keys into flat \"level1__level2\" keys, that stay grouped together in the UI by alphabetical ordering.\r\n\r\n```python\r\nparam = {\"level1\": {\"level2-key1\": \"value1\", \"level2-key2\": \"value2\"}}\r\nmlflow.log_params(flatten(param, separator='__'))\r\n```\r\n\r\n![image](https://github.com/mlflow/mlflow/assets/10188846/ea755c6e-ed87-411b-9fe0-5870192684c8)\r\n"
      }
    ]
  },
  {
    "issue_number": 4129,
    "title": "[BUG] 'mlflow' is not recognized as an internal or external command, operable program or batch file on windows.",
    "author": "59ranjbar",
    "state": "open",
    "created_at": "2021-02-21T03:01:08Z",
    "updated_at": "2025-06-02T13:23:18Z",
    "labels": [
      "bug"
    ],
    "body": "Hi,\r\nI have installed mlflow using pip on windows 10. When I try to run mlflow_tracking.py from mlflow/examples/quickstart the .py file runs ok from cmd but when I try to run mlflow ui from cmd to open the user interface it gives the not recognized error.\r\n\r\nPlease help me here.\r\nThank you\r\n![mlflow_rror](https://user-images.githubusercontent.com/7506647/108614290-32a08980-73c7-11eb-9778-52ec81616613.png)\r\n\r\n",
    "comments": [
      {
        "user": "dmatrix",
        "body": "@59ranjbar I think you might be running into PATH issues on windows. Can you check that mlflow installation path is accessible in your PATH?"
      },
      {
        "user": "59ranjbar",
        "body": "\r\n\r\n> @59ranjbar I think you might be running into PATH issues on windows. Can you check that mlflow installation path is accessible in your PATH?\r\n\r\nI have a virtual Env (called Streamlit plz see the pic above). I am sure mlflow is installed on that virtual Env.  Should I do something else to check the accessibility path in my path?\r\nPlease help.\r\nThank you."
      },
      {
        "user": "barhom1307",
        "body": "hi @59ranjbar ,\r\nI have the same issue exactly. have you managed to solve this?"
      }
    ]
  },
  {
    "issue_number": 4645,
    "title": "[FR] \"Freeze\" experiments in ui to static html site",
    "author": "napulen",
    "state": "open",
    "created_at": "2021-08-03T19:28:24Z",
    "updated_at": "2025-06-02T13:21:16Z",
    "labels": [
      "enhancement",
      "area/tracking",
      "priority/awaiting-more-evidence"
    ],
    "body": "Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nI'd like to dump mlflow's `ui` into a static site with a few experiments, to be hosted at gh-pages or a similar service. These experiments are the accompanying material to a research paper. No new experiments will be added, and no changes will be done; so doing one \"dump\" of the ui interface into plain HTML/CSS/JS would solve this.\r\n\r\nI couldn't find any previous posts about this.\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\n\r\nI have a set of experiments accompanying an upcoming paper. There will be no more runs for this project, and I want to share all the results of the experiments I did. I find the `mlflow ui` very easy to use and, ideally, I'd just like to deploy the relevant experiments in a gh-pages or similar site.  \r\n\r\n- Why is this use case valuable to support for MLflow users in general?\r\n\r\nIt might be relevant for people who want to share the interactive visualization of their experiments without too much hazzle or reinventing a website from scratch.\r\n\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n\r\nIt is my impression that it shouldn't be that difficult to do, assuming I already have everything in my `mlruns` folder and that there will be no further experiments for this project. I essentially want to \"freeze\" these visualizations permanently as a supplementary material of my research paper.\r\n\r\nMaybe something like `Frozen-Flask` will do? But I'd be very grateful if I can get some help on this.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n## Details\r\n\r\nAny guidance on how to achieve this would be greatly appreciated.\r\n\r\nI already tried scraping my `localhost:5000` but I keep getting error messages. Maybe `ajax` operations going on? I understand if it's unfeasible to do this, but I thought that what I want to store is really those HTML files with concluded experiments. Maybe doable?\r\n\r\nThanks.\r\n\r\n(Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please refer to the [Contributing Guide](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.rst#contribution-guidelines).)\r\n",
    "comments": [
      {
        "user": "AveshCSingh",
        "body": "Thank you for submitting this feature request @napulen. This is a neat idea and valid use case, though I think it's a bit unusual so have applied the label `priority/awaiting-more-evidence`.\r\n\r\nScraping dynamic webpages is tricky. You're likely right that the ajax operations are causing errors in your scrape. I think you may need to use a web driver rather than just sending GET requests. Eg you could try using Selenium for this."
      },
      {
        "user": "napulen",
        "body": "Thank you, @AveshCSingh. I actually spent some time looking into this. I was able to generate a static version of the react front end (you just need to build the js using the provided recipe). I also tried using [Frozen-Flask](https://pythonhosted.org/Frozen-Flask/) to try to make the flask part static. It is possible for the basic url patterns without an issue. Things get more complicated with things involving the `/api/2.0/` and `/ajax-api/2.0/` urls, apparently those are all constructed programmatically with some code deeper in the mlflow codebase.\r\n\r\nFrozen-Flask does not see those urls, so it doesn't create a static path for them. You can try to force them and they should be theoretically added to your static version, but I wasn't successful at this.\r\n\r\nI think the main problem is that, for my use case, I don't need any of the `POST`-related operations (e.g., creating an experiment). However, in the `mlflow` code, these are heavily intertwined with the `GET`-related operations (e.g., querying for an experiment id). So, I couldn't go much further because I realized it would have taken me a lot of time to look into the mlflow codebase to figure it out.\r\n\r\nFor what is worth, I made my experiments available. There is a service called tensorboard.dev that sort of delivers what I need (a free way to host visualizations). Here is how it looks for my experiments: https://tensorboard.dev/experiment/l6CPJ7TdSdOjxCbibzQJwA/#scalars\r\n\r\nI honestly prefer the `mlflow` interface. If I had a chance to share my experiments within gh-pages, that would be awesome.\r\n\r\nI unfortunately don't have the medium ($$$) to host an mlflow server somewhere for an undetermined amount of time. So anything that is free is what I can provide to other researchers. In that respect, I think there's a pretty good opportunity for the mlflow community to generate static sites. Much less expensive to host, thanks to services like gh-pages. "
      },
      {
        "user": "nikunjbjj",
        "body": "@napulen  have you tried using something like this- https://chrome.google.com/webstore/detail/take-webpage-screenshots/mcbpblocgmgfnpjjppndjkmgjaogfceg?utm_source=chrome-ntp-icon  which kind of takes the screenshot of your entire page and gives you a static view that can be done? Or did you care about the actual HTML code of the page? "
      }
    ]
  },
  {
    "issue_number": 16013,
    "title": "Support string guidelines for `meets_guidelines` judge",
    "author": "B-Step62",
    "state": "closed",
    "created_at": "2025-06-02T09:01:32Z",
    "updated_at": "2025-06-02T13:03:54Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "The `mlflow.genai.judges.meets_guidelines` should allow both single string and list of strings inputs. The underlying databricks judge only accept a list of judge, so our judge API should wrap it in a list. The change should be tested in `test_guideline_adherence` test case in `tests/genai/scorers/test_builtin_scorers.py`",
    "comments": []
  },
  {
    "issue_number": 12151,
    "title": "[BUG] Unable to load images logged by mlflow.log_image",
    "author": "cnguyen10",
    "state": "open",
    "created_at": "2024-05-28T09:21:54Z",
    "updated_at": "2025-06-02T08:51:15Z",
    "labels": [
      "bug",
      "area/uiux"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\n\n### MLflow version\n\n2.13.0\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 22.04\r\n- **Python version**: 3.10\r\n- **yarn version, if running the dev UI**: N/A\r\n\n\n### Describe the problem\n\nAfter logging numpy array images through `mlflow.log_image`, the web UI cannot display the images due to bad naming. For example, an image logged into Mlflow displays on the web UI with the following path:\r\n> Path: mlflow-artifacts:/882356406785044617/a5f5b37d727a46bd971278d870b5886d/artifacts/images/image_0%step\u0010%timestamp\u001716852222145ï¿½f28485-e176-42c1-9a02-1f3929e37af3.png\r\n\r\nBut the image can still be viewed by following the path in the local machine and open using any photo viewer software.\n\n### Steps to reproduce the bug\n\nCode to log images is identical to the one specified on Mlflow documents:\r\n```python\r\nimport mlflow\r\nimport numpy as np\r\n\r\nimage = np.random.randint(0, 256, size=(100, 100, 3), dtype=np.uint8)\r\n\r\nwith mlflow.start_run():\r\n    mlflow.log_image(image, key=\"dogs\", step=3)\r\n```\r\nAfter logging the image, open the web UI, go to that experiment and run, go to the Artifact tab, then one can see the image stored there, but unable to view on the web browser.\n\n### Code to generate data required to reproduce the bug\n\n```python\r\nimport mlflow\r\nimport numpy as np\r\n\r\nimage = np.random.randint(0, 256, size=(100, 100, 3), dtype=np.uint8)\r\n\r\nwith mlflow.start_run():\r\n    mlflow.log_image(image, key=\"dogs\", step=3)\r\n```\n\n### Is the console panel in DevTools showing errors relevant to the bug?\n\n```\r\n:8080/get-artifact?path=images%2Fimage_0%25step%10%25timestamp%1716852222145%EF%BF%BDf28485-e176-42c1-9a02-1f3929e37af3.png&run_uuid=a5f5b37d727a46bd971278d870b5886d:1 \r\n        \r\n        \r\n       Failed to load resource: the server responded with a status of 400 (Bad Request)\r\nArtifactUtils.ts:99 o\r\n(anonymous) @ ArtifactUtils.ts:99\r\n:8080/#/experiments/â€¦0b5886d/artifacts:1 Uncaught (in promise) o\r\nArtifactUtils.ts:22 \r\n        \r\n        \r\n       GET http://localhost:8080/get-artifact?path=images%2Fimage_0%25step%100%25timestamp%1716863738870%0Cae5948-5079-412d-b1ee-dcb4dee182f4.png&run_uuid=a5f5b37d727a46bd971278d870b5886d 400 (Bad Request)\r\n(anonymous) @ ArtifactUtils.ts:22\r\n(anonymous) @ ArtifactUtils.ts:82\r\na @ ArtifactUtils.ts:80\r\nl @ ArtifactUtils.ts:110\r\n(anonymous) @ ShowArtifactImageView.tsx:29\r\nos @ react-dom.production.min.js:244\r\nCc @ react-dom.production.min.js:286\r\ncc @ react-dom.production.min.js:273\r\nzo @ react-dom.production.min.js:127\r\nuc @ react-dom.production.min.js:274\r\nRe @ react-dom.production.min.js:52\r\nUr @ react-dom.production.min.js:109\r\nZt @ react-dom.production.min.js:74\r\n$t @ react-dom.production.min.js:73\r\nShow 9 more frames\r\nShow less\r\nArtifactUtils.ts:99 oÂ {status: 400, text: '<html>\\n  <head>\\n    <title>Bad Request</title>\\n  <â€¦ee182f4.png&quot;\\\\&#x27;&#x27;\\n  </body>\\n</html>\\n', textJson: null}status: 400text: \"<html>\\n  <head>\\n    <title>Bad Request</title>\\n  </head>\\n  <body>\\n    <h1><p>Bad Request</p></h1>\\n    Invalid HTTP Header: &#x27;\\\\&#x27;attachment; filename=&quot;image_0%step\\\\\\\\x100%timestamp\\\\\\\\x1716863738870\\\\\\\\x0cae5948-5079-412d-b1ee-dcb4dee182f4.png&quot;\\\\&#x27;&#x27;\\n  </body>\\n</html>\\n\"textJson: null[[Prototype]]: Object\r\n(anonymous) @ ArtifactUtils.ts:99\r\nawait in (anonymous) (async)\r\na @ ArtifactUtils.ts:80\r\nl @ ArtifactUtils.ts:110\r\n(anonymous) @ ShowArtifactImageView.tsx:29\r\nos @ react-dom.production.min.js:244\r\nCc @ react-dom.production.min.js:286\r\ncc @ react-dom.production.min.js:273\r\nzo @ react-dom.production.min.js:127\r\nuc @ react-dom.production.min.js:274\r\nRe @ react-dom.production.min.js:52\r\nUr @ react-dom.production.min.js:109\r\nZt @ react-dom.production.min.js:74\r\n$t @ react-dom.production.min.js:73\r\nShow 9 more frames\r\nShow less\r\n:8080/#/experiments/â€¦0b5886d/artifacts:1 Uncaught (in promise) oÂ {status: 400, text: '<html>\\n  <head>\\n    <title>Bad Request</title>\\n  <â€¦ee182f4.png&quot;\\\\&#x27;&#x27;\\n  </body>\\n</html>\\n', textJson: null}status: 400text: \"<html>\\n  <head>\\n    <title>Bad Request</title>\\n  </head>\\n  <body>\\n    <h1><p>Bad Request</p></h1>\\n    Invalid HTTP Header: &#x27;\\\\&#x27;attachment; filename=&quot;image_0%step\\\\\\\\x100%timestamp\\\\\\\\x1716863738870\\\\\\\\x0cae5948-5079-412d-b1ee-dcb4dee182f4.png&quot;\\\\&#x27;&#x27;\\n  </body>\\n</html>\\n\"textJson: null[[Prototype]]: Object\r\nPromise.then (async)\r\n(anonymous) @ ShowArtifactImageView.tsx:29\r\nos @ react-dom.production.min.js:244\r\nCc @ react-dom.production.min.js:286\r\ncc @ react-dom.production.min.js:273\r\nzo @ react-dom.production.min.js:127\r\nuc @ react-dom.production.min.js:274\r\nRe @ react-dom.production.min.js:52\r\nUr @ react-dom.production.min.js:109\r\nZt @ react-dom.production.min.js:74\r\n$t @ react-dom.production.min.js:73\r\nShow 9 more frames\r\nShow less\r\n\r\n:8080/get-artifact?path=images%2Fimage_0%25step%10%25timestamp%1716852222145%EF%BF%BDf28485-e176-42c1-9a02-1f3929e37af3.png&run_uuid=a5f5b37d727a46bd971278d870b5886d:1 \r\n        \r\n        \r\n       Failed to load resource: the server responded with a status of 400 (Bad Request)\r\nArtifactUtils.ts:99 o\r\n(anonymous) @ ArtifactUtils.ts:99\r\n:8080/#/experiments/â€¦0b5886d/artifacts:1 Uncaught (in promise) o\r\nArtifactUtils.ts:22 \r\n        \r\n        \r\n       GET http://localhost:8080/get-artifact?path=images%2Fimage_0%25step%100%25timestamp%1716863738870%0Cae5948-5079-412d-b1ee-dcb4dee182f4.png&run_uuid=a5f5b37d727a46bd971278d870b5886d 400 (Bad Request)\r\n(anonymous) @ ArtifactUtils.ts:22\r\n(anonymous) @ ArtifactUtils.ts:82\r\na @ ArtifactUtils.ts:80\r\nl @ ArtifactUtils.ts:110\r\n(anonymous) @ ShowArtifactImageView.tsx:29\r\nos @ react-dom.production.min.js:244\r\nCc @ react-dom.production.min.js:286\r\ncc @ react-dom.production.min.js:273\r\nzo @ react-dom.production.min.js:127\r\nuc @ react-dom.production.min.js:274\r\nRe @ react-dom.production.min.js:52\r\nUr @ react-dom.production.min.js:109\r\nZt @ react-dom.production.min.js:74\r\n$t @ react-dom.production.min.js:73\r\nShow 9 more frames\r\nShow less\r\nArtifactUtils.ts:99 oÂ {status: 400, text: '<html>\\n  <head>\\n    <title>Bad Request</title>\\n  <â€¦ee182f4.png&quot;\\\\&#x27;&#x27;\\n  </body>\\n</html>\\n', textJson: null}status: 400text: \"<html>\\n  <head>\\n    <title>Bad Request</title>\\n  </head>\\n  <body>\\n    <h1><p>Bad Request</p></h1>\\n    Invalid HTTP Header: &#x27;\\\\&#x27;attachment; filename=&quot;image_0%step\\\\\\\\x100%timestamp\\\\\\\\x1716863738870\\\\\\\\x0cae5948-5079-412d-b1ee-dcb4dee182f4.png&quot;\\\\&#x27;&#x27;\\n  </body>\\n</html>\\n\"textJson: null[[Prototype]]: Object\r\n(anonymous) @ ArtifactUtils.ts:99\r\nawait in (anonymous) (async)\r\na @ ArtifactUtils.ts:80\r\nl @ ArtifactUtils.ts:110\r\n(anonymous) @ ShowArtifactImageView.tsx:29\r\nos @ react-dom.production.min.js:244\r\nCc @ react-dom.production.min.js:286\r\ncc @ react-dom.production.min.js:273\r\nzo @ react-dom.production.min.js:127\r\nuc @ react-dom.production.min.js:274\r\nRe @ react-dom.production.min.js:52\r\nUr @ react-dom.production.min.js:109\r\nZt @ react-dom.production.min.js:74\r\n$t @ react-dom.production.min.js:73\r\nShow 9 more frames\r\nShow less\r\n:8080/#/experiments/â€¦0b5886d/artifacts:1 Uncaught (in promise) oÂ {status: 400, text: '<html>\\n  <head>\\n    <title>Bad Request</title>\\n  <â€¦ee182f4.png&quot;\\\\&#x27;&#x27;\\n  </body>\\n</html>\\n', textJson: null}status: 400text: \"<html>\\n  <head>\\n    <title>Bad Request</title>\\n  </head>\\n  <body>\\n    <h1><p>Bad Request</p></h1>\\n    Invalid HTTP Header: &#x27;\\\\&#x27;attachment; filename=&quot;image_0%step\\\\\\\\x100%timestamp\\\\\\\\x1716863738870\\\\\\\\x0cae5948-5079-412d-b1ee-dcb4dee182f4.png&quot;\\\\&#x27;&#x27;\\n  </body>\\n</html>\\n\"textJson: null[[Prototype]]: Object\r\nPromise.then (async)\r\n(anonymous) @ ShowArtifactImageView.tsx:29\r\nos @ react-dom.production.min.js:244\r\nCc @ react-dom.production.min.js:286\r\ncc @ react-dom.production.min.js:273\r\nzo @ react-dom.production.min.js:127\r\nuc @ react-dom.production.min.js:274\r\nRe @ react-dom.production.min.js:52\r\nUr @ react-dom.production.min.js:109\r\nZt @ react-dom.production.min.js:74\r\n$t @ react-dom.production.min.js:73\r\nShow 9 more frames\r\nShow less\r\n\r\n:8080/get-artifact?path=images%2Fimage_0%25step%10%25timestamp%1716852222145%EF%BF%BDf28485-e176-42c1-9a02-1f3929e37af3.png&run_uuid=a5f5b37d727a46bd971278d870b5886d:1 \r\n        \r\n        \r\n       Failed to load resource: the server responded with a status of 400 (Bad Request)\r\nArtifactUtils.ts:99 o\r\n(anonymous) @ ArtifactUtils.ts:99\r\n:8080/#/experiments/â€¦0b5886d/artifacts:1 Uncaught (in promise) o\r\nArtifactUtils.ts:22 \r\n        \r\n        \r\n       GET http://localhost:8080/get-artifact?path=images%2Fimage_0%25step%100%25timestamp%1716863738870%0Cae5948-5079-412d-b1ee-dcb4dee182f4.png&run_uuid=a5f5b37d727a46bd971278d870b5886d 400 (Bad Request)\r\n(anonymous) @ ArtifactUtils.ts:22\r\n(anonymous) @ ArtifactUtils.ts:82\r\na @ ArtifactUtils.ts:80\r\nl @ ArtifactUtils.ts:110\r\n(anonymous) @ ShowArtifactImageView.tsx:29\r\nos @ react-dom.production.min.js:244\r\nCc @ react-dom.production.min.js:286\r\ncc @ react-dom.production.min.js:273\r\nzo @ react-dom.production.min.js:127\r\nuc @ react-dom.production.min.js:274\r\nRe @ react-dom.production.min.js:52\r\nUr @ react-dom.production.min.js:109\r\nZt @ react-dom.production.min.js:74\r\n$t @ react-dom.production.min.js:73\r\nShow 9 more frames\r\nShow less\r\nArtifactUtils.ts:99 oÂ {status: 400, text: '<html>\\n  <head>\\n    <title>Bad Request</title>\\n  <â€¦ee182f4.png&quot;\\\\&#x27;&#x27;\\n  </body>\\n</html>\\n', textJson: null}status: 400text: \"<html>\\n  <head>\\n    <title>Bad Request</title>\\n  </head>\\n  <body>\\n    <h1><p>Bad Request</p></h1>\\n    Invalid HTTP Header: &#x27;\\\\&#x27;attachment; filename=&quot;image_0%step\\\\\\\\x100%timestamp\\\\\\\\x1716863738870\\\\\\\\x0cae5948-5079-412d-b1ee-dcb4dee182f4.png&quot;\\\\&#x27;&#x27;\\n  </body>\\n</html>\\n\"textJson: null[[Prototype]]: Object\r\n(anonymous) @ ArtifactUtils.ts:99\r\nawait in (anonymous) (async)\r\na @ ArtifactUtils.ts:80\r\nl @ ArtifactUtils.ts:110\r\n(anonymous) @ ShowArtifactImageView.tsx:29\r\nos @ react-dom.production.min.js:244\r\nCc @ react-dom.production.min.js:286\r\ncc @ react-dom.production.min.js:273\r\nzo @ react-dom.production.min.js:127\r\nuc @ react-dom.production.min.js:274\r\nRe @ react-dom.production.min.js:52\r\nUr @ react-dom.production.min.js:109\r\nZt @ react-dom.production.min.js:74\r\n$t @ react-dom.production.min.js:73\r\nShow 9 more frames\r\nShow less\r\n:8080/#/experiments/â€¦0b5886d/artifacts:1 Uncaught (in promise) oÂ {status: 400, text: '<html>\\n  <head>\\n    <title>Bad Request</title>\\n  <â€¦ee182f4.png&quot;\\\\&#x27;&#x27;\\n  </body>\\n</html>\\n', textJson: null}status: 400text: \"<html>\\n  <head>\\n    <title>Bad Request</title>\\n  </head>\\n  <body>\\n    <h1><p>Bad Request</p></h1>\\n    Invalid HTTP Header: &#x27;\\\\&#x27;attachment; filename=&quot;image_0%step\\\\\\\\x100%timestamp\\\\\\\\x1716863738870\\\\\\\\x0cae5948-5079-412d-b1ee-dcb4dee182f4.png&quot;\\\\&#x27;&#x27;\\n  </body>\\n</html>\\n\"textJson: null[[Prototype]]: Object\r\nPromise.then (async)\r\n(anonymous) @ ShowArtifactImageView.tsx:29\r\nos @ react-dom.production.min.js:244\r\nCc @ react-dom.production.min.js:286\r\ncc @ react-dom.production.min.js:273\r\nzo @ react-dom.production.min.js:127\r\nuc @ react-dom.production.min.js:274\r\nRe @ react-dom.production.min.js:52\r\nUr @ react-dom.production.min.js:109\r\nZt @ react-dom.production.min.js:74\r\n$t @ react-dom.production.min.js:73\r\nShow 9 more frames\r\nShow less\r\n```\n\n### Does the network panel in DevTools contain failed requests relevant to the bug?\n\n_No response_",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "cc @jessechancy could you take a look at this bug with `log_image()`? I was able to reproduce this using the code in the issueâ€”my suspicion is that there's some problem parsing the space in the artifact path"
      },
      {
        "user": "jessechancy",
        "body": "I think the issue here is that \"%\" doesn't work as a delimiter for filepaths in certain environments. We could change the delimiter so that it works - what do you think about \",\"? @daniellok-db "
      },
      {
        "user": "daniellok-db",
        "body": "Hmm, I think we can try it and see if it works. I'm wondering if we need to store all this metadata in the path thoughâ€”is it feasible to store it in some external metadata file? Or maybe we can base64 encode the metadata we want before including it in the path"
      }
    ]
  },
  {
    "issue_number": 16006,
    "title": "Remove .github/workflows/sync.py as it is no longer used",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-02T06:54:10Z",
    "updated_at": "2025-06-02T08:01:27Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "The file `.github/workflows/sync.py` is no longer used and should be removed from the repository to avoid confusion and reduce clutter.",
    "comments": []
  },
  {
    "issue_number": 15944,
    "title": "[BUG] Security Vulnerability",
    "author": "oxqnd",
    "state": "closed",
    "created_at": "2025-05-28T21:08:32Z",
    "updated_at": "2025-06-02T07:33:16Z",
    "labels": [
      "bug",
      "has-closing-pr"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n.\n\n\n### System information\n\n.\n\n### Describe the problem\n\nSecurity issue reported via email as instructed in the MLflow Security Policy.\n\n### Tracking information\n\n\n\n\n### Code to reproduce issue\n\n.\n\n\n### Stack trace\n\n.\n\n\n### Other info / logs\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "Thanks for reaching out. Replied to your email."
      }
    ]
  },
  {
    "issue_number": 15760,
    "title": "[BUG] Issue with image grid creation",
    "author": "koenkoopen",
    "state": "open",
    "created_at": "2025-05-15T14:12:04Z",
    "updated_at": "2025-06-02T04:52:12Z",
    "labels": [],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.22.0\n- Tracking server: 2.22.0\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Darwin Kernel Version 23.4.0\n- **Python version**: 3.10.17\n\n\n### Describe the problem\n\nI cannot log images in such a way so that the logged images (in my artifacts) can be used to create a Image Grid chart. Please fix this or add some documentation on how to do so.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nSystem information: Darwin Darwin Kernel Version 24.4.0: Fri Apr 11 18:33:46 PDT 2025; root:xnu-11417.101.15~117/RELEASE_ARM64_T8112\nPython version: 3.10.17\nMLflow version: 2.21.3\nMLflow module location: /Users/koen/opt/anaconda3/envs/myne/lib/python3.10/site-packages/mlflow/__init__.py\nTracking URI: file:///Users/koen/Documents/Koen/Data_Science_Lab/Klanten/Myne/ml_ops/mlruns\nRegistry URI: file:///Users/koen/Documents/Koen/Data_Science_Lab/Klanten/Myne/ml_ops/mlruns\nMLflow dependencies: \n  Flask: 3.1.0\n  Jinja2: 3.1.6\n  alembic: 1.15.2\n  boto3: 1.38.6\n  botocore: 1.38.6\n  docker: 7.1.0\n  fastapi: 0.115.12\n  graphene: 3.4.3\n  gunicorn: 23.0.0\n  markdown: 3.7\n  matplotlib: 3.10.1\n  mlflow-skinny: 2.21.3\n  numpy: 2.2.4\n  pandas: 2.2.3\n  pyarrow: 19.0.1\n  scikit-learn: 1.6.1\n  scipy: 1.15.2\n  sqlalchemy: 2.0.40\n  uvicorn: 0.34.0\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "Could you elaborate your feature request?\nWe have [log_image](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.html#mlflow.log_image) API to log images"
      },
      {
        "user": "koenkoopen",
        "body": "I am able to log the images using log_image(), they appear in the artifacts tab. However, when I am in the MLflow UI in the model metrics tab of an experiment. I cannot add a chart of type 'image grid' because there aren't any images to select although I did log images using log_image"
      },
      {
        "user": "serena-ruan",
        "body": "I see. @daniellok-db Do you know how the images are filtered if we want to create an image grid?"
      }
    ]
  },
  {
    "issue_number": 15998,
    "title": "[BUG] Security Vulnerability",
    "author": "MaximilianSwipejobs",
    "state": "closed",
    "created_at": "2025-06-02T00:49:23Z",
    "updated_at": "2025-06-02T03:14:49Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "As per https://github.com/mlflow/mlflow/security/policy, email sent to mlflow-oss-maintainers@googlegroups.com",
    "comments": []
  },
  {
    "issue_number": 15979,
    "title": "[FR] Add endpoint in tracking service that can be used for health-checking with auth is enabled",
    "author": "ggprod",
    "state": "open",
    "created_at": "2025-05-30T17:25:37Z",
    "updated_at": "2025-06-02T01:06:49Z",
    "labels": [
      "enhancement",
      "area/uiux",
      "area/tracking",
      "area/docker",
      "area/server-infra"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nCurrently when the tracking service is set with authentication enabled all endpoints require authentication.  This can be problematic when deploying the tracking service as a container in a pod in Kubernetes exposed to the internet via Cloud ALBs (Application Load Balancers).\n\nWhen deploying the tracking service in Kubernetes in cloud environments an Ingress or Gateway can be used to provide appropriate Layer7 exposure to the internet. These ALBs typically have health checking that often isn't quite as flexible as Kubernetes readiness checks.\n\nIn particular in Google Cloud, using the bitnami helm chart for MLFlow there is no way to expose the MLFLow tracking service with Ingress (which creates a Google Cloud ALB) while also having authentication enabled as there is no way to configure the ALB health check to successfully call any endpoint (so the ALB would consider all pods unhealthy and not route any traffic to any of them).\n\nThis request is for a public endpoint (that could just give a 200 OK response, it could be rate-limited for security) that can be used for ALB health checks in such configurations (where authentication is enabled)\n\n### Motivation\n\n> #### What is the use case for this feature?\nTo be able to deploy MLFLow to Kubernetes in Google Cloud (GKE) and expose it behind a Google cloud ALB.  I didn't check but similar problems may also exist in AWS and Azure\n\n> #### Why is this use case valuable to support for MLflow users in general?\nSupport secure, production-grade deployments in Kubernetes cloud environments\n\n> #### Why is this use case valuable to support for your project(s) or organization?\nsame as 2 points above\n\n> #### Why is it currently difficult to achieve this use case?\nCan't have authentication enabled and expose the MLFlow tracking service to internet traffic with Ingress/Google ALB\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [x] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [x] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [x] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "The proposal makes sense!"
      }
    ]
  },
  {
    "issue_number": 15995,
    "title": "Fix `deny_unpinned_actions` in `.github/policy.rego` to work for composite actions",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-06-01T00:56:56Z",
    "updated_at": "2025-06-02T00:51:02Z",
    "labels": [],
    "body": "`deny_unpinned_actions` in `.github/policy.rego` doesn't work for composite actions (see `.github/actions` directory for an example) because a composite actions doesn't have a `jobs` field. It has a `runs` field instead.",
    "comments": []
  },
  {
    "issue_number": 15873,
    "title": "[FR] Add quota management for MLflow entities",
    "author": "kimminw00",
    "state": "open",
    "created_at": "2025-05-26T00:14:52Z",
    "updated_at": "2025-06-01T04:13:28Z",
    "labels": [
      "enhancement",
      "area/artifacts",
      "area/model-registry",
      "area/tracking"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nThis feature proposes adding quota management for core MLflow entities (experiments, runs, artifacts, registered models, traces, prompts, metrics and params etc.) at both **instance-wide** and **per-user** levels. Quotas would allow administrators to set global limits for the entire MLflow instance and individual limits per user, preventing resource abuse and controlling storage costs in multi-user environments.\n\n### Motivation\n\n> #### What is the use case for this feature?\nAdministrators need to prevent uncontrolled growth of MLflow entities (e.g., a user accidentally creating 10K redundant runs). This is critical for organizations with limited storage budgets,  or shared MLflow tracking servers.\n\n> #### Why is this use case valuable to support for MLflow users in general?\n- **Cost Control**: Avoid unexpected storage/DB costs from excessive data\n\n> #### Why is it currently difficult to achieve this use case?\n- MLflow has no native way to restrict entity creation â€“ users can infinitely create experiments/runs/models/traces/prompts\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [x] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "Quota control is valuable.\n\nOne design level question:\n\nShall we (1) implement all quota control code in Mlflow codebase ? \nOr (2) we can provide hooking handlers for core Mlflow RPC handlers such as create experiments / create runs / upload artifacts / etc.\n\nI incline to (2) which is more scalable, it can not only support quota control but also other kinds of control in Mlflow server side, and each customer can implement the optimal quota control code based on their own use cases.\n\n@BenWilson2 What do you think ?\n\n"
      },
      {
        "user": "WeichenXu123",
        "body": "Talked with @BenWilson2 offline, we all incline to use Option (2) in https://github.com/mlflow/mlflow/issues/15873#issuecomment-2909509348"
      },
      {
        "user": "WeichenXu123",
        "body": "assigned to @joelrobin18"
      }
    ]
  },
  {
    "issue_number": 15992,
    "title": "`git commit -sam` in `autoformat.yml` does not commit untracked files",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-31T10:53:20Z",
    "updated_at": "2025-05-31T12:34:39Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": null,
    "comments": []
  },
  {
    "issue_number": 15990,
    "title": "`Create path` step in `.github/workflows/autoformat.yml` does not include untracked files in patch",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-31T09:37:44Z",
    "updated_at": "2025-05-31T10:20:05Z",
    "labels": [],
    "body": "The `Create path` step in `.github/workflows/autoformat.yml` does not include untracked files in the patch:\n\n\nhttps://github.com/mlflow/mlflow/blob/e92e694fe9dc20309c343da920c20fc1668b0533/.github/workflows/autoformat.yml#L149-L154\n",
    "comments": []
  },
  {
    "issue_number": 15988,
    "title": "Fix ./build-rdoc.sh not found in autoformat.yml",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-31T09:03:17Z",
    "updated_at": "2025-05-31T09:19:25Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": null,
    "comments": []
  },
  {
    "issue_number": 15977,
    "title": "Add a rule in .github/policy.rego to enforce that github actions is pinned by full commit SHA",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-30T14:25:42Z",
    "updated_at": "2025-05-31T02:03:47Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "`github/policy.rego` define rules for github action config files. To enhance security, we should add a rule to enforce that gi\n\n\n```yml\n# bad\n- uses: actions/checkout@v4\n\n# good\n- uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n\n# exception\n# actions stariting with './.github' should be ignored\n- uses: ./.github/...\n```",
    "comments": []
  },
  {
    "issue_number": 9155,
    "title": "[BUG]Failed to start the server using MLflow authentication. error: Reason: Worker failed to boot.",
    "author": "wwwwf",
    "state": "open",
    "created_at": "2023-07-25T09:36:30Z",
    "updated_at": "2025-05-30T14:38:06Z",
    "labels": [
      "bug",
      "area/tracking",
      "area/server-infra"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Willingness to contribute\n\nNo. I cannot contribute a bug fix at this time.\n\n### MLflow version\n\n- Client: 2.5.0\r\n- Tracking server: 2.5.0\r\n\n\n### System information\n\n- Docker image :https://hub.docker.com/r/adacotechjp/mlflow\r\n- Linux Ubuntu 22.04\r\n- Python 3.9.14\r\n\r\n\n\n### Describe the problem\n\nI am using a Docker image and updated MLflow to version 2.5.0 within it. When trying to start MLflow with the command 'mlflow server --app-name basic-auth', it fail.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```shell\r\nSystem information: Linux #1 SMP PREEMPT Thu May 25 07:27:39 UTC 2023\r\nPython version: 3.9.14\r\nMLflow version: 2.5.0\r\nMLflow module location: /usr/local/python-3.9.14/lib/python3.9/site-packages/mlflow/__init__.py\r\nTracking URI: file:///opt/apps/mlruns\r\nRegistry URI: file:///opt/apps/mlruns\r\nMLflow dependencies:\r\n  Flask: 2.3.2\r\n  Jinja2: 3.1.2\r\n  alembic: 1.11.1\r\n  click: 8.1.3\r\n  cloudpickle: 2.2.1\r\n  databricks-cli: 0.17.7\r\n  docker: 6.1.3\r\n  entrypoints: 0.4\r\n  gitpython: 3.1.31\r\n  gunicorn: 20.1.0\r\n  importlib-metadata: 6.6.0\r\n  markdown: 3.4.3\r\n  matplotlib: 3.7.1\r\n  numpy: 1.22.4\r\n  packaging: 23.1\r\n  pandas: 1.4.2\r\n  protobuf: 3.20.1\r\n  pyarrow: 12.0.1\r\n  pytz: 2022.1\r\n  pyyaml: 6.0\r\n  querystring-parser: 1.2.4\r\n  requests: 2.27.1\r\n  scikit-learn: 1.1.1\r\n  scipy: 1.6.1\r\n  sqlalchemy: 2.0.16\r\n  sqlparse: 0.4.4\r\n```\r\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\ndocker pull adacotechjp/mlflow\r\ndocker run --rm --name mlflow-container -it -p 8001:5000 adacotechjp/mlflow:2.4.0 bash\r\npip install -U mlflow\r\nmlflow server --app-name basic-auth\r\n```\r\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\n[2023-07-25 09:24:52 +0000] [45] [INFO] Starting gunicorn 20.1.0\r\n[2023-07-25 09:24:52 +0000] [45] [INFO] Listening at: http://127.0.0.1:5000 (45)\r\n[2023-07-25 09:24:52 +0000] [45] [INFO] Using worker: sync\r\n[2023-07-25 09:24:52 +0000] [46] [INFO] Booting worker with pid: 46\r\n[2023-07-25 09:24:52 +0000] [47] [INFO] Booting worker with pid: 47\r\n[2023-07-25 09:24:53 +0000] [48] [INFO] Booting worker with pid: 48\r\n[2023-07-25 09:24:53 +0000] [49] [INFO] Booting worker with pid: 49\r\n2023/07/25 09:24:54 WARNING mlflow.server.auth: This feature is still experimental and may change in a future release without warning\r\nINFO  [alembic.runtime.migration] Context impl SQLiteImpl.\r\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\r\nINFO  [alembic.runtime.migration] Running upgrade  -> 8606fa83a998, initial_migration\r\n2023/07/25 09:24:54 WARNING mlflow.server.auth: This feature is still experimental and may change in a future release without warning\r\nINFO  [alembic.runtime.migration] Context impl SQLiteImpl.\r\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\r\nINFO  [alembic.runtime.migration] Running upgrade  -> 8606fa83a998, initial_migration\r\n2023/07/25 09:24:54 WARNING mlflow.server.auth: This feature is still experimental and may change in a future release without warning\r\nINFO  [alembic.runtime.migration] Context impl SQLiteImpl.\r\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\r\nINFO  [alembic.runtime.migration] Running upgrade  -> 8606fa83a998, initial_migration\r\n2023/07/25 09:24:54 WARNING mlflow.server.auth: This feature is still experimental and may change in a future release without warning\r\nINFO  [alembic.runtime.migration] Context impl SQLiteImpl.\r\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\r\nINFO  [alembic.runtime.migration] Running upgrade  -> 8606fa83a998, initial_migration\r\n[2023-07-25 09:24:54 +0000] [45] [WARNING] Worker with pid 48 was terminated due to signal 15\r\n[2023-07-25 09:24:54 +0000] [45] [WARNING] Worker with pid 49 was terminated due to signal 15\r\n[2023-07-25 09:24:54 +0000] [45] [WARNING] Worker with pid 47 was terminated due to signal 15\r\n[2023-07-25 09:24:54 +0000] [45] [INFO] Shutting down: Master\r\n[2023-07-25 09:24:54 +0000] [45] [INFO] Reason: Worker failed to boot.\r\nRunning the mlflow server failed. Please see the logs above for details.\r\n```\r\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nNone\r\n```\r\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/gateway`: AI Gateway service, Gateway client APIs, third-party Gateway integrations\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [X] `area/server-infra`: MLflow Tracking server backend\n- [X] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "@wwwwf Can you reproduce the issue constantly?"
      },
      {
        "user": "wwwwf",
        "body": "> @wwwwf Can you reproduce the issue constantly?\r\n\r\nI can consistently reproduce the issue on two Macs, but I experienced it once on Linux, and now I cannot reproduce it."
      },
      {
        "user": "harupy",
        "body": "@wwwwf Got it. Can you run `mlflow server` with `--workers=1`?\r\n\r\n```\r\nmlflow server --app-name basic-auth --workers 1\r\n```"
      }
    ]
  },
  {
    "issue_number": 925,
    "title": "MLflow worker timeout when opening UI",
    "author": "jseppanen",
    "state": "closed",
    "created_at": "2019-02-26T12:46:55Z",
    "updated_at": "2025-05-30T14:37:59Z",
    "labels": [
      "area/uiux",
      "priority/important-soon"
    ],
    "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.5\r\n- **MLflow installed from (source or binary)**: pip install mlflow\r\n- **MLflow version (run ``mlflow --version``)**: mlflow, version 0.8.2\r\n- **Python version**: Python 3.6.6 :: Anaconda, Inc.\r\n- **npm version (if running the dev UI):\r\n- **Exact command to reproduce**: `mlflow server --file-store /bigdata/mlflow --host 0.0.0.0`\r\n\r\n### Describe the problem\r\nMLflow UI shows Niagara falls with \"Oops! Something went wrong\" every time I try opening it. I've been using it for two months, but recently it has started crashing until today I cannot get the UI to open at all anymore.\r\n\r\n### Logs\r\n\r\nserver logs after fresh restart:\r\n```\r\n[2019-02-26 12:34:36 +0000] [9] [INFO] Starting gunicorn 19.9.0\r\n[2019-02-26 12:34:36 +0000] [9] [INFO] Listening at: http://0.0.0.0:5000 (9)\r\n[2019-02-26 12:34:36 +0000] [9] [INFO] Using worker: sync\r\n[2019-02-26 12:34:36 +0000] [12] [INFO] Booting worker with pid: 12\r\n[2019-02-26 12:34:36 +0000] [14] [INFO] Booting worker with pid: 14\r\n[2019-02-26 12:34:36 +0000] [15] [INFO] Booting worker with pid: 15\r\n[2019-02-26 12:34:36 +0000] [18] [INFO] Booting worker with pid: 18\r\n[2019-02-26 12:35:30 +0000] [9] [CRITICAL] WORKER TIMEOUT (pid:14)\r\n[2019-02-26 12:35:30 +0000] [14] [INFO] Worker exiting (pid: 14)\r\n[2019-02-26 12:35:30 +0000] [28] [INFO] Booting worker with pid: 28\r\n```\r\n\r\n\r\nï¿¼browser console logs when opening UI:\r\n```\r\nsetupAjaxHeaders.js:22 \r\n{_xsrf: \"2|a583f945|b32757069a3ea1c54e37f87dba1c1428|1549020795\"}\r\nservice-worker.js:1 Uncaught (in promise) Error: Request for http://localhost:5000/static-files/static-files/static/css/main.fbf8a477.css returned a response with status 404\r\n    at service-worker.js:1\r\nservice-worker.js:1 Uncaught (in promise) Error: Request for http://localhost:5000/static-files/static-files/static/css/main.fbf8a477.css returned a response with status 404\r\n    at service-worker.js:1\r\njquery.js:9355 POST http://localhost:5000/ajax-api/2.0/preview/mlflow/runs/search net::ERR_EMPTY_RESPONSE\r\nActions.js:155 XHR failed \r\n{readyState: 0, getResponseHeader: Æ’, getAllResponseHeaders: Æ’, setRequestHeader: Æ’, overrideMimeType: Æ’, â€¦}\r\nreact-dom.production.min.js:151 TypeError: Cannot read property 'getErrorCode' of undefined\r\n    at errorRenderFunc (ExperimentPage.js:122)\r\n    at e.value (RequestStateWrapper.js:51)\r\n    at f (react-dom.production.min.js:131)\r\n    at beginWork (react-dom.production.min.js:138)\r\n    at o (react-dom.production.min.js:176)\r\n    at a (react-dom.production.min.js:176)\r\n    at x (react-dom.production.min.js:182)\r\n    at y (react-dom.production.min.js:181)\r\n    at v (react-dom.production.min.js:181)\r\n    at d (react-dom.production.min.js:180)\r\nAppErrorBoundary.js:19 TypeError: Cannot read property 'getErrorCode' of undefined\r\n    at errorRenderFunc (ExperimentPage.js:122)\r\n    at e.value (RequestStateWrapper.js:51)\r\n    at f (react-dom.production.min.js:131)\r\n    at beginWork (react-dom.production.min.js:138)\r\n    at o (react-dom.production.min.js:176)\r\n    at a (react-dom.production.min.js:176)\r\n    at x (react-dom.production.min.js:182)\r\n    at y (react-dom.production.min.js:181)\r\n    at v (react-dom.production.min.js:181)\r\n    at d (react-dom.production.min.js:180)\r\n:5000/#/experiments/1:1 Uncaught (in promise) \r\nt {xhr: {â€¦}}\r\nï»¿```",
    "comments": [
      {
        "user": "5ke",
        "body": "Same problem here! Started a parameter search before the weekend and have therefore run far more experiments than before. Now I can't start the ui anymore. Where do I start trouble shooting?"
      },
      {
        "user": "5ke",
        "body": "It turns out that the ui simply doesn't handle too many runs (in my case it starts struggling when mlruns contains more than circa 1000 experiments). Around this threshold the ui becomes unstable (sometimes crashes, sometimes works, but it's never quick&responsive), and eventually there are too many runs and it won't load at all. \r\n\r\nThis goes a bit against the philosophy of being able to track _all_ your experiments. \r\n\r\nWould using a local db instead of file storage help? Hosting externally is not an option for me. \r\n\r\nAs a side note: during troubleshooting I discovered that when you move runs around into different folders, it's important to update the artifact_location parameter in the main meta.yaml, otherwise you'll experience a different type of crash, without a clear warning."
      },
      {
        "user": "selimonat",
        "body": "same here. ui either timeouts, or crashes with about 650 runs... sometimes works, mostly doesn't."
      }
    ]
  },
  {
    "issue_number": 15972,
    "title": "Fix empty notebooks in `mlflow-3-docs-refactor` branch",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-30T10:18:26Z",
    "updated_at": "2025-05-30T12:50:42Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "The `mlflow-3-docs-refactor` branch has empty notebooks in the docs directory. We should put dummy contents to make sure python lint tools can parse them. The PR base branch must be `mlflow-3-docs-refactor` branch.",
    "comments": []
  },
  {
    "issue_number": 15943,
    "title": "[BUG] Mlflow server deployed through IIS not saving artifacts",
    "author": "gabycalderon01",
    "state": "open",
    "created_at": "2025-05-28T19:55:40Z",
    "updated_at": "2025-05-30T08:24:05Z",
    "labels": [
      "bug"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.18.0\n- Tracking server: 2.22.0\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows server 2016\n- **Python version**: 3.9\n\n\n### Describe the problem\n\nI am trying to deploy ml flow on a Windows 2016 server using IIS. The idea is that in the app directory there are a batch file that is called by a web.config, initializing the server:\n\n```batch\n@echo off\nCALL \"C:\\ProgramData\\anaconda3\\Scripts\\activate.bat\" mlflow2\nmlflow server ^\n    --host 127.0.0.1 ^\n    --port %HTTP_PLATFORM_PORT% ^\n    --default-artifact-root file:///C:/MLFlowData/mlruns ^\n    --serve-artifacts ^\n    --backend-store-uri sqlite:///C:/MLFlowData/mlruns.db\n```\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n    <system.webServer>\n\n        <handlers>\n            <add name=\"httpPlatformHandler\" path=\"*\" verb=\"*\" modules=\"httpPlatformHandler\" resourceType=\"Unspecified\"/>\n        </handlers>\n\n        <httpPlatform \n            processPath=\"C:\\Windows\\System32\\cmd.exe\" \n            arguments=\"/c start_mlflow.bat\" \n            stdoutLogEnabled=\"true\" \n            stdoutLogFile=\".\\logs\\stdout.log\"\n        >\n        </httpPlatform>\n\n    </system.webServer>\n</configuration>\n```\n\nThe initializing of the server works fine, and the UI and database work fine. The problem is that none of the artifacts are saved. This is an example of how I call the server.\n\n```python\nimport mlflow\n\nmlflow.set_tracking_uri('http:{server_url}:{port_number}')\nmlflow.set_experiment('txt-test')\n\nwith mlflow.start_run():\n    with open('test.txt', 'w') as f:\n        f.write('hello artifact')\n    mlflow.log_artifact('test.txt')\n```\n\nBut nothing happens. Like I said before, the run is saved in the database and available to view from the UI. Additionally, the artifact uri is saved correctly to the database:\n\nfile:///C:/MLFlowData/mlruns/exp/run_id/artifacts\n\nI have tried everything but can't figure out what is the problem. I have also played around with IIS pool identities and nothing has worked. Please help.",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "The artifact root path can't be the same with `C:/MLFlowData/mlruns`, can you use another path ?"
      },
      {
        "user": "WeichenXu123",
        "body": "~~btw, are you sure whether the path format like `file:///C:/...` is correct ? I feel like the format has some issue.~~"
      },
      {
        "user": "gabycalderon01",
        "body": "> The artifact root path can't be the same with `C:/MLFlowData/mlruns`, can you use another path ?\n\nDo you mean because the folder /mlruns can't be named the same as mlruns.db? I tried using C:/MLFlowData/artifacts and it still didn't work."
      }
    ]
  },
  {
    "issue_number": 15886,
    "title": "[FR]Make it possible to set Request headers for authentication in the python client",
    "author": "Flametaa",
    "state": "closed",
    "created_at": "2025-05-26T20:26:44Z",
    "updated_at": "2025-05-30T08:10:34Z",
    "labels": [
      "enhancement",
      "area/model-registry",
      "area/tracking"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nCurrently we are not able to add a request header in the python client calls (for example mlflow.create_experiment). The issue is that if we setup custom authentication we will need to send headers with our mlflow calls.\nWhat i propose is the possibility to add an authentication function that we can override that is called in the host_creds maybe? so that we set that function once in the code and it will be used to inject headers in each API call.\nIf there is a simpler solution I would love to hear it.\n\n### Motivation\n\n> #### What is the use case for this feature?\nCustom authentication \n> #### Why is this use case valuable to support for MLflow users in general?\nBeing able to send request headers make sense as it is supported in the Rest API but not in the python client.\n> #### Why is this use case valuable to support for your project(s) or organization?\nBeing able to use custom authentication without having to set a Token as an env variable\n> #### Why is it currently difficult to achieve this use case?\nCurrently we can either:\n- Use the request header plugin but in this case we will need to set the TOKEN in an env variable which is not great for security\n- Patch the http_request function in https://github.com/mlflow/mlflow/blob/master/mlflow/utils/rest_utils.py#L54C5-L54C17 to include our headers in each call.\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [x] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "One question:\n\nFor setting up custom authentication, it seems not only modifying the HTTP header, but we also need to modify MLflow server handler to handle the custom auth logic.\n\ne.g. , if you attach a custom auth token into the HTTP header, in MLflow server side , we might need to do some check like:\n\n```\n@app.before_request\ndef check_custom_auth():\n    token = request.headers.get(\"X-Custom-Auth\")\n    if token != \"expected-token\":\n        abort(401)\n```\n\ndo you plan to add some interface for pluggable auth validator in MLflow server side too ?"
      },
      {
        "user": "Flametaa",
        "body": "Hello, no i don't plan on modifying the authentication logic in MLFLow (because it's specific to the company that i work with). But I was wondering if we can have a customizable function, that will try to use `username` and `password` by default, but that the user can set to send anything else (for example a token).\nSo maybe the user will call for example `mlflow.authenticate(function_to_execute_here)` before calling the mlflow client.\nIf there is another way please tell me."
      },
      {
        "user": "WeichenXu123",
        "body": "> Hello, no i don't plan on modifying the authentication logic in MLFLow (because it's specific to the company that i work with). But I was wondering if we can have a customizable function, that will try to use `username` and `password` by default, but that the user can set to send anything else (for example a token). So maybe the user will call for example `mlflow.authenticate(function_to_execute_here)` before calling the mlflow client. If there is another way please tell me.\n\nI think this authentication process is different from what you said. We can't simply add a step `mlflow.authenticate(function_to_execute_here)` in client side to achieve this goal of authentication, the Server side handler needs related logic to authenticate credentials in the http header.\n\nwe have a `login` API https://github.com/mlflow/mlflow/blob/98f37d333a5791f406ab7068fcfa76808e167005/mlflow/utils/credentials.py#L76 which is kind of similar to what you said,   but it just configures the Databricks auth credentials, and the Databricks MLflow server endpoints have logic to authenticate the credentials."
      }
    ]
  },
  {
    "issue_number": 15276,
    "title": "[BUG] Security Vulnerability",
    "author": "zdi-disclosures",
    "state": "closed",
    "created_at": "2025-04-09T14:43:44Z",
    "updated_at": "2025-05-29T23:43:13Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "Hello,\n\nTrend Micro's Zero Day Initiative is a security/vulnerability research organization. We have discovered a vulnerability in  MLflowÂ  and we would like to disclose it responsibly to your company. Thank you and best regards,\nKholoud Altookhy\nProgram Manager, Zero Day Initiative\nTrend Micro\n[ZDI-DISCLOSURES@trendmicro.com](mailto:ZDI-DISCLOSURES@trendmicro.com)",
    "comments": [
      {
        "user": "BenWilson2",
        "body": "Hi @zdi-disclosures we have received your report and will be filing a fix for it this week. Thank you for identifying it! The fix will be in MLflow 2.22.0. "
      },
      {
        "user": "zdi-disclosures",
        "body": "Hi @BenWilson2 thank you for fixing ZDI-CAN-26916. Have you requested a CVE for the fixed issue? ZDI is a CNA we can assign a CVE if you have not already done.\n\nWe have submitted a new vulnerability report \"ZDI-CAN-26921\" via email to mlflow-oss-maintainers@googlegroups.com, please let us know if you have any questions"
      }
    ]
  },
  {
    "issue_number": 15891,
    "title": "[FR] OpenLineage support",
    "author": "phitoduck",
    "state": "open",
    "created_at": "2025-05-27T07:01:33Z",
    "updated_at": "2025-05-29T20:30:47Z",
    "labels": [
      "enhancement",
      "area/models",
      "area/tracking"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nThere's a growing appetite for data/feature/ml lineage in our space right now. \n\nIt's hard to keep track of \n\n1. what data \n2. feeds into which features, which\n3. feeds into which model, which\n4. feeds into which inferences, which\n5. are consumed by which products\n\nUnity Catalog tracks lineage for MLflow models and their input features thanks to spark. (although lineage disappears if you leave spark to do operations in pandas/polars--it's a major complaint).\n\nDataHub has [experimental support](https://docs.datahub.com/docs/generated/ingestion/sources/mlflow/) for registering MLflow experiments, models, etc. in it's lineage graph.\n\nAWS DataZone has [preview support for lineage using OpenLineage](https://aws.amazon.com/blogs/big-data/amazon-datazone-introduces-openlineage-compatible-data-lineage-visualization-in-preview/). Airflow has `openlineage-airflow` which can provide this lineage to lineage backends including AWS DataZone. Similar concept with Redshift (managed assets), Athena, etc.\n\n### Motivation\n\n> #### What is the use case for this feature?\n\nThere's a growing appetite for data/feature/ml lineage in our space right now. \n\nIt's hard to keep track of \n\n1. what data \n2. feeds into which model versions\n3. feeds into which inferences \n4. consumed by which products\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\nI think it's likely that all teams, once they hit a certain number of models, struggle to keep track of the above ^^^.\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\nWe are at this stage now. We have models being used for offline and online inference. We do not have a single pane of glass to trace back which input data and code produced downstream models and in turn downstream inferences. \n\nThis stresses us out because we don't know what internal/external users may be affected by\n\n1. changes to upstream data\n2. changes to featurization logic\n3. updates to a model\n4. recent batches (or real-time inferences) produced by a given model\n\nIf we had this information:\n\n1. we could quickly see who is affected by any of these changes and notify them\n2. we could more easily reproduce experimental results because we could find the exact input data, even if it is massive\n3. we could identify which projects are not being used and can therefore be deprecated, freeing up human and tooling cost\n4. we would generally have more awareness of all the models in our organization and their inputs/outputs, which I'm sure would have many unpredictable benefits that come from good discoverability and knowledge sharing\n\n> #### Why is it currently difficult to achieve this use case?\n\nWe are on Snowflake and Metaflow. The best we can do for this right now is look at the source code of our Metaflow DAGs which train models to try to see which data fed into a model. There's no way to pull this up now, especially not in a graph view.\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "I think we should add these requested feature to integrate with Unity catalog https://docs.databricks.com/aws/en/data-governance/unity-catalog/data-lineage\n\n> (although lineage disappears if you leave spark to do operations in pandas/polars--it's a major complaint).\n\nI think it can support non-spark dataframes such as pandas dataframe, related doc:\nhttps://docs.databricks.com/aws/en/machine-learning/manage-model-lifecycle/#track-the-data-lineage-of-a-model-in-unity-catalog\n"
      },
      {
        "user": "phitoduck",
        "body": "Agree it'd be good for Unity Catalog to have this metadata, but also all other OpenLineage backends, e.g.\n\n- OpenMetadata\n- DataHub (they pull for this)\n- AWS DataZone (supports OpenLineage data! so cool)\n- ... and other OpenLineage-compatible backends\n\nMy thought was: Airflow has can be configured to point to a \"lineage backend\", which is basically an endpoint that complies with the OpenLineage API contract. \n\nI'd love to be able to configure the same sort of value in my MLflow instance and have it push all my lineage events for me. Basically report to the lineage backend \"we've got another model version here! And optionally: it's upstream of ___. And you're welcome to declare other things as downstream from it.\"\n\nsource: https://airflow.apache.org/docs/apache-airflow/2.4.3/lineage.html#lineage-backend"
      }
    ]
  },
  {
    "issue_number": 15941,
    "title": "[BUG] - Mlflow caching ModelWrapper implementation on pyfunc.load_model",
    "author": "B4tzz",
    "state": "closed",
    "created_at": "2025-05-28T13:39:01Z",
    "updated_at": "2025-05-29T12:12:32Z",
    "labels": [
      "bug",
      "area/models"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nDatabricks\n\n### MLflow version\n\n- Client: 2.22.0\n- Tracking server: 2.7.1 (not sure, i'm using runtime 13.3 LTS on databricks)\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Databricks runtime\n- **Python version**: 3.10.12\n- **yarn version, if running the dev UI**: NA\n\n\n### Describe the problem\n\nIt seems like a caching issue. I have several models registered on Databricks. Let's say I have just two models for simplicity. Both are custom PythonModels with custom methods inside their wrapper classes.\n\nModel A doesnâ€™t have a method called return_structured_data, while Model B does.\n\nIf I load Model A first and then Model B, it throws an error because the input data isnâ€™t processed by the return_structured_data method as expected. But if I load Model B first, everything works fine â€” and then Model A also works without issues.\n\n### Tracking information\n\n```shell\nMLflow version: 2.22.0\nTracking URI: databricks\nArtifact URI: dbfs:/databricks/mlflow-tracking/377897348738463/663f03fd8ced406a87af3309512af4c6/artifacts\nSystem information: Linux #33~22.04.1-Ubuntu SMP Fri Apr 25 06:39:10 UTC 2025\nPython version: 3.10.12\nMLflow version: 2.22.0\nMLflow module location: /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/mlflow/__init__.py\nTracking URI: databricks\nRegistry URI: databricks\nDatabricks runtime version: 13.3\n```\n\n\n### Code to reproduce issue\n\nModel A wrapper:\n```\nclass ModelWrapper(mlflow.pyfunc.PythonModel):\n  def __init__(self, model, feature_generation_config):\n    self._model = model\n\n  def generate_recursive_forecast(self, data, steps_ahead):\n    ...\n    return forecast\n\n   def predict(self, context, model_input, params={\"steps\": 1}):\n     return generate_recursive_forecast(model_input, params[\"steps\"])\n\n   def load_context(self):\n      ....\n```\n\n\nModel B wrapper:\n```\nclass ModelWrapper(mlflow.pyfunc.PythonModel):\n  def __init__(self, model, feature_generation_config):\n    self._model = model\n\n  def return_structured_data(self, data):\n    ...\n    return structured_data\n\n  def generate_recursive_forecast(self, data, steps_ahead):\n    ...\n    return forecast\n\n   def predict(self, context, model_input, params={\"steps\": 1}):\n     stuctured_data = self.return_structured_data(model_input)\n\n     return generate_recursive_forecast(stuctured_data, params[\"steps\"])\n\n   def load_context(self):\n      ....\n```\n\n```\n!pip install xgboost==1.7.6\n!pip install catboost==1.2.8\n!pip install lightgbm==3.3.5\n!pip install mlflow==2.22.0\n!pip install cloudpickle==2.0.0\n\nmodel_a = mlflow.pyfunc.load_model(f\"models:/model-A/production\")\nmodel_a.predict(df, params={\"steps\": 30})\n\nmodel_b = mlflow.pyfunc.load_model(f\"models:/model-B/production\")\nmodel_b.predict(df, params={\"steps\": 30}) # Throws error becaused data wasn't structured correctly, seems like predict method used is the method of model A\n```\n\n```\n!pip install xgboost==1.7.6\n!pip install catboost==1.2.8\n!pip install lightgbm==3.3.5\n!pip install mlflow==2.22.0\n!pip install cloudpickle==2.0.0\n\nmodel_b = mlflow.pyfunc.load_model(f\"models:/model-B/production\")\nmodel_b.predict(df, params={\"steps\": 30})\n\nmodel_a = mlflow.pyfunc.load_model(f\"models:/model-A/production\")\nmodel_a.predict(df, params={\"steps\": 30})\n# Runs successfully\n```\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nmodel_a = mlflow.pyfunc.load_model(f\"models:/model-A/production\")\n\nmodel_b = mlflow.pyfunc.load_model(f\"models:/model-B/production\")\nmodel_b._model_impl.return_structured_data\n\nAttributeError: '_PythonModelPyfuncWrapper' object has no attribute 'return_structured_data'\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nFile <command-8684859427670829>, line 1\n----> 1 model_2._model_impl.return_structured_data\n\nAttributeError: '_PythonModelPyfuncWrapper' object has no attribute 'return_structured_data'\n```\n\n### Other info / logs\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "Could you also share the code of \"log_model\" call ?\n\nIt seems like when you load model A and then load model B,  when loading model B, it wrongly reuses the cached class `ModelWrapper`. "
      },
      {
        "user": "WeichenXu123",
        "body": "If you use `code_paths` when logging the model, the issue is expected.\nsee https://mlflow.org/docs/latest/model/dependencies/#limitation-of-code_paths-in-loading-multiple-models-with-the-same-module-name-but-different-implementations for details.\n\nthe workaround can be one of the following :\n\n* before loading the second model, invalidate the old cached python module `sys.modules.pop(\"<the module that contains the custom ModelWrapper >\")` \n* use different python module name for the 2 models.\n* do not use `code_paths`, define the custom model class in the `__main__` module (and python pickler can handle it well when logging the model)"
      },
      {
        "user": "B4tzz",
        "body": "> If you use `code_paths` when logging the model, the issue is expected. see https://mlflow.org/docs/latest/model/dependencies/#limitation-of-code_paths-in-loading-multiple-models-with-the-same-module-name-but-different-implementations for details.\n> \n> the workaround can be one of the following :\n> \n> * before loading the second model, invalidate the old cached python module `sys.modules.pop(\"<the module that contains the custom ModelWrapper >\")`\n> * use different python module name for the 2 models.\n> * do not use `code_paths`, define the custom model class in the `__main__` module (and python pickler can handle it well when logging the model)\n\nThis is the code used to log/rgister the models:\n```\ndef log_model(self):\n\n    experiment_name = f\"{self.project_folder}/{self.project_name}\"\n\n    model = self.return_model_wrapped()\n\n    mlflow.set_experiment(experiment_name)\n    with mlflow.start_run() as run:\n      # Definindo a assinatura do modelo\n      signature = infer_signature(model_input=self.model_input[:2], params={\"steps\": 2})\n\n      for metric_name, metric_value in self.model[\"metrics\"].items():\n        if isinstance(metric_value, (int, float)):\n          mlflow.log_metric(f\"{metric_name} oot\", metric_value)\n\n      conda_env = mlflow.utils.environment._mlflow_conda_env(\n        additional_pip_deps=[\n          \"xgboost==1.7.6\",\n          \"catboost==1.2.8\",\n          \"lightgbm==3.3.5\",\n        ]\n      )\n\n      temp_dir, temp_subdir = self.copy_code_module()\n\n      artifact_dir = tempfile.mkdtemp()\n      artifact_path = os.path.join(artifact_dir, \"model_and_config.pkl\")\n      with open(artifact_path, \"wb\") as f:\n          cloudpickle.dump({\n              \"model\": self.model[\"model\"],\n              \"feature_generation_config\": self.feature_generation_config,\n          }, f)\n\n\n      artifact_named_dir = os.path.join(artifact_dir, \"model_config_artifact\")\n      os.makedirs(artifact_named_dir)\n      shutil.move(artifact_path, os.path.join(artifact_named_dir, \"model_and_config.pkl\"))\n\n      # Salvando o modelo\n      mlflow.pyfunc.log_model(\n        \"model\",\n        python_model=model,\n        signature=signature,\n        code_paths=[temp_subdir],\n        conda_env=conda_env,\n        artifacts={\"model_config_artifact\": artifact_named_dir}\n      )\n\n      # Limpeza\n      shutil.rmtree(artifact_dir)\n      shutil.rmtree(temp_dir)\n```\n\n\nI also tried this approach, popping the modules from sys, but it doesn't work. It seems Mlflow caches the code in other temporary files. When popping the module, the console returns \"<module 'ml_pipeline_libbs.ts_model_registry' from '/local_disk0/repl_tmp_data/ReplId-1afad-102dc-f18e1-d/tmprty_kw68/code/ml_pipeline_libbs/ts_model_registry.py'>\". I tried deleting these temporary files as well, but the cached code issue persists.\n```\nimport shutil\nimport os\n\nsys.modules.pop(\"ml_pipeline_libbs\")\nsys.modules.pop(\"ml_pipeline_libbs.ts_model_registry\")\n\nshutil.rmtree(\"/local_disk0/repl_tmp_data\")\n```"
      }
    ]
  },
  {
    "issue_number": 15838,
    "title": "[FR] @mlflow.trace decorator doesn't work with classmethod.",
    "author": "B-Step62",
    "state": "closed",
    "created_at": "2025-05-22T09:24:24Z",
    "updated_at": "2025-05-29T02:58:32Z",
    "labels": [
      "enhancement",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nThe @mlflow.trace decorator should be able to trace classmethod.\n\n```\nclass` Model:\n\n    @mlflow.trace\n    @classmethod\n    def predict(cls, x, y):\n        return x + y\n\nModel.predict(1, 2)\n```\n\nThis currently fails.\n\n### Motivation\n\n.\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": []
  },
  {
    "issue_number": 15952,
    "title": "Update dev/show_package_release_dates.py to display release time as well as date",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-29T02:12:05Z",
    "updated_at": "2025-05-29T02:55:01Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "The dev/show_package_release_dates.py script currently displays the release date of installed packages in the format YYYY-MM-DD. It would be helpful to also display the release time (e.g., YYYY-MM-DD HH:MM:SS) for more precise information.\n\nCurrent output example:\n```\nPackage                                 Version        Release Date\n-------------------------------------------------------------------\nlitellm                                 1.71.1         2025-05-25  \nhttpx-aiohttp                           0.1.4          2025-05-23  \n...\n```\n\nRequested change:\nUpdate the script to show both the date and time of the release for each package, such as 2025-05-25 14:23:01.",
    "comments": []
  },
  {
    "issue_number": 15911,
    "title": "[BUG] get_model_info() error when 'environment_variables.txt' is not available",
    "author": "Lucashsmello",
    "state": "open",
    "created_at": "2025-05-27T15:27:03Z",
    "updated_at": "2025-05-29T00:25:29Z",
    "labels": [
      "bug",
      "area/artifacts",
      "area/models",
      "area/server-infra"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.22.0 (also tested on 2.21.3)\n- Tracking server: 2.22.0 (also tested on 2.21.3)\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 6.15.0-2-cachyos\n- **Python version**: 3.12.10\n\n\n### Describe the problem\n\nWhen retrieving model information (`mlflow.models.get_model_info`) using the Python client API with a remote MLflow server (http://localhost:5000), the server returns **a 500 error**. \n\n**Important**: If a local artifact URI (i.e \"file:/\" instead of \"http:/\"), the code runs fine, returning a `ModelInfo.env_vars` being None (this is expected). Check the code block below, and comment the `os.environ[...` code line to reproduce this one.\n\n**My thoughts:** The server is getting a 500 error because `environment_variables.txt` is not present in the model artifact URI. In my humble opinion, if this file is not present, the server should return a `ModelInfo` with a None `.env_vars` attribute value. Consequently, this will be consistent with the \"file:/\" behaviour as explained in my important note above. I am willing to open a PR with this proposed solution.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nSystem information: Linux #1 SMP PREEMPT_DYNAMIC Mon, 26 May 2025 15:25:40 +0000\nPython version: 3.12.10\nMLflow version: 2.22.0\nMLflow module location: [/home/lhsmello/projects/testmlflow/.venv/lib/python3.12/site-packages/mlflow/__init__.py](https://file+.vscode-resource.vscode-cdn.net/home/lhsmello/projects/testmlflow/.venv/lib/python3.12/site-packages/mlflow/__init__.py)\nTracking URI: file:///home/lhsmello/projects/testmlflow/notebooks/mlruns\nRegistry URI: file:///home/lhsmello/projects/testmlflow/notebooks/mlruns\nMLflow dependencies: \n  Flask: 3.1.0\n  Flask-WTF: 1.2.2\n  Jinja2: 3.1.3\n  aiohttp: 3.9.5\n  alembic: 1.15.2\n  docker: 7.1.0\n  fastapi: 0.109.0\n  graphene: 3.4.3\n  gunicorn: 23.0.0\n  markdown: 3.7\n  matplotlib: 3.9.2\n  mlflow-skinny: 2.22.0\n  mlserver: 1.7.0\n  mlserver-mlflow: 1.7.0\n  numpy: 1.26.4\n  pandas: 2.1.4\n  pyarrow: 19.0.1\n  scikit-learn: 1.6.1\n  scipy: 1.14.1\n  sqlalchemy: 2.0.40\n  uvicorn: 0.34.0\n  watchfiles: 0.24.0\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n\nRun localserver first:\n```bash\nmlflow server --port 5000\n```\n\n```python\nimport mlflow\nimport torch\nimport os\n\nos.environ[\"MLFLOW_TRACKING_URI\"] = \"http://localhost:5000\"\n\n# logging PyTorch model here, but any model will result in the same bug\nmodelinfo = mlflow.pytorch.log_model(\n    torch.nn.Linear(4, 1),\n    artifact_path=\"modeltest\",\n    input_example=torch.rand(1, 4).numpy(),\n)\n\ncurrent_run = mlflow.active_run()\nrun_id = current_run.info.run_id\n\nmlflow.models.get_model_info( \n    model_uri=f'runs:/{run_id}/modeltest',\n) # ERROR here: The server will have a 500 error code, because 'environment_variables.txt' is not present.\n```\n\n\n\n### Stack trace\n\nNone is available\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\nIf proper logging is enabled in the server, we have this:\n```\nDownloading artifacts:   0%|                                                                                                                                               | 0/1 [00:00<?, ?it/s]\nGET /api/2.0/mlflow-artifacts/artifacts/1/adef54b6d4314cddb17647596389c835/artifacts/modeltest/environment_variables.txt HTTP/1.1\" 500 1703 \"-\"  \"mlflow-python-client/2.22.0\n```\n\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [x] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "emm, I can't reproduce this bug even if I set tracking URI to `http://localhost:5000\"`,\n\nIn my test , the logged artifacts do not contain `environment_variables` and client loaded model info env_vars is None.\n\nDo you have the full error stack in the MLflow server side ? "
      },
      {
        "user": "Lucashsmello",
        "body": "After updating and restarting my Linux system, I am unable to replicate the problem. I have made no changes in my virtual Python environment. I've downgraded some of the Linux updated packages, still no bug. I am clueless about what was causing the bug. I had two virtual Python environments consistently having this bug.\n\nOne interesting point: Logging artifact environment_variables.txt with just the headers into the model artifact URI prevented the bug from happening.\n\nIt might be a custom Linux service I have installed or something else outside mlflow scope.  So I guess this is game over for this issue. Sorry for the inconvenience.\n\n"
      },
      {
        "user": "WeichenXu123",
        "body": "> I had two virtual Python environments consistently having this bug.\n\nInteresting. I feel like the Mlflow library installed in the python environment is corrupted.  Could you try fully remove the MLflow in the python env (remove the directory printed by `print(mlflow.__file__)`) and reinstall it ?"
      }
    ]
  },
  {
    "issue_number": 12627,
    "title": "[BUG] 'Can't locate revision identified by' and 'No such revision or branch'",
    "author": "gioargyr",
    "state": "open",
    "created_at": "2024-07-10T12:51:59Z",
    "updated_at": "2025-05-28T22:12:12Z",
    "labels": [
      "bug",
      "area/sqlalchemy",
      "area/server-infra",
      "area/deployments"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nOther\n\n### Willingness to contribute\n\nYes. I can contribute a fix for this bug independently.\n\n### MLflow version\n\n- Client: 2.9.2\r\n- Tracking server: 2.9.2\r\n\n\n### System information\n\n- **Debian 11**:\r\n- **Python 3.10.13**:\r\n\r\n\n\n### Describe the problem\n\nI deploy MLFlow in Kubernetes.\r\n- I am using the official docker image (ghcr.io/mlflow/mlflow:latest) and I install extra the packages 'psycopg2-binary' and 'boto3', so the MLFlow can be supported by a MinIO and a PostgreSQL\r\n- The PostgreSQL and MinIO instances work without any issue.\r\n- The docker container / Kubernetes is deployed with this entrypoint / command: \r\n`$ mlflow server --backend-store-uri postgresql://<PG_USER>:<PG_PWD>@<PG_HOST>:5432/<PG_DB> --host 0.0.0.0 --serve-artifacts --artifacts-destination s3://<BUCKET_NAME>`\r\nAccordingly I have defined the proper environment variables '`MLFLOW_S3_ENDPOINT_URL`',  '`AWS_ACCESS_KEY_ID`' and '`AWS_SECRET_ACCESS_KEY`' which are used by MLFlow server to access the MinIO instance.\r\n\r\nThe deployment was fine and I am pretty sure the MLFlow server restarted several times recently.\r\nHowever during it's last restart I see the following error:\r\n```\r\n2024/07/10 07:18:53 ERROR mlflow.cli: Error initializing backend store\r\n2024/07/10 07:18:53 ERROR mlflow.cli: Can't locate revision identified by '5b0e9adcef9c'\r\n```\r\nfollowed by\r\n`alembic.script.revision.ResolutionError: No such revision or branch '5b0e9adcef9c'`\r\n\r\nI searched and I found a few things on this issue but nothing solves my problem.\r\nI can clearly see there is a table `alembic_version` in the database and indeed there is a one row with only one value `5b0e9adcef9c`.\r\nI deleted this table but it didn't fix the issue (it was one of the \"solutions\" I found). So, I reverted everything, as I have a copy of the database.\r\n\r\nCan someone tell me with what value does the  MLFlow server cross-checks the entry on `alembic_version` table ? (another solution I found is that I can manually update the value of `alembic_version` table, so it will match the real one.)\r\nHas anyone else experienced this issue?\r\nCan I deactivate something when I deploy MLFlow?\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```shell\r\nREPLACE_ME\r\n```\r\n\n\n### Code to reproduce issue\n\nN/A\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nREPLACE_ME\r\n```\r\n\n\n### Other info / logs\n\n```\r\n2024/07/10 07:18:53 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n2024/07/10 07:18:53 INFO mlflow.store.db.utils: Updating database tables\r\nINFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\nINFO  [alembic.runtime.migration] Will assume transactional DDL.\r\n2024/07/10 07:18:53 ERROR mlflow.cli: Error initializing backend store\r\n2024/07/10 07:18:53 ERROR mlflow.cli: Can't locate revision identified by '5b0e9adcef9c'\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/script/base.py\", line 251, in _catch_revision_errors\r\n    yield\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/script/base.py\", line 461, in _upgrade_revs\r\n    for script in reversed(list(revs))\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/script/revision.py\", line 800, in iterate_revisions\r\n    revisions, heads = fn(\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/script/revision.py\", line 1453, in _collect_upgrade_revisions\r\n    current_revisions = self.get_revisions(lower)\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/script/revision.py\", line 530, in get_revisions\r\n    return sum([self.get_revisions(id_elem) for id_elem in id_], ())\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/script/revision.py\", line 530, in <listcomp>\r\n    return sum([self.get_revisions(id_elem) for id_elem in id_], ())\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/script/revision.py\", line 553, in get_revisions\r\n    return tuple(\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/script/revision.py\", line 554, in <genexpr>\r\n    self._revision_for_ident(rev_id, branch_label)\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/script/revision.py\", line 625, in _revision_for_ident\r\n    raise ResolutionError(\r\nalembic.script.revision.ResolutionError: No such revision or branch '5b0e9adcef9c'\r\nid_elem\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/site-packages/mlflow/cli.py\", line 426, in server\r\n    initialize_backend_stores(backend_store_uri, registry_store_uri, default_artifact_root)\r\n  File \"/usr/local/lib/python3.10/site-packages/mlflow/server/handlers.py\", line 299, in initialize_backend_stores\r\n    _get_tracking_store(backend_store_uri, default_artifact_root)\r\n  File \"/usr/local/lib/python3.10/site-packages/mlflow/server/handlers.py\", line 276, in _get_tracking_store\r\n    _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\n  File \"/usr/local/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/registry.py\", line 39, in get_store\r\n    return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri)\r\n  File \"/usr/local/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/registry.py\", line 49, in _get_store_with_resolved_uri\r\n    return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri)\r\n  File \"/usr/local/lib/python3.10/site-packages/mlflow/server/handlers.py\", line 137, in _get_sqlalchemy_store\r\n    return SqlAlchemyStore(store_uri, artifact_uri)\r\n  File \"/usr/local/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py\", line 156, in __init__\r\n    mlflow.store.db.utils._initialize_tables(self.engine)\r\n  File \"/usr/local/lib/python3.10/site-packages/mlflow/store/db/utils.py\", line 92, in _initialize_tables\r\n    _upgrade_db(engine)\r\n  File \"/usr/local/lib/python3.10/site-packages/mlflow/store/db/utils.py\", line 215, in _upgrade_db\r\n    command.upgrade(config, \"heads\")\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/command.py\", line 401, in upgrade\r\n    script.run_env()\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/script/base.py\", line 585, in run_env\r\n    util.load_python_file(self.dir, \"env.py\")\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/util/pyfiles.py\", line 93, in load_python_file\r\n    module = load_module_py(module_id, path)\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/util/pyfiles.py\", line 109, in load_module_py\r\n    spec.loader.exec_module(module)  # type: ignore\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/usr/local/lib/python3.10/site-packages/mlflow/store/db_migrations/env.py\", line 86, in <module>\r\n    run_migrations_online()\r\n  File \"/usr/local/lib/python3.10/site-packages/mlflow/store/db_migrations/env.py\", line 80, in run_migrations_online\r\n    context.run_migrations()\r\n  File \"<string>\", line 8, in run_migrations\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/runtime/environment.py\", line 939, in run_migrations\r\n    self.get_context().run_migrations(**kw)\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/runtime/migration.py\", line 612, in run_migrations\r\n    for step in self._migrations_fn(heads, self):\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/command.py\", line 390, in upgrade\r\n    return script._upgrade_revs(revision, rev)\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/script/base.py\", line 449, in _upgrade_revs\r\n    with self._catch_revision_errors(\r\n  File \"/usr/local/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/usr/local/lib/python3.10/site-packages/alembic/script/base.py\", line 283, in _catch_revision_errors\r\n    raise util.CommandError(resolution) from re\r\nalembic.util.exc.CommandError: Can't locate revision identified by '5b0e9adcef9c'\r\n\r\n```\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [X] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [X] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [X] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "I guess last database migration execution was interrupted, then it causes the database going into inconsistent state.\r\nTo recover from a failed migration, I think this section helps you:\r\nhttps://github.com/mlflow/mlflow/blob/master/mlflow/store/db_migrations/README.md#recovering-from-a-failed-migration\r\n\r\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      },
      {
        "user": "gioargyr",
        "body": "As documentation advises, I dropped the table `latest_metrics`, however the problem persists.\r\n\r\nSo, I continued, by deleting the only row of table `alembic_version`.\r\nI launched mlflow and this time I get this error:\r\n```\r\n[SQL: ALTER TABLE metrics ADD COLUMN step BIGINT DEFAULT '0' NOT NULL]\r\n(Background on this error at: https://sqlalche.me/e/20/f405)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1969, in _exec_single_context\r\n    self.dialect.do_execute(\r\n  File \"/usr/local/lib/python3.10/site-packages/sqlalchemy/engine/default.py\", line 922, in do_execute\r\n    cursor.execute(statement, parameters)\r\npsycopg2.errors.DuplicateColumn: column \"step\" of relation \"metrics\" already exists\r\n```\r\n\r\nShould I delete the column `step`?\r\nAs far as I can understand, MLFlow tries to add column `step` on table `metrics` when it is launched.\r\nCan someone tell me why?\r\nOr, can you point me to documentation that explains all the actions MLFlow does every time it is launched?\r\n"
      }
    ]
  },
  {
    "issue_number": 15933,
    "title": "Remove flaml in `requirements/test-requirements.txt`",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-28T11:22:12Z",
    "updated_at": "2025-05-28T14:45:45Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "MLflow no longer uses FLAML. Let's remove it.",
    "comments": []
  },
  {
    "issue_number": 15935,
    "title": "Create a composite action for `actions/checkout`",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-28T11:53:42Z",
    "updated_at": "2025-05-28T12:17:19Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "# Create a composite action for `actions/checkout`\n\n## Goal\n\nIn this repo, we use the following commit of `actions/checkout`:\n\n```\n- uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683\n```\n\nRepeating the same commit in every workflow file is not ideal. Instead, we can create a composite action that uses this specific commit of `actions/checkout` and then use that composite action in all our workflows.\n\n## Instructions\n\n- The composite action should the inputs used in this repository. For example, the original `actions/checkout` action has `github-server-url`, but we don't use it in this repository, so it should not be included in the composite action.\n- The composite action name must be `checkout`.\n- The composite action should be placed in the `.github/actions/checkout` directory.\n- The composite action should use the `actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683` commit.\n\n## Steps\n\n1. Create the composite action file.\n2. Search for all workflow files in the `.github/workflows` directory that use `actions/checkout`.\n3. Replace the `actions/checkout` step in each workflow file with the new composite action.",
    "comments": []
  },
  {
    "issue_number": 15927,
    "title": "Refactor: Create a composite GitHub Action for github-script usage and reuse it in workflows",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-28T05:36:38Z",
    "updated_at": "2025-05-28T09:49:24Z",
    "labels": [],
    "body": "There are currently 10 workflow files using the same `actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea` step. To improve maintainability and reduce duplication, please create a composite action in the `.github/actions` directory that wraps this usage, and update all workflows to use the new composite action instead of directly referencing `actions/github-script`.\n\nThis will make it easier to update the version or logic in one place and keep workflows consistent.",
    "comments": []
  },
  {
    "issue_number": 15931,
    "title": "[DOC-BUG] AccessDenied on API Reference for 2.22.0 (e.g. mlflow.org/docs/2.22.0/api_reference/)",
    "author": "aladwein-gr",
    "state": "closed",
    "created_at": "2025-05-28T07:49:07Z",
    "updated_at": "2025-05-28T09:00:13Z",
    "labels": [
      "area/docs"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute a documentation fix at this time.\n\n### URL(s) with the issue\n\nBroken:\nhttps://mlflow.org/docs/2.22.0/api_reference/python_api/mlflow.pytorch.html\nhttps://mlflow.org/docs/2.22.0/api_reference/\n\nWorking:\nhttps://mlflow.org/docs/latest/api_reference/\nhttps://mlflow.org/docs/latest/api_reference/python_api/index.html\nhttps://mlflow.org/docs/latest/api_reference/python_api/mlflow.pytorch.html\nhttps://mlflow.org/docs/2.21.3/api_reference/\nhttps://mlflow.org/docs/2.21.3/api_reference/python_api/index.html\nhttps://mlflow.org/docs/2.21.3/api_reference/python_api/mlflow.pytorch.html\nhttps://mlflow.org/docs/2.20.3/python_api/mlflow.pytorch.html\n\n### Description of proposal (what needs changing)\n\nThe following links to the `2.22.0` API reference return an â€œAccess Deniedâ€ error instead of the expected documentation:\n\tâ€¢\thttps://mlflow.org/docs/2.22.0/api_reference/\n\tâ€¢\thttps://mlflow.org/docs/2.22.0/api_reference/python_api/mlflow.pytorch.html\n\nExample error:\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>AccessDenied</Code><Message>Access Denied</Message></Error>\n```\n\nThese links work for other versions, such as `latest` or `2.21.3`.\nIt would be helpful if the `2.22.0` documentation could be made accessible as well.\n\nNot sure what the underlying issue is, but happy to help if possible. Thanks for looking into this!\n",
    "comments": [
      {
        "user": "harupy",
        "body": "@aladwein-gr Thanks for reaching out. The 2.22.0 docs have been redeployed. It should be available now: https://mlflow.org/docs/2.22.0/api_reference/python_api/index.html"
      }
    ]
  },
  {
    "issue_number": 15925,
    "title": "Add PIP_CONSTRAINT to requirements.yml workflow",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-28T05:32:32Z",
    "updated_at": "2025-05-28T08:27:28Z",
    "labels": [],
    "body": "The `.github/workflows/requirements.yml` workflow is missing the `PIP_CONSTRAINT` environment variable in the `env` section. This variable is present in other workflows and should be added for consistency and to ensure proper dependency management.\n\nPlease update the workflow to include:\n\n```\n  PIP_CONSTRAINT: ${{ github.workspace }}/requirements/constraints.txt\n```\n\nin the `env` section.",
    "comments": []
  },
  {
    "issue_number": 15908,
    "title": "Replace `pre-commit install` with `pre-commit install --install-hooks`",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-27T14:23:03Z",
    "updated_at": "2025-05-27T15:15:54Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "See https://pre-commit.com/#pre-commit-install-hooks for details",
    "comments": []
  },
  {
    "issue_number": 15903,
    "title": "Add type hints in `dev/show_package_release_dates.py`",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-27T13:48:32Z",
    "updated_at": "2025-05-27T14:22:25Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "Let's add type hints in `dev/show_package_release_dates.py`. Note we're using Python 3.9.",
    "comments": []
  },
  {
    "issue_number": 15871,
    "title": "[BUG] `tags.model = 'xgboost'` filter not returning recent autologged runs in MLflow UI",
    "author": "recluzegeek",
    "state": "open",
    "created_at": "2025-05-25T12:23:38Z",
    "updated_at": "2025-05-27T13:41:33Z",
    "labels": [
      "bug",
      "area/uiux"
    ],
    "body": "### MLflow version\n\n2.22.0\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\n- **Python version**: 3.9.21\n- **yarn version, if running the dev UI**:\n\n\n### Describe the problem\n\nI've ran quite few runs of the same experiment with logging enabled using `mlflow.xgboost.autolog()`, and each run is tagged with `xgboost`, as can be seen in the image\n\n![Image](https://github.com/user-attachments/assets/aaf9db38-c3a4-40b4-9ef1-73c0b0d1b977)\n\nwhen i try to filter the runs with this query `tags.model = 'xgboost'`, I dont' get all the runs, its missing the latest runs as can be seen in the image\n\n![Image](https://github.com/user-attachments/assets/0648b54e-0c05-4747-8ef6-2664f26344e1)\n\n### Steps to reproduce the bug\n\nFirst, I'm using `hyperopt` to find optimized hyper-parameters, and the runs that're successfully filtered with `tags.model` are from this code\n\n```bash\ndef objective(params):\n    mlflow.xgboost.autolog()\n    with mlflow.start_run():\n        mlflow.set_tag(\"model\", \"xgboost\")\n        mlflow.log_params(params)\n        booster = xgb.train(\n            params=params,\n            dtrain = train,\n            num_boost_round=50,\n            evals=[(valid, 'validation')],\n            early_stopping_rounds=10\n        )\n        y_pred = booster.predict(valid)\n        rmse = root_mean_squared_error(y_val, y_pred)\n        mlflow.log_metric(\"rmse\", rmse)\n\n    return {'loss': rmse, 'status': STATUS_OK}\n```\n```python\nsearch_space = {\n    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n    'objective': 'reg:linear',\n    'seed': 42\n}\n\nbest_result = fmin(\n    fn=objective,\n    space=search_space,\n    algo=tpe.suggest,\n    max_evals=5,\n    trials=Trials()\n)\n```\n\nwhen i get the optimized hyperparams, i'm training model again, and this is where the issue is arising\n\n```python\nbest_result['max_depth'] = int(best_result['max_depth'])\n\nmlflow.xgboost.autolog() # i experimented with and without this, and no change in result\n\nwith mlflow.start_run():\n    booster = xgb.train(\n                params=best_result,\n                dtrain = train,\n                num_boost_round=50,\n                evals=[(valid, 'validation')],\n                early_stopping_rounds=10\n            )\n```\n\nHere are the running logs of the above code\n\n```bash\n2025/05/25 17:04:50 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '79b6db02ce1d4d9496e965a7fbceea5f', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n[0]\tvalidation-rmse:8.42094\n[1]\tvalidation-rmse:7.68445\n[2]\tvalidation-rmse:7.06735\n..................\n[48]\tvalidation-rmse:4.26594\n[49]\tvalidation-rmse:4.26527\n2025/05/25 17:06:02 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n2025/05/25 17:06:02 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\rgk\\miniconda3\\envs\\mlflow\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:06:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\"\n2025/05/25 17:06:07 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n```\n\nand here's the logs of `mlflow.doctor()`\n\n```bash\n>>> mlflow.doctor()\nSystem information: Windows 10.0.19045\nPython version: 3.9.21\nMLflow version: 2.22.0\nMLflow module location: C:\\Users\\rgk\\miniconda3\\envs\\mlflow\\lib\\site-packages\\mlflow\\__init__.py\nTracking URI: file:///E:/mlops-zoomcamp/mlruns\nRegistry URI: file:///E:/mlops-zoomcamp/mlruns\nMLflow dependencies:\n  Flask: 3.1.1\n  Jinja2: 3.1.6\n  alembic: 1.16.1\n  boto3: 1.38.23\n  botocore: 1.38.23\n  docker: 7.1.0\n  fastapi: 0.115.12\n  graphene: 3.4.3\n  markdown: 3.8\n  matplotlib: 3.9.4\n  mlflow-skinny: 2.22.0\n  numpy: 2.0.2\n  pandas: 2.2.3\n  pyarrow: 19.0.1\n  scikit-learn: 1.6.1\n  scipy: 1.13.1\n  sqlalchemy: 2.0.41\n  uvicorn: 0.34.2\n  waitress: 3.0.2\n```\n\n### Code to generate data required to reproduce the bug\n\nProvided above.\n\n### Is the console panel in DevTools showing errors relevant to the bug?\n\nNo the browser console is clear.\n\n### Does the network panel in DevTools contain failed requests relevant to the bug?\n\nNo network panel is clean as well.",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "Could you try:\n\n```\nmlflow.search_runs(experiment_ids=[\"<your-experiment-id\"], filter_string=\"tags.model = 'xgboost'\")\n```\nand see if it returns all the related runs ?\n\n\nand could you also try the following code on the **missing** runs ?  to verify if these runs actually logged the tag of key 'model'\n\n```\nrun_data = mlflow.get_run(\"<the-run-id>\")\nprint(\"model\" in run.data.tags)\n```\n"
      },
      {
        "user": "recluzegeek",
        "body": "Here're the logs you've asked for:\n\n```bash\nIn [13]: mlflow.search_runs(experiment_ids=[1], filter_string=\"tags.model = 'xgboost'\")\nOut[13]: \n                             run_id experiment_id  ... tags.mlflow.user tags.mlflow.source.type\n0  aafebacaaa9e4b3eb31a8c3ea6b0c21c             1  ...              rgk                   LOCAL\n1  ab9c6d5df81345b4953d8d4e3d7be2de             1  ...              rgk                   LOCAL\n2  0c1bb3b398a94975ab811aace69e5542             1  ...              rgk                   LOCAL\n3  9750d9e2f0e64825ab3f78532d5a1ac0             1  ...              rgk                   LOCAL\n4  60814b4dee4f4a019830b89be5aa1fa3             1  ...              rgk                   LOCAL\n\n[5 rows x 28 columns]\n```\nand this is the log of `mlflow.search_runs(experiments_id=[1])`\n\n```bash\nIn [14]: mlflow.search_runs(experiment_ids=[1])\nOut[14]: \n                              run_id experiment_id  ... tags.mlflow.autologging tags.model\n0   4bb9e6e921af41b69ef9066b6b61f970             1  ...                    None       None\n1   79b6db02ce1d4d9496e965a7fbceea5f             1  ...                 xgboost       None\n2   340206a0679943369662d9d53aabe6e4             1  ...                 xgboost       None\n3   408767284fea4d06be8383429e3fd178             1  ...                 xgboost       None\n4   59752b2c15f249e49b3decdb7d237e70             1  ...                 xgboost       None\n5   aafebacaaa9e4b3eb31a8c3ea6b0c21c             1  ...                    None    xgboost\n6   ab9c6d5df81345b4953d8d4e3d7be2de             1  ...                    None    xgboost\n7   0c1bb3b398a94975ab811aace69e5542             1  ...                    None    xgboost\n8   9750d9e2f0e64825ab3f78532d5a1ac0             1  ...                    None    xgboost\n9   60814b4dee4f4a019830b89be5aa1fa3             1  ...                    None    xgboost\n10  e1356eaee31f4f0caf95594be34fa44b             1  ...                    None       None\n11  4f26d38abe2d4683a9af141b2871564f             1  ...                    None       None\n12  d2520cb7307448c7a429dd289d04506c             1  ...                    None       None\n13  c5d0bf8bdb38460f822a5935b2ded6fa             1  ...                    None       None\n14  b9aa2b2e0b1a47c682f21f4bf5518289             1  ...                    None       None\n15  310f0a25cdac47a2ac8d37e7e6eb5396             1  ...                    None       None\n16  acf5ab20fef243a4b71149124c7c8be9             1  ...                    None       None\n17  ea38a85a65304e8fa0f221b304dbabe0             1  ...                    None       None\n18  e0b407790e304e6380c4a889c6c5f27f             1  ...                    None       None\n19  7e80f5081a9340e989a075679a746dc1             1  ...                    None       None\n20  ea35f08a5e2249baa127d6a9ce9d45a3             1  ...                    None       None\n\n[21 rows x 39 columns]\n```\n\nand here's the `get_run()` on missing values\n\n```bash\nIn [15]: mlflow.get_run(\"4bb9e6e921af41b69ef9066b6b61f970\")\nOut[15]: \n<Run: data=<RunData: metrics={'best_iteration': 49.0,\n 'stopped_iteration': 49.0,\n 'validation-rmse': 4.265274957703522}, params={'custom_metric': 'None',\n 'early_stopping_rounds': '10',\n 'learning_rate': '0.1273187828523247',\n 'max_depth': '61',\n 'maximize': 'None',\n 'min_child_weight': '2.865851898206365',\n 'num_boost_round': '50',\n 'reg_alpha': '0.07844676886985424',\n 'reg_lambda': '0.3401462581447941',\n 'verbose_eval': 'True'}, tags={'mlflow.log-model.history': '[{\"run_id\": \"4bb9e6e921af41b69ef9066b6b61f970\", '\n                             '\"artifact_path\": \"model\", \"utc_time_created\": '\n                             '\"2025-05-25 12:21:53.010895\", \"model_uuid\": '\n                             '\"2adba047202d4b4f9915163c6fc61d2c\", \"flavors\": '\n                             '{\"python_function\": {\"loader_module\": '\n                             '\"mlflow.xgboost\", \"python_version\": \"3.9.21\", '\n                             '\"data\": \"model.xgb\", \"env\": {\"conda\": '\n                             '\"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, '\n                             '\"xgboost\": {\"xgb_version\": \"2.1.4\", \"data\": '\n                             '\"model.xgb\", \"model_class\": '\n                             '\"xgboost.core.Booster\", \"model_format\": \"xgb\", '\n                             '\"code\": null}}}]',\n 'mlflow.runName': 'silent-stoat-542',\n 'mlflow.source.name': 'c:\\\\Users\\\\rgk\\\\miniconda3\\\\envs\\\\mlflow\\\\lib\\\\site-packages\\\\ipykernel_launcher.py',\n 'mlflow.source.type': 'LOCAL',\n 'mlflow.user': 'rgk'}>, info=<RunInfo: artifact_uri='/home/vagrant/mlflow-datatalks/mlruns/1/4bb9e6e921af41b69ef9066b6b61f970/artifacts', end_time=1748175718834, experiment_id='1', lifecycle_stage='active', run_id='4bb9e6e921af41b69ef9066b6b61f970', run_name='silent-stoat-542', run_uuid='4bb9e6e921af41b69ef9066b6b61f970', start_time=1748175641538, status='FINISHED', user_id='rgk'>, inputs=<RunInputs: dataset_inputs=[<DatasetInput: dataset=<Dataset: digest='c2d51407', name='dataset', profile=('{\"features_shape\": [1343254, 519], \"features_size\": 697148826, '\n '\"features_nbytes\": 2788595304}'), schema=('{\"mlflow_tensorspec\": {\"features\": \"[{\\\\\"type\\\\\": \\\\\"tensor\\\\\", '\n '\\\\\"tensor-spec\\\\\": {\\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [-1, 519]}}]\", '\n '\"targets\": null}}'), source=('{\"tags\": {\"mlflow.user\": \"rgk\", \"mlflow.source.name\": '\n '\"c:\\\\\\\\Users\\\\\\\\rgk\\\\\\\\miniconda3\\\\\\\\envs\\\\\\\\mlflow\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\ipykernel_launcher.py\", ' \n '\"mlflow.source.type\": \"LOCAL\"}}'), source_type='code'>, tags=[<InputTag: key='mlflow.data.context', value='train'>]>,\n <DatasetInput: dataset=<Dataset: digest='4ef593a0', name='validation', profile=('{\"features_shape\": [1340859, 519], \"features_size\": 695905821, '\n '\"features_nbytes\": 2783623284}'), schema=('{\"mlflow_tensorspec\": {\"features\": \"[{\\\\\"type\\\\\": \\\\\"tensor\\\\\", '\n '\\\\\"tensor-spec\\\\\": {\\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [-1, 519]}}]\", '\n '\"targets\": null}}'), source=('{\"tags\": {\"mlflow.user\": \"rgk\", \"mlflow.source.name\": '\n '\"c:\\\\\\\\Users\\\\\\\\rgk\\\\\\\\miniconda3\\\\\\\\envs\\\\\\\\mlflow\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\ipykernel_launcher.py\", ' \n '\"mlflow.source.type\": \"LOCAL\"}}'), source_type='code'>, tags=[<InputTag: key='mlflow.data.context', value='eval'>]>]>>\n\nIn [16]: run_data = mlflow.get_run(\"4bb9e6e921af41b69ef9066b6b61f970\")\n\nIn [17]: print(\"model\" in run.data.tags)\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 1\n----> 1 print(\"model\" in run.data.tags)\n\nNameError: name 'run' is not defined\n\nIn [18]: print(\"model\" in run_data.data.tags)\nFalse\n\nIn [19]: run_data = mlflow.get_run(\"408767284fea4d06be8383429e3fd178\")\n\nIn [20]: print(\"model\" in run_data.data.tags)\nFalse\n```\n\nAlthough, I could see the `model` is returning `False`, so I believe not an issue with the UI filtering, but how could it add xgboost icon, suggesting the model is xgboost and it could be filtered out with `tags.model = 'xgboost'`"
      },
      {
        "user": "WeichenXu123",
        "body": "> but how could it add xgboost icon\n\nThis should be determined by the `mlflow.log-model.history` tag (MLflow generates it automatically) . You can load this tag and see details.  The tag includes logged model 'flavor' information "
      }
    ]
  },
  {
    "issue_number": 15899,
    "title": "Move `[tool.pytest.ini_options]` section after `[tool.typos.default]` section in `pyproject.toml`",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-27T13:13:17Z",
    "updated_at": "2025-05-27T13:39:20Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "title",
    "comments": []
  },
  {
    "issue_number": 15892,
    "title": "Add [![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow) badge to README.md",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-27T07:12:19Z",
    "updated_at": "2025-05-27T08:25:26Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "Please add the following badge to the top of the README.md file:\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)\n\nThis will provide a quick link to DeepWiki for users seeking more information or help about the project.",
    "comments": []
  },
  {
    "issue_number": 15888,
    "title": "Remove 'submodules: recursive' from GitHub Actions workflows",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-27T02:45:28Z",
    "updated_at": "2025-05-27T04:46:11Z",
    "labels": [],
    "body": "The GitHub Actions workflow configuration files currently include 'submodules: recursive' in the checkout steps. This was necessary when the repository contained a submodule, but the submodule has since been removed. Please update all workflow files to remove 'submodules: recursive' from the actions/checkout steps, as it is no longer needed.",
    "comments": []
  },
  {
    "issue_number": 15837,
    "title": "[FR] Remove the hardcoded --rm flag in docker run",
    "author": "jhockx",
    "state": "open",
    "created_at": "2025-05-22T09:20:50Z",
    "updated_at": "2025-05-26T11:45:01Z",
    "labels": [
      "enhancement",
      "area/projects",
      "area/docker",
      "area/deployments"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\n\n### Proposal Summary\n\nWhen deploying locally using Docker, the mlflow cli command `mlflow run` is converted to a `docker run` command, for which you can pass docker arguments using the `--docker-args` flag for `mlflow run`. However, one flag is _always_ configured, and that is the `--rm` flag. It is hardcoded here: https://github.com/mlflow/mlflow/blob/master/mlflow/projects/backend/local.py#L291\n\nAt the moment it is impossible to keep the existing container due to this `--rm` flag. As a solution, I would argue that this should be just another flag to be passed, if you want to run it with `--rm`. It would be a simple removal of the hardcoded `--rm` flag in the code. However, this will of course not be backwards compatible. If it is not overly complex, I would not be against implementing some extra code for backwards compatibility. I'm not super familiar with the code base though.\n\nHope to hear your your thoughts on how to move forward. \n\n### Motivation\n\n> #### What is the use case for this feature?\nBeing able to keep the docker container that was started with `mlflow run`.\n\n> #### Why is this use case valuable to support for MLflow users in general?\nFor instance to run it once more or on a scheduled basis etc, without using the `mlflow run` command using.\n\n> #### Why is this use case valuable to support for your project(s) or organization?\nScheduling\n\n> #### Why is it currently difficult to achieve this use case?\nIt is not possible, since the container has been removed.\n\n\n### Details\n\nSee proposal summary\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [x] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [x] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [x] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "rahuja23",
        "body": "If this issue needs assistance I am happy to contribute ðŸ˜„ Let me know if I can start working on it "
      },
      {
        "user": "WeichenXu123",
        "body": "Thanks!\n\nWould you share your use-case that needs to keep the container after `mlflow run` command exits ? Do you need it for debugging purpose ?\n\nYes we can safely convert the `--rm` argument into a configurable argument of `mlflow run` ( defaultly enabled ), if we have real use-case that needs it."
      }
    ]
  },
  {
    "issue_number": 15712,
    "title": "[FR] Add support for prompt permission",
    "author": "kimminw00",
    "state": "open",
    "created_at": "2025-05-13T06:30:17Z",
    "updated_at": "2025-05-26T09:16:30Z",
    "labels": [
      "enhancement",
      "good first issue",
      "area/tracking",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nAdd permission controls for prompts.\n\n### Motivation\n\nCurrently, MFlow does not support permissions or access controls for prompts.\nhttps://github.com/mlflow/mlflow/blob/master/mlflow/server/auth/__init__.py\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [X] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "Thank you for the report, this looks like a good first issue"
      },
      {
        "user": "joelrobin18",
        "body": "Hi @TomeHirata Can I work on this FR?"
      },
      {
        "user": "Vaishnavi-Raghupathi",
        "body": "Hi @TomeHirata I'd like to work on this "
      }
    ]
  },
  {
    "issue_number": 15181,
    "title": "Is there a way to make MLFlow runs inside a project read-only?",
    "author": "mesllo-bc",
    "state": "closed",
    "created_at": "2025-03-31T21:21:59Z",
    "updated_at": "2025-05-26T09:13:11Z",
    "labels": [],
    "body": "The question is rather self-explanatory but I'll elaborate a bit. I'd like to understand if there is a way to set this whether it is via the UI itself, or the API. The reason is to protect some important experiments from accidental deletion especially since the model weights are also saved as part of the run.",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "MLflow has an access control feature where you can provide read-only access to your collaborators. Can you take a look at the page below and let me know if this fits your usecase?\nhttps://mlflow.org/docs/latest/auth/"
      }
    ]
  },
  {
    "issue_number": 15713,
    "title": "[BUG] nested run do not follow parent_run_id",
    "author": "retzero",
    "state": "open",
    "created_at": "2025-05-13T06:39:18Z",
    "updated_at": "2025-05-26T08:47:04Z",
    "labels": [
      "bug",
      "area/tracking"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.22.0\n- Tracking server: 2.22.0\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 24.04\n- **Python version**: 3.12.10\n- **yarn version, if running the dev UI**:\n\n\n### Describe the problem\n\nIf I expose `MLFLOW_RUN_ID` as environment variable, nested run fail to use `parent_run_id` input parameter.\nSo the parent run is overwrittened by the first child run as below screen capture.\nCan you please give `parent_run_id` higher priority than the environment variable?\nOr explicitly call `MLFLOW_RUN_ID.unset()` at the front of the `start_run( )` if `parent_run_id` was given?\n\nNOK | OK\n-- | --\n![Image](https://github.com/user-attachments/assets/cc201d14-f5bb-4398-ab46-6e8e54a9ed87) | ![Image](https://github.com/user-attachments/assets/59fc85e7-603d-4413-9041-ae33827e7a4d)\n\n\n### Tracking information\n\n```\nMLflow version: 2.22.0\nTracking URI: http://localhost/mlflow/myteam-1/\nflow >= 2.0\n```\n\n### Code to reproduce issue\n\n\n```python\nimport os\nimport mlflow \n\n_parent = mlflow.start_run(run_name='Test parent')\nos.environ['MLFLOW_RUN_ID'] = _parent.info.run_id\n\nfor i in range(4):\n    with mlflow.start_run(nested=True, run_name=f'Child #{i}', parent_run_id=_parent.info.run_id):\n        mlflow.log_param(\"index\", i)\n        if i == 2:\n            mlflow.end_run(\"FAILED\")\n\nmlflow.end_run()\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "Thank you for the report! What is your motivation to explicitly set MLFLOW_RUN_ID env var?"
      },
      {
        "user": "retzero",
        "body": "> Thank you for the report! What is your motivation to explicitly set MLFLOW_RUN_ID env var?\n\nI'm implementing some control logic in our company, mainly to distribute multiple jobs with the same parent run."
      },
      {
        "user": "TomeHirata",
        "body": "Got it, can't you pass the parent run id explicitly when distributing multiple threads / processes instead of using `MLFLOW_RUN_ID`? I don't think `MLFLOW_RUN_ID` is designed for the purpose."
      }
    ]
  },
  {
    "issue_number": 15880,
    "title": "Migrate pytest.ini configuration to pyproject.toml",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-26T06:35:38Z",
    "updated_at": "2025-05-26T08:18:04Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "## Summary\n\nMigrate the current pytest configuration from `pytest.ini` to `pyproject.toml` as recommended by the latest [pytest documentation](https://docs.pytest.org/en/stable/reference/customize.html#pyproject-toml).\n\n## Details\n- Move all configuration options from `pytest.ini` to the `[tool.pytest.ini_options]` section in `pyproject.toml`.\n- Ensure that `addopts` is specified as a string.\n- Ensure that `filterwarnings` is specified as a list of strings.\n- Migrate all other relevant options (e.g., `timeout`, etc.) according to the documentation.\n- Remove `pytest.ini` after confirming the migration is successful.\n\n## Reference\n- https://docs.pytest.org/en/stable/reference/customize.html#pyproject-toml\n\n## Acceptance Criteria\n- All pytest configuration is managed in `pyproject.toml`.\n- The configuration works identically to the current setup.\n- `pytest.ini` is removed from the repository.",
    "comments": []
  },
  {
    "issue_number": 15878,
    "title": "Update astral-sh/setup-uv step in .github/actions/setup-python/action.yml",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-26T05:20:39Z",
    "updated_at": "2025-05-26T06:27:41Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "The `astral-sh/setup-uv` step in `.github/actions/setup-python/action.yml` is currently pinned to version `v3.2.4` and installs `uv` version `0.5.4`:\n\n```yaml\n- uses: astral-sh/setup-uv@caf0cab7a618c569241d31dcd442f54681755d39 # v3.2.4\n  with:\n    version: 0.5.4\n```\n\nPlease update both the `setup-uv` action version and the `uv` version to the latest stable releases.",
    "comments": []
  },
  {
    "issue_number": 12942,
    "title": "[FR] Authentication - Basic Auth - Add endpoint to list users with access to a certain experiment_id",
    "author": "BorjaEst",
    "state": "closed",
    "created_at": "2024-08-14T09:04:15Z",
    "updated_at": "2025-05-26T06:17:30Z",
    "labels": [
      "enhancement",
      "area/sqlalchemy",
      "area/server-infra",
      "area/deployments"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nMLflow Authentication REST API covers most of the required endpoints to manage user authentication. However, currently it is not possible to use the REST API to know who has access to a certain experiment.\n\n### Motivation\n\n> #### What is the use case for this feature?\r\nAs owner of certain experiments (Manage access) I would like to know which permissions and users have access to my experiment. This would be very helpful to remove or edit access in case that one of my team members leaves the project.\r\n\r\n> #### Why is this use case valuable to support for MLflow users in general?\r\nEveryone would benefit of a more complete and usefull basic authentication implementation.\r\n\r\n> #### Why is this use case valuable to support for your project(s) or organization?\r\nWould simplify users management to our MLFlow instance.\r\n\r\n> #### Why is it currently difficult to achieve this use case?\r\nExtension of the API methods require modification of the \"basic auth\" plug-in modules.\r\n\n\n### Details\n\nI suggest to modify/extend the API adding `GET 2.0/mlflow/experiments/permissions` to return a list of `experiment_permission` where `experiment_id` and `username` are optinal query arguments. The response should be paginated to avoid collection an return of the whole database. \r\n\r\n> This addition might get the endpoint `GET 2.0/mlflow/experiments/permissions/get` not necessary any longer as the method described might be used with the corresponding filers. Consider replacement if frontend or other methods are not broken after removal.\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [X] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [X] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [X] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "@BorjaEst Thank you for the proposal! Adding a way to retrieve list of authenticated users makes sense to me.\r\n\r\nAs for the solution, I don't think we can remove the usage of existing `mlflow/experiments/permissions/get` API, which breaks unknown numbers of existing workloads. For consistency, I would prefer just adding a new endpoint like `mlflow/experiments/permissions/list`.\r\n\r\ncc: @gabrielfu as the author of the most of the current auth implementation"
      },
      {
        "user": "BorjaEst",
        "body": "That sounds great, just give me the go and I will start the development in a PR.\r\nI expect I should add the following:\r\n - Code for the new entrypoint (with the sqlquery)\r\n - A test to ensure it does not break\r\n - The documentation for the endpoint (just like all the others)\r\n\r\nSomething else I am missing?\r\n - i.e. does it has sense to do something similar for models? Should I give it a try?\r\n\r\nI would implement it by adding a method to `SqlAlchemyStore` more or less like:\r\n```python\r\nquery = session.query(SqlExperimentPermission)\r\nif experiment_id:\r\n    query.filter(SqlExperimentPermission.experiment_id == experiment_id)\r\nif username:\r\n    user = self._get_user(session, username=username)\r\n    query.filter(SqlExperimentPermission.user_id== user.id)\r\n```\r\n"
      },
      {
        "user": "gabrielfu",
        "body": "Sounds good! Thanks for the proposal!\r\n\r\nThere's already a `mlflow/users/get` endpoint that returns a `User`, which contains a list of permissions attached to it. So for the new endpoint, we can do the opposite -- list the users with access to an experiment.\r\n\r\nWe'll also need to do the same for registered models and add the corresponding methods to `mlflow.server.auth.client.AuthServiceClient` :)"
      }
    ]
  },
  {
    "issue_number": 15876,
    "title": "Fix typo in test function name: test_differenet_requirements_create_different_environments",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-26T05:10:24Z",
    "updated_at": "2025-05-26T05:27:52Z",
    "labels": [
      "good first issue",
      "has-closing-pr"
    ],
    "body": "There is a typo in the test function name `test_differenet_requirements_create_different_environments` in `tests/pyfunc/test_virtualenv.py`. The word \"differenet\" should be corrected to \"different\".\n\n**Steps to fix:**\n1. Rename the function to `test_different_requirements_create_different_environments`.\n2. Update any references to this function if present.\n\nThis is a good first issue for new contributors.",
    "comments": []
  },
  {
    "issue_number": 13643,
    "title": "Add support for session backend",
    "author": "kimminw00",
    "state": "open",
    "created_at": "2024-11-02T03:53:12Z",
    "updated_at": "2025-05-25T18:36:54Z",
    "labels": [
      "enhancement",
      "help wanted",
      "area/server-infra"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nCurrently, MLflow does not support a session backend, making it difficult to maintain a consistent user session across multiple MLflow instances deployed for traffic distribution. Without session persistence, users experience the inconvenience of repeatedly re-entering session information, which reduces efficiency in a distributed environment.\r\n\r\nReference\r\nhttps://github.com/apache/airflow/blob/c1bd9c5c0141ec8b981e12232224ee1595cb83d1/airflow/www/session.py\n\n### Motivation\n\n> #### What is the use case for this feature?\r\n\r\n> #### Why is this use case valuable to support for MLflow users in general?\r\n\r\n> #### Why is this use case valuable to support for your project(s) or organization?\r\n\r\n> #### Why is it currently difficult to achieve this use case?\r\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [X] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "@kimminw00 Thanks for the FR. I'll look into https://github.com/apache/airflow/pull/21478 to see how Airflow uses Flask-Session."
      },
      {
        "user": "rahuja23",
        "body": "@harupy  Is this issue up for grabs? Would love to contribute here ðŸ˜„ \n"
      }
    ]
  },
  {
    "issue_number": 10292,
    "title": "[FR] Add PolarsDataset",
    "author": "elenavolkova93",
    "state": "open",
    "created_at": "2023-11-03T17:08:22Z",
    "updated_at": "2025-05-25T09:46:33Z",
    "labels": [
      "enhancement",
      "area/tracking",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\n\n### Proposal Summary\n\nAdd `PolarsDataset` class for logging datasets based on [polars](https://www.pola.rs/) dataframes, and a corresponding `.from_polars()` method.\n\n### Motivation\n\n> #### What is the use case for this feature?\r\n\r\nTo log a Dataset based on a polars dataframe with `mlflow.log_input`.\r\n\r\n> #### Why is this use case valuable to support for MLflow users in general?\r\n\r\nPolars is a library that's been gaining popularity in data science, as it can be a lot faster and more memory-efficient than pandas, and for some projects it's not an option to convert to pandas or numpy just to create an MLFlow dataset.\r\n\r\n> #### Why is this use case valuable to support for your project(s) or organization?\r\n\r\nWe use polars for faster data loading and preprocessing and would like to use `mlflow.data` to log the datasets.\r\n\r\n> #### Why is it currently difficult to achieve this use case?\r\n\r\nNot implemented.\n\n### Details\n\nAs far as I can see, the implementation is just:\r\n - adding `PolarsDataset(Dataset)`\r\n - adding `.from_polars()` function\r\n - although not strictly required for a minimal working implementation, would be nice to have this as well https://github.com/mlflow/mlflow/issues/7642, which would allow to infer schema of a polars dataset.\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/gateway`: AI Gateway service, Gateway client APIs, third-party Gateway integrations\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [X] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      },
      {
        "user": "mbignotti",
        "body": "Any news on this?"
      },
      {
        "user": "nrccua-timr",
        "body": "Would be nice to have ðŸ’¯ "
      }
    ]
  },
  {
    "issue_number": 9513,
    "title": "Official Mlflow docker image does not support postgres",
    "author": "benelot",
    "state": "open",
    "created_at": "2023-09-01T19:51:36Z",
    "updated_at": "2025-05-23T18:59:00Z",
    "labels": [],
    "body": "Hello!\n\nI am wondering why the default mlflow/mlflow docker image in the github container registry does not have the necessary requirements installed to run with a postgres server? It seems that many tutorials for the mlflow server on the official website use postgres://... uris for the db backend. As far as I can see, even your tests run some postgres, but the image does not support it (missing psycopg package). Am I wrong? Do I really have to roll my own Dockerfile instead of using one that comes from the official repo?\n\nThanks!\n\nBenjamin",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "Hi @benelot, thanks for raising the issue! For installing postgres related requirements it would increase image size, and we're not sure if all customers are using it. But we can keep this issue open to see if any more people vote on this :D"
      },
      {
        "user": "shantanu-bbai",
        "body": "Would it be possible to provide a flag that would download the psycopg2-binary or if postgres environment variables are detected? You'd likely need some sort of startup script.\r\n\r\n@benelot I didn't need to roll my own Dockerfile... passing this as the command works (I'm starting up the mlflow server in a docker compose):\r\n\r\n`bash -c \"python3 -m pip install pip --upgrade && \\\r\n      python3 -m pip install psycopg2-binary && \\\r\n      mlflow server ${WHATEVER_FLAGS_YOU_NEED}\"`"
      },
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      }
    ]
  },
  {
    "issue_number": 15864,
    "title": "Update .github/actions/validate-author/index.js to leave a comment when validation fails",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-23T13:51:55Z",
    "updated_at": "2025-05-23T16:46:52Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": null,
    "comments": []
  },
  {
    "issue_number": 15512,
    "title": "[FR] ChatAgentChunk optional delta field",
    "author": "bahtman",
    "state": "open",
    "created_at": "2025-04-27T09:23:29Z",
    "updated_at": "2025-05-23T16:21:39Z",
    "labels": [
      "enhancement",
      "area/models"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nMake delta field optional in ChatAgentChunk.\n\n### Motivation\n\n> #### What is the use case for this feature?\nDuring streaming of OpenAI like providers the usage and finish reason has empty content/tool calls. Currently one has to make a dummy delta with non-null content/tool_calls.\n\nThe same is the case for providing custom outputs during streaming.\n\n\n> #### Why is this use case valuable to support for MLflow users in general?\nRefer to above\n\n\n> #### Why is this use case valuable to support for your project(s) or organization?\nOur LangGraph applications would need to be able to yield state updates via `custom_outputs` without having to provide a dummy delta, which is more coupled with LLMs.\n\nOur OpenAI completions are streamed with a final usage + finish reason. We would like to propagate this\n\n> #### Why is it currently difficult to achieve this use case?\ndelta is mandatory.\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [X] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "The proposal makes sense to me. @bbqiu @smurching  WDYT?"
      },
      {
        "user": "bahtman",
        "body": "@B-Step62 is there anything I can do to move this along? :)\nI have also made https://github.com/mlflow/mlflow/pull/15385 to add Async support. Would love at look on that.\nBoth of these features would improve the readiness for production."
      },
      {
        "user": "bahtman",
        "body": "@B-Step62, @bbqiu or @smurching I would love to move to implementing this. But would need a confirm that it makes sense, to avoid wasted effort."
      }
    ]
  },
  {
    "issue_number": 15842,
    "title": "[BUG] mlflow.sagemaker.SageMakerDeploymentClient.list_endpoints only returning the first page of paginated results",
    "author": "dennis-johnson-dev",
    "state": "open",
    "created_at": "2025-05-22T19:56:37Z",
    "updated_at": "2025-05-23T16:14:17Z",
    "labels": [
      "bug",
      "integrations/sagemaker",
      "has-closing-pr",
      "area/deployments"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nDatabricks\n\n### MLflow version\n\n- Client: 2.22.0\n- boto3: [Boto3 1.38.21](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html#)\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Databricks [DBR 15.4](https://docs.databricks.com/aws/en/release-notes/runtime/15.4lts)\n- **Python version**: 3.11.11\n\n\n### Describe the problem\n\nHello!\n\nWe have been using the `mlflow.sagemaker.SageMakerDeploymentClient.list_deployments` method for listing the existing deployments. We are seeing an issue where `list_deployments` does not return all of the deployments, but rather only the first page of results from the underlying `list_endpoints` boto3 call.\n\n[The current implementation](https://github.com/mlflow/mlflow/blob/master/mlflow/sagemaker/__init__.py#L2742) uses the default arguments for boto3's `SageMaker.Client.list_endpoints` including `MaxResults` which is set to 10. Our systems currently have more than 10 endpoints.\n\n```python\n# no pagination information so we default to 10 results\nreturn sage_client.list_endpoints()[\"Endpoints\"]\n```\n\n[AWS Sagemaker client docs for list_endpoints](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/list_endpoints.html)\n\nIt would be great if either the `list_endpoints` method paginated through all pages itself and returned the entire list or provide pagination configuration so consumers could paginate themselves.\n\nI'd also be up for putting in a PR to help address this.\n\nThanks!\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nSystem information: Linux #29~22.04.1-Ubuntu SMP Sun Mar 30 07:45:38 UTC 2025\nPython version: 3.11.11\nMLflow version: 2.22.0\nMLflow module location: /local_disk0/.ephemeral_nfs/envs/pythonEnv-5f9a7c60-91fe-4ac5-9626-c0d5adc1d87e/lib/python3.11/site-packages/mlflow/__init__.py\nTracking URI: databricks\nRegistry URI: databricks-uc\nDatabricks runtime version: 15.4\nMLflow environment variables: \n  MLFLOW_CONDA_HOME: /databricks/conda\n  MLFLOW_DEPLOYMENTS_TARGET: databricks\n  MLFLOW_GATEWAY_URI: databricks\n  MLFLOW_PYTHON_EXECUTABLE: /databricks/spark/scripts/mlflow_python.sh\n  MLFLOW_REGISTRY_URI: databricks-uc\n  MLFLOW_TRACKING_URI: databricks\nMLflow dependencies: \n  aiohttp: 3.8.5\n  azure-storage-file-datalake: 12.20.0\n  boto3: 1.38.21\n  botocore: 1.38.21\n  cachetools: 5.5.2\n  click: 8.2.1\n  cloudpickle: 3.1.1\n  databricks-sdk: 0.29.0\n  fastapi: 0.115.12\n  gitpython: 3.1.44\n  google-cloud-storage: 3.1.0\n  importlib_metadata: 8.6.1\n  langchain: 0.1.20\n  opentelemetry-api: 1.33.1\n  opentelemetry-sdk: 1.33.1\n  packaging: 24.2\n  protobuf: 4.25.7\n  pyarrow: 14.0.1\n  pydantic: 1.10.22\n  pyyaml: 6.0.2\n  requests: 2.32.3\n  sqlparse: 0.5.3\n  tiktoken: 0.5.2\n  typing-extensions: 4.13.2\n  uvicorn: 0.34.2\n  virtualenv: 20.24.2\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```python\nfrom mlflow.sagemaker import SageMakerDeploymentClient\n\nclient = SageMakerDeploymentClient(target_uri=\"sagemaker:/us-east-1/<iam-role-arn\")\n\ndeployments = client.list_deployments()\nprint(len(deployments))\n# 10\n\nimport boto3\n\nsage_client = boto3.client(\"sagemaker\", region_name=\"us-east-1\")\nendpoints = sage_client.list_endpoints(MaxResults=50)[\"Endpoints\"]\nprint(len(endpoints))\n# 22\n```\n\n\n### Stack trace\n\nNo stacktrace from an exception, just unexpected results.\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [x] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [x] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "Hi @dennis-johnson-dev \n\nThanks for reporting this!\n\naccording to the mlflow API `list_deployments` definition https://github.com/mlflow/mlflow/blob/af867687d26bbaca9580e0f65a1905a6b91f410d/mlflow/deployments/base.py#L168 , it should return the entire list of the endpoints. Would you file a PR for it ? "
      },
      {
        "user": "dennis-johnson-dev",
        "body": "Sure thing, I'll start working on a PR to address it. Thanks!"
      },
      {
        "user": "dennis-johnson-dev",
        "body": "@WeichenXu123 Thank you for taking a look. I have started a PR here to address the issue - https://github.com/mlflow/mlflow/pull/15866"
      }
    ]
  },
  {
    "issue_number": 15862,
    "title": "Remove mlflow.db in repository root",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-23T13:13:21Z",
    "updated_at": "2025-05-23T13:29:28Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "mlflow.db was pushed accidentally. Let's remove it.",
    "comments": []
  },
  {
    "issue_number": 15853,
    "title": "Remove try-catch block in update-status step of autoformat workflow",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-23T06:38:16Z",
    "updated_at": "2025-05-23T09:01:53Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "In the `.github/workflows/autoformat.yml` file, there is a TODO comment in the `update-status` job:\n\n```js\n// TODO: Remove try-catch block once we are confident that the code works fine.\ntry {\n  const push_head_sha = '${{ needs.push.outputs.head_sha }}';\n  if (push_head_sha) {\n    await autoformat.approveWorkflowRuns(context, github, push_head_sha);\n  }\n} catch (error) {\n  core.warning(`Failed to approve workflow runs: ${error}`);\n}\n```\n\nWe should monitor the workflow and, once we are confident in its stability, remove the try-catch block as indicated by the TODO. This will help keep the workflow clean and maintainable.",
    "comments": []
  },
  {
    "issue_number": 15851,
    "title": "Unpin OpenAI",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-23T06:23:31Z",
    "updated_at": "2025-05-23T08:59:42Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "https://pypi.org/project/llama-index-llms-openai/0.3.44 has been released. Let's remove the pin for OpenAI added by https://github.com/mlflow/mlflow/pull/15846.\n\n\n",
    "comments": []
  },
  {
    "issue_number": 15847,
    "title": "PIn pyspark to < 4.0",
    "author": "harupy",
    "state": "closed",
    "created_at": "2025-05-23T04:29:39Z",
    "updated_at": "2025-05-23T05:30:01Z",
    "labels": [],
    "body": "pyspark 4.0.0 has been released but we're not ready for that. Let's add a pin in requirements/constraints.txt.",
    "comments": []
  },
  {
    "issue_number": 15306,
    "title": "[FR] Passing params for inference in pytorch",
    "author": "MarkBusschers",
    "state": "closed",
    "created_at": "2025-04-14T12:27:55Z",
    "updated_at": "2025-05-23T04:12:44Z",
    "labels": [
      "enhancement",
      "integrations/databricks",
      "has-closing-pr",
      "area/deployments"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nDuring inference, I want to pass parameters to my custom pytorch model. I am able to log the model with mlflow.pytorch.log_model() and have written a custom forward method that requires input array 'x' and dictionary 'params'. When making an inference call, in the _PyTorchWrapper class of the mlflow/pytorch/__init__.py file, params are not passed to the forward call: preds = self.pytorch_model(input_tensor). Is it possible to change this to preds = self.pytorch_model(input_tensor, params=params)\n\n### Motivation\n\n> #### What is the use case for this feature?\nEnabling model serving for custom pytorch models\n> #### Why is this use case valuable to support for MLflow users in general?\nEnabling model serving for custom pytorch models\n> #### Why is this use case valuable to support for your project(s) or organization?\nThis enables us to log a NeuralProphet model and serve the model using databricks\n> #### Why is it currently difficult to achieve this use case?\nWe are unable to log a NeuralProphet model directly (no integrated package in mlflow). When we are enabled to pass extra parameters for inference, we are able to implement a workaround when logging a model with pytorch.\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [x] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [x] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "BenWilson2",
        "body": "@serena-ruan was there a reason why we explicitly didn't pass `params` to this callable instance? "
      },
      {
        "user": "MarkBusschers",
        "body": "Hi @serena-ruan could you please help me further with this issue?"
      },
      {
        "user": "MarkBusschers",
        "body": "@BenWilson2 Is there someone else to help us out?"
      }
    ]
  },
  {
    "issue_number": 15808,
    "title": "[BUG] 25/05/20 07:02:56 ERROR ReplAwareSparkDataSourceListener: Unexpected exception when attempting to handle SparkListenerSQLExecutionEnd event. Please report this error, along with the following stacktrace, on https://github.com/mlflow/mlflow/issues:",
    "author": "panwroblewski",
    "state": "open",
    "created_at": "2025-05-20T07:04:45Z",
    "updated_at": "2025-05-22T12:01:01Z",
    "labels": [
      "bug"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\njava.lang.RuntimeException: Unable to find method with name tableVersion of object with class com.databricks.sql.transaction.tahoe.files.TahoeBatchFileIndex. Available methods: [com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$$filePathToInScanIdMap, generateFileInScanId, generateFileInScanId, partitionFiltersGenerated, com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$$filesByInScanId, com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$$super$fileStatusWithMetadataFromAddFile, shouldGenerateFileInScanId, generateFileInScanIdSynchronized, com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$_setter_$generateFileInScanId_$eq, com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$_setter_$com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$$filePathToInScanIdMap_$eq, com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$_setter_$com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$$filesByInScanId_$eq, super$spark, $anonfun$matchingFiles$1, inputFiles, $anonfun$inputFiles$2, actionType, rowIndexFilters, targetFileSizeOption, addFiles, getFilesByInScanId, sizeInBytes$lzycompute, $anonfun$sizeInBytes$1$adapted, $anonfun$sizeInBytes$1, fileStatusWithMetadataFromAddFile, sizeInBytes, matchingFiles, $deserializeLambda$, snapshot, refresh, $lessinit$greater$default$7, $lessinit$greater$default$8, $lessinit$greater$default$9, $lessinit$greater$default$10, $anonfun$new$1, $anonfun$new$2, $anonfun$new$1$adapted, $anonfun$getNumOfFilesAndBytesPruned$1, $anonfun$getNumOfFilesAndBytesPruned$2, getNumOfFilesAndBytesPruned, numOfFilesIfKnown, sizeInBytesIfKnown, version, protocol, $deserializeLambda$, metadata, rootPaths, deltaLog, partitionSchema, listPartitionDirectoriesAndFiles, metadataOpsTimeNs, rowIndexFilters, getRowIndexFilterForFile, targetFileSizeOption, makePartitionDirectories, getNumOfFilesAndBytesPruned, listFilesSortedByInsertionTime, listPartitionsAsAddFiles, isCatalogOwned, useRelativePathsInFileListing, rowIndexFiltersDefinedForAllFiles, com$databricks$sql$transaction$tahoe$files$SupportsRowIndexFilters$_setter_$rowIndexFiltersDefinedForAllFiles_$eq, convertPartitionsToInternalRow, $anonfun$convertPartitionsToInternalRow$1, $anonfun$listPartitionsAsAddFiles$1, listAddFiles, $anonfun$listPartitionDirectoriesAndFiles$1, $anonfun$listFilesSortedByInsertionTime$1, $anonfun$listFilesSortedByInsertionTime$4$adapted, closeGroup$1, $anonfun$listAddFiles$1, useRelativePathsInFileListing$lzycompute, $anonfun$useRelativePathsInFileListing$1, fileStatusWithMetadataFromAddFile, $anonfun$fileStatusWithMetadataFromAddFile$1, $anonfun$fileStatusWithMetadataFromAddFile$2$adapted, $anonfun$fileStatusWithMetadataFromAddFile$3$adapted, $anonfun$fileStatusWithMetadataFromAddFile$4, $anonfun$makePartitionDirectories$1, getPartitionValuesRow, $anonfun$getPartitionValuesRow$1, $anonfun$listFilesSortedByInsertionTime$2, $anonfun$listFilesSortedByInsertionTime$3, $anonfun$listFilesSortedByInsertionTime$4, $anonfun$fileStatusWithMetadataFromAddFile$2, $anonfun$fileStatusWithMetadataFromAddFile$3, $anonfun$makePartitionDirectories$2, truncateRight, schema, getBasePath, absolutePath, matchingFiles, toString, path, $deserializeLambda$, listFiles, spark, finalize, wait, wait, wait, equals, toString, hashCode, getClass, clone, notify, notifyAll]\n\tat org.mlflow.spark.autologging.ReflectionUtils$.$anonfun$callMethod$2(ReflectionUtils.scala:51)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.mlflow.spark.autologging.ReflectionUtils$.callMethod(ReflectionUtils.scala:49)\n\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.$anonfun$maybeGetDeltaTableInfo$1(DatasourceAttributeExtractor.scala:112)\n\tat scala.Option.map(Option.scala:230)\n\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.maybeGetDeltaTableInfo(DatasourceAttributeExtractor.scala:110)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfoToLog(DatasourceAttributeExtractor.scala:47)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfoToLog$(DatasourceAttributeExtractor.scala:46)\n\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfoToLog(DatasourceAttributeExtractor.scala:103)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.$anonfun$getTableInfos$1(DatasourceAttributeExtractor.scala:89)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:89)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)\n\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)\n\tat org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)\n\tat org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)\n\tat org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1643)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\n- **Python version**:\n- **yarn version, if running the dev UI**:\n\n\n### Describe the problem\n\njava.lang.RuntimeException: Unable to find method with name tableVersion of object with class com.databricks.sql.transaction.tahoe.files.TahoeBatchFileIndex. Available methods: [com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$$filePathToInScanIdMap, generateFileInScanId, generateFileInScanId, partitionFiltersGenerated, com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$$filesByInScanId, com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$$super$fileStatusWithMetadataFromAddFile, shouldGenerateFileInScanId, generateFileInScanIdSynchronized, com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$_setter_$generateFileInScanId_$eq, com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$_setter_$com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$$filePathToInScanIdMap_$eq, com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$_setter_$com$databricks$sql$transaction$tahoe$SupportsDeltaFileInScanId$$filesByInScanId_$eq, super$spark, $anonfun$matchingFiles$1, inputFiles, $anonfun$inputFiles$2, actionType, rowIndexFilters, targetFileSizeOption, addFiles, getFilesByInScanId, sizeInBytes$lzycompute, $anonfun$sizeInBytes$1$adapted, $anonfun$sizeInBytes$1, fileStatusWithMetadataFromAddFile, sizeInBytes, matchingFiles, $deserializeLambda$, snapshot, refresh, $lessinit$greater$default$7, $lessinit$greater$default$8, $lessinit$greater$default$9, $lessinit$greater$default$10, $anonfun$new$1, $anonfun$new$2, $anonfun$new$1$adapted, $anonfun$getNumOfFilesAndBytesPruned$1, $anonfun$getNumOfFilesAndBytesPruned$2, getNumOfFilesAndBytesPruned, numOfFilesIfKnown, sizeInBytesIfKnown, version, protocol, $deserializeLambda$, metadata, rootPaths, deltaLog, partitionSchema, listPartitionDirectoriesAndFiles, metadataOpsTimeNs, rowIndexFilters, getRowIndexFilterForFile, targetFileSizeOption, makePartitionDirectories, getNumOfFilesAndBytesPruned, listFilesSortedByInsertionTime, listPartitionsAsAddFiles, isCatalogOwned, useRelativePathsInFileListing, rowIndexFiltersDefinedForAllFiles, com$databricks$sql$transaction$tahoe$files$SupportsRowIndexFilters$_setter_$rowIndexFiltersDefinedForAllFiles_$eq, convertPartitionsToInternalRow, $anonfun$convertPartitionsToInternalRow$1, $anonfun$listPartitionsAsAddFiles$1, listAddFiles, $anonfun$listPartitionDirectoriesAndFiles$1, $anonfun$listFilesSortedByInsertionTime$1, $anonfun$listFilesSortedByInsertionTime$4$adapted, closeGroup$1, $anonfun$listAddFiles$1, useRelativePathsInFileListing$lzycompute, $anonfun$useRelativePathsInFileListing$1, fileStatusWithMetadataFromAddFile, $anonfun$fileStatusWithMetadataFromAddFile$1, $anonfun$fileStatusWithMetadataFromAddFile$2$adapted, $anonfun$fileStatusWithMetadataFromAddFile$3$adapted, $anonfun$fileStatusWithMetadataFromAddFile$4, $anonfun$makePartitionDirectories$1, getPartitionValuesRow, $anonfun$getPartitionValuesRow$1, $anonfun$listFilesSortedByInsertionTime$2, $anonfun$listFilesSortedByInsertionTime$3, $anonfun$listFilesSortedByInsertionTime$4, $anonfun$fileStatusWithMetadataFromAddFile$2, $anonfun$fileStatusWithMetadataFromAddFile$3, $anonfun$makePartitionDirectories$2, truncateRight, schema, getBasePath, absolutePath, matchingFiles, toString, path, $deserializeLambda$, listFiles, spark, finalize, wait, wait, wait, equals, toString, hashCode, getClass, clone, notify, notifyAll]\n\tat org.mlflow.spark.autologging.ReflectionUtils$.$anonfun$callMethod$2(ReflectionUtils.scala:51)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.mlflow.spark.autologging.ReflectionUtils$.callMethod(ReflectionUtils.scala:49)\n\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.$anonfun$maybeGetDeltaTableInfo$1(DatasourceAttributeExtractor.scala:112)\n\tat scala.Option.map(Option.scala:230)\n\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.maybeGetDeltaTableInfo(DatasourceAttributeExtractor.scala:110)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfoToLog(DatasourceAttributeExtractor.scala:47)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfoToLog$(DatasourceAttributeExtractor.scala:46)\n\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfoToLog(DatasourceAttributeExtractor.scala:103)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.$anonfun$getTableInfos$1(DatasourceAttributeExtractor.scala:89)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos(DatasourceAttributeExtractor.scala:89)\n\tat org.mlflow.spark.autologging.DatasourceAttributeExtractorBase.getTableInfos$(DatasourceAttributeExtractor.scala:85)\n\tat org.mlflow.spark.autologging.ReplAwareDatasourceAttributeExtractor$.getTableInfos(DatasourceAttributeExtractor.scala:142)\n\tat org.mlflow.spark.autologging.ReplAwareSparkDataSourceListener.onSQLExecutionEnd(ReplAwareSparkDataSourceListener.scala:49)\n\tat org.mlflow.spark.autologging.SparkDataSourceListener.$anonfun$onOtherEvent$1(SparkDataSourceListener.scala:39)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.mlflow.spark.autologging.ExceptionUtils$.tryAndLogUnexpectedError(ExceptionUtils.scala:26)\n\tat org.mlflow.spark.autologging.SparkDataSourceListener.onOtherEvent(SparkDataSourceListener.scala:39)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1643)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "Could you provide code to repro?\ncc @WeichenXu123 I see this emits from https://github.com/mlflow/mlflow/blob/d4c906e4b76c425a027badbb146b015983e4e36b/mlflow/java/spark/src/main/scala/org/mlflow/spark/autologging/ExceptionUtils.scala#L17 Do we need this?"
      },
      {
        "user": "steffenbeermann",
        "body": "I  have the same issue with a databricks streaming job. It run without issues for month now and since a few days we have the issues with the same error.\n\nThis is the code of the streaming job\n```\nsubscription_id = ***\nresource_group = ***\nclient_id = ***\nclient_secret = ***\ntenant_id = ***\n\ndf_stream = (\n    spark.readStream.format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"parquet\")\n    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n    .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n    .option(\"cloudFiles.useNotifications\", \"true\")\n    .option(\"cloudFiles.includeExistingFiles\", \"false\")\n    .option(\"cloudFiles.resourceGroup\", resource_group)\n    .option(\"cloudFiles.subscriptionId\", subscription_id)\n    .option(\"cloudFiles.tenantId\", tenant_id)\n    .option(\"cloudFiles.clientId\", client_id)\n    .option(\"cloudFiles.clientSecret\", client_secret)\n    .option(\"cloudFiles.maxFilesPerTrigger\", 20)\n    .load(external_location)\n    .withColumn(\"file_path\", f.col(\"_metadata.file_path\"))\n    .withColumn(\"ts\", f.col(\"ts\").cast(\"string\"))\n    .withColumn(\"value\", f.col(\"measurement_value\").cast(\"string\"))\n    .withColumn(\"ingestion_type\", f.lit(\"live\"))\n    .selectExpr(\n        \"plant\",\n        \"control_system_identifier\",\n        \"value\",\n        \"ts\",\n        \"file_path\",\n        \"ingestion_type\",\n    )\n)\ndf_stream.writeStream.option(\"checkpointLocation\", checkpoint_path)\n        .option(\"mergeSchema\", \"true\")\n        .toTable(output_table_path)\n```\n\nthe stream is writing to a table that is configured to use liquid clustering"
      },
      {
        "user": "serena-ruan",
        "body": "Seems like an issue with spark? There's no Mlflow involved in your case"
      }
    ]
  },
  {
    "issue_number": 15813,
    "title": "MLflow projects using docker",
    "author": "nafiz09",
    "state": "open",
    "created_at": "2025-05-20T11:26:07Z",
    "updated_at": "2025-05-22T08:28:57Z",
    "labels": [],
    "body": "while trying the run the example/docker part for MLflow projects using docker,  I encountered an error and haven't been able to find a solution.\n\n2025/05/20 16:48:48 INFO mlflow.projects.docker: mlflow-docker-example already exists\n2025/05/20 16:48:49 INFO mlflow.projects.utils: === Created directory C:\\Users\\ssclb\\AppData\\Local\\Temp\\tmpmweth8r5 for downloading remote URIs passed to arguments of type 'path' ===\n2025/05/20 16:48:49 INFO mlflow.projects.backend.local: === Running command 'docker run --rm -v D:\\mlflow_projects\\mlruns:/mlflow/tmp/mlruns -v D:\\mlflow_projects\\mlruns\\0\\da8a1a8313ca44998aa3225f3f1bf5c0\\artifacts:D:\\mlflow_projects\\mlruns\\0\\da8a1a8313ca44998aa3225f3f1bf5c0\\artifacts -e MLFLOW_RUN_ID=da8a1a8313ca44998aa3225f3f1bf5c0 -e MLFLOW_TRACKING_URI=file:///D:/mlflow/tmp/mlruns -e MLFLOW_EXPERIMENT_ID=0 mlflow-docker-example:latest python train.py --alpha 0.5 --l1-ratio 0.1' in run with ID 'da8a1a8313ca44998aa3225f3f1bf5c0' ===\ndocker: Error response from daemon: invalid mode: \\mlflow_projects\\mlruns\\0\\da8a1a8313ca44998aa3225f3f1bf5c0\\artifacts.\nSee 'docker run --help'.\n2025/05/20 16:48:49 ERROR mlflow.cli: === Run (ID 'da8a1a8313ca44998aa3225f3f1bf5c0') failed ===",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "Which MLflow version are you using? Could you provide full stacktrace?"
      },
      {
        "user": "nafiz09",
        "body": "> Which MLflow version are you using? Could you provide full stacktrace?\n\nmlflow, version 2.15.1\nPython 3.13.1\nWindows 11"
      },
      {
        "user": "serena-ruan",
        "body": "That is quite old, could you try upgrading mlflow and try again? If it doesn't work pls provide full stacktrace"
      }
    ]
  },
  {
    "issue_number": 14214,
    "title": "[FR] Support AMD GPU system metrics",
    "author": "kaixin96",
    "state": "closed",
    "created_at": "2025-01-09T03:43:50Z",
    "updated_at": "2025-05-22T06:08:01Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\n\n### Proposal Summary\n\nCurrent GPUMonitor relies on pynvml, which only supports Nvidia GPUs. Adding AMD GPU support would be a helpful plus\n\n### Motivation\n\n> #### What is the use case for this feature?\r\n\r\nMonitor AMD GPU usage under system metrics\r\n\r\n> #### Why is this use case valuable to support for MLflow users in general?\r\n\r\nAMD GPUs are quite common now. PyTorch and many other packages already support AMD GPUs and ROCm. A lot of MLflow users would benefit from this feature.\r\n\r\n> #### Why is this use case valuable to support for your project(s) or organization?\r\n\r\nI'm using azure & azure-mlflow. This use case would be very helpful for MI300 cluster users.\r\n\r\n> #### Why is it currently difficult to achieve this use case?\r\n\r\nI don't see any particular difficulties to achieve this. Simply extending existing api to support amdsmi would work.\r\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "@kaixin96 Makes sense. Added `help wanted` label for a volunteer."
      },
      {
        "user": "mayijun",
        "body": "@kaixin96 i had a workaround before by monkey patching the Mlflow GPU monitoring code, as my org use NVidia MIG enabled GPU, but current GPU monitor code is not compatible to Nvidia MIG. i also added torch cuda memory into system metrics. so if you want you can do thing similar to me.\r\n\r\n```\r\nimport logging\r\nimport sys\r\nfrom mlflow.system_metrics.metrics.base_metrics_monitor import BaseMetricsMonitor\r\nimport torch\r\n\r\n_logger = logging.getLogger(__name__)\r\n\r\ntry:\r\n    import pynvml\r\nexcept ImportError:\r\n    pass\r\n\r\nclass CustomGPUMonitor(BaseMetricsMonitor):\r\n    \"\"\"Custom class for monitoring GPU stats, including MIG devices.\"\"\"\r\n\r\n    def __init__(self):\r\n        if \"pynvml\" not in sys.modules:\r\n            raise ImportError(\r\n                \"`pynvml` is not installed, to log GPU metrics please run `pip install pynvml` \"\r\n                \"to install it.\"\r\n            )\r\n        try:\r\n            pynvml.nvmlInit()\r\n        except pynvml.NVMLError as e:\r\n            raise RuntimeError(f\"Failed to initialize NVML, skip logging GPU metrics: {e}\")\r\n\r\n        super().__init__()\r\n        self.num_gpus = pynvml.nvmlDeviceGetCount()\r\n        self._init_mig_devices()\r\n\r\n    def _init_mig_devices(self):\r\n        # Initialize MIG device handles\r\n        self.mig_device_handles = []\r\n        for i in range(self.num_gpus):\r\n            device_handle = pynvml.nvmlDeviceGetHandleByIndex(i)\r\n            for j in range(pynvml.nvmlDeviceGetMaxMigDeviceCount(device_handle)):\r\n                try:\r\n                    mig_device_handle = pynvml.nvmlDeviceGetMigDeviceHandleByIndex(device_handle, j)\r\n                    self.mig_device_handles.append((i, j, mig_device_handle))\r\n                except pynvml.NVMLError_NotFound:\r\n                    break\r\n\r\n    def collect_metrics(self):\r\n        for device_index, mig_index, handle in self.mig_device_handles:\r\n            try:\r\n                memory = pynvml.nvmlDeviceGetMemoryInfo(handle)\r\n                self._metrics[f\"gpu_{device_index}_mig_{mig_index}_memory_usage_percentage\"].append(\r\n                    round(memory.used / memory.total * 100, 1)\r\n                )\r\n                self._metrics[f\"gpu_{device_index}_mig_{mig_index}_memory_usage_megabytes\"].append(\r\n                    memory.used / 1e6\r\n                )\r\n                self._metrics[f\"torch_allocated_memory_GB\"].append(\r\n                     round(torch.cuda.memory_allocated() / 1024 / 1024 / 1024, 3)\r\n                )\r\n                self._metrics[f\"torch_max_memory_reserved_GB\"].append(\r\n                     round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\r\n                )\r\n            except pynvml.NVMLError as e:\r\n                _logger.warning(f\"Encountered error {e} when trying to collect GPU metrics for \"\r\n                                f\"GPU {device_index}, MIG {mig_index}.\")\r\n\r\n    def aggregate_metrics(self):\r\n        return {k: round(sum(v) / len(v), 1) for k, v in self._metrics.items()}\r\n \r\nimport mlflow\r\nmlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))\r\n\r\nimport mlflow.system_metrics.metrics.gpu_monitor as original_gpu_module\r\noriginal_gpu_module.GPUMonitor = CustomGPUMonitor\r\nmlflow.enable_system_metrics_logging()\r\n```\r\n\r\n\r\n<img width=\"950\" alt=\"image\" src=\"https://github.com/user-attachments/assets/650250ac-4452-4192-8de1-d505a2099862\" />\r\n\r\n\r\nI use monkey patch as the Mlflow GPUmonitor class or interface is currently not exposed to user, but i believe it should be exposed , as it's highly related to hardware platform. end user need to do customization according to his/her hardware.\r\n**what we need is not supporting specific hardware by default, but a way to customize such GPU monitor class**"
      },
      {
        "user": "ffund",
        "body": "I noticed that this is implemented in #12694 "
      }
    ]
  },
  {
    "issue_number": 15454,
    "title": "[FR] Add hierarchy in the model registry (add the notion of a parent model)",
    "author": "Flametaa",
    "state": "closed",
    "created_at": "2025-04-23T17:27:35Z",
    "updated_at": "2025-05-21T17:33:04Z",
    "labels": [
      "enhancement",
      "area/model-registry"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nMake it possible to group models under a \"parent model\", this will allow us to have a tree hierarchy for models (similar to what we have for runs).\nThis will be very helpful for LoRAs, and for a better organization of our models\n\n### Motivation\n\n> #### What is the use case for this feature?\nBetter organization for our models, and better support for finetuned models for different users.\n> #### Why is this use case valuable to support for MLflow users in general?\nImprove model organization and UI experience\n> #### Why is this use case valuable to support for your project(s) or organization?\nBetter organization of LoRAs\n> #### Why is it currently difficult to achieve this use case?\nWe are training the same model on different data for each customer. We currently have to either save all models as model versions which doesn't scale very well, or create a separate model for each client which makes the UI and the model search experience not great. \n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [x] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "@Flametaa The feature and use case makes sense, however, this requires a large UI change on Model Registry. Have you tried adding tags to the child models, with which you can search all child models for a particular base model? \n\nhttps://mlflow.org/docs/latest/model-registry/#deploy-and-organize-models-with-aliases-and-tags\n\nIndeed, Run's parent-child relationship is also recoded as tags. Therefore technically it should provide the same functionality, except the built-in tree view on UI."
      },
      {
        "user": "Flametaa",
        "body": "@B-Step62 Yes that was the workaround that I was going for. But it would be really great to have this feature in the UI. Do you think the change is really that large?\nBtw there is also a scaling issue in the model registry UI that i described in this [issue](https://github.com/mlflow/mlflow/issues/15384) , It would be great to get your feedback for it as well!"
      },
      {
        "user": "daniellok-db",
        "body": "@Flametaa could you explain more about what the grouping criteria would be in your use-case? I'm curious why it's not possible to use a registered model as the \"parent\", and individual model versions as the \"child\". Or is the ask to have the ability to further group model versions?"
      }
    ]
  },
  {
    "issue_number": 15295,
    "title": "[FR] Allow langgraph ChatAgentToolNode to parse artifacts from langchain tools",
    "author": "georgemarlow",
    "state": "open",
    "created_at": "2025-04-13T20:24:54Z",
    "updated_at": "2025-05-21T09:08:12Z",
    "labels": [
      "enhancement",
      "area/models"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nNewer versions of langchain allow you to specify `response_format: Literal[\"content\", \"content_and_artifact\"]` when creating tools. These produce tool messages with an additional `artifact` field: https://python.langchain.com/docs/concepts/messages/#toolmessage. The artifact field is helpful for ensuring any metadata from the tool is returned without being passed to the LLM in the agent\n\nWhen we use ChatAgentToolNode it will parse a dictionary with keys: \"content\", \"attachments\" and \"custom_outputs\" but it would improve the dx if it could parse the `artifact` attribute of the ToolMessage and allow us to store a serialized version in the attachments or similar\n\n\nThe current solution I have is to create a CustomChatAgentToolNode, similar to ChatAgentToolNode with the following change:\n\nOriginal `invoke` method:\n ```python\n    def invoke(self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any) -> Any:\n        \"\"\"\n        Wraps the standard ToolNode invoke method to:\n        - Parse ChatAgentState into LangChain messages\n        - Parse dictionary string outputs from both UC function and standard LangChain python tools\n          that include keys ``content``, ``attachments``, and ``custom_outputs``.\n        \"\"\"\n        messages = input[\"messages\"]\n        for msg in messages:\n            for tool_call in msg.get(\"tool_calls\", []):\n                tool_call[\"name\"] = tool_call[\"function\"][\"name\"]\n                tool_call[\"args\"] = json.loads(tool_call[\"function\"][\"arguments\"])\n        input[\"messages\"] = convert_to_messages(messages)\n\n        result = super().invoke(input, config, **kwargs)\n\n        messages = []\n        custom_outputs = None\n        for m in result[\"messages\"]:\n            try:\n                return_obj = json.loads(m.content)\n                if all(key in return_obj for key in (\"format\", \"value\", \"truncated\")):\n                    # Dictionary output with custom_outputs and attachments from a UC function\n                    try:\n                        return_obj = json.loads(return_obj[\"value\"])\n                    except Exception:\n                        pass\n                if \"custom_outputs\" in return_obj:\n                    custom_outputs = return_obj[\"custom_outputs\"]\n                if m.id is None:\n                    m.id = str(uuid4())\n                messages.append(parse_message(m, attachments=return_obj.get(\"attachments\")))\n            except Exception:\n                messages.append(parse_message(m))\n        return {\"messages\": messages, \"custom_outputs\": custom_outputs}\n```\n\nModified last part of invoke method:\n```python\n        except Exception as e:\n            messages.append(\n                parse_message(\n                    m,\n                    attachments={\"documents\": json.dumps([doc.metadata for doc in m.artifact])},\n                )\n            )\n```\n\n\n### Motivation\n\n> #### What is the use case for this feature?\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\n> #### Why is it currently difficult to achieve this use case?\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "> attachments={\"documents\": json.dumps([doc.metadata for doc in m.artifact])},\n\nDo you need to store the whole artifact field as attachments, or you want to use a custom function to extract certain artifact fields ?"
      },
      {
        "user": "georgemarlow",
        "body": "For my use case the tool is a document retriever so the artifact is a list of langchain Document objects (https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html). From that I only want to take the metadata which is a dictionary with information like the document ID, URL, author, publication date etc. Also I wasn't able to json serialize the Document directly so a custom function there was helpful to select the content that I wanted and convert it to a `dict[str, str]` for the mlflow type. I'm not sure if there are other use cases for this though!"
      },
      {
        "user": "WeichenXu123",
        "body": "yea I think we need a \"custom function\" as the argument. We need to discuss internally about the API."
      }
    ]
  },
  {
    "issue_number": 15632,
    "title": "[BUG] FastAPI server does not use GPU under invocations call",
    "author": "hermda02",
    "state": "open",
    "created_at": "2025-05-08T13:37:28Z",
    "updated_at": "2025-05-21T08:52:13Z",
    "labels": [
      "bug",
      "area/docker",
      "area/deployments"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.21.3\n- Tracking server: 2.21.3\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 22.04\n- **Python version**: 3.10\n- **yarn version, if running the dev UI**:\n\n\n### Describe the problem\n\nI've created a Dockerfile using the CLI to create a docker image for serving an in-house model. Everything works as expected, however the GPU is never used when the invocations are called.\n\nWhile creating the container I use the `--gpus all` argument, which works as I have access to `nvidia-smi` within the container. When running the invocations, I see that no processes are utilized by the GPU. Is this expected? If not, what is the correct method for utilizing the GPU for inference? I have scoured the docs and source code to no avail.\n\n\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nmlflow models generate-dockerfile -m models:/test_model/1\ndocker build -t test_image .\ndocker run -itd --name mlflow_server -p 5002:8080 -e DISABLE_NGINX=true --gpus all test_image\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [x] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [x] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "Hi, @hermda02. Can you share more details about your model? Did your model use GPUs if they are run on GPU compute machines?"
      },
      {
        "user": "hermda02",
        "body": "The model itself does not define any device handling, but is handled at the training step. The model used has been constructed using Pytorch Lightning.\n\nExplicitly, for training my UNet model implementation:\n```\ndatamodule = DataModulel(args)\nmodel = UNet(args)\nmodel.to(device)\ntrainer = Lightning.Trainer(args)\nmlflwo.pytorch.autotlog()\nwith mlflow.start_run() as run\n    trainer.fit(model, datamodule=datamodule)\n```\n\nDoes the device handling have to be handled internally within the defined model?"
      },
      {
        "user": "hermda02",
        "body": "Little update:\nI tested inference locally following the python code on the experiments page of the MLFlow tracking server and I can verify that `mlflow.model.predict` does utilize the GPU with the \"uv\" environment manager."
      }
    ]
  },
  {
    "issue_number": 15461,
    "title": "[BUG] \"mlflow gc\" does not check if artifact really exists before calling deletion causing issues when using proxied artifact serving",
    "author": "niander",
    "state": "open",
    "created_at": "2025-04-23T22:54:50Z",
    "updated_at": "2025-05-21T06:15:10Z",
    "labels": [
      "bug",
      "area/artifacts",
      "area/server-infra"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nOther\n\n### MLflow version\n\n- Client: 2.21.3\n- Tracking server: 2.21.3\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\n- **Python version**: 3.11\n- **yarn version, if running the dev UI**: N/A\n\n\n### Describe the problem\n\nI have a tracking server that is using an Azure Blob Storage with artifact proxying and local directory as backend metadata. When running `mlflow gc` in the same machine where the tracking server is running the code is timing out when calling the local host HTTP server for the artifact deletion (`.../api/2.0/mlflow-artifacts/artifacts....`).\n\nI went ahead and tried to do the same thing that `mlflow gc` is doing, which is using `get_artifact_repository(run.info.artifact_uri)` and calling `artifact_repository.delete_artifacts()` and I was able to get the same issue. It hangs until timeout with the error saying retry failed with too many 500.\n\nThe reason seems to be that the artifact folder in the Azure Blob Storage really doesn't exists because the run didn't have any artifact anyways. However, the `mlflow gc` command tried to delete anyways.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nurllib3.exceptions.ResponseError: too many 500 error responses\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 942, in urlopen\n    return self.urlopen(\n           ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 942, in urlopen\n    return self.urlopen(\n           ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 942, in urlopen\n    return self.urlopen(\n           ^^^^^^^^^^^^^\n  [Previous line repeated 4 more times]\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 932, in urlopen\n    retries = retries.increment(method, url, response=response, _pool=self)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/urllib3/util/retry.py\", line 519, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow-artifacts/artifacts/922075866158470819/79433a41c3764469bdf85932f0c63cf7/artifacts/ (Caused by ResponseError('too many 500 error responses'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/mlflow/utils/rest_utils.py\", line 181, in http_request\n    return _get_http_response_with_retries(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/mlflow/utils/request_utils.py\", line 237, in _get_http_response_with_retries\n    return session.request(method, url, allow_redirects=allow_redirects, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/requests/adapters.py\", line 691, in send\n    raise RetryError(e, request=request)\nrequests.exceptions.RetryError: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow-artifacts/artifacts/922075866158470819/79433a41c3764469bdf85932f0c63cf7/artifacts/ (Caused by ResponseError('too many 500 error responses'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/bin/mlflow\", line 8, in <module>\n    sys.exit(cli())\n             ^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 1161, in __call__\n    return self.main(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 1082, in main\n    rv = self.invoke(ctx)\n         ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 1697, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 1443, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 788, in invoke\n    return __callback(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/mlflow/cli.py\", line 627, in gc\n    artifact_repo.delete_artifacts()\n  File \"/usr/local/lib/python3.11/site-packages/mlflow/store/artifact/http_artifact_repo.py\", line 113, in delete_artifacts\n    resp = http_request(self._host_creds, endpoint, \"DELETE\", stream=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/mlflow/utils/rest_utils.py\", line 204, in http_request\n    raise MlflowException(f\"API request to {url} failed with exception {e}\")\nmlflow.exceptions.MlflowException: API request to http://localhost:5000/api/2.0/mlflow-artifacts/artifacts/922075866158470819/79433a41c3764469bdf85932f0c63cf7/artifacts/ failed with exception HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/2.0/mlflow-artifacts/artifacts/922075866158470819/79433a41c3764469bdf85932f0c63cf7/artifacts/ (Caused by ResponseError('too many 500 error responses'))\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [x] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "@niander Thank you for bringing this issue to our attention.\n\nIt seems there are a few different issues involved:\n* `gc` command only catches `InvalidUrlException` and not resource not exist case. Since it is possible that a Run doesn't contain any artifact, that case should be handled gracefully.\n* The proxy tracking server returns 500 when artifact deletion request is made to non-existing artifacts. It should return 404 instead so that the client can handle it properly.\n\nWe will work on the fix soon."
      },
      {
        "user": "niander",
        "body": "@B-Step62 I tried to see if I could contribute with a fix, but I found a bit confusing where exactly the fix needs to go since the artifact repo manager could be the proxied one or the actual artifact repo. But I am interested in seeing this fix. Please tag me if you work on it. Thanks!"
      },
      {
        "user": "niander",
        "body": "@B-Step62 Has this been fixed in the new release?"
      }
    ]
  },
  {
    "issue_number": 15802,
    "title": "[BUG] UI only displays run nesting when \"Sort: Created\" is selected",
    "author": "mazer-ai",
    "state": "open",
    "created_at": "2025-05-19T23:35:29Z",
    "updated_at": "2025-05-21T02:46:23Z",
    "labels": [
      "bug",
      "area/uiux"
    ],
    "body": "### MLflow version\n\n2.22.0\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 24.04 LTS\n- **Python version**: 3.12.3\n- **yarn version, if running the dev UI**: N/A\n\n\n### Describe the problem\n\nThe nesting structure for nested runs is only displayed when the sort is set to \"Sort: Created\". For any other sort option, the nesting icon (+'s in boxes) goes away and the nested runs are displayed as top-level runs Compare these two screen shots of the same runs. First with Sort: Created, second with Sort: User:\n\n<img width=\"455\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bd85d2ea-f876-4c1f-8f79-22936d4c8683\" />\n\nvs\n\n<img width=\"451\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5f7e7b7e-45d5-496b-b18a-22b3ac3671bb\" />\n\nPerhaps this is intentional, but severely limits the utility of nesting if you can't keep nested runs together as a unit when sorting based on a parameter or metric. My intuition is that sort should only be applied to the parent run and the children go along for the ride and remain nested...\n\n### Steps to reproduce the bug\n\nCreated a nested run and select any sort option in the GUI for the experiment other than Created. See screenshots above.\n\n\n### Code to generate data required to reproduce the bug\n\nN/A\n\n### Is the console panel in DevTools showing errors relevant to the bug?\n\nN/A\n\n### Does the network panel in DevTools contain failed requests relevant to the bug?\n\nN/A",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "Thanks for reporting this @mazer-ai, I can verify this behavior.\nCode to repro:\n```\nimport mlflow\n\nwith mlflow.start_run():\n    for i in range(5):\n        with mlflow.start_run(nested=True):\n            mlflow.log_param(\"key\", str(i))\n```\n\nThe UI just sends request to search_runs, and in the backend we don't differentiate between child runs or parent runs when ordering.\nCould you elaborate your use case more on when would you expect to order by only parent runs?"
      },
      {
        "user": "mazer-ai",
        "body": "The use case for me is that the parent run is a training run that trains a model and the nested runs are analysis runs that push data through the model and save results of the analysis to child runs. This is a common pattern for me and my colleagues. The issue comes up if you try to do something like sort the parent runs based on performance (say training accuracy) and the child runs, pop out into the main list.\n\nSeems (to me) like the right thing is for the UI should always used the nesting display, regardless of the sort key when there are child runs. Not sure by the \"Created\" sort should be special.\n"
      },
      {
        "user": "serena-ruan",
        "body": "Thanks, seems like the runs only rendered as nested in the default view where order-by defaults to 'Created'.\nWhile to support this, seems we need to filter parent runs and then order by, and render child runs within parent runs. cc @daniellok-db is this something we're considering improving?"
      }
    ]
  },
  {
    "issue_number": 15289,
    "title": "[BUG] Artifacts uri is impossible to set correctly in local mode",
    "author": "killjoygoose",
    "state": "closed",
    "created_at": "2025-04-11T04:14:49Z",
    "updated_at": "2025-05-21T02:00:23Z",
    "labels": [
      "bug",
      "area/artifacts"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n2.21.3\n\n### System information\n\ndocker image: ghcr.io/mlflow/mlflow:v2.21.3\npython client: mlflow = \"^2.21.3\"\n\n### Describe the problem\n\nHi all, \nI am trying to set up an mlflow instance for local use with the following setup:\ndocker image: ghcr.io/mlflow/mlflow:v2.21.3\npython client: mlflow = \"^2.21.3\"\nI tried a myriad of possible combinations for the --default-artifact-root such that the artifacts are actually written to the **local** `./mlruns` folder, however it seems that if I don't set it with an absolute path it will always be transformed to `/mlruns` in the `run.info.artifact_uri` by the client. This leads to the client trying to store artifacts in `/`... \nNow, if I, as I said, set the --default-artifact-root explicitly to the absolute path of the local `./mlruns`, then the artifacts are stored correctly, but not present in the UI, because in the metadata now the uri is pointing to a directory, external to the container....\nAm I doing something wrong?\n(of course ./mlruns already exists in the project directory at the same level as the docker compose file and has read and write permissions for all users)\n\nHere is my docker-compose file contents\n\n\n### Tracking information\n\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n```\nservices:\n  mlflow:\n    image: ghcr.io/mlflow/mlflow:v2.21.3\n    user: 1000:1000\n    ports:\n      - 5000:5000\n    command: >\n      mlflow server\n      --host 0.0.0.0\n      --backend-store-uri \"./mlruns\"\n      --default-artifact-root ./mlruns\n    volumes:\n      - ./mlruns:/mlruns\n```\nHere is some code to reproduce:\n```\nmlflow.set_tracking_uri(\"http://localhost:5000/\")\nmlflow.set_experiment(\"some_experiment\")\nwith mlflow.start_run(run_name=\"some_run\") as run:\n    mlflow.log_text(\"random text\", \"artifact.txt\") # here I am getting a permission error because it is trying to write to `/mlruns` and not to `./mlruns`\n\n```\n\n\n### Stack trace\n\n```\nsite-packages/mlflow/utils/file_utils.py\", line 208, in mkdir\n    os.makedirs(target, exist_ok=True)\n  File \"<frozen os>\", line 215, in makedirs\n  File \"<frozen os>\", line 215, in makedirs\n  File \"<frozen os>\", line 215, in makedirs\n  File \"<frozen os>\", line 225, in makedirs\nPermissionError: [Errno 13] Permission denied: '/mlruns'\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "WeichenXu123",
        "body": "Hi, `--default-artifact-root` can not be the same with `--backend-store-uri`, you can specify another local directory for `--default-artifact-root`, like:\n\n```\nmlflow server -h 127.0.0.1 -p 5678 --backend-store-uri \"./mlruns\" --default-artifact-root \"./my_artifacts\"\n```\n\nif you do not specify `--default-artifact-root`, the default `./mlartifacts` local path is used.\n\nNote that for existing experiments, the artifact storage path is fixed by previous `mlflow server` command setting, so you need to create a new experiment to make new `--default-artifact-root` setting effective."
      }
    ]
  },
  {
    "issue_number": 15401,
    "title": "[FR] MLflow Tracing for smolagents",
    "author": "B-Step62",
    "state": "closed",
    "created_at": "2025-04-21T07:16:16Z",
    "updated_at": "2025-05-21T00:08:11Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\n### Summary\n\nExpand [auto-tracing integration](https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing) of MLflow Tracing to [smolagents](https://github.com/huggingface/smolagents) by HuggingFace. \n\n### Required Changes\nPlease refer to [how to add new integration to MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/contribute.html) for the actual steps of adding new auto-tracing integration. MLflow maintainers will provide attentive support for designing and implementing the change.\n\n\n### Expected Behavior\n\n1. Users can enable auto-tracing by calling `mlflow.smolagents.autolog()`\n```python\nimport mlflow\n\n# Enable auto-tracing\nmlflow.smolagents.tracing()\n\nfrom smolagents import (\n    CodeAgent,\n    ToolCallingAgent,\n    DuckDuckGoSearchTool,\n    VisitWebpageTool,\n    InferenceClientModel,\n)\n\nmodel = InferenceClientModel()\n\nsearch_agent = ToolCallingAgent(\n    tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],\n    model=model,\n    name=\"search_agent\",\n    description=\"This is an agent that can do web search.\",\n)\n\nmanager_agent = CodeAgent(\n    tools=[],\n    model=model,\n    managed_agents=[search_agent],\n)\nmanager_agent.run(\n    \"If the US keeps its 2024 growth rate, how many years will it take for the GDP to double?\"\n)\n```\n2. This will generates a trace like this (This example is taken from DSpy tracing. The structure will be different for SmolAgents tracing):\n![image](https://github.com/user-attachments/assets/c6c636fe-e90e-4f0e-ada3-151717c334a7)\n\n### Note\nSmolagents provides native OpenTelemetry integration. One promising approach would be building on-top-of it.\nhttps://huggingface.co/docs/smolagents/en/tutorials/inspect_runs\n\n### Contact\n\nIf you have any question or willing to work on this feature, leave a comment with tagging @B-Step62 \n",
    "comments": [
      {
        "user": "y-okt",
        "body": "Hi @B-Step62 , let me work on this feature. Thank you!"
      },
      {
        "user": "y-okt",
        "body": "@B-Step62 I'm currently encountering the error related to `tensorflow-cpu`. It seems https://github.com/mlflow/mlflow/pull/10998 tried to address the issue, but this PR hasn't been merged yet. I'll follow this solution provided by that PR, but just wanted to let you know that there is a bug in the installation of `tensorflow-cpu` and we encounter that bug when following `CONTRIBUTING.md`.\nAs that PR is almost 1 year ago, I can raise the PR when this task finishes, if necessary.\n\n```\nERROR: Could not find a version that satisfies the requirement tensorflow-cpu<=2.12.0 (from versions: none)\nERROR: No matching distribution found for tensorflow-cpu<=2.12.0\n```\n\n[Update] That error seems to be related to Apple Silicon. Only the Tensorflow version 2.13.0 or higher supports Apple Silicon ([article](https://blog.tensorflow.org/2023/07/whats-new-in-tensorflow-213-and-keras-213.html))."
      },
      {
        "user": "B-Step62",
        "body": "@y-okt Thanks for the heads-up! Yes, the tensorflow installation error on apple sillicon has been there for a bit while. It would be highly appreciated if you could find some good fix, but please go ahead with excluding the dependency for now and prioritize this FRðŸ™‚"
      }
    ]
  },
  {
    "issue_number": 14908,
    "title": "[BUG] Failed to infer model signature for custom python model with params args",
    "author": "micedre",
    "state": "open",
    "created_at": "2025-03-07T12:15:51Z",
    "updated_at": "2025-05-20T17:27:19Z",
    "labels": [
      "bug",
      "area/models"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.20.3\n- Tracking server: 2.11.3\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: code-server: v4.97.2\n- **Python version**: 3.12\n- **yarn version, if running the dev UI**:\n\n\n### Describe the problem\n\nWhen defining a custom model with a predict method accepting params, the params arguments is not inferred which gives an error when loading the model and using `predict` on it. \n\n```\nWARNING mlflow.models.utils: `params` can only be specified at inference time if the model signature defines a params schema. This model does not define a params schema. Ignoring provided params: ['k']\n```\n\nIt seems to work better when using `input_example` when logging the model. But in our case, this generates an error at logging when mlflow tries to infer the types from calling the predict function.\n\nLooking in the code, I don't see where the type hints given to the function `predict` are used.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```python\nimport mlflow\nfrom typing import Optional, Dict,  List\n\n\nclass MyModelWrapper(mlflow.pyfunc.PythonModel):\n    def predict(\n        self,\n        context: mlflow.pyfunc.PythonModelContext,\n        model_input: List[str],\n        params: Optional[Dict[str, str]]\n    ) -> List[str]:\n        print(params)\n        return model_input\n\n\ndef train_and_predict(\n    experiment_name,\n    run_name\n):\n    \"\"\"\n    Train a MyModel.\n    \"\"\"\n    mlflow.set_experiment(experiment_name)\n    mlflow.set_tracking_uri(\"\")\n    with mlflow.start_run(run_name=run_name) as run:\n        inference_params = {\n            \"k\": 1,\n        }\n        # Infer the signature including parameters\n        signature = mlflow.models.infer_signature(\n            model_input=[\"to_predict\"],\n            model_output=[\"prediction\"],\n            params=inference_params\n        )\n        with open(\"artifact.bin\", \"w\") as f:\n            f.write(\"Test\")\n        artifacts = {\n            \"model_path\": \"artifact.bin\"\n        }\n        mlflow.pyfunc.log_model(\n            artifact_path=run_name,\n            python_model=MyModelWrapper(),\n            artifacts=artifacts,\n            signature=signature\n        )\n    \n    model_uri = \"runs:/{}/test\".format(run.info.run_id)\n    loaded_model = mlflow.pyfunc.load_model(model_uri)\n    loaded_model.predict([\"Test Predict\"], params={'k': 1})\n\n\nif __name__ == \"__main__\":\n    train_and_predict(\n        \"test-signature\",\n        \"test\")\n\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\n2025/03/07 12:11:04 INFO mlflow.models.signature: Inferring model signature from type hints\n2025/03/07 12:11:04 WARNING mlflow.pyfunc: Provided signature does not match the signature inferred from the Python model's `predict` function type hint. Signature inferred from type hint will be used:\ninputs: \n  [string (required)]\noutputs: \n  [string (required)]\nparams: \n  None\n\nRemove the `signature` parameter or ensure it matches the inferred signature. In a future release, this warning will become an exception, and the signature must align with the type hint.\nDownloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 5315.97it/s]\n2025/03/07 12:11:07 WARNING mlflow.models.utils: `params` can only be specified at inference time if the model signature defines a params schema. This model does not define a params schema. Ignoring provided params: ['k']\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "micedre",
        "body": "For information, it works fine with mlflow==2.19."
      },
      {
        "user": "harupy",
        "body": "@micedre Thanks for reaching out. I was able to reproduce the issue with your example.\n\ncc @serena-ruan for signature inference."
      },
      {
        "user": "sapphire008",
        "body": "```\nRemove the `signature` parameter or ensure it matches the inferred signature. In a future release, this warning will become an exception, and the signature must align with the type hint.\n```\nThis decision also seems concerning. If a signature is provided, then it should be used for the data validation by default. Unless using more complicated annotations systems like Pydantic, type hints cannot annotate specific fields very well. MLFlow should be able to validate the types of specific fields, rather than passing on the generic \"Any\", which is what is being inferred when multiple data types exist in the input. MLFlow should also keep supporting the PyArrow-like signature, so that the inputs can accept Python native dtypes, instead of forcing the validation via Pydantic through type hints."
      }
    ]
  },
  {
    "issue_number": 15764,
    "title": "[BUG]  Inconsistent name mlflow.environment_variables.MLFLOW_CONFIGURE_LOGGING = 'MLFLOW_LOGGING_CONFIGURE_LOGGING'",
    "author": "lendle",
    "state": "closed",
    "created_at": "2025-05-15T22:05:41Z",
    "updated_at": "2025-05-20T15:11:25Z",
    "labels": [
      "bug",
      "good first issue",
      "has-closing-pr"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.20.0\n- Tracking server: N/A\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX, Linux\n- **Python version**: 3.11\n- **yarn version, if running the dev UI**: N/A\n\n\n### Describe the problem\n\nThe env var associated with `mlflow.environment_variables.MLFLOW_CONFIGURE_LOGGING`\n has a different value that the variable name: `MLFLOW_LOGGING_CONFIGURE_LOGGING`. \n\nThe [docs](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.environment_variables.html#mlflow.environment_variables.MLFLOW_CONFIGURE_LOGGING) are correct but it's quite confusing. I mistakenly tried to use `MLFLOW_CONFIGURE_LOGGING` and it took a bit to figure why it wasn't working. \n\nI'm not sure if this is intentional or not, but unless I missed something it looks like all of the other names are consistent in the docs.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "Thanks @lendle for finding this! Are you willing to file a fix to make them consistent? "
      },
      {
        "user": "lendle",
        "body": "If it's as simple as committing a one line change maybe with a simple test then absolutely. However, it's not obvious to me what the right fix is, renaming the python variable to match the environment variable or vice-versa are both breaking changes since they're both part of the public & documented API. So I think it would require some discussion. \n\nIMO it would be better to rename the python variable to `MLFLOW_LOGGING_CONFIGURE_LOGGING` because\n1. Looks like env vars in general are of the form `MLFLOW_<domain>_<thing>` so that's consistent with what else is there.\n2. It will be less likely to cause a silent failure because linters/type checkers have a chance to flag that `mlflow.environment_variables.MLFLOW_CONFIGURE_LOGGING` does not exist and if someone does try to access it, it will cause a runtime error.\n\n\n"
      },
      {
        "user": "rahuja23",
        "body": "@serena-ruan  I pushed a PR regarding the review ðŸ˜ƒ  please let me know what you guys think about it ðŸ˜„ "
      }
    ]
  },
  {
    "issue_number": 13327,
    "title": "Deprecate `generate_signature_output` in favor of input_example",
    "author": "B-Step62",
    "state": "open",
    "created_at": "2024-10-05T11:37:22Z",
    "updated_at": "2025-05-20T14:54:20Z",
    "labels": [
      "good first issue"
    ],
    "body": "### Summary\n\nThe [mlflow.transformers.generate_signature_output](https://mlflow.org/docs/latest/python_api/mlflow.transformers.html#mlflow.transformers.generate_signature_output) function is an utility function for generating prediction output for a Transformers model, so that users can pass it to the `mlflow.models.infer_signature` for getting the auto-inferred model signature for the model.\r\n\r\nHowever, MLflow now support inferring signature from `input_example` specified when logging a model. Users no longer need this API and can just pass an input example to let MLflow infer the signature.\r\n\r\nWe should mark the `generate_signature_output` deprecated, as well as update the examples and tutorials in the documentation.\r\n\r\n**Old**\r\n```\r\nsignature = mlflow.models.infer_signature(\r\n    input_example,\r\n    mlflow.transformers.generate_signature_output(generation_pipeline, input_example),\r\n    parameters,\r\n)\r\n\r\nwith mlflow.start_run() as run:\r\n    model_info = mlflow.transformers.log_model(\r\n        transformers_model=generation_pipeline,\r\n        artifact_path=\"text_generator\",\r\n        input_example=[\"prompt 1\", \"prompt 2\", \"prompt 3\"],\r\n        signature=signature,\r\n    )\r\n```\r\n\r\n**Current**\r\n```\r\nwith mlflow.start_run() as run:\r\n    model_info = mlflow.transformers.log_model(\r\n        transformers_model=generation_pipeline,\r\n        artifact_path=\"text_generator\",\r\n        # By passing a tuple of input example and parameters, MLflow can infer model signature including parameters.\r\n        input_example=([\"prompt 1\", \"prompt 2\", \"prompt 3\"], parameters)\r\n    )\r\n```\r\n\r\n\n\n### Notes\n\n- Make sure to open a PR from a **non-master** branch.\r\n- Sign off the commit using the `-s` flag when making a commit:\r\n\r\n  ```sh\r\n  git commit -s -m \"...\"\r\n           # ^^ make sure to use this\r\n  ```\r\n\r\n- Include `#{issue_number}` (e.g. `#123`) in the PR description when opening a PR.\r\n",
    "comments": [
      {
        "user": "admin1-saurabh",
        "body": "Hi @B-Step62 \r\nIâ€™m interested in working on this issue. Could you please assign it to me?\r\nThank you!"
      },
      {
        "user": "B-Step62",
        "body": "@admin1-saurabh Sure, thanks for picking up this work! AssignedðŸ™‚"
      },
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      }
    ]
  },
  {
    "issue_number": 13388,
    "title": "[BUG] onnx model wrapper doesn't load GPU provider",
    "author": "palex351",
    "state": "open",
    "created_at": "2024-10-11T12:22:58Z",
    "updated_at": "2025-05-20T14:41:53Z",
    "labels": [
      "bug",
      "help wanted",
      "area/models",
      "area/docker"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### Willingness to contribute\n\nNo. I cannot contribute a bug fix at this time.\n\n### MLflow version\n\n- Client: 2.15.1\r\n- Tracking server:  2.8.1\r\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  ubuntu:23.10\r\n- **Python version**: 3.10\r\n- **yarn version, if running the dev UI**:\r\n\n\n### Describe the problem\n\nLatest versions of onnxruntime (onnxruntime-gpu==1.17.1) when it is initialized without providers will use CPU provider by default. This will not went into try-catch block of mlflow onnx wrapper that is initializing the onnxruntime with providers.\r\nThis way, when executing the model with CUDA, the model will always be executed with CPU.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```shell\r\nREPLACE_ME\r\n```\r\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nimport mlflow\r\nimport torch\r\nimport numpy as np\r\n\r\nprint(f\"Cuda available: {torch.cuda.is_available()}\")\r\nrand_input = np.random.rand(1,144,6)\r\nmodel = mlflow.pyfunc.load_model(model_uri=\"URI:onnx_model_stored_in_mlflow_tracking\")\r\nmodel.predict(rand_input)\r\n\r\n```\r\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\n# execution log\r\nCuda available: True\r\n\r\n# Running nvidia-smi command shows zero GPU activity\r\n\r\n```\r\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nREPLACE_ME\r\n```\r\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [X] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [X] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "Thanks for the report, am I understanding correctly that this only happens with newer versions of ONNX?"
      },
      {
        "user": "palex351",
        "body": "> Thanks for the report, am I understanding correctly that this only happens with newer versions of ONNX?\r\n\r\nCorrect"
      },
      {
        "user": "daniellok-db",
        "body": "Got it, the team might not have time to look into this immediately but I'll tag this as `help wanted` in case someone else from the community can contribute a fix"
      }
    ]
  },
  {
    "issue_number": 11077,
    "title": "[BUG] Unable to Check Experiment Existence with path starting with `/Workspace/` Directory in Databricks Platform",
    "author": "acn-thanapat-sontayasara",
    "state": "open",
    "created_at": "2024-02-09T09:01:41Z",
    "updated_at": "2025-05-20T13:16:11Z",
    "labels": [
      "bug",
      "area/tracking"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nDatabricks\n\n### Willingness to contribute\n\nNo. I cannot contribute a bug fix at this time.\n\n### MLflow version\n\n2.10.2\n\n### System information\n\n`Databricks Runtime:` 13.3 LTS (includes Apache Spark 3.4.1, Scala 2.12)\r\n`Access mode:` Single user\r\n`Python:` 3.10.12\n\n### Describe the problem\n\nIn Databricks, when attempting to set an experiment with an `experiment_name` specified as an absolute path from `/Workspace/Shared/mlflow_experiment/<experiment_name>`, the `mlflow.set_experiment()` function succeeds in creating the experiment during the initial run.\r\nHowever, upon subsequent attempts to set the experiment again using the same method, it results in an error.\r\nI believe that this error occurs because the system fails to accurately check the existence of the experiment and mistakenly attempts to create it again with the same name, resulting in a `RESOURCE_ALREADY_EXISTS` error.\r\n<img width=\"1375\" alt=\"image\" src=\"https://github.com/mlflow/mlflow/assets/113094208/2e2aa00a-3f2a-4b14-9f40-2af74a721f38\">\r\n\r\n\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```shell\r\nSystem information: Linux #59~20.04.1-Ubuntu SMP Tue Oct 17 16:45:08 UTC 2023\r\nPython version: 3.10.12\r\nMLflow version: 2.10.2\r\nMLflow module location: /local_disk0/.ephemeral_nfs/envs/pythonEnv-ae93a728-e899-4420-ae18-7555b7064319/lib/python3.10/site-packages/mlflow/__init__.py\r\nTracking URI: databricks\r\nRegistry URI: databricks\r\nDatabricks runtime version: 13.3.x-scala2.12\r\nMLflow environment variables: \r\n  MLFLOW_CONDA_HOME: /databricks/conda\r\n  MLFLOW_GATEWAY_URI: databricks\r\n  MLFLOW_PYTHON_EXECUTABLE: /databricks/spark/scripts/mlflow_python.sh\r\n  MLFLOW_TRACKING_URI: databricks\r\nMLflow dependencies: \r\n  Flask: 3.0.2\r\n  Jinja2: 3.1.3\r\n  alembic: 1.13.1\r\n  boto3: 1.24.28\r\n  botocore: 1.27.28\r\n  click: 8.1.7\r\n  cloudpickle: 3.0.0\r\n  docker: 7.0.0\r\n  entrypoints: 0.4\r\n  gitpython: 3.1.41\r\n  gunicorn: 21.2.0\r\n  importlib-metadata: 4.6.4\r\n  markdown: 3.5.2\r\n  matplotlib: 3.5.2\r\n  numpy: 1.24.4\r\n  packaging: 21.3\r\n  pandas: 1.4.4\r\n  protobuf: 3.19.4\r\n  pyarrow: 8.0.0\r\n  pydantic: 1.10.6\r\n  pytz: 2022.1\r\n  pyyaml: 6.0.1\r\n  querystring-parser: 1.2.4\r\n  requests: 2.28.1\r\n  scikit-learn: 1.1.1\r\n  scipy: 1.9.1\r\n  sqlalchemy: 2.0.25\r\n  sqlparse: 0.4.4\r\n  virtualenv: 20.16.3\r\n```\r\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n``` python\r\nimport os\r\nimport mlflow\r\n\r\n# Create mlflow experiment directory first\r\nos.makedirs(\"/Workspace/Shared/mlflow_experiment\", exist_ok=True)\r\n\r\n# Set experiment 1st run --> expect to be successed\r\nmlflow.set_experiment(experiment_name=\"/Workspace/Shared/mlflow_experiment/some_experiment_name\")\r\n\r\n# Set experiment 2nd run --> expect to be failed\r\nmlflow.set_experiment(experiment_name=\"/Workspace/Shared/mlflow_experiment/some_experiment_name\")\r\n```\r\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n``` python\r\n---------------------------------------------------------------------------\r\nRestException                             Traceback (most recent call last)\r\nFile <command-1155596787592745>, line 1\r\n----> 1 mlflow.set_experiment(experiment_name=f\"{parent_dir}/{model_name}\")\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/mlflow/tracking/fluent.py:147, in set_experiment(experiment_name, experiment_id)\r\n    140         _logger.info(\r\n    141             \"Experiment with name '%s' does not exist. Creating a new experiment.\",\r\n    142             experiment_name,\r\n    143         )\r\n    144         # NB: If two simultaneous threads or processes attempt to set the same experiment\r\n    145         # simultaneously, a race condition may be encountered here wherein experiment creation\r\n    146         # fails\r\n--> 147         experiment_id = client.create_experiment(experiment_name)\r\n    148         experiment = client.get_experiment(experiment_id)\r\n    149 else:\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/mlflow/tracking/client.py:570, in MlflowClient.create_experiment(self, name, artifact_location, tags)\r\n    522 def create_experiment(\r\n    523     self,\r\n    524     name: str,\r\n    525     artifact_location: Optional[str] = None,\r\n    526     tags: Optional[Dict[str, Any]] = None,\r\n    527 ) -> str:\r\n    528     \"\"\"Create an experiment.\r\n    529 \r\n    530     :param name: The experiment name. Must be unique.\r\n   (...)\r\n    568         Lifecycle_stage: active\r\n    569     \"\"\"\r\n--> 570     return self._tracking_client.create_experiment(name, artifact_location, tags)\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:235, in TrackingServiceClient.create_experiment(self, name, artifact_location, tags)\r\n    224 \"\"\"Create an experiment.\r\n    225 \r\n    226 :param name: The experiment name. Must be unique.\r\n   (...)\r\n    231 :return: Integer ID of the created experiment.\r\n    232 \"\"\"\r\n    233 _validate_experiment_artifact_location(artifact_location)\r\n--> 235 return self.store.create_experiment(\r\n    236     name=name,\r\n    237     artifact_location=artifact_location,\r\n    238     tags=[ExperimentTag(key, value) for (key, value) in tags.items()] if tags else [],\r\n    239 )\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:98, in RestStore.create_experiment(self, name, artifact_location, tags)\r\n     94 tag_protos = [tag.to_proto() for tag in tags] if tags else []\r\n     95 req_body = message_to_json(\r\n     96     CreateExperiment(name=name, artifact_location=artifact_location, tags=tag_protos)\r\n     97 )\r\n---> 98 response_proto = self._call_endpoint(CreateExperiment, req_body)\r\n     99 return response_proto.experiment_id\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:59, in RestStore._call_endpoint(self, api, json_body)\r\n     57 endpoint, method = _METHOD_TO_INFO[api]\r\n     58 response_proto = api.Response()\r\n---> 59 return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:220, in call_endpoint(host_creds, endpoint, method, json_body, response_proto, extra_headers)\r\n    218     call_kwargs[\"json\"] = json_body\r\n    219     response = http_request(**call_kwargs)\r\n--> 220 response = verify_rest_response(response, endpoint)\r\n    221 js_dict = json.loads(response.text)\r\n    222 parse_dict(js_dict=js_dict, message=response_proto)\r\n\r\nFile /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:152, in verify_rest_response(response, endpoint)\r\n    150 if response.status_code != 200:\r\n    151     if _can_parse_as_json_object(response.text):\r\n--> 152         raise RestException(json.loads(response.text))\r\n    153     else:\r\n    154         base_msg = (\r\n    155             f\"API request to endpoint {endpoint} \"\r\n    156             f\"failed with error code {response.status_code} != 200\"\r\n    157         )\r\n\r\nRestException: RESOURCE_ALREADY_EXISTS: Node named 'somewhat_model_name' already exists\r\n```\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nREPLACE_ME\r\n```\r\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [X] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "acn-thanapat-sontayasara",
        "body": "Update on this:\r\n\r\nI've just found out that even when not in the `Shared` directory but start with `/Workspace/`,\r\nit also fails to check for experiment existence.\r\n<img width=\"1080\" alt=\"image\" src=\"https://github.com/mlflow/mlflow/assets/113094208/3be08a74-4c2c-4c12-8227-b04d5d33dc41\">\r\n\r\nWithout `/Workspace/`, it works perfectly fine.\r\n<img width=\"1147\" alt=\"image\" src=\"https://github.com/mlflow/mlflow/assets/113094208/8249b25d-8219-4d31-885f-205c8945dcfa\">\r\n"
      },
      {
        "user": "harupy",
        "body": "@acn-thanapat-sontayasara Thanks for reporting this issue! I was able to reproduce it. We're investigating how to fix this."
      },
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      }
    ]
  },
  {
    "issue_number": 15384,
    "title": "[FR] Add pagination support in Model Page UI",
    "author": "Flametaa",
    "state": "open",
    "created_at": "2025-04-19T09:19:13Z",
    "updated_at": "2025-05-19T22:12:23Z",
    "labels": [
      "enhancement",
      "area/uiux",
      "area/model-registry",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nAdd pagination support in the [model page](https://github.com/mlflow/mlflow/blob/master/mlflow/server/js/src/model-registry/components/ModelPage.tsx) UI (similar to what we have in the [ModelListPage](https://github.com/mlflow/mlflow/blob/master/mlflow/server/js/src/model-registry/components/ModelListPage.tsx). This should be already supported by the searchModelVersionsApi.\n\n### Motivation\n\n> #### What is the use case for this feature?\n\nThe model registry is currently loading 200k model versions at once which is slowing down the UI and causing OOM issues when the server doesn't have enough resources. the UI can handle a large number of registered models, but struggles with large number of model versions under the same model\n\n> #### Why is this use case valuable to support for MLflow users in general?\nThis will make the model registry UI scale to handle hundreds of thousands of model versions.\n> #### Why is this use case valuable to support for your project(s) or organization?\nScaling issue with models that have thousands of versions\n> #### Why is it currently difficult to achieve this use case?\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [x] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [x] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": []
  },
  {
    "issue_number": 15778,
    "title": "[BUG] `mlflow.transformers.log_model()` with task `llm/v1/chat` not working with several LLMs from HuggingFace",
    "author": "Albert-Herrando-GenCat",
    "state": "open",
    "created_at": "2025-05-16T14:05:06Z",
    "updated_at": "2025-05-19T07:45:55Z",
    "labels": [
      "bug",
      "area/models",
      "area/docker",
      "integrations/databricks",
      "area/deployments"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nDatabricks\n\n### MLflow version\n\nmlflow, version 2.11.4\n\n\n### System information\n\nDatabricks Serverless and General Purpose Compute.\n\n\n### Describe the problem\n\nHello,\n\nI am currently trying to deploy a HuggingFace LLM model to Databricks with the MLflow task llm/v1/chat in order to use it as a chat.\n\nI have tried several models like:\n\n[TinyLlama/TinyLlama_v1.1 Â· Hugging Face](https://huggingface.co/TinyLlama/TinyLlama_v1.1)\n[BSC-LT/salamandra-7b-instruct Â· Hugging Face](https://huggingface.co/BSC-LT/salamandra-7b-instruct)\n\nHowever, once deployed into Databricks Model Serving (Provisioned Throughput), the models act very weirdly:\n\n![Image](https://github.com/user-attachments/assets/45944221-1d0c-47fc-8a1d-3ae45eec1f0b)\n\n![Image](https://github.com/user-attachments/assets/3e173a5d-fcd5-4805-8ed0-7b3cb599d9ce)\n\nThe code that I used can be found in the \"Code to reproduce the issue\" section.\n\nI am encountering this problem with several LLMs from HuggingFace. It seems that there is a mismatch when the prompt is generated or the chat template is not properly applied.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\n\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\n%pip install transformers\n%pip install torch\n%pip install accelerate\n%pip install torchvision\ndbutils.library.restartPython()\n\nimport mlflow\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport transformers\nimport torch\nfrom huggingface_hub import ModelCard\n\nmodel_id = \"TinyLlama/TinyLlama_v1.1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntransformers_model = {\"model\": model, \"tokenizer\": tokenizer}\n\n# The signature will be automatically inferred using the input_example by MLflow for llm/v1/chat\ninput_example = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Hello!\"\n        }\n    ],\n    # These are optional parameters for the llm/v1/chat endpoint\n    # \"temperature\": 0.6,\n    # \"max_tokens\": 300\n}\n\n# --- Unity Catalog Setup ---\n# Make sure the catalog and schema exist in Unity Catalog\nuc_catalog = \"dts_proves_pre\"\nuc_schema = \"llms\"\nregistered_model_name = f\"{uc_catalog}.{uc_schema}.TinyLlama_v1-1\"\n\n# Configure MLflow to use Unity Catalog\nmlflow.set_registry_uri(\"databricks-uc\")\n\n# Log model\nwith mlflow.start_run():\n  model_info = mlflow.transformers.log_model(\n      transformers_model=transformers_model,\n      task = \"llm/v1/chat\",\n      model_card = ModelCard.load(model_id),\n      artifact_path=\"TinyLlama_v1.1-model\",\n      # signature=signature, # The signature will be automatically inferred using the input_example by MLflow for llm/v1/chat\n      input_example=input_example,\n      registered_model_name=registered_model_name,\n      extra_pip_requirements=[\"transformers\", \"torch\", \"torchvision\", \"accelerate\"]\n  )\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [x] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [x] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [x] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "Hi @Albert-Herrando-GenCat Could you repro this without databricks playground involved? e.g. load the model with `mllfow.pyfunc.load_model` and call `predict`?\nWe resolve MLflow OSS issues in this repo, but for databricks products issue please raise an ES ticket instead :)"
      }
    ]
  },
  {
    "issue_number": 15779,
    "title": "[FR] PyArrow 20",
    "author": "apmorton",
    "state": "closed",
    "created_at": "2025-05-16T15:58:45Z",
    "updated_at": "2025-05-19T03:41:21Z",
    "labels": [
      "enhancement",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nRaise the pin on pyarrow to `<21`\n\n### Motivation\n\nIntegration with other software which requires the latest pyarrow\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": []
  },
  {
    "issue_number": 14843,
    "title": "[FR] Support runs:/ and models:/ for scoring",
    "author": "f2cf2e10",
    "state": "open",
    "created_at": "2025-03-04T20:25:30Z",
    "updated_at": "2025-05-18T11:08:40Z",
    "labels": [
      "enhancement",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\n\n### Proposal Summary\n\nOn this [PR discussion](https://github.com/mlflow/mlflow/pull/9538#discussion_r1318088799) the possibility to use runs:/ and models:/ for scoring has been proposed but it was decided to proceed with the PR without this feature. \nIs there any plan to add this feature in the coming future? Imho this would enhance the testing possibilities by a lot withing mlflow.\n\n\n### Motivation\n\n> #### What is the use case for this feature?\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\n> #### Why is it currently difficult to achieve this use case?\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "Thanks @f2cf2e10 for raising this! This makes sense, are you willing to contribute to this feature?  I think we can use _load_model_or_server to load the model and send payload to it for prediction :)"
      },
      {
        "user": "f2cf2e10",
        "body": "I am happy to contribute here. I will experiment around with it and check the guidelines on how-to-contribute."
      },
      {
        "user": "f2cf2e10",
        "body": "Hi @serena-ruan, I did change 2 lines in the model_utils.py file and I am using the feature already. I am opening a PR resolving this."
      }
    ]
  },
  {
    "issue_number": 10329,
    "title": "We'd like your feedback on our MLflow experience!",
    "author": "AbeOmor",
    "state": "open",
    "created_at": "2023-11-08T18:36:19Z",
    "updated_at": "2025-05-16T10:59:09Z",
    "labels": [],
    "body": "Would love if you could take sometime to share your experience with the MLflow team.\r\n\r\nIn the past year, MLflow has released multiple features notably MLflow evaluate, Prompt UI, MLflow datasets, new chart experience, and more, and the MLflow product team would love to hear your thoughts on some of the new features and how you envision MLflow growing to meet your needs, especially with Deep Learning or LLMs.\r\n\r\nThe survey would take 3-5 mins of your time and would greatly help the MLflow team in designing the next chapter of the product.\r\n\r\n[Fill out the survey HERE](https://surveys.training.databricks.com/jfe/form/SV_9nraprWp02EoLj0 )\r\n\r\nFeel free to add more feedback in the comments below",
    "comments": [
      {
        "user": "Jamesetay1",
        "body": "I have just recently started using MLFLow for Prompt Engineering and interacting with the gateway through a notebook has been really seamless. One feature in the Prompt UI I'd like to see (or perhaps don't understand) is that when you add a new row to or duplicate a run it automatically populates the variables for you. I only want to change one of the variables of the three for a new run, but currently I have to copy and paste all of them from outside the UI."
      },
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      },
      {
        "user": "e-dorigatti",
        "body": "Please improve the responsiveness of the web UI! Performance issues were raised several times in the past (#627, #925, #1517, #1571, #1763, #1902, #3334, #4034, #4305, #4454, #5653) and are still not fixed. It only takes a few hundreds runs to bring the web UI to its knees. At this point I am barely able to browse the results of a single hyperparameter search, and I need to start looking for alternative products."
      }
    ]
  },
  {
    "issue_number": 15429,
    "title": "[FR] Using HFDataset with Non-HF-DataSources",
    "author": "guptakvgaurav",
    "state": "closed",
    "created_at": "2025-04-22T12:40:40Z",
    "updated_at": "2025-05-16T08:11:24Z",
    "labels": [
      "enhancement",
      "area/uiux",
      "help wanted",
      "area/projects",
      "integrations/azure",
      "integrations/databricks",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nI am using HF's dataset library to load the dataset and my dataset resides on a filesystem (local or s3). I would like to see the link to the dataset in my `run`. \n\nMy current implementation that does not works:\n\n```\n\nfrom datasets import load_dataset\n# Load the domain-specific dataset (assuming a text file with one sentence per line)\ndataset = load_dataset(\"text\", data_files={\n    \"train\": train_file, \n    \"validation\": validation_file,\n    \"test\":test_file\n})\n\nwith mlflow.start_run(experiment_id=EXPERIMENT_ID) as run:\n    train_dataset = mlflow.data.from_huggingface(\n            dataset[\"train\"],\n            name=f\"text-train\",\n            source=train_file,  # would like to show the link to the file\n        )\n\n    mlflow.log_input(train_dataset, \n                     context=\"training\", \n                     tags={\"split\": \"train\", \"source_type\": \"text_file\"},\n                     )\n\n\n```\n\nAs an alternative, as of now i need to convert the HF Dataset to Pandas Dataset just for registeration purposes. Since Dataset will be huge, it will eat up the RAM.\n\n### Motivation\n\n> #### What is the use case for this feature?\nTo get support of listing/registering the HFDataset to work with Non-HFDataSources.\n\n> #### Why is this use case valuable to support for MLflow users in general?\nHF's Dataset library is becoming popular to access and tune small Language models, and many a times dataset used is private and is on private infrastructure. To maintain the lineage in experiment, this feature will be helpful.\n\n> #### Why is this use case valuable to support for your project(s) or organization?\nIt will help in maintaining the lineage throughout the experiment.\n\n> #### Why is it currently difficult to achieve this use case?\nBecause `mlflow.data.from_huggingface` does not support `source` argument. \n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [x] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [x] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [x] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [x] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "@guptakvgaurav FR makes sense to me. Just to clarify, is the suggestion is the following:\n\n1. Add `source` argument to the `from_hugging_face` API.\n2. Make it accept `FileSystemDatasetSource` instance."
      },
      {
        "user": "guptakvgaurav",
        "body": "Having the `source` argument so that as a user I can see the URL of the dataset which I loaded using HF dataset. \nSince `source`'s value is URL, which can point to S3, GCS or any other link (private or public), so this will fall in the category of `FileSystemDatasetSource`.\n\nSo, above is the immediate ask.\n\nBelow, I would like to have a solution where user can create an instance of any source `FileSystemSource`, or `HTTPSource` or any other source, and can pass it to DataSet's convenience functions like `from_pandas` or `from_hugging_face`. This will allow user to play with multiple permutation and combination.\n\nSince immediate ask looks reasonable to you, even I can spend time on it."
      },
      {
        "user": "B-Step62",
        "body": "@guptakvgaurav Yes, please go ahead to implement the source for HF dataset. Thank you for the willingness to contribute!\n\n> where user can create an instance of any source FileSystemSource, or HTTPSource or any other source, and can pass it to DataSet's convenience functions like from_pandas or from_hugging_face\n\nIt seems most of those functions e.g. `from_numpy`, `from_pandas`, `from_tensorflow` already accept `source` argument. Is this sufficient or are you suggesting something else?"
      }
    ]
  },
  {
    "issue_number": 15721,
    "title": "`pipenv install` shows `invalid metadata` warning",
    "author": "khteh",
    "state": "closed",
    "created_at": "2025-05-14T02:32:33Z",
    "updated_at": "2025-05-16T03:12:23Z",
    "labels": [],
    "body": "```\nWARNING:pipenv.patched.pip._internal.resolution.resolvelib.found_candidates:Ignoring version 1.27.0 of mlflow since it has invalid metadata:\nRequested mlflow from https://files.pythonhosted.org/packages/12/49/1c6f1535bb8b9f35463b043c08a902531a367ed42b0fe4afb3882bb28f8a/mlflow-1.27.0-py3-none-any.whl (from -r /tmp/pipenv-cxqpsjl6-requirements/pipenv-2ojdv5q3-constraints.txt (line 22)) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n    scikit-learn (>=1.0.*) ; extra == 'pipelines'\n                  ~~~~~~^\nPlease use pip<24.1 if you need to use this version.\n```",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "Hi, @khteh. Can you please follow the issue template to provide enough context? And it seems you're using very old version of mlflow, can you use a newer one such as 2.22.0?"
      }
    ]
  },
  {
    "issue_number": 15757,
    "title": "[BUG] `mlflow.log_dict` says in it's Type Annotation that it does not support `int` keys, but in reality, it does",
    "author": "benglewis",
    "state": "open",
    "created_at": "2025-05-15T12:33:46Z",
    "updated_at": "2025-05-16T02:16:12Z",
    "labels": [
      "bug",
      "area/tracking",
      "has-closing-pr"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.22.0\n- Tracking server: 2.21.2\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\n- **Python version**: N/A\n- **yarn version, if running the dev UI**: N/A\n\n\n### Describe the problem\n\nThe Type Annotation on `mlflow.log_dict` should allow keys of type `int`.\n\n### Tracking information\n\n_No response_\n\n### Code to reproduce issue\n\n```python\nimport mlflow\n\nmlflow.log_dict({ 1: True, })\n```\n\n### Stack trace\n\nArgument of type \"set[int]\" cannot be assigned to parameter \"dictionary\" of type \"dict[str, Any]\" in function \"log_dict\"\n  \"set[int]\" is not assignable to \"dict[str, Any]\"Pyright[reportArgumentType](https://github.com/microsoft/pyright/blob/main/docs/configuration.md#reportArgumentType)\n\n### Other info / logs\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "Hi @benglewis I think we didn't mean to support int, do you require int support? Could you convert it to string instead?"
      }
    ]
  },
  {
    "issue_number": 13420,
    "title": "[FR] Add option to copy relative directory structure when specifying `code_paths` in `mlflow.pyfunc.log_model` call",
    "author": "djalusic",
    "state": "open",
    "created_at": "2024-10-15T14:09:05Z",
    "updated_at": "2025-05-15T16:56:29Z",
    "labels": [
      "enhancement",
      "area/docs",
      "area/examples",
      "area/model-registry",
      "area/models"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\n\n### Proposal Summary\n\nWhen logging a model using `mlflow.pyfunc.log_model`, a developer can specify extra code dependencies through `code_paths` argument, a list of strings which are either paths to files or directories. This FR aims to add support for copying the relative directory structure of the specified `code_paths` along with the specified directories/files.\n\n### Motivation\n\n> #### What is the use case for this feature?\r\n\r\nThe specified paths are copied directly into the destination folder (disregarding the parent directories). \r\n\r\nIf a file or a directory is contained in an another directory, the model inference wouldn't work after loading the model from the model registry since the implementation of the `PythonModel` expects the file to be located at the same directory. Therefore, the developer has to specify the entire module for the model inference to work after loading the model from model registry, which is not always needed since some files might not be relevant at all and more code is exposed than necessary. \r\n\r\nBy enabling copying relative directory structure, the imports in the file implementing the `PythonModel` class will be preserved.\r\n\r\n> #### Why is this use case valuable to support for MLflow users in general?\r\n\r\nThis use-case would make customizing MLFlow PyFuncs more flexible and more easy to use them in production environments.\r\n\r\n> #### Why is this use case valuable to support for your project(s) or organization?\r\n\r\nUnnecessary code wouldn't be logged as well as the model dependencies, so the exposed code in production would be minimized.\r\n\r\n> #### Why is it currently difficult to achieve this use case?\r\n\r\nTo be able to achieve this use-case, manipulating the directory structures would have to be done before logging the model, to \"hide\" the unnecessary files from `log_model`Â function. \r\n\r\nSplitting these files into separate modules just to avoid copying them to the model registry along with the necessary files would result in a less coherent directory structure.\r\n\n\n### Details\n\nTracing the `mlflow.pyfunc.log_model` function, I concluded that the logic for copying the files is in the `_copy_file_or_tree` function from `mlflow.utils.file_utils`. [This Colab notebook](https://colab.research.google.com/drive/1Dw-oBZE29vziYOzzEigv_ERo6LKXRlf9?usp=sharing) also shows what is the current behaviour of that function versus what I'd like to be supported.\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [X] `area/docs`: MLflow documentation pages\n- [X] `area/examples`: Example code\n- [X] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [X] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "This makes sense to me. @BenWilson2 was there some particular reason that nested directories were out of scope for the `code_paths` feature? [According to the docs](https://mlflow.org/docs/latest/model/dependencies.html#caveats-of-code-paths-option) it looks like this is explicitly not supported, but it reads like more of an implementation issue."
      },
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      },
      {
        "user": "akarnavasco",
        "body": "Adding the directory structure of the `code_path` to `code_dir_subpath`, if the path is relevant and is a file, in \n[mlflow.utils.model_utils._validate_and_copy_code_paths](https://github.com/mlflow/mlflow/blob/76b8e5e12f60a22d63f5ebaf2d6d0949a8f969ae/mlflow/utils/model_utils.py#L133) in lines [147](https://github.com/mlflow/mlflow/blob/76b8e5e12f60a22d63f5ebaf2d6d0949a8f969ae/mlflow/utils/model_utils.py#L147) might fix this issue while changing the behavior only for code_paths.\n```diff  \ndef _validate_and_copy_code_paths(code_paths, path, default_subpath=\"code\"):\n    \"\"\"Validates that a code path is a valid list and copies the code paths to a directory. This\n    can later be used to log custom code as an artifact.\n\n    Args:\n        code_paths: A list of files or directories containing code that should be logged\n            as artifacts.\n        path: The local model path.\n        default_subpath: The default directory name used to store code artifacts.\n    \"\"\"\n    _validate_code_paths(code_paths)\n    if code_paths is not None:\n        code_dir_subpath = default_subpath\n        for code_path in code_paths:\n+           if not os.path.isabs(code_path) and os.path.isfile(code_path):\n+                code_dir_subpath = os.path.join(code_dir_subpath, os.path.dirname(code_path))\n            try:\n                _copy_file_or_tree(src=code_path, dst=path, dst_dir=code_dir_subpath)\n            except OSError as e:\n                # A common error is code-paths includes Databricks Notebook. We include it in error\n                # message when running in Databricks, but not in other envs tp avoid confusion.\n                example = \", such as Databricks Notebooks\" if is_in_databricks_runtime() else \"\"\n                raise MlflowException(\n                    message=(\n                        f\"Failed to copy the specified code path '{code_path}' into the model \"\n                        \"artifacts. It appears that your code path includes file(s) that cannot \"\n                        f\"be copied{example}. Please specify a code path that does not include \"\n                        \"such files and try again.\",\n                    ),\n                    error_code=INVALID_PARAMETER_VALUE,\n                ) from e\n    else:\n        code_dir_subpath = None\n    return code_dir_subpath\n```"
      }
    ]
  },
  {
    "issue_number": 15692,
    "title": "mlflow.openai.autolog() raises 'NotGiven' object is not iterable when tracing with Ollama + Agents SDK",
    "author": "dauvannam1804",
    "state": "closed",
    "created_at": "2025-05-12T16:25:46Z",
    "updated_at": "2025-05-15T14:52:10Z",
    "labels": [],
    "body": "### Please read this first\n\n- **Have you read the docs?**[Agents SDK docs](https://openai.github.io/openai-agents-python/) Yes\n- **Have you searched for related issues?** Yes https://github.com/openai/openai-agents-python/issues/233\n\n### Question\nI'm trying out tracing following the example in the [MLflow OpenAI Agent integration guide](https://mlflow.org/docs/latest/tracing/integrations/openai-agent), using a local Ollama model.\n\nAfter following the suggestion in [this issue](https://github.com/openai/openai-agents-python/issues/233), the previous error\n```\nTracing client error 401: {\n  \"error\": {\n    \"message\": \"Incorrect API key provided: AIzaSyAe***************************o4jk.\",\n    ...\n}\n```\nis gone. However, I now encounter this new warning and error:\n```\n2025/05/12 11:39:52 WARNING mlflow.openai._openai_autolog: Encountered unexpected error when ending trace: 'NotGiven' object is not iterable\nTraceback (most recent call last):\n  File \".../mlflow/openai/_openai_autolog.py\", line 352, in _end_span_on_success\n    set_span_chat_attributes(span, inputs, result)\n  File \".../mlflow/openai/utils/chat_schema.py\", line 46, in set_span_chat_attributes\n    if tools := _parse_tools(inputs):\n  File \".../mlflow/openai/utils/chat_schema.py\", line 261, in _parse_tools\n    for tool in tools:\nTypeError: 'NotGiven' object is not iterable\n```\n\nMy setup:\n```\nimport asyncio\n\nimport mlflow\nfrom agents import Agent, OpenAIChatCompletionsModel, Runner\nfrom agents.model_settings import ModelSettings\nfrom openai import AsyncOpenAI\nfrom agents import set_default_openai_client, set_tracing_disabled, set_trace_processors\n\n\nexternal_client = AsyncOpenAI(\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"ollama\",  # required, but unused\n)\n\n# Enable auto tracing for OpenAI Agents SDK\nmlflow.openai.autolog()\n\n# Optional: Set a tracking URI and an experiment\nmlflow.set_tracking_uri(\"http://localhost:8080\")\nmlflow.set_experiment(\"OpenAI Agent\")\nset_trace_processors([])\n\n# Define a simple multi-agent workflow\nvietnamese_agent = Agent(\n    name=\"Vietnamese agent\",\n    instructions=\"You only speak Vietnamese.\",\n    model=OpenAIChatCompletionsModel(\n        model=\"qwen3:0.6b\",\n        openai_client=external_client,\n    ),\n)\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n    model=OpenAIChatCompletionsModel(\n        model=\"qwen3:0.6b\",\n        openai_client=external_client,\n    ),\n)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n    handoffs=[vietnamese_agent, english_agent],\n    model=OpenAIChatCompletionsModel(\n        model=\"qwen3:0.6b\",\n        openai_client=external_client,\n    ),\n)\n\n\nasync def main():\n    result = await Runner.run(triage_agent, input=\"Xin chÃ o, báº¡n cÃ³ khá»e khÃ´ng?\")\n    print(result.final_output)\n\n\n# If you are running this code in a Jupyter notebook, replace this with `await main()`.\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\nOutput\n```\npython tracing_example.py \nINFO:httpx:HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n2025/05/12 11:39:52 WARNING mlflow.openai._openai_autolog: Encountered unexpected error when ending trace: 'NotGiven' object is not iterable\nTraceback (most recent call last):\n  File \"/home/namdv/anaconda3/envs/chatbot/lib/python3.10/site-packages/mlflow/openai/_openai_autolog.py\", line 352, in _end_span_on_success\n    set_span_chat_attributes(span, inputs, result)\n  File \"/home/namdv/anaconda3/envs/chatbot/lib/python3.10/site-packages/mlflow/openai/utils/chat_schema.py\", line 46, in set_span_chat_attributes\n    if tools := _parse_tools(inputs):\n  File \"/home/namdv/anaconda3/envs/chatbot/lib/python3.10/site-packages/mlflow/openai/utils/chat_schema.py\", line 261, in _parse_tools\n    for tool in tools:\nTypeError: 'NotGiven' object is not iterable\n<think>\nOkay, the user sent a greeting in Vietnamese. I need to respond in Vietnamese. Let me check the previous messages again. The user asked, \"Xin chÃ o, báº¡n cÃ³ khá»e khÃ´ng?\" which translates to \"Hello, are you healthy?\" in Vietnamese. My role is to keep Vietnamese. So, I should respond in Vietnamese as well. Make sure the answer is polite and friendly. Maybe say something like \"Hello! Are you healthy?\" and offer a response. Keep it simple.\n</think>\n\nXin chÃ o! Báº¡n cÃ³ khá»e khÃ´ng? ðŸ˜Š\n```\n\nQuestions:\nHow can I avoid this 'NotGiven' object is not iterable error when using mlflow.openai.autolog() with Ollama backend?\n\nThanks in advance!\n",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "Thanks for the report, and thank you @joelrobin18 for the PR. I'll take a look."
      }
    ]
  },
  {
    "issue_number": 15725,
    "title": "[DOC-FIX] Deploy MLflow Model to Kubernetes page has broken link to Seldon docs.",
    "author": "JustGitting",
    "state": "closed",
    "created_at": "2025-05-14T07:26:59Z",
    "updated_at": "2025-05-15T10:20:21Z",
    "labels": [
      "area/docs",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute a documentation fix at this time.\n\n### URL(s) with the issue\n\nhttps://mlflow.org/docs/latest/deployment/deploy-model-to-kubernetes/index.html\n\n### Description of proposal (what needs changing)\n\nHi,\n\nThe following link is broken on the Deploy MLflow Model to Kubernetes https://mlflow.org/docs/latest/deployment/deploy-model-to-kubernetes/index.html page.\n\nhttps://docs.seldon.io/projects/seldon-core/en/latest/servers/mlflow.html\n\nIt shows the error: \n\n> Sorry. Seldon Core 2 page was not found!\n\nI've search for an alternative link, but not sure what it originally contained. Maybe the replacement page is this:\n\nhttps://mlserver.readthedocs.io/en/latest/examples/mlflow/README.html",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "Thank you for the report, filed a PR for the fix."
      }
    ]
  },
  {
    "issue_number": 15689,
    "title": "[BUG] Gunicorn arg --workers has no effect while runnning the docker container for inference",
    "author": "NemanjaML",
    "state": "closed",
    "created_at": "2025-05-12T13:44:17Z",
    "updated_at": "2025-05-15T09:37:12Z",
    "labels": [
      "bug",
      "area/scoring",
      "has-closing-pr"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.22.0\n\n\n### System information\n\n- **OS Platform and Distribution: Linux Ubuntu 22.04\n- **Python version: 3.10.14\n\n\n### Describe the problem\n\nSince you moved from using Flask+Gunicorn to FastAPI+Uvicorn in the last few versions, we are unable to define the number of workers for running the docker container for inference. Now container always takes all the available workers (CPU cores) which leads to excessive use of resources. \n\nWe noticed that you removed that option in the pull request: https://github.com/mlflow/mlflow/pull/14800 but we are not sure now how to set the number of workers with the new setup, if its possible at all, because the comment we saw is:  \n\nTomeHirata on Mar 3 â€¢ \nWe could change the logic a bit to read \"MLFLOW_WORKERS\", but this GUNICORN_CMD_ARGS feature was not documented anywhere and worked for flask & non-windows cases only. So I guess we can wait until requested.\n\n\nThanks a lot in advance!\n\n### Tracking information\n\n_No response_\n\n### Code to reproduce issue\n\ndocker run --name test -p 5008:8080 --env GUNICORN_CMD_ARGS=\"--workers=1\" test\n\n\n### Stack trace\n\nno stack trace\n\n\n### Other info / logs\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [x] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "Thank you for the issue, we'll enable you to specify the worker count for the uvicorn process in the docker container"
      }
    ]
  },
  {
    "issue_number": 15652,
    "title": "[BUG] PydanticDeprecatedSince20 warning for 'min_items' in mlflow.types.chat.BaseRequestPayload with Pydantic V2",
    "author": "Eric-LANGE",
    "state": "closed",
    "created_at": "2025-05-09T14:44:02Z",
    "updated_at": "2025-05-15T09:29:20Z",
    "labels": [
      "bug",
      "area/model-registry",
      "has-closing-pr"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\nMLflow Version: 2.22.0\n\n\n### System information\n\n\npydantic: 2.11.4         \npydantic-core: 2.33.2 \nPython Version: 3.11.12\nOperating System: Fedora Workstation\nMLflow installed in a Conda environment\n\n\n### Describe the problem\n\nWhen using mlflow==2.22.0 with a Pydantic V2.x environment, PydanticDeprecatedSince20 warnings are triggered due to mlflow's internal use of min_items in its Pydantic models.\nMention that Pydantic V2 recommends using min_length instead of min_items.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\n# Filename: reproduce_mlflow_pydantic_warning_v2.py\n\nimport warnings\nimport importlib\nimport sys\n\n# Attempt to import the specific Pydantic warning class\nPydanticDeprecatedSince20 = None\nPYDANTIC_WARNING_CLASS_AVAILABLE = False\npydantic_version = \"Not available\"\n\ntry:\n    # Import the pydantic.warnings submodule\n    pydantic_warnings_module = importlib.import_module('pydantic.warnings')\n    # Access the PydanticDeprecatedSince20 class from this submodule\n    PydanticDeprecatedSince20 = pydantic_warnings_module.PydanticDeprecatedSince20\n    PYDANTIC_WARNING_CLASS_AVAILABLE = True\n\n    # Get Pydantic version for context\n    pydantic_core_module = importlib.import_module('pydantic')\n    pydantic_version = pydantic_core_module.__version__\n\nexcept ImportError:\n    print(\"Failed to import 'pydantic.warnings' or 'PydanticDeprecatedSince20' class not found in it.\")\n    print(\"Please ensure Pydantic v2+ is installed correctly.\")\nexcept AttributeError:\n    print(\"Failed to find 'PydanticDeprecatedSince20' within 'pydantic.warnings'.\")\n    print(\"This might indicate an unexpected Pydantic version or structure.\")\n\n# Try to import mlflow to get its version and then its types\nmlflow_version = \"Not available\"\ntry:\n    mlflow_module = importlib.import_module('mlflow')\n    MLFLOW_AVAILABLE = True\n    mlflow_version = mlflow_module.__version__\nexcept ImportError:\n    print(\"MLflow library is not available. This script requires MLflow to run.\")\n    MLFLOW_AVAILABLE = False\n\nif not PYDANTIC_WARNING_CLASS_AVAILABLE or not MLFLOW_AVAILABLE:\n    print(\"Exiting due to missing prerequisites (Pydantic warning class or MLflow).\")\n    sys.exit(1)\n\nprint(f\"--- System Information ---\")\nprint(f\"Python version: {sys.version.split()[0]}\")\nprint(f\"MLflow version: {mlflow_version}\")\nprint(f\"Pydantic version: {pydantic_version}\")\nprint(f\"--------------------------\\n\")\n\nwarnings.filterwarnings(\n    \"error\",\n    message=r\".*`min_items` is deprecated.*use `min_length` instead.*\", # Updated regex\n    category=PydanticDeprecatedSince20 # Use the directly imported class\n)\n\nprint(\"Attempting to import 'mlflow.types.chat'...\")\nprint(\"This import is expected to trigger the PydanticDeprecatedSince20 warning for 'min_items' from within MLflow's codebase.\")\n\ntry:\n    import mlflow.types.chat\n\n    print(\"\\nImport of 'mlflow.types.chat' was successful.\")\n    print(\"If the PydanticDeprecatedSince20 warning was not raised as an error,\")\n    print(\"it might indicate that the warning is filtered differently in this script,\")\n    print(\"the issue is not triggered by this specific import path alone in this context,\")\n    print(\"or it has been fixed in your current library versions.\")\n\nexcept PydanticDeprecatedSince20 as e: # Use the directly imported class here too\n    print(f\"\\n>>> SUCCESS: Caught the expected PydanticDeprecatedSince20 warning as an error <<<\")\n    print(f\"Error message: {e}\")\n    print(\"\\nThis confirms that importing 'mlflow.types.chat' (or its dependencies)\")\n    print(\"triggers the deprecated use of 'min_items' within the MLflow library.\")\n    print(\"The traceback for this error (if run directly) will point to the MLflow source code.\")\n\nexcept Exception as e_other:\n    print(f\"\\nAn unexpected error occurred during the import of 'mlflow.types.chat':\")\n    print(f\"{type(e_other).__name__}: {e_other}\")\n    print(\"This could be due to other issues or if the warning was not correctly filtered as an error.\")\n\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\n================================================================================ test session starts =================================================================================\nplatform linux -- Python 3.11.12, pytest-8.3.5, pluggy-1.5.0\nrootdir: /home/eric/Bureau/project_p7\nplugins: anyio-4.9.0\ncollected 0 items / 1 error                                                                                                                                                          \n\n======================================================================================= ERRORS =======================================================================================\n_________________________________________________________________________ ERROR collecting tests/test_app.py _________________________________________________________________________\ntests/test_app.py:16: in <module>\n    from credit_risk_app.main import app\nsrc/credit_risk_app/main.py:15: in <module>\n    import mlflow.pyfunc\n../../.anaconda3/envs/credit_risk_env/lib/python3.11/site-packages/mlflow/pyfunc/__init__.py:427: in <module>\n    import mlflow.pyfunc.loaders\n../../.anaconda3/envs/credit_risk_env/lib/python3.11/site-packages/mlflow/pyfunc/loaders/__init__.py:1: in <module>\n    import mlflow.pyfunc.loaders.chat_agent\n../../.anaconda3/envs/credit_risk_env/lib/python3.11/site-packages/mlflow/pyfunc/loaders/chat_agent.py:8: in <module>\n    from mlflow.pyfunc.model import (\n../../.anaconda3/envs/credit_risk_env/lib/python3.11/site-packages/mlflow/pyfunc/model.py:40: in <module>\n    from mlflow.types.agent import (\n../../.anaconda3/envs/credit_risk_env/lib/python3.11/site-packages/mlflow/types/agent.py:5: in <module>\n    from mlflow.types.chat import BaseModel, ChatUsage, ToolCall\n../../.anaconda3/envs/credit_risk_env/lib/python3.11/site-packages/mlflow/types/chat.py:157: in <module>\n    class BaseRequestPayload(BaseModel):\n../../.anaconda3/envs/credit_risk_env/lib/python3.11/site-packages/mlflow/types/chat.py:162: in BaseRequestPayload\n    stop: Optional[list[str]] = Field(None, min_items=1)\n../../.anaconda3/envs/credit_risk_env/lib/python3.11/site-packages/pydantic/fields.py:1045: in Field\n    warn('`min_items` is deprecated and will be removed, use `min_length` instead', DeprecationWarning)\nE   pydantic.warnings.PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n============================================================================== short test summary info ===============================================================================\nERROR tests/test_app.py - pydantic.warnings.PydanticDeprecatedSince20: `min_items` is deprecated and will be removed, use `min_length` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pyda...\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n================================================================================== 1 error in 1.35s ==================================================================================\n\n\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [x] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "Thanks for the report, filed a PR to fix it."
      }
    ]
  },
  {
    "issue_number": 2922,
    "title": "[FR] Ability to search for missing values/NAs using search_runs",
    "author": "andycraig",
    "state": "open",
    "created_at": "2020-06-11T09:01:04Z",
    "updated_at": "2025-05-15T07:25:11Z",
    "labels": [
      "enhancement",
      "area/tracking",
      "priority/backlog"
    ],
    "body": "## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\nMaybe. I would need to discuss with my organisation.\r\n\r\n## Proposal Summary\r\n\r\nA way to search for missing values/NAs using the `MlflowClient().search_runs()` parameter `filter_string`.\r\n\r\n## Motivation\r\n\r\nMy use case is searching for runs completed before a parameter was added.\r\n\r\nRuns completed before adding the parameter 'my_new_param' show a '-' under 'my_new_param' in the Parameters section of the UI. I would like to search for these runs programmatically. I cannot see a way to do it using https://github.com/mlflow/mlflow/blob/4ceeb45b3a72b6ae596a017d031cfa816098986b/mlflow/utils/search_utils.py I have tried using a `filter_string` containing `\"params.my_new_param = '-'\"` and `\"params.my_new_param = ''\"` but they do not seem to work.\r\n\r\nMy current workaround is to export a CSV with `mlflow experiments csv`, read it back in and filter using `isna()` from pandas.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for\r\nModel Registry\r\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: Local serving, model deployment tools, spark UDFs\r\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area/uiux`: Front-end, user experience, JavaScript, plotting\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n\r\nIntegrations\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n\r\n## Details\r\n\r\nApologies in advance if this is already possible and I have missed it, or if my 'use case' is actually the result of a bad MLflow workflow. In either of these cases, I'd be very pleased to hear what I should be doing instead!\r\n\r\nThank you.",
    "comments": [
      {
        "user": "smurching",
        "body": "@andycraig I think your current workaround is the current-best way to identify runs that are missing a particular param (you could also use the `mlflow.search_runs()` API without a filter to get the runs as a pandas DataFrame, and then call `isna` directly on the DataFrame, if you want to do this entirely programatically).\r\n\r\nI'd be happy to brainstorm ways to enhance the search API to support your use case, although I suspect it might require some pretty complex/slow-running backend logic (i.e. the added complexity may not be worth it). In particular, note that currently in the SQL backend, `metrics`, `params`, and `runs` are stored in their own tables, and so it seems tricky to write a SQL query to identify runs for which no corresponding param named `my_new_param` exists (AFAICT)."
      },
      {
        "user": "andycraig",
        "body": "Hi @smurching, thank you for considering. Thank you also for the suggestion of `mlflow.search_runs()` without a filter - I've switched to that and it's much simpler.\r\n\r\nIn terms of enhancing the search API I could imagine something like:\r\n\r\n```sql\r\nparams.my_new_param is null and metrics.error <= 0.05\r\n```\r\n\r\nBut I'm entirely happy with the `mlflow.search_runs()` pandas approach, so if there hasn't been much demand for this feature and adding it to the search API would be complicated then it could probably be left as-is."
      },
      {
        "user": "albertotb",
        "body": "Another use case I've just run into is to search for runs that are parent runs by checking wether the tag \"tags.`mlflow.parentRunId`\" is null or not. For some reason MLFlow does not collapse child runs until I click \"Load more\""
      }
    ]
  },
  {
    "issue_number": 10051,
    "title": "[BUG] Prompt engineering UI does not accept large prompts",
    "author": "calwoo",
    "state": "open",
    "created_at": "2023-10-20T18:56:35Z",
    "updated_at": "2025-05-15T02:28:29Z",
    "labels": [
      "bug",
      "area/tracking"
    ],
    "body": "### Issues Policy acknowledgement\r\n\r\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\r\n\r\n### Where did you encounter this bug?\r\n\r\nLocal machine\r\n\r\n### Willingness to contribute\r\n\r\nYes. I can contribute a fix for this bug independently.\r\n\r\n### MLflow version\r\n\r\n- Client: 2.7.1\r\n- Tracking server: 2.7.1\r\n\r\n\r\n### System information\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 22.04\r\n- **Python version**: 3.10\r\n\r\n\r\n### Describe the problem\r\n\r\nIf the prompt template is too large, the run will fail to generate in the prompt engineering UI. Take for example the tutorial, but with the full documentation replaced in the prompt template\r\n\r\n```\r\nRead the following article from the MLflow documentation that appears between triple\r\nbackticks. Then, answer the question about the documentation that appears between triple quotes.\r\nInclude relevant links and code examples in your answer.\r\n\r\n'''\r\nYou can log data to runs using the MLflow Python, R, Java, or REST API. This section shows the Python API.\r\n\r\nIn this section:\r\n\r\n[Logging functions](https://mlflow.org/docs/latest/tracking.html#logging-functions)\r\n\r\n[Launching Multiple Runs in One Program](https://mlflow.org/docs/latest/tracking.html#launching-multiple-runs-in-one-program)\r\n\r\n[Performance Tracking with Metrics](https://mlflow.org/docs/latest/tracking.html#performance-tracking-with-metrics)\r\n\r\n[Visualizing Metrics](https://mlflow.org/docs/latest/tracking.html#visualizing-metrics)\r\n\r\n[Logging functions](https://mlflow.org/docs/latest/tracking.html#id64)\r\n[mlflow.set_tracking_uri()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_tracking_uri) connects to a tracking URI. You can also set the MLFLOW_TRACKING_URI environment variable to have MLflow find a URI from there. In both cases, the URI can either be a HTTP/HTTPS URI for a remote server, a database connection string, or a local path to log data to a directory. The URI defaults to mlruns.\r\n\r\n[mlflow.get_tracking_uri()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.get_tracking_uri) returns the current tracking URI.\r\n\r\n[mlflow.create_experiment()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.create_experiment) creates a new experiment and returns its ID. Runs can be launched under the experiment by passing the experiment ID to mlflow.start_run.\r\n\r\n[mlflow.set_experiment()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_experiment) sets an experiment as active. If the experiment does not exist, creates a new experiment. If you do not specify an experiment in [mlflow.start_run()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.start_run), new runs are launched under this experiment.\r\n\r\n[mlflow.start_run()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.start_run) returns the currently active run (if one exists), or starts a new run and returns a [mlflow.ActiveRun](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.ActiveRun) object usable as a context manager for the current run. You do not need to call start_run explicitly: calling one of the logging functions with no active run automatically starts a new one.\r\n\r\nNote\r\n\r\nIf the argument run_name is not set within [mlflow.start_run()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.start_run), a unique run name will be generated for each run.\r\n\r\n[mlflow.end_run()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.end_run) ends the currently active run, if any, taking an optional run status.\r\n\r\n[mlflow.active_run()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.active_run) returns a [mlflow.entities.Run](https://mlflow.org/docs/latest/python_api/mlflow.entities.html#mlflow.entities.Run) object corresponding to the currently active run, if any. Note: You cannot access currently-active run attributes (parameters, metrics, etc.) through the run returned by mlflow.active_run. In order to access such attributes, use the [MlflowClient](https://mlflow.org/docs/latest/python_api/mlflow.client.html#mlflow.client.MlflowClient) as follows:\r\n\r\nclient = mlflow.MlflowClient()\r\ndata = client.get_run(mlflow.active_run().info.run_id).data\r\n\r\n[mlflow.last_active_run()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.last_active_run) returns a [mlflow.entities.Run](https://mlflow.org/docs/latest/python_api/mlflow.entities.html#mlflow.entities.Run) object corresponding to the currently active run, if any. Otherwise, it returns a [mlflow.entities.Run](https://mlflow.org/docs/latest/python_api/mlflow.entities.html#mlflow.entities.Run) object corresponding the last run started from the current Python process that reached a terminal status (i.e. FINISHED, FAILED, or KILLED).\r\n\r\n[mlflow.get_parent_run()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.get_parent_run) returns a [mlflow.entities.Run](https://mlflow.org/docs/latest/python_api/mlflow.entities.html#mlflow.entities.Run) object corresponding to the parent run for the given run id, if one exists. Otherwise, it returns None.\r\n\r\n[mlflow.log_param()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_param) logs a single key-value param in the currently active run. The key and value are both strings. Use [mlflow.log_params()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_params) to log multiple params at once.\r\n\r\n[mlflow.log_metric()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metric) logs a single key-value metric. The value must always be a number. MLflow remembers the history of values for each metric. Use [mlflow.log_metrics()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metrics) to log multiple metrics at once.\r\n\r\n[mlflow.log_input()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_input) logs a single [mlflow.data.dataset.Dataset](https://mlflow.org/docs/latest/python_api/mlflow.data.html#mlflow.data.dataset.Dataset) object corresponding to the currently active run. You may also log a dataset context string and a dict of key-value tags.\r\n\r\n[mlflow.set_tag()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_tag) sets a single key-value tag in the currently active run. The key and value are both strings. Use [mlflow.set_tags()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_tags) to set multiple tags at once.\r\n\r\n[mlflow.log_artifact()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact) logs a local file or directory as an artifact, optionally taking an artifact_path to place it in within the runâ€™s artifact URI. Run artifacts can be organized into directories, so you can place the artifact in a directory this way.\r\n\r\n[mlflow.log_artifacts()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifacts) logs all the files in a given directory as artifacts, again taking an optional artifact_path.\r\n\r\n[mlflow.get_artifact_uri()](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.get_artifact_uri) returns the URI that artifacts from the current run should be logged to.\r\n'''\r\n\r\n\"\"\"\r\n{{question}}\r\n\"\"\"\r\n```\r\n\r\nThis will fail, despite the fact that the original prompt in https://mlflow.org/docs/latest/llms/prompt-engineering.html#step-6-try-a-prompt-of-your-choosing is and the documentation injected as a variable creates the same prompt to the model endpoint.\r\n\r\n### Tracking information\r\n\r\nN/A\r\n\r\n### Code to reproduce issue\r\n\r\nN/A\r\n\r\n### Stack trace\r\n\r\nN/A\r\n\r\n### Other info / logs\r\n\r\nThere are no logs in the underlying Postgres DB backend, it seems this run just fails silently.\r\n\r\n### What component(s) does this bug affect?\r\n\r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/gateway`: AI Gateway service, Gateway client APIs, third-party Gateway integrations\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [X] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n### What interface(s) does this bug affect?\r\n\r\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\n### What language(s) does this bug affect?\r\n\r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\n### What integration(s) does this bug affect?\r\n\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "BenWilson2",
        "body": "@sunishsheth2009 do you have any insight into this mechanism?"
      },
      {
        "user": "sunishsheth2009",
        "body": "Hey @calwoo, thank you for trying out prompt lab UI. :) \r\nCan you share the error message you are seeing?\r\nI just tried the exact same prompt with one of out models and it did return back the results as expected. Let me know if I am missing something? \r\n![screenshot](https://github.com/mlflow/mlflow/assets/5621432/ec44d6cb-a958-44a3-8169-1ad54e7a1567)\r\n"
      },
      {
        "user": "calwoo",
        "body": "Thanks @sunishsheth2009! The error is fairly silent-- I can't see anything in the pod logs. Seeing the output is fine-- the issue is when I try to `Create run`-- the resulting run failed and no table is populated as an artifact."
      }
    ]
  },
  {
    "issue_number": 6195,
    "title": "[DOC-FIX] Document models:/ URIs explicitly in OSS MLflow docs",
    "author": "smurching",
    "state": "closed",
    "created_at": "2022-07-05T21:05:02Z",
    "updated_at": "2025-05-14T08:47:19Z",
    "labels": [
      "good first issue",
      "area/docs",
      "help wanted"
    ],
    "body": "### Willingness to contribute\r\n\r\nNo. I cannot contribute a documentation fix at this time.\r\n\r\n### URL(s) with the issue\r\n\r\nhttps://mlflow.org/docs/latest/tracking.html#artifact-stores\r\n\r\n### Description of proposal (what needs changing)\r\n\r\nI wanted to link to what an â€œMLflow model URIâ€ could be in a design doc, and searched for it in MLflow docs. It looks like we donâ€™t actually describe supported model URIs except in [CLI docs](https://mlflow.org/docs/latest/cli.html#cmdoption-mlflow-azureml-build-image-m) which say\r\n\r\n```\r\nRequired URI to the model. A local path, a 'runs:/' URI, or a remote storage URI (e.g., an 's3://' URI). For more information about supported remote URIs for model artifacts, see https://mlflow.org/docs/latest/tracking.html#artifact-stores\r\n```\r\n\r\nHowever the [linked artifacts doc page ](https://mlflow.org/docs/latest/tracking.html#artifact-stores)doesnâ€™t say anything about `models:/` URIs. Currently, it looks like we document `models:/` URIs in the model registry page: https://mlflow.org/docs/latest/model-registry.html#fetching-an-mlflow-model-from-the-model-registry.\r\n\r\nTo address this, one idea is to document model registry `models:/` artifact storage and `runs:/` artifact storage in https://mlflow.org/docs/latest/tracking.html#artifact-stores. We could also add a more explicit section documenting `models:/` URIs in the model registry doc page and link to it from the tracking page",
    "comments": [
      {
        "user": "mrhimanshu",
        "body": "Is anyone working on this ? If not then, I can take it up."
      },
      {
        "user": "mlflow-automation",
        "body": "<!-- assign-maintainer -->\n@BenWilson2 @dbczumar @harupy @WeichenXu123 Please assign a maintainer and start triaging this issue."
      },
      {
        "user": "mlflow-automation",
        "body": "This issue is stale because it has been open 7 days with no activity. Remove stale label or comment or this will be closed in 14 days."
      }
    ]
  },
  {
    "issue_number": 11642,
    "title": "[BUG] Unable to permanently delete an experiment - invalid URI scheme 'file'",
    "author": "ondratkadlec",
    "state": "closed",
    "created_at": "2024-04-08T08:47:13Z",
    "updated_at": "2025-05-14T08:31:46Z",
    "labels": [
      "bug",
      "area/artifacts",
      "area/tracking",
      "has-closing-pr"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### Willingness to contribute\n\nNo. I cannot contribute a bug fix at this time.\n\n### MLflow version\n\n- Client: 2.11.3\r\n- Tracking server: 2.11.3\r\n\n\n### System information\n\n- Ubuntu 22.04\r\n- Python 3.10\r\n\n\n### Describe the problem\n\nI run a mlflow server on a remote linux machine (e.g. 10.83.182.46, port 8001). I want to store the artifact on this machine and the logs in an sqlite DB. So I run '/home/projects/.local/bin/mlflow server --backend-store-uri sqlite:////home/projects/mlflow/mlruns.db --artifacts-destination /home/projects/mlflow/artifacts -h 0.0.0.0 -p 8001'.\r\nEverything runs smoothly, I can access the http://10.83.182.46:8001, and if I create and experiment, I can see it together with the artifacts (that I am also able to download). On the linux server, I can see the /home/projects/mlflow/artifacts folder. \r\nThe problem starts when I manually delete the experiment in UI. I cannot see the experiment in UI, but running '/home/projects/.local/bin/mlflow gc sqlite:////home/projects/mlflow/mlruns.db' results in error 'mlflow.exceptions.MlflowException: The configured tracking uri scheme: 'file' is invalid for use with the proxy mlflow-artifact scheme. The allowed tracking schemes are: {'https', 'http'}'. I tried changing the artifacts-location and play with other stuff, but in the end, this error appears every time.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```shell\r\nREPLACE_ME\r\n```\r\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nimport mlflow\r\n\r\nMLFLOW_TRACKING_URI = \"http://10.83.182.46:8001\"\r\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\r\nmlflow.set_experiment(\"my_experiment\")\r\n\r\n\r\n# Create a run\r\nwith mlflow.start_run(run_name='My Run') as run:\r\n    mlflow.log_param(\"param1\", 5)\r\n    mlflow.log_metric(\"metric1\", 1)\r\n    mlflow.log_artifact(local_path=\"FEATURES.parquet\")\r\n    mlflow.log_artifact(local_path=\"balance.png\")\r\n    mlflow.end_run()\r\n```\r\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\n/home/projects/.local/bin/mlflow gc --backend-store-uri sqlite:////home/projects/mlflow/mlruns.db\r\nTraceback (most recent call last):\r\n  File \"/home/projects/.local/bin/mlflow\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/home/adlec001/.local/lib/python3.10/site-packages/click/core.py\", line 1157, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/adlec001/.local/lib/python3.10/site-packages/click/core.py\", line 1078, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/adlec001/.local/lib/python3.10/site-packages/click/core.py\", line 1688, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/adlec001/.local/lib/python3.10/site-packages/click/core.py\", line 1434, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/adlec001/.local/lib/python3.10/site-packages/click/core.py\", line 783, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/adlec001/.local/lib/python3.10/site-packages/mlflow/cli.py\", line 605, in gc\r\n    artifact_repo = get_artifact_repository(run.info.artifact_uri)\r\n  File \"/home/adlec001/.local/lib/python3.10/site-packages/mlflow/store/artifact/artifact_repository_registry.py\", line 124, in get_artifact_repository\r\n    return _artifact_repository_registry.get_artifact_repository(artifact_uri)\r\n  File \"/home/adlec001/.local/lib/python3.10/site-packages/mlflow/store/artifact/artifact_repository_registry.py\", line 77, in get_artifact_repository\r\n    return repository(artifact_uri)\r\n  File \"/home/adlec001/.local/lib/python3.10/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\", line 45, in __init__\r\n    super().__init__(self.resolve_uri(artifact_uri, get_tracking_uri()))\r\n  File \"/home/otkadlec001/.local/lib/python3.10/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\", line 59, in resolve_uri\r\n    _validate_uri_scheme(track_parse.scheme)\r\n  File \"/home/adlec001/.local/lib/python3.10/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\", line 35, in _validate_uri_scheme\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: The configured tracking uri scheme: 'file' is invalid for use with the proxy mlflow-artifact scheme. The allowed tracking schemes are: {'https', 'http'}\r\n```\r\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nREPLACE_ME\r\n```\r\n\n\n### What component(s) does this bug affect?\n\n- [X] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [X] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "@ondratkadlec Thanks for reporting this! I was able to reproduce the error. I'm investigating how it happens."
      },
      {
        "user": "github-actions[bot]",
        "body": "<!-- assign-maintainer -->\n@mlflow/mlflow-team Please assign a maintainer and start triaging this issue."
      },
      {
        "user": "oleg-z",
        "body": "Not a contributor. Please let me know if it's ok to comment and provide workaround and happy to contribute once I figure out how I can help. \r\n\r\nIt seems the issue is when running `mlflow gc` utils.get_tracking_uri() is not set defaults to file schema. Unfortunatelly GC needs API interface in order to issue delete API calls. \r\n\r\nWorkaround is to set `MLFLOW_TRACKING_URI` env var to point to your MLFlow installation. In your case following code should properly execute:\r\n```\r\nexport MLFLOW_TRACKING_URI=http://10.83.182.46:8001\r\n/home/projects/.local/bin/mlflow gc --backend-store-uri sqlite:////home/projects/mlflow/mlruns.db\r\n```\r\n\r\nExample of before and after setting environment variable:\r\nBefore:\r\n```\r\nolegz@dev mlflow % mlflow gc --backend-store-uri sqlite:///$(pwd)/mlruns.db\r\n...\r\nFile \"/Users/olegz/Library/Python/3.9/lib/python/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\", line 35, in _validate_uri_scheme\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: The configured tracking uri scheme: 'file' is invalid for use with the proxy mlflow-artifact scheme. The allowed tracking schemes are: {'https', 'http'}\r\n```\r\n\r\nAfter:\r\n```\r\nolegz@dev mlflow % export MLFLOW_TRACKING_URI=http://127.0.0.1:8080\r\nolegz@dev mlflow % mlflow gc --backend-store-uri sqlite:///$(pwd)/mlruns.db\r\nRun with ID 7df4f883bdcc466c90405637cabe62c6 has been permanently deleted.\r\nRun with ID 5d5445d3b97845c4a6d6afe798625d3a has been permanently deleted.\r\nRun with ID c2133b7cf23a4a8586b9ce2fd7dd9b6f has been permanently deleted.\r\nRun with ID d967acc86d1642afac33d89e2fbb68aa has been permanently deleted.\r\nRun with ID b706a93dc78f46dfb6c3a2e27641bd6f has been permanently deleted.\r\nExperiment with ID 1 has been permanently deleted.\r\n```\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 7819,
    "title": "[BUG] log_artifact fails when tracking uri scheme is 'file'",
    "author": "yossibiton",
    "state": "closed",
    "created_at": "2023-02-12T12:33:05Z",
    "updated_at": "2025-05-14T08:31:46Z",
    "labels": [
      "bug",
      "area/artifacts",
      "area/tracking",
      "has-closing-pr"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Willingness to contribute\n\nNo. I cannot contribute a bug fix at this time.\n\n### MLflow version\n\n- Client: 2.1.1\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 22.04\r\n- **Python version**:  3.9.12\n\n### Describe the problem\n\nI'm using ml flow on localhost (as described [here](https://mlflow.org/docs/latest/tracking.html#scenario-1-mlflow-on-localhost)). When I call log_artifact for live run I get the following error : \r\n`mlflow.exceptions.MlflowException: The configured tracking uri scheme: 'file' is invalid for use with the proxy mlflow-artifact scheme. The allowed tracking schemes are: {'https', 'http'}\r\n`\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```shell\r\nMLflow module location: /home/ubuntu/miniconda3/lib/python3.9/site-packages/mlflow/__init__.py\r\nTracking URI: file:///efs/mlflow/mlruns\r\nRegistry URI: file:///efs/mlflow/mlruns\r\nMLflow environment variables: \r\n  MLFLOW_EXPERIMENT_NAME: object_detection\r\nMLflow dependencies: \r\n  Flask: 2.2.2\r\n  Jinja2: 3.1.2\r\n  alembic: 1.9.3\r\n  click: 8.1.3\r\n  cloudpickle: 2.2.1\r\n  databricks-cli: 0.17.4\r\n  docker: 6.0.1\r\n  entrypoints: 0.4\r\n  gitpython: 3.1.30\r\n  gunicorn: 20.1.0\r\n  importlib-metadata: 5.0.0\r\n  markdown: 3.4.1\r\n  matplotlib: 3.6.2\r\n  numpy: 1.23.4\r\n  packaging: 21.3\r\n  pandas: 1.5.1\r\n  protobuf: 3.20.3\r\n  pyarrow: 10.0.1\r\n  pytz: 2022.6\r\n  pyyaml: 6.0\r\n  querystring-parser: 1.2.4\r\n  requests: 2.27.1\r\n  scikit-learn: 1.2.1\r\n  scipy: 1.9.3\r\n  shap: 0.41.0\r\n  sqlalchemy: 1.4.46\r\n  sqlparse: 0.4.3\r\n\r\n```\r\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nimport mlflow\r\nmlflow_exp_name = 'object_detection'\r\nmlflow.set_tracking_uri('file:///efs/mlflow/mlruns')\r\nos.environ['MLFLOW_EXPERIMENT_NAME'] = mlflow_exp_name\r\n\r\npath_html = 'example.html'\r\nwith open(path_html, \"w\") as f:\r\n    f.write('')\r\nwith mlflow.start_run():\r\n    mlflow.log_artifact(path_html)\r\n\r\n```\r\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nTraceback (most recent call last):\r\n  File \"/efs/demo.py\", line 36, in <module>\r\n    mlflow.log_artifact(path_html)\r\n  File \"/home/ubuntu/miniconda3/lib/python3.9/site-packages/mlflow/tracking/fluent.py\", line 776, in log_artifact\r\n    MlflowClient().log_artifact(run_id, local_path, artifact_path)\r\n  File \"/home/ubuntu/miniconda3/lib/python3.9/site-packages/mlflow/tracking/client.py\", line 1002, in log_artifact\r\n    self._tracking_client.log_artifact(run_id, local_path, artifact_path)\r\n  File \"/home/ubuntu/miniconda3/lib/python3.9/site-packages/mlflow/tracking/_tracking_service/client.py\", line 431, in log_artifact\r\n    artifact_repo = self._get_artifact_repo(run_id)\r\n  File \"/home/ubuntu/miniconda3/lib/python3.9/site-packages/mlflow/tracking/_tracking_service/client.py\", line 416, in _get_artifact_repo\r\n    artifact_repo = get_artifact_repository(artifact_uri)\r\n  File \"/home/ubuntu/miniconda3/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repository_registry.py\", line 106, in get_artifact_repository\r\n    return _artifact_repository_registry.get_artifact_repository(artifact_uri)\r\n  File \"/home/ubuntu/miniconda3/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repository_registry.py\", line 72, in get_artifact_repository\r\n    return repository(artifact_uri)\r\n  File \"/home/ubuntu/miniconda3/lib/python3.9/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\", line 46, in __init__\r\n    super().__init__(self.resolve_uri(artifact_uri, get_tracking_uri()))\r\n  File \"/home/ubuntu/miniconda3/lib/python3.9/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\", line 61, in resolve_uri\r\n    _validate_uri_scheme(track_parse.scheme)\r\n  File \"/home/ubuntu/miniconda3/lib/python3.9/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\", line 35, in _validate_uri_scheme\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: The configured tracking uri scheme: 'file' is invalid for use with the proxy mlflow-artifact scheme. The allowed tracking schemes are: {'http', 'https'}\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nREPLACE_ME\r\n```\r\n\n\n### What component(s) does this bug affect?\n\n- [X] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [X] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "dbczumar",
        "body": "Hi @yossibiton, thank you for raising this issue. The problem appears to be that your MLflow experiment with name `object_detection` was created using an HTTP request to `mlflow server` or was created by manually specifying an `mlflow-artifacts://` URI as the `artifact_location`. In order to log artifacts to this experiment, you'll need to run your `mlflow server` and set the MLflow Tracking URI to communicate with the MLflow server, e.g. something like `mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")`.\r\n\r\nThank you for using MLflow!"
      },
      {
        "user": "yossibiton",
        "body": "Thank you for your response !\r\nHowever, I'm still not able to solve the issue. I'm not using mlflow server, just storing the files locally on local path (/efs/mlflow) as you can see in my code example.\r\n\r\nIn order to create the experiments I did this : \r\n```\r\ncd /efs/mlflow\r\nmlflow ui\r\n```\r\n   \r\nAnd then in the UI I created a new experiment : \r\n![image](https://user-images.githubusercontent.com/6518016/218670827-3a1ea767-7951-4c08-9c12-91f4d873fc21.png)\r\n\r\nCan you explain please what did I do wrong ?\r\nDo I have to set \"Artifact Location\" ?"
      },
      {
        "user": "dbczumar",
        "body": "Hi @yossibiton , thank you for clarifying. I think this is a bug in `mlflow ui`. Because `mlflow ui` shares the same code with `mlflow server`, it uses a default artifact location of the form `mlflow-artifacts:/` for newly-created experiments. Instead, it should use the local filesystem.\r\n\r\nFor now, you can circumvent the problem by specifying `artifact_location` and passing in a local filesystem path. I've reopened this issue, and we'll get it addressed in the next release."
      }
    ]
  },
  {
    "issue_number": 5852,
    "title": "[BUG]mlflow gc command raise exception when serve-artifacts as local file",
    "author": "trillionmonster",
    "state": "closed",
    "created_at": "2022-05-12T03:04:14Z",
    "updated_at": "2025-05-14T08:31:46Z",
    "labels": [
      "bug",
      "good first issue",
      "area/artifacts",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute a fix for this bug independently.\n\n### System information\n\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**:  no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04\r\n- **MLflow installed from (source or binary)**: binary \r\n- **MLflow version (run ``mlflow --version``)**: 1.25.1 \r\n- **Python version**: 3.9 \r\n- **npm version, if running the dev UI**: None\r\n\n\n### Describe the problem\n\nmlflow gc command raise exception when serve-artifacts as local file \r\n\r\nmy solution is \r\nadd code to  mlflow/mlflow/store/artifact/mlflow_artifacts_repo.py line 61\r\n```\r\n        # if uri is file, return the artifacts under \"./mlartifacts\", same fold with mlruns\r\n        if track_parse.scheme == \"file\":\r\n\r\n            return os.path.join(os.path.dirname(track_parse.path),\"mlartifacts\",uri_parse.path[1:])\r\n```\r\n\n\n### Tracking information\n\n_No response_\n\n### Code to reproduce issue\n\n```\r\ncd ./mlflow-root\r\nmlflow server -h 0.0.0.0 -p 18888 --serve-artifacts\r\n```\r\nafter delete mlflow-runs and under mlflow-root then use \r\n```\r\nmlflow gc\r\n```\n\n### Other info / logs\n\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/anaconda3/envs/mlflow/bin/mlflow\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/opt/anaconda3/envs/mlflow/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/opt/anaconda3/envs/mlflow/lib/python3.9/site-packages/click/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/opt/anaconda3/envs/mlflow/lib/python3.9/site-packages/click/core.py\", line 1659, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/opt/anaconda3/envs/mlflow/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/opt/anaconda3/envs/mlflow/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/opt/anaconda3/envs/mlflow/lib/python3.9/site-packages/mlflow/cli.py\", line 489, in gc\r\n    artifact_repo = get_artifact_repository(run.info.artifact_uri)\r\n  File \"/opt/anaconda3/envs/mlflow/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repository_registry.py\", line 107, in get_artifact_repository\r\n    return _artifact_repository_registry.get_artifact_repository(artifact_uri)\r\n  File \"/opt/anaconda3/envs/mlflow/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repository_registry.py\", line 73, in get_artifact_repository\r\n    return repository(artifact_uri)\r\n  File \"/opt/anaconda3/envs/mlflow/lib/python3.9/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\", line 46, in __init__\r\n    super().__init__(self.resolve_uri(artifact_uri, get_tracking_uri()))\r\n  File \"/opt/anaconda3/envs/mlflow/lib/python3.9/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\", line 61, in resolve_uri\r\n    _validate_uri_scheme(track_parse.scheme)\r\n  File \"/opt/anaconda3/envs/mlflow/lib/python3.9/site-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\", line 35, in _validate_uri_scheme\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: The configured tracking uri scheme: 'file' is invalid for use with the proxy mlflow-artifact scheme. The allowed tracking schemes are: {'https', 'http'}\r\n```\n\n### What component(s) does this bug affect?\n\n- [X] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "dbczumar",
        "body": "@BenWilson2 @harupy Can you take a look at this?"
      },
      {
        "user": "BenWilson2",
        "body": "Hi @trillionmonster , could you verify these attempts at reproducing this failure and let me know if I'm missing the plot here?\r\n\r\n\r\n\r\nTrial #1: \r\n\r\n\r\nterminal1: \r\ncd ~\r\nmlflow server -h 0.0.0.0 -p 8889 --serve-artifacts\r\n\r\nscript:\r\n\r\n``` python\r\nimport mlflow\r\nfrom mlflow.tracking import MlflowClient\r\nfrom random import random, randint\r\nfrom sklearn.ensemble import RandomForestRegressor\r\n\r\nmlflow.set_tracking_uri(\"http://0.0.0.0:8889\")\r\n\r\nclient = MlflowClient()\r\nclient.list_experiments()\r\n>> [<Experiment: artifact_location='./mlruns/0', experiment_id='0', lifecycle_stage='active', name='Default', tags={}>]\r\n\r\nmlflow.set_experiment(\"gc_test\")\r\n\r\nrun_name = \"gc_test_1\"\r\n\r\nwith mlflow.start_run(run_name=run_name) as run:\r\n    print(f\"Artifact uri: {run.info.artifact_uri}\\n\")\r\n    params = {\"n_estimators\": 5, \"random_state\": 42}\r\n    model = RandomForestRegressor(**params)\r\n    \r\n    mlflow.log_params(params)\r\n    mlflow.log_param(\"param_1\", randint(0, 100))\r\n    mlflow.log_metrics({\"metric_1\": random(), \"metric_2\": random() + 1})\r\n\r\n    mlflow.sklearn.log_model(\r\n        sk_model=model,\r\n        artifact_path=f\"{run_name}/model\"\r\n    )\r\n    print(f\"\\nRun info: {run.info}\")\r\n\r\n# Execute above twice\r\n\t\r\nclient.list_experiments()\r\n>> [<Experiment: artifact_location='./mlruns/0', experiment_id='0', lifecycle_stage='active', name='Default', tags={}>,\r\n <Experiment: artifact_location='mlflow-artifacts:/1', experiment_id='1', lifecycle_stage='active', name='gc_test', tags={}>]\r\n\r\nexp2 = client.list_run_infos(experiment_id=1)\r\nexp2\r\n>> [<RunInfo: artifact_uri='mlflow-artifacts:/1/4152b9895f8045e0af602b2e7bb62684/artifacts', end_time=1652744412469, experiment_id='1', lifecycle_stage='active', run_id='4152b9895f8045e0af602b2e7bb62684', run_uuid='4152b9895f8045e0af602b2e7bb62684', start_time=1652744411081, status='FINISHED', user_id='benjamin.wilson'>,\r\n <RunInfo: artifact_uri='mlflow-artifacts:/1/b17935eb8098467bbe4accd21afdcddd/artifacts', end_time=1652744403790, experiment_id='1', lifecycle_stage='active', run_id='b17935eb8098467bbe4accd21afdcddd', run_uuid='b17935eb8098467bbe4accd21afdcddd', start_time=1652744402413, status='FINISHED', user_id='benjamin.wilson'>]\r\n\r\nmlflow.delete_run(exp2[0].run_id)\r\n\r\nexp3 = client.list_run_infos(experiment_id=1)\r\nexp3\r\n>> [<RunInfo: artifact_uri='mlflow-artifacts:/1/b17935eb8098467bbe4accd21afdcddd/artifacts', end_time=1652744403790, experiment_id='1', lifecycle_stage='active', run_id='b17935eb8098467bbe4accd21afdcddd', run_uuid='b17935eb8098467bbe4accd21afdcddd', start_time=1652744402413, status='FINISHED', user_id='benjamin.wilson'>]\r\n\r\nmlflow.delete_run(exp3[0].run_id)\r\n\r\nexp3 = client.list_run_infos(experiment_id=1)\r\nexp3\r\n>> []\r\n\r\n```\r\n\r\nterm2:\r\n```sh\r\ncd ~/mlruns\r\nmlflow gc\r\n\r\ncd ~/mlartifacts\r\nmlflow gc\r\n```\r\nNo exception is thrown.\r\n\r\n\r\nTrial #2\r\n\r\n1. Start server\r\n\t1. `mlflow server -h 0.0.0.0 -p 8889 --serve-artifacts`\r\n3. Log 3 runs to new experiment\r\n4. perform cli operations as below:\r\n\r\n```zsh\r\n~ via ðŸ…’ mlflow-dev-env via ðŸ dev-env\r\nâžœ ls\r\nDownloads                Movies                     repos\r\nApplications             JupyterNotebooks         Music                    miniconda3               universe\r\nDesktop                  Library                  Pictures                 miniforge3              \r\nDocuments                Public                   mlruns\r\n(mlflow-dev-env)\r\n~ via ðŸ…’ mlflow-dev-env via ðŸ dev-env\r\nâžœ ls\r\nDownloads                Movies                     mlruns\r\nApplications             JupyterNotebooks         Music                    miniconda3               repos\r\nDesktop                  Library                  Pictures                 miniforge3              \r\nDocuments                Public                   mlartifacts             \r\n(mlflow-dev-env)\r\n~ via ðŸ…’ mlflow-dev-env via ðŸ dev-env\r\nâžœ rm -rf mlruns\r\n(mlflow-dev-env)\r\n~ via ðŸ…’ mlflow-dev-env via ðŸ dev-env\r\nâžœ ls\r\nDownloads                Movies                    repos\r\nApplications             JupyterNotebooks         Music                    miniconda3              \r\nDesktop                  Library                  Pictures                 miniforge3              \r\nDocuments                Public                   mlartifacts\r\n(mlflow-dev-env)\r\n~ via ðŸ…’ mlflow-dev-env via ðŸ dev-env\r\nâžœ mlflow gc\r\n(mlflow-dev-env)\r\n~ via ðŸ…’ mlflow-dev-env via ðŸ dev-env took 2s\r\nâžœ ls\r\nDownloads                Movies                     mlruns\r\nApplications             JupyterNotebooks         Music                    miniconda3               repos\r\nDesktop                  Library                  Pictures                 miniforge3              \r\nDocuments                Public                   mlartifacts              \r\n(mlflow-dev-env)\r\n```\r\n\r\nTrial #3\r\n\r\n1. Start server\r\n\t1. `mlflow server -h 0.0.0.0 -p 8889 --serve-artifacts`\r\n3. Perform cli operations as below:\r\n\r\n```zsh\r\n~ via ðŸ…’ mlflow-dev-env via ðŸ dev-env\r\nâžœ ls -l\r\ntotal 56480\r\ndrwx------@   4 benjamin.wilson  staff       128 Apr 25 11:35 Applications\r\ndrwx------@   7 benjamin.wilson  staff       224 May  9 19:22 Desktop\r\ndrwx------@   3 benjamin.wilson  staff        96 Apr 25 09:47 Documents\r\ndrwx------@   6 benjamin.wilson  staff       192 May 13 14:23 Downloads\r\ndrwxr-xr-x    9 benjamin.wilson  staff       288 May 16 19:46 JupyterNotebooks\r\ndrwx------@  77 benjamin.wilson  staff      2464 May 12 14:56 Library\r\ndrwx------    3 benjamin.wilson  staff        96 Apr 25 09:47 Movies\r\ndrwx------+   4 benjamin.wilson  staff       128 Apr 25 10:37 Music\r\ndrwx------+   4 benjamin.wilson  staff       128 Apr 25 10:37 Pictures\r\ndrwxr-xr-x+   4 benjamin.wilson  staff       128 Apr 25 09:47 Public\r\ndrwxr-xr-x   15 benjamin.wilson  staff       480 May 12 12:37 miniconda3\r\ndrwxr-xr-x   16 benjamin.wilson  staff       512 May  5 13:04 miniforge3\r\ndrwxr-xr-x    9 benjamin.wilson  staff       288 May  3 20:44 repos\r\n(mlflow-dev-env)\r\n~ via ðŸ…’ mlflow-dev-env via ðŸ dev-env\r\nâžœ ls -l\r\ntotal 56480\r\ndrwx------@   4 benjamin.wilson  staff       128 Apr 25 11:35 Applications\r\ndrwx------@   7 benjamin.wilson  staff       224 May  9 19:22 Desktop\r\ndrwx------@   3 benjamin.wilson  staff        96 Apr 25 09:47 Documents\r\ndrwx------@   6 benjamin.wilson  staff       192 May 13 14:23 Downloads\r\ndrwxr-xr-x    9 benjamin.wilson  staff       288 May 16 19:46 JupyterNotebooks\r\ndrwx------@  77 benjamin.wilson  staff      2464 May 12 14:56 Library\r\ndrwx------    3 benjamin.wilson  staff        96 Apr 25 09:47 Movies\r\ndrwx------+   4 benjamin.wilson  staff       128 Apr 25 10:37 Music\r\ndrwx------+   4 benjamin.wilson  staff       128 Apr 25 10:37 Pictures\r\ndrwxr-xr-x+   4 benjamin.wilson  staff       128 Apr 25 09:47 Public\r\ndrwxr-xr-x   15 benjamin.wilson  staff       480 May 12 12:37 miniconda3\r\ndrwxr-xr-x   16 benjamin.wilson  staff       512 May  5 13:04 miniforge3\r\n**drwxr-xr-x    4 benjamin.wilson  staff       128 May 16 19:57 mlruns**\r\ndrwxr-xr-x    9 benjamin.wilson  staff       288 May  3 20:44 repos\r\n(mlflow-dev-env)\r\n~ via ðŸ…’ mlflow-dev-env via ðŸ dev-env\r\nâžœ rm -rf  mlruns\r\n(mlflow-dev-env)\r\n~ via ðŸ…’ mlflow-dev-env via ðŸ dev-env\r\nâžœ ls -l\r\ntotal 56480\r\ndrwx------@   4 benjamin.wilson  staff       128 Apr 25 11:35 Applications\r\ndrwx------@   7 benjamin.wilson  staff       224 May  9 19:22 Desktop\r\ndrwx------@   3 benjamin.wilson  staff        96 Apr 25 09:47 Documents\r\ndrwx------@   6 benjamin.wilson  staff       192 May 13 14:23 Downloads\r\ndrwxr-xr-x    9 benjamin.wilson  staff       288 May 16 19:46 JupyterNotebooks\r\ndrwx------@  77 benjamin.wilson  staff      2464 May 12 14:56 Library\r\ndrwx------    3 benjamin.wilson  staff        96 Apr 25 09:47 Movies\r\ndrwx------+   4 benjamin.wilson  staff       128 Apr 25 10:37 Music\r\ndrwx------+   4 benjamin.wilson  staff       128 Apr 25 10:37 Pictures\r\ndrwxr-xr-x+   4 benjamin.wilson  staff       128 Apr 25 09:47 Public\r\ndrwxr-xr-x   15 benjamin.wilson  staff       480 May 12 12:37 miniconda3\r\ndrwxr-xr-x   16 benjamin.wilson  staff       512 May  5 13:04 miniforge3\r\ndrwxr-xr-x    9 benjamin.wilson  staff       288 May  3 20:44 repos\r\n(mlflow-dev-env)\r\n~ via ðŸ…’ mlflow-dev-env via ðŸ dev-env\r\nâžœ mlflow gc\r\n(mlflow-dev-env)\r\n~ via ðŸ…’ mlflow-dev-env via ðŸ dev-env\r\nâžœ ls -l\r\ntotal 56480\r\ndrwx------@   4 benjamin.wilson  staff       128 Apr 25 11:35 Applications\r\ndrwx------@   7 benjamin.wilson  staff       224 May  9 19:22 Desktop\r\ndrwx------@   3 benjamin.wilson  staff        96 Apr 25 09:47 Documents\r\ndrwx------@   6 benjamin.wilson  staff       192 May 13 14:23 Downloads\r\ndrwxr-xr-x    9 benjamin.wilson  staff       288 May 16 19:46 JupyterNotebooks\r\ndrwx------@  77 benjamin.wilson  staff      2464 May 12 14:56 Library\r\ndrwx------    3 benjamin.wilson  staff        96 Apr 25 09:47 Movies\r\ndrwx------+   4 benjamin.wilson  staff       128 Apr 25 10:37 Music\r\ndrwx------+   4 benjamin.wilson  staff       128 Apr 25 10:37 Pictures\r\ndrwxr-xr-x+   4 benjamin.wilson  staff       128 Apr 25 09:47 Public\r\ndrwxr-xr-x   15 benjamin.wilson  staff       480 May 12 12:37 miniconda3\r\ndrwxr-xr-x   16 benjamin.wilson  staff       512 May  5 13:04 miniforge3\r\n**drwxr-xr-x    4 benjamin.wilson  staff       128 May 16 19:58 mlruns**\r\ndrwxr-xr-x    9 benjamin.wilson  staff       288 May  3 20:44 repos\r\n(mlflow-dev-env)\r\n```"
      },
      {
        "user": "DraXus",
        "body": "I'm having the same issue as @trillionmonster. This is my code to reproduce:\r\n\r\nLaunch server:\r\n```\r\nmlflow server --backend-store-uri sqlite:///C:/mlflow/mlflow.db --artifacts-destination file:///C:/mlflow --serve-artifacts --host 0.0.0.0 -p 5000\r\n```\r\n\r\nCall gc:\r\n```\r\nmlflow gc --backend-store-uri sqlite:///C:/mlflow/mlflow.db\r\n```\r\n\r\nI'm running on Windows 10, Python 3.8.10 and mlflow 1.26.1"
      }
    ]
  },
  {
    "issue_number": 15310,
    "title": "Advocating Continuance of H2O in MLFlow",
    "author": "Bernard-H2O",
    "state": "closed",
    "created_at": "2025-04-14T16:00:51Z",
    "updated_at": "2025-05-14T06:02:31Z",
    "labels": [
      "has-closing-pr"
    ],
    "body": "To the MLFlow Development Team,\n\nThis is in reference to https://github.com/mlflow/mlflow/pull/15258.\n\nMy name is Bernard Ong, Principal Data Scientist at H2O.ai, writing to advocate for the continued integration of H2O components within MLFlow. Recent discussions about potential deprecation and removal have raised significant concerns among our shared enterprise user base, and I urge you to consider the following points:\n\nStrategic Synergy in Enterprise Ecosystems\nMLFlowâ€™s H2O integration is mission-critical for organizations managing complex machine learning workflows. Enterprises rely on this interoperability to streamline MLOps pipelines, combining H2Oâ€™s automated machine learning (AutoML) and distributed capabilities with MLFlowâ€™s lifecycle management. Removing this integration would fracture workflows that depend on H2O for model development and MLFlow for deployment tracking.\n\nCommunity and Ecosystem Impact\nThe H2O-MLFlow integration has become a cornerstone for collaborative ML projects, enabling seamless model versioning, experiment comparison, and reproducibility. Deprecating H2O support would disrupt thousands of production systems and force organizations to adopt less standardized solutions, undermining MLFlowâ€™s position as a unified ML platform.\n\nTechnical and Business Value\n\nUnified Tracking: MLFlowâ€™s ability to log H2O AutoML leaderboards, hyperparameters, and metrics provides unparalleled transparency in model selection.\n\nDeployment Efficiency: The mlflow.h2o.log_model() functionality ensures smooth transitions from experimentation to production, reducing operational overhead.\n\nCross-Platform Collaboration: Native H[2](https://www.restack.io/docs/mlflow-knowledge-mlflow-h2o-automl-integration)O model loading via mlflow.h2o.load_model() preserves interoperability for teams using hybrid toolchains.\n\nPartnership Opportunities\nH2O.ai is committed to collaborating on maintenance, documentation, and user support to ensure this integration remains robust. We propose establishing a joint working group to enhance features like H2O AutoML tracking and explainability integration, strengthening both platforms.\n\nAppeal for Continuity\nThe H2O-MLFlow integration exemplifies the open-source ethos of interoperability and user empowerment. Retaining this functionality honors the trust of our shared community and preserves MLFlowâ€™s reputation as an inclusive, extensible framework.\n\nWe hope for your sustaining and continuance of this vital integration. Thank you.\n\nSincerely yours,\nBernard Ong\nPrincipal Data Scientist, Customer Data Science\nH2O.ai",
    "comments": [
      {
        "user": "BenWilson2",
        "body": "Hi Bernard,\n\nI sincerely apologize for our misguided attempt to remove the H2O integration from MLflow. What we incorrectly assessed as a low-usage component is clearly a critical integration for numerous enterprise users and the broader ML community.\n\nWe've immediately reverted the removal PR (#15258) and want to assure you that MLflow will continue to fully support the H2O integration with no plans of deprecation. Your detailed explanation of the integration's importance to enterprise workflows, community collaboration, and technical value was eye-opening and greatly appreciated.\n\nRather than removing components without proper consultation, we should be exploring ways to strengthen these partnerships. I appreciate you bringing this context to our attention and advocating for your user base.\n\nMoving forward, we'll implement a more thorough evaluation process before considering any integration changes, including direct outreach to integration partners like H2O.ai. We value our relationship with the H2O community and are committed to maintaining the interoperability that benefits our shared users.\n\nThank you for your prompt feedback and for holding us accountable to the open-source values of collaboration and inclusivity that MLflow stands for.\n\nRespectfully,\nBen Wilson"
      },
      {
        "user": "harupy",
        "body": "I'll close this issue. The h2o flavor has been restored. #15722 will remove the deprecation warning."
      },
      {
        "user": "Bernard-H2O",
        "body": "Thank you again for the great work from the MLFlow Team for ensuring the integration continuity of H2O components.\nOur user community and enterprises will continue benefit from this strategic collaboration.\n"
      }
    ]
  },
  {
    "issue_number": 15718,
    "title": "[BUG] Invalid path when logging image to GCS on step 23",
    "author": "dails08",
    "state": "open",
    "created_at": "2025-05-13T23:38:20Z",
    "updated_at": "2025-05-14T00:53:46Z",
    "labels": [
      "bug",
      "area/uiux",
      "area/artifacts"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nOther\n\n### MLflow version\n\n- Client: 2.22.0\n- Tracking server: 2.22.0\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 25.04\n- **Python version**: 3.13\n- **yarn version, if running the dev UI**: NA\n\n\n\n### Describe the problem\n\nWhen running mlflow.log_image(image_data, key=key, step=step), on step 23, mlflow consistently throws an INVALID PATH error. Uncommenting the code that skips specifically step 23 results in no error.\n\nArtifacts are tracked in GCS, so it may be a permissions error, but no other steps error and the images (png and webp) log and store fine. Backend is GCP CloudSQL postgres, so it could be a failure there, but again, no other steps error out. All other run information is persisted.\n\nThis one error also seems to invalidate some logic checking for the mlflow UI, since the images (all of which except for step 23 are persisted and viewable individually) no longer show up as a slider-enabled series of plots in the model metrics tab, nor do they populate in the image grid option Images dropdown.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nSystem information: Linux #19~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Feb 17 11:51:52 UTC 2\nPython version: 3.12.3\nMLflow version: 2.22.0\nMLflow module location: /home/chris/Desktop/repos/redacted/research/mlflow/env-mlflow/lib/python3.12/site-packages/mlflow/__init__.py\nTracking URI: http://xx.xx.xxx.xxx:8885/\nRegistry URI: http://xx.xx.xxx.xxx:8885/\nMLflow environment variables: \n  MLFLOW_TRACKING_URI: http://xx.xx.xxx.xxx:8885/\nMLflow dependencies: \n  Flask: 3.1.0\n  Jinja2: 3.1.6\n  alembic: 1.15.2\n  docker: 7.1.0\n  fastapi: 0.115.12\n  google-cloud-storage: 3.1.0\n  graphene: 3.4.3\n  gunicorn: 23.0.0\n  markdown: 3.8\n  matplotlib: 3.10.1\n  mlflow-skinny: 2.22.0\n  numpy: 2.2.5\n  pandas: 2.2.3\n  pyarrow: 19.0.1\n  scikit-learn: 1.6.1\n  scipy: 1.15.2\n  sqlalchemy: 2.0.40\n  uvicorn: 0.34.2\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\nfrom google.cloud import storage\nimport numpy as np\nfrom PIL import Image\n\nfrom loguru import logger\n\nimport mlflow\n\nimport os\nfrom io import BytesIO\n\nmlflow.set_tracking_uri(\"http://xx.xx.xxx.xxx:8885\")\n\nmlflow.search_experiments() # to verify connection and credentials\n\nrng = np.random.default_rng()\n\ndef plot_estimates_at_step(model, step):\n    fig, ax = plt.subplots(figsize = (5,5))\n    preds_X = np.linspace(0,100,10).reshape((-1, 1))\n    preds_dataset = model(preds_X).detach().numpy()\n    ax.scatter(raw_data.X, raw_data.Y)\n    ax.scatter(preds_X, preds_dataset)\n    ax.set_title(f\"Step {step}\")\n    plt.tight_layout()\n    buf = BytesIO()\n    plt.savefig(buf)\n    plt.close()\n\n    buf.seek(0)\n    img = Image.open(buf)\n    return img\n\nclass SimpleLinear(nn.Module):\n    def __init__(self, input_shape = 5, output_shape = 1):\n        super(SimpleLinear, self).__init__()\n\n\n        self.model = nn.Sequential(\n            nn.Linear(input_shape, 6, dtype = torch.float32),\n            nn.ReLU(),\n            nn.Linear(6, output_shape, dtype = torch.float32)\n        )\n        # self.model = nn.Linear(input_shape, output_shape)\n\n    def forward(self, x):\n        x = torch.Tensor(x)\n        output = self.model(x)\n        # print(output)\n        return output\n\n\nclass SimpleDataset(Dataset):\n    def __init__(self):\n        self.X = torch.from_numpy(np.linspace(0,100,200, dtype = np.float32))\n        eps = rng.normal(size = 200)\n        self.Y = 1.12 * self.X**2 + 7.75 + eps\n        self.Y = torch.from_numpy(np.asarray(self.Y, dtype = np.float32))\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.Y[idx]\n\nraw_data = SimpleDataset()\n\nlinear_model = SimpleLinear(input_shape=1, output_shape=1)\noptimizer = torch.optim.Adam(linear_model.parameters(), lr = 0.01)\nloss_fn = nn.MSELoss()\n\nloader = torch.utils.data.DataLoader( dataset = raw_data, batch_size = 9, shuffle = True)\n\nnum_epochs = 4\nwith mlflow.start_run(\n    run_name=\"Step 23 bug MWE\",\n    description=\"Demonstrate consistent error when logging an image at step 23\",\n) as outer_run:\n    logger.info(\"Starting run\")\n    mlflow.log_param(\"model\", \"simple_linear\")\n    mlflow.log_param(\"dataset\", \"SimpleDataset\")\n    mlflow.log_param(\"epochs\", \"5\")\n    mlflow.log_param(\"optimizer\", \"adam\")\n    mlflow.log_param(\"lr\", 0.01)\n    mlflow.log_param(\"loss\", \"mse\")\n    step = 0\n    for epoch in range(num_epochs):\n        logger.info(f\"Starting epoch {epoch}\")\n        for ix, (x_batch, y_batch) in enumerate(loader):\n            logger.debug(f\"Step {step}\")\n            x_batch = x_batch.reshape((-1,1))\n            y_batch = y_batch.reshape((-1, 1))\n            preds = linear_model(x_batch)\n            loss = loss_fn(preds, y_batch)\n    \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            mlflow.log_metric(\"loss\", np.mean(loss.detach().numpy()), step = step)\n            # if step != 23:\n            #     mlflow.log_image( plot_estimates_at_step(linear_model, step), key=\"estimate\", step = step)\n            mlflow.log_image( plot_estimates_at_step(linear_model, step), key=\"estimate\", step = step)\n            step += 1\n\n\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\n2025/05/13 19:03:04 ERROR mlflow.utils.async_logging.async_artifacts_logging_queue: Failed to log artifact estimates%step%23%timestamp%1747177383576%kf4f30c9-adf8-4c92-8e06-687589887029%compressed.webp. Exception: 400 Client Error: BAD REQUEST for url: http://xx.xx.xxx.xxx:8885/api/2.0/mlflow-artifacts/artifacts/445020544189088996/3c66290bf7fd47bc97922f349a0b193f/artifacts/images/estimates%25step%2523%25timestamp%251747177383576%25kf4f30c9-adf8-4c92-8e06-687589887029%25compressed.webp. Response text: {\"error_code\": \"INVALID_PARAMETER_VALUE\", \"message\": \"Invalid path\"}\n2025/05/13 19:03:05 ERROR mlflow.utils.async_logging.async_artifacts_logging_queue: Failed to log artifact estimates%step%23%timestamp%1747177383576%kf4f30c9-adf8-4c92-8e06-687589887029.png. Exception: API request to http://xx.xx.xxx.xxx:8885/api/2.0/mlflow-artifacts/artifacts/445020544189088996/3c66290bf7fd47bc97922f349a0b193f/artifacts/images/estimates%step%23%timestamp%1747177383576%kf4f30c9-adf8-4c92-8e06-687589887029.png failed with exception (\"Connection broken: ConnectionResetError(104, 'Connection reset by peer')\", ConnectionResetError(104, 'Connection reset by peer'))\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\n(env-mlflow) redacted@mlflow-server:~$ mlflow server --host 0.0.0.0 --port 8885 --artifacts-destination gs://redacted/mlflow --app-name basic-auth\n[2025-05-13 17:29:12 +0000] [1753] [INFO] Starting gunicorn 23.0.0\n[2025-05-13 17:29:12 +0000] [1753] [INFO] Listening at: http://0.0.0.0:8885 (1753)\n[2025-05-13 17:29:12 +0000] [1753] [INFO] Using worker: sync\n[2025-05-13 17:29:12 +0000] [1754] [INFO] Booting worker with pid: 1754\n[2025-05-13 17:29:12 +0000] [1755] [INFO] Booting worker with pid: 1755\n[2025-05-13 17:29:12 +0000] [1756] [INFO] Booting worker with pid: 1756\n[2025-05-13 17:29:12 +0000] [1757] [INFO] Booting worker with pid: 1757\n2025/05/13 17:29:17 WARNING mlflow.server.auth: This feature is still experimental and may change in a future release without warning\n2025/05/13 17:29:17 WARNING mlflow.server.auth: This feature is still experimental and may change in a future release without warning\n2025/05/13 17:29:17 WARNING mlflow.server.auth: This feature is still experimental and may change in a future release without warning\n2025/05/13 17:29:17 WARNING mlflow.server.auth: This feature is still experimental and may change in a future release without warning\n```\n\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [x] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "dails08",
        "body": "This feels related to https://github.com/mlflow/mlflow/issues/12151, as images, even when skipping step 23, fail to register both in the slider-enabled model metric and in the dropdown for creating an image grid."
      },
      {
        "user": "TomeHirata",
        "body": "Thank you for the report. %23 means \"#\" in percent encoding, so I guess the rest of the string is recognized as a URL fragment. In a few months, we will try to replace the separator with something else to fix the log_image behavior completely."
      }
    ]
  },
  {
    "issue_number": 15456,
    "title": "[BUG] Security Vulnerability",
    "author": "geckosecurity",
    "state": "open",
    "created_at": "2025-04-23T18:53:24Z",
    "updated_at": "2025-05-13T21:10:53Z",
    "labels": [],
    "body": "Hello,\n\nWeâ€™ve discovered a security vulnerability in MLflow and would like to responsibly disclose it following your coordinated disclosure process. Further details have been sent to `mlflow-oss-maintainers@googlegroups.com`.\n\nJJ \nGecko Security",
    "comments": [
      {
        "user": "B-Step62",
        "body": "Responded in the group."
      },
      {
        "user": "geckosecurity",
        "body": "Hi @B-Step62, are there any updates on the vulnerability?"
      }
    ]
  },
  {
    "issue_number": 15457,
    "title": "[BUG] Security Vulnerability",
    "author": "geckosecurity",
    "state": "open",
    "created_at": "2025-04-23T18:53:41Z",
    "updated_at": "2025-05-13T21:10:43Z",
    "labels": [],
    "body": "Hello,\n\nWeâ€™ve discovered a security vulnerability in MLflow and would like to responsibly disclose it following your coordinated disclosure process. Further details have been sent to `mlflow-oss-maintainers@googlegroups.com`.\n\nJJ \nGecko Security",
    "comments": [
      {
        "user": "B-Step62",
        "body": "Responded in the group."
      },
      {
        "user": "geckosecurity",
        "body": "Hi @B-Step62, are there any updates on the vulnerability?"
      }
    ]
  },
  {
    "issue_number": 15693,
    "title": "[BUG] Security Vulnerability",
    "author": "TaiPhung217",
    "state": "open",
    "created_at": "2025-05-12T17:37:37Z",
    "updated_at": "2025-05-13T09:42:32Z",
    "labels": [
      "bug"
    ],
    "body": "Hello,\n\nIâ€™ve discovered a security vulnerability in MLflow and would like to responsibly disclose it following your coordinated disclosure process. Further details have been sent to mlflow-oss-maintainers@googlegroups.com vá»›i title: [BUG] Security Vulnerability in MLflow â€“ Pending ...\n\nBest regards,\nPhÃ¹ng TÃ i",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "Thank you for the report, we'll check the email."
      }
    ]
  },
  {
    "issue_number": 15711,
    "title": "[FR] Add system information in system metrics when System Metrics Logging is enabled",
    "author": "ShlomiRex",
    "state": "open",
    "created_at": "2025-05-13T05:56:26Z",
    "updated_at": "2025-05-13T08:57:20Z",
    "labels": [
      "enhancement",
      "area/uiux"
    ],
    "body": "### Willingness to contribute\n\nYes. I would be willing to contribute this feature with guidance from the MLflow community.\n\n### Proposal Summary\n\nIt would be great to see the system information, such as CPU (type, architecture, cores...), GPU (cores, VRAM, ...), RAM (amount, brand, type) and so on in System Metrics tab in MLFlow UI.\nThis information is only collected when the user specifies `mlflow.enable_system_metrics_logging()`.\nThis information would appear on the top of the system metrics tab:\n\n![Image](https://github.com/user-attachments/assets/ef2b3eac-ba01-4f4c-8692-ee4c8437a050)\n\nExample (from `py-cpuinfo` and `gpuinfo` packages):\n\n```\n{\n    \"cpu_info\": {\n        \"cpu_model\": 113,\n        \"cpu_architecture\": \"AMD64\",\n        \"cpu_cores\": 6,\n        \"cpu_threads\": 12,\n        \"cpu_frequency\": 3600.0,\n\n    },\n    \"memory_info\": {\n        \"total_memory_gb\": 31.933452606201172,\n    },\n    \"ram_info\": {\n        ...\n    },\n    \"gpu_info\": {\n        ...\n    },\n}\n```\n\n### Motivation\n\n> #### What is the use case for this feature?\n\nTo further inspect the system of the training machine.\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\nIt is verbose information of the system.\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\nEach run can have different training machine (e.g. cloud) for different runs.\n\n> #### Why is it currently difficult to achieve this use case?\n\nIt is not really difficult since we already collect system metrics, while this information is more verbose.\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [x] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "TomeHirata",
        "body": "Hi, @ShlomiRex. Are you proposing to enhance the system metrics by adding the more system information? Can you list all the metrics you want to introduce in addition to what is available now (refer to [our doc](https://mlflow.org/docs/latest/system-metrics/index.html))? Also, please note that mlflow metrics have a flat structure and values are floats rather than strings."
      }
    ]
  },
  {
    "issue_number": 15086,
    "title": "[FR] Allow suppressing \"View experiment at [URL]\" messages",
    "author": "svpino",
    "state": "open",
    "created_at": "2025-03-24T13:22:11Z",
    "updated_at": "2025-05-13T01:24:04Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nRight now, there's no way to suppress the \"View experiment at [URL]\" messages produced by Mlflow. \n\nPlease, allow a way to suppress these messages.\n\n### Motivation\n\n> #### What is the use case for this feature?\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\n> #### Why is it currently difficult to achieve this use case?\n\n\n### Details\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "daniellok-db",
        "body": "cc @harupy should we add an env var or something to disable?"
      },
      {
        "user": "svpino",
        "body": "> cc @harupy should we add an env var or something to disable?\n\nI see two possible options:\n\n1. Replace any `print` statements with proper logs (Using the `logging` library). This would be consistent with the rest of MLflow, and it will allow the caller to control whether they want the logs, their format, etc.\n\n2. Add an environment variable to deactivate these logs. \n\nI think the first option is much more elegant and consistent, while the second option is probably faster to implement but much more clunkier."
      },
      {
        "user": "lhrotk",
        "body": "Hey @harupy, what's your opinion about this idea? I really hope an env var can be added to reduce stdout verbosity and I can add it if you don't mind."
      }
    ]
  },
  {
    "issue_number": 10255,
    "title": "Readme.rst: Docker instrustions still shows v2.2.1",
    "author": "david-thrower",
    "state": "closed",
    "created_at": "2023-10-31T19:06:45Z",
    "updated_at": "2025-05-13T00:57:36Z",
    "labels": [
      "good first issue",
      "area/docs"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute a documentation fix independently.\n\n### URL(s) with the issue\n\nhttps://github.com/mlflow/mlflow/blob/master/README.rst\n\n### Description of proposal (what needs changing)\n\nreplace: \r\n\r\n`docker pull ghcr.io/mlflow/mlflow:v2.2.1` \r\n\r\nwith  \r\n\r\n`\r\ndocker pull ghcr.io/mlflow/mlflow:v2.7.1\r\n`",
    "comments": [
      {
        "user": "pabitra0011",
        "body": "\r\nI am new to open source, and I'd like to start my journey by tackling a small issue. Can you please assign me to this issue?"
      },
      {
        "user": "david-thrower",
        "body": "@pabitra0011, I'm not authorized to assign it, but this does look like a perfect contribution to learn the system. I would certainly recommend they do."
      },
      {
        "user": "pabitra0011",
        "body": "@david-thrower  Thank you."
      }
    ]
  },
  {
    "issue_number": 13910,
    "title": "[FR] MLflow Tracing for Haystack",
    "author": "B-Step62",
    "state": "open",
    "created_at": "2024-11-29T02:42:27Z",
    "updated_at": "2025-05-12T16:32:21Z",
    "labels": [
      "enhancement",
      "help wanted"
    ],
    "body": "### Summary\r\n\r\nExpand [auto-tracing integration](https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing) of MLflow Tracing to [Haystack](https://haystack.deepset.ai/). \r\n\r\n### Required Changes\r\nPlease refer to [how to add new integration to MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/contribute.html) for the actual steps of adding new auto-tracing integration. MLflow maintainers will provide attentive support for designing and implementing the change.\r\n\r\n\r\n### Expected Behavior\r\n\r\n1. Users can enable auto-tracing by calling `mlflow.haystack.autolog()`\r\n```python\r\nimport mlflow\r\n\r\nfrom haystack import Pipeline\r\nfrom haystack.components.generators import OpenAIGenerator\r\nfrom haystack.components.builders.prompt_builder import PromptBuilder\r\n\r\nmlflow.haystack.autolog()\r\n\r\n# Initialize the pipeline\r\npipeline = Pipeline()\r\n\r\nllm = OpenAIGenerator(model=\"gpt-4o-mini\")\r\nprompt_template = \"\"\"Answer the question. {{question}}\"\"\"\r\nprompt_builder = PromptBuilder(template=prompt_template)\r\n\r\npipeline = Pipeline()\r\npipeline.add_component(\"prompt_builder\", prompt_builder)\r\npipeline.add_component(\"llm\", llm)\r\npipeline.connect(\"prompt_builder\", \"llm\")\r\n\r\nquestion = \"Who lives in Paris?\"\r\nresults = pipeline.run({\"prompt_builder\": {\"question\": question}})\r\n```\r\n2. This will generates a trace like this (This example is taken from DSpy tracing. The structure will be different for Haystack tracing):\r\n![image](https://github.com/user-attachments/assets/c6c636fe-e90e-4f0e-ada3-151717c334a7)\r\n\r\n### Contact\r\n\r\nIf you have any question or willing to work on this feature, leave a comment with tagging @B-Step62 ",
    "comments": [
      {
        "user": "preyasshah9",
        "body": "Hi @B-Step62 I'm new to `mlflow`. I'm willing to work on this feature, let me know if that sounds good to you!"
      },
      {
        "user": "B-Step62",
        "body": "@preyasshah9 Absolutely! Thank you for the willingness to contribute. Please let us know when you cave out the high level approach to implement this integration!"
      },
      {
        "user": "aimlnerd",
        "body": "Looking forward to Haystack support for MLflow tracing. Many ml platforms already have mlflow inbuilt, so this makes it enabling tracing for llm tracing \"free\" and \"automatic\". Instead of other paid tracing solutions."
      }
    ]
  },
  {
    "issue_number": 15535,
    "title": "[FR] Handle nan values better on experiment run",
    "author": "BrianaB2206",
    "state": "open",
    "created_at": "2025-04-28T10:11:53Z",
    "updated_at": "2025-05-10T17:21:38Z",
    "labels": [
      "enhancement",
      "area/uiux",
      "area/tracking",
      "area/server-infra"
    ],
    "body": "### Willingness to contribute\n\nYes. I can contribute this feature independently.\n\n### Proposal Summary\n\nI\n\n![Image](https://github.com/user-attachments/assets/aac9c4db-e053-474b-a9b5-2ebaf8b1c402)\n\n### Motivation\n\n> #### What is the use case for this feature?\n\n> #### Why is this use case valuable to support for MLflow users in general?\n\n> #### Why is this use case valuable to support for your project(s) or organization?\n\n> #### Why is it currently difficult to achieve this use case?\n\n\n### Details\n\nIf even one metric has a nan value, then the whole experiment run page is unavailable.\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [x] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [x] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "@BrianaB2206 Could you share a reproducible code and your environment information?"
      },
      {
        "user": "joelrobin18",
        "body": "Hi @B-Step62 This issue happens if we use Nan, inf or -inf:\n\nCode1:\n```python\nwith mlflow.start_run(experiment_id=exp_id):\n    mlflow.log_metric(\"loss\", float(\"inf\")) \n```\n\nCode2:\n```python\nwith mlflow.start_run(experiment_id=exp_id):\n    mlflow.log_metric(\"loss\", float(\"nan\")) \n```\n\nCode3:\n```python\nwith mlflow.start_run(experiment_id=exp_id):\n    mlflow.log_metric(\"loss\", float(\"-inf\")) \n```"
      }
    ]
  },
  {
    "issue_number": 15455,
    "title": "[BUG] Remote tracking server - Unable to log models - Error 500",
    "author": "sniafas",
    "state": "closed",
    "created_at": "2025-04-23T18:35:44Z",
    "updated_at": "2025-05-10T08:53:49Z",
    "labels": [
      "bug",
      "area/model-registry",
      "area/tracking"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.21.3\n- Tracking server: 2.21.3\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 22.04\n- **Python version**: 3.10\n- **yarn version, if running the dev UI**:\n\n\n### Describe the problem\n\n- Unable to log models when setting a remote tracking server (aurora+s3). This happens with other flavors as well e.g. Tensorflow/Keras. Report it with sklearn for easier reproduction.\n- Receiving 500 error code from [api/2.0/mlflow/runs/log-model](http://0.0.0.0:5000/api/2.0/mlflow/runs/log-model)\n- No problem with localhost\n\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```python\nimport mlflow\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nmlflow.set_tracking_uri(uri=\"http://0.0.0.0:5000\")\n# mlflow.set_tracking_uri(uri=\"<remote_uri>\")\n\n# Generate some dummy data\nnp.random.seed(0)\nX = np.random.rand(100, 1) * 10  # 100 samples, 1 feature\ny = 2 * X + 1 + np.random.randn(100, 1)  # Linear relationship with noise\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nmlflow.autolog(log_models=False, log_datasets=True, log_traces=False, silent=True)\nwith mlflow.start_run(run_name=\"sklearn_test\"):\n    \n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    mlflow.sklearn.log_model(model, \"models\")\n    # Make predictions on the test set\n    y_pred = model.predict(X_test)\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\n2025/04/23 21:10:54 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under s3://path1/17fffda95b9c49f5bb8a7fcb3e0a997c/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n2025/04/23 21:10:54 DEBUG mlflow.models.model: \nurllib3.exceptions.ResponseError: too many 500 error responses\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 942, in urlopen\n    return self.urlopen(\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 942, in urlopen\n    return self.urlopen(\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 942, in urlopen\n    return self.urlopen(\n  [Previous line repeated 4 more times]\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 932, in urlopen\n    retries = retries.increment(method, url, response=response, _pool=self)\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/urllib3/util/retry.py\", line 519, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='mlflow.<uri>.com', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-model (Caused by ResponseError('too many 500 error responses'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/mlflow/utils/rest_utils.py\", line 186, in http_request\n    return _get_http_response_with_retries(\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/mlflow/utils/request_utils.py\", line 239, in _get_http_response_with_retries\n    return session.request(method, url, allow_redirects=allow_redirects, **kwargs)\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/requests/adapters.py\", line 691, in send\n    raise RetryError(e, request=request)\nrequests.exceptions.RetryError: HTTPConnectionPool(host='mlflow.<uri>.com', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-model (Caused by ResponseError('too many 500 error responses'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/mlflow/models/model.py\", line 969, in log\n    mlflow.tracking.fluent._record_logged_model(mlflow_model, run_id)\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/mlflow/tracking/fluent.py\", line 1608, in _record_logged_model\n    MlflowClient()._record_logged_model(run_id, mlflow_model)\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/mlflow/tracking/client.py\", line 3196, in _record_logged_model\n    self._tracking_client._record_logged_model(run_id, mlflow_model)\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py\", line 884, in _record_logged_model\n    self.store.record_logged_model(run_id, mlflow_model)\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py\", line 664, in record_logged_model\n    self._call_endpoint(LogModel, req_body)\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py\", line 90, in _call_endpoint\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/mlflow/utils/rest_utils.py\", line 396, in call_endpoint\n    response = http_request(**call_kwargs)\n  File \"/home/sniafas/.cache/pypoetry/virtualenvs/size-recommendation-6RgXBFFX-py3.10/lib/python3.10/site-packages/mlflow/utils/rest_utils.py\", line 209, in http_request\n    raise MlflowException(f\"API request to {url} failed with exception {e}\")\nmlflow.exceptions.MlflowException: API request to http://mlflow.<uri>.com:5000/api/2.0/mlflow/runs/log-model failed with exception HTTPConnectionPool(host='mlflow.<uri>.com', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-model (Caused by ResponseError('too many 500 error responses'))\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\nprint(kwargs)\nhttps://github.com/mlflow/mlflow/blob/master/mlflow/utils/request_utils.py#L237\n```python\n{'headers': {'User-Agent': 'mlflow-python-client/2.21.3'}, 'verify': True, 'timeout': 120, 'json': {'run_id': '17fffda95b9c49f5bb8a7fcb3e0a997c', 'model_json': '{\"run_id\": \"17fffda95b9c49f5bb8a7fcb3e0a997c\", \"artifact_path\": \"models\", \"utc_time_created\": \"2025-04-23 18:06:42.890941\", \"model_uuid\": \"615e08b78ed84184b92b803f53d35f74\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.6.1\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}'}\n```\n# Localhost\n![Image](https://github.com/user-attachments/assets/065f28f4-4355-40dc-b07a-029e0257c34f)\n\n# Remote\n\n![Image](https://github.com/user-attachments/assets/a7692843-eb9f-4bc3-83a2-1685a9c7550f)\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [x] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "@sniafas \n\n> 2025/04/23 21:10:54 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under s3://path1/17fffda95b9c49f5bb8a7fcb3e0a997c/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n\nCan you try setting logging level debug and see if it prints out more concrete information about the error?"
      },
      {
        "user": "sniafas",
        "body": "@B-Step62 The rest of stacktrace is the output of setting the logging.DEBUG"
      },
      {
        "user": "sniafas",
        "body": "ÎÎ¿ bug. Issue on server setup"
      }
    ]
  },
  {
    "issue_number": 15625,
    "title": "[FR] Improve Download Trace Data Speed by Avoiding Resource Creation in span.from_dict()",
    "author": "Ethan-cw",
    "state": "closed",
    "created_at": "2025-05-08T07:59:38Z",
    "updated_at": "2025-05-09T16:25:04Z",
    "labels": [
      "enhancement",
      "area/tracking",
      "has-closing-pr"
    ],
    "body": "### Willingness to contribute\n\nNo. I cannot contribute this feature at this time.\n\n### Proposal Summary\n\nThe process of downloading trace data in mlflow.search_traces() involves creating a Resource object within the span.from_dict() method, which can be time-consuming and inefficient.\n\nThis feature request proposes optimizing the download trace data functionality by bypassing unnecessary resource creation, thereby improving overall performance. \n\nThe expected result is a faster and more efficient trace data downloading process, enhancing system responsiveness and user experience.\n\n### Motivation\n\n> #### What is the use case for this feature?\nUsers who frequently download large volumes of trace data will benefit from reduced wait times and improved responsiveness.\n> #### Why is this use case valuable to support for MLflow users in general?\nFor MLflow users, optimizing the download trace data process is valuable because it enhances the efficiency of tracking and managing machine learning experiments.\n> #### Why is this use case valuable to support for your project(s) or organization?\nThis use case is valuable for our organization because we regularly need to search and analyze large volumes of trace data. \n> #### Why is it currently difficult to achieve this use case?\nAchieving this use case is difficult because the existing process of  mlflow.search_traces()  involves creating unnecessary Resource objects within the span.from_dict() method, which slows down the download of trace data. \n\n### Details\n\nAvoid creating unnecessary resource objects within span.from_dict() during downloading trace data.\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "Ethan-cw",
        "body": "Downloading trace data takes over 7 seconds, with most of the time spent on creating resources within span.from_dict().\n![Image](https://github.com/user-attachments/assets/aa90cd92-ee6c-4966-b474-10b1534122ef)\n"
      }
    ]
  },
  {
    "issue_number": 8104,
    "title": "Incomplete download of large artifacts [BUG]",
    "author": "TobiasRothlin",
    "state": "open",
    "created_at": "2023-03-27T08:41:40Z",
    "updated_at": "2025-05-09T15:38:50Z",
    "labels": [
      "bug",
      "area/artifacts"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Willingness to contribute\n\nYes. I can contribute a fix for this bug independently.\n\n### MLflow version\n\n- Client: 2.2.2\r\n- Tracking server: 2.2.2\r\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Docker Image Python:3.8 (Debian 11)\r\npython@sha256:4f33b4ec9683b0ff1f116fb3904b10dd2be205d25c16af61c8a1e8f264635cb0\r\n- **Python version**: Python 3.8.16 \r\n- **yarn version, if running the dev UI**:Is not ui issue\n\n### Describe the problem\n\nWe use the download_artifacts method to download a model from the MlFlow Tracking Server. When we run the code locally on the developer machine, the method downloads all files correctly. When we run the code in the docker container, PyTorch raises an exception informing us that the weights can't be loaded correctly. Our Tracking server uses a HttpArtifactRepository for storing the files. After some investigation, we found that the weights file (~500MB) downloaded from the Tracking Server is not the same size as the file on the Tracking Server. When looking at the Bytes in the file, we noted that the file download just stopped at about 80%. \r\n![Capture](https://user-images.githubusercontent.com/25767557/227859172-9edaa8ce-686f-4e70-be58-6248c49e19db.PNG)\r\nAfter rerunning the code several times and reinspecting the file, we found that the download stopped at different locations; however, the file was never complete. This error occurred solely when downloading larger files (~500MB). \r\n\r\nAfter looking at the code implemented in the MlFlow.artifacts Package, we found that the _download_file function uses the request library to download the files. It uses the GET method to load the files via a stream and writes MB by MB into the file system. However, it never checks if the entire file is downloaded. The requests library does not raise an exception. Thus, MlFlow does not raise an exception but continues to the subsequent file. \r\nWe suspect the network connection is interrupted shortly during the download of the larger file.\r\n\r\nTo solve the problem, we implemented a new _download_file method that compares the content-length in the request's header with the downloaded file size. If this length does not match, it will restart the file download at the last position using the Range attribute in the request header. If this fails, the function will raise an exception.\r\n\r\nThis solved our problem, and the files are now all downloading correctly.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```shell\r\n System information: Linux #1 SMP Fri Jan 27 02:56:13 UTC 2023\r\n Python version: 3.8.16\r\n MLflow version: 2.2.2\r\n MLflow module location: /usr/local/lib/python3.8/site-packages/mlflow/__init__.py\r\n Tracking URI: file:///app/mlruns\r\n Registry URI: file:///app/mlruns\r\n MLflow dependencies:\r\n   Flask: 2.1.3\r\n   Jinja2: 3.1.2\r\n   alembic: 1.10.2\r\n   click: 8.1.3\r\n   cloudpickle: 2.2.1\r\n   databricks-cli: 0.17.5\r\n   docker: 6.0.1\r\n   entrypoints: 0.4\r\n   gitpython: 3.1.31\r\n   gunicorn: 20.1.0\r\n   importlib-metadata: 6.1.0\r\n   markdown: 3.4.1\r\n   matplotlib: 3.7.1\r\n   numpy: 1.23.0\r\n   packaging: 23.0\r\n   pandas: 1.5.3\r\n   protobuf: 3.20.2\r\n   pyarrow: 11.0.0\r\n   pytz: 2022.7.1\r\n   pyyaml: 6.0\r\n   querystring-parser: 1.2.4\r\n   requests: 2.28.2\r\n   scikit-learn: 1.2.2\r\n   scipy: 1.9.3\r\n   shap: 0.41.0\r\n   sqlalchemy: 1.4.47\r\n   sqlparse: 0.4.3\r\n```\r\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nimport mlflow\r\nimport os\r\n\r\ndef load_files_from_mlflow(url=\"mlflow-artifacts:/10/a4f04eb95cf141268f32e17cdec79f75/artifacts\"):\r\n    os.environ['MLFLOW_TRACKING_USERNAME'] = '_'\r\n    os.environ['MLFLOW_TRACKING_PASSWORD'] = '_'\r\n    mlflow.set_tracking_uri(\"_\")\r\n\r\n    print(f\"loading files from MLflow ({url})\")\r\n    path = mlflow.artifacts.download_artifacts(url, dst_path=\"./models\")\r\n    print(f\"saved files in {path}\")\r\n    return path\r\n\r\n\r\nif __name__ == '__main__':\r\n    load_files_from_mlflow()\r\n```\r\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\nThe  mlflow.artifacts.download_artifacts does not raise any Exception if the download is incomplete.\r\n```\r\n\r\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\r\n```\r\n\r\n```\r\n\n\n### What component(s) does this bug affect?\n\n- [X] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "harupy",
        "body": "@TobiasRothlin Thanks for reporting the issue. I found a couple related issues:\r\n\r\n- https://github.com/psf/requests/issues/4956\r\n- https://stackoverflow.com/questions/69919912/requests-iter-content-thinks-file-is-complete-but-its-not\r\n\r\n\r\nIt looks like this issue is not fixed yet in `requests`. Looking forward to your PR :)"
      },
      {
        "user": "harupy",
        "body": "@TobiasRothlin We found responses from the tracking server don't contain the `Content-Length` header. Can you add it in your PR?"
      },
      {
        "user": "TobiasRothlin",
        "body": "@harupy Thank you for the information. I have noticed the same issue. In the above-explained solution, I used the\r\n``/get-artifact`` endpoint. This endpoint is used in the UI to download the artifacts. The endpoint in the _download_file method utilises the ``/api/2.0/mlflow-artifacts/artifacts/...`` endpoint which does not return the ``Content-Length``(as you stated). To solve this problem I have now changed the request to the ``/get-artifact`` endpoint. We still call the http_request function but now just to a different endpoint. I have tested it in the docker container and now all files are downloaded correctly. The fork on which I have implemented the solution is under this URL: [fix_incomplete_download_of_large_artifacts](https://github.com/TobiasRothlin/mlflow/tree/fix_incomplete_download_of_large_artifacts). I have not made a PR at this point since to call the ``/get-artifact`` endpoint I have to reparse the ``endpoint`` and ``self.artifact_uri`` variables which is not the cleanest solution and would like to know if there would be a better way."
      }
    ]
  },
  {
    "issue_number": 14219,
    "title": "[BUG] filtering by dataset in the UI doesn't work",
    "author": "dmartinalbo-phrase",
    "state": "closed",
    "created_at": "2025-01-09T12:02:47Z",
    "updated_at": "2025-05-09T09:23:50Z",
    "labels": [
      "bug",
      "area/uiux"
    ],
    "body": "### MLflow version\n\n2.19.0\n\n### System information\n\nManaged MLflow on Databricks\n\n### Describe the problem\n\nIn the MLflow UI, filtering by dataset using the Dataset dropdown doesn't work.\n\n### Steps to reproduce the bug\n\n1. Open the MLflow UI and navigate to an experiment that has runs associated with one or more datasets.\r\n2. Identify a dataset from the list of datasets displayed in the UI.\r\n3. Click on the dataset to filter runs by it.\r\n4. Observe the results. Instead of displaying the runs associated with the selected dataset, the UI shows no runs and displays the following message: _\"All runs are filtered. All runs in this experiment have been filtered. Change or clear filters to view runs.\"_\r\n5. Clear the filter and verify that the runs are visible again without any filters applied.\r\n\n\n### Code to generate data required to reproduce the bug\n\n_No response_\n\n### Is the console panel in DevTools showing errors relevant to the bug?\n\n_No response_\n\n### Does the network panel in DevTools contain failed requests relevant to the bug?\n\n_No response_",
    "comments": [
      {
        "user": "harupy",
        "body": "Thanks for reporting. I'll try to reproduce it."
      },
      {
        "user": "harupy",
        "body": "@dmartinalbo-phrase I was able to reproduce the error on Databricks."
      },
      {
        "user": "harupy",
        "body": "It turns out this is an issue in the mlflow databricks backend."
      }
    ]
  },
  {
    "issue_number": 15641,
    "title": "[BUG] Error when using gemini-2.0-flash in prompt engineering ui \"The chat route is not implemented for Gemini models.\"",
    "author": "mfiqihalayubi",
    "state": "closed",
    "created_at": "2025-05-09T04:13:47Z",
    "updated_at": "2025-05-09T04:42:37Z",
    "labels": [
      "area/uiux",
      "area/docker",
      "area/deployments"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.22.0\n- Tracking server: 2.22.0\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 11\n- **Python version**: 3..11.5\n- **yarn version, if running the dev UI**: \n\n\n### Describe the problem\n\nI have deployed MLflow and AI Gateway using Docker to try out the Prompt Engineering UI feature. I am able to test prompts in the Prompt Engineering UI with the GPT provider, but not with Gemini (both with chat and completions). When using Gemini, I get the following error message in the UI:\n\nMLflow deployment returned the following error:\n\"Deployments proxy request failed with error code 501. Error message: {\"detail\":\"The chat route is not implemented for Gemini models.\"}\"\n\nMy question is: Does MLflow AI Gateway and the Prompt Engineering UI feature already support models from the Gemini provider?\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nREPLACE_ME\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nendpoints:\n  - name: openai-chat-4o-mini\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n\n  - name: openai-chat-4o\n    endpoint_type: llm/v1/chat\n    model:\n      provider: openai\n      name: gpt-4o\n      config:\n        openai_api_key: $OPENAI_API_KEY\n        \n  - name: openai-chat-4o-mini-completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-4o-mini\n      config:\n        openai_api_key: $OPENAI_API_KEY\n\n  - name: openai-chat-4o-completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: openai\n      name: gpt-4o\n      config:\n        openai_api_key: $OPENAI_API_KEY\n\n  - name: gemini-2.0-flash-completions\n    endpoint_type: llm/v1/completions\n    model:\n      provider: gemini\n      name: gemini-2.0-flash\n      config:\n        gemini_api_key: $GEMINI_API_KEY\n\n  - name: gemini-2.0-flash-chat\n    endpoint_type: llm/v1/chat\n    model:\n      provider: gemini\n      name: gemini-2.0-flash\n      config:\n        gemini_api_key: $GEMINI_API_KEY\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [x] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [x] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [x] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "mfiqihalayubi",
        "body": "@joelrobin18 Can you help me with this ? since i've seen that you are the person who actively update the ai gateway for gemini"
      },
      {
        "user": "TomeHirata",
        "body": "Hi, @mfiqihalayubi. https://github.com/mlflow/mlflow/pull/15069 has been merged to support chat/completion for Gemini Gateway. You can try it by installing the dev version of mlflow using `pip install git+[https://github.com/mlflow/mlflow.git](https://github.com/mlflow/mlflow.git)`"
      }
    ]
  },
  {
    "issue_number": 15515,
    "title": "[BUG] OSError: [Errno 45] Operation not supported: '/home/work'",
    "author": "ZiqiXiao",
    "state": "open",
    "created_at": "2025-04-27T14:53:04Z",
    "updated_at": "2025-05-09T01:21:48Z",
    "labels": [
      "bug",
      "area/artifacts"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nLocal machine\n\n### MLflow version\n\n- Client: 2.22.0\n- Tracking server: 2.22.0\n\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Client: Macos 15.3.2, Tracking Server: Ubuntu 24.04.2 LTS\n- **Python version**: client: 3.10.16, tracking server: 3.12.9\n- **yarn version, if running the dev UI**:\n\n\n### Describe the problem\n\nI am tring to log a lightgbm model to a remote tracking server. It seems that mlflow.utils.file_utils.py mkdir() func is making directory on my client computer, which causes an OSError. I think it should create an directory on server not locally. And it is not problem when using localhost (client and trakcing server on the same machine). And it seems happens only when saving artifacts to the server, the params, metrics and tags so on is not affected by this.\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nset -e\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"\nMLFLOW_ROOT=\"file://${SCRIPT_DIR}/mlruns\"\n\nmlflow server \\\n  --host 0.0.0.0 \\\n  --port 8016 \\\n  --default-artifact-root \"${MLFLOW_ROOT}\" \\\n  --backend-store-uri \"${MLFLOW_ROOT}\" \\\n  --serve-artifacts \\\n  --artifacts-destination \"${MLFLOW_ROOT}\"\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```python\nimport mlflow\nfrom mlflow.models import infer_signature\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Load the Iris dataset\nX, y = datasets.load_iris(return_X_y=True)\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Define the model hyperparameters\nparams = {\n    \"solver\": \"lbfgs\",\n    \"max_iter\": 1000,\n    \"multi_class\": \"auto\",\n    \"random_state\": 8888,\n}\n\n# Train the model\nlr = LogisticRegression(**params)\nlr.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = lr.predict(X_test)\n\n# Calculate metrics\naccuracy = accuracy_score(y_test, y_pred)\n\n# Set our tracking server uri for logging\nmlflow.set_tracking_uri(uri=\"http://your-tracking-server\") # don't use localhost\n\n# Create a new MLflow Experiment\nmlflow.set_experiment(\"MLflow Quickstart\")\n\n# Start an MLflow run\nwith mlflow.start_run():\n# Log the hyperparameters\nmlflow.log_params(params)\n\n# Log the loss metric\nmlflow.log_metric(\"accuracy\", accuracy)\n\n# Set a tag that we can use to remind ourselves what this run was for\nmlflow.set_tag(\"Training Info\", \"Basic LR model for iris data\")\n\n# Infer the model signature\nsignature = infer_signature(X_train, lr.predict(X_train))\n\n# Log the model\nmodel_info = mlflow.sklearn.log_model(\n    sk_model=lr,\n    artifact_path=\"iris_model\",\n    signature=signature,\n    input_example=X_train,\n    registered_model_name=\"tracking-quickstart\",\n)\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```python\n---------------------------------------------------------------------------\nOSError                                   Traceback (most recent call last)\nCell In[5], line 22\n     19 signature = infer_signature(X_train, lr.predict(X_train))\n     21 # Log the model\n---> 22 model_info = mlflow.sklearn.log_model(\n     23     sk_model=lr,\n     24     artifact_path=\"iris_model\",\n     25     signature=signature,\n     26     input_example=X_train,\n     27     registered_model_name=\"tracking-quickstart\",\n     28 )\n\nFile ~/baidu/dev/mlflow_example/.conda/lib/python3.10/site-packages/mlflow/sklearn/__init__.py:413, in log_model(sk_model, artifact_path, conda_env, code_paths, serialization_format, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, pyfunc_predict_fn, metadata)\n    334 @format_docstring(LOG_MODEL_PARAM_DOCS.format(package_name=\"scikit-learn\"))\n    335 def log_model(\n    336     sk_model,\n   (...)\n    348     metadata=None,\n    349 ):\n    350     \"\"\"\n    351     Log a scikit-learn model as an MLflow artifact for the current run. Produces an MLflow Model\n    352     containing the following flavors:\n   (...)\n    411 \n    412     \"\"\"\n--> 413     return Model.log(\n    414         artifact_path=artifact_path,\n    415         flavor=mlflow.sklearn,\n    416         sk_model=sk_model,\n    417         conda_env=conda_env,\n    418         code_paths=code_paths,\n    419         serialization_format=serialization_format,\n    420         registered_model_name=registered_model_name,\n    421         signature=signature,\n    422         input_example=input_example,\n    423         await_registration_for=await_registration_for,\n    424         pip_requirements=pip_requirements,\n    425         extra_pip_requirements=extra_pip_requirements,\n    426         pyfunc_predict_fn=pyfunc_predict_fn,\n    427         metadata=metadata,\n    428     )\n\nFile ~/baidu/dev/mlflow_example/.conda/lib/python3.10/site-packages/mlflow/models/model.py:921, in Model.log(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, **kwargs)\n    918     for prompt in prompts:\n    919         client.log_prompt(run_id, prompt)\n--> 921 mlflow.tracking.fluent.log_artifacts(local_path, mlflow_model.artifact_path, run_id)\n    923 # if the model_config kwarg is passed in, then log the model config as an params\n    924 if model_config := kwargs.get(\"model_config\"):\n\nFile ~/baidu/dev/mlflow_example/.conda/lib/python3.10/site-packages/mlflow/tracking/fluent.py:1219, in log_artifacts(local_dir, artifact_path, run_id)\n   1185 \"\"\"\n   1186 Log all the contents of a local directory as artifacts of the run. If no run is active,\n   1187 this method will create a new active run.\n   (...)\n   1216             mlflow.log_artifacts(tmp_dir, artifact_path=\"states\")\n   1217 \"\"\"\n   1218 run_id = run_id or _get_or_start_run().info.run_id\n-> 1219 MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n\nFile ~/baidu/dev/mlflow_example/.conda/lib/python3.10/site-packages/mlflow/tracking/client.py:2428, in MlflowClient.log_artifacts(self, run_id, local_dir, artifact_path)\n   2381 def log_artifacts(\n   2382     self, run_id: str, local_dir: str, artifact_path: Optional[str] = None\n   2383 ) -> None:\n   2384     \"\"\"Write a directory of files to the remote ``artifact_uri``.\n   2385 \n   2386     Args:\n   (...)\n   2426 \n   2427     \"\"\"\n-> 2428     self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n\nFile ~/baidu/dev/mlflow_example/.conda/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:964, in TrackingServiceClient.log_artifacts(self, run_id, local_dir, artifact_path)\n    955 def log_artifacts(self, run_id, local_dir, artifact_path=None):\n    956     \"\"\"Write a directory of files to the remote ``artifact_uri``.\n    957 \n    958     Args:\n   (...)\n    962 \n    963     \"\"\"\n--> 964     self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n\nFile ~/baidu/dev/mlflow_example/.conda/lib/python3.10/site-packages/mlflow/store/artifact/local_artifact_repo.py:66, in LocalArtifactRepository.log_artifacts(self, local_dir, artifact_path)\n     62 artifact_dir = (\n     63     os.path.join(self.artifact_dir, artifact_path) if artifact_path else self.artifact_dir\n     64 )\n     65 if not os.path.exists(artifact_dir):\n---> 66     mkdir(artifact_dir)\n     67 shutil.copytree(src=local_dir, dst=artifact_dir, dirs_exist_ok=True)\n\nFile ~/baidu/dev/mlflow_example/.conda/lib/python3.10/site-packages/mlflow/utils/file_utils.py:211, in mkdir(root, name)\n    209 except OSError as e:\n    210     if e.errno != errno.EEXIST or not os.path.isdir(target):\n--> 211         raise e\n    212 return target\n\nFile ~/baidu/dev/mlflow_example/.conda/lib/python3.10/site-packages/mlflow/utils/file_utils.py:208, in mkdir(root, name)\n    206 target = os.path.join(root, name) if name is not None else root\n    207 try:\n--> 208     os.makedirs(target, exist_ok=True)\n    209 except OSError as e:\n    210     if e.errno != errno.EEXIST or not os.path.isdir(target):\n\nFile ~/baidu/dev/mlflow_example/.conda/lib/python3.10/os.py:215, in makedirs(name, mode, exist_ok)\n    213 if head and tail and not path.exists(head):\n    214     try:\n--> 215         makedirs(head, exist_ok=exist_ok)\n    216     except FileExistsError:\n    217         # Defeats race condition when another thread created the path\n    218         pass\n\nFile ~/baidu/dev/mlflow_example/.conda/lib/python3.10/os.py:215, in makedirs(name, mode, exist_ok)\n    213 if head and tail and not path.exists(head):\n    214     try:\n--> 215         makedirs(head, exist_ok=exist_ok)\n    216     except FileExistsError:\n    217         # Defeats race condition when another thread created the path\n    218         pass\n\n    [... skipping similar frames: makedirs at line 215 (4 times)]\n\nFile ~/baidu/dev/mlflow_example/.conda/lib/python3.10/os.py:215, in makedirs(name, mode, exist_ok)\n    213 if head and tail and not path.exists(head):\n    214     try:\n--> 215         makedirs(head, exist_ok=exist_ok)\n    216     except FileExistsError:\n    217         # Defeats race condition when another thread created the path\n    218         pass\n\nFile ~/baidu/dev/mlflow_example/.conda/lib/python3.10/os.py:225, in makedirs(name, mode, exist_ok)\n    223         return\n    224 try:\n--> 225     mkdir(name, mode)\n    226 except OSError:\n    227     # Cannot rely on checking for EEXIST, since the operating system\n    228     # could give priority to other errors like EACCES or EROFS\n    229     if not exist_ok or not path.isdir(name):\n\nOSError: [Errno 45] Operation not supported: '/home/work'\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nREPLACE_ME\n```\n\n\n### What component(s) does this bug affect?\n\n- [x] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "B-Step62",
        "body": "@ZiqiXiao Could you test what does `mlflow.get_run(\"<your-run-id>\").info.artifact_uri` return? My suspicion is that it returns the local URI (on tracking server) you've passed to `--artifacts-destination`, which confuses MLflow to save artifacts locally. If that's the case, we should fix that bug and set the correct remote artifact URI there."
      },
      {
        "user": "ZiqiXiao",
        "body": "> [@ZiqiXiao](https://github.com/ZiqiXiao) Could you test what does `mlflow.get_run(\"<your-run-id>\").info.artifact_uri` return? My suspicion is that it returns the local URI (on tracking server) you've passed to `--artifacts-destination`, which confuses MLflow to save artifacts locally. If that's the case, we should fix that bug and set the correct remote artifact URI there.\n\nThe return is a route on the tracking server. \n```python\n# Set our tracking server uri for logging\nfrom pathlib import Path\n\nmlflow.set_tracking_uri(uri=\"http://tracking-server.com\")\n\n# Create a new MLflow Experiment\nmlflow.create_experiment(\n    \"Iris v1\", artifact_location=Path.cwd().joinpath(\"mlruns\").as_uri()\n)\n\n# Start an MLflow run\nwith mlflow.start_run() as run:\n    print(f\"{mlflow.get_run(run.info.run_id).info.artifact_uri}\")\n    # Log the hyperparameters\n    mlflow.log_params(params)\n\n    # Log the loss metric\n    mlflow.log_metric(\"accuracy\", accuracy)\n\n    # Set a tag that we can use to remind ourselves what this run was for\n    mlflow.set_tag(\"Training Info\", \"Basic LR model for iris data\")\n\n    # Infer the model signature\n    signature = infer_signature(X_train, lr.predict(X_train))\n\n    # Log the model\n    model_info = mlflow.sklearn.log_model(\n        sk_model=lr,\n        artifact_path=\"iris_model\",\n        signature=signature,\n        input_example=X_train,\n        registered_model_name=\"tracking-quickstart\",\n    )\n```\n\nThe return is \n`file:///home/work/dev/mlflow/mlruns/0/4e7979448d8c49a3850967f38ee9c3dd/artifacts`"
      },
      {
        "user": "ZiqiXiao",
        "body": "> [@ZiqiXiao](https://github.com/ZiqiXiao) Could you test what does `mlflow.get_run(\"<your-run-id>\").info.artifact_uri` return? My suspicion is that it returns the local URI (on tracking server) you've passed to `--artifacts-destination`, which confuses MLflow to save artifacts locally. If that's the case, we should fix that bug and set the correct remote artifact URI there.\n\n@B-Step62 Any follow up on this?"
      }
    ]
  },
  {
    "issue_number": 15227,
    "title": "[BUG] Unable to register LangChain chain model using prompty",
    "author": "edgBR",
    "state": "open",
    "created_at": "2025-04-04T13:48:53Z",
    "updated_at": "2025-05-07T10:50:36Z",
    "labels": [
      "bug",
      "area/model-registry",
      "area/models",
      "integrations/azure"
    ],
    "body": "### Issues Policy acknowledgement\n\n- [x] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\n\n### Where did you encounter this bug?\n\nAzure Machine Learning\n\n### MLflow version\n\n- Client: mlflow, version 2.21.3\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 24.10.18\n- **Python version**:  3.10.14=h955ad1f_1\n\n\n### Describe the problem\n\nI am trying to register a langchain model that is composed of a very simple chain to invoke AzureOpenAI and uses langchain-prompty extension to load the prompt.\n\nUnfortunately, besides the multiple warnings the code issue I am not able to debug why this is failing. I do understand some of them refering to the code thing and I also tried to pass a serializable function using azure_ad_token_provider but I have not been successull.\n\nThe interesting thing is that if you see the logs you can clearly see a 200 from my OpenAI endpoint.\n\n\n\n\n### Tracking information\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```shell\nmlflow.doctor()\nSystem information: Linux #82~20.04.1-Ubuntu SMP Tue Sep 3 12:27:43 UTC 2024\nPython version: 3.10.14\nMLflow version: 2.21.3\nMLflow module location: /anaconda/envs/scoring_env/lib/python3.10/site-packages/mlflow/__init__.py\nTracking URI: azureml://XXXXXXXXXXXXXXXXXXXXXXXXXXXX.workspace.westeurope.api.azureml.ms/mlflow/v1.0/subscriptions/xxxxxxxxxxxxxxxxxxxxxxxxxxx/resourceGroups/XXXXXXXXXXXXXXXXXXX/providers/Microsoft.MachineLearningServices/workspaces/XXXXXXXXXXXXXXXXXXXXXXX\nRegistry URI: azureml://XXXXXXXXXXXXXXXXXXXXXXX.workspace.westeurope.api.azureml.ms/mlflow/v1.0/subscriptions/XXXXXXXXXXXXXXXXXXXXXXXX/resourceGroups/XXXXXXXXXXXXXXXXXXXXX/providers/Microsoft.MachineLearningServices/workspaces/XXXXXXXXXXXXXXXXXXX\nMLflow environment variables: \n  MLFLOW_EXPERIMENT_ID: XXXXXXXXXXXXXXXXXXXXXXXXXX\n  MLFLOW_TRACKING_URI: azureml://XXXXXXXXXXXXXXXXXX.workspace.westeurope.api.azureml.ms/mlflow/v1.0/subscriptions/XXXXXXXXXXXXXXXXXXXXXXX/resourceGroups/XXXXXXXXXXXXXXXXXXXXXXX/providers/Microsoft.MachineLearningServices/workspaces/xxxxxxxxxxxxxxxxxx\nMLflow dependencies: \n  Flask: 3.1.0\n  Jinja2: 3.1.6\n  aiohttp: 3.11.16\n  alembic: 1.15.2\n  azure-storage-file-datalake: 12.20.0\n  docker: 7.1.0\n  fastapi: 0.115.12\n  graphene: 3.4.3\n  gunicorn: 23.0.0\n  langchain: 0.3.22\n  markdown: 3.7\n  matplotlib: 3.10.1\n  mlflow-skinny: 2.21.3\n  numpy: 1.26.4\n  pandas: 2.2.3\n  pyarrow: 18.1.0\n  scikit-learn: 1.6.1\n  scipy: 1.15.2\n  sqlalchemy: 2.0.40\n  tiktoken: 0.9.0\n  uvicorn: 0.34.0\n```\n\n\n### Code to reproduce issue\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```python\n\nimport argparse\nimport os\nfrom pathlib import Path\nfrom typing import Any, Dict\nimport mlflow\nimport tiktoken\nfrom langchain_core.runnables.base import RunnableSerializable\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain_prompty import create_chat_prompt\nfrom utils.logger import Logger\nfrom utils.sdk_utils import AMLManager\n\nlogging = Logger().getLogger(__name__)\n\n\nclass PromptRegistrator(AMLManager):\n    \"\"\"Registration of prompt.\n\n    The class log and register the prompt as a model in shared registry.\n\n    Attributes\n    ----------\n......................\n    \"\"\"\n\n    def __init__(self, args) -> None:\n        \"\"\"__init__ method.\n\n        Parameters\n        ----------\n        args : Namespace\n            Command-line arguments.\n        \"\"\"\n        ## Resource variables\n        self.cluster_identity_id = os.getenv(\"CLUSTER_IDENTITY_ID\")\n        self.subscription_id = os.getenv(\"SUBSCRIPTION_ID\")\n        self.resource_group = os.getenv(\"RESOURCE_GROUP\")\n        self.workspace_name = os.getenv(\"WORKSPACE_NAME\")\n        self.registry_name = os.getenv(\"REGISTRY_NAME\")\n        self.registry_region = os.getenv(\"REGISTRY_REGION\")\n        self.azure_openai_endpoint = os.getenv(\n            \"AZURE_OPENAI_ENDPOINT\"\n        )\n        self.azure_openai_api_version = os.getenv(\"AZURE_OPENAI_VERSION\", \"2024-02-01\")\n\n        ## Super class initiation and clients\n        super().__init__(self.cluster_identity_id)\n        self.setup_client_workspace(\n            subscription_id=self.subscription_id,\n            resource_group=self.resource_group,\n            workspace_name=self.workspace_name,\n        )\n\n        # Config\n        self.logger = logging\n        self.args = args\n\n        # Tokens encoding class\n        self.encoding = tiktoken.encoding_for_model(self.args.llm_model)\n\n    def _create_langchain_model(self) -> RunnableSerializable[Dict[str, Any], Any]:\n        \"\"\"Create a LangChain model with Azure OpenAI authentication.\n\n        Returns\n        -------\n        chain : RunnablePassthrough\n            LangChain chain that can be invoked.\n        \"\"\"\n        # Get authentication token for Azure OpenAI\n        token = self.credentials.get_token(\"https://cognitiveservices.azure.com/.default\").token\n        # Create AzureChatOpenAI model with proper authentication\n        llm = AzureChatOpenAI(\n            azure_endpoint=self.azure_openai_endpoint,\n            api_version=self.azure_openai_api_version,\n            azure_deployment=self.args.llm_model,\n            azure_ad_token=token,\n            top_p=1,\n            temperature=0,\n            seed=42,\n            max_retries=5,\n            request_timeout=180,\n        )\n\n        # Load the prompt using prompty\n        folder = Path(__file__).parent.absolute().as_posix()  # lol\n        prompt = create_chat_prompt(os.path.join(folder, self.args.prompt_path))\n\n        # # Create parser for structured output\n        # output_parser\n\n        # Create the chain\n        chain = prompt | llm  # | output_parser # not suported yet\n\n        return chain\n\n    def register_prompt_as_model(self) -> None:\n        \"\"\"Method that register the prompt as model.\"\"\"\n        try:\n            # Loading prompt from txt file\n            with open(os.path.join(Path(__file__).parent.absolute().as_posix(), self.args.prompt_path), \"r\") as f:\n                prompt = f.read()\n\n            # Counting prompt tokens\n            token_number = len(self.encoding.encode(prompt))\n\n            # Setting experiment\n            ws = self.ml_client_workspace.workspaces.get(self.ml_client_workspace.workspace_name)\n            mlflow.set_tracking_uri(ws.mlflow_tracking_uri)\n            mlflow.set_experiment(\"prompt_registration\")\n\n            # Model details\n            model_name = self.args.project_name + \"_\" + self.args.prompt_name\n            model_tags = {\n                \"project_name\": self.args.project_name,\n                \"stage\": os.getenv(\"ENV\", \"dev\"),\n                \"type\": \"prompt\",\n                \"prompt_tokens\": token_number,\n            }\n\n            with mlflow.start_run():\n                # Model logging\n                self.logger.info(\"Logging prompt to run.\")\n                artifact_path = \"models\"\n                chain = self._create_langchain_model()\n\n                logged_model = mlflow.langchain.log_model(\n                    chain,\n                    artifact_path=artifact_path,\n                    input_example={\n                        \"input_text\": \"Hello my name is Maria and I am calling because XYZ\",\n                        \"file_id\": \"12345\",\n                    },\n                )\n                model_uri = mlflow.get_artifact_uri(artifact_path)\n                self.logger.info(f\"Logged model URI: {model_uri}\")\n\n                # Model registration\n                registered_model = mlflow.register_model(model_uri=model_uri, name=model_name, tags=model_tags)\n                self.logger.info(\n                    f\"Registered model {registered_model.name}:{registered_model.version} in {self.registry_name}\"\n                )\n\n                # Model sharing\n                self.logger.info(f\"Sharing model '{model_name}:{logged_model.registered_model_version}'\")\n                self.ml_client_workspace.models.share(\n                    name=registered_model.name,\n                    version=registered_model.version,\n                    registry_name=self.registry_name,\n                    share_with_name=registered_model.name,\n                    share_with_version=registered_model.version,\n                )\n\n        except Exception as e:\n            self.logger.error(\"Failed to register prompt.\")\n            raise e\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--project_name\", type=str, help=\"Name of the project.\")\n    parser.add_argument(\"--llm_model\", type=str, default=\"gpt-4o-reproducible\", help=\"Name of the LLM model.\")\n    # parser.add_argument(\"--prompt_path\", type=str, help=\"Path to file with prompt definition.\")\n    parser.add_argument(\n        \"--prompt_path\",\n        type=str,\n        default=\"prompts/example.prompty\",\n        help=\"Path to file with prompt definition.\",\n    )\n    parser.add_argument(\n        \"--prompt_name\",\n        type=str,\n        choices=[\"X\"; \"Y\", \"LangChainValidationPrompt\"],\n        default=\"LangChainValidationPrompt\",\n        help=\"Name of prompt.\",\n    )\n    args, _ = parser.parse_known_args()\n\n    p = PromptRegistrator(args=args)\n    p.register_prompt_as_model()\n```\n\nAML manager class as follows:\n\n```python\nclass AMLManager:\n    \"\"\"MLHelper utility class.\"\"\"\n\n    def __init__(self, client_id: str):\n        \"\"\"MLHelper constructor, creates chain credentials.\n\n        Parameters\n        ----------\n        client_id : str\n            The client ID associated with the managed identity, used when creating\n            the `ManagedIdentityCredential`.\n\n        \"\"\"\n        self.client_id = client_id\n        self.credentials = self.get_chained_credentials()\n\n    def get_chained_credentials(self):\n        \"\"\"Creates and returns a chained token credential for Azure authentication.\n\n        This function initializes a `ChainedTokenCredential` instance that combines\n        a managed identity credential and a default credential.\n\n        Returns\n        -------\n        chained_creds : ChainedTokenCredential\n            A chained token credential combining `ManagedIdentityCredential` and\n            `DefaultAzureCredential`, allowing applications to authenticate using\n            either managed identity or default credentials.\n        \"\"\"\n        try:\n            chained_creds = ChainedTokenCredential(\n                AzureCliCredential(),\n                ManagedIdentityCredential(client_id=self.client_id),\n            )\n            return chained_creds\n        except Exception as e:\n            raise e\n```\n\n\n### Stack trace\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nException has occurred: MlflowException\nFailed to save runnable sequence: {'1': 'AzureChatOpenAI -- Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.'}.\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/ebr-spain/code/Users/edgar.bahilo/spain-voice-analytics/aml/ml_logic/train/register_prompt.py\", line 177, in register_prompt_as_model\n    raise e\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/ebr-spain/code/Users/edgar.bahilo/spain-voice-analytics/aml/ml_logic/train/register_prompt.py\", line 177, in register_prompt_as_model\n    raise e\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/ebr-spain/code/Users/edgar.bahilo/spain-voice-analytics/aml/ml_logic/train/register_prompt.py\", line 201, in <module>\n    p.register_prompt_as_model()\nmlflow.exceptions.MlflowException: Failed to save runnable sequence: {'1': 'AzureChatOpenAI -- Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.'}.\n```\n\n\n### Other info / logs\n\n<!-- PLEASE KEEP BACKTICKS AND CHECK PREVIEW -->\n```\nINFO:azureml.mlflow._internal.utils:Parsing tracking uri /mlflow/v1.0/subscriptionsyyyyyyyyyyyyyyyyyyyyyyyyyyyyy/resourceGroups/XXXXXXXXXXXXXXXXXXXXXXXXXX/providers/Microsoft.MachineLearningServices/workspaces/XXXXXXXXXXX\nINFO:azureml.mlflow._internal.utils:Tracking uri /mlflow/v1.0/subscriptions/eXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/resourceGroups/XXXXXXXXXXXXXXXXXXXXXXXXXXXX/providers/Microsoft.MachineLearningServices/workspaces/XXXXXXXXXXXXXXXXX has sub idXXXXXXXXXXXXXXXXXXXXXX\n_cloud.cloud:Fetched cloud name from environment variable AZUREML_CURRENT_CLOUD\nINFO:__main__:Logging prompt to run.\n/anaconda/envs/scoring_env/lib/python3.10/site-packages/mlflow/langchain/utils/__init__.py:231: LangChainDeprecationWarning: Importing OpenAI from langchain.llms is deprecated. Please replace deprecated imports:\n\n>> from langchain.llms import OpenAI\n\nwith new imports of:\n\n>> from langchain_community.llms import OpenAI\nYou can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n  if cls := getattr(module, class_name, None):\n/anaconda/envs/scoring_env/lib/python3.10/site-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.llms import HuggingFacePipeline`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/anaconda/envs/scoring_env/lib/python3.10/site-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.llms import Databricks`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/anaconda/envs/scoring_env/lib/python3.10/site-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.llms import Mlflow`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/anaconda/envs/scoring_env/lib/python3.10/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.chat_models import ChatDatabricks`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/anaconda/envs/scoring_env/lib/python3.10/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.chat_models import ChatMlflow`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/anaconda/envs/scoring_env/lib/python3.10/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.chat_models import ChatOpenAI`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/anaconda/envs/scoring_env/lib/python3.10/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.chat_models import AzureChatOpenAI`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\nINFO:httpx:HTTP Request: POST https://XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/openai/deployments/XXXXXXXXXXXXXXXXXXXXXXXXXXXX/chat/completions?api-version=2024-02-01 \"HTTP/1.1 200 OK\"\n/anaconda/envs/scoring_env/lib/python3.10/site-packages/mlflow/langchain/runnables.py:292: UserWarning: Your model contains a class imported from the LangChain partner package `langchain-openai`. When loading the model back, MLflow will use the community version of the classes instead of the partner packages, which may lead to unexpected behavior. To ensure that the model is loaded correctly, it is recommended to save the model with the 'model-from-code' method instead: https://mlflow.org/docs/latest/models.html#models-from-code\n  warnings.warn(\nWARNING:langchain_community.chat_models.openai:WARNING! seed is not default parameter.\n                    seed was transferred to model_kwargs.\n                    Please confirm that seed is what you intended.\nWARNING:langchain_community.chat_models.openai:WARNING! stream is not default parameter.\n                    stream was transferred to model_kwargs.\n                    Please confirm that stream is what you intended.\nWARNING:langchain_community.chat_models.openai:WARNING! top_p is not default parameter.\n                    top_p was transferred to model_kwargs.\n                    Please confirm that top_p is what you intended.\nðŸƒ View run elated_glove_wj9z22xl at: https://XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXc.workspace.westeurope.api.azureml.ms/mlflow/v2.0/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXdev/#/experiments/XXXXXXXXXXXXXXXXXXXXXX4/runs/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nðŸ§ª View experiment at: https://XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX1c.workspace.westeurope.api.azureml.ms/mlflow/v2.0/subscriptions/XXXXXXXXXXXXXXXXXXXXXXXX/resourceGroups/rxxxxxxxxxxx/providers/Microsoft.MachineLearningServices/workspaces/XXXXXXXXXXXXXXXXXXXXXXXXXXXX/#/experiments/XXXXXXXXXXXXXXXXXXXXXX\nERROR:__main__:Failed to register prompt.\n```\n\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [x] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [x] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations",
    "comments": [
      {
        "user": "serena-ruan",
        "body": "@edgBR Does this fail at `mlflow.langchain.log_model` step? If yes, the recommended approach to log a langchain model is through [code-based logging](https://mlflow.org/docs/latest/model/models-from-code)"
      },
      {
        "user": "edgBR",
        "body": "Hi @serena-ruan \n\nI managed to register the model by using a separate python file called prompt_model.py:\n```python\n\n\"\"\"Module defining the Prompty LangChain model for MLflow code-based approach.\"\"\"\n\nimport os\nfrom typing import Any\n\nimport mlflow\nfrom azure.identity import ChainedTokenCredential, DefaultAzureCredential, ManagedIdentityCredential\nfrom langchain_core.messages import BaseMessage\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain_prompty import create_chat_prompt\nfrom mlflow.models import set_model\nfrom mlflow.pyfunc import PythonModel\n\n\nclass PromptyLangChainModel(PythonModel):\n    \"\"\"MLflow PythonModel wrapper for LangChain + Prompty models using environment variables.\"\"\"\n\n    def predict(self, context, model_input) -> BaseMessage | Any:\n        \"\"\"Make predictions using the LangChain chain.\"\"\"\n        # Handle different input formats\n        if hasattr(model_input, 'to_dict'):\n            # Convert DataFrame to dict\n            records = model_input.to_dict('records')\n            if records:\n                model_input = records[0]\n            else:\n                model_input = {}\n\n        # Run prediction with the chain\n        result = self.chain.invoke(model_input)\n        return result\n\n    def load_context(self, context) -> None:\n        \"\"\"Load artifacts from the MLflow model context using environment variables.\"\"\"\n        # Get configuration from environment variables with fallbacks\n        azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\",\n                                         \"XXXXXXXX\")\n        azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"XXXXXXXXXX\")\n        llm_model = os.getenv(\"LLM_MODEL\", \"XXXXXXXXXXXX\")\n\n        # Get Azure token using chained credential method for better flexibility\n        credentials = ChainedTokenCredential(\n            ManagedIdentityCredential(),\n            DefaultAzureCredential()\n        )\n        token = credentials.get_token(\"https://cognitiveservices.azure.com/.default\").token\n\n        # Create LLM\n        llm = AzureChatOpenAI(\n            azure_endpoint=azure_openai_endpoint,\n            api_version=azure_openai_api_version,\n            azure_deployment=llm_model,\n            azure_ad_token=token,\n            top_p=1,\n            temperature=0,\n            seed=42,\n            max_retries=5,\n            request_timeout=180,\n        )\n\n        # Load the prompt using prompty\n        prompt = create_chat_prompt(context.artifacts[\"prompt_path\"])\n\n        # Create the chain\n        self.chain = prompt | llm\n\n\n# Create the model instance and set it as the model for MLflow\nmodel_instance = PromptyLangChainModel()\nset_model(model_instance)\n\n```\n\nAnd then in my original code I switched to:\n```python\n\ndef register_prompt_as_model(self) -> None:\n        \"\"\"Method that register the prompt as model using code-based approach.\"\"\"\n        try:\n            # Loading prompt from txt file\n            prompt_file_path = os.path.join(Path(__file__).parent.absolute().as_posix(), self.args.prompt_path)\n            with open(prompt_file_path, \"r\") as f:\n                prompt = f.read()\n\n            # Counting prompt tokens\n            token_number = len(self.encoding.encode(prompt))\n\n            # Setting experiment\n            ws = self.ml_client_workspace.workspaces.get(self.ml_client_workspace.workspace_name)\n            mlflow.set_tracking_uri(ws.mlflow_tracking_uri)\n            mlflow.set_experiment(\"prompt_registration\")\n\n            # Model details\n            model_name = self.args.project_name + \"_\" + self.args.prompt_name\n            model_tags = {\n                \"project_name\": self.args.project_name,\n                \"stage\": os.getenv(\"ENV\", \"dev\"),\n                \"type\": \"prompt\",\n                \"country_code\": \"xxx\",\n                \"prompt_tokens\": token_number,\n                \"llm_model\": self.args.llm_model,\n            }\n\n            # Path to the model script file\n            model_script_path = os.path.join(\n                Path(__file__).parent.absolute().as_posix(),\n                \"prompt_model.py\" # parametrize this\n            )\n\n            with mlflow.start_run():\n                # Create temp directory for artifacts\n                with tempfile.TemporaryDirectory() as tmp_dir:\n                    # Copy prompt file to artifacts\n                    prompt_dest = os.path.join(tmp_dir, os.path.basename(self.args.prompt_path))\n                    shutil.copy(prompt_file_path, prompt_dest)\n\n                    # Define conda env with dependencies\n                    conda_env = {\n                        \"channels\": [\"conda-forge\"],\n                        \"dependencies\": [\n                            \"python=3.10\",\n                            \"pip\",\n                            {\"pip\": [\n                                \"mlflow>=2.12.2\",  # Updated version for Models from Code\n                                \"langchain-core>=0.1.0\",\n                                \"langchain-openai>=0.0.5\",\n                                \"langchain-prompty>=0.0.1\",\n                                \"azure-identity>=1.15.0\",\n                                \"tiktoken>=0.5.2\"\n                            ]},\n                        ],\n                        \"name\": \"prompty_langchain_env\"\n                    }\n\n                    # Log the model with the code-based approach\n                    self.logger.info(\"Logging prompt to run using MLflow code-based approach.\")\n                    artifact_path = \"models\"\n\n                    # Key change: Pass script path instead of model instance\n                    logged_model = mlflow.pyfunc.log_model(  # noqa: F841\n                        artifact_path=artifact_path,\n                        python_model=model_script_path,  # Path to script instead of model instance\n                        artifacts={\n                            \"prompt_path\": prompt_dest\n                        },\n                        conda_env=conda_env,\n                        input_example={\n                            \"transcription\": \"Hello my name is XXXX and my B is YYYY\",\n                            \"file_id\": \"12345\",\n                        }\n                    )\n\n                    # Rest of the registration process remains the same\n                    model_uri = mlflow.get_artifact_uri(artifact_path)\n                    self.logger.info(f\"Logged model URI: {model_uri}\")\n\n                    # Model registration\n                    registered_model = mlflow.register_model(\n                        model_uri=model_uri,\n                        name=model_name,\n                        tags=model_tags\n                    )\n                    self.logger.info(\n                        f\"Registered model {registered_model.name}:{registered_model.version} in {self.registry_name}\"\n                    )\n\n                    # Model sharing\n                    self.logger.info(f\"Sharing model '{model_name}:{registered_model.version}'\")\n                    self.ml_client_workspace.models.share(\n                        name=registered_model.name,\n                        version=registered_model.version,\n                        registry_name=self.registry_name,\n                        share_with_name=registered_model.name,\n                        share_with_version=registered_model.version,\n                    )\n\n        except Exception as e:\n            self.logger.error(f\"Failed to register prompt: {str(e)}\")\n            raise e\n\n```\n\nHowever I would like to understand a couple of things.\n\nMy PromptyLangChain class is based on PythonModel, I wonder if I can use an adapter pattern here something like:\n\n```python\nPromptyLangChain(AMLManager, PythonModel):\n\ndef init():\n   self.pythonmodel = PythonModel()\n   self.aml_manager = AMLManager()\n```\n\nThe reason I am asking this is because my AML Manager contains quite a lot of useful utilities (i.e setting up the azure credentials, a couple of environment variables I would like to reuse etc...)\n\nBasically I would like to reuse code as much as possible.\n\nIs this usage supported?\n\n"
      },
      {
        "user": "edgBR",
        "body": "Hi @serena-ruan \n\nThe problem with the langchain flavour is that if my prompts are in a different folder and I have no way to import them to the model.\n\nI mean I have tested to run mlflow.log_artifact() beforehand to place the prompty file in the same directory that the model but the problem is that I have no way to pass the artifact path to the create_chat_prompt() function in my chain. I need to hardcode then but if I do that the registration fails:\n\n` No such file or directory: \\'/anaconda/envs/scoring_env/lib/python3.10/site-packages/langchain_core/runnables/my_prompt_name.prompty\\'\\n\\n request payload`\n\nI was thinking in use the code_paths functionality but the prompty file is not a .py file. So I am not sure if this applies.\n\nAny help is appreaciated."
      }
    ]
  }
]