[
  {
    "issue_number": 21090,
    "title": "Set 'dynamic_ncols = True' PLEEEASE",
    "author": "Farenweh",
    "state": "open",
    "created_at": "2025-06-17T13:20:12Z",
    "updated_at": "2025-06-17T14:07:57Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nCurrently the progress bar does not dynamically adjust its width when the terminal window is resized. If you start a long-running process and then change the size of your terminal, the progress bar can either wrap unattractively onto the next line or become truncated, leading to a poor visual experience and a flush on terminal.\n\nSimply set `dynamic_ncols = True` for anywhere uses `tqdm` would help a lot, I guess. They are left defaultly as `Fasle` now.\n\n### Use case\n\nAlmost any case.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Farenweh, thank you for your suggestion and for contributing to the Ultralytics community 🚀! We encourage reviewing our [Docs](https://docs.ultralytics.com/) if you’re new here, as they cover many common questions and usage details.\n\nYour feature request regarding the dynamic adjustment of the progress bar using `dynamic_ncols = True` for `tqdm` is noted. This is an automated response to acknowledge your input—an Ultralytics engineer will review your suggestion and assist you soon.\n\nIf you’d like to further illustrate your request, please feel free to share a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us understand the impact more clearly.\n\nIf you have any additional context or run into related issues, you can always join our community for discussion and real-time support:\n- [Discord](https://discord.com/invite/ultralytics) 🎧\n- [Discourse](https://community.ultralytics.com/)\n- [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nTo ensure you have the latest improvements, upgrade to the newest `ultralytics` package and make sure all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) are met in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can run YOLO in any of these up-to-date, verified environments (with dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU:\n  <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a>\n  <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n  <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your feedback and for helping improve Ultralytics!"
      },
      {
        "user": "Y-T-G",
        "body": "Can you open a PR for this?"
      }
    ]
  },
  {
    "issue_number": 21074,
    "title": "The non-frame native parameters of multi-GPU training cannot be verified",
    "author": "Dasenyoung",
    "state": "closed",
    "created_at": "2025-06-16T13:01:29Z",
    "updated_at": "2025-06-17T13:43:53Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI use # 8620 method to create pos_weight (https://github.com/ultralytics/ultralytics/pull/8620#issue-2165568112), which weights assignment each category respectively; It was very successful when training with a single GPU, but it failed when training with multiple GPUs and reported an error:\nPS D:\\DZY\\yolov12> & C:/Users/dzy/miniconda3/envs/yolov5/python.exe d:/DZY/yolov12/train.py\nFlashAttention is not available on this device. Using scaled_dot_product_attention instead.\nTransferred 36/845 items from pretrained weights\nUltralytics 8.3.63 � Python-3.11.5 torch-2.1.0+cu118 CUDA:0 (NVIDIA GeForce RTX 4090 D, 24564MiB)\n                                                      CUDA:1 (NVIDIA GeForce RTX 4090 D, 24564MiB)\nengine\\trainer: task=detect, mode=train, model=ultralytics/cfg/models/v12/yolov12s-FB.yaml, data=./data.yaml, epochs=300, time=None, patience=100, batch=32, imgsz=640, save=T\nrue, save_period=-1, cache=False, device=0,1, workers=32, project=runs-you1200s-FB-class_weights-testgpu/train, name=exp6, exist_ok=False, pretrained=yolov12s.pt, optimizer=a\nuto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=False, fraction=1.0, profile=False, freeze=None,\n multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=Fa\nlse, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False,\n save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, opt\nimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warm\nup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pos_weight=[1], pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, sc\nale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.2, copy_paste=0.6, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop\n_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs-you1200s-FB-class_weights-testgpu\\train\\exp6\nOverriding model.yaml nc=80 with nc=4\n\n                   from  n    params  module                                       arguments\n  0                  -1  1         0  ultralytics.nn.modules.block.InputWrapper    [3]\n  1                   0  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]\n  2                   0  1     76519  ultralytics.nn.modules.FBM.FrequencyBranch   [64, 4]\n  3                   1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]\n  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]\n  5             [-1, 2]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n  6                  -1  1    221440  ultralytics.nn.modules.conv.Conv             [192, 128, 3, 2]\n  7                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]\n  8                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]\n  9                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 4]\n 10                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]\n 11                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 1]\n 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']\n 13             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 14                  -1  1    345856  ultralytics.nn.modules.block.A2C2f           [768, 256, 1, False, -1]\n 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']\n 16             [-1, 7]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 17                  -1  1     95104  ultralytics.nn.modules.block.A2C2f           [512, 128, 1, False, -1]\n 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]\n 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 20                  -1  1    296704  ultralytics.nn.modules.block.A2C2f           [384, 256, 1, False, -1]\n 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]\n 22            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 23                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]\n 24        [17, 20, 23]  1    820956  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]\nYOLOv12s-FB summary: 573 layers, 9,404,931 parameters, 9,404,915 gradients, 25.0 GFLOPs\n\nTransferred 839/845 items from pretrained weights\nDDP: debug command C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\python.exe -m torch.distributed.run --nproc_per_node 2 --master_port 54890 C:\\Users\\dzy\\AppData\\Roaming\\yolov12\\DDP\\_te\nmp_icz3tjqs1853797725904.py\n[2025-06-16 20:36:25,798] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\nTraceback (most recent call last):\n  File \"C:\\Users\\dzy\\AppData\\Roaming\\yolov12\\DDP\\_temp_icz3tjqs1853797725904.py\", line 11, in <module>\n    trainer = DetectionTrainer(cfg=cfg, overrides=overrides)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\site-packages\\ultralytics\\engine\\trainer.py\", line 101, in __init__\n    self.args = get_cfg(cfg, overrides)\n                ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py\", line 295, in get_cfg\n    check_dict_alignment(cfg, overrides)\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py\", line 483, in check_dict_alignment\n    raise SyntaxError(string + CLI_HELP_MSG) from e\nSyntaxError: 'pos_weight' is not a valid YOLO argument.\n\n    Arguments received: ['yolo']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of {'pose', 'obb', 'detect', 'classify', 'segment'}\n                MODE (required) is one of {'predict', 'track', 'train', 'benchmark', 'val', 'export'}\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Streamlit real-time webcam inference GUI\n        yolo streamlit-predict\n\n    6. Ultralytics solutions usage\n        yolo solutions count or in ['count', 'heatmap', 'queue', 'speed', 'workout', 'analytics', 'help'] source=\"path/to/video/file.mp4\"\n\n    7. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n\nTraceback (most recent call last):\n  File \"C:\\Users\\dzy\\AppData\\Roaming\\yolov12\\DDP\\_temp_icz3tjqs1853797725904.py\", line 11, in <module>\n    trainer = DetectionTrainer(cfg=cfg, overrides=overrides)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\site-packages\\ultralytics\\engine\\trainer.py\", line 101, in __init__\n    self.args = get_cfg(cfg, overrides)\n                ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py\", line 295, in get_cfg\n    check_dict_alignment(cfg, overrides)\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py\", line 483, in check_dict_alignment\n    raise SyntaxError(string + CLI_HELP_MSG) from e\nSyntaxError: 'pos_weight' is not a valid YOLO argument.\n\n    Arguments received: ['yolo']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of {'pose', 'obb', 'detect', 'classify', 'segment'}\n                MODE (required) is one of {'predict', 'track', 'train', 'benchmark', 'val', 'export'}\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Streamlit real-time webcam inference GUI\n        yolo streamlit-predict\n\n    6. Ultralytics solutions usage\n        yolo solutions count or in ['count', 'heatmap', 'queue', 'speed', 'workout', 'analytics', 'help'] source=\"path/to/video/file.mp4\"\n\n    7. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n\n[2025-06-16 20:36:35,844] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 24584) of binary: C:\\Users\\dzy\\miniconda3\\envs\\yolov\n5\\python.exe\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\site-packages\\torch\\distributed\\run.py\", line 810, in <module>\n    main()\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\errors\\__init__.py\", line 346, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\site-packages\\torch\\distributed\\run.py\", line 806, in main\n    run(args)\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\site-packages\\torch\\distributed\\run.py\", line 797, in run\n    elastic_launch(\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\site-packages\\torch\\distributed\\launcher\\api.py\", line 134, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\site-packages\\torch\\distributed\\launcher\\api.py\", line 264, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n============================================================\nC:\\Users\\dzy\\AppData\\Roaming\\yolov12\\DDP\\_temp_icz3tjqs1853797725904.py FAILED\n------------------------------------------------------------\nFailures:\n[1]:\n  time      : 2025-06-16_20:36:35\n  host      : WIN-AALTK24T28U\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 20348)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-06-16_20:36:35\n  host      : WIN-AALTK24T28U\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 24584)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\nTraceback (most recent call last):\n  File \"d:\\DZY\\yolov12\\train.py\", line 10, in <module>\n    model.train(data='./data.yaml', epochs=300, batch=32, device='0,1', imgsz=640, scale=0.9,  mosaic=1.0, workers=32, cache=False, mixup=0.2, copy_paste=0.6,\n  File \"d:\\DZY\\yolov12\\ultralytics\\engine\\model.py\", line 811, in train\n    self.trainer.train()\n  File \"d:\\DZY\\yolov12\\ultralytics\\engine\\trainer.py\", line 202, in train\n    raise e\n  File \"d:\\DZY\\yolov12\\ultralytics\\engine\\trainer.py\", line 200, in train\n    subprocess.run(cmd, check=True)\n  File \"C:\\Users\\dzy\\miniconda3\\envs\\yolov5\\Lib\\subprocess.py\", line 571, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['C:\\\\Users\\\\dzy\\\\miniconda3\\\\envs\\\\yolov5\\\\python.exe', '-m', 'torch.distributed.run', '--nproc_per_node', '2', '--master_port', '548\n90', 'C:\\\\Users\\\\dzy\\\\AppData\\\\Roaming\\\\yolov12\\\\DDP\\\\_temp_icz3tjqs1853797725904.py']' returned non-zero exit status 1.\nI attempted to add pos_weight to the parameter natively supported by the ultralytics framework, but failed. I don't know if the addition was successful, nor do I know if this is the reason for the training failure. I need help. Thank you. Note: I modified the code exactly in accordance with #8620; In addition, here is my training code:\nfrom ultralytics.models import YOLO\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nif __name__ == '__main__':\n    model = YOLO(model='ultralytics/cfg/models/v12/yolov12s-FB.yaml')\n    model.load('yolov12s.pt')\n    model.train(data='./data.yaml', epochs=300, batch=32, device='0,1', pos_weight:=[6, 1.0, 1.0, 4]  ,  imgsz=640, scale=0.9,  mosaic=1.0, workers=32, cache=False, mixup=0.2, copy_paste=0.6,\n                amp=False, project='runs-you1200s-FB-class_weights-testgpu/train', name='exp')\n\n\n\n\n\n \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Dasenyoung, thank you for reaching out and providing detailed information! 🚀 This is an automated response to help you get started—an Ultralytics engineer will also assist you soon.\n\nWe recommend reviewing the [Docs](https://docs.ultralytics.com/) for guidance on [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, as many common questions and issues are addressed there.\n\nIf this is a 🐛 Bug Report, please ensure you provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/) so we can best assist in debugging.\n\nFor custom training or advanced usage ❓, please share as much context as possible, including relevant code snippets, dataset samples, and full training logs. Make sure you’re following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin our community for real-time discussion and support:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for live chat\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) for broader community knowledge\n\n## Upgrade\n\nFirst, please ensure you are running the latest `ultralytics` package along with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [Python>=3.8](https://www.python.org/) environment with [PyTorch>=1.8](https://pytorch.org/get-started/locally/). This helps verify your issue hasn’t already been resolved:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can run YOLO in any of the following up-to-date, verified environments (all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) are preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. These tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across supported platforms.\n\nThank you for your patience! An Ultralytics engineer will review your issue and provide additional guidance soon."
      },
      {
        "user": "Y-T-G",
        "body": "You need to install the modified repo by running:\n\n`pip install .` (dot at the end)\n\ninside the repo directory."
      }
    ]
  },
  {
    "issue_number": 18220,
    "title": "Why latest version is downloading `yolo11n.pt` while training `yolov8*.pt` (or any other model)?",
    "author": "hdnh2006",
    "state": "closed",
    "created_at": "2024-12-13T10:55:11Z",
    "updated_at": "2025-06-17T13:42:40Z",
    "labels": [
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nTrain\n\n### Bug\n\nHello Ultralytics team,\r\n\r\nI noticed that `ultralytics` package is downloading the `yolo11n.pt` model while trying to train any other version of the YOLO.\r\n\r\nBelow you will find an example with `yolov8s.pt` but the same results occur when training other versions like `yolov9e.pt`, etc. As you can see, the first downloaded model is `yolov8s.pt` but in the next progress bar you will see another download which references to `yolo11n.pt`. \r\n\r\nMaybe there's some mocked code in the latest version by error; I don't know, guys. Can you please check it? This can be confusing for the user..\r\n\r\n```bash\r\nyolo train model=yolov8s.pt\r\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\r\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21.5M/21.5M [00:00<00:00, 112MB/s]\r\nWARNING ⚠️ 'data' argument is missing. Using default 'data=coco8.yaml'.\r\nUltralytics 8.3.49 🚀 Python-3.11.0rc1 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7940MiB)\r\nengine/trainer: task=detect, mode=train, model=yolov8s.pt, data=coco8.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\r\n\r\n                   from  n    params  module                                       arguments                     \r\n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \r\n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \r\n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \r\n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \r\n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \r\n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \r\n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \r\n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \r\n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \r\n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \r\n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \r\n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \r\n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \r\n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \r\n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \r\n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \r\n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \r\n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \r\n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \r\n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \r\n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \r\n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \r\n 22        [15, 18, 21]  1   2147008  ultralytics.nn.modules.head.Detect           [80, [128, 256, 512]]         \r\nModel summary: 225 layers, 11,166,560 parameters, 11,166,544 gradients, 28.8 GFLOPs\r\n\r\nTransferred 355/355 items from pretrained weights\r\nFreezing layer 'model.22.dfl.conv.weight'\r\nAMP: running Automatic Mixed Precision (AMP) checks...\r\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\r\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.35M/5.35M [00:00<00:00, 106MB/s]\r\nAMP: checks passed ✅\r\ntrain: Scanning /home/henry/Projects/CV_solutions/ultralytics/datasets/coco8/labels/train.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]\r\nval: Scanning /home/henry/Projects/CV_solutions/ultralytics/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]\r\nPlotting labels to runs/detect/train/labels.jpg... \r\noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \r\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\r\n```\n\n### Environment\n\n```bash\r\nyolo checks\r\nUltralytics 8.3.49 🚀 Python-3.11.0rc1 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7940MiB)\r\nSetup complete ✅ (16 CPUs, 14.9 GB RAM, 590.3/937.3 GB disk)\r\n\r\nOS                  Linux-6.5.0-45-generic-x86_64-with-glibc2.35\r\nEnvironment         Linux\r\nPython              3.11.0rc1\r\nInstall             pip\r\nRAM                 14.86 GB\r\nDisk                590.3/937.3 GB\r\nCPU                 AMD Ryzen 7 7735HS with Radeon Graphics\r\nCPU count           16\r\nGPU                 NVIDIA GeForce RTX 4060 Laptop GPU, 7940MiB\r\nGPU count           1\r\nCUDA                12.4\r\n\r\nnumpy               ✅ 2.1.3>=1.23.0\r\nnumpy               ✅ 2.1.3<2.0.0; sys_platform == \"darwin\"\r\nmatplotlib          ✅ 3.9.2>=3.3.0\r\nopencv-python       ✅ 4.10.0.84>=4.6.0\r\npillow              ✅ 11.0.0>=7.1.2\r\npyyaml              ✅ 6.0.2>=5.3.1\r\nrequests            ✅ 2.32.3>=2.23.0\r\nscipy               ✅ 1.14.1>=1.4.1\r\ntorch               ✅ 2.5.1>=1.8.0\r\ntorch               ✅ 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\r\ntorchvision         ✅ 0.20.1>=0.9.0\r\ntqdm                ✅ 4.67.1>=4.64.0\r\npsutil              ✅ 6.1.0\r\npy-cpuinfo          ✅ 9.0.0\r\npandas              ✅ 2.2.3>=1.1.4\r\nseaborn             ✅ 0.13.2>=0.11.0\r\nultralytics-thop    ✅ 2.0.12>=2.0.0\r\n\r\n```\n\n### Minimal Reproducible Example\n\n```bash\r\nyolo train model=yolov8s.pt\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @hdnh2006, thank you for bringing this up and for your detailed issue report! 🚀 \n\nWe recommend checking the [Ultralytics Docs](https://docs.ultralytics.com) as a starting point for understanding default behaviors, including model training. You'll find examples for [CLI](https://docs.ultralytics.com/usage/cli/) and [Python](https://docs.ultralytics.com/usage/python/) usage there, which might help clarify your concern.\n\nIf this is indeed a 🐛 Bug Report, we appreciate the detailed logs you've provided. However, could you please confirm/reproduce this behavior by ensuring you are running the latest version of the Ultralytics package? Start by upgrading via the following command:\n\n```bash\npip install -U ultralytics\n```\n\nWe also request a [Minimum Reproducible Example (MRE)](https://docs.ultralytics.com/help/minimum_reproducible_example/) where possible. You've already provided some great context and logs—thank you! If you can identify any additional reproducible steps beyond what you've shared, please add them here.\n\nTo ensure the cause is not environment-related, please verify that your setup matches one of our supported environments. YOLO can be run in the following configurations:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\nLastly, our community provides great support options:  \n- Join [Discord](https://discord.com/invite/ultralytics) 🎧 to chat in real-time  \n- Visit our [Discourse Forum](https://community.ultralytics.com) for detailed discussions  \n- Explore our [Subreddit](https://reddit.com/r/Ultralytics) to share ideas  \n\nThis is an automated response to help you get started; an Ultralytics engineer will review your issue and provide further assistance soon! 😊 Let's get this sorted out together."
      },
      {
        "user": "Y-T-G",
        "body": "Duplicate of https://github.com/ultralytics/ultralytics/issues/17801"
      },
      {
        "user": "hdnh2006",
        "body": "> Duplicate of #17801\r\n\r\nOk, sorry about it!"
      }
    ]
  },
  {
    "issue_number": 20773,
    "title": "Feature Request: Consideration for State-of-the-Art Real-time DETR Detectors (e.g., lw-detr, d-fine)",
    "author": "Farenweh",
    "state": "closed",
    "created_at": "2025-05-23T12:40:09Z",
    "updated_at": "2025-06-17T13:38:04Z",
    "labels": [
      "enhancement",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nFirst and foremost, thank you for your incredible work on the YOLO series and the Ultralytics ecosystem. It's an invaluable resource for the computer vision community.\n\nI would like to respectfully suggest consideration for tracking or potentially integrating advancements from some of the latest state-of-the-art real-time DETR-based object detectors. Specifically, models like lw-detr and d-fine have recently demonstrated very promising performance, achieving an excellent balance between speed and accuracy, which aligns well with the philosophy of YOLO.\n\nIncorporating insights or even providing support for such architectures could further enhance the capabilities of YOLO and offer users and researchers more cutting-edge options for their detection tasks.\n\nThank you for your time and consideration of this feature request. We greatly appreciate your continuous efforts in advancing the field.\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Farenweh, thank you for your thoughtful suggestion and for being part of the Ultralytics community! 🚀 We appreciate your recognition of the YOLO series and Ultralytics HUB, and your interest in pushing the boundaries of real-time detection.\n\nYour feature request regarding state-of-the-art DETR-based detectors like lw-detr and d-fine is valuable and has been noted. This is an automated response to acknowledge your input—an Ultralytics engineer will review your suggestion and follow up with you soon.\n\nFor more on current features and supported models, please visit our [Docs](https://docs.ultralytics.com/). If you have additional details, references, or use cases, feel free to add them to this thread to help us better understand your proposal.\n\nJoin the Ultralytics community where it suits you best:\n- For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussions, check out [Discourse](https://community.ultralytics.com/)\n- Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nTo take advantage of the latest features and improvements, ensure you are using the newest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following verified environments (with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "Thank you @Farenweh for the thoughtful suggestion and kind words about the Ultralytics ecosystem! We already support [RT-DETR models](https://docs.ultralytics.com/models/rtdetr/) which provide state-of-the-art transformer-based detection capabilities. While we continuously monitor emerging architectures, our focus remains on maintaining and improving the [YOLO11](https://docs.ultralytics.com/models/yolo11/) series which offers an excellent balance of speed, accuracy, and versatility across multiple tasks. For specific DETR variants like lw-detr or d-fine, we encourage community contributions through PRs if there's strong interest and demonstrated benefits over existing options."
      }
    ]
  },
  {
    "issue_number": 20187,
    "title": "Inquiry Regarding the Interpretation of YOLO11 Hyperparameter Tuning Results",
    "author": "bjh03205",
    "state": "open",
    "created_at": "2025-04-16T06:59:56Z",
    "updated_at": "2025-06-17T13:31:59Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello,\n\nI have a few questions regarding the interpretation of the hyperparameter tuning results for the YOLO11 model.\n\nI have two specific inquiries:\n\nUpon reviewing the tune_scatter_plots.png, I noticed that 7 of the 23 hyperparameters are set to a default value of 0. The absence of any variance during the random search tuning process suggests that these parameters are fixed. Could you please confirm if these values are intentionally configured to remain unchanged?\n\nIf these hyperparameters are indeed set to be fixed, I am curious to know the reasoning behind this decision.\n\nI have also attached an example result for your reference.\n\n![Image](https://github.com/user-attachments/assets/a147cb21-4c2f-46c4-98fa-bf5f672be378)\n<Some results were artificially masked>\n\nThank you very much for your time and assistance.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @bjh03205, thank you for your interest in Ultralytics 🚀! We recommend checking out our [Docs](https://docs.ultralytics.com/) for additional resources on [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, as many common questions are addressed there.\n\nThis is an automated response to help streamline your experience. An Ultralytics engineer will also review your question and assist you further soon.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate.\n\nFor custom training or hyperparameter tuning ❓ Questions, please share as much detail as possible (including dataset samples, training logs, and config files) and confirm you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community in your preferred space! For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧. For deeper discussions, try [Discourse](https://community.ultralytics.com/), or join our [Subreddit](https://reddit.com/r/Ultralytics) to connect with other users.\n\n## Upgrade\n\nTo make sure your issue isn't already resolved, upgrade to the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in any of the following up-to-date verified environments (with all dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your detailed question and for providing reference images! Your inquiry has been received and will be reviewed by the team soon."
      },
      {
        "user": "Y-T-G",
        "body": "What was the command you used to launch tuning?"
      },
      {
        "user": "bjh03205",
        "body": "Most settings were configured with default values, and the training command is as follows.\n\n```python\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolo11s.yaml').load()\nmodel.tune(data=\"training.yaml\", epochs=30, iterations=300, optimizer='SGD', plots=False, save=False, val=False, device=1)\n``` "
      }
    ]
  },
  {
    "issue_number": 21082,
    "title": "Support class-wise loss weighting for handling dataset imbalance",
    "author": "Y-T-G",
    "state": "open",
    "created_at": "2025-06-17T08:08:45Z",
    "updated_at": "2025-06-17T13:20:50Z",
    "labels": [
      "enhancement",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nUser requests to add configurable per-class weights\n\n### Use case\n\nCompensate for class imbalance in dataset\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Y-T-G, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nThis is an automated response to let you know your feature request has been received. An Ultralytics engineer will review your suggestion and assist you soon 😊.\n\nIf you have any additional details, example code, or references regarding class-wise loss weighting, please feel free to add them here—they'll help our team better understand your use case.\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "Burhan-Q",
        "body": "Related PR https://github.com/ultralytics/ultralytics/pull/8620"
      }
    ]
  },
  {
    "issue_number": 12983,
    "title": "How to chose what metrics use as goal for model.tune?",
    "author": "medphisiker",
    "state": "closed",
    "created_at": "2024-05-21T11:28:18Z",
    "updated_at": "2025-06-17T13:14:16Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nHello,\r\n\r\nI read new tutorial about model hyperparameters tuning ([link](https://docs.ultralytics.com/guides/hyperparameter-tuning/)).\r\nWhen using `optuna`, we can set a function that will measure some metric that we minimize or maximize.\r\n\r\nDocumentation also tell us \"Determine the metrics you will use to evaluate the model's performance. This could be AP50, F1-score, or others.\"\r\n\r\nHow can we set metric which we want to evaluate? )\r\nThere is a `fitness` metric as I understand in `https://github.com/ultralytics/ultralytics/blob/c8b6a8bc37ce196a6b7e0c153539402db4383c16/ultralytics/engine/tuner.py#L33`.\r\n\r\n```\r\n        if self.tune_csv.exists():  # if CSV file exists: select best hyps and mutate\r\n            # Select parent(s)\r\n            x = np.loadtxt(self.tune_csv, ndmin=2, delimiter=\",\", skiprows=1)\r\n            fitness = x[:, 0]  # first column\r\n            n = min(n, len(x))  # number of previous results to consider\r\n            x = x[np.argsort(-fitness)][:n]  # top n mutations\r\n            w = x[:, 0] - x[:, 0].min() + 1e-6  # weights (sum > 0)\r\n            if parent == \"single\" or len(x) == 1:\r\n                # x = x[random.randint(0, n - 1)]  # random selection\r\n                x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\r\n            elif parent == \"weighted\":\r\n                x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\r\n ```\r\n \r\n fitness = x[:, 0]  # first column\r\n \r\nAs I understand we get first column from model's results.csv (first metric) ?\r\nIf I understood everything correctly, it might be convenient to be able to specify the metric that we are optimizing as a parameter.\r\n\r\nAnd if we use code with data validation, then perhaps we would like to set the label obtained from validation as such a metric.\r\nI run this code:\r\n\r\n```\r\nfrom ultralytics import YOLO\r\n\r\n# Initialize the YOLO model\r\nmodel = YOLO(\"yolov8l-pose.pt\")\r\n\r\n# Tune hyperparameters on COCO8 for 30 epochs\r\nmodel.tune(\r\n    data=\"my_data.yaml\",\r\n    epochs=5,\r\n    iterations=5,\r\n    optimizer=\"AdamW\",\r\n    plots=False,\r\n    save=False,\r\n    val=True\r\n)\r\n```\r\n \r\nAnd I get this result\r\n```\r\nTuner: Best fitness=0.04088 observed at iteration 4\r\nTuner: Best fitness metrics are {'metrics/precision(B)': 0.02043, 'metrics/recall(B)': 0.2568, 'metrics/mAP50(B)': 0.01292, 'metrics/mAP50-95(B)': 0.00302, 'metrics/precision(P)': 0.06191, 'metrics/recall(P)': 0.59819, 'metrics/mAP50(P)': 0.05638, 'metrics/mAP50-95(P)': 0.0347, 'val/box_loss': nan, 'val/pose_loss': nan, 'val/kobj_loss': nan, 'val/cls_loss': nan, 'val/dfl_loss': nan, 'fitness': 0.04088}\r\n```\r\nAll validations metrics are set as `nan`.\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@medphisiker hello!\n\nIn the current implementation of the `model.tune()` method in Ultralytics YOLO, the fitness metric used for hyperparameter tuning is predefined and typically represents a composite score derived from several performance metrics (like precision, recall, mAP, etc.). This composite score is what's used to guide the optimization process.\n\nCurrently, there isn't a built-in option to specify a custom metric directly via the `model.tune()` function. The fitness metric is automatically calculated and used internally. If you're looking to optimize based on a specific metric from the validation results, you would need to modify the source code of the tuning process to adjust which metrics are considered for calculating fitness.\n\nFor handling `nan` values in your validation metrics, ensure that your dataset is correctly annotated and that the model is receiving the right inputs. `nan` often indicates issues with input data or configuration settings.\n\nIf customizing the fitness function is crucial for your application, you might consider contributing to the repository or creating a fork where you can implement this functionality. The community or the Ultralytics team could also find this feature beneficial.\n\nHope this helps! Let us know if you have further questions."
      },
      {
        "user": "medphisiker",
        "body": "Thank you Glenn, for your quick and detailed reply)\r\nNow I understand how it counts fitness metric)\r\nThanks, I'll experiment with the source code, if something good turns out, I'll send a Pull Request)\r\n\r\nAlso I wanted to ask about the comment in the 78th line of the file [link](https://github.com/ultralytics/ultralytics/blob/c8b6a8bc37ce196a6b7e0c153539402db4383c16/ultralytics/engine/tuner.py).\r\n\r\n```\r\n    def __init__(self, args=DEFAULT_CFG, _callbacks=None):\r\n        \"\"\"\r\n        Initialize the Tuner with configurations.\r\n\r\n        Args:\r\n            args (dict, optional): Configuration for hyperparameter evolution.\r\n        \"\"\"\r\n        self.space = args.pop(\"space\", None) or {  # key: (min, max, gain(optional))\r\n            # 'optimizer': tune.choice(['SGD', 'Adam', 'AdamW', 'NAdam', 'RAdam', 'RMSProp']),\r\n            \"lr0\": (1e-5, 1e-1),  # initial learning rate (i.e. SGD=1E-2, Adam=1E-3)\r\n            \"lrf\": (0.0001, 0.1),  # final OneCycleLR learning rate (lr0 * lrf)\r\n  ```\r\n  \r\n  There is interesting comment about \r\n```\r\n# 'optimizer': tune.choice(['SGD', 'Adam', 'AdamW', 'NAdam', 'RAdam', 'RMSProp'])\r\n```\r\n\r\nI wanted to clarify, can we set parameters to iterate over parameters using `ray tune`'s methods?\r\nFor example\r\n```\r\n pip install \"ray[tune]\"\r\n```\r\nAnd then run code like this:\r\n```\r\nfrom ultralytics import YOLO\r\nfrom ray import tune\r\n\r\n# Initialize the YOLO model\r\nmodel = YOLO(\"yolov8l-pose.pt\")\r\n\r\n# Tune hyperparameters on COCO8 for 30 epochs\r\nmodel.tune(\r\n    project=\"tuner_runs_test\",\r\n    data=\"data.yaml\",\r\n    epochs=30,\r\n    iterations=300,\r\n    optimizer=\"Adam\",\r\n    plots=False,\r\n    save=False,\r\n    val=True,\r\n    space={\"param1\": (1e-5, 1e-1), \r\n        \"param2\": tune.grid_search([0.001, 0.01, 0.1, 1.0]),\r\n        \"param3\": tune.choice([1, 2, 3]}\r\n)\r\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "Hello @medphisiker,\n\nAbsolutely! You can indeed use Ray Tune's methods to iterate over different hyperparameters, including using functions like `tune.grid_search` and `tune.choice` as you've shown in your example. This allows for a more dynamic and extensive exploration of the parameter space, which can be very beneficial for optimizing your model's performance.\n\nJust ensure that the `space` parameter in your `model.tune()` call correctly defines the search space using Ray Tune's search space definitions, as you've done. This should enable you to experiment with various configurations effectively.\n\nIf you have any more questions or need further assistance, feel free to ask. Good luck with your tuning experiments! 😊🚀"
      }
    ]
  },
  {
    "issue_number": 21088,
    "title": "Segmentation mask includes incorrect pixels at bounding box edge (mask spill over the bounding box border)",
    "author": "GuptaKishan",
    "state": "closed",
    "created_at": "2025-06-17T11:40:04Z",
    "updated_at": "2025-06-17T11:52:48Z",
    "labels": [
      "bug",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nPredict\n\n### Bug\n\nAfter fine-tuning yolo11x-seg.pt on my custom dataset (pallet detection), I noticed that the predicted segmentation masks always leak or include incorrect pixels at the edge where the bounding box touches the mask boundary. This results in noisy pixels outside the true object shape.\n\nExpected behavior:\nThe mask should follow the true contour of the object and not spill out where the bounding box touches the mask. This is critical in high-precision applications like volume estimation.\n\n### Environment\n\nUltralytics 8.3.153 🚀 Python-3.10.12 torch-2.5.1+cu124 CUDA:0 (NVIDIA RTX A2000 12GB, 11907MiB)\nSetup complete ✅ (32 CPUs, 62.5 GB RAM, 289.4/431.9 GB disk)\n\nOS                  Linux-5.15.0-135-generic-x86_64-with-glibc2.35\nEnvironment         Linux\nPython              3.10.12\nInstall             git\nPath                /home/doks/repos/volume_estimation/venv/lib/python3.10/site-packages/ultralytics\nRAM                 62.51 GB\nDisk                289.4/431.9 GB\nCPU                 13th Gen Intel Core(TM) i9-13900E\nCPU count           32\nGPU                 NVIDIA RTX A2000 12GB, 11907MiB\nGPU count           1\nCUDA                12.4\n\nnumpy               ✅ 1.26.4>=1.23.0\nmatplotlib          ✅ 3.9.2>=3.3.0\nopencv-python       ✅ 4.10.0.84>=4.6.0\npillow              ✅ 11.0.0>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.14.1>=1.4.1\ntorch               ✅ 2.5.1>=1.8.0\ntorch               ✅ 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.20.1>=0.9.0\ntqdm                ✅ 4.67.0>=4.64.0\npsutil              ✅ 6.1.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.2.3>=1.1.4\nultralytics-thop    ✅ 2.0.14>=2.0.0\n\n### Minimal Reproducible Example\n\nyolo segment train \\\n  model=yolo11x-seg.pt \\\n  data=data.yaml \\\n  epochs=200 \\\n  batch=16 \\\n  imgsz=1024 \\\n  optimizer=SGD \\\n  patience=20 \\\n  val=True\n\n\nmodel = YOLO(\"fine_tuned_model_weights_path\") \nresults = model(image_path)\nmask_image_with_bb = results[0].plot(boxes = True)\nmask_image_without_bb = results[0].plot(boxes = False)\n\nmasks = results[0].masks.data.cpu().numpy() \ncombined_mask = np.any(masks, axis=0).astype(np.uint8)\nimage_size = (1224, 1024)\nresized_mask = cv2.resize(combined_mask, image_size, interpolation=cv2.INTER_NEAREST)\nbinary_mask_image_without_bb = resized_mask * 255\n\n### Additional\n\n![Image](https://github.com/user-attachments/assets/f6e1af92-a2cb-4f34-95d4-e1c4beadb359)\n\n![Image](https://github.com/user-attachments/assets/90429a60-d201-47fb-8988-b1b4083874d3)\n\n![Image](https://github.com/user-attachments/assets/64291a68-de39-419f-87e4-2e55fe12f634)\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @GuptaKishan, thank you for submitting your detailed report and for using Ultralytics 🚀! This is an automated response to help streamline your issue, and an Ultralytics engineer will review and assist you soon.\n\nWe recommend checking out the [Docs](https://docs.ultralytics.com/) for helpful guidance on [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, as well as our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nSince this is a 🐛 Bug Report:  \nThank you for providing a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) and clear visualizations—this is very helpful for debugging!\n\nJoin the Ultralytics community to connect and share insights:\n- For real-time chat, join our [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth questions, visit our [Discourse](https://community.ultralytics.com/)\n- For knowledge sharing and discussions, check out our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease ensure you are using the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) installed in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/), as this can resolve many issues:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date verified environments, with all dependencies (including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/)) preinstalled:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. The CI system verifies correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your report and patience while we investigate!"
      },
      {
        "user": "Y-T-G",
        "body": "You can use `retina_masks=True` in `model.predict()`"
      },
      {
        "user": "GuptaKishan",
        "body": "Thank you. It worked."
      }
    ]
  },
  {
    "issue_number": 21084,
    "title": "TypeError: plot_images() got an unexpected keyword argument 'batch_idx'",
    "author": "monkeycc",
    "state": "open",
    "created_at": "2025-06-17T09:35:56Z",
    "updated_at": "2025-06-17T11:37:47Z",
    "labels": [
      "question",
      "dependencies",
      "fixed"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nultralytics                    8.3.155\n` imgsz=640 , epochs=200, batch = 0.80`\n\n```python\n\nImage sizes 640 train, 640 val \nUsing 8 dataloader workers\nLogging results to MX/yolo\nStarting training for 200 epochs...\n\n      Epoch    GPU_mem       loss  Instances       Size\n      1/200      72.5G      9.333        409        640:   0%|          | 0/34 [00:20<?, ?it/s]Exception in thread Thread-27 (plot_images):\nTraceback (most recent call last):\n      1/200      72.5G      9.333        409        640:   3%|▒         | 1/34 [00:20<11:09, 20.28s/it]  File \"/root/miniconda3/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n    self.run()\n  File \"/root/miniconda3/lib/python3.12/threading.py\", line 1010, in run\n    self._target(*self._args, **self._kwargs)\nTypeError: plot_images() got an unexpected keyword argument 'batch_idx'\n      1/200      72.5G      9.576        409        640:   3%|▒         | 1/34 [00:27<11:09, 20.28s/it]Exception in thread Thread-28 (plot_images):\n      1/200      72.5G      9.576        409        640:   6%|▒         | 2/34 [00:27<06:37, 12.44s/it]Traceback (most recent call last):\n  File \"/root/miniconda3/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n    self.run()\n  File \"/root/miniconda3/lib/python3.12/threading.py\", line 1010, in run\n    self._target(*self._args, **self._kwargs)\nTypeError: plot_images() got an unexpected keyword argument 'batch_idx'\n      1/200      72.5G      9.653        409        640:   6%|▒         | 2/34 [00:30<06:37, 12.44s/it]Exception in thread Thread-29 (plot_images):\nTraceback (most recent call last):\n  File \"/root/miniconda3/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n      1/200      72.5G      9.653        409        640:   9%|▒         | 3/34 [00:30<04:09,  8.05s/it]    self.run()\n  File \"/root/miniconda3/lib/python3.12/threading.py\", line 1010, in run\n      1/200      72.5G      9.525        409        640:  21%|▒▒        | 7/34 [00:35<01:04,  2.38s/it]\n      1/200      72.5G      9.554        409        640:  15%|▒▒        | 5/34 [00:34<02:08,  4.42s/it]\n      1/200      72.5G      9.631        409        640:  15%|▒▒        | 4/34 [00:34<02:43,  5.46s/it]\n```\n\n\n```python\n\nTypeError: plot_images() got an unexpected keyword argument 'batch_idx'\n      1/200      72.5G      9.653        409        640:   6%|▒         | 2/34 [00:30<06:37, 12.44s/it]Exception in thread Thread-29 (plot_images):\nTraceback (most recent call last):\n      1/200      72.5G      5.941        120        640: 100%|▒▒▒▒▒▒▒▒▒▒| 34/34 [01:36<00:00,  2.84s/it]\n               classes   top1_acc   top5_acc: 100%|▒▒▒▒▒▒▒▒▒▒| 5/5 [00:02<00:00,  2.29it/s]3,  1.71s/it]   self.run()\n                   all      0.335      0.688        640:  94%|▒▒▒▒▒▒▒▒▒▒| 31/34 [01:34<00:05,  1.69s/it]\n      2/200      64.8G      1.856        409        640:  18%|▒▒        | 6/34 [00:06<00:38,  1.38s/it]Killed\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp# Image sizes 640 train, 640 val00:30,  1.06s/it]]\nbash: Image: command not found902        409        640:  15%|▒▒        | 4/34 [00:04<00:20,  1.47it/s]]\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp# Using 8 dataloader workers1:02<00:59,  2.96s/it]\nbash: Using: command not found\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp# Logging results to MX/yolo\nbash: Logging: command not found\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp# Starting training for 200 epochs...\nbash: Starting: command not found\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#       Epoch    GPU_mem       loss  Instances       Size\nbash: Epoch: command not found\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#       1/200      72.5G      9.333        409        640:   0%|          | 0/34 [00:20<?, ?it/s]Exception in thread Thread-27 (plot_images):\nbash: syntax error near unexpected token `|'\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp# Traceback (most recent call last):\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#       1/200      72.5G      9.333        409        640:   3%|▒         | 1/34 [00:20<11:09, 20.28s/it]  File \"/root/miniconda3/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\nbash: $'\\342\\226\\222': command not found8:~/autodl-tmp#       1/200      72.5G      9.333        409        640:   3%|▒         | 1/34 [00:20<11:09, 20.28s/it]  File \"/root/miniconda3/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\nbash: 1/200: No such file or directory\nbash: 11:09,: No such file or directory\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#     self.run()\n>   File \"/root/miniconda3/lib/python3.12/threading.py\", line 1010, in run\nbash: syntax error near unexpected token `File'\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#     self._target(*self._args, **self._kwargs)\nbash: syntax error near unexpected token `*self._args,'\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp# TypeError: plot_images() got an unexpected keyword argument 'batch_idx'\nbash: syntax error near unexpected token `('\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#       1/200      72.5G      9.576        409        640:   3%|▒         | 1/34 [00:27<11:09, 20.28s/it]Exception in thread Thread-28 (plot_images):\nbash: syntax error near unexpected token `('\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#       1/200      72.5G      9.576        409        640:   6%|▒         | 2/34 [00:27<06:37, 12.44s/it]Traceback (most recent call last):\nbash: syntax error near unexpected token `('\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#   File \"/root/miniconda3/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\nbash: File: command not found\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#     self.run()\n>   File \"/root/miniconda3/lib/python3.12/threading.py\", line 1010, in run\nbash: syntax error near unexpected token `File'\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#     self._target(*self._args, **self._kwargs)\nbash: syntax error near unexpected token `*self._args,'\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp# TypeError: plot_images() got an unexpected keyword argument 'batch_idx'\nbash: syntax error near unexpected token `('\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#       1/200      72.5G      9.653        409        640:   6%|▒         | 2/34 [00:30<06:37, 12.44s/it]Exception in thread Thread-29 (plot_images):\nbash: syntax error near unexpected token `('\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp# Traceback (most recent call last):\nbash: syntax error near unexpected token `most'\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#   File \"/root/miniconda3/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\nbash: File: command not found\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#       1/200      72.5G      9.653        409        640:   9%|▒         | 3/34 [00:30<04:09,  8.05s/it]    self.run()\nbash: syntax error near unexpected token `('\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#   File \"/root/miniconda3/lib/python3.12/threading.py\", line 1010, in run\nbash: File: command not found\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#       1/200      72.5G      9.525        409        640:  21%|▒▒        | 7/34 [00:35<01:04,  2.38s/it]\nbash: 01:04,: No such file or directory\nbash: $'\\342\\226\\222\\342\\226\\222': command not found\nbash: 1/200: No such file or directory\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#       1/200      72.5G      9.554        409        640:  15%|▒▒        | 5/34 [00:34<02:08,  4.42s/it]\nbash: $'\\342\\226\\222\\342\\226\\222': command not found\nbash: 02:08,: No such file or directory\nbash: 1/200: No such file or directory\nroot@autodl-container-8bf9409db0-ae75ea78:~/autodl-tmp#       1/200      72.5G      9.631        409        640:  15%|▒▒        | 4/34 [00:34<02:43,  5.46s/it]\nbash: $'\\342\\226\\222\\342\\226\\222': command not found\nbash: 02:43,: No such file or directory\nbash: 1/200: No such file or directory\n\n```\n\n\n```\npip install ultralytics==8.3.65\nWill not prompt these errors\nBut it won't generate images\n```\n\n\n```\n\nPackage                        Version\n------------------------------ --------------\nabsl-py                        2.1.0\nalbucore                       0.0.24\nalbumentations                 2.0.8\nanaconda-anon-usage            0.4.4\nannotated-types                0.7.0\nanyio                          4.6.2.post1\narchspec                       0.2.3\nargon2-cffi                    23.1.0\nargon2-cffi-bindings           21.2.0\narrow                          1.3.0\nasttokens                      2.4.1\nasync-lru                      2.0.4\nattrs                          24.2.0\nbabel                          2.16.0\nbeautifulsoup4                 4.12.3\nbleach                         6.2.0\nboltons                        23.0.0\nBrotli                         1.0.9\ncertifi                        2024.2.2\ncffi                           1.16.0\ncharset-normalizer             2.0.4\ncomm                           0.2.2\nconda                          24.4.0\nconda-content-trust            0.2.0\nconda-libmamba-solver          24.1.0\nconda-package-handling         2.2.0\nconda_package_streaming        0.9.0\ncontourpy                      1.3.1\ncryptography                   42.0.5\ncycler                         0.12.1\ndebugpy                        1.8.9\ndecorator                      5.1.1\ndefusedxml                     0.7.1\ndistro                         1.9.0\nexecuting                      2.1.0\nfastjsonschema                 2.20.0\nfilelock                       3.16.1\nfonttools                      4.55.0\nfqdn                           1.5.1\nfsspec                         2024.10.0\ngrpcio                         1.68.0\nh11                            0.14.0\nhttpcore                       1.0.7\nhttpx                          0.27.2\nidna                           3.7\nipykernel                      6.29.5\nipython                        8.29.0\nipywidgets                     8.1.5\nisoduration                    20.11.0\njedi                           0.19.2\nJinja2                         3.1.4\njson5                          0.9.28\njsonpatch                      1.33\njsonpointer                    2.1\njsonschema                     4.23.0\njsonschema-specifications      2024.10.1\njupyter_client                 8.6.3\njupyter_core                   5.7.2\njupyter-events                 0.10.0\njupyter-lsp                    2.2.5\njupyter_server                 2.14.2\njupyter_server_terminals       0.5.3\njupyterlab                     4.3.1\njupyterlab-language-pack-zh-CN 4.2.post3\njupyterlab_pygments            0.3.0\njupyterlab_server              2.27.3\njupyterlab_widgets             3.0.13\nkiwisolver                     1.4.7\nlibmambapy                     1.5.8\nMarkdown                       3.7\nMarkupSafe                     3.0.2\nmatplotlib                     3.9.2\nmatplotlib-inline              0.1.7\nmenuinst                       2.0.2\nmistune                        3.0.2\nmpmath                         1.3.0\nnbclient                       0.10.0\nnbconvert                      7.16.4\nnbformat                       5.10.4\nnest-asyncio                   1.6.0\nnetworkx                       3.4.2\nnotebook_shim                  0.2.4\nnumpy                          2.1.3\nnvidia-cublas-cu12             12.4.5.8\nnvidia-cuda-cupti-cu12         12.4.127\nnvidia-cuda-nvrtc-cu12         12.4.127\nnvidia-cuda-runtime-cu12       12.4.127\nnvidia-cudnn-cu12              9.1.0.70\nnvidia-cufft-cu12              11.2.1.3\nnvidia-curand-cu12             10.3.5.147\nnvidia-cusolver-cu12           11.6.1.9\nnvidia-cusparse-cu12           12.3.1.170\nnvidia-nccl-cu12               2.21.5\nnvidia-nvjitlink-cu12          12.4.127\nnvidia-nvtx-cu12               12.4.127\nopencv-python                  4.11.0.86\nopencv-python-headless         4.11.0.86\noverrides                      7.7.0\npackaging                      23.2\npandas                         2.3.0\npandocfilters                  1.5.1\nparso                          0.8.4\npexpect                        4.9.0\npillow                         11.0.0\npip                            24.0\nplatformdirs                   3.10.0\npluggy                         1.0.0\nprometheus_client              0.21.0\nprompt_toolkit                 3.0.48\nprotobuf                       5.28.3\npsutil                         6.1.0\nptyprocess                     0.7.0\npure_eval                      0.2.3\npy-cpuinfo                     9.0.0\npycosat                        0.6.6\npycparser                      2.21\npydantic                       2.11.7\npydantic_core                  2.33.2\nPygments                       2.18.0\npyparsing                      3.2.0\nPySocks                        1.7.1\npython-dateutil                2.9.0.post0\npython-json-logger             2.0.7\npytz                           2025.2\nPyYAML                         6.0.2\npyzmq                          26.2.0\nreferencing                    0.35.1\nrequests                       2.31.0\nrfc3339-validator              0.1.4\nrfc3986-validator              0.1.1\nrpds-py                        0.21.0\nruamel.yaml                    0.17.21\nscipy                          1.15.3\nSend2Trash                     1.8.3\nsetuptools                     69.5.1\nsimsimd                        6.4.9\nsix                            1.16.0\nsniffio                        1.3.1\nsoupsieve                      2.6\nstack-data                     0.6.3\nstringzilla                    3.12.5\nsupervisor                     4.2.5\nsympy                          1.13.1\ntensorboard                    2.18.0\ntensorboard-data-server        0.7.2\nterminado                      0.18.1\ntinycss2                       1.4.0\ntorch                          2.5.1+cu124\ntorchvision                    0.20.1+cu124\ntornado                        6.4.2\ntqdm                           4.66.2\ntraitlets                      5.14.3\ntriton                         3.1.0\ntruststore                     0.8.0\ntypes-python-dateutil          2.9.0.20241003\ntyping_extensions              4.12.2\ntyping-inspection              0.4.1\ntzdata                         2025.2\nultralytics                    8.3.155\nultralytics-thop               2.0.14\nuri-template                   1.3.0\nurllib3                        2.1.0\nwcwidth                        0.2.13\nwebcolors                      24.11.1\nwebencodings                   0.5.1\nwebsocket-client               1.8.0\nWerkzeug                       3.1.3\nwheel                          0.43.0\nwidgetsnbextension             4.0.13\nzstandard                      0.22.0\n\n```\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @monkeycc, thank you for reporting this issue and for providing detailed logs and your package list! 🐛 Our team appreciates your effort to help improve Ultralytics.\n\nThis is an automated response to help you get the fastest possible support. An Ultralytics engineer will review your report and assist you soon.\n\nIn the meantime:\n\n- If this is a 🐞 Bug Report (such as the `TypeError: plot_images() got an unexpected keyword argument 'batch_idx'` you're seeing), please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) if you haven't already. This helps us identify and resolve the problem as quickly as possible.\n\n- If your issue is related to custom training or usage, please include more details such as your exact command, relevant code, dataset image samples, and training logs. Also, review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nWe recommend visiting the [Docs](https://docs.ultralytics.com/) for many helpful [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, as many common questions are already addressed there.\n\n## Upgrade\n\nPlease upgrade to the latest `ultralytics` package (and [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml)) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to ensure your issue is not already resolved:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can run YOLO in any of these verified environments (all dependencies preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nFor real-time help, join our [Discord](https://discord.com/invite/ultralytics) 🎧, engage in deeper conversations on [Discourse](https://community.ultralytics.com/), or connect with the community on our [Subreddit](https://reddit.com/r/Ultralytics).\n\nThank you for your patience and for helping us improve Ultralytics! 🚀"
      }
    ]
  },
  {
    "issue_number": 21080,
    "title": "New bug introduced in 8.3.154 | RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 200 but got size 800 for tensor number 6 in the list.",
    "author": "brunoaduarte",
    "state": "closed",
    "created_at": "2025-06-17T06:56:45Z",
    "updated_at": "2025-06-17T11:30:38Z",
    "labels": [
      "bug",
      "fixed",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nTrain\n\n### Bug\n\n@Laughing-q @glenn-jocher I get this error after updating to `Ultralytics 8.3.154` (and also `8.3.155`) when training an `instance segmentation` dataset with `yolo11m-seg.pt` model.\n```\nRuntimeError: Sizes of tensors must match except in dimension 0. Expected size 200 but got size 800 for tensor number 6 in the list.\n```\n\nError does not occur in `8.3.153`\n\n```\nyolo train model=yolo11m-seg.pt data=~/seg-dataset/data.yaml epochs=5 imgsz=1376 plots=True batch=8 device=0 verbose=False\n\nNew https://pypi.org/project/ultralytics/8.3.155 available 😃 Update with 'pip install -U ultralytics'\nUltralytics 8.3.153 🚀 Python-3.12.3 torch-2.8.0.dev20250616+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32108MiB)\nengine/trainer: agnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/root/seg-dataset/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1376, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11m-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/segment/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments\n  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]\n  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]\n  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]\n  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]\n  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]\n  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]\n  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]\n  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]\n  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]\n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]\n 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]\n 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']\n 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]\n 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']\n 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]\n 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]\n 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]\n 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]\n 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]\n 23        [16, 19, 22]  1   3718003  ultralytics.nn.modules.head.Segment          [1, 32, 256, [256, 512, 512]]\nYOLO11m-seg summary: 253 layers, 22,359,987 parameters, 22,359,971 gradients, 123.6 GFLOPs\n\nTransferred 705/711 items from pretrained weights\nFreezing layer 'model.23.dfl.conv.weight'\nAMP: running Automatic Mixed Precision (AMP) checks...\nAMP: checks passed ✅\ntrain: Fast image access ✅ (ping: 0.0±0.0 ms, read: 2347.8±577.1 MB/s, size: 177.9 KB)\ntrain: Scanning /root/seg-dataset/train/labels.cache... 1656 images, 286 backgrounds, 0 corrupt: 100%|██████████| 1656/1656 [00:00<?, ?it/s]\nval: Fast image access ✅ (ping: 0.0±0.0 ms, read: 1292.6±663.8 MB/s, size: 177.4 KB)\nval: Scanning /root/seg-dataset/valid/labels.cache... 199 images, 59 backgrounds, 0 corrupt: 100%|██████████| 199/199 [00:00<?, ?it/s]\nPlotting labels to runs/segment/train/labels.jpg...\noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically...\noptimizer: AdamW(lr=0.002, momentum=0.9) with parameter groups 115 weight(decay=0.0), 126 weight(decay=0.0005), 125 bias(decay=0.0)\nImage sizes 1376 train, 1376 val\nUsing 8 dataloader workers\nLogging results to runs/segment/train\nStarting training for 5 epochs...\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n        1/5        22G     0.8544      1.311      1.607     0.9595         21       1376: 100%|██████████| 207/207 [01:09<00:00,  2.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:06<00:00,  2.02it/s]\n                   all        199        225    0.00349      0.902    0.00335    0.00261    0.00349      0.902    0.00335    0.00267\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n        2/5      22.5G     0.8771      1.253     0.9286     0.9734         15       1376: 100%|██████████| 207/207 [00:58<00:00,  3.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  6.34it/s]\n                   all        199        225      0.835      0.766      0.822      0.665      0.835      0.766      0.823      0.654\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n        3/5      22.3G     0.8533      1.173     0.8824     0.9646         17       1376: 100%|██████████| 207/207 [00:58<00:00,  3.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  6.35it/s]\n                   all        199        225        0.8      0.869      0.896      0.745        0.8      0.869      0.896      0.749\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n        4/5      22.3G     0.7622      1.083     0.7179     0.9259         33       1376: 100%|██████████| 207/207 [01:06<00:00,  3.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  6.37it/s]\n                   all        199        225      0.873      0.888      0.945      0.835      0.871      0.896      0.949      0.788\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n        5/5      22.3G     0.6513     0.9241     0.5919     0.8934         35       1376: 100%|██████████| 207/207 [01:04<00:00,  3.22it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:04<00:00,  3.16it/s]\n                   all        199        225      0.927      0.843      0.956      0.863      0.927      0.843      0.956      0.826\n\n5 epochs completed in 0.095 hours.\nOptimizer stripped from runs/segment/train/weights/last.pt, 45.2MB\nOptimizer stripped from runs/segment/train/weights/best.pt, 45.2MB\n\nValidating runs/segment/train/weights/best.pt...\nUltralytics 8.3.153 🚀 Python-3.12.3 torch-2.8.0.dev20250616+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32108MiB)\nYOLO11m-seg summary (fused): 138 layers, 22,336,083 parameters, 0 gradients, 123.0 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  4.98it/s]\n                   all        199        225      0.927      0.843      0.956      0.863      0.927      0.843      0.956      0.826\nSpeed: 0.3ms preprocess, 4.4ms inference, 0.0ms loss, 2.2ms postprocess per image\nResults saved to runs/segment/train\n💡 Learn more at https://docs.ultralytics.com/modes/train\n```\n\nAfter updating to `8.3.155`\n```\nyolo train model=yolo11m-seg.pt data=~/seg-dataset/data.yaml epochs=5 imgsz=1376 plots=True batch=8 device=0 verbose=False\n\nUltralytics 8.3.155 🚀 Python-3.12.3 torch-2.8.0.dev20250616+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32108MiB)\nengine/trainer: agnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/root/seg-dataset/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1376, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11m-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/segment/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments\n  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]\n  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]\n  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]\n  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]\n  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]\n  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]\n  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]\n  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]\n  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]\n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]\n 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]\n 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']\n 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]\n 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']\n 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]\n 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]\n 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]\n 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]\n 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]\n 23        [16, 19, 22]  1   3718003  ultralytics.nn.modules.head.Segment          [1, 32, 256, [256, 512, 512]]\nYOLO11m-seg summary: 253 layers, 22,359,987 parameters, 22,359,971 gradients, 123.6 GFLOPs\n\nTransferred 705/711 items from pretrained weights\nFreezing layer 'model.23.dfl.conv.weight'\nAMP: running Automatic Mixed Precision (AMP) checks...\nAMP: checks passed ✅\ntrain: Fast image access ✅ (ping: 0.0±0.0 ms, read: 2838.1±819.4 MB/s, size: 177.9 KB)\ntrain: Scanning /root/seg-dataset/train/labels.cache... 1656 images, 286 backgrounds, 0 corrupt: 100%|██████████| 1656/1656 [00:00<?, ?it/s]\nval: Fast image access ✅ (ping: 0.0±0.0 ms, read: 1407.8±709.1 MB/s, size: 177.4 KB)\nval: Scanning /root/seg-dataset/valid/labels.cache... 199 images, 59 backgrounds, 0 corrupt: 100%|██████████| 199/199 [00:00<?, ?it/s]\nPlotting labels to runs/segment/train/labels.jpg...\noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically...\noptimizer: AdamW(lr=0.002, momentum=0.9) with parameter groups 115 weight(decay=0.0), 126 weight(decay=0.0005), 125 bias(decay=0.0)\nImage sizes 1376 train, 1376 val\nUsing 8 dataloader workers\nLogging results to runs/segment/train\nStarting training for 5 epochs...\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n        1/5        22G     0.8544      1.311      1.607     0.9595         21       1376: 100%|██████████| 207/207 [01:10<00:00,  2.93it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:06<00:00,  2.01it/s]\n                   all        199        225    0.00349      0.902    0.00335    0.00261    0.00349      0.902    0.00335    0.00267\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n        2/5      22.4G     0.8771      1.253     0.9286     0.9734         15       1376: 100%|██████████| 207/207 [00:59<00:00,  3.48it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  6.31it/s]\n                   all        199        225      0.835      0.766      0.822      0.665      0.835      0.766      0.823      0.654\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n        3/5      22.3G     0.8533      1.173     0.8824     0.9646         17       1376: 100%|██████████| 207/207 [00:58<00:00,  3.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  6.31it/s]\n                   all        199        225        0.8      0.869      0.896      0.745        0.8      0.869      0.896      0.749\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n        4/5      22.3G     0.7622      1.083     0.7179     0.9259         33       1376: 100%|██████████| 207/207 [00:59<00:00,  3.48it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  6.19it/s]\n                   all        199        225      0.873      0.888      0.945      0.835      0.871      0.896      0.949      0.788\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n        5/5      22.3G     0.6513     0.9241     0.5919     0.8934         35       1376: 100%|██████████| 207/207 [01:08<00:00,  3.04it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  5.20it/s]\n                   all        199        225      0.927      0.843      0.956      0.863      0.927      0.843      0.956      0.826\n\n5 epochs completed in 0.094 hours.\nOptimizer stripped from runs/segment/train/weights/last.pt, 45.2MB\nOptimizer stripped from runs/segment/train/weights/best.pt, 45.2MB\n\nValidating runs/segment/train/weights/best.pt...\nUltralytics 8.3.155 🚀 Python-3.12.3 torch-2.8.0.dev20250616+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32108MiB)\nYOLO11m-seg summary (fused): 138 layers, 22,336,083 parameters, 0 gradients, 123.0 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/13 [00:00<?, ?it/s]\nTraceback (most recent call last):\n  File \"/root/.venv/bin/yolo\", line 8, in <module>\n    sys.exit(entrypoint())\n             ^^^^^^^^^^^^\n  File \"/root/.venv/lib/python3.12/site-packages/ultralytics/cfg/__init__.py\", line 985, in entrypoint\n    getattr(model, mode)(**overrides)  # default args from model\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py\", line 797, in train\n    self.trainer.train()\n  File \"/root/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py\", line 227, in train\n    self._do_train(world_size)\n  File \"/root/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py\", line 497, in _do_train\n    self.final_eval()\n  File \"/root/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py\", line 742, in final_eval\n    self.metrics = self.validator(model=f)\n                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.venv/lib/python3.12/site-packages/ultralytics/engine/validator.py\", line 224, in __call__\n    self.plot_predictions(batch, preds, batch_i)\n  File \"/root/.venv/lib/python3.12/site-packages/ultralytics/models/yolo/segment/val.py\", line 210, in plot_predictions\n    super().plot_predictions(batch, preds, ni, max_det=50)  # plot bboxes\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/.venv/lib/python3.12/site-packages/ultralytics/models/yolo/detect/val.py\", line 333, in plot_predictions\n    batched_preds = {k: torch.cat([x[k][:max_det] for x in preds], dim=0) for k in keys}\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Sizes of tensors must match except in dimension 0. Expected size 200 but got size 800 for tensor number 1 in the list.\nterminate called without an active exception\nAborted (core dumped)\n```\n\n### Environment\n\n```\nyolo checks\n\nUltralytics 8.3.155 🚀 Python-3.12.3 torch-2.8.0.dev20250616+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32108MiB)\nSetup complete ✅ (56 CPUs, 125.7 GB RAM, 120.1/418.9 GB disk)\n\nOS                  Linux-6.6.0-hiveos-x86_64-with-glibc2.39\nEnvironment         Docker\nPython              3.12.3\nInstall             pip\nPath                /root/.venv/lib/python3.12/site-packages/ultralytics\nRAM                 125.70 GB\nDisk                120.1/418.9 GB\nCPU                 Intel Xeon E5-2690 v4 2.60GHz\nCPU count           56\nGPU                 NVIDIA GeForce RTX 5090, 32108MiB\nGPU count           1\nCUDA                12.8\n\nnumpy               ✅ 2.1.2>=1.23.0\nmatplotlib          ✅ 3.10.3>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 11.0.0>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.32.4>=2.23.0\nscipy               ✅ 1.15.3>=1.4.1\ntorch               ✅ 2.8.0.dev20250616+cu128>=1.8.0\ntorch               ✅ 2.8.0.dev20250616+cu128!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.23.0.dev20250616+cu128>=0.9.0\ntqdm                ✅ 4.67.1>=4.64.0\npsutil              ✅ 7.0.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.3.0>=1.1.4\nultralytics-thop    ✅ 2.0.14>=2.0.0\n```\n\n### Minimal Reproducible Example\n\n```\nyolo train model=yolo11m-seg.pt data=~/dataset/data.yaml epochs=5 imgsz=1376 plots=True batch=8 device=0 verbose=False\n```\n\n### Additional\n\nhttps://github.com/ultralytics/ultralytics/blame/main/ultralytics/models/yolo/detect/val.py#L333\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @brunoaduarte, thank you for reporting this issue and for your detailed logs and context! 🚀 This is an automated response to help streamline troubleshooting until an Ultralytics engineer can assist you further.\n\nWe recommend reviewing the [Docs](https://docs.ultralytics.com/) for the latest updates, as well as [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples. For similar issues, you might also want to check open and closed [issues](https://github.com/ultralytics/ultralytics/issues).\n\nSince this is a 🐛 Bug Report, please ensure you can provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/) if you haven’t already. This helps us reproduce and debug the issue as quickly as possible.\n\nJoin the Ultralytics community for more support and discussion:\n- Real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth topics on [Discourse](https://community.ultralytics.com/)\n- Community threads on our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nBefore diving deeper, please ensure you’re on the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) are satisfied in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following up-to-date environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) daily and on every commit across macOS, Windows, and Ubuntu.\n\nThank you for your patience! An Ultralytics engineer will review and follow up with you soon."
      },
      {
        "user": "Laughing-q",
        "body": "@brunoaduarte thanks for reporting! Yes we recently refactored the validator part in the codebase, which might be the cause of this. I'm going to check on this."
      },
      {
        "user": "Laughing-q",
        "body": "@brunoaduarte I'm not able to reproduce it using our internal test segment dataset `coco8-seg.yaml`.\n```bash\nyolo segment train model=yolo11m-seg.pt epochs=3 imgsz=1376 batch=4 plots=True\n```\nI added some debug code in `debug-1` branch which you can install the package by `pip install git+https://github.com/ultralytics/ultralytics.git@debug-1`. Can you try installing this and run the training again and post the log here? Thank you!"
      }
    ]
  },
  {
    "issue_number": 20984,
    "title": "`engine` export with int8 is always forcing dynamic input.",
    "author": "testdummyvt",
    "state": "closed",
    "created_at": "2025-06-07T21:28:36Z",
    "updated_at": "2025-06-17T11:08:16Z",
    "labels": [
      "bug",
      "fixed",
      "embedded",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nExport\n\n### Bug\n\nWhen I try to export to `engine` with `int8=True`, the input is always set to dynamic. \n\nexport code:\n```python\nmodel.export(format=\"engine\", imgsz = 320, data = \"/root/datasets/yolo_format/data.yaml\", int8=True, device=\"dla:0\", simplify = True, dynamic = False)  # dla:0 or dla:1 corresponds to the DLA cores\n``` \n`dynamic=False` is required for dla on jetson devices. Otherwise, all the layers will be moved to GPU instead of DLA. \n\n\n`[06/07/2025-21:23:28] [TRT] [W] DLA requests all profiles have same min, max, and opt value. All dla layers are falling back to GPU`\n\n```bash\nUltralytics 8.3.151 🚀 Python-3.10.12 torch-2.5.0a0+872d972e41.nv24.08 CUDA:0 (Orin, 62841MiB)\nYOLO11n summary (fused): 100 layers, 2,588,782 parameters, 0 gradients, 6.3 GFLOPs\n\nPyTorch: starting from '/root/narsi/models/yolo11n_hagridv2.pt' with input shape (1, 3, 320, 320) BCHW and output shape(s) (1, 300, 6) (15.3 MB)\n\nONNX: starting export with onnx 1.17.0 opset 19...\nONNX: slimming with onnxslim 0.1.56...\nONNX: export success ✅ 6.8s, saved as '/root/narsi/models/yolo11n_hagridv2.onnx' (10.0 MB)\n\nTensorRT: starting export with TensorRT 10.3.0...\nTensorRT: collecting INT8 calibration images from 'data=/root/narsi/datasets/yolo_format/data.yaml'\nFast image access ✅ (ping: 0.0±0.0 ms, read: 1675.1±475.8 MB/s, size: 111.7 KB)\nScanning /root/narsi/datasets/yolo_format/labels/val.cache... 1700 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1700/1700 [00:00<?, ?it/s]\n[06/07/2025-21:23:25] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 707, GPU 20401 (MiB)\n[06/07/2025-21:23:28] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +927, GPU +1091, now: CPU 1677, GPU 21509 (MiB)\nTensorRT: enabling DLA on core 0...\n[06/07/2025-21:23:28] [TRT] [I] ----------------------------------------------------------------\n[06/07/2025-21:23:28] [TRT] [I] Input filename:   /root/narsi/models/yolo11n_hagridv2.onnx\n[06/07/2025-21:23:28] [TRT] [I] ONNX IR version:  0.0.9\n[06/07/2025-21:23:28] [TRT] [I] Opset version:    19\n[06/07/2025-21:23:28] [TRT] [I] Producer name:    pytorch\n[06/07/2025-21:23:28] [TRT] [I] Producer version: 2.5.0\n[06/07/2025-21:23:28] [TRT] [I] Domain:           \n[06/07/2025-21:23:28] [TRT] [I] Model version:    0\n[06/07/2025-21:23:28] [TRT] [I] Doc string:       \n[06/07/2025-21:23:28] [TRT] [I] ----------------------------------------------------------------\nTensorRT: input \"images\" with shape(-1, 3, -1, -1) DataType.FLOAT\nTensorRT: output \"output0\" with shape(-1, 300, 6) DataType.FLOAT\nWARNING ⚠️ TensorRT: 'dynamic=True' model requires max batch size, i.e. 'batch=16'\nTensorRT: building INT8 engine as /root/narsi/models/yolo11n_hagridv2.engine\n[06/07/2025-21:23:28] [TRT] [W] DLA requests all profiles have same min, max, and opt value. All dla layers are falling back to GPU\n[06/07/2025-21:23:28] [TRT] [W] Layer '/model/model.0/conv/Conv' (CONVOLUTION): Unsupported on DLA. Switching this layer's device type to GPU.\n[06/07/2025-21:23:28] [TRT] [W] Layer '/model/model.0/act/Sigmoid' (ACTIVATION): Unsupported on DLA. Switching this layer's device type to GPU.\n[06/07/2025-21:23:28] [TRT] [W] Layer '/model/model.0/act/Mul' (ELEMENTWISE): Unsupported on DLA. Switching this layer's device type to GPU.\n[06/07/2025-21:23:28] [TRT] [W] Layer '/model/model.1/conv/Conv' (CONVOLUTION): Unsupported on DLA. Switching this layer's device type to GPU.\n\n```\n\n### Environment\n\n```bash\nUltralytics 8.3.151 🚀 Python-3.10.12 torch-2.5.0a0+872d972e41.nv24.08 CUDA:0 (Orin, 62841MiB)\nSetup complete ✅ (12 CPUs, 61.4 GB RAM, 363.1/1831.3 GB disk)\n\nOS                  Linux-5.15.148-tegra-aarch64-with-glibc2.35\nEnvironment         Docker\nPython              3.10.12\nInstall             git\nPath                /ultralytics/ultralytics\nRAM                 61.37 GB\nDisk                363.1/1831.3 GB\nCPU                 ARMv8 Processor rev 1 (v8l)\nCPU count           12\nGPU                 Orin, 62841MiB\nGPU count           1\nCUDA                12.6\n\nnumpy               ✅ 1.26.4>=1.23.0\nmatplotlib          ✅ 3.10.3>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 11.2.1>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.15.3>=1.4.1\ntorch               ✅ 2.5.0a0+872d972e41.nv24.8>=1.8.0\ntorch               ✅ 2.5.0a0+872d972e41.nv24.8!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.20.0a0+afc54f7>=0.9.0\ntqdm                ✅ 4.67.1>=4.64.0\npsutil              ✅ 7.0.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.3.0>=1.1.4\nultralytics-thop    ✅ 2.0.14>=2.0.0\n\n```\n\n### Minimal Reproducible Example\n\n```python\nmodel.export(format=\"engine\", imgsz = 320, data = \"/root/datasets/yolo_format/data.yaml\", int8=True, device=\"dla:0\", simplify = True, dynamic = False)  # dla:0 or dla:1 corresponds to the DLA cores\n``` \n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @testdummyvt, thank you for reporting this issue and for providing detailed information and logs! 🚀 This is an automated response to let you know that an Ultralytics engineer will review your report and assist you soon.\n\nIf you haven't already, please ensure your example is a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate 🐛. Your provided export code snippet is a great start.\n\nFor reference and troubleshooting, please also check the [Docs](https://docs.ultralytics.com/), especially the [Export](https://docs.ultralytics.com/modes/export/) section, as well as our [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples.\n\nJoin the Ultralytics community for real-time support or discussion:\n- [Discord](https://discord.com/invite/ultralytics) 🎧\n- [Discourse](https://community.ultralytics.com/)\n- [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nTo ensure your issue is not already resolved in the latest version, please upgrade to the newest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for helping improve Ultralytics!"
      },
      {
        "user": "Y-T-G",
        "body": "It's intentional due to issues with calibration when batch size is fixed\n\nhttps://github.com/ultralytics/ultralytics/issues/15856#issuecomment-2316508582"
      },
      {
        "user": "glenn-jocher",
        "body": "@Y-T-G @lakshanthad @ambitious-octopus user is saying this makes DLA use impossible if I understand correctly, so we need to revert this change or somehow handle calibration batch sizes in a smarter way."
      }
    ]
  },
  {
    "issue_number": 21085,
    "title": "What is a calibration dataset?",
    "author": "Abhishek0075",
    "state": "open",
    "created_at": "2025-06-17T09:58:20Z",
    "updated_at": "2025-06-17T10:33:06Z",
    "labels": [
      "question",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI am a beginner in converting the models into TensorRT engines. When I was converting it was seen converting the model into int8 engine we should have a calibration dataset. What is a calibration dataset ?. As a dataset I am having training dataset and test dataset. What should I do?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Abhishek0075, thank you for your interest in Ultralytics 🚀! We recommend visiting the [Docs](https://docs.ultralytics.com/) for new users, where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and answers to many common questions.\n\nSince this is a ❓ Question, please provide as much information as possible about your workflow, including the datasets you're using and any logs or scripts relevant to your conversion process. If you encounter any bugs during model conversion, be sure to provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us assist you efficiently.\n\nAn Ultralytics engineer will review your question soon, but in the meantime, you can also join our community for extra support:\n- Real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth discussions on [Discourse](https://community.ultralytics.com/)\n- Community threads on [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to ensure you have the newest features and bug fixes:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "Y-T-G",
        "body": "Your training dataset"
      }
    ]
  },
  {
    "issue_number": 21081,
    "title": "How to set up exporting onnx using xyxy instead of xywh",
    "author": "rm-ra",
    "state": "open",
    "created_at": "2025-06-17T07:52:13Z",
    "updated_at": "2025-06-17T10:32:18Z",
    "labels": [
      "question",
      "exports"
    ],
    "body": "![Image](https://github.com/user-attachments/assets/85f7faa5-d2ab-44b6-b8ac-ad68b2c2800f)\nFor example, after reshaping here, it should be xyxy. How can I modify the code by removing the part of xyxy2xywh and corresponding onnx reasoning to adapt @",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @rm-ra, thank you for reaching out to Ultralytics 🚀! This is an automated response to help you get started. An Ultralytics engineer will review your issue and assist you soon.\n\nWe recommend checking out the [Docs](https://docs.ultralytics.com/) for detailed guidance and many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples, where you might find answers to similar questions.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate further.\n\nIf you’re asking about custom export or inference behavior, please share as much detail as possible, including code snippets, model details, and any logs or screenshots (like the one you attached) to help us understand your use case.\n\nJoin the Ultralytics community for real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧, in-depth discussions on [Discourse](https://community.ultralytics.com/), or connect on our [Subreddit](https://reddit.com/r/Ultralytics) to learn and share with others.\n\n## Upgrade\n\nPlease make sure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your contribution and patience!"
      },
      {
        "user": "Y-T-G",
        "body": "You can run the following:\n\n```python\nIn [4]: model.model.model[-1].xyxy = True\n\nIn [5]: model.export(format=\"onnx\")\n```"
      },
      {
        "user": "rm-ra",
        "body": "\nI tried, but the output is still xywh, and the conversion part of onnx is still there. It doesn't seem to have changed at all. I just changed it to True here, and it's the same\n\n![Image](https://github.com/user-attachments/assets/17d8d37e-8c29-426c-a0f3-cf343fded54e)\n\n@Y-T-G "
      }
    ]
  },
  {
    "issue_number": 2065,
    "title": "Can I train keypoint detection model with multi-class bbox and each bbox have different keypoints?",
    "author": "TommyZihao",
    "state": "closed",
    "created_at": "2023-04-17T01:07:18Z",
    "updated_at": "2025-06-17T09:53:05Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nCan I train keypoint detection model with multi-class bbox and each bbox have different keypoints?\r\nIn coco8-pose sample dataset, only one class(person) is supported, so I wonder if I can train keypoint detection model with multi-class bbox and each bbox have different keypoints?\r\nFor example, there are 3 kinds of bbox in my dataset, crab has 10 keypoints, fish has 15 keypoints, frog has 5 keypoints.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@TommyZihao yes, you can train a keypoint detection model with multi-class bbox where each bbox has different keypoints. The coco8-pose sample dataset only supports one class (person), but that does not limit the ability for YOLOv8 to train on datasets with multiple classes, each with their own set of keypoints. You can create your own dataset with 3 classes, Crab, Fish and Frog, and define their respective keypoints. You can then use YOLOv8 to train on this dataset and detect each class with their corresponding keypoints. It's important to note that the number of keypoints and classes can affect the overall accuracy and speed of the model, so proper tuning and optimization may be required. For more information, please refer to the Ultralytics YOLOv8 documentation at https://docs.ultralytics.com."
      },
      {
        "user": "TommyZihao",
        "body": "If then, how do I change yaml file for multiple bbox class?\r\n`coco8-pose.yaml` only contains one class(person)\r\n"
      },
      {
        "user": "Laughing-q",
        "body": "@TommyZihao that's impossible for now I think. we only support single-class/multi-classes pose training but with the same number of keypoints."
      }
    ]
  },
  {
    "issue_number": 1906,
    "title": "Unique keypoints for multi-class pose estimation?",
    "author": "Ghassan-Spexal",
    "state": "closed",
    "created_at": "2023-04-08T10:36:46Z",
    "updated_at": "2025-06-17T09:17:46Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello ultralytics team!\r\n\r\nThank you for your amazing work as always!\r\n\r\nI was wondering if the new yolov8 pose estimation supports multiple classes, where each class has its own unique set of keypoints. The provided example shows a single class: human, with 17 keypoints (the standard keypoints in coco).\r\n\r\nMuch appreciated for the support!\r\n\r\nGhas\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @Ghassan-Spexal, thank you for your interest in YOLOv8 🚀! We recommend a visit to the [YOLOv8 Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) in a [**Python>=3.7**](https://www.python.org/) environment with [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@Ghassan-Spexal yes multiple classes are supported, though there is a constraint that all classes should share a common keypoint count (count itself is arbitrary), so i.e. if you have cats, dogs and humans with 30 keypoints each this would work.\r\n\r\n![image](https://user-images.githubusercontent.com/26833433/230721979-ba6923b3-41ab-4676-92ff-06ab0998ad46.png)\r\n"
      },
      {
        "user": "Ghassan-Spexal",
        "body": "well noted glenn!\r\n\r\nTaking an example: cat and human - I am interested in the nose of the cat and the eyes of the human (left and right), so a total of three keypoints.\r\nWhen the cat is labeled, what x, y and visible values must be given for the left and right eye? I assume zeros like keypoints outside the image boundary in coco.\r\n\r\nThanks!"
      }
    ]
  },
  {
    "issue_number": 8959,
    "title": "YOLOv8 class_weights",
    "author": "ksv87",
    "state": "closed",
    "created_at": "2024-03-15T07:43:55Z",
    "updated_at": "2025-06-17T08:30:07Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nAre you planing do it?\r\n\r\nhttps://github.com/ultralytics/ultralytics/blob/2d513a9e4bf51e961a4199067383d2052f483874/ultralytics/models/yolo/detect/train.py#L84\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@ksv87 hello there! 🚀 Thanks for reaching out. As of now, we don't have immediate plans to implement `class_weights` directly within the training script. We generally prioritize features based on community demand, so this may change in the future. \n\nMeanwhile, should you wish to experiment with class weighting yourself, you can modify the loss calculation in the training script. For example, you could manually apply different weights to different classes in the loss computation based on your requirements. \n\nRemember, every dataset is unique, and what works best can vary greatly. Therefore, we encourage experimentation and adaptation to your specific needs. \n\nIf you decide to go down that path and need further guidance or if you have any more questions, feel free to ask! Happy coding! 🎉"
      },
      {
        "user": "MonteiroAlvesMatheus",
        "body": "@glenn-jocher hello! \r\n\r\ni was looking in the ultralytics' files of yolov8 and i can't find the loss calculation to modify it. \r\n\r\nHow can i manually apply different weights to different classes using YOLOv8? Can you help me?\r\n\r\n\r\n\r\n"
      },
      {
        "user": "ksv87",
        "body": "> @glenn-jocher hello!\r\n> \r\n> i was looking in the ultralytics' files of yolov8 and i can't find the loss calculation to modify it.\r\n> \r\n> How can i manually apply different weights to different classes using YOLOv8? Can you help me?\r\n\r\nmay be this [ultralytics/utils/loss.py](https://github.com/ultralytics/ultralytics/blob/4a7ccba0af45834498d7a6362258c46acc993e2c/ultralytics/utils/loss.py)"
      }
    ]
  },
  {
    "issue_number": 12802,
    "title": "Multi-class pose estimation with class weights",
    "author": "xzxorb",
    "state": "closed",
    "created_at": "2024-05-18T19:18:30Z",
    "updated_at": "2025-06-17T08:29:46Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi! I am using the yolov8-pose model for keypoints detection of multiple classes, but  some classes have very bad precision. How can I modify the code to add class weight to let some classes have higher weight when computing loss? Could you give me some advice? Thank you very much.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "Hello! To incorporate class weights into your YOLOv8-pose model for keypoints detection, you'll need to modify the loss calculation to factor in these weights. Currently, YOLOv8 does not directly support weighted losses out-of-the-box for pose estimation.\n\nHowever, you can achieve this by customizing the loss function in the source code. Here’s a general approach:\n\n1. Identify the part of the code in `models/yolo.py` where the keypoint loss is computed.\n2. Introduce a weighting factor based on your class weights. You can define a tensor of class weights and multiply it by the loss values of corresponding classes.\n\nHere's a conceptual snippet:\n```python\n# Assuming 'loss' is your calculated keypoint loss tensor and 'class_weights' is a tensor containing weights for each class\nweighted_loss = loss * class_weights[targets[:, 5].long()]  # targets[:, 5] should correspond to your class labels assuming they're at index 5\n```\n\nYou would need to appropriately define the `class_weights` tensor to match the number of classes you have, and ensure it's moved to the same device as your model and loss tensors.\n\nPlease adjust this concept according to your specific implementation details and requirements. If you need further guidance, don't hesitate to consult!"
      },
      {
        "user": "xzxorb",
        "body": "Thank you, I will try this later."
      },
      {
        "user": "glenn-jocher",
        "body": "@xzxorb great! If you run into any further issues or have questions as you implement this, feel free to reach out. Happy coding! 😊"
      }
    ]
  },
  {
    "issue_number": 12840,
    "title": "YOLOv8 Weight Penalty",
    "author": "NerminMostafa",
    "state": "closed",
    "created_at": "2024-05-19T10:54:48Z",
    "updated_at": "2025-06-17T08:29:37Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\r\n\r\n\r\nHello! First of all, thank you very much for your team's outstanding contributions!\r\n\r\n### Question: Is there a way to apply a weight penalty to train specific classes more effectively?\r\n\r\nI'm training a YOLOv8n model with a custom dataset (PlantVillage dataset) to detect leaf diseases. The dataset is balanced and split 60:40. I trained the model for 150 epochs with the following settings: imgsz = 256, batch_size = 64, and using a GPU.\r\n\r\nDespite trying different versions of the model (YOLOv8s, YOLOv8m) and experimenting with various training parameters (learning rate adjustments, increasing the number of epochs, changing weight decay, modifying patience number, and increasing close_mosaic to 25), the results remain almost the same.\r\n\r\nThe model performs very well on some classes but struggles with others, specifically classes 4, 5, 7, 8, 9, and 11 (zero-based indexing). I need to improve the model's learning for these specific classes.\r\n\r\n### More info:\r\nHere are the losses from the last epoch:\r\nTrain box_loss = 1.4227\r\nTrain cls_loss = 0.94637\r\nTrain dfl_loss = 1.2384\r\n\r\nValid box_loss = 1.8663\r\nValid cls_loss = 1.3516\r\nValid dfl_loss = 1.5154\r\n\r\nHere's the confusion matrix\r\n![new](https://github.com/ultralytics/ultralytics/assets/93271872/219ef803-5af5-4daf-ba73-89d8301f21d4)\r\n\r\nPlease let me know if there is a way to apply a weight penalty to improve my model and how to apply it, Thank you.\r\n\r\n\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @NerminMostafa, thank you for your interest in Ultralytics YOLOv8 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "Hello! Thanks for reaching out and for your detailed explanation of the issue.\n\nTo apply a weight penalty specifically to certain classes in YOLOv8, you can adjust the `cls` weights in the loss function for those classes. This can be done by modifying the `cls` parameter in your training configuration file. You can increase the weight for classes 4, 5, 7, 8, 9, and 11 to give more importance to these classes during the loss calculation.\n\nHere's a quick example of how you might adjust these settings in your YAML configuration file:\n\n```yaml\n# Assuming 'cls' weights are not initially set, you can define them like this:\ncls_weights: [1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1]  # Increasing weights for classes 4, 5, 7, 8, 9, 11\n```\n\nMake sure the length of `cls_weights` matches the number of classes, and increase the values for the classes you want to penalize less (or focus more on correcting). This should help the model pay more attention to these specific classes during training.\n\nLet me know if this helps or if you need further assistance! 🚀"
      },
      {
        "user": "NerminMostafa",
        "body": "@glenn-jocher Thank you so much for your quick response and helpful instructions! I implemented the class weights as you described, but unfortunately, the improvement was not as significant as I expected. Could you please provide further assistance or suggest another solution to enhance the model's performance for these specific classes? Your support is greatly appreciated!"
      }
    ]
  },
  {
    "issue_number": 13358,
    "title": "class weights",
    "author": "BNislam",
    "state": "closed",
    "created_at": "2024-06-04T12:56:35Z",
    "updated_at": "2025-06-17T08:29:10Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nhello, I want to use the weights classes to balance my dataset to train a yolov8 model out of 12 class I calculated the weights [0.02019275 0.09361391 0.11182256 0.09952436 0.01905868 0.06931413\r\n0.05859288 0.10397119 0.05381769 0.11471 0.06815406 0.18722781] my question is how can I use it??? and is this solution useful or not?????\r\n\r\nthanks in advance\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @BNislam, thank you for your interest in Ultralytics YOLOv8 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@BNislam hello! Great to see that you're looking to balance your dataset using class weights for training your YOLOv8 model. Currently, this is not possible to do with Ultralytics.\nIf you need further assistance or have more questions, feel free to ask. Happy training! 🚀\n\n_Updated on date 02.05.25_"
      },
      {
        "user": "BNislam",
        "body": "thank you for your reply, My objective is to detect some anomaly in the electronic cards and the components are very small. Is there a configuration that I must do?\r\n![Uploading exp.png…]()\r\n"
      }
    ]
  },
  {
    "issue_number": 1531,
    "title": "How to use Varifocal Loss",
    "author": "AnabelGRios",
    "state": "closed",
    "created_at": "2023-03-21T08:55:29Z",
    "updated_at": "2025-06-17T08:26:25Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello. First of all, thank you so much for your wonderful work.\r\n\r\nI am trying to use the Varifocal Loss defined in yolo/utils/loss.py instead of BCE loss to perform object detection because I have a very imbalanced dataset. To do that, I have changed the yolo/v8/detect/train.py file to uncomment line 185 and comment line 186. As a consequence, in line 177 I also save the `target_labels` and I define `self.varifocal_loss as self.varifocal_loss = VarifocalLoss().to(device)` in the init function. However, when running this, I get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\agr\\Documents\\ultralytics\\ultralytics\\yolo\\engine\\model.py\", line 320, in train\r\n    self.trainer.train()\r\n  File \"C:\\Users\\agr\\Documents\\ultralytics\\ultralytics\\yolo\\engine\\trainer.py\", line 186, in train\r\n    self._do_train(RANK, world_size)\r\n  File \"C:\\Users\\agr\\Documents\\ultralytics\\ultralytics\\yolo\\engine\\trainer.py\", line 309, in _do_train\r\n    self.loss, self.loss_items = self.criterion(preds, batch)\r\n  File \"C:\\Users\\agr\\Documents\\ultralytics\\ultralytics\\yolo\\v8\\detect\\train.py\", line 74, in criterion\r\n    return self.compute_loss(preds, batch)\r\n  File \"C:\\Users\\agr\\Documents\\ultralytics\\ultralytics\\yolo\\v8\\detect\\train.py\", line 192, in __call__\r\n    loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\r\n  File \"C:\\Users\\agr\\Environments\\PyT1.10Cu10.2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1102, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"C:\\Users\\agr\\Documents\\ultralytics\\ultralytics\\yolo\\utils\\loss.py\", line 17, in forward\r\n    weight = alpha * pred_score.sigmoid().pow(gamma) * (1 - label) + gt_score * label\r\nRuntimeError: The size of tensor a (10) must match the size of tensor b (11109) at non-singleton dimension 2\r\n```\r\nCould you please tell me what am I doing wrong? Thank you so much.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @AnabelGRios, thank you for your interest in YOLOv8 🚀! We recommend a visit to the [YOLOv8 Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Install\n\nPip install the `ultralytics` package including all [requirements.txt](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) in a [**Python>=3.7**](https://www.python.org/) environment with [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "Laughing-q",
        "body": "@AnabelGRios hi, VFL is no longer maintained since we don't use it. If I remember this correctly you have to generate a one-hot vector first based on the current target_labels before passing it to VFL.\r\nrelated issue: #1448 "
      },
      {
        "user": "AnabelGRios",
        "body": "Ok, thanks!"
      }
    ]
  },
  {
    "issue_number": 2208,
    "title": "How to solve the YOLOv8-cls class imbalance problem",
    "author": "kimdodo97",
    "state": "closed",
    "created_at": "2023-04-24T04:45:39Z",
    "updated_at": "2025-06-17T08:25:50Z",
    "labels": [
      "question",
      "fixed",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI am using the YOLOv8 classification model.\r\nIn the model, the classification is classifying a total of 5 labels.\r\nHere I have about 1800,500,1200,5000,20000 data points for each label.\r\nI expect to encounter class imbalance issues in this classification model, and I was wondering if a method like class balanced loss is currently included in the YOLOv8-cls model?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@jungms4536 thank you for your question. Yes, we do have class-balanced loss available in our YOLOv8-cls model. Class imbalance can indeed be an issue in classification tasks, and using class-balanced loss can help address this problem. You may also refer to our documentation at https://docs.ultralytics.com/ for more information on how to use this feature. Let us know if you have any further questions or concerns."
      },
      {
        "user": "kimdodo97",
        "body": "I also checked that the focal loss function is defined in yolov8, but I don't see anything in the documentation on how to use class balance loss, if you have a reference or method, could you please let me know?"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 2703,
    "title": "Is it possible to change class weight?",
    "author": "KazuhideMimura",
    "state": "closed",
    "created_at": "2023-05-19T07:19:18Z",
    "updated_at": "2025-06-17T08:25:28Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi, I'd like to train on a dataset with imbalanced number of classes.\r\nI trained a detection model with on a dataset with 2 classes, and class1:class2 is  ~1:4.\r\nThe trained model seems to be more fitted to detect class2, and cannot detect class1 as I expected. Therefore, I'd like to increase the weight for class1 and decrease class2's weight, which is typically conducted in image classification tasks.\r\n \r\nIs that possible?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @KazuhideMimura, thank you for your interest in YOLOv8 🚀! We recommend a visit to the [YOLOv8 Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) in a [**Python>=3.7**](https://www.python.org/) environment with [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@KazuhideMimura hello!\r\nYes, it is possible to change class weights in YOLOv8. You can apply a loss weight multiplier to each class in the loss function. By adjusting the weight values, you can adjust the model's bias towards specific classes and achieve better detection results on imbalanced datasets. \r\nTo implement this, check the `weight` parameter in the `nn.BCEWithLogitsLoss()` as this will allow you to modify the per-class weights. We recommend using a higher value for the underrepresented class to balance out the distribution.\r\nBest of luck with your project!"
      },
      {
        "user": "KazuhideMimura",
        "body": "@glenn-jocher Hi, thank you for your reply!\r\nDo you mean that I should edit [L147 of v8/detect/train.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/v8/detect/train.py#L147) as follows?\r\n`pos_weight=torch.FloatTensor([2.5, 0.625]) # 4:1 (for example)`\r\n`self.bce = nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)`"
      }
    ]
  },
  {
    "issue_number": 3660,
    "title": "Handling Imbalanced classes dataset",
    "author": "jaideep11061982",
    "state": "closed",
    "created_at": "2023-07-11T14:15:36Z",
    "updated_at": "2025-06-17T08:24:43Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n@glenn-jocher  I was wondering if there is any way we can handle the class imbalance that could arise when one object type class occurring more frequently and in numbers compared to the other object class type  , imbalance class distribution may make model to predict only dominant object class in scene if it appears quite similar to the other object classes  in certain attributes.\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@jaideep11061982 hi! Thank you for reaching out to us with your question. Handling class imbalance is an important consideration when training models, including YOLOv8.\n\nTo address class imbalance in YOLOv8, there are a few strategies you can try:\n1. **Data augmentation**: By augmenting the minority class with various transformations such as rotations, flips, and translations, you can increase the number of samples and balance the dataset.\n2. **Class weighting**: Assign different weights to each class during training. This allows the model to pay more attention to the minority class and can help balance the impact of class imbalance.\n3. **Oversampling or undersampling**: You can oversample the minority class by duplicating samples or undersample the majority class by removing samples. Both methods aim to balance the class distribution, but they should be used with caution to avoid introducing bias.\n\nIt's important to experiment with these techniques and evaluate their effectiveness on your specific dataset. You can adjust the parameters and hyperparameters of the training process to achieve the best results.\n\nIf you are using the Ultralytics YOLOv8 repo, you can modify the code accordingly to implement these strategies. You can find guidance on this in the YOLOv8 documentation and by referring to the examples and tutorials provided.\n\nI hope this information helps! If you have any further questions, feel free to ask."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      },
      {
        "user": "ashray21",
        "body": "@glenn-jocher How can I give different weights to each class in yolov8 classification parameters ?"
      }
    ]
  },
  {
    "issue_number": 6027,
    "title": "Weighed Random Sampler",
    "author": "aymuos15",
    "state": "closed",
    "created_at": "2023-10-30T13:28:19Z",
    "updated_at": "2025-06-17T08:23:32Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nIs there any way to integrate a weighed random sampler?\r\n\r\nEx: https://github.com/ultralytics/yolov5/issues/1238 in yolov5\n\n### Use case\n\nDirectly improve performance on imbalanced datasets.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "aymuos15",
        "body": "I've found it. Didn't realise yolov8 had it as well. Apologies. Can delete this issue if required."
      },
      {
        "user": "glenn-jocher",
        "body": "@aymuos15 no problem at all! I'm glad to hear that you found what you were looking for in YOLOv8. It's always a good idea to explore and understand the full capabilities of the model. If you have any other questions or concerns in the future, don't hesitate to reach out.\n\nHaving discussions here can be beneficial to other users as well, so there's no specific need to delete this issue. Perhaps this conversation might help someone else with the same question in the future. Thank you for contributing to the YOLOv8 community!"
      },
      {
        "user": "adnan119",
        "body": "@aymuos15 I'm looking for a similar solution that can perform the functionality of a weighted random sampler or what `--image-weights` does in YOLO v5. Searched out this repo but couldn't find it, can you show me where exactly you found this?"
      }
    ]
  },
  {
    "issue_number": 5738,
    "title": "Yolov8 detect train class imbalanced problem",
    "author": "duymanh-111",
    "state": "closed",
    "created_at": "2023-10-16T12:08:31Z",
    "updated_at": "2025-06-17T08:22:49Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nDear Ultralytics Team!\r\nI have my custom dataset to classify 30 classes.\r\nBut the classes's count are very different.\r\nFor example one class1's count is 200, class2's count is 30000.\r\nI trained with this imbalanced dataset, the box detection is good but class detection is very bad.\r\nIs there the parameter in yolov8 training params to solve the class imbalanced problem without manual data augment.\r\nThanks.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@duymanh-111 hello,\n\nThank you for reaching out to us on this issue! In situations with imbalance in class distributions, it is common to observe the performance of a model to be suboptimal, especially for the classes with fewer representations in the dataset.\n\nWithin the Ultralytics YOLOv8 implementation, we have considered this point. You may adjust the 'cls' loss weight in the loss configuration of your training script. Increasing this value makes the model pay more attention to classification accuracy, which could help to alleviate the imbalance problem.\n\nThough the 'cls' loss weight adjustment can help, it might not fully solve the imbalance problem for extremely skewed distributions. Hence, for robust performance and optimal results, generating additional representations for the under-represented classes using data augmentation techniques, although manual, is still highly recommended.\n\nOn another note, the model's predictions and the class score output can be modulated using the confidence thresholding and non-maximum suppression (NMS) parameters during inference, which can also contribute to improving overall class detection performance.\n\nI hope this information assists you in improving the performance of your model, and we'd like to hear how you get on. \n\nBest,\nGlenn Jocher"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      },
      {
        "user": "Y-T-G",
        "body": "Duplicate of https://github.com/ultralytics/ultralytics/issues/21082"
      }
    ]
  },
  {
    "issue_number": 6204,
    "title": "Handling Imbalanced Class Instance in YOLOv8 ?",
    "author": "aimad1234",
    "state": "closed",
    "created_at": "2023-11-08T11:54:13Z",
    "updated_at": "2025-06-17T08:22:38Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nmy question is : does YOLOv8 offer any configuration options that could alleviate the problem, or resolve it in the data preparation phase?\r\n\r\n### Additional\r\n\r\nI have 5 classes in my dataset and I'm faced with an imbalance in the number of instances distributed over five distinct classes, as shown in the figure above.\r\n![Screenshot 2023-11-08 at 12 35 39](https://github.com/ultralytics/ultralytics/assets/66996795/bd8c3ab3-5447-4fa2-8cff-824a0d57306a)",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @aimad1234, thank you for your interest in YOLOv8 🚀! We recommend a visit to the [YOLOv8 Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@aimad1234 hello,\n\nThank you for reaching out with your question regarding handling imbalanced class instances in YOLOv8.\n\nClass imbalance is indeed a common challenge in training object detector models when some classes are underrepresented in the dataset compared to others. YOLOv8 has internal mechanisms to mitigate this issue during training, though they are not directly exposed as configurable options.\n\nOne of the primary ways YOLOv8 tackles class imbalance is through the use of Focal Loss, which automatically gives more weight to harder, less frequent examples and less weight to easier, more common examples. This can help in rebalancing the learning focus, especially when dealing with imbalanced datasets.\n\nAdditionally, YOLOv8 uses data augmentation strategies during training that can help provide a more balanced view of different classes by artificially augmenting underrepresented classes more frequently. Augmentations such as random scaling, cropping, and flipping can increase the variability and effective sample size of minority classes in your dataset.\n\nIn the data preparation phase, you could manually augment your dataset for the underrepresented classes or gather more data for those classes. Alternatively, carefully curated oversampling of the minority class instances or synthesizing new instances using techniques such as SMOTE could be considered outside the scope of YOLOv8 functionalities.\n\nBalancing class instances is often dataset-specific and requires some experimentation. The best approach depends on the degree of imbalance and the particularities of your dataset. Monitoring the training progress and validating the performance of each class can guide you on whether additional balance-focused interventions are necessary.\n\nIf you continue to experience issues or would like more personalized insights on addressing class imbalance during your YOLOv8 training, feel free to describe your situation further, and we can delve deeper into the specifics of your use case.\n\nWishing you success with your YOLOv8 model training!"
      },
      {
        "user": "aimad1234",
        "body": "@glenn-jocher \r\nIt is very kind of you to give me this information, I will contact you again if I encounter any difficulties."
      }
    ]
  },
  {
    "issue_number": 8578,
    "title": "pos_weight for dealing with imbalanced dataset",
    "author": "hulkds",
    "state": "closed",
    "created_at": "2024-03-01T13:03:18Z",
    "updated_at": "2025-06-17T08:20:57Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nAdd a weight of positive examples to be broadcasted with target (for Detection).\n\n### Use case\n\nBased on [torch.nn.BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html), add pos_weight parameter can help when working with imbalanced dataset.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @hulkds, thank you for your interest in Ultralytics YOLOv8 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "hulkds",
        "body": "PR: https://github.com/ultralytics/ultralytics/pull/8557"
      },
      {
        "user": "glenn-jocher",
        "body": "@hulkds thank you for bringing up the idea of incorporating `pos_weight` for handling imbalanced datasets in YOLOv8 and for the PR! 🚀 This is indeed a valuable addition, especially for those working with datasets where the class distribution is skewed. \n\nYour suggestion to leverage `torch.nn.BCEWithLogitsLoss`'s `pos_weight` parameter aligns well with our goal to enhance model performance across a variety of scenarios. We'll review the PR at https://github.com/ultralytics/ultralytics/pull/8557 closely and provide feedback or merge it after thorough testing.\n\nFor anyone interested, this feature allows you to adjust the weight of positive examples dynamically, which can be particularly useful when dealing with classes that are underrepresented in your dataset. Here's a quick snippet on how you might adjust the loss function:\n\n```python\nimport torch.nn as nn\n\n# Assuming 'pos_weight' is calculated based on your dataset's class distribution\nloss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n```\n\nWe appreciate your contribution and look forward to more collaborations! 🙌"
      }
    ]
  },
  {
    "issue_number": 9128,
    "title": "Question Related To Class Imbalance .",
    "author": "gftmadan",
    "state": "closed",
    "created_at": "2024-03-19T20:49:03Z",
    "updated_at": "2025-06-17T08:20:51Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nWhats The Best Way To Handle Class Imbalance In The Current Version Of YoloV8 implementation.\r\nI Have Read Other Discussions Like Adding `pos_weight` Parameter To BCELogitLoss Function , But All those Changes Cannot Be Implemented in the Current Version Of YoloV8.\r\n\r\n# `So What The Best Ways To Handle Class Imbalance Problem like Increasing the Weightage of dfl Loss or Any Other?`\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @gftmadan, thank you for your interest in Ultralytics YOLOv8 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@gftmadan hello there! 😊\n\nHandling class imbalance is a common challenge in object detection tasks. In YOLOv8, you can address class imbalance through several strategies during your training process, even though direct modifications like adding `pos_weight` might not be directly applicable. \n\nOne effective approach includes adjusting the `--img-weights` flag during training, which dynamically weights each image's loss based on the number of targets present, indirectly helping to balance the class frequencies.\n\nAnother strategy involves manipulating the loss coefficients in your configuration file for the classes or components (like `box`, `cls`, `obj`) that are more affected by the imbalance. Tweaking these coefficients can help the model pay more attention to underrepresented classes. \n\nFor example, increasing the `cls` (class loss) coefficient might encourage the model to focus more on getting the class predictions right:\n\n```yaml\n# In your .yaml config file\nloss:\n  box: 0.05  # You might keep these constant\n  obj: 0.5  \n  cls: 1.0   # Increase this for emphasizing class prediction\n```\n\nAugmenting your dataset with more examples of the underrepresented classes or using techniques like SMOTE for oversampling can also indirectly help.\n\nExperimentation is key, as the effectiveness of these strategies can vary depending on your specific dataset and task. 🧪 Good luck and happy training!"
      },
      {
        "user": "gftmadan",
        "body": "@glenn-jocher Thanks for your Reply.\r\n\r\nCan you Please Give a Snippet of Code Of How to Use `--img-weights` Flag , and Specially When Training With Python.\r\nBecause I Cannot Find Any `--img-weights ` Parameter in the Train Setting Documentation."
      }
    ]
  },
  {
    "issue_number": 12849,
    "title": "Giving class weight to Yolov8",
    "author": "jaiydv",
    "state": "closed",
    "created_at": "2024-05-19T20:24:51Z",
    "updated_at": "2025-06-17T08:19:52Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n\r\nI want to train a yolov8 model for object detection and i have these 6 classes\r\n Class 2: 1378 \r\n Class 4: 2491 \r\n Class 5: 21815 \r\n Class 0: 821 \r\n Class 1: 478 \r\n Class 3: 1113\r\n\r\nI want to pass class weights to yolov8 for training but I didn't find any solution. \r\nis there any way I can tackle this situation apart from augmentation and oversampling, I specifically want to try giving class weights to model while training\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @jaiydv, thank you for your interest in Ultralytics YOLOv8 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "Hello! Thanks for reaching out with your question about using class weights in YOLOv8 training. Currently, YOLOv8 does not natively support direct class weighting within the training process. However, you can manage class imbalance by focusing on data augmentation and oversampling strategies, as you've mentioned.\n\nIf you're specifically looking to integrate class weights, you might consider modifying the loss computation part of the source code to manually incorporate these weights. This would involve adjusting the loss functions in the model's training script to account for the imbalance based on your class frequencies.\n\nFor a more straightforward approach, ensuring your dataset is as balanced as possible or using techniques like image augmentation to artificially boost underrepresented classes remains a practical solution.\n\nIf you need further guidance on how to modify the loss functions or any other assistance, feel free to ask. Happy training! 🚀"
      },
      {
        "user": "jaiydv",
        "body": "@glenn-jocher  I am curious why there is no support to pass on class weights, we can have a parameter of class weights that we can pass while training, that will be easier and more feasible, and having this functionality will be good for our YOLOv8 training process."
      }
    ]
  },
  {
    "issue_number": 14624,
    "title": "Hello! How do I decide upon the class weights?  Do I initialize them randomly based on class preference ?",
    "author": "Quratf",
    "state": "closed",
    "created_at": "2024-07-23T11:03:48Z",
    "updated_at": "2025-06-17T08:18:44Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "              Hello!\r\n\r\nCertainly! To adjust the `cls_loss` for handling class imbalance, you can modify the loss function to incorporate class weights. Here's a basic snippet on how you might approach this in your training configuration:\r\n\r\n```python\r\nfrom ultralytics import YOLO\r\n\r\n# Load your model\r\nmodel = YOLO('yolov8n.pt')\r\n\r\n# Define class weights (example weights, adjust based on your dataset needs)\r\nclass_weights = [0.5, 2.0, 1.5]  # Example: for three classes\r\n\r\n# Set class weights in the model\r\nmodel.class_weights = class_weights\r\n\r\n# Proceed with training\r\nresults = model.train(data='your_dataset.yaml')\r\n```\r\n\r\nThis example manually sets weights for each class to help the model pay more attention to underrepresented classes. Adjust the weights based on the frequency of each class in your dataset.\r\n\r\nHope this helps! Let me know if there's anything else you need. 😊\r\n\r\n_Originally posted by @glenn-jocher in https://github.com/ultralytics/ultralytics/issues/4219#issuecomment-2147849760_\r\n            ",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @Quratf, thank you for your interest in Ultralytics YOLOv8 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@Quratf hello! To decide on class weights, you should base them on the frequency of each class in your dataset. Typically, less frequent classes should have higher weights to ensure the model pays more attention to them. You can calculate these weights by taking the inverse of the class frequencies. This approach helps to balance the influence of each class during training. If you encounter any issues, please ensure you are using the latest version of the package. Let me know if you need further assistance!"
      },
      {
        "user": "Quratf",
        "body": "Thanks for the help, Glen. However I have another question that has stalled\r\nmy research progress. How to train the model on MIOTCD( traffic dataset) so\r\nthat vehicles captured from all camera height are correctly identified and\r\nlocalized. It seems with the current model only vehicles captured from a\r\nhigher angle and height have satisfactory results, while with footage\r\ncaptured from the height of car roof, the results are not usable for\r\nfurther analysis.\r\n\r\nThanks\r\n\r\nOn Tue, 13 Aug 2024 at 10:37 AM, Glenn Jocher ***@***.***>\r\nwrote:\r\n\r\n> @Quratf <https://github.com/Quratf> hello! To decide on class weights,\r\n> you should base them on the frequency of each class in your dataset.\r\n> Typically, less frequent classes should have higher weights to ensure the\r\n> model pays more attention to them. You can calculate these weights by\r\n> taking the inverse of the class frequencies. This approach helps to balance\r\n> the influence of each class during training. If you encounter any issues,\r\n> please ensure you are using the latest version of the package. Let me know\r\n> if you need further assistance!\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/ultralytics/ultralytics/issues/14624#issuecomment-2286553943>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AVQYDTW2EKYHRZ3VKCNBJITZRIR4TAVCNFSM6AAAAABLKF2P46VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDEOBWGU2TGOJUGM>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      }
    ]
  },
  {
    "issue_number": 21018,
    "title": "Why doesn't Yolo fix unbalanced classes with a parameter?",
    "author": "sarpx",
    "state": "open",
    "created_at": "2025-06-11T06:26:00Z",
    "updated_at": "2025-06-17T08:17:19Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nclass_weights is nice but wouldn't it be better if this was automated with a parameter? We have to write extra code, that's fine but every time a version changes there are a lot of files to change..\nLet's just say you set the class_weights for these classes and that's it :) also we sometimes start trainings from the console for testing purposes.. this feature would be great :)) If I had the capacity to add this to the yolo code I would do it so please don't blame me for making such a request.\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @sarpx, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nThis is an automated response—an Ultralytics engineer will also assist you soon.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question or feature suggestion, please provide as much detail as possible, including your specific use case, dataset examples, and any code or logs that might help us better understand your request. Make sure you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "Y-T-G",
        "body": "Technically, class weights would need to be applied to the box, pose, keypoint and mask losses too, besides the classification loss, which is a lot of extra code and handling.\n\nDoes class weights really help improve the performance?"
      },
      {
        "user": "sarpx",
        "body": "@Y-T-G \nIf the dataset:\n\nSome classes are very few (for example, only 5 cancer cells but 2000 healthy cells),\n\nThe model may focus on the majority class and not learn the minority classes, i.e. it will tend to say \"no cancer\".\n\nIn this case, it seems that by giving more weight to the minority classes with class_weights, the model can make them predict correctly.\n\nFor example:\n\nClass 0 (normal): many examples → low weight (for example 0.1)\n\nClass 1 (deformed): few examples → high weight (for example 2.0)\n\nIn this way, the model's errors are penalized proportionally as follows:\n\nIf normal, wrong prediction → low penalty\n\nIf deformed, wrong prediction → high penalty"
      }
    ]
  },
  {
    "issue_number": 15044,
    "title": "How to change class weights in YOLOv8n to address class imbalance?",
    "author": "mmd-nemati",
    "state": "closed",
    "created_at": "2024-08-08T17:20:17Z",
    "updated_at": "2025-06-17T08:16:11Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI'm working on a cancer dataset which has ~54000 images. It has a huge imbalance; there are only 2% images with `cancer = True`. So it leads to poor performances on `cancer = True` class.\r\n\r\nSince gathering more data is not possible, I tried to change the weights for each class in the training step, ```{cancer_zero = 1.0, cancer_one = 50.0}```, but I couldn't find the right argument or configuration for this.\r\n\r\n```cls_weight``` or ```hyp``` or ```pos_weights``` parameters, as mentioned in various other issues like [this ](https://github.com/ultralytics/ultralytics/issues/2358) one are not available in the ```YOLO('yolov8n.pt')``` class or in the ```train``` function. \r\nAlso editing ```data.yaml``` with adding the class weights to it didn't work.\r\n\r\nSo can you provide the correct way or parameter for the Model constructor or ```train``` function to modify the class weights?\r\n\r\nThanks!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @mmd-nemati, thank you for your interest in Ultralytics YOLOv8 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "Y-T-G",
        "body": "There's no official way to do this. You can try using callbacks to modify the loss function.\r\n\r\nFor example, for `Detect` model:\r\n\r\n```python\r\nimport torch.nn as nn\r\nimport torch\r\nfrom ultralytics import YOLO\r\n\r\nmodel = YOLO(\"yolov8n.pt\")\r\ndef add_weights(trainer):\r\n    m = trainer.model\r\n    m.criterion = m.init_criterion()\r\n    m.criterion.bce = nn.BCEWithLogitsLoss(reduction=\"none\", pos_weight=torch.tensor([1, 50]).to(trainer.device))\r\nmodel.add_callback(\"on_train_start\", add_weights)\r\nmodel.train()\r\n```"
      },
      {
        "user": "mmd-nemati",
        "body": "Hello @Y-T-G ,\r\nThank you so much for providing the code, I really appreciate it. Unfortunately, adjusting the class weights didn't significantly improve the performance, so I think I may need to explore other approaches to address this issue."
      }
    ]
  },
  {
    "issue_number": 15184,
    "title": "labels_to_class_weights & labels_to_image_weights",
    "author": "54HaoHao-hue",
    "state": "closed",
    "created_at": "2024-08-10T12:01:15Z",
    "updated_at": "2025-06-17T08:16:03Z",
    "labels": [
      "enhancement",
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi, thank you for your interest in Ultralytics YOLOv8 🚀! the sample distribution of my data is unbalanced, and I have found that YOLOV7 and V9 have ensembles \"labels_to_class_weights\" and \"labels_to_image_weights\" in the general.py, and these two strategies are effective when dealing with sample imbalance. However I didn't find these two strategies in v8, v10, I guess  the strategies might be beneficial to improve v8's performance when the sample is unbalanced and I want to integrate them into the project, but I don't know which files to add, can you give me a little advice? Thank you very much!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @54HaoHao-hue, thank you for your interest in Ultralytics YOLOv8 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "pderrenger",
        "body": "@54HaoHao-hue thank you for your question. In YOLOv8, the strategies \"labels_to_class_weights\" and \"labels_to_image_weights\" are not directly implemented as they were in YOLOv7 and YOLOv9. However, you can still address class imbalance by modifying the training pipeline. You can implement custom weighting strategies by adjusting the loss function or data loader to incorporate class weights.\n\nTo integrate these strategies, you would typically modify the training script where the loss function is defined. You can add class weights to the loss calculation to give more importance to underrepresented classes. Additionally, you can adjust the data loader to apply image weights during the sampling process.\n\nIf you need further assistance with specific code modifications, please let us know. For more detailed guidance, you can refer to the Ultralytics documentation or explore the community discussions for similar implementations."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 15615,
    "title": "如何设置多目标检测类别损失权重解决数据集分布不均衡的问题",
    "author": "wangyang581",
    "state": "closed",
    "created_at": "2024-08-15T09:28:43Z",
    "updated_at": "2025-06-17T08:15:43Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\n我正在训练一个多目标检测的数据集，在我的数据集中，不同类别的标注框数量很不均衡，某些类别标注框数量比例达到了10:1，我想通过调整不同类别的损失权重训练模型优化检测效果，我参考了[https://github.com/ultralytics/ultralytics/issues/13330](url)中的设置方法，但是训练并没有成功出现了报错如下：\r\n![image](https://github.com/user-attachments/assets/02a91602-b691-4eb2-882e-d84987762bf4)\r\n我通过终端命令行进行训练，训练命令为：\r\nyolo detect train data=custum.yaml model=yolov8s.pt epochs=500 imgsz=1280 device=3 batch=128 name=gamble_v3.4_s weight_decay=0.001 cls=[0.08,0.16,0.16,0.16,0.16,0.08,0.08,0.08]\r\n看起来cls参数设置的有问题，请问我如何设置才能实现我想要的效果。\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@wangyang581 `cls` is the classification loss gain. This must be a float like 1.0, it can not be a list."
      },
      {
        "user": "chuang091",
        "body": "@wangyang581 根據 #13330 你應該要調整的是 `custum.yaml` 裡面的weights 但你調整的這個list看起來像是**超參數 (hyperparameter)**\r\n對於超參數， @glenn-jocher 所提到的 `cls` 請參考 #10375 "
      },
      {
        "user": "glenn-jocher",
        "body": "@chuang091 您需要在 `custum.yaml` 文件中调整类别权重，而不是在命令行中设置 `cls` 参数。请参考 #10375 中关于 `cls` 的说明来正确设置超参数。"
      }
    ]
  },
  {
    "issue_number": 15934,
    "title": "How do I set parameters for an unbalanced number of categories？",
    "author": "HuaWeng",
    "state": "closed",
    "created_at": "2024-09-01T02:30:07Z",
    "updated_at": "2025-06-17T08:15:32Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nCar: 23456\r\nCyclist: 6548\r\nTruck: 4297\r\nTram: 1681\r\nPedestrian: 4901\r\nTricycle: 227\r\nThe above is the number of categories in my training set. I want to train a model with a higher mAP based on yolov8. In the face of such class imbalance, does yolov8 have Settings such as class weights that allow the model to give more attention to a smaller number of classes\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "Y-T-G",
        "body": "No. You can try this:\nhttps://github.com/ultralytics/ultralytics/issues/15044#issuecomment-2276387717\n\nBut it only changes classification weights and not localization weights."
      },
      {
        "user": "HuaWeng",
        "body": "> No. You can try this:不，你可以试试这个： [#15044 (comment)](https://github.com/ultralytics/ultralytics/issues/15044#issuecomment-2276387717)\r\n> \r\n> But it only changes classification weights and not localization weights.但它只改变分类权重，而不改变定位权重。\r\n\r\nThank you for your advice. I have tried to train in this way, but the effect is still unsatisfactory, are there any tips and precautions in the weight setting of various categories? Here's how I set it up (reciprocal of category frequency) :\r\ndef add_weights(trainer):\r\n    m = trainer.model\r\n    m.criterion = m.init_criterion()\r\n    weights = torch.tensor([7.69, 5.88, 1.61, 9.09, 25.00, 100.00]).to(trainer.device)\r\n    m.criterion.bce = nn.BCEWithLogitsLoss(reduction=\"none\", pos_weight=weights.to(trainer.device))\r\nmodel.add_callback(\"on_train_start\", add_weights)\r\nmodel.train(\r\n    data=\"/datasets.yaml\",\r\n    epochs = 100,\r\n    batch = 0.90,\r\n    imgsz = 960,\r\n    patience = 10,\r\n    optimizer = 'AdamW'\r\n)"
      },
      {
        "user": "glenn-jocher",
        "body": "@HuaWeng thank you for sharing your approach. To improve class balance, consider experimenting with different weighting schemes and adjusting the `pos_weight` values. Additionally, you might explore data augmentation techniques to balance the dataset further. For more detailed guidance, please refer to our [model training tips](https://docs.ultralytics.com/guides/model-training-tips/)."
      }
    ]
  },
  {
    "issue_number": 16205,
    "title": "Class Imbalance Problem (Proposed Solution)",
    "author": "AWallyAllah",
    "state": "closed",
    "created_at": "2024-09-11T04:19:49Z",
    "updated_at": "2025-06-17T08:15:18Z",
    "labels": [
      "enhancement",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\r\n\r\n\r\n### Ultralytics YOLO Component\r\n\r\nVal\r\n\r\n### Bug\r\n\r\nThe class imbalance problem is really common. However, this is approached in this repo by only implementing Focal Loss (Distribution Focal Loss or dfl) along with classification loss, the classification loss should as well be WEIGHTED. The other proposed [solution from contributer](https://github.com/ultralytics/ultralytics/issues/6204#issuecomment-2332826348) in the issues is to use weighted data loaders is not efficient since it is more CPU/ GPU consuming. The work should be done here is:\r\n\r\n1. Change Classification loss to be weighted based on the given dataset labels counts (BCEWithLogistics should be weighted). To do this the dataloaders should return the classes weights:\r\n```    \r\ndef get_weights(self):\r\n        \"\"\"\r\n        Count the number of instances per class\r\n\r\n        Returns:\r\n            dict: A dict containing the counts for each class.\r\n        \"\"\"\r\n        counts = [0 for i in range(len(self.data[\"names\"]))]\r\n        for label in self.labels:\r\n            cls = label['cls'].reshape(-1).astype(int)\r\n            for id in cls:\r\n                counts[id] += 1\r\n\r\n        counts = np.array(counts)\r\n        counts = np.where(counts == 0, 1, counts)\r\n\r\n        total_counts = np.sum(counts)\r\n        weights = total_counts / counts\r\n        return weights\r\n```\r\nThis should be passed to the loss classes or set as model attribute:\r\n```\r\nclass v8DetectionLoss:\r\n    \"\"\"Criterion class for computing training losses.\"\"\"\r\n\r\n    def __init__(self, model, tal_topk=10, weight=None):  # model must be de-paralleled\r\n        \"\"\"Initializes v8DetectionLoss with the model, defining model-related properties and BCE loss function.\"\"\"\r\n        device = next(model.parameters()).device  # get model device\r\n        h = model.args  # hyperparameters\r\n\r\n        m = model.model[-1]  # Detect() module\r\n        # print(f\"Defining Loss with weights: {model.classes_weights}\")\r\n        if model.classes_weights is not None:\r\n            self.bce = nn.BCEWithLogitsLoss(reduction=\"none\", weight=torch.from_numpy(model.classes_weights).to(\"cuda\"))\r\n        else:\r\n            self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\r\n```\r\n\r\n\r\n2. [Even more important than point 1] The Fitness score in the Yolo which determines a better model, is calculated as: 0.1xmAP50 + 0.9xmAP50:95. The mAP is over all classes, being affected by the class imbalance problem as well, resulting in having a model with high fitness score but still unweighted and suffers from minority issues. The solution to this is pass the class weights to the `class Metric(SimpleClass):` constructor:\r\n```\r\nclass Metric(SimpleClass):\r\n    \"\"\"\r\n    Class for computing evaluation metrics for YOLOv8 model.\r\n\r\n    Attributes:\r\n        p (list): Precision for each class. Shape: (nc,).\r\n        r (list): Recall for each class. Shape: (nc,).\r\n        f1 (list): F1 score for each class. Shape: (nc,).\r\n        all_ap (list): AP scores for all classes and all IoU thresholds. Shape: (nc, 10).\r\n        ap_class_index (list): Index of class for each AP score. Shape: (nc,).\r\n        nc (int): Number of classes.\r\n\r\n    Methods:\r\n        ap50(): AP at IoU threshold of 0.5 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\r\n        ap(): AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\r\n        mp(): Mean precision of all classes. Returns: Float.\r\n        mr(): Mean recall of all classes. Returns: Float.\r\n        map50(): Mean AP at IoU threshold of 0.5 for all classes. Returns: Float.\r\n        map75(): Mean AP at IoU threshold of 0.75 for all classes. Returns: Float.\r\n        map(): Mean AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: Float.\r\n        mean_results(): Mean of results, returns mp, mr, map50, map.\r\n        class_result(i): Class-aware result, returns p[i], r[i], ap50[i], ap[i].\r\n        maps(): mAP of each class. Returns: Array of mAP scores, shape: (nc,).\r\n        fitness(): Model fitness as a weighted combination of metrics. Returns: Float.\r\n        update(results): Update metric attributes with new evaluation results.\r\n    \"\"\"\r\n\r\n    def __init__(self, classes_weights=None) -> None:\r\n        \"\"\"Initializes a Metric instance for computing evaluation metrics for the YOLOv8 model.\"\"\"\r\n        self.p = []  # (nc, )\r\n        self.r = []  # (nc, )\r\n        self.f1 = []  # (nc, )\r\n        self.all_ap = []  # (nc, 10)\r\n        self.ap_class_index = []  # (nc, )\r\n        self.nc = 0\r\n        self.classes_weights = classes_weights\r\n```\r\nAny weighting across classes should be weighted then. For instance:\r\n```\r\n    @property\r\n    def mp(self):\r\n        \"\"\"\r\n        Returns the Mean Precision of all classes.\r\n\r\n        Returns:\r\n            (float): The mean precision of all classes.\r\n        \"\"\"\r\n        return self.p.mean() if len(self.p) else 0.0\r\n```\r\nShould be:\r\n```\r\n    @property\r\n    def mp(self):\r\n        \"\"\"\r\n        Returns the Mean Precision of all classes.\r\n\r\n        Returns:\r\n            (float): The mean precision of all classes.\r\n        \"\"\"\r\n        return (sum(weight * precision_score for weight, precision_score in zip(self.p, self.classes_weights)) / len(self.p)) if len(self.p) else 0.0\r\n```\r\nThis only issue will be having Precision larger than one but this is a weighted precision and describes the data. The class_weights can be normalized.Or Add weighted metrics be used in fitness score and keep original metrics to be descriptive:\r\n\r\n```\r\n    @property\r\n    def weighted_mp(self):\r\n        \"\"\"\r\n        Returns the Mean Precision of all classes.\r\n\r\n        Returns:\r\n            (float): The mean precision of all classes.\r\n        \"\"\"\r\n        return (sum(weight * precision_score for weight, precision_score in zip(self.p, self.classes_weights)) / len(self.p)) if len(self.p) else 0.0\r\n\r\n    @property\r\n    def weighted_mr(self):\r\n        \"\"\"\r\n        Returns the Mean Recall of all classes.\r\n\r\n        Returns:\r\n            (float): The mean recall of all classes.\r\n        \"\"\"\r\n        return (sum(weight * precision_score for weight, precision_score in zip(self.r, self.classes_weights)) / len(self.r)) if len(self.r) else 0.0\r\n\r\n    @property\r\n    def weighted_map50(self):\r\n        \"\"\"\r\n        Returns the mean Average Precision (mAP) at an IoU threshold of 0.5.\r\n\r\n        Returns:\r\n            (float): The mAP at an IoU threshold of 0.5.\r\n        \"\"\"\r\n        return (self.all_ap[:, 0] * self.class_weights).mean() if len(self.all_ap) else 0.0\r\n\r\n```\r\n\r\n\r\n### Environment\r\n\r\nUltralytics YOLOv8.2.91 🚀 Python-3.10.12 torch-2.4.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060, 12029MiB)\r\nSetup complete ✅ (12 CPUs, 31.2 GB RAM, 172.1/374.8 GB disk)\r\n\r\nOS                  Linux-6.8.0-40-generic-x86_64-with-glibc2.35\r\nEnvironment         Linux\r\nPython              3.10.12\r\nInstall             git\r\nRAM                 31.22 GB\r\nCPU                 Intel Core(TM) i5-10400F 2.90GHz\r\nCUDA                12.4\r\n\r\nnumpy               ✅ 1.23.5<2.0.0,>=1.23.0\r\nmatplotlib          ✅ 3.8.3>=3.3.0\r\nopencv-python       ✅ 4.9.0.80>=4.6.0\r\npillow              ✅ 10.2.0>=7.1.2\r\npyyaml              ✅ 6.0.1>=5.3.1\r\nrequests            ✅ 2.31.0>=2.23.0\r\nscipy               ✅ 1.12.0>=1.4.1\r\ntorch               ✅ 2.4.0+cu124>=1.8.0\r\ntorchvision         ✅ 0.19.0+cu124>=0.9.0\r\ntqdm                ✅ 4.66.5>=4.64.0\r\npsutil              ✅ 5.9.8\r\npy-cpuinfo          ✅ 9.0.0\r\npandas              ✅ 2.2.2>=1.1.4\r\nseaborn             ✅ 0.13.2>=0.11.0\r\nultralytics-thop    ✅ 2.0.0>=2.0.0\r\ntorch               ✅ 2.4.0+cu124!=2.4.0,>=1.8.0; sys_platform == \"win32\"\r\n\r\n### Minimal Reproducible Example\r\n\r\nRunning YOLO with very imbalanced dataset.\r\n\r\n### Additional\r\n\r\n_No response_\r\n\r\n### Are you willing to submit a PR?\r\n\r\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "Y-T-G",
        "body": "> The other proposed solution from contributer in the issues is to use weighted data loaders is not efficient since it is more CPU/ GPU consuming. \n\nWhy would it consume more CPU/GPU? Because it's sampling an index based on the probabilities each time? You can cache the sampled indices for the next N iterations samples if you want then.\n\nYour proposed solution only changes the weights for classification (which I [suggested to someone](https://github.com/ultralytics/ultralytics/issues/15044#issuecomment-2276387717) before and they reported no improvements). And the`weights` argument [isn't for class weights](https://stackoverflow.com/a/71690725/8061030). It's the weight given to each element in the batch, which is likely not in the order of class. You're also not accounting for the box loss, DFL loss and keypoint loss."
      },
      {
        "user": "Zephyr69",
        "body": "Correcting metrics and loss for class imbalance like this should only be a choice, and it is just moving from one assumption to another.\r\n\r\nThe need to do this correction relies on the assumption that the correct recognition of classes that appear more often is more important, which may very well not be the case. In some tasks, it indeed is. In other tasks, all classes may be equally important regardless of their frequencies. And there are also some tasks where the tasks' specific needs decide the class's importance."
      },
      {
        "user": "AWallyAllah",
        "body": "> The need to do this correction relies on the assumption that the correct recognition of classes that appear more often is more important, which may very well not be the case. In some tasks, it indeed is. In other tasks, all classes may be equally important regardless of their frequencies. And there are also some tasks where the tasks' specific needs decide the class's importance.\r\n\r\n@Zephyr69  Yes this is correct, but at least it should be added as argument to enable class importance as you mentioned."
      }
    ]
  },
  {
    "issue_number": 16888,
    "title": "How to adjust weight classes for training?",
    "author": "nichyuen",
    "state": "closed",
    "created_at": "2024-10-14T03:21:15Z",
    "updated_at": "2025-06-17T08:15:09Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi! I will like to know how do I tweak the class weightage parameters as my dataset have imbalance data issues. Some shared it can be done by adjusting the configuration file but no detailed steps were provided. Thank you in advance!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @nichyuen, thank you for reaching out to Ultralytics 🚀! For guidance on adjusting class weights due to data imbalance, our [Documentation](https://docs.ultralytics.com) is a great starting point, and it might have the information you need. \n\nIf this is a ❓ question regarding training, please provide more context about your dataset and any configurations you've tried. Ensuring you're following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) can make a big difference.\n\nJoin our community for real-time help on [Discord](https://ultralytics.com/discord) 🎧, or engage in detailed discussions on [Discourse](https://community.ultralytics.com) or our [Subreddit](https://reddit.com/r/ultralytics).\n\n## Upgrade\n\nMake sure you're using the latest version of `ultralytics` by upgrading your package, along with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml), in a Python>=3.8 environment:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nFor experimenting with various setups, consider using the environments below which have all dependencies pre-installed:\n\n- **Notebooks** with GPU: \n  - [Run on Gradient](https://console.paperspace.com/github/ultralytics/ultralytics)\n  - [Open In Colab](https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb)\n  - [Open In Kaggle](https://www.kaggle.com/models/ultralytics/yolov8)\n  \n- **Cloud Services**:\n  - [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n  - [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n  \n- **Docker Image**: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nThe badge above indicates if all continuous integration tests are passing, which includes testing across different operating systems.\n\nThis response is automated, but an Ultralytics engineer will assist you further soon. 😊"
      },
      {
        "user": "Y-T-G",
        "body": "`ultralytics` doesn't provide a method to change class weights.\n\nYou can try this, but it may or may not be useful:\nhttps://github.com/ultralytics/ultralytics/issues/15044#issuecomment-2276387717\n\nOr you can try this, but this wouldn't be useful if your classes are coupled, for example, an image always has more players than ball, so increasing images don't help with balancing:\nhttps://y-t-g.github.io/tutorials/yolo-class-balancing/"
      },
      {
        "user": "fengecho",
        "body": "> `ultralytics` doesn't provide a method to change class weights.\r\n> \r\n> You can try this, but it may or may not be useful: [#15044 (comment)](https://github.com/ultralytics/ultralytics/issues/15044#issuecomment-2276387717)\r\n> \r\n> Or you can try this, but this wouldn't be useful if your classes are coupled, for example, an image always has more players than ball, so increasing images don't help with balancing: https://y-t-g.github.io/tutorials/yolo-class-balancing/\r\n\r\nFor cases of class imbalance, one approach could be to use a pre-trained model to infer on a portion of the dataset—perhaps half or less—and mask (in solid white or black) the regions of frequently occurring objects, based on the bounding box detections. Do you think this method could be effective?"
      }
    ]
  },
  {
    "issue_number": 21065,
    "title": "ZeroDivisionError: integer division or modulo by zero after modifying the model parameters of yolo11-seg",
    "author": "keDaYao",
    "state": "closed",
    "created_at": "2025-06-16T03:25:31Z",
    "updated_at": "2025-06-17T08:14:55Z",
    "labels": [
      "enhancement",
      "question",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n1、I modified the depth, width, and max_channels parameters in the yolo11-seg.yaml file.\n```yaml\nnc: 80 # number of classes\nscales: # model compound scaling constants, i.e. 'model=yolo11n-seg.yaml' will call yolo11-seg.yaml with scale 'n'\n  # [depth, width, max_channels]\n  n: [0.20, 0.25, 128] \n```\n\n2、The following error occurred after starting the training process.\n```txt\nTraceback (most recent call last):\n  File \"D:\\MyCodes\\25Q3\\ultralytics\\test\\train.py\", line 5, in <module>\n    model = YOLO(r\"./yolo11-seg.yaml\")\n  File \"D:\\MyCodes\\25Q3\\ultralytics\\ultralytics\\models\\yolo\\model.py\", line 79, in __init__\n    super().__init__(model=model, task=task, verbose=verbose)\n  File \"D:\\MyCodes\\25Q3\\ultralytics\\ultralytics\\engine\\model.py\", line 149, in __init__\n    self._new(model, task=task, verbose=verbose)\n  File \"D:\\MyCodes\\25Q3\\ultralytics\\ultralytics\\engine\\model.py\", line 261, in _new\n    self.model = (model or self._smart_load(\"model\"))(cfg_dict, verbose=verbose and RANK == -1)  # build model\n  File \"D:\\MyCodes\\25Q3\\ultralytics\\ultralytics\\nn\\tasks.py\", line 562, in __init__\n    super().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)\n  File \"D:\\MyCodes\\25Q3\\ultralytics\\ultralytics\\nn\\tasks.py\", line 397, in __init__\n    self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelist\n  File \"D:\\MyCodes\\25Q3\\ultralytics\\ultralytics\\nn\\tasks.py\", line 1737, in parse_model\n    m_ = torch.nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module\n  File \"D:\\MyCodes\\25Q3\\ultralytics\\ultralytics\\nn\\modules\\block.py\", line 1516, in __init__\n    self.m = nn.Sequential(*(PSABlock(self.c, attn_ratio=0.5, num_heads=self.c // 64) for _ in range(n)))\n  File \"D:\\MyCodes\\25Q3\\ultralytics\\ultralytics\\nn\\modules\\block.py\", line 1516, in <genexpr>\n    self.m = nn.Sequential(*(PSABlock(self.c, attn_ratio=0.5, num_heads=self.c // 64) for _ in range(n)))\n  File \"D:\\MyCodes\\25Q3\\ultralytics\\ultralytics\\nn\\modules\\block.py\", line 1398, in __init__\n    self.attn = Attention(c, attn_ratio=attn_ratio, num_heads=num_heads)\n  File \"D:\\MyCodes\\25Q3\\ultralytics\\ultralytics\\nn\\modules\\block.py\", line 1331, in __init__\n    self.head_dim = dim // num_heads\nZeroDivisionError: integer division or modulo by zero\n```\n\n3、I tried modifying the code, and now it works. However, I'm a beginner, so I'm not sure what impact this might have on the model.\n```txt\nIndex: ultralytics/nn/modules/block.py\nIDEA additional info:\nSubsystem: com.intellij.openapi.diff.impl.patch.CharsetEP\n<+>UTF-8\n===================================================================\ndiff --git a/ultralytics/nn/modules/block.py b/ultralytics/nn/modules/block.py\n--- a/ultralytics/nn/modules/block.py\t(revision 1784300ef71fde63991f2e423bd68bea71db3dc8)\n+++ b/ultralytics/nn/modules/block.py\t(date 1750043123844)\n@@ -1513,7 +1513,7 @@\n         self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n         self.cv2 = Conv(2 * self.c, c1, 1)\n \n-        self.m = nn.Sequential(*(PSABlock(self.c, attn_ratio=0.5, num_heads=self.c // 64) for _ in range(n)))\n+        self.m = nn.Sequential(*(PSABlock(self.c, attn_ratio=0.5, num_heads=max(self.c // 64, 1)) for _ in range(n)))\n \n     def forward(self, x: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n\n```\nI would like to ask if it is appropriate to modify the yolo11-seg model parameters in this way?\n\nAdditionally, I’m trying to improve the inference speed of yolo-seg on CPU.\nCurrently, I’m using the YOLO11n-seg model with ONNX inference, and it takes about 60ms per image.\nI’m hoping to reduce this to around 10–20ms per image, and I’m willing to trade off some accuracy for better performance.\n\nCould you kindly advise on whether this direction is feasible, and if there are any suggestions for further optimization?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @keDaYao, thank you for your interest in Ultralytics 🚀! We recommend checking out the [Docs](https://docs.ultralytics.com/) for guidance, where you'll find helpful [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples, as well as answers to many common questions.\n\nSince this appears to be a 🐛 Bug Report related to modifying model parameters in the yolo11-seg.yaml file, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) if you haven't already. This will help our team debug the issue more efficiently.\n\nIf your question is related to custom training, please share as much detail as possible, including dataset image examples and training logs, and make sure you're following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community wherever you prefer! For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧. For detailed discussions, try [Discourse](https://community.ultralytics.com/), or check out our [Subreddit](https://reddit.com/r/Ultralytics) to connect with others.\n\n## Upgrade\n\nMake sure you are using the latest `ultralytics` package, including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml), in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following up-to-date, verified environments (with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response. An Ultralytics engineer will review your issue and provide further assistance soon!"
      },
      {
        "user": "Y-T-G",
        "body": "You can try OpenVINO for better speed on CPUs"
      },
      {
        "user": "keDaYao",
        "body": "Unfortunately, I am using an AMD CPU."
      }
    ]
  },
  {
    "issue_number": 17397,
    "title": "Changing class weights in yolov11n ",
    "author": "shrutichakraborty",
    "state": "closed",
    "created_at": "2024-11-07T14:38:44Z",
    "updated_at": "2025-06-17T08:14:53Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nHello! I am training a yolov11n network on my custom dataset with only 1 single class. The confusion matrix I obtain shows that all the background images are incorrectly classified to contain detections. This is potentially due to a class imbalance. In my training set I have 400 imgaes and 58 background images, and in the validation set I have 216 images, and 18 background images. I would like to know how I can update the weights attributed to the classes during training? \r\n\r\nThanks! \r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @shrutichakraborty, thank you for reaching out to the Ultralytics community 🚀! It sounds like you're encountering issues with class imbalance, which can indeed impact the performance of your model. \n\nTo address this, we encourage you to review the [Docs](https://docs.ultralytics.com), specifically the [model training tips](https://docs.ultralytics.com/guides/model-training-tips/) that might help optimize your training process. \n\nIf you suspect this is a bug 🐛, please help us by providing a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/). This would greatly assist us in diagnosing the problem.\n\nIn the meantime, ensure that you have updated to the latest version of `ultralytics` using the following command, as improvements and bug fixes are continually being made:\n\n```bash\npip install -U ultralytics\n```\n\nAdditionally, consider joining our Ultralytics community for more interactive support:\n- For real-time discussions, jump into our [Discord](https://ultralytics.com/discord) 🎧.\n- Engage in deeper discussions on [Discourse](https://community.ultralytics.com).\n- Or share insights and seek advice on our [Subreddit](https://reddit.com/r/ultralytics).\n\nYour issue will soon receive more personalized attention as an Ultralytics engineer reviews it. Thank you for your patience and for contributing to our community! 😊\n\n## Environments\n\nYou can also run YOLO in any of the following verified environments with all dependencies preinstalled:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM.\n- **Amazon** Deep Learning AMI.\n- **Docker Image**: <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, it means all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are passing, verifying the correct operation of all YOLO modes and tasks."
      },
      {
        "user": "Y-T-G",
        "body": "The confusion matrix is not image level.\n\nhttps://github.com/ultralytics/ultralytics/issues/16695#issuecomment-2394795256\n"
      },
      {
        "user": "shrutichakraborty",
        "body": "> The confusion matrix is not image level.\r\n> \r\n> [#16695 (comment)](https://github.com/ultralytics/ultralytics/issues/16695#issuecomment-2394795256)\r\n\r\nHello, yes I understand that. Here is the confsuion matrix I obtain, all instances of background are falsely detected as an object. Therefore I am looking to modify the weight attributed to them while computing loss. Can you tell me more on this? \r\n\r\n![confusion_matrix](https://github.com/user-attachments/assets/4cdf241d-2fec-472e-998d-ddeb8e51f0b8)\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 19453,
    "title": "image_weights",
    "author": "shining-love",
    "state": "closed",
    "created_at": "2025-02-27T05:47:21Z",
    "updated_at": "2025-06-17T08:13:32Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nFirst of all, thank you for your excellent work. Secondly, I would like to know how to set the image_weights parameter in this project using v8, v5, v11, etc., to control the long-tail distribution of data (data imbalance). In previous projects like YOLOv5, there was an image_weights hyperparameter, but I couldn't find where to set and use image_weights in this repository. I look forward to your reply.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @shining-love, thank you for your interest in Ultralytics 🚀! We appreciate your insightful question regarding the `image_weights` parameter.\n\nFor your inquiry:\n\n- If this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it. This will make it easier for us to assist you effectively.\n- If this is a ❓ Question regarding custom training or usage, kindly share additional details to provide more context, such as logs, configurations, or relevant examples.\n\n## Resources\nWe recommend exploring our [Docs](https://docs.ultralytics.com/) for comprehensive guidance. You might find the following sections particularly helpful:\n- [Python Usage](https://docs.ultralytics.com/usage/python/)\n- [CLI Usage](https://docs.ultralytics.com/usage/cli/)\n- [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)\n\nFeel free to also join the Ultralytics community for engagement and support:\n- Discuss your queries on our [Discord](https://discord.com/invite/ultralytics) 🎧 for real-time chat.\n- Connect on [Discourse](https://community.ultralytics.com/) for in-depth discussions.\n- Join the conversation on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge.\n\n## Upgrade and Verify\nMake sure you're using the latest `ultralytics` package to ensure access to the latest features and bug fixes. Upgrade your environment as follows:\n```bash\npip install -U ultralytics\n```\nEnsure your Python environment meets the requirements ([**Python>=3.8**](https://www.python.org/) and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/)).\n\n## Environments\nYOLO models are tested in the following environments. You can try your workflow in these for streamlined performance:\n- **Notebooks with free GPU**: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: See the [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: Follow the [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: Refer to the [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf the CI badge above is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are running smoothly. CI tests validate the behavior of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across multiple platforms.\n\n🔧 Thank you for your patience! Please note that this is an automated response. An Ultralytics engineer will review your question and respond with further assistance soon."
      },
      {
        "user": "Y-T-G",
        "body": "You can't set weights for images in Ultralytics "
      },
      {
        "user": "shining-love",
        "body": "> You can't set weights for images in Ultralytics\n\nHello, how can I implement the image_weights function? I remember that this function was available in previous versions. Why was this function removed in the new version?"
      }
    ]
  },
  {
    "issue_number": 20022,
    "title": "Weighted loss function",
    "author": "pow3rpi",
    "state": "closed",
    "created_at": "2025-04-04T23:41:37Z",
    "updated_at": "2025-06-17T08:12:22Z",
    "labels": [
      "enhancement",
      "question",
      "fixed"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nA year or 2 ago I saw a discussion (here in issues) about how to apply/implement weighted loss function in YOLO models to deal with class imbalance (I experienced the same problem). As I can see today, it hasn't been resolved still, so people have to manually edit source code to use it in their projects (you have to do it after each library installation). If you still cannot configure weights via data.yml or model parameters I would be glad to do it for the community and to implement this feature.\n\n### Use case\n\nClass imbalance\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @pow3rpi, thank you for your interest in Ultralytics 🚀 and for your willingness to contribute to the project!\n\nWeighted loss functions to address class imbalance are indeed an important topic, and we appreciate your initiative in offering to implement this feature for the community. To ensure we fully understand the scope of your proposal, please provide any additional details or specific ideas you have for the feature implementation. For example, how you envision configuring the class weights (e.g., via `data.yml`, model parameters, or another method).\n\nFor reference, you may want to explore our [Docs](https://docs.ultralytics.com/) for information on our library's current structure and functionalities. If you're ready to proceed, you can submit your feature through a Pull Request (PR). To get started, please review our [Contributing Guidelines](https://github.com/ultralytics/ultralytics/blob/main/CONTRIBUTING.md) for best practices.\n\nIn the meantime, we recommend the following actions to ensure your environment is up to date:\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package along with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following verified environments with dependencies preinstalled:\n\n- **Notebooks** with free GPU:  \n  <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)  \n  <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Community Support\n\nFor further discussions and assistance, join our Ultralytics community:\n- [Discord](https://discord.com/invite/ultralytics) for real-time chat 🎧\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with others\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests ensure the reliability of YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu.\n\nThis is an automated response, but don't worry—an Ultralytics engineer will review your issue and provide further guidance soon. Thank you for contributing and being part of the Ultralytics community! 🚀✨"
      },
      {
        "user": "glenn-jocher",
        "body": "@pow3rpi thank you for your interest in enhancing YOLO's capabilities! Our current loss implementations like FocalLoss already support class weighting through the `alpha` parameter, which can help address class imbalance. You can customize this by modifying your model's loss configuration or subclassing the loss function as shown in the Advanced Customization guide. \n\nWe welcome PRs that implement this in a maintainable way without requiring source edits post-installation. Please ensure any changes maintain backward compatibility and follow our Contributing Guide. The community will appreciate your contribution! 🚀"
      },
      {
        "user": "pow3rpi",
        "body": "@glenn-jocher thenk you for clarification. I will take a look."
      }
    ]
  },
  {
    "issue_number": 20259,
    "title": "Handling Extreme Class Imbalance in Custom YOLOv8 Dataset",
    "author": "NEREUScode",
    "state": "closed",
    "created_at": "2025-04-21T11:07:39Z",
    "updated_at": "2025-06-17T08:12:02Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi everyone! I'm training a custom object detection model using YOLOv8 on a dataset that contains a significant class imbalance, and I wanted to get some advice on the best way to handle it.\n\nHere's a breakdown of the class distribution in my dataset:\n\n- **Bottle**: 719  \n- **Plastic fragment**: 356  \n- **Cap**: 195  \n- **Metal**: 6  \n- **Cup**: 10  \n- **Glass bottle**: 79  \n- **Can**: 23  \n- **Tire**: 19  \n- **Wood**: 101  \n- **Tin can**: 12  \n- **Net**: 68  \n- **Bag**: 26  \n- **Paper**: 2  \n- **Cardboard**: 4  \n- **Straw**: 1  \n\nAs you can see, some classes like **Bottle** have hundreds of examples, while others like **Straw**, **Paper**, and **Cardboard** have fewer than 5. I'm worried that this imbalance will cause YOLO to train poorly on the underrepresented classes or even ignore them entirely during inference.\n\nOne of the strategies I'm currently considering is to **apply targeted augmentation only on images containing rare classes**, to try and bring the dataset closer to a balance without overfitting the dominant classes. However, I'm unsure how effective this will be, especially for extremely rare cases like **Straw** (just one instance). I can't find more real samples for these rare classes at the moment, so I'm not sure what the best alternative is.\n\n---\n\n### ❓ My Questions:\n- Is augmenting only the images that contain minority classes a good approach?\n- For classes with **extremely few samples** (like 1 or 2), is there any YOLO-specific trick or technique that helps in such cases (e.g., synthetic generation, class weighting, etc.)?\n- Would it make sense to **temporarily exclude these ultra-rare classes** to improve model focus, or will YOLO still learn something useful from a single example?\n- Is it recommended to use class weights or focal loss with YOLOv8 in this context?\n\nAny thoughts, experience, or suggestions would be super helpful. Thanks a lot in advance!\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @NEREUScode, thank you for reaching out and sharing your detailed dataset breakdown with us! 🚀 This is an automated response to help you get started, and an Ultralytics engineer will review and assist you further soon.\n\nWe recommend starting with the [Docs](https://docs.ultralytics.com/) where you can find guides on [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, including tips on data preparation and augmentation strategies. Many common questions around class imbalance and training best practices are addressed there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate further. For custom training ❓ Questions like yours, sharing additional dataset examples or training logs can help us deliver more targeted advice. Make sure you’re following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community for discussion and real-time support:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for chat\n- [Discourse](https://community.ultralytics.com/) for in-depth topics\n- [Subreddit](https://reddit.com/r/Ultralytics) to connect with other users\n\n## Upgrade\n\nBefore proceeding, ensure you are running the latest release of `ultralytics` and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can run YOLO in any of these up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your question and for using Ultralytics! An engineer will follow up to provide additional guidance on handling extreme class imbalance and your specific training questions. 😊"
      },
      {
        "user": "Y-T-G",
        "body": "You can try this\nhttps://y-t-g.github.io/tutorials/yolo-class-balancing/\n\nBut the lower number of samples is a bigger issue here than imbalance. Artificially increasing the sample through augmentations wouldn't help with the lack of diversity which would affect the model's ability to generalize. If anything, it would cause overfitting."
      },
      {
        "user": "Jordan-Pierce",
        "body": "@NEREUScode can confirm that @Y-T-G 's method works well (but it's also not a miracle worker)"
      }
    ]
  },
  {
    "issue_number": 20838,
    "title": "Class Imbalance",
    "author": "carmecorbi",
    "state": "open",
    "created_at": "2025-05-27T15:49:59Z",
    "updated_at": "2025-06-17T08:11:10Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI am training a YOLOv12 model to detect a single class — the ball — but most detections are being classified as background. I would like to adjust the alpha parameter of the Focal Loss to improve class imbalance.\n\nWhich loss function does YOLOv12 use by default — are all the losses mentioned here used?\n👉 [ultralytics/utils/loss.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/loss.py)\n\nIn this discussion:\n👉 https://github.com/ultralytics/ultralytics/issues/20022\nthey provide the code to implement Focal Loss and to modify alpha. Is that approach correct? I have tried what this discussion says 👉 https://github.com/ultralytics/ultralytics/issues/20022 but it seems like it’s not doing anything. How can I modify the alpha value in the loss?\n\nAlso, could you clarify the function of the single_cls=True parameter during training? I'm working with a single class (the ball), so I wonder if enabling this would help. Does it affect the loss calculation or classification behaviour?\n\n![Image](https://github.com/user-attachments/assets/89ab4b69-e0e6-41b2-8142-b1eb4e430299)\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @carmecorbi, thank you for reaching out and for your detailed questions about training with class imbalance in YOLOv12! 🚀 This is an automated response to help you get started. An Ultralytics engineer will assist you soon with your specific queries.\n\nWe recommend checking our [Docs](https://docs.ultralytics.com/) for up-to-date information, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples. Many common training and customization questions are covered there.\n\nIf this is a 🐛 Bug Report, please assist us by providing a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug the issue.\n\nIf you have a custom training ❓ Question (like this one), please share as much information as possible, such as:\n- Dataset image examples\n- Training logs\n- The exact code or configuration you have modified, especially around the loss function\n\nAlso, please make sure you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin our community for further support and discussion! For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧. For in-depth threads, check out [Discourse](https://community.ultralytics.com/) or join conversations on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nTo ensure you have the latest features and bug fixes (including those related to loss functions and training), please upgrade to the most recent `ultralytics` package along with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO runs in any of these verified environments (with dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for being part of the Ultralytics community!"
      },
      {
        "user": "glenn-jocher",
        "body": "Looking at your questions about class imbalance and loss functions:\n\n**YOLO12 Loss Functions**: YOLO12 (like other YOLO models) uses multiple loss components from `ultralytics/utils/loss.py` - primarily [BboxLoss for bounding boxes](https://docs.ultralytics.com/reference/utils/loss/#ultralytics.utils.loss.BboxLoss), binary cross-entropy for classification, and [DFLoss for distribution focal loss](https://docs.ultralytics.com/reference/utils/loss/#ultralytics.utils.loss.DFLoss) on box regression. The default classification loss is BCE, not Focal Loss.\n\n**Focal Loss Implementation**: The approach in issue #20022 requires custom model modifications since Focal Loss isn't used by default. However, I'd recommend trying YOLO11 instead - YOLO12 is experimental, unstable during training, and significantly slower.\n\n**single_cls Parameter**: Yes, `single_cls=True` should help with your single-class ball detection. It modifies the loss calculation by treating the problem as binary classification (object vs background) rather than multi-class, which can improve performance for single-class scenarios.\n\nFor better results with class imbalance, try training with `single_cls=True` first before implementing custom loss functions, and consider using YOLO11 which is more stable and faster than YOLO12."
      },
      {
        "user": "carmecorbi",
        "body": "Thanks! I’ll try setting single_cls=True to see if it improves the results — right now the model mostly predicts \"background\" and struggles to detect the ball.\n\nRegarding YOLOv11, I have a question:\n👉 Does the classification loss in YOLOv11 use Focal Loss by default, or is it still Binary Cross-Entropy (BCE)?\nI’m asking because I have a strong class imbalance between background and ball, and I think Focal Loss might help.\n\nAlso — how can I debug or visualize which loss functions the model is actually using during training? I’d like to make sure it’s using the expected loss and potentially monitor the classification loss separately.\n\nThanks again!"
      }
    ]
  },
  {
    "issue_number": 20921,
    "title": "Class imbalance issues",
    "author": "RuthKassahun",
    "state": "open",
    "created_at": "2025-06-03T23:33:53Z",
    "updated_at": "2025-06-17T08:10:51Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi everyone! I’m running into an issue with class imbalance in my dataset. My first class has around 15K bounding boxes but my second and third classes are much smaller (1.4K and 600). I worked with a similar imbalanced dataset before and the network worked fairly well after I gave higher class weights for under represented classes\n```\n        class_weights = torch.tensor([0.0,0.3,0.7]).cuda()\n        loss = F.cross_entropy(preds, batch[\"cls\"], weight=class_weights)\n```\n\nThis time around it's performing very poorly. What are the best work around in this situation? Thanks!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @RuthKassahun, thank you for reaching out and sharing your experience with Ultralytics 🚀! Your question about handling class imbalance is a common one, and we appreciate the detailed context you’ve provided.\n\nIf this is a 🐛 Bug Report, please help us by providing a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) so we can investigate further.\n\nIf this is a custom training ❓ Question, please include as much information as possible, such as sample images from your dataset, your training logs, and confirm you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nFor great resources on using Ultralytics models, check out our [Docs](https://docs.ultralytics.com/), including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage guides. Many common training and class imbalance questions are addressed there.\n\nJoin our community for more help and discussion:\n- For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth topics, use [Discourse](https://community.ultralytics.com/)\n- Or join our [Subreddit](https://reddit.com/r/Ultralytics) for community support\n\n## Upgrade\n\nPlease ensure you’re using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these verified environments (dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response. An Ultralytics engineer will review your issue and provide further assistance soon. Thank you for your patience and for contributing to the Ultralytics community!"
      },
      {
        "user": "Y-T-G",
        "body": "The code you highlighted isn't used for detection training. It's for training classification models. So the changes you made there never had any effect."
      },
      {
        "user": "Y-T-G",
        "body": "If you want to adjust weights you can use this\n\nhttps://github.com/ultralytics/ultralytics/issues/15044#issuecomment-2276387717\n\nor you can try this\n\nhttps://y-t-g.github.io/tutorials/yolo-class-balancing/\n\nBut neither of them compensates for lack of sufficient data. Your dataset is not just imbalanced, there's also not enough data for the minority classes.\n\nhttps://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/#dataset"
      }
    ]
  },
  {
    "issue_number": 2863,
    "title": "How to get class probabilities from yolo",
    "author": "joejep",
    "state": "closed",
    "created_at": "2023-05-28T01:27:07Z",
    "updated_at": "2025-06-17T07:46:51Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI have gone through the issues and discussion and this is what i found \r\nhttps://github.com/ultralytics/ultralytics/issues/2423\r\n\r\n\r\nPlease is there a way to get the class probabilities for each class in an image and for multiple images\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @eiyike123, thank you for your interest in YOLOv8 🚀! We recommend a visit to the [YOLOv8 Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) in a [**Python>=3.7**](https://www.python.org/) environment with [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "Hello @eiyike123,\r\nYes, you can get the class probabilities for each class in an image with YOLOv8. You can call the `.pred` property on the `Detect` object, which contains all the predictions made by the model including the class probabilities. \r\nFor multiple images, you can loop over the images and call `.pred` for each image. \r\nPlease note that the class probabilities are already normalized to [0,1] and sum to 1 for each prediction. \r\nI hope this helps! Let me know if you have any further questions."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 21079,
    "title": "How to ignore a specific regions in object detection?",
    "author": "NOTGOOOOD",
    "state": "open",
    "created_at": "2025-06-17T06:41:31Z",
    "updated_at": "2025-06-17T07:43:06Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi, dear developer:\n\nSome datasets contains `ignore regions` bounding boxes annotation, but I haven't found how such annotations can be used in yolo.\n\nI have two ideas for doing this:\n\n1. Fill the 'ignore' area directly with a random color, so that it can be safely considered as the 'background'.\n\n2. Create a new class for the 'ignore region' called '_ignore_', and rewrite the training logic to prevent features in the ignore regions to participate back-propagation.\n\nI'm wondering if some similar operation is already included in the ultralytics library. If not, can you provide some suggestions?\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @NOTGOOOOD, thank you for reaching out and for your interest in Ultralytics 🚀! For your question about handling ignore regions in object detection datasets, we recommend checking the [Docs](https://docs.ultralytics.com/), which provide many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples and might already address similar use-cases.\n\nSince this is a custom training ❓ question, please share as much detail as possible—such as dataset samples, annotation formats, and any relevant training logs—so we can better understand your setup. If you have a specific implementation or a small [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) demonstrating your approach, that would be very helpful!\n\nThis is an automated response 🤖. An Ultralytics engineer will review your issue and assist you further soon.\n\nJoin our community for more support and real-time discussion! Hop onto [Discord](https://discord.com/invite/ultralytics) 🎧, share ideas on [Discourse](https://community.ultralytics.com/), or explore threads on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nFirst, please make sure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "Y-T-G",
        "body": "There's no internal support for this. You would have to modify `loss.py` to add gradient masking:\n\nhttps://github.com/ultralytics/ultralytics/blob/16a20ac09bed1f513841d073a77fbb71eaf10483/ultralytics/utils/loss.py"
      }
    ]
  },
  {
    "issue_number": 21077,
    "title": "why yolo best model metrics p, r = 0, 0?",
    "author": "yeonhyochoi",
    "state": "open",
    "created_at": "2025-06-17T04:46:23Z",
    "updated_at": "2025-06-17T06:44:48Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n![Image](https://github.com/user-attachments/assets/860c870f-a4c2-406f-87ef-49145638d9d7)\n\nWhy is the metric used to select the best model in the yolo detection model like this? In my case, a model with the lowest FP occurrence is more important than the degree of exact overlap of the boxes. How should I approach this?\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @yeonhyochoi, thank you for reaching out and sharing your question with Ultralytics! 🚀 This is an automated response to help guide you, and an Ultralytics engineer will also assist you soon.\n\nWe recommend reviewing the [Docs](https://docs.ultralytics.com/) for detailed explanations of model metrics and selection criteria. You can find useful [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples that might address your scenario.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate further.\n\nIf you’re looking for custom training advice or metric adjustments, please include as much detail as possible: dataset sample images, training logs, and specifics on your evaluation preferences. Also, see our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community for more support! For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧. For in-depth discussions, check out [Discourse](https://community.ultralytics.com/), or share ideas on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nMake sure you’re using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can run YOLO in any of these verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for contributing to Ultralytics! Your feedback helps us improve YOLO and Ultralytics HUB for everyone."
      },
      {
        "user": "Y-T-G",
        "body": "> In my case, a model with the lowest FP occurrence is more important than the degree of exact overlap of the boxes. How should I approach this?\n\nYou can add some weights to precision. By default the weight for that is 0."
      }
    ]
  },
  {
    "issue_number": 21075,
    "title": "How to realize dynamic gesture recognition through yolo",
    "author": "jiokul",
    "state": "open",
    "created_at": "2025-06-17T03:32:33Z",
    "updated_at": "2025-06-17T03:32:55Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHow to realize dynamic gesture recognition through yolo？\nI need to recognize static gestures and dynamic gestures.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @jiokul, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response. An Ultralytics engineer will also assist you soon 😊."
      }
    ]
  },
  {
    "issue_number": 21064,
    "title": "training_loss and val_loss become nan after longtime training",
    "author": "echo385",
    "state": "open",
    "created_at": "2025-06-16T01:59:10Z",
    "updated_at": "2025-06-17T01:54:28Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nWhy does the training end directly become 0, including the loss also becomes nan?\n\n<img width=\"1448\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c42f02a9-1a49-4c15-872f-d7a5a590711b\" />\n<img width=\"1151\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fda3d1e4-22aa-4f79-906d-0ff662291d55\" />\n![Image](https://github.com/user-attachments/assets/3a81b1b4-b03c-40b9-96bc-f632b8d1bc79)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @echo385, thank you for reaching out and sharing your training results with Ultralytics 🚀! Your question is important to us.\n\nWe suggest checking out the [Docs](https://docs.ultralytics.com/) for helpful [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples, as well as answers to many common training issues.\n\nSince this may be a 🐛 Bug Report, could you please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE)? This will help our team investigate the `nan` loss values you’re seeing. Please include:\n- The exact training command used\n- Model and dataset details\n- Relevant logs and, if possible, a snippet of your data or code\n\nIf you're customizing your training or dataset, make sure to share sample images and logs, and reference our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) to rule out common pitfalls.\n\nJoin the Ultralytics community for more support:\n- For real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussion: [Discourse](https://community.ultralytics.com/)\n- For sharing and learning: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nBefore proceeding, please ensure you're using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response 🦾. An Ultralytics engineer will review your issue and provide further assistance soon!"
      },
      {
        "user": "Y-T-G",
        "body": "What's the command you used for training?"
      },
      {
        "user": "echo385",
        "body": "> What's the command you used for training?\n\n`yolo detect train data=datasets/silk_data_V6.yaml model=yolo11l.yaml device=1 imgsz=1280 epochs=3000 batch=16 lr0=0.0001 lrf=0.01 optimizer='Adam' resume=True cache=True workers=16`"
      }
    ]
  },
  {
    "issue_number": 21071,
    "title": "How can classification models be used for images as small as 16x16? I could use some help",
    "author": "leyestd",
    "state": "open",
    "created_at": "2025-06-16T10:19:02Z",
    "updated_at": "2025-06-17T01:43:06Z",
    "labels": [
      "question",
      "classify"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n[](url)\n\n![Image](https://github.com/user-attachments/assets/b6df6991-80ef-4c4f-8bdf-ba836266bec5)\n![Image](https://github.com/user-attachments/assets/7787491e-9ff7-4774-a848-fe7e8c270dde)   \nThe images are very small, what's the best way to process them? Thanks.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @leyestd, thank you for your interest in Ultralytics 🚀! We recommend checking out the [Docs](https://docs.ultralytics.com/) for guidance, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, where many common questions are already addressed.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate further.\n\nIf this is a classification or custom training ❓ Question, please include as much detail as possible—such as more image examples, information about your dataset, model settings, and any relevant training logs. Also, be sure to review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) for optimizing your workflow.\n\nConnect with the Ultralytics community for more support:\n- [Discord](https://discord.com/invite/ultralytics) for real-time chat 🎧\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) to share and learn with others\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to ensure your issue is not already resolved:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response. An Ultralytics engineer will review your question and assist you soon! 😊"
      },
      {
        "user": "Y-T-G",
        "body": "The minimum Ultralytics support is 32x32. The images will be automatically resized to that size."
      },
      {
        "user": "leyestd",
        "body": "Thanks. What methods or parameters are best for detecting defects in a mix of 16x16, 20x20, and 50x50 images? I mainly need this to distinguish between different defect categories. Could you teach me the optimal parameter configuration, or what parts I should modify?\n\n![Image](https://github.com/user-attachments/assets/97b0f377-8925-4b3f-ab3d-dc957af1d938)\n\n![Image](https://github.com/user-attachments/assets/d8188ca1-00db-4a2d-a5f1-765f7c67bb18)"
      }
    ]
  },
  {
    "issue_number": 20833,
    "title": "yolov8-obb 框不全问题",
    "author": "xiaoyao123456789",
    "state": "closed",
    "created_at": "2025-05-27T09:25:40Z",
    "updated_at": "2025-06-17T01:04:48Z",
    "labels": [
      "question",
      "OBB",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n我需要训练针对于道路病害检测的obb模型，但是不论怎么调整数据集，也会存在超长尺寸框会框不全的问题。我尝试了增加P6yaml+yolov8/11-obb.pt训练都是这样，且数据集是严丝合缝处理好的，无问题。具体效果如下图\n\n![Image](https://github.com/user-attachments/assets/4219b944-4ec0-4bad-9c21-cc2e5047e92e)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @xiaoyao123456789, thank you for reaching out and sharing your experience with Ultralytics 🚀! This is an automated response to help you get the fastest assistance possible. An Ultralytics engineer will review your issue and provide further help soon.\n\nWe recommend checking the [Docs](https://docs.ultralytics.com/) for guides, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, which cover many common questions.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/), including the relevant code, a sample of your data, and any logs to help us debug the problem.\n\nIf your question is about custom training or dataset formatting, please share as much detail as possible, such as dataset samples and training logs, and review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community for more support:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for real-time chat\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) for sharing knowledge\n\n## Upgrade\n\nPlease make sure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). Update with:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be used in these verified environments (with all dependencies and [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThanks again for your report and for helping improve Ultralytics!"
      },
      {
        "user": "Y-T-G",
        "body": "You can try smaller imgsz"
      },
      {
        "user": "xiaoyao123456789",
        "body": "> 你可以尝试更小的 imgsz\n\n您好，我有尝试640的，但是效果并没有改进。\n\n下图是我使用640尺寸训练40epoch时模型推理效果\n![Image](https://github.com/user-attachments/assets/e491de46-166d-4cba-abee-cb3b53d12d1c)\n\n\n更困惑的是到60epoch时模型推理效果如下，更差了\n![Image](https://github.com/user-attachments/assets/2c641093-a64c-4b14-b99e-f15e21ac6605)\n\n我看到了类似朋友的疑问，与我的有些相似，目前我也更改了reg_max=1，但是没理解为啥这位朋友说改成1为关闭dfl loss\nhttps://github.com/ultralytics/ultralytics/issues/8962\n"
      }
    ]
  },
  {
    "issue_number": 18616,
    "title": "Question about self.loss *= world_size in trainer",
    "author": "wcyjerry",
    "state": "open",
    "created_at": "2025-01-09T12:55:27Z",
    "updated_at": "2025-06-17T00:24:26Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi there,\r\nRecently, I'm reading your code to build my own framework, because some function's implementation is awesome.\r\nI have little question here in trainer.py line 378:\r\n`\r\n  # Forward\r\n  with autocast(self.amp):\r\n      batch = self.preprocess_batch(batch)\r\n      self.loss, self.loss_items = self.model(batch)\r\n      if RANK != -1:\r\n          self.loss *= world_size\r\n      self.tloss = (\r\n          (self.tloss * i + self.loss_items) / (i + 1) if self.tloss is not None else self.loss_items\r\n      )\r\n\r\n  # Backward\r\n  self.scaler.scale(self.loss).backward()\r\n`\r\nwhy there using additional: self.loss *= world_size? \r\nIf we want to sum the loss between all local_rank, as far as I known, ddp mode will do all-reduce(op=sum) automatically.\r\nAnd even it doesn't, loss * world_size doesn't get correct loss sum, just make single_rank loss bigger. \r\n\r\nIt will be of great help if you can explain it to me thx.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @wcyjerry, thank you for your interest in Ultralytics 🚀! We appreciate your detailed question about the `self.loss *= world_size` implementation.\n\nWe recommend reviewing the [Docs](https://docs.ultralytics.com) to explore our project further, including detailed guides on [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage. These resources may provide additional context for understanding our implementation choices and training workflow.\n\nTo clarify your question, this appears to be an area for potential discussion or detailed explanation. Since this is not a direct bug report or MRE-related issue, a deeper dive into the logic of distributed training as implemented in Ultralytics might require further context from the team. An Ultralytics engineer will assist you soon to provide a precise explanation for the `self.loss *= world_size` behavior in our distributed training loop.\n\n### Community Support\nMeanwhile, feel free to connect with our vibrant community to discuss and collaborate further:\n- Join us on [Discord](https://discord.com/invite/ultralytics) 🎧 for real-time discussions.\n- Share your insights or follow similar threads on our [Discourse](https://community.ultralytics.com).\n- Explore or post about related topics in our [Subreddit](https://reddit.com/r/Ultralytics).\n\n### Reference Environment\nAs a gentle reminder, ensure you are running the latest `ultralytics` package with a [**Python>=3.8**](https://www.python.org/) and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) environment to avoid discrepancies with older versions. You can upgrade using the following command:\n\n```bash\npip install -U ultralytics\n```\n\nYou can also explore our recommended training environments:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n\n### CI Status\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>  \nYou can monitor the CI badge to verify the operational status of all functionalities tested across platforms.\n\nThis is an automated response 🤖. An Ultralytics engineer will follow up with more details soon. Stay tuned, and thank you for engaging with us! 😊"
      },
      {
        "user": "glenn-jocher",
        "body": "@wcyjerry our current implementation ensures consistency across different batch sizes and DDP world sizes, so i.e. if you train the following commands all losses (individually and collectively) will be directly comparable:\r\n\r\n- `yolo train device=0 batch=64`\r\n- `yolo train device=0 batch=32`  # different batch size\r\n- `yolo train device=0,1 batch=128`  # different world size and different batch size\r\n\r\nIf there is a comparable more seamless torch solution we'd be happy to investigate. Please submit a PR with overlaid losses we can use to analyze before and after results at different batch and world sizes."
      },
      {
        "user": "wcyjerry",
        "body": "@glenn-jocher \r\nStill comfusing, what you mean directly comparable. But, I have done some small experiments.\r\n- exp1, batch=32, device=0, 20epochs, box_loss: 0.59 cls_loss:0.38 pre:0.72 rec: 0.48\r\n- exp2, batch=64, device=1, 20epochs, box_loss: 0.59 cls_loss:0.38 pre:0.73 rec: 0.51\r\n- exp3, batch=64, device=2,3 20epochs, box_loss: 0.74 cls_loss: 0.48, pre:0.68 rec:0.35\r\n- exp4, batch=128, device=2,3 20epochs, box_loss: 0.81, cls_lss:0.52, pre:0.70 rec:0.32\r\nSeems like ddp converge badly.\r\nAnother thing, I have found in loss code: \r\nloss[0] *= self.hyp.box  # box gain\r\nloss[1] *= self.hyp.cls  # cls gain\r\nloss[2] *= self.hyp.dfl  # dfl gain\r\nreturn loss.sum() * batch_size, loss.detach()  # loss(box, cls, dfl).\r\nwhy there should times batch_size, if we want to make loss is irrelevant to batch_size?\r\nin exp1 and exp2, exp2's loss should be twice of exp1's from code? but they seems in the same scale. could you help me with this?\r\n"
      }
    ]
  },
  {
    "issue_number": 18660,
    "title": "Missing labels in batches when using copy_paste and mosaic",
    "author": "uuu686",
    "state": "open",
    "created_at": "2025-01-13T12:22:54Z",
    "updated_at": "2025-06-17T00:24:24Z",
    "labels": [
      "question",
      "Stale",
      "segment"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nhello,\r\nwhen I use my custom data to train yolo11 segment task, I find something wrong with the train_batch.jpg. Just like\r\n![111](https://github.com/user-attachments/assets/7f977639-b791-40e4-bf96-6b33b574d45d)\r\nThe chips label is missing.\r\nI have checked my dataset label and there is no problem, and why is that?\r\nNow because the data split results are inconsistent, I can not reproduce for the time being.\r\nThanks!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @uuu686, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, it would be very helpful if you could provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug the issue. Specifically, if there are inconsistencies in the data splitting results, please try to replicate the setup where this occurs and share any relevant code, configuration files, or dataset information.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including screenshots (like the one attached), dataset examples, as well as training logs. Additionally, ensure you're adhering to our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n### Clarifications and Recommendations\n- Ensure your environment uses the latest `ultralytics` package and dependencies by upgrading:\n```bash\npip install -U ultralytics\n```\n- This ensures that your issue has not already been resolved in the latest release.\n\n### Join the Community\nWe encourage you to leverage the Ultralytics community for further assistance and lively discussions. Hop into:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for real-time support\n- [Discourse](https://community.ultralytics.com) for detailed discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) for engaging community threads\n\n## Run Environments\nYOLO may also be tested seamlessly in verified environments that come pre-configured, such as:\n- Free GPU-powered **Notebooks**: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- Cloud Services: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/) 🌥️ or [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- Pre-integrated Docker Images: <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nLastly, rest assured that this is an automated response 😊, and an Ultralytics engineer will review your issue and assist you shortly!"
      },
      {
        "user": "Y-T-G",
        "body": "What version of Ultralytics is this?"
      },
      {
        "user": "uuu686",
        "body": "I did not use the Ultralytics pip package, I just clone the default repository to do this."
      }
    ]
  },
  {
    "issue_number": 20358,
    "title": "Fusion detection of infrared light images and visible light images",
    "author": "HTU2024",
    "state": "open",
    "created_at": "2025-04-26T03:36:46Z",
    "updated_at": "2025-06-17T00:24:16Z",
    "labels": [
      "enhancement",
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nHello, I would like to achieve the fusion detection of infrared light and visible light through the YOLO series network to enhance the performance of detecting visible light images separately. May I ask if there are any methods to fuse infrared images?\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @HTU2024, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\n⚡️ This is an automated response to help you get started. An Ultralytics engineer will also review your question and provide further assistance soon!"
      },
      {
        "user": "Y-T-G",
        "body": "Did you try adding it as extra channel?\n\n"
      },
      {
        "user": "HTU2024",
        "body": "> Did you try adding it as extra channel?\n\nI want to extract different forms of information through different feature extraction backbones and then fuse them. For example, after extracting RGB images and infrared images, fuse them.\nAre you talking about directly detecting the information of infrared images?"
      }
    ]
  },
  {
    "issue_number": 20360,
    "title": "All validation losses going up (`seg_loss`, `dfl_loss`, `box_loss`) except for `class_loss`",
    "author": "massisenergy",
    "state": "closed",
    "created_at": "2025-04-26T08:39:43Z",
    "updated_at": "2025-06-17T00:24:15Z",
    "labels": [
      "question",
      "Stale",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI'm not sure why the model is learning about classification (decreasing loss) but other losses are increasing?\n\n![Image](https://github.com/user-attachments/assets/d76a48a2-3b80-4d3a-b45d-d0e42481094c)\n\n```\n$ awk 'NR<61' w16_241class1_24bkg_251b1pL_500w10b2_934w11b3_431w12b4_dummy2500train50val_imgsz1280ep600/w16_yolo_trLe_241class1_24bkg_251b1pL_500w10b2_934w11b3_431w12b4_dummy2500train50val_imgsz1280ep600.log\nNew https://pypi.org/project/ultralytics/8.3.111 available 😃 Update with 'pip install -U ultralytics'\nUltralytics 8.3.99 🚀 Python-3.11.7 torch-2.6.0+cu124 CUDA:1 (NVIDIA A40, 45518MiB)\nengine/trainer: task=segment, mode=train, model=/data1/icmr/sourav/workspace/projects/Yolo/pretrainedModelsUltralytics/yolo11x-seg.pt, data=/data1/icmr/sourav/workspace/Datasets/yolo_trLe_241class1_24bkg_251b1pL_500w10b2_934w11b3_431w12b4_dummy2500train50val/data.yaml, epochs=600, time=None, patience=600, batch=10, imgsz=1280, save=True, save_period=100, cache=False, device=1, workers=8, project=w16_241class1_24bkg_251b1pL_500w10b2_934w11b3_431w12b4_dummy2500train50val_imgsz1280ep600, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=w16_241class1_24bkg_251b1pL_500w10b2_934w11b3_431w12b4_dummy2500train50val_imgsz1280ep600/train\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments\n  0                  -1  1      2784  ultralytics.nn.modules.conv.Conv             [3, 96, 3, 2]\n  1                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]\n  2                  -1  2    389760  ultralytics.nn.modules.block.C3k2            [192, 384, 2, True, 0.25]\n  3                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]\n  4                  -1  2   1553664  ultralytics.nn.modules.block.C3k2            [384, 768, 2, True, 0.25]\n  5                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]\n  6                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]\n  7                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]\n  8                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]\n  9                  -1  1   1476864  ultralytics.nn.modules.block.SPPF            [768, 768, 5]\n 10                  -1  2   3264768  ultralytics.nn.modules.block.C2PSA           [768, 768, 2]\n 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']\n 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 13                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]\n 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']\n 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 16                  -1  2   1700352  ultralytics.nn.modules.block.C3k2            [1536, 384, 2, True]\n 17                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]\n 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 19                  -1  2   5317632  ultralytics.nn.modules.block.C3k2            [1152, 768, 2, True]\n 20                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]\n 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]\n 22                  -1  2   5612544  ultralytics.nn.modules.block.C3k2            [1536, 768, 2, True]\n 23        [16, 19, 22]  1   8323187  ultralytics.nn.modules.head.Segment          [1, 32, 384, [384, 768, 768]]\nYOLO11x-seg summary: 379 layers, 62,051,411 parameters, 62,051,395 gradients, 319.7 GFLOPs\n\nTransferred 1071/1077 items from pretrained weights\nTensorBoard: Start with 'tensorboard --logdir w16_241class1_24bkg_251b1pL_500w10b2_934w11b3_431w12b4_dummy2500train50val_imgsz1280ep600/train', view at http://localhost:6006/\nFreezing layer 'model.23.dfl.conv.weight'\nAMP: running Automatic Mixed Precision (AMP) checks...\nAMP: checks passed ✅\ntrain: Scanning /data1/icmr/sourav/workspace/Datasets/yolo_trLe_241class1_24bkg_251b1pL_500w10b2_934w11b3_431w12b4_dummy2500train50val/labels/train... 2678 images, 2517 backgrounds, 0 corrupt: 100%|██████████| 5177/5177 [00:04<00:00, 1122.47it/s]\ntrain: New cache created: /data1/icmr/sourav/workspace/Datasets/yolo_trLe_241class1_24bkg_251b1pL_500w10b2_934w11b3_431w12b4_dummy2500train50val/labels/train.cache\nval: Scanning /data1/icmr/sourav/workspace/Datasets/yolo_trLe_241class1_24bkg_251b1pL_500w10b2_934w11b3_431w12b4_dummy2500train50val/labels/val... 52 images, 57 backgrounds, 0 corrupt: 100%|██████████| 102/102 [00:00<00:00, 1580.54it/s]\nval: New cache created: /data1/icmr/sourav/workspace/Datasets/yolo_trLe_241class1_24bkg_251b1pL_500w10b2_934w11b3_431w12b4_dummy2500train50val/labels/val.cache\nPlotting labels to w16_241class1_24bkg_251b1pL_500w10b2_934w11b3_431w12b4_dummy2500train50val_imgsz1280ep600/train/labels.jpg...\noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically...\noptimizer: SGD(lr=0.01, momentum=0.9) with parameter groups 176 weight(decay=0.0), 187 weight(decay=0.00046875), 186 bias(decay=0.0)\nTensorBoard: model graph visualization added ✅\nImage sizes 1280 train, 1280 val\nUsing 8 dataloader workers\nLogging results to w16_241class1_24bkg_251b1pL_500w10b2_934w11b3_431w12b4_dummy2500train50val_imgsz1280ep600/train\nStarting training for 600 epochs...\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      1/600      40.5G      1.807      3.058      5.631      2.201          7       1280: 100%|██████████| 518/518 [12:22<00:00,  1.43s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:04<00:00,  1.47it/s]\n                   all        102         46      0.686      0.523      0.633      0.287      0.689      0.531      0.629      0.296\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n      2/600        43G       1.74      2.538      2.047      2.004         10       1280: 100%|██████████| 518/518 [12:43<00:00,  1.47s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:03<00:00,  1.61it/s]\n                   all        102         46        0.4      0.522      0.445      0.157       0.42      0.543      0.456      0.163\n\n```\n\n### Additional\n\nMethod:\n- transfer learning with yolo11x-seg.pt\n- one class classification+segmentation\n- with variable number of dummy images (without any corresponding .txt files)\n- without the dummy images the training was going fine (all loss decreasing, also predictions were good)",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @massisenergy, thank you for reaching out and providing detailed logs and background info! 🚀 This is an automated response to help streamline your issue—an Ultralytics engineer will assist you soon.\n\nWe recommend visiting the [Docs](https://docs.ultralytics.com/) where common questions and troubleshooting strategies are outlined. If you haven’t already, please review the [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) for guidance on custom training setups.\n\nSince this appears to be a 🐛 Bug Report, please provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/) if possible. This helps us investigate and resolve your issue more efficiently. Key details to include:\n- A snippet or script that reproduces the increasing loss issue\n- A few sample images (and label files) from your dataset\n- Any relevant config files or hyperparameter changes\n\nJoin our community for faster feedback and collaborative support:\n- [Discord](https://discord.com/invite/ultralytics) for real-time chat 🎧\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) for sharing and learning with others\n\n## Upgrade\n\nMake sure you are running the latest `ultralytics` package and dependencies ([requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml)) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following verified environments (with all dependencies preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThanks again for your report! An Ultralytics engineer will review your issue and follow up soon."
      },
      {
        "user": "glenn-jocher",
        "body": "This appears to be related to your use of dummy images without corresponding annotation files. While your model is improving at classification (decreasing cls_loss), it's struggling with localization and segmentation, leading to increasing box_loss, seg_loss, and dfl_loss.\n\nThe dummy images without annotations are likely confusing the model - it sees objects but has no ground truth for their locations or segments. During training, this creates a conflict where the model tries to predict boxes/masks for objects in unannotated images, but then gets penalized since there are no ground truth annotations to match with.\n\nI'd recommend either:\n1. Remove the dummy images from your dataset\n2. Create proper annotations for all dummy images\n3. If you need to keep unannotated images, consider using a semi-supervised approach with pseudo-labeling\n\nFor segmentation tasks especially, having accurate mask annotations is critical for proper training."
      },
      {
        "user": "massisenergy",
        "body": "Thanks for your quick response! \n\n> 1. Remove the dummy images from your dataset\n\n -- I need to have those 2500 images because, in the previous prediction cycle the model identified some areas in them. But these 2500 images are actually false positives (shouldn't have any annotation/detection at all). So, the rationale is if I include those as dummy the model will learn better?\n\nHowever, I'm not sure if I understand correctly:\n\n> 2. Create proper annotations for all dummy images\n\n -- Meaning, create totally empty `file.txt` and place in the train folder, where `file` corresponds to particular image name for each of the 2500 dummies? \n\n> 3. If you need to keep unannotated images, consider using a semi-supervised approach with pseudo-labeling\n\n -- Can you please elaborate on this? Any links to understand it also will be fine.\n"
      }
    ]
  },
  {
    "issue_number": 20387,
    "title": "YOLO11 detection loss",
    "author": "PasekRastislav",
    "state": "closed",
    "created_at": "2025-04-28T01:01:11Z",
    "updated_at": "2025-06-17T00:24:14Z",
    "labels": [
      "enhancement",
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI am trying to implement modified version of IA-YOLO(https://github.com/wenyyu/Image-Adaptive-YOLO) using YOLO11l among other modifications in the whole structure. I want to use YOLO's detection loss for training of the enhancement model to get even better results but I am having trouble accessing and using the detection loss in my pipeline. Maybe I am missing something, but is there some simple way or something how can I use it? \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @PasekRastislav, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response to help guide you more quickly 😊. An Ultralytics engineer will also review your issue and provide further assistance soon."
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @PasekRastislav,\n\nTo access YOLO11's detection loss for your IA-YOLO modification, you can look at the implementation in `ultralytics/utils/loss.py`. The detection losses in YOLO11 are implemented in the `DetectionLoss` class.\n\nThe most direct approach would be to:\n\n1. Import the loss class: `from ultralytics.utils.loss import DetectionLoss`\n2. Initialize it with your hyperparameters\n3. Use it in your enhancement model training pipeline\n\nIf you need to customize it further, you might want to subclass `DetectionLoss` and override specific methods. The main components include box loss, classification loss, and distribution focal loss.\n\nFor an example of how the loss is used in training, check the `DetectionTrainer` class in the codebase which shows how predictions and targets are formatted before passing to the loss function.\n\nLet me know if you need more specific guidance about any particular part of the implementation."
      },
      {
        "user": "PasekRastislav",
        "body": "Hello, thank you for your response, maybe I am wrong but in the \"ultralytics.utils.loss\" only class that has DetectionLoss is \"v8DetectionLoss:\" did you mean that one? I want to use it in training pipeline for the enhancement to get even better results. I have annotated dataset with paired images(adverse/clear) where I want to enhance the adverse image to be as close to target(clear) image as possible. Currently I am using histogram loss for that and I wanted to integrate also the detection loss from YOLO11 so it can learn also from that how the enhancements improve or worsen the visibility for detection. I am having trouble with this part of code as it throws error saying : AttributeError in loss function: 'dict' object has no attribute 'box'\n\"loss[0] *= self.hyp.box  # box gain\n  loss[1] *= self.hyp.cls  # cls gain\n  loss[2] *= self.hyp.dfl  # dfl gain\"\nMaybe I am trying to access it the wrong way, if you could help me with that, I would be grateful as my deadline for this project is close. "
      }
    ]
  },
  {
    "issue_number": 20412,
    "title": "Tesla T4 infer/second for YOLO",
    "author": "MiqdaadIndori",
    "state": "closed",
    "created_at": "2025-04-29T05:29:25Z",
    "updated_at": "2025-06-17T00:24:13Z",
    "labels": [
      "question",
      "Stale",
      "detect",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nsince YOLO is made for speed, are there any benchmarks on how many images/second we can get on certain GPUs? \nfor example I am using the Tesla T4 for now, how many images/second should I expect? \nthis is important for me as to ascertain the cost and also whether I  am using an efficient architecture. right now with Nvidia triton and the yolo11n onnx with the tensorRT optimization I get about 70 infer/sec which I feel is pretty low.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @MiqdaadIndori, thank you for your interest in Ultralytics 🚀! For benchmarking performance and deployment questions, you may find helpful information in the [Docs](https://docs.ultralytics.com/), especially around [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage as well as deployment guides.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) so our team can investigate more efficiently.\n\nFor performance and custom deployment ❓ questions, please include:\n- Details about your inference pipeline (batch size, image size, model settings)\n- Example input data and logs\n- Any relevant code snippets or deployment configs\n\nYou’re also welcome to join the Ultralytics community for more discussion and knowledge sharing:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for live chat\n- [Discourse](https://community.ultralytics.com/) for detailed threads\n- [Subreddit](https://reddit.com/r/Ultralytics) to engage with others\n\n## Upgrade\n\nMake sure you’re running the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). This helps ensure you benefit from the latest speed and compatibility improvements:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can run YOLO in various verified environments with preinstalled dependencies, including:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response. An Ultralytics engineer will review your question and assist you soon! 😊"
      },
      {
        "user": "Y-T-G",
        "body": "You can run benchmark with Ultralytics by using\n\n`model.benchmark(format=\"engine\", half=True)`\n\nDid you export with `half=True`?"
      },
      {
        "user": "MiqdaadIndori",
        "body": "so when I export with format=engine, Triton is not able to read it. what I did was export as ONNX and then use the tensorrt optimization which converts the model to to tensorRT format. I am getting 70 infer/sec with this for yolo11n and I have tried with different batch sizes, like 4 and 8."
      }
    ]
  },
  {
    "issue_number": 20422,
    "title": "After RTDETR convert to torchscript using export(), inference error",
    "author": "ynhuhu",
    "state": "closed",
    "created_at": "2025-04-29T13:05:03Z",
    "updated_at": "2025-06-17T00:24:12Z",
    "labels": [
      "question",
      "Stale",
      "detect",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nWhen inferring using torchscript，_7 = torch.arange(annotate(number, pos_dim), dtype=6, layout=None, device=torch.device(\"cpu\"), pin_memory=False)\n_8 = torch.pow(10000., torch.div(_7, pos_dim))\n~~~~~~~~~ <--- HERE\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cpu!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @ynhuhu, thank you for reaching out and for providing details about your experience with Ultralytics 🚀! This is an automated response to help guide you while an Ultralytics engineer reviews your issue.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) if you haven't already. An MRE will help us pinpoint and resolve the issue more efficiently.\n\nIf your question is about custom training or export workflows, please share as much context as possible—such as config files, code snippets, and logs—to assist our team in troubleshooting.\n\nFor new users, or if you haven't already, we recommend checking out the [Docs](https://docs.ultralytics.com/) where you’ll find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, as well as answers to common questions.\n\nJoin the Ultralytics community for real-time support and discussion:\n- [Discord](https://discord.com/invite/ultralytics) 🎧\n- [Discourse](https://community.ultralytics.com/)\n- [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nEnsure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). Run the following command to upgrade:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in any of these up-to-date verified environments (with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nAn Ultralytics engineer will assist you soon. Thank you for your patience and for using Ultralytics!"
      },
      {
        "user": "Y-T-G",
        "body": "If you export with `torchscript`, you can only infer on the same device it was exported on."
      },
      {
        "user": "ynhuhu",
        "body": "model.export(format=\"torchscript\", imgsz=img_size, device='cuda'). This is the script I exported"
      }
    ]
  },
  {
    "issue_number": 20427,
    "title": "Mismatch between YOLO's exported engine version and local TensorRT version",
    "author": "CrazyLearner98",
    "state": "open",
    "created_at": "2025-04-30T02:07:24Z",
    "updated_at": "2025-06-17T00:24:11Z",
    "labels": [
      "question",
      "Stale",
      "detect",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI encountered an issue when exporting the YOLO engine using the same TensorRT version as my local setup. Despite using the same machine environment and TensorRT version for both the export and inference processes, I faced an error when trying to load the exported engine file during inference.\n\nI use official export function to get the engine file like:\nmodel = YOLO(\"yolo11l.pt\")  # load an official model\nmodel = YOLO(r\"C:\\Users\\97151\\Desktop\\work\\road\\yolo11l.pt\")  # load a custom trained model\nmodel.export(format=\"engine\", half=True, device=0, dynamic=True)\nand it shows:\nTensorRT: starting export with TensorRT 10.9.0.34...\n[04/30/2025-09:28:34] [TRT] [I] [MemUsageChange] Init CUDA: CPU +5, GPU +0, now: CPU 12585, GPU 1745 (MiB)\n[04/30/2025-09:28:37] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +4253, GPU +444, now: CPU 17113, GPU 2189 (MiB)\n[04/30/2025-09:28:37] [TRT] [I] ----------------------------------------------------------------\n[04/30/2025-09:28:37] [TRT] [I] Input filename:   C:\\Users\\97151\\Desktop\\work\\road\\yolo11l.onnx\n[04/30/2025-09:28:37] [TRT] [I] ONNX IR version:  0.0.9\n[04/30/2025-09:28:37] [TRT] [I] Opset version:    19\n[04/30/2025-09:28:37] [TRT] [I] Producer name:    pytorch\n[04/30/2025-09:28:37] [TRT] [I] Producer version: 2.6.0\n[04/30/2025-09:28:37] [TRT] [I] Domain:\n[04/30/2025-09:28:37] [TRT] [I] Model version:    0\n[04/30/2025-09:28:37] [TRT] [I] Doc string:       \n[04/30/2025-09:28:37] [TRT] [I] ----------------------------------------------------------------\nTensorRT: input \"images\" with shape(-1, 3, -1, -1) DataType.FLOAT\nTensorRT: output \"output0\" with shape(-1, 84, -1) DataType.FLOAT\nTensorRT: WARNING ⚠️ 'dynamic=True' model requires max batch size, i.e. 'batch=16'\nTensorRT: building FP16 engine as C:\\Users\\97151\\Desktop\\work\\road\\yolo11l.engine\n[04/30/2025-09:28:37] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n[04/30/2025-09:30:34] [TRT] [I] Compiler backend is used during engine build.\n[04/30/2025-09:32:07] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n[04/30/2025-09:32:11] [TRT] [I] Total Host Persistent Memory: 991024 bytes\n[04/30/2025-09:32:11] [TRT] [I] Total Device Persistent Memory: 0 bytes\n[04/30/2025-09:32:11] [TRT] [I] Max Scratch Memory: 2973184 bytes\n[04/30/2025-09:32:11] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 306 steps to complete.\n[04/30/2025-09:32:11] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 20.0294ms to assign 11 blocks to 306 nodes requiring 37479424 bytes.\n[04/30/2025-09:32:11] [TRT] [I] Total Activation Memory: 37478400 bytes\n[04/30/2025-09:32:11] [TRT] [I] Total Weights Memory: 50718088 bytes\n[04/30/2025-09:32:11] [TRT] [I] Compiler backend is used during engine execution.\n[04/30/2025-09:32:11] [TRT] [I] Engine generation completed in 213.575 seconds.\n[04/30/2025-09:32:11] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 4 MiB, GPU 261 MiB\nTensorRT: export success ✅ 232.8s, saved as 'C:\\Users\\97151\\Desktop\\work\\road\\yolo11l.engine' (54.3 MB)  \n\nBut when I use the tensorrt file, it will shows the error:[04/30/2025-09:58:11] [TRT] [E] IRuntime::deserializeCudaEngine: Error Code 1: Serialization (Serialization assertion header.magicTag == rt::PlanMagicTag::kPLAN_MAGIC_TAG failed.Trying to load an engine created with incompatible serialization version (1817 != 1953657958). Check that the engine was not created using safety runtime, same OS was used and version compatibility parameters were set accordingly and that it is a TRT engine file.)\nTraceback (most recent call last):\n  File \"C:\\Users\\97151\\Desktop\\work\\road\\tensorrt_monitor.py\", line 302, in <module>\n    process_video(video_file,output_folder)\n  File \"C:\\Users\\97151\\Desktop\\work\\road\\tensorrt_monitor.py\", line 127, in process_video\n    yolov11l_engine_context = load_engine(\"yolo11l.engine\")\n  File \"C:\\Users\\97151\\Desktop\\work\\road\\tensorrt_monitor.py\", line 39, in load_engine\n    context = engine.create_execution_context()\nAttributeError: 'NoneType' object has no attribute 'create_execution_context'\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @CrazyLearner98, thank you for reaching out and sharing your detailed experience with Ultralytics 🚀! This is an automated response to help guide you while an Ultralytics engineer reviews your issue.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) if you haven't already. This will help us investigate and resolve the issue more efficiently.\n\nIf your question is related to custom training or deployment, please include as much detail as possible, such as your exact TensorRT and CUDA versions, relevant code snippets, and any custom modifications. Double-check our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) for best practices.\n\nFor further support and to connect with the Ultralytics community:\n- Join [Discord](https://discord.com/invite/ultralytics) for real-time chat 🎧\n- Visit [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- Engage with others on our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nMake sure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). This helps ensure any recent fixes or improvements are included:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nAn Ultralytics engineer will assist you soon. Thank you for your patience and for helping improve Ultralytics!"
      },
      {
        "user": "ctx289",
        "body": "Can you show your CUDA and TensorRT versions, as well as the locally supported CUDA version, to ensure consistency? Also, make sure that the TensorRT version used when executing the engine matches the TensorRT version used during conversion."
      },
      {
        "user": "Y-T-G",
        "body": "You can remove these lines before export\n\nhttps://github.com/ultralytics/ultralytics/blob/fba1b3f513da0aafc2d821eb4475f4ac5d8a238c/ultralytics/utils/export.py#L216-L217"
      }
    ]
  },
  {
    "issue_number": 20429,
    "title": "Consistent low recall on medical dataset",
    "author": "mw-boop",
    "state": "open",
    "created_at": "2025-04-30T09:05:13Z",
    "updated_at": "2025-06-17T00:24:10Z",
    "labels": [
      "question",
      "Stale",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi everyone, \n\nI've been training the YOLO-v11s-seg on a medical dataset. After trying out a lot and after just finishing a grid search, I still get an incredibly low sensitivity. My specificity and Dice scores are alright. First things first:\n\n**DATASET**\nDataset: train/val split: \nTrain set: 4480 images, 3364 healthy, 1116 tumor\nVal: 4804 images, 2859 healthy, 1945 tumor. \n\nThe validation set was hand picked to include all sorts of different tumors: small, big, hard/easy to segment by hand. \n\nWhat is good to know is that by default EVERY frame has only ONE tumor. \n\nAll frames went through the exact same preprocessing (normalization) and are the same size/recorded with the exact same system parameters.\n\n**TRAINING ARGS**\n\ntask: segment\nmode: train\nmodel: yolo11s-seg.pt\nepochs: 150\ntime: null\npatience: 500\nbatch: 32\nimgsz: 384\nsave: true\nsave_period: -1\ncache: false\ndevice: cuda\nworkers: 10\nexist_ok: false\npretrained: true\noptimizer: auto\nverbose: true\nseed: 0\ndeterministic: true\nsingle_cls: true\nrect: false\ncos_lr: false\nclose_mosaic: 0\nresume: false\namp: true\nfraction: 1.0\nprofile: false\nfreeze: null\nmulti_scale: false\noverlap_mask: true\nmask_ratio: 4\ndropout: 0.1\nval: true\nsplit: val\nsave_json: false\nsave_hybrid: false\nconf: null\niou: 0.4\nmax_det: 300\nhalf: false\ndnn: false\nplots: true\nsource: null\nvid_stride: 1\nstream_buffer: false\nvisualize: true\naugment: false\nagnostic_nms: false\nclasses: null\nretina_masks: false\nembed: null\nshow: false\nsave_frames: false\nsave_txt: false\nsave_conf: false\nsave_crop: false\nshow_labels: true\nshow_conf: true\nshow_boxes: true\nline_width: null\nformat: torchscript\nkeras: false\noptimize: false\nint8: false\ndynamic: false\nsimplify: true\nopset: null\nworkspace: null\nnms: false\nlr0: 0.04\nlrf: 0.01\nmomentum: 0.937\nweight_decay: 0.0005\nwarmup_epochs: 3.0\nwarmup_momentum: 0.8\nwarmup_bias_lr: 0.1\nbox: 3.44\ncls: 4.16\ndfl: 2.02\npose: 12.0\nkobj: 1.0\nnbs: 64\nhsv_h: 0.0\nhsv_s: 0.0\nhsv_v: 0.0\ndegrees: 20\ntranslate: 0.2\nscale: 0.3\nshear: 20\nperspective: 0.0\nflipud: 0.0\nfliplr: 0.5\nbgr: 0.0\nmosaic: 0.0\nmixup: 0.0\ncopy_paste: 0.0\ncopy_paste_mode: flip\nauto_augment: null\nerasing: 0.0\ncrop_fraction: 1.0\ncfg: null\ntracker: botsort.yaml\n\n**INFERENCE ARGS (rest is default)**\nSetting,Value\nmodel_path,\\\\weights\\best.pt\nimgsz,384\ntask,segment\ndevice,cuda\nbatch,256\nsave_conf,True\nmax_det,1\nsave_txt,True\nsave,True\n\n**Grid search results**\nMy latest grid search differed in loss configs/augmentation profiles/image sizes:\n# Augmentation profiles\nAUGMENTATION_PROFILES = {\n    \"light\": dict(degrees=10, translate=0.1, scale=0.2, shear=10, fliplr=0.5),\n    \"medium\": dict(degrees=20, translate=0.2, scale=0.3, shear=20, fliplr=0.5),\n    \"heavy\": dict(degrees=30, translate=0.5, scale=0.5, shear=30, fliplr=0.5)\n}\n# Loss configs\nLOSS_CONFIGS = {\n    \"default\": dict(box=7.5, cls=0.5, dfl=1.5),         # Official YOLOv8 default weights\n    \"seg-boost\": dict(box=3.44, cls=4.16, dfl=2.02)      # Custom segmentation-boosted weights\n}\n\n# Values to grid search\nIMG_SIZES = [256, 384, 512]\n\nThese are the results \n\n![Image](https://github.com/user-attachments/assets/1f613fcf-8b21-482f-ab6d-0ad9874198be)\n\nAnd here are the results from yolo training from the 'best performing' of this bunch: seg-boost loss function, 'heavy' augmentation and 256 images ize. \n\n![Image](https://github.com/user-attachments/assets/7cb41321-8124-4add-852b-db75897449ab)\n\n![Image](https://github.com/user-attachments/assets/93508538-d21e-42b3-a4e6-df32f4516266)\n\n![Image](https://github.com/user-attachments/assets/40cae7b4-191e-4415-b6a6-bf1d04f4d49a)\n\nAre there settings I'm missing? Anything that stands out to you? \n\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @mw-boop, thank you for sharing these detailed results and for your interest in Ultralytics 🚀! We appreciate the comprehensive information on your dataset, training configuration, and grid search efforts.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate the issue more efficiently.\n\nIf this is a custom training ❓ Question about improving recall or segmentation performance, please include as much information as possible, such as sample images (if possible), training logs, or any additional details that might help us understand your setup. Also, ensure you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nFor quick guidance, check out the [Docs](https://docs.ultralytics.com/) where you’ll find examples for [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, and answers to many common questions.\n\nJoin the Ultralytics community to connect with others and get real-time support:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for chat\n- [Discourse](https://community.ultralytics.com/) for in-depth discussion\n- [Subreddit](https://reddit.com/r/Ultralytics) for community threads\n\n## Upgrade\n\nPlease make sure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to ensure your issue is not already resolved:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can run in any of these verified environments (with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\n✨ This is an automated response to help get you started. An Ultralytics engineer will review your issue and assist you soon!"
      },
      {
        "user": "Y-T-G",
        "body": "Instead of posting all the possible training arguments, you should only post what was modified. Otherwise this is hard to read and understand what was changed."
      },
      {
        "user": "Y-T-G",
        "body": "> All frames went through the exact same preprocessing (normalization)\n\nDo you mean you manually normalized it? The preprocessing is handled by Ultralytics. You shouldn't be normalizing it manually."
      }
    ]
  },
  {
    "issue_number": 20431,
    "title": "YoloPose relation between keypoints and bounding boxes",
    "author": "artium-d",
    "state": "closed",
    "created_at": "2025-04-30T09:51:13Z",
    "updated_at": "2025-06-17T00:24:09Z",
    "labels": [
      "question",
      "Stale",
      "pose"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello,\n\nI have a question related YOLO-pose (specifically yolov8m-pose) model implementation:\n\nIs there any coupling/dependency between the bounding box outputs and key points outputs? an example for coupling would be, if the network predicts the key point locations as offsets relative to the (predicted) bounding box center.\n\nThis question can arise in the following scenario: suppose I have poor performance on bounding boxes, because they were annotated inconsistently for example and decent performance on key points (which WERE annotated consistently).\nIn this case, would improving performance on bounding boxes also improve performance on the key points?\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @artium-d, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response. An Ultralytics engineer will review your question and assist you soon 😊"
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @artium-d,\n\nYes, there is coupling between bounding boxes and keypoints in YOLO pose models. Looking at the implementation, keypoints aren't directly predicted as offsets from the box center, but both are derived from the same anchor points system.\n\nFrom the source code in `ultralytics/utils/loss.py`, keypoints are decoded using:\n\n```python\ndef kpts_decode(anchor_points, pred_kpts):\n    y = pred_kpts.clone()\n    y[..., :2] *= 2.0\n    y[..., 0] += anchor_points[:, [0]] - 0.5\n    y[..., 1] += anchor_points[:, [1]] - 0.5\n    return y\n```\n\nThis shows keypoints are positioned relative to the same anchor points used for bounding box predictions.\n\nIn your scenario, improving bounding box performance would likely have a positive effect on keypoint detection since:\n1. The model would better localize persons in the image\n2. The anchor assignment during training would improve, benefiting both tasks\n3. The shared feature extraction backbone would learn better representations\n\nLet me know if you need any additional details about the implementation."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20453,
    "title": "On the Impact of Precision-Recall Imbalance on mAP Interpretation",
    "author": "bjh03205",
    "state": "closed",
    "created_at": "2025-05-02T06:54:45Z",
    "updated_at": "2025-06-17T00:24:08Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI have been evaluating the same model under various parameter settings, and I observed that when there is a large disparity between precision and recall values, the resulting mAP tends to be relatively low.\n\nAlthough I understand that precision and recall generally have a trade-off relationship, I am cautious about attributing the lower mAP directly to this imbalance, especially since the PR curve used to compute mAP also incorporates confidence scores.\n\nGiven this, I would like to ask: Assuming that the sum of precision and recall remains constant (i.e., indicating a similar overall performance), is it reasonable to say that a greater disparity between precision and recall leads to a smaller area under the PR curve—hence, a lower mAP—compared to cases where the values are more balanced?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @bjh03205, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response 🤖. An Ultralytics engineer will assist you soon with your specific question about mAP interpretation and the impact of precision-recall balance!"
      },
      {
        "user": "glenn-jocher",
        "body": "You've made an insightful observation. You're correct that when precision and recall have a significant imbalance (even with the same sum), the area under the PR curve—and consequently the mAP—tends to be lower than when they're balanced.\n\nThis is mathematically related to how area under the PR curve works. The PR curve isn't linear, and the relationship between precision and recall at various confidence thresholds creates a curve where balanced values typically enclose more area. This is similar to how the F1 score (harmonic mean of precision and recall) is always maximized when precision equals recall for a given sum.\n\nLooking at the [computation of AP in the metrics.py reference](https://docs.ultralytics.com/reference/utils/metrics/#ultralytics.utils.metrics.compute_ap), where AP is calculated by integrating the precision-recall curve, extreme imbalances reduce this area more significantly than balanced values.\n\nThis isn't just a statistical quirk but reflects a practical reality: models that maintain a good balance between precision and recall generally perform better across various scenarios than those excelling in only one metric."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20478,
    "title": "How to modify the loading of clear-degraded image pairs in knowledge distillation",
    "author": "Wangzhongqiu410",
    "state": "closed",
    "created_at": "2025-05-04T07:29:46Z",
    "updated_at": "2025-06-17T00:24:07Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI improved yolo and used the idea of ​​offline distillation. I first used clear images to train a teacher model model_t, and then used low-quality images to train the student model model_s. During the training process, the teacher model loaded clear images in eval mode. The student model detects objects on degraded images.\nI have now implemented the teacher and student loading the same set of data. I now want to modify the data loading part to load image pairs during training, but I don't know how to modify the dataloader part to meet my needs.\nPlease tell me which part of the code I should modify, thank you\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Wangzhongqiu410, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users, where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response 🤖. An Ultralytics engineer will assist you here soon!"
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @Wangzhongqiu410, \n\nFor implementing offline distillation with paired clear-degraded images, you'll need to create a custom dataset class by extending `BaseDataset`. The main files to focus on are:\n\n1. `ultralytics/data/base.py` - Create a custom dataset class extending `BaseDataset`\n2. `ultralytics/data/build.py` - Either modify `build_yolo_dataset` or create your own builder function\n\nIn your custom dataset, you'll need to implement a way to:\n- Track both clear and degraded image paths (perhaps storing them as parallel lists)\n- Override the `load_image` method to load your paired images\n- Modify `get_image_and_label` to return both versions\n\nFor the trainer, you may need to extend `DetectionTrainer` to use your custom dataset and handle feeding the appropriate images to the teacher and student models during training.\n\nThe core modification will be in how you implement the dataset class to manage the paired data structure and provide the correct images to each model."
      },
      {
        "user": "Wangzhongqiu410",
        "body": "> 你好[@Wangzhongqiu410](https://github.com/Wangzhongqiu410)，\n> \n> 要使用成对的清晰和降级图像实现离线蒸馏，您需要通过扩展来创建自定义数据集类`BaseDataset`。需要关注的主要文件是：\n> \n> 1. `ultralytics/data/base.py`- 创建扩展的自定义数据集类`BaseDataset`\n> 2. `ultralytics/data/build.py`- 修改`build_yolo_dataset`或创建您自己的构建器函数\n> \n> 在您的自定义数据集中，您需要实现一种方法来：\n> \n> * 跟踪清晰和退化的图像路径（可能将它们存储为并行列表）\n> * 重写`load_image`方法来加载配对的图像\n> * 修改`get_image_and_label`以返回两个版本\n> \n> 对于训练师，您可能需要扩展`DetectionTrainer`使用自定义数据集，并在训练期间处理向教师和学生模型提供适当的图像。\n> \n> 核心修改在于如何实现数据集类来管理配对数据结构并为每个模型提供正确的图像。\n\nHello, thank you for your reply. I modified BaseDataset and build.py under base.py according to your suggestion and added the loading of sharp_img. However, after I modified it, I found that there was a problem when loading the batch in trainer.py. The obtained batch did not have bbox, keypoints, etc. I don’t know where the problem is. I look forward to your reply.\n\n![Image](https://github.com/user-attachments/assets/1947931e-be5d-4eeb-9476-6e13f89b1213)"
      }
    ]
  },
  {
    "issue_number": 20486,
    "title": "Configurable detection loss",
    "author": "pow3rpi",
    "state": "open",
    "created_at": "2025-05-04T23:38:51Z",
    "updated_at": "2025-06-17T00:24:06Z",
    "labels": [
      "enhancement",
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nI would like to allow flexible parametrization of the loss function when training YOLO models. This will enable users to easily choose loss function and adjust loss-related hyperparameters (such as alpha, gamma or class weights) without modifying the source code, making it easier to experiment with different loss behaviors (e.g. BCE, FocalLoss, VarifocalLoss etc.).\n\n@glenn-jocher @Y-T-G @Laughing-q I have actually opened this issue, so you could provide me with some tips or recommendations you would like me to get before starting implementing this feature. Maybe there are some pitfalls I should be aware about.\n\n### Use case\n\nLoss function tuning\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @pow3rpi, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response 🤖. An Ultralytics engineer will review your feature request and provide guidance or recommendations soon. Thank you for your willingness to contribute and for helping improve YOLO!"
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @pow3rpi,\n\nThis is a valuable feature request that would benefit many users experimenting with different training approaches. \n\nFor implementation, I'd recommend focusing on:\n\n1. The `v8DetectionLoss` class in `ultralytics/utils/loss.py` which is the main entry point for detection losses\n2. The `init_criterion()` method in model classes which instantiates the appropriate loss\n3. Extending the configuration system to accept loss-specific parameters\n\nSome considerations:\n- Ensure backward compatibility so existing configs continue working\n- Allow for both high-level loss selection (e.g., \"focal\", \"bce\", \"varifocal\") and fine-grained parameter control\n- Consider impact on different tasks (detection, segmentation, pose)\n\nThe YOLO model architecture uses task-specific losses, so you'll need to consider how your implementation handles different model types. Looking forward to your PR!"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20487,
    "title": "yaml file incorrect for package segmentation dataset",
    "author": "ACooperDev",
    "state": "closed",
    "created_at": "2025-05-05T00:00:24Z",
    "updated_at": "2025-06-17T00:24:05Z",
    "labels": [
      "bug",
      "Stale",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nhttps://github.com/ultralytics/assets/releases/download/v0.0.0/package-seg.zip\n\nThe yaml file that comes with this set of images and labels is shows an incorrect file structure.\n\nIt shows:\npath: ../datasets/package-seg # dataset root dir\ntrain: images/train # train images (relative to 'path') 1920 images\nval: images/val # val images (relative to 'path') 89 images\ntest: test/images # test images (relative to 'path') 188 images\n\nWhen it should show:\npath: ../datasets/package-seg # dataset root dir\ntrain: train/images # train images (relative to 'path') 1920 images\nval: valid/images # val images (relative to 'path') 89 images\ntest: test/images # test images (relative to 'path') 188 images\n\n### Environment\n\nNA\n\n### Minimal Reproducible Example\n\nNA\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @ACooperDev, thank you for bringing this to our attention and for your interest in Ultralytics 🚀! We recommend checking out the [Docs](https://docs.ultralytics.com/) for guidance, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, as many common questions are already answered there.\n\nAs this is a 🐛 Bug Report related to the dataset yaml file structure, could you please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/)? This will help us quickly diagnose and address the issue.\n\nJoin the Ultralytics community wherever you prefer:\n- For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussions, check out [Discourse](https://community.ultralytics.com/)\n- To share knowledge, head to our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease ensure you are running the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response. An Ultralytics engineer will review your issue and assist you soon 😊"
      },
      {
        "user": "glenn-jocher",
        "body": "Thank you for reporting this issue with the package segmentation dataset YAML file. You're absolutely correct about the path discrepancy. I've verified that the actual dataset structure uses `train/images`, `valid/images`, and `test/images` directories, while the YAML incorrectly references `images/train` and `images/val`.\n\nWe'll update the YAML file in the package-seg.zip to correctly reflect the actual directory structure. This fix will be implemented in the next repository update.\n\nFor users encountering this issue, you can temporarily fix this by editing the YAML file locally with the correct paths as you suggested:\n```\npath: ../datasets/package-seg\ntrain: train/images\nval: valid/images\ntest: test/images\n```\n\nThanks for your contribution to improving the Ultralytics codebase!"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20488,
    "title": "What type of data should be as target input in v8DetectionLoss ?",
    "author": "SaidSun",
    "state": "closed",
    "created_at": "2025-05-05T05:50:07Z",
    "updated_at": "2025-06-17T00:24:04Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi!\nI've recently started working on pre-training the Yolov8 model along with an additional image preprocessing model for my task. The problem is that I'm trying to use the loss presented in your sources for image detection, but I don't seem to fully understand how it works. My understanding from the code is that the target data should be a dictionary with the keys “batch_idx”, “cls” and “bboxes” and I have some questions about the data belonging to the corresponding keys:\n1. Do I understand correctly, the values in the key “cls” is a one-dimensional array(or vector) with a set of classes? And “batch_idx” is needed to distinguish which image contains an object of the corresponding class? I.e. “batch_idx” also contains a vector of index values of images in the batches\n2. In one of the answers to the v8DetectionLoss class question it was stated that the input data should be normalized - is this rule true for classes? Is it necessary to normalize class values to the range [0;1] for basic model retraining?\n\nThank you a lot for your reply)\n\n### Additional\n\nI can't actually figure out if my model is being trained correctly? Judging by the error values that the model outputs, it looks like there are problems in the training due to incorrect data dimensionality. The values of the individual components of the error functions seem to be too large for a batch size of 8.\n\n![Image](https://github.com/user-attachments/assets/7a96cd7a-1489-4d67-9216-1c51f981b3bf)",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @SaidSun, thank you for reaching out and for your interest in Ultralytics 🚀! This is an automated response to help guide you while an Ultralytics engineer reviews your question and will assist you soon.\n\nFor new users, we recommend checking out the [Docs](https://docs.ultralytics.com/)—you'll find helpful [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples, and answers to many common questions.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question (such as yours), please include as much detail as possible, including dataset image examples, your data formatting, and any relevant training logs. Make sure you’re following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community wherever you prefer! For real-time chat, try our [Discord](https://discord.com/invite/ultralytics) 🎧. For deeper topics, visit [Discourse](https://community.ultralytics.com/). Or share your experience on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package—including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml)—in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to make sure your issue isn't already resolved:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in these up-to-date, verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for providing details! An Ultralytics engineer will review your question and assist you further 🔎"
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @SaidSun,\n\nYou're on the right track. For the `v8DetectionLoss` class:\n\n1. Yes, \"cls\" is a one-dimensional tensor containing class indices for detected objects, and \"batch_idx\" indicates which image in the batch each object belongs to. Both are essentially tensor vectors that help match objects to their respective images in the batch.\n\n2. Only the bounding box coordinates need to be normalized to [0,1] range (relative to image size). Class values should be integer indices (0, 1, 2...) representing the class categories - no normalization needed.\n\nLooking at your loss values, they do seem unusually high. This often indicates either:\n- Incorrectly formatted input data (particularly unnormalized bounding boxes)\n- Issues with anchor generation or matching\n- Inconsistent target format\n\nMake sure your bounding boxes are formatted as [x_center, y_center, width, height] and properly normalized. You can check the format by examining how `targets` are processed in the `preprocess` method of the loss function."
      },
      {
        "user": "SaidSun",
        "body": "Thank you for the reply!\nIt really helped me a lot to understand some things)\nBut can I ask one more question - my task is to merge yolo model with my other image processing model. In doing so, I want to specifically retrofit your initialized yolov8 model.\nIf I initialize your model as shown in the code below in __init__(), will the weights of the pre-trained model also be contained in self.yolo?\n```\nclass TwoStepModel(nn.Module):\n    def __init__(self, levels, channels, lvls_kernel, pools, use_checkpoint=True, model_path=\"yolov8x-oiv7.pt\", device = \"cpu\"):\n        super().__init__()\n        self.u_net = UNET(levels, channels, lvls_kernel, pools, use_checkpoint)\n        self.yolo = YOLO(model_path).model.to(device)\n        self.nc = self.yolo.model[-1].nc \n        self.stride = self.yolo.stride  \n        \n    def forward(self, X):\n        self.intermadiate_result = nn.functional.interpolate(self.u_net(X), size=(640, 640)).requires_grad_(True)\n        self.intermadiate_result = self.intermadiate_result.clamp(0, 1).requires_grad_(True)\n        print(f\"Max val: {torch.max(self.intermadiate_result)}\\nMin val: {torch.min(self.intermadiate_result)}\")\n        print(self.intermadiate_result.requires_grad)\n        return self.yolo(self.intermadiate_result)\n    def train_yolo(self):\n        for param in self.yolo.parameters():\n             param.requires_grad = True\n        \n        for param in self.u_net.parameters():\n            param.requires_grad = False\n    def train_u_net(self):\n        for param in self.yolo.parameters():\n            param.requires_grad = False\n        \n        for param in self.u_net.parameters():\n            param.requires_grad = True\n\n    def test(self):\n        for param in self.yolo.parameters():\n            param.requires_grad = False\n        for param in self.u_net.parameters():\n            param.requires_grad = False \n```"
      }
    ]
  },
  {
    "issue_number": 20497,
    "title": "Hello, I would like to know if Mamba-YOLO will be considered for integration in the future☺️",
    "author": "TheQYQ",
    "state": "closed",
    "created_at": "2025-05-05T11:05:44Z",
    "updated_at": "2025-06-17T00:24:03Z",
    "labels": [
      "enhancement",
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nHello, I would like to know if Mamba-YOLO will be considered for integration in the future☺️\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @TheQYQ, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nThis is an automated response to acknowledge your feature request regarding Mamba-YOLO integration. An Ultralytics engineer will review your suggestion and assist you further soon 🤖.\n\nIf you have additional context, use cases, or references to share, please provide them to help us better understand your needs.\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @TheQYQ, thank you for your interest in Mamba-YOLO integration. While we don't have immediate plans to incorporate Mamba-YOLO, we're always evaluating promising architectures for potential addition to our library. Currently, we're focusing on stabilizing and optimizing YOLO11, which delivers state-of-the-art performance across multiple tasks. If you're interested in contributing to this integration, we welcome community PRs that demonstrate significant performance improvements or novel capabilities. The Ultralytics team regularly reviews architecture innovations based on their production readiness, performance benefits, and community demand."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20496,
    "title": "Number of backbone layer in the Yolo11x",
    "author": "shelfex-abhishek",
    "state": "closed",
    "created_at": "2025-05-05T11:00:11Z",
    "updated_at": "2025-06-17T00:24:03Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\ni want to train the yolo11x model with my custom dataset by freezing the backbone layer but i dot know what number of backbone  layer are present in the the yolo11x model can anyone tell me its help in my training process\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @shelfex-abhishek, thank you for your interest in Ultralytics 🚀! If you're looking to customize the YOLO11x model for your training process, we recommend reviewing the [Docs](https://docs.ultralytics.com/)—especially the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage guides, which may help answer questions about model architecture and customization.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) so we can better assist.\n\nFor custom training or architecture questions, please share as much detail as possible, including your training configuration, dataset samples, and any relevant logs. Also, take a look at our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) to optimize your workflow.\n\nJoin the Ultralytics community:\n- [Discord](https://discord.com/invite/ultralytics) for real-time chat 🎧\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) for knowledge sharing\n\n## Upgrade\n\nEnsure you have the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) installed in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can run YOLO in these up-to-date, verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response 🤖—an Ultralytics engineer will review your question and assist you soon!"
      },
      {
        "user": "Y-T-G",
        "body": "You can find that in the yaml.\n\nhttps://github.com/ultralytics/ultralytics/blob/52e7453d4cb72c4cda0e844265cfeba5393bf8e6/ultralytics/cfg/models/11/yolo11.yaml#L20\n\nEach line like this is a layer."
      },
      {
        "user": "shelfex-abhishek",
        "body": "thank you for your quick response as i can see  the backbone layer is written for yolo11n is around 10 layer is it varied with increase of the model size if no then can conform that my assumption is correct there is 10 layer in it"
      }
    ]
  },
  {
    "issue_number": 20511,
    "title": "Total cars",
    "author": "JeyMarsh",
    "state": "closed",
    "created_at": "2025-05-06T09:29:09Z",
    "updated_at": "2025-06-17T00:24:01Z",
    "labels": [
      "question",
      "Stale",
      "detect",
      "solutions"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI have a project to count the number of vehicles that enter and leave a parking lot when passing through a line, but I needed the total number of vehicles present by subtracting those that left from those that entered. This project I did only gives the answer in this format: \"SolutionResults(classwise_count={'bench': {'IN': 0, 'OUT': 0}}, total_tracks=1\". In short, I needed these two numbers \"IN and OUT\" to become one.\n\n-I am using YOLOv8 and the \"cv2\" library\n\n### Additional\n\nimport os\nimport cv2\nimport serial\nfrom ultralytics.solutions import object_counter\nfrom ultralytics import YOLO\n\nmodel = YOLO(r\"Repositório\\runs\\detect\\train12\\weights\\best.pt\")\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\ncap = cv2.VideoCapture(0)\n#Concectando com o Arduino\nporta = \"COM7\"\nvelocidade = 921600\ntry:\n    conecao = serial.Serial(porta, velocidade, timeout=0.5)\n    print(\"Conectado com a porta\", conecao.portstr)\nexcept serial.SerialException:\n    print(\"Não conectada\")\n    pass\n\nassert cap.isOpened(), \"Error reading video file\"\n\n\n# region_points = [(20, 400), (1080, 400)]                                      # line counting\nregion_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]  # rectangle region\n# region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360), (20, 400)]   # polygon region\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\nclasses_to_count = [0]\n\n# Initialize object counter object\ncounter = object_counter.ObjectCounter()\ncounter.set_args(view_img=True,\n    show=True,  # display the output\n    region=region_points,  # pass region points\n    model=(r\"Repositório\\runs\\detect\\train12\\weights\\best.pt\"),\n     # count specific classes i.e. person and car with COCO pretrained model.\n    #tracker=\"custom_dataset.yaml\",\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n\n\n\n\n\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n    tracks = model.track(im0, persist=True, show=False, classes=classes_to_count)\n    results = counter(im0)\n    resposta = counter(im0)\n    print(\"resposta\", resposta)\n\n   # boxes = results[0].boxes.xyxy.cpu().tolist()\n   # clss = results[0].boxes.clss.cpu().tolist()\n   # confs = results[0].boxes.conf.cpu().tolist()\n\n    #if boxes is not None:\n    #    for box, cls, conf in zip(boxes, clss, confs):\n    #        color = colors(int(cls), True)\n\n    #    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color, 2)\n\n    #    label = f'{model.names[int(cls)]} {conf:.2f}'\n    #    cv2.putText(im0, label, (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    # Mostra o frame processado com as caixas\n   # cv2.imshow(\"Output\", im0)\n   # if cv2.waitKey(1) & 0xFF == ord('q'):  # Sai do loop se apertar a tecla 'q' for pressionada\n   #     cap.release()\n   #     video_writer.release()\n   #     cv2.destroyAllWindows()\n\n\n\n    # print(results)  # access the output\n\n    video_writer.write(results.plot_im)  # write the processed frame.\n    if results == 1:\n        print(\"SolutionResults(classwise_count={'car': {'IN': 0, 'OUT': 0}})\")\n        conecao.write (b'1')\n\ncv2.Imshow(\"Output\", im0)\n\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\nconecao.close()",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @JeyMarsh, thank you for reaching out and for sharing detailed information about your project 🚗🚦! This is an automated response to help guide your next steps while an Ultralytics engineer will also assist you soon.\n\nWe recommend checking out the [Docs](https://docs.ultralytics.com/) for in-depth guides and usage examples, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) workflows. Many common questions are already answered there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us reproduce and diagnose the issue effectively.\n\nFor custom training or model adaptation ❓, please share as much context as possible (dataset examples, training logs, etc.), and make sure to review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community for support and collaboration:\n- [Discord](https://discord.com/invite/ultralytics) for real-time chat 🎧\n- [Discourse](https://community.ultralytics.com/) for detailed discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) for broader community exchange\n\n## Upgrade\n\nTo ensure you have the latest features and bug fixes, please upgrade your `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [Python>=3.8](https://www.python.org/) environment with [PyTorch>=1.8](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in these verified, up-to-date environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for contributing to Ultralytics! 🚀"
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @JeyMarsh,\n\nTo calculate the total vehicles present (IN minus OUT) with the ObjectCounter solution, you can access these values directly from the results object:\n\n```python\n# Inside your while loop\nresults = counter(im0)\nin_count = results.in_count\nout_count = results.out_count\ntotal_present = in_count - out_count\nprint(f\"Total vehicles present: {total_present}\")\n\n# For class-specific counting\n# If you want the count for a specific class (e.g., 'car')\nif 'car' in results.classwise_counts:\n    cars_in = results.classwise_counts['car']['IN']\n    cars_out = results.classwise_counts['car']['OUT']\n    total_cars_present = cars_in - cars_out\n    print(f\"Total cars present: {total_cars_present}\")\n    \n    # Send to Arduino if needed\n    if total_cars_present > 0:\n        conecao.write(b'1')\n```\n\nThe ObjectCounter returns these counts separately, and you need to calculate the difference yourself to get the total present. You can also access the detailed documentation on the [object counter solution](https://docs.ultralytics.com/reference/solutions/object_counter/) for more options and information."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20510,
    "title": "I can't charge my trained yolo",
    "author": "JeyMarsh",
    "state": "closed",
    "created_at": "2025-05-06T09:15:35Z",
    "updated_at": "2025-06-17T00:24:01Z",
    "labels": [
      "question",
      "Stale",
      "detect",
      "solutions"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI'm trying to count toy cars with Yolo detection, but when I do the counting with the \"cv2\" library, it detects much more objects than my trained YOLO should (In this case only 1 class = toy cars)\n\n\n\n### Additional\n```\nimport os\nimport cv2\nimport serial\nfrom ultralytics.solutions import object_counter\nfrom ultralytics import YOLO\n\nmodel = YOLO(r\"Repositório\\runs\\detect\\train12\\weights\\best.pt\")\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\ncap = cv2.VideoCapture(0)\n#Concectando com o Arduino\nporta = \"COM7\"\nvelocidade = 921600\ntry:\n    conecao = serial.Serial(porta, velocidade, timeout=0.5)\n    print(\"Conectado com a porta\", conecao.portstr)\nexcept serial.SerialException:\n    print(\"Não conectada\")\n    pass\n\nassert cap.isOpened(), \"Error reading video file\"\n\n\n# region_points = [(20, 400), (1080, 400)]                                      # line counting\nregion_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]  # rectangle region\n# region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360), (20, 400)]   # polygon region\n\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\nvideo_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\nclasses_to_count = [0]\n\n# Initialize object counter object\ncounter = object_counter.ObjectCounter()\ncounter.set_args(view_img=True,\n    show=True,  # display the output\n    region=region_points,  # pass region points\n    model=(r\"Repositório\\runs\\detect\\train12\\weights\\best.pt\"),\n     # count specific classes i.e. person and car with COCO pretrained model.\n    #tracker=\"custom_dataset.yaml\",\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n\n\n\n\n\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n    tracks = model.track(im0, persist=True, show=False, classes=classes_to_count)\n    results = counter(im0)\n    resposta = counter(im0)\n    print(\"resposta\", resposta)\n\n   # boxes = results[0].boxes.xyxy.cpu().tolist()\n   # clss = results[0].boxes.clss.cpu().tolist()\n   # confs = results[0].boxes.conf.cpu().tolist()\n\n    #if boxes is not None:\n    #    for box, cls, conf in zip(boxes, clss, confs):\n    #        color = colors(int(cls), True)\n\n    #    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color, 2)\n\n    #    label = f'{model.names[int(cls)]} {conf:.2f}'\n    #    cv2.putText(im0, label, (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    # Mostra o frame processado com as caixas\n   # cv2.imshow(\"Output\", im0)\n   # if cv2.waitKey(1) & 0xFF == ord('q'):  # Sai do loop se apertar a tecla 'q' for pressionada\n   #     cap.release()\n   #     video_writer.release()\n   #     cv2.destroyAllWindows()\n\n\n\n    # print(results)  # access the output\n\n    video_writer.write(results.plot_im)  # write the processed frame.\n    if results == 1:\n        print(\"SolutionResults(classwise_count={'car': {'IN': 0, 'OUT': 0}})\")\n        conecao.write (b'1')\n\ncv2.Imshow(\"Output\", im0)\n\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\nconecao.close()\n```",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @JeyMarsh, thank you for reaching out and sharing your code with the Ultralytics community 🚀! For new users, we recommend checking out the [Docs](https://docs.ultralytics.com/) where you’ll find [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples, as many common questions are already answered there.\n\nSince you've described an object counting issue, if this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) that isolates the problem. This will help us debug the issue more efficiently.\n\nIf this is a custom training ❓ Question, please include:\n- Example images from your dataset\n- Training logs and results\n- Confirmation that you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)\n\nJoin the Ultralytics community for real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧, in-depth discussions on [Discourse](https://community.ultralytics.com/), or to share insights on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease ensure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) within a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date verified environments (with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response to help you get started quickly 💡. An Ultralytics engineer will review your issue and assist you soon!"
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @JeyMarsh,\n\nI see you're having an issue with object counting using your custom-trained YOLO model. The main issue I notice is that you're creating the model twice in different ways, which may be causing the confusion.\n\nHere's how to properly use the ObjectCounter solution with your trained model:\n\n```python\nfrom ultralytics import solutions\n\n# Initialize object counter with your model directly\ncounter = solutions.ObjectCounter(\n    show=True,\n    region=region_points,\n    model=r\"Repositório\\runs\\detect\\train12\\weights\\best.pt\",\n    classes=[0],  # Only count class 0 (toy cars)\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n    if not success:\n        break\n    \n    # Process frame with counter\n    results = counter(im0)\n    \n    # Access results properly (results is a SolutionResults object, not an integer)\n    if results.in_count > 0:\n        conecao.write(b'1')\n    \n    video_writer.write(results.plot_im)\n```\n\nYou don't need to load the model separately with `YOLO()` and then call `.track()` - the ObjectCounter handles all that internally when you pass your model path to it.\n\nAlso, you can access results like `results.in_count`, `results.out_count`, or `results.classwise_count` as shown in the [object counting documentation](https://docs.ultralytics.com/guides/object-counting/)."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20515,
    "title": "Feature Request: Include Numerical Classification Metrics in Output (val=True)",
    "author": "VictorZani",
    "state": "closed",
    "created_at": "2025-05-06T12:03:29Z",
    "updated_at": "2025-06-17T00:24:00Z",
    "labels": [
      "enhancement",
      "Stale",
      "classify"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nHello Ultralytics team,\n\nFirst, thank you for your outstanding work on the YOLOv8 framework. It has been instrumental in several computer vision projects I've undertaken.\n\nI would like to propose an enhancement to the classification tasks within the framework. Currently, when performing classification with val=True, the output includes metrics like accuracy. However, it would be highly beneficial to also include the following numerical metrics:\n\nSensitivity (Recall)\n\nSpecificity\n\nReceiver Operating Characteristic (ROC) Curve\n\nArea Under the ROC Curve (AUC)\n\nThese metrics are standard in classification tasks and provide deeper insights into model performance, especially in scenarios with class imbalance or when the costs of false positives and false negatives differ.\n\nProposed Implementation:\n\n  Calculate and display Sensitivity and Specificity for each class during validation.\n  \n  Generate and save ROC curves for each class, possibly as roc_curve_classname.png.\n  \n  Compute and report AUC values for each class, as well as macro and micro averages.\n  \n  Include these metrics in the validation logs and save them in a structured format (e.g., JSON) for further analysis.\n  \nJustification:\n\n  Enhances the interpretability and evaluation of classification models.\n  \n  Aligns with standard practices in machine learning and statistical analysis.\n  \n  Provides users with comprehensive tools to assess model performance beyond accuracy alone.\n  \n  Additional Context:\n  \nWhile there have been discussions on computing these metrics manually using the confusion matrix and external libraries like scikit-learn, integrating them directly into the framework would streamline the evaluation process and reduce the overhead for users.\n\nI am willing to contribute to the implementation or testing of this feature if needed.\n\nThank you for considering this enhancement.\n\nBest regards,\nVictor Hugo Zani\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @VictorZani, thank you for your thoughtful feature request and for your interest in Ultralytics 🚀! We appreciate your detailed suggestions on enhancing classification outputs in the framework.\n\nThis is an automated response to help you get the fastest possible support. An Ultralytics engineer will review your request and assist you further soon.\n\nIn the meantime, you might find it helpful to explore our [Docs](https://docs.ultralytics.com/) for more information on current classification metrics, [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, and to see if any workarounds or related discussions are available.\n\nIf you have additional context, example outputs, or specific usage scenarios that would help clarify your needs, please feel free to share them below.\n\nJoin the Ultralytics community for real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧, in-depth discussion on [Discourse](https://community.ultralytics.com/), or to share insights on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nTo make sure you’re testing with the latest features and improvements, please upgrade the `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can also test and develop in these verified environments (all dependencies preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your suggestion and willingness to contribute!"
      },
      {
        "user": "glenn-jocher",
        "body": "Hello Victor,\n\nThanks for your thoughtful feature request and kind words about YOLO. You've identified a valuable enhancement for classification tasks. Currently, ClassificationValidator produces top1/top5 accuracy metrics and generates a confusion matrix, but doesn't explicitly calculate sensitivity, specificity, ROC curves, or AUC.\n\nThis is a well-justified request as these metrics provide more comprehensive model evaluation, especially for imbalanced datasets. The foundation for implementing these metrics exists in our codebase - we have the confusion matrix generation and curve plotting utilities that could be extended.\n\nWe'll consider adding these metrics to the `ClassifyMetrics` class. If you're interested in contributing, you could start by examining `ultralytics/utils/metrics.py` and `ultralytics/models/yolo/classify/val.py` which contain the relevant code.\n\nThanks for helping improve Ultralytics YOLO!"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20518,
    "title": "cannot import name 'YAML' from 'ultralytics.utils'",
    "author": "StefanNa3Shape",
    "state": "closed",
    "created_at": "2025-05-06T13:58:23Z",
    "updated_at": "2025-06-17T00:23:59Z",
    "labels": [
      "Stale",
      "HUB"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nSo I reinstalled ultralytics from scratch with `pip install --upgrade --force-reinstall ultralytics ` to make sure I have the newest version in case that is where the error stems from.\n\nBut when I go to the ultralytics hub, plan a training and then in the last step want to train locally, this happens:\n\n```\nfrom ultralytics import YOLO, checks, hub\nchecks()\n\nhub.login('79d9593b38f3171674*****')\n\nmodel = YOLO('https://hub.ultralytics.com/models/AUvZk3zg1MF63q5U7lzr')\nresults = model.train()\n```\n\nThe full error:\n\n```\n----> [7](vscode-notebook-cell:?execution_count=15&line=7) results = model.train()\n\nFile c:\\StefansConda\\envs\\teeth2D\\lib\\site-packages\\ultralytics\\engine\\model.py:791, in train(self, trainer, **kwargs)\n      [0](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/model.py:0) <Error retrieving source code with stack_data see ipython/ipython#13598>\n\nFile c:\\StefansConda\\envs\\teeth2D\\lib\\site-packages\\ultralytics\\engine\\trainer.py:211, in train(self)\n    [208](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/trainer.py:208)         ddp_cleanup(self, str(file))\n    [210](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/trainer.py:210) else:\n--> [211](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/trainer.py:211)     self._do_train(world_size)\n\nFile c:\\StefansConda\\envs\\teeth2D\\lib\\site-packages\\ultralytics\\engine\\trainer.py:327, in BaseTrainer._do_train(self, world_size)\n    [325](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/trainer.py:325) if world_size > 1:\n    [326](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/trainer.py:326)     self._setup_ddp(world_size)\n--> [327](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/trainer.py:327) self._setup_train(world_size)\n    [329](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/trainer.py:329) nb = len(self.train_loader)  # number of batches\n    [330](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/trainer.py:330) nw = max(round(self.args.warmup_epochs * nb), 100) if self.args.warmup_epochs > 0 else -1  # warmup iterations\n\nFile c:\\StefansConda\\envs\\teeth2D\\lib\\site-packages\\ultralytics\\engine\\trainer.py:269, in BaseTrainer._setup_train(self, world_size)\n    [267](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/trainer.py:267) if self.amp and RANK in {-1, 0}:  # Single-GPU and DDP\n    [268](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/trainer.py:268)     callbacks_backup = callbacks.default_callbacks.copy()  # backup callbacks as check_amp() resets them\n...\n    [102](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/exporter.py:102)     is_sudo_available,\n    [103](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/exporter.py:103) )\n    [104](file:///C:/StefansConda/envs/teeth2D/lib/site-packages/ultralytics/engine/exporter.py:104) from ultralytics.utils.downloads import attempt_download_asset, get_github_assets, safe_download\n\nImportError: cannot import name 'YAML' from 'ultralytics.utils' (c:\\StefansConda\\envs\\teeth2D\\lib\\site-packages\\ultralytics\\utils\\__init__.py)\n```\n\nAnd in my `__init__.py` ther is also `class YAML:`\n\n\n### Environment\n\nUltralytics 8.3.107  \nPython-3.10.16 \ntorch-2.6.0+cu118 CUDA:0 \n(NVIDIA GeForce RTX 3090, 24575MiB)\nSetup complete  (32 CPUs, 63.9 GB RAM, 877.9/930.9 GB disk)\n\n### Minimal Reproducible Example\n\n```\nfrom ultralytics import YOLO, checks, hub\nchecks()\n\nhub.login('79d9593b38f3171674*****')\n\nmodel = YOLO('https://hub.ultralytics.com/models/AUvZk3zg1MF63q5U7lzr') # or just any model...\nresults = model.train()\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @StefanNa3Shape, thank you for reporting this issue and providing a detailed description and minimal reproducible example! 🚀 This is an automated response to help guide you while an Ultralytics engineer will also assist you soon.\n\nFor new users, we recommend checking our [Docs](https://docs.ultralytics.com/) for answers to common questions and usage examples in both [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/).\n\nSince this is a 🐛 Bug Report and you've already included a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (thank you!), our team will investigate further.\n\nIn the meantime, please make sure you are running the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). You can upgrade with:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO runs in several up-to-date, verified environments with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Community & Support\n\nJoin the Ultralytics community for real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧, in-depth discussions on [Discourse](https://community.ultralytics.com/), or knowledge sharing on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests cover all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your report and for helping improve Ultralytics! An engineer will provide further assistance soon."
      },
      {
        "user": "Y-T-G",
        "body": "You can try latest version "
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20520,
    "title": "Discrepancy Between Inference Results and Visualized Outputs",
    "author": "bunchou-gif",
    "state": "closed",
    "created_at": "2025-05-06T15:28:21Z",
    "updated_at": "2025-06-17T00:23:58Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI trained a model using YOLOv11 and ran inference on a set of images. The inference results indicated that 63 instances of the \"apple\" class were detected. However, the generated visualized images only display 14 detections. I have already set the confidence threshold (conf) to 0.1 and the Intersection over Union (IoU) threshold to 1.0. Could you help me understand why there's such a discrepancy?\n\n![Image](https://github.com/user-attachments/assets/3219c4d5-9f1c-403f-b3bf-5b89c3c565e7)\n![Image](https://github.com/user-attachments/assets/7195302c-3e0d-4eb3-aa28-63f0060f326c)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @bunchou-gif, thank you for reaching out and providing detailed information along with images! 🚀 This is an automated response to help you get started—an Ultralytics engineer will also assist you soon.\n\nWe recommend checking the [Docs](https://docs.ultralytics.com/) for guidance, where you’ll find [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples, as well as answers to many common questions.\n\nSince this appears to be a 🐛 Bug Report, could you please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/)? This will help us quickly identify the source of the discrepancy between your inference results and the visualized outputs.\n\nIf you’re using custom training, sharing relevant dataset samples and training logs will also be helpful. Be sure to review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) to ensure optimal performance.\n\nJoin the Ultralytics community for real-time support and peer discussion:\n- [Discord](https://discord.com/invite/ultralytics) 🎧\n- [Discourse](https://community.ultralytics.com/)\n- [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease upgrade to the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue persists in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following up-to-date verified environments (all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for helping us improve Ultralytics!"
      },
      {
        "user": "Y-T-G",
        "body": "There are multiple overlapping detections. You shouldn't be using `iou` 1.0."
      },
      {
        "user": "bunchou-gif",
        "body": "> 存在多个重叠检测。您不应该使用`iou`1.0。\n\nThank you, but doesn't setting IoU to 1.0 mean that no overlapping detection boxes will be suppressed?"
      }
    ]
  },
  {
    "issue_number": 20522,
    "title": "YOLOv11 Output Tensor Shape Incorrect When Using 48 Classes (Output Shape 52 Instead of 53)",
    "author": "Khueeeee",
    "state": "closed",
    "created_at": "2025-05-06T16:16:50Z",
    "updated_at": "2025-06-17T00:23:57Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi Ultralytics team,\n\nI'm encountering an issue with YOLOv11 where the output tensor shape does not match the expected number of classes.\n\n1. Dataset: I trained a model with 48 classes (nc=48), and the dataset.yaml is correctly configured with 48 class names.\n2. Training: The model trained successfully, and the logs confirm that all 48 classes are present and evaluated correctly. The final mAP and per-class metrics cover all 48 classes.\n3. Model info:\nModel: YOLOv11n\nUltralytics version: (before & after upgrade) – issue persists even with the latest version.\nPython: 3.11\nTorch: 2.6.0\n4. Bug: When I inspect the output tensor using:\n```\nfrom ultralytics import YOLO\nimport torch\n\nmodel = YOLO('/content/drive/MyDrive/Traffic/result/weights/best.pt')\nout = model.model(torch.randn(1, 3, 640, 640))[0]\nprint(out.shape)\n```\n\nI consistently get:\n\n`torch.Size([1, 52, 8400])`\n\nSince the model has 48 classes, I expect:\n\n`num_classes + 5 = 48 + 5 = 53`\n\n5. Verification:\n\nmodel.model.names returns 48 class names ✅,\n\nAll classes are present in the metrics after training ✅,\n\nInference & prediction on real data detect all 48 classes as expected ✅,\n\nOnly the output tensor shape is incorrect ❌.\n\n\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Khueeeee, thank you for bringing this to our attention and for providing detailed context! 🚀 This is an automated response to help get you started—an Ultralytics engineer will review your issue and assist you soon.\n\nWe recommend reviewing our [Docs](https://docs.ultralytics.com/) for additional details, particularly the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples. Many common questions are addressed there.\n\nSince this appears to be a 🐛 Bug Report, could you please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE)? This helps us quickly track down and resolve the issue.\n\nIf you are experimenting with custom training, please include any additional information such as a snippet of your `dataset.yaml`, relevant training logs, and further details on your inference workflow.\n\nJoin the Ultralytics community in your preferred channel for more support!\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth discussions: [Discourse](https://community.ultralytics.com/)\n- Community threads: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease ensure you are running the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in the following up-to-date, verified environments (all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) are preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify the correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your report—your detailed feedback helps us improve Ultralytics HUB and YOLO models! 🦾"
      },
      {
        "user": "Y-T-G",
        "body": "It's `num_classes + 4 = 52`. It's correct.\n\nhttps://www.reddit.com/r/Ultralytics/comments/1jwl6se/ultralytics_postprocessing_guide/"
      },
      {
        "user": "Khueeeee",
        "body": "> It's `num_classes + 4 = 52`. It's correct.\n> \n> https://www.reddit.com/r/Ultralytics/comments/1jwl6se/ultralytics_postprocessing_guide/\nI think\n`52 = 4 (bbox) + 1 (objectness) + 47 (class scores)`"
      }
    ]
  },
  {
    "issue_number": 20521,
    "title": "Yolov11 custom python oonx vs ultralytics CLI produce different results",
    "author": "913-Gaina-Florin",
    "state": "closed",
    "created_at": "2025-05-06T15:53:23Z",
    "updated_at": "2025-06-17T00:23:57Z",
    "labels": [
      "question",
      "Stale",
      "detect",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nimport onnxruntime as ort\nimport numpy as np\nimport cv2\n \n \n \nclass YoloInference:\n    # List of class names for YOLO model\n    classes = [\n    'speed20', 'speed30', 'speed50', 'speed60', 'speed70', 'speed80', 'speed100', 'speed120'\n    ]\n    \n    def __init__(self, model_path):\n        self.session = ort.InferenceSession(model_path)\n        self.input_name = self.session.get_inputs()[0].name\n        self.output_name = self.session.get_outputs()[0].name\n \n    def preprocess(self, image):\n        \"\"\"This functions preprocess the image before inference\n        preprocesses:resize(640x640),normalize,HWC to CHW,adds batch\n        Returns:\n            img preprocessed\n        \"\"\"\n        img = cv2.resize(image, (640, 640))  # Resize to model input size\n        img = img.astype(np.float32)\n        img = img / 255.0  # Normalize\n        img = np.transpose(img, [2, 0, 1])  # Change data layout from HWC to CHW\n        img = np.expand_dims(img, axis=0)  # Add batch dimension\n        return img\n \n    def postprocess(self, output, conf_threshold=0.5, nms_threshold=0.1):\n        detections = output[0].T  # Transpose the array\n \n        boxes = []\n        confidences = []\n        class_ids = []\n \n        for detection in detections:  # Iterate through each of the 8400 predictions\n            box = detection[:4]\n            class_scores = detection[4:]\n \n            class_id = np.argmax(class_scores)\n            confidence = class_scores[class_id]\n            if confidence > conf_threshold:\n                # Convert from center coordinates to corner coordinates\n                cx, cy,box_width, box_height = box  # Note the order: x, y, height, width\n                cx = cx.item()\n                cy = cy.item()\n                box_height = box_height.item()\n                box_width = box_width.item()\n                x1 = int((cx - box_width / 2))\n                y1 = int((cy - box_height / 2))\n                x2 = int(cx + box_width / 2)\n                y2 = int(cy + box_height / 2)\n                boxes.append([x1, y1, x2, y2])\n                confidences.append(float(confidence.item()))\n                class_ids.append(class_id)\n \n        # Apply non-maximum suppression (NMS)\n        indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n        \n        print(confidences)\n        \n        processed_results = []\n        if len(indices) > 0:\n            for i in indices.flatten():\n                x1, y1, x2, y2 = boxes[i]\n                processed_results.append({\n                    'class_id': self.classes[class_ids[i]],\n                    'confidence': confidences[i],\n                    'box': [x1, y1, x2, y2]\n                })\n        return processed_results\n \n    def infer(self, image,desired_class_id=None): # 77-tedy 21-bear\n        h,w,_ = image.shape\n        input_image = self.preprocess(image)\n        output = self.session.run([self.output_name], {self.input_name: input_image})\n        processed_results = self.postprocess(output)\n        orig_height, orig_width = image.shape[:2]\n        scale_x = orig_width / w\n        scale_y = orig_height / h\n        if desired_class_id is not None:\n            boxes = [result['box'] for result in processed_results if result['class_id']==self.classes[desired_class_id]]\n            scores = [result['confidence'] for result in processed_results if result['class_id']==self.classes[desired_class_id]]\n            class_ids = [result['class_id'] for result in processed_results if result['class_id']==self.classes[desired_class_id]]\n        else:\n            boxes = [result['box'] for result in processed_results]\n            scores = [result['confidence'] for result in processed_results]\n            class_ids = [result['class_id'] for result in processed_results]\n        return boxes, scores, class_ids, scale_x, scale_y\n \n    def draw_detections(self,image, boxes, scores, class_ids, scale_x, scale_y):\n        for box, score, class_id in zip(boxes, scores, class_ids):\n            x1, y1, x2, y2 = box\n \n            # Scale the coordinates back to the original image size\n            x1 = int(x1 * scale_x)\n            y1 = int(y1 * scale_y)\n            x2 = int(x2 * scale_x)\n            y2 = int(y2 * scale_y)\n            \n            print(x1, y1, x2, y2)\n \n            # Ensure class_id is used correctly to fetch class name\n            class_name = class_id if isinstance(class_id, str) else self.classes[class_id]\n \n            # Draw the bounding box\n            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n \n            # Label the bounding box with class name and confidence score\n            label = f'{class_name}: {score:.2f}'\n            cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n \n        return image\n \n    # Load and run inference\n    yolo = YoloInference(model_path)\n    image = cv2.imread(image_path)\n    boxes, scores, class_ids, scale_x, scale_y = yolo.infer(image)\n \n    # Draw and display detections\n    image_with_detections = yolo.draw_detections(image, boxes, scores, class_ids, scale_x, scale_y)\n    cv2.imwrite(\"Detectionsl.jpg\", image_with_detections)\n\nHello! I am trying to run an exported verision of a fine-tunned yolov11n to detect roadisgns. When i run the model with the yolo cli on this exact same image, I get a object detected with confidence 0.86, but when i run it with my custom script, I get no meaningful detections, all confidence values are around 1e-7. Can anyoane please help me, I have been struggling to solve this for the past few days, I tried tensorflow, Pillow, opencv, differnet normaliations for preprocessing, but nothing seems to work.\n\n![Image](https://github.com/user-attachments/assets/36d8e960-c34e-4d7f-8bda-9888b3f23aac)\n\n![Image](https://github.com/user-attachments/assets/f469ee26-474c-42db-8158-47f15dcf5581)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @913-Gaina-Florin, thank you for reaching out and sharing your detailed code and observations with the Ultralytics team! 🚦🔎 This is an automated response to help you get the most effective support, and an Ultralytics engineer will assist you soon.\n\nWe recommend reviewing the [Docs](https://docs.ultralytics.com/) for guidance on ONNX model export and inference, as well as checking the available [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples. Many common questions and troubleshooting steps are covered there.\n\nSince this appears to be a 🐛 Bug Report (inference result mismatch), could you please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) if the code above does not already qualify? This helps us debug and compare results more effectively.\n\nA few suggestions as you troubleshoot:\n- Double-check that your ONNX export was performed with the latest `ultralytics` package.\n- Ensure the input preprocessing in your custom script matches the preprocessing expected by the YOLO11 ONNX model. Even small differences (such as channel order, normalization, or image resizing) can cause large discrepancies.\n- Compare your postprocessing (especially confidence thresholding and NMS) to the Ultralytics reference implementation.\n\nJoin the Ultralytics community for additional support:\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth discussions: [Discourse](https://community.ultralytics.com/)\n- Share knowledge: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease upgrade to the latest `ultralytics` package (and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml)) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to ensure your issue is not already resolved:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your report and patience! An Ultralytics engineer will review your issue and follow up with you soon."
      },
      {
        "user": "Y-T-G",
        "body": "The preprocessing is wrong.\n\nYou can check the ONNX example here:\nhttps://github.com/ultralytics/ultralytics/tree/main/examples"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20523,
    "title": "YOLOv11 + Jetson Orin + DeepStream 7.1 - Random cudaErrorIllegal Address crash",
    "author": "amarwingxpand",
    "state": "closed",
    "created_at": "2025-05-06T18:52:59Z",
    "updated_at": "2025-06-17T00:23:56Z",
    "labels": [
      "Stale",
      "detect",
      "embedded"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nI'm using DeepStream Python bindings to run a YOLOv11 model on the Jetson Orin. I've modified an [example deepstream python app](https://github.com/NVIDIA-AI-IOT/deepstream_python_apps/blob/master/apps/deepstream-test1-usbcam/deepstream_test_1_usb.py) to work with the YOLOv11 model. This application ingests USB webcam input and outputs the stream locally with bounding boxes. Unfortunately, the application crashes after 1-15 minutes with a cudaErrorIllegalAddress error.\nI am running this on a Jetson Orin 16 GB with the following specifications:\n\nDeepStream Version: 7.1\nJetPack Version: 6.1\nLinux Version: 36.4.0\nModel: YOLOv11 small variant\nPower mode: 25W\n\nI followed the directions provided in this [Ultralytics guide to convert the YOLOv11 model](https://docs.ultralytics.com/guides/deepstream-nvidia-jetson/) from PyTorch to an ONNX model.\nAny ideas on why I am getting this cudaErrorIllegalAddress error message?\n\n\n`ERROR: [TRT]: IExecutionContext::enqueueV3: Error Code 1: Cask (Cask convolution execution)\nERROR: Failed to enqueue trt inference batch\nERROR: Infer context enqueue buffer failed, nvinfer error:NVDSINFER_TENSORRT_ERROR\n0:14:01.194592896 11233 0xaaaaf88bee00 WARN                 nvinfer gstnvinfer.cpp:1420:gst_nvinfer_input_queue_loop:<primary-inference> error: Failed to queue input batch for inferencing\nError: gst-stream-error-quark: Failed to queue input batch for inferencing (1): /dvs/git/dirty/git-master_linux/deepstream/sdk/src/gst-plugins/gst-nvinfer/gstnvinfer.cpp(1420): gst_nvinfer_input_queue_loop (): /GstPipeline:pipeline0/GstNvInfer:primary-inference\nlibnvosd (1504):(ERROR) : cuGraphicsEGLUnRegisterResource failed: 700 \nlibnvosd (1386):(ERROR) : cuGraphicsEGLRegisterImage failed : 700 \n0:14:01.196674710 11233 0xaaaaf39a0330 WARN                 nvdsosd gstnvdsosd.c:645:gst_nvds_osd_transform_ip:<onscreendisplay> error: Unable to draw shapes onto video frame by GPU\n0:14:01.196755608 11233 0xaaaaf39a0330 WARN                 nvinfer gstnvinfer.cpp:2423:gst_nvinfer_output_loop:<primary-inference> error: Internal data stream error.\n0:14:01.196773496 11233 0xaaaaf39a0330 WARN                 nvinfer gstnvinfer.cpp:2423:gst_nvinfer_output_loop:<primary-inference> error: streaming stopped, reason error (-5)\nCUDA Runtime error cudaFreeHost(host_) # an illegal memory access was encountered, code = cudaErrorIllegalAddress [ 700 ] in file /dvs/git/dirty/git-master_linux/deepstream/sdk/src/utils/nvll_osd/memory.hpp:78\nCUDA Runtime error cudaFree(device_) # an illegal memory access was encountered, code = cudaErrorIllegalAddress [ 700 ] in file /dvs/git/dirty/git-master_linux/deepstream/sdk/src/utils/nvll_osd/memory.hpp:79\nCUDA Runtime error cudaFreeHost(host_) # an illegal memory access was encountered, code = cudaErrorIllegalAddress [ 700 ] in file /dvs/git/dirty/git-master_linux/deepstream/sdk/src/utils/nvll_osd/memory.hpp:78\nCUDA Runtime error cudaFree(device_) # an illegal memory access was encountered, code = cudaErrorIllegalAddress [ 700 ] in file /dvs/git/dirty/git-master_linux/deepstream/sdk/src/utils/nvll_osd/memory.hpp:79\nCUDA Runtime error cudaFreeHost(host_) # an illegal memory access was encountered, code = cudaErrorIllegalAddress [ 700 ] in file /dvs/git/dirty/git-master_linux/deepstream/sdk/src/utils/nvll_osd/memory.hpp:78\nCUDA Runtime error cudaFree(device_) # an illegal memory access was encountered, code = cudaErrorIllegalAddress [ 700 ] in file /dvs/git/dirty/git-master_linux/deepstream/sdk/src/utils/nvll_osd/memory.hpp:79\nCUDA Runtime error cudaFreeHost(host_) # an illegal memory access was encountered, code = cudaErrorIllegalAddress [ 700 ] in file /dvs/git/dirty/git-master_linux/deepstream/sdk/src/utils/nvll_osd/memory.hpp:78\nCUDA Runtime error cudaFree(device_) # an illegal memory access was encountered, code = cudaErrorIllegalAddress [ 700 ] in file /dvs/git/dirty/git-master_linux/deepstream/sdk/src/utils/nvll_osd/memory.hpp:79\nCUDA Runtime error cudaFreeHost(host_) # an illegal memory access was encountered, code = cudaErrorIllegalAddress [ 700 ] in file /dvs/git/dirty/git-master_linux/deepstream/sdk/src/utils/nvll_osd/memory.hpp:78\nCUDA Runtime error cudaFree(device_) # an illegal memory access was encountered, code = cudaErrorIllegalAddress [ 700 ] in file /dvs/git/dirty/git-master_linux/deepstream/sdk/src/utils/nvll_osd/memory.hpp:79\nCUDA Runtime error cudaFreeHost(host_) # an illegal memory access was encountered, code = cudaErrorIllegalAddress [ 700 ] in file /dvs/git/dirty/git-master_linux/deepstream/sdk/src/utils/nvll_osd/memory.hpp:78\nCUDA Runtime error cudaFree(device_) # an illegal memory access was encountered, code = cudaErrorIllegalAddress [ 700 ] in file /dvs/git/dirty/git-master_linux/deepstream/sdk/src/utils/nvll_osd/memory.hpp:79\nERROR: Failed to make stream wait on event, cuda err_no:700, err_str:cudaErrorIllegalAddress\nERROR: Preprocessor transform input data failed., nvinfer error:NVDSINFER_CUDA_ERROR\n0:14:01.203695052 11233 0xaaaaf88bee00 WARN                 nvinfer gstnvinfer.cpp:1420:gst_nvinfer_input_queue_loop:<primary-inference> error: Failed to queue input batch for inferencing\nSegmentation fault (core dumped)\n`\n\n### Environment\n\nUltralytics 8.3.127 🚀 Python-3.10.12 torch-2.7.0+cpu CPU (ARMv8 Processor rev 1 (v8l))\nSetup complete ✅ (8 CPUs, 15.3 GB RAM, 57.9/437.6 GB disk)\n\nOS                  Linux-5.15.148-tegra-aarch64-with-glibc2.35\nEnvironment         Linux\nPython              3.10.12\nInstall             git\nPath                /home/jetson/Documents/Video-Pipeline/ultralytics/ultralytics\nRAM                 15.29 GB\nDisk                57.9/437.6 GB\nCPU                 ARMv8 Processor rev 1 (v8l)\nCPU count           8\nGPU                 None\nGPU count           None\nCUDA                None\n\nnumpy               ✅ 1.26.0>=1.23.0\nmatplotlib          ✅ 3.5.1>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 9.0.1>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.25.1>=2.23.0\nscipy               ✅ `1.15.2>=1.4.1`\ntorch               ✅ 2.7.0>=1.8.0\ntorch               ✅ 2.7.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.22.0>=0.9.0\ntqdm                ✅ 4.67.1>=4.64.0\npsutil              ✅ 7.0.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 1.3.5>=1.1.4\nseaborn             ✅ 0.13.2>=0.11.0\nultralytics-thop    ✅ 2.0.14>=2.0.0\n{'OS': 'Linux-5.15.148-tegra-aarch64-with-glibc2.35', 'Environment': 'Linux', 'Python': '3.10.12', 'Install': 'git', 'Path': '/home/jetson/Documents/Video-Pipeline/ultralytics/ultralytics', 'RAM': '15.29 GB', 'Disk': '57.9/437.6 GB', 'CPU': 'ARMv8 Processor rev 1 (v8l)', 'CPU count': 8, 'GPU': None, 'GPU count': None, 'CUDA': None, 'Package Info': {'numpy': '✅ 1.26.0>=1.23.0', 'matplotlib': '✅ 3.5.1>=3.3.0', 'opencv-python': '✅ 4.11.0.86>=4.6.0', 'pillow': '✅ 9.0.1>=7.1.2', 'pyyaml': '✅ 6.0.2>=5.3.1', 'requests': '✅ 2.25.1>=2.23.0', 'scipy': '✅ 1.15.2>=1.4.1', 'torch': '✅ 2.7.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"', 'torchvision': '✅ 0.22.0>=0.9.0', 'tqdm': '✅ 4.67.1>=4.64.0', 'psutil': '✅ 7.0.0', 'py-cpuinfo': '✅ 9.0.0', 'pandas': '✅ 1.3.5>=1.1.4', 'seaborn': '✅ 0.13.2>=0.11.0', 'ultralytics-thop': '✅ 2.0.14>=2.0.0'}}\n\n\n### Minimal Reproducible Example\n\nI am trying to run the Yolov11 model in deepstream. As per the directions, I converted the small yolov11 model to an onnx model using the [DeepStream-Yolo repo](https://github.com/marcoslucianops/DeepStream-Yolo). Afterwards, as linked above, I am using example code from Nvidia which ingests a usb webcam feed and outputs the image with bounding boxes locally. Below, is the modified code. The only two things I changed with the streammux image size and the infer config file. I included the infer config file as well.\n\n```\n#!/usr/bin/env python3\n\nimport sys\nsys.path.append('../')\nimport gi\ngi.require_version('Gst', '1.0')\nfrom gi.repository import GLib, Gst\nfrom common.platform_info import PlatformInfo\nfrom common.bus_call import bus_call\n\nimport pyds\n\nPGIE_CLASS_ID_VEHICLE = 0\nPGIE_CLASS_ID_BICYCLE = 1\nPGIE_CLASS_ID_PERSON = 2\nPGIE_CLASS_ID_ROADSIGN = 3\nMUXER_BATCH_TIMEOUT_USEC = 33000\n\ndef osd_sink_pad_buffer_probe(pad,info,u_data):\n    frame_number=0\n    #Intiallizing object counter with 0.\n    obj_counter = {\n        PGIE_CLASS_ID_VEHICLE:0,\n        PGIE_CLASS_ID_PERSON:0,\n        PGIE_CLASS_ID_BICYCLE:0,\n        PGIE_CLASS_ID_ROADSIGN:0\n    }\n    num_rects=0\n\n    gst_buffer = info.get_buffer()\n    if not gst_buffer:\n        print(\"Unable to get GstBuffer \")\n        return\n\n    # Retrieve batch metadata from the gst_buffer\n    # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the\n    # C address of gst_buffer as input, which is obtained with hash(gst_buffer)\n    batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer))\n    l_frame = batch_meta.frame_meta_list\n    while l_frame is not None:\n        try:\n            # Note that l_frame.data needs a cast to pyds.NvDsFrameMeta\n            # The casting is done by pyds.NvDsFrameMeta.cast()\n            # The casting also keeps ownership of the underlying memory\n            # in the C code, so the Python garbage collector will leave\n            # it alone.\n           frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data)\n        except StopIteration:\n            break\n\n        # frame_number=frame_meta.frame_num\n        # num_rects = frame_meta.num_obj_meta\n        # l_obj=frame_meta.obj_meta_list\n        # while l_obj is not None:\n        #     try:\n        #         # Casting l_obj.data to pyds.NvDsObjectMeta\n        #         obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data)\n        #     except StopIteration:\n        #         break\n        #     obj_counter[obj_meta.class_id] += 1\n        #     try: \n        #         l_obj=l_obj.next\n        #     except StopIteration:\n        #         break\n\n        # # Acquiring a display meta object. The memory ownership remains in\n        # # the C code so downstream plugins can still access it. Otherwise\n        # # the garbage collector will claim it when this probe function exits.\n        # display_meta=pyds.nvds_acquire_display_meta_from_pool(batch_meta)\n        # display_meta.num_labels = 1\n        # py_nvosd_text_params = display_meta.text_params[0]\n        # # Setting display text to be shown on screen\n        # # Note that the pyds module allocates a buffer for the string, and the\n        # # memory will not be claimed by the garbage collector.\n        # # Reading the display_text field here will return the C address of the\n        # # allocated string. Use pyds.get_string() to get the string content.\n        # py_nvosd_text_params.display_text = \"Frame Number={} Number of Objects={} Vehicle_count={} Person_count={}\".format(frame_number, num_rects, obj_counter[PGIE_CLASS_ID_VEHICLE], obj_counter[PGIE_CLASS_ID_PERSON])\n\n        # # Now set the offsets where the string should appear\n        # py_nvosd_text_params.x_offset = 10\n        # py_nvosd_text_params.y_offset = 12\n\n        # # Font , font-color and font-size\n        # py_nvosd_text_params.font_params.font_name = \"Serif\"\n        # py_nvosd_text_params.font_params.font_size = 10\n        # # set(red, green, blue, alpha); set to White\n        # py_nvosd_text_params.font_params.font_color.set(1.0, 1.0, 1.0, 1.0)\n\n        # # Text background color\n        # py_nvosd_text_params.set_bg_clr = 1\n        # # set(red, green, blue, alpha); set to Black\n        # py_nvosd_text_params.text_bg_clr.set(0.0, 0.0, 0.0, 1.0)\n        # # Using pyds.get_string() to get display_text as string\n        # print(pyds.get_string(py_nvosd_text_params.display_text))\n        # pyds.nvds_add_display_meta_to_frame(frame_meta, display_meta)\n        try:\n            l_frame=l_frame.next\n        except StopIteration:\n            break\n\t\t\t\n    return Gst.PadProbeReturn.OK\t\n\n\ndef main(args):\n    # Check input arguments\n    if len(args) != 2:\n        sys.stderr.write(\"usage: %s <v4l2-device-path>\\n\" % args[0])\n        sys.exit(1)\n\n    platform_info = PlatformInfo()\n    # Standard GStreamer initialization\n    Gst.init(None)\n\n    # Create gstreamer elements\n    # Create Pipeline element that will form a connection of other elements\n    print(\"Creating Pipeline \\n \")\n    pipeline = Gst.Pipeline()\n\n    if not pipeline:\n        sys.stderr.write(\" Unable to create Pipeline \\n\")\n\n    # Source element for reading from the file\n    print(\"Creating Source \\n \")\n    source = Gst.ElementFactory.make(\"v4l2src\", \"usb-cam-source\")\n    if not source:\n        sys.stderr.write(\" Unable to create Source \\n\")\n\n    caps_v4l2src = Gst.ElementFactory.make(\"capsfilter\", \"v4l2src_caps\")\n    if not caps_v4l2src:\n        sys.stderr.write(\" Unable to create v4l2src capsfilter \\n\")\n\n\n    print(\"Creating Video Converter \\n\")\n\n    # Adding videoconvert -> nvvideoconvert as not all\n    # raw formats are supported by nvvideoconvert;\n    # Say YUYV is unsupported - which is the common\n    # raw format for many logi usb cams\n    # In case we have a camera with raw format supported in\n    # nvvideoconvert, GStreamer plugins' capability negotiation\n    # shall be intelligent enough to reduce compute by\n    # videoconvert doing passthrough (TODO we need to confirm this)\n\n\n    # videoconvert to make sure a superset of raw formats are supported\n    vidconvsrc = Gst.ElementFactory.make(\"videoconvert\", \"convertor_src1\")\n    if not vidconvsrc:\n        sys.stderr.write(\" Unable to create videoconvert \\n\")\n\n    # nvvideoconvert to convert incoming raw buffers to NVMM Mem (NvBufSurface API)\n    nvvidconvsrc = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor_src2\")\n    if not nvvidconvsrc:\n        sys.stderr.write(\" Unable to create Nvvideoconvert \\n\")\n\n    caps_vidconvsrc = Gst.ElementFactory.make(\"capsfilter\", \"nvmm_caps\")\n    if not caps_vidconvsrc:\n        sys.stderr.write(\" Unable to create capsfilter \\n\")\n\n    # Create nvstreammux instance to form batches from one or more sources.\n    streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\")\n    if not streammux:\n        sys.stderr.write(\" Unable to create NvStreamMux \\n\")\n\n    # Use nvinfer to run inferencing on camera's output,\n    # behaviour of inferencing is set through config file\n    pgie = Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\")\n    if not pgie:\n        sys.stderr.write(\" Unable to create pgie \\n\")\n\n    # Use convertor to convert from NV12 to RGBA as required by nvosd\n    nvvidconv = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor\")\n    if not nvvidconv:\n        sys.stderr.write(\" Unable to create nvvidconv \\n\")\n\n    # Create OSD to draw on the converted RGBA buffer\n    nvosd = Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\")\n\n    if not nvosd:\n        sys.stderr.write(\" Unable to create nvosd \\n\")\n\n    # Finally render the osd output\n    if platform_info.is_integrated_gpu():\n        print(\"Creating nv3dsink \\n\")\n        sink = Gst.ElementFactory.make(\"nv3dsink\", \"nv3d-sink\")\n        if not sink:\n            sys.stderr.write(\" Unable to create nv3dsink \\n\")\n    else:\n        if platform_info.is_platform_aarch64():\n            print(\"Creating nv3dsink \\n\")\n            sink = Gst.ElementFactory.make(\"nv3dsink\", \"nv3d-sink\")\n        else:\n            print(\"Creating EGLSink \\n\")\n            sink = Gst.ElementFactory.make(\"nveglglessink\", \"nvvideo-renderer\")\n        if not sink:\n            sys.stderr.write(\" Unable to create egl sink \\n\")\n\n    print(\"Playing cam %s \" %args[1])\n    caps_v4l2src.set_property('caps', Gst.Caps.from_string(\"video/x-raw, framerate=30/1\"))\n    caps_vidconvsrc.set_property('caps', Gst.Caps.from_string(\"video/x-raw(memory:NVMM)\"))\n    source.set_property('device', args[1])\n    streammux.set_property('width', 640)\n    streammux.set_property('height', 640)\n    streammux.set_property('batch-size', 1)\n    streammux.set_property('batched-push-timeout', MUXER_BATCH_TIMEOUT_USEC)\n    pgie.set_property('config-file-path', \"config_infer_primary_yolo11.txt\")\n    # Set sync = false to avoid late frame drops at the display-sink\n    sink.set_property('sync', False)\n\n    print(\"Adding elements to Pipeline \\n\")\n    pipeline.add(source)\n    pipeline.add(caps_v4l2src)\n    pipeline.add(vidconvsrc)\n    pipeline.add(nvvidconvsrc)\n    pipeline.add(caps_vidconvsrc)\n    pipeline.add(streammux)\n    pipeline.add(pgie)\n    pipeline.add(nvvidconv)\n    pipeline.add(nvosd)\n    pipeline.add(sink)\n\n    # we link the elements together\n    # v4l2src -> nvvideoconvert -> mux -> \n    # nvinfer -> nvvideoconvert -> nvosd -> video-renderer\n    print(\"Linking elements in the Pipeline \\n\")\n    source.link(caps_v4l2src)\n    caps_v4l2src.link(vidconvsrc)\n    vidconvsrc.link(nvvidconvsrc)\n    nvvidconvsrc.link(caps_vidconvsrc)\n\n    sinkpad = streammux.request_pad_simple(\"sink_0\")\n    if not sinkpad:\n        sys.stderr.write(\" Unable to get the sink pad of streammux \\n\")\n    srcpad = caps_vidconvsrc.get_static_pad(\"src\")\n    if not srcpad:\n        sys.stderr.write(\" Unable to get source pad of caps_vidconvsrc \\n\")\n    srcpad.link(sinkpad)\n    streammux.link(pgie)\n    pgie.link(nvvidconv)\n    nvvidconv.link(nvosd)\n    nvosd.link(sink)\n\n    # create an event loop and feed gstreamer bus mesages to it\n    loop = GLib.MainLoop()\n    bus = pipeline.get_bus()\n    bus.add_signal_watch()\n    bus.connect (\"message\", bus_call, loop)\n\n    # Lets add probe to get informed of the meta data generated, we add probe to\n    # the sink pad of the osd element, since by that time, the buffer would have\n    # had got all the metadata.\n    osdsinkpad = nvosd.get_static_pad(\"sink\")\n    if not osdsinkpad:\n        sys.stderr.write(\" Unable to get sink pad of nvosd \\n\")\n\n    osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0)\n\n    # start play back and listen to events\n    print(\"Starting pipeline \\n\")\n    pipeline.set_state(Gst.State.PLAYING)\n    try:\n        loop.run()\n    except:\n        pass\n    # cleanup\n    pipeline.set_state(Gst.State.NULL)\n\nif __name__ == '__main__':\n    sys.exit(main(sys.argv))\n\n```\n**This is my config infer file:**\n```\n[property]\ngpu-id=0\nnet-scale-factor=0.0039215697906911373\nmodel-color-format=0\nonnx-file=yolo11s.pt.onnx\nmodel-engine-file=model_b1_gpu0_fp32.engine\n#int8-calib-file=calib.table\nlabelfile-path=labels.txt\nbatch-size=1\nnetwork-mode=0\nnum-detected-classes=80\ninterval=0\ngie-unique-id=1\nprocess-mode=1\nnetwork-type=0\ncluster-mode=2\nmaintain-aspect-ratio=1\nsymmetric-padding=1\n#workspace-size=2000\nparse-bbox-func-name=NvDsInferParseYolo\n#parse-bbox-func-name=NvDsInferParseYoloCuda\ncustom-lib-path=../DeepStream-Yolo/nvdsinfer_custom_impl_Yolo/libnvdsinfer_custom_impl_Yolo.so\nengine-create-func-name=NvDsInferYoloCudaEngineGet\n\n[class-attrs-all]\nnms-iou-threshold=0.45\npre-cluster-threshold=0.25\ntopk=300\n\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @amarwingxpand, thank you for reaching out and providing detailed information about your setup and the issue you’re experiencing with YOLO11, Jetson Orin, and DeepStream 7.1! 🚀 This is an automated response to help you get started—an Ultralytics engineer will also review your report and assist you soon.\n\nWe recommend checking the [Docs](https://docs.ultralytics.com/) for guidance, especially the [DeepStream and NVIDIA Jetson guide](https://docs.ultralytics.com/guides/deepstream-nvidia-jetson/) you’ve referenced, as well as [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples that address many common questions.\n\nSince this is a 🐛 Bug Report and you’ve already included a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/), thank you for that! If you have any additional details (such as logs, ONNX export steps, or custom pipeline modifications), feel free to add them—they help us reproduce and diagnose the issue faster.\n\nJoin the Ultralytics community for further discussion or real-time troubleshooting:\n- For live chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth Q&A, check out [Discourse](https://community.ultralytics.com/)\n- To share and learn from community experiences, visit our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease ensure you are running the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). Upgrading may resolve some compatibility issues with exported models:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in any of the following up-to-date environments (with all dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your thorough report and for helping improve Ultralytics! An engineer will review your issue and follow up as soon as possible."
      },
      {
        "user": "Y-T-G",
        "body": "You can open an issue on NVIDIA Forums since this is more of a DeepStream issue than Ultralytics."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20528,
    "title": "CBAM",
    "author": "NguyenNgocHaiLuan",
    "state": "closed",
    "created_at": "2025-05-07T05:13:28Z",
    "updated_at": "2025-06-17T00:23:55Z",
    "labels": [
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nIntegrations\n\n### Bug\n\nimport math\n    from ultralytics.utils.torch_utils import make_divisible\n\n    anchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\n    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors\n    no = na * (nc + 5)\n    layers, save, c2 = [], [], ch[-1]\n    ch = ch.copy()\n    max_channels = max(ch)\n\n    for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):\n        m = eval(m) if isinstance(m, str) else m\n        for j, a in enumerate(args):\n            try:\n                args[j] = eval(a) if isinstance(a, str) else a\n            except:\n                pass\n\n        n = max(round(n * gd), 1) if n > 1 else n\n        if m in {Conv, Bottleneck, SPP, SPPF, DWConv, Focus, GhostConv, GhostBottleneck, BottleneckCSP, C3, C3TR, C3SPP, C3Ghost, ConvTranspose, C2f}:\n            c1, c2 = ch[f], args[0]\n            if c2 != nc:\n                c2 = make_divisible(min(c2, max_channels) * gw, 8)\n            args = [c1, c2, *args[1:]]\n        elif m is nn.BatchNorm2d:\n            args = [ch[f]]\n        elif m is Concat:\n            c2 = sum(ch[x] for x in f)\n        elif m is CBAM:\n            c1, c2 = ch[f], args[0]\n            if c2 != nc:\n                c2 = make_divisible(min(c2, max_channels) * gw, 8)\n            args = [c1, *args[1:]]\n        elif m in {Detect, Segment, Pose}:\n            args.append([ch[x] for x in f])\n            if m is Detect:\n                args.insert(2, nc)\n            elif m is Segment:\n                args.insert(2, d['nsegments'])\n            elif m is Pose:\n                args.insert(2, d['nkpt'])\n                args.insert(3, d['kpt_shape'])\n        else:\n            c2 = ch[f]\n\n        m_ = nn.Sequential(*[m(*args) for _ in range(n)]) if n > 1 else m(*args)\n        t = str(m)[8:-2].replace('__main__.', '')  # module type\n        np = sum(x.numel() for x in m_.parameters())  # number params\n        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\n        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to save list\n        layers.append(m_)\n        if isinstance(c2, list):\n            ch.append(c2[-1])\n        else:\n            ch.append(c2)\n\n    return nn.Sequential(*layers), sorted(save)\nAfter typing the update parse_model command, I ran the custom yolov8s.yaml file but it still reported CBAM error.\n![Image](https://github.com/user-attachments/assets/a3b77242-fdbd-4b56-bec0-e13f1827f0de)\nThis is the yolov8s.yaml file that I customized to add CBAM into yolov8\n\n![Image](https://github.com/user-attachments/assets/13f2f090-efeb-4848-9e97-4b54351e0373)\n\n\n### Environment\n\nvscode\n\n### Minimal Reproducible Example\n\n#10758\n\n### Additional\n\nhelp me fix bugs\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @NguyenNgocHaiLuan, thank you for your interest in Ultralytics 🚀! We recommend checking out the [Docs](https://docs.ultralytics.com/) for guidance—there are many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples available, and many common integration questions may already be answered there.\n\nSince this appears to be a 🐛 Bug Report related to custom model integration, please ensure you provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) if you haven’t already. This will help us assist you more efficiently. It looks like you referenced #10758 as an MRE—please make sure all relevant code and configuration files are included and up to date.\n\nIf your question is about custom training or integration, please share as much detail as possible, including your full custom YAML file, integration steps, and any error tracebacks you encounter. Also, check our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) for further guidance.\n\nJoin the Ultralytics community where it suits you best:\n- For real-time support, join us on [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussions, visit [Discourse](https://community.ultralytics.com/)\n- To share and learn from others, check our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease upgrade to the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to ensure your issue is not already resolved:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date verified environments (with all dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\n🔔 This is an automated response. An Ultralytics engineer will review your issue and provide further assistance soon."
      },
      {
        "user": "glenn-jocher",
        "body": "Looking at your error message, the issue is that the CBAM module is defined in `ultralytics/nn/modules/conv.py` but it's not being imported properly when your custom YOLOv8 model is processed. The error `NameError: name 'CBAM' is not defined` indicates this missing import.\n\nTo fix this, you need to:\n\n1. Make sure CBAM is imported wherever your modified parse_model function is being used. Add this import:\n```python\nfrom ultralytics.nn.modules.conv import CBAM\n```\n\n2. If you're using a custom modification of the Ultralytics codebase, ensure that all necessary modules are imported in the file where you're parsing the model configuration.\n\nIn your YAML configuration, the CBAM layer appears to be correctly defined based on your screenshot, but the actual class needs to be accessible to the parser. The CBAM class takes parameters of `c1` (input channels) and optionally `kernel_size` (defaults to 7)."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20534,
    "title": "I am using the brain-tumor dataset on yolov11, but only get a result of map0.5 = 0.45 (+/- 0.4).",
    "author": "Ayn1631",
    "state": "closed",
    "created_at": "2025-05-07T08:54:55Z",
    "updated_at": "2025-06-17T00:23:54Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nCode reproduction\n```python\nfrom ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data=\"brain-tumor.yaml\", epochs=100, imgsz=640)。\n```\n\nCode reference source:\nurl: https://docs.ultralytics.com/zh/datasets/detect/brain-tumor/#usage\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Ayn1631, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users, where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response. An Ultralytics engineer will also assist you soon."
      },
      {
        "user": "Ayn1631",
        "body": "Could you tell me if this situation is due to the model's capability or is it because of an error I made?\n```cmd\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce GTX 1080        Off |   00000000:4B:00.0 Off |                  N/A |\n| 49%   74C    P2            125W /  180W |    7161MiB /   8192MiB |     39%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n```\nVersion of library:\n```cmd\ntorch                     2.6.0\ntorchaudio                2.6.0\ntorchvision               0.21.0\nultralytics               8.3.127\nultralytics-thop          2.0.14\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "The mAP@0.5 score of 0.45 (±0.4) suggests significant variability in your model's performance, which is common when working with specialized medical datasets like brain tumor detection. This performance could be due to several factors:\n\nFirst, YOLO11n is the smallest YOLO11 variant and may have limited capacity for complex medical imaging tasks. Consider trying YOLO11s or YOLO11m which provide better performance with a reasonable speed tradeoff.\n\nSecond, the high variance (±0.4) indicates training instability. Some suggestions to improve this:\n\n1. Increase your batch size if your GPU memory allows\n2. Try implementing more data augmentation specifically relevant to medical images\n3. Consider using a smaller learning rate for more stable convergence\n4. Monitor training metrics to identify potential overfitting or underfitting\n\nMedical image datasets often benefit from specialized preprocessing and augmentation techniques. The brain tumor dataset is relatively small compared to general object detection datasets, which can make generalization challenging.\n\nHave you checked your training logs for any specific warnings or performance plateaus during training?"
      }
    ]
  },
  {
    "issue_number": 20568,
    "title": "`ValueError` when training `yoloe-11n-seg` from scratch",
    "author": "xperroni",
    "state": "open",
    "created_at": "2025-05-09T15:33:31Z",
    "updated_at": "2025-06-17T00:23:49Z",
    "labels": [
      "bug",
      "Stale",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nTrain\n\n### Bug\n\nI'm trying to train the `yoloe-11n-seg` model from scratch. Following the [instructions](https://docs.ultralytics.com/models/yoloe/#launching-training-from-scratch) in the YOLOE page, I downloaded all datasets and created the script below, with only minor changes to account for limited GPU resources (see the MRP below). However, when I run the script I run into the following error:\n\n```\nTraceback (most recent call last):\n  File \"train_yoloe.py\", line 22, in <module>\n    model.train(\n  File \"/home/user/airchair/lib/python3.8/site-packages/ultralytics/engine/model.py\", line 791, in train\n    self.trainer.train()\n  File \"/home/user/airchair/lib/python3.8/site-packages/ultralytics/engine/trainer.py\", line 211, in train\n    self._do_train(world_size)\n  File \"/home/user/airchair/lib/python3.8/site-packages/ultralytics/engine/trainer.py\", line 385, in _do_train\n    loss, self.loss_items = self.model(batch)\n  File \"/home/user/airchair/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/user/airchair/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/user/airchair/lib/python3.8/site-packages/ultralytics/nn/tasks.py\", line 119, in forward\n    return self.loss(x, *args, **kwargs)\n  File \"/home/user/airchair/lib/python3.8/site-packages/ultralytics/nn/tasks.py\", line 1043, in loss\n    return self.criterion(preds, batch)\n  File \"/home/user/airchair/lib/python3.8/site-packages/ultralytics/utils/loss.py\", line 332, in __call__\n    masks = F.interpolate(masks[None], (mask_h, mask_w), mode=\"nearest\")[0]\n  File \"/home/user/airchair/lib/python3.8/site-packages/torch/nn/functional.py\", line 3983, in interpolate\n    raise ValueError(\nValueError: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [] and output size of (160, 160). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.\n```\n\nAny guesses on what's going wrong?\n\n### Environment\n\n```\nUltralytics 8.3.107 🚀 Python-3.8.10 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24135MiB)\nSetup complete ✅ (20 CPUs, 94.0 GB RAM, 1645.0/1832.7 GB disk)\n\nOS                  Linux-6.8.0-52-generic-x86_64-with-glibc2.29\nEnvironment         Linux\nPython              3.8.10\nInstall             pip\nPath                /home/user/airchair/lib/python3.8/site-packages/ultralytics\nRAM                 93.98 GB\nDisk                1645.0/1832.7 GB\nCPU                 Intel Core(TM) i9-10900X 3.70GHz\nCPU count           20\nGPU                 NVIDIA GeForce RTX 3090, 24135MiB\nGPU count           2\nCUDA                12.1\n\nnumpy               ✅ 1.24.4<=2.1.1,>=1.23.0\nmatplotlib          ✅ 3.7.5>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 8.4.0>=7.1.2\npyyaml              ✅ 5.3.1>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.10.1>=1.4.1\ntorch               ✅ 2.4.1>=1.8.0\ntorch               ✅ 2.4.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.19.1>=0.9.0\ntqdm                ✅ 4.67.1>=4.64.0\npsutil              ✅ 5.9.8\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.0.3>=1.1.4\nseaborn             ✅ 0.13.2>=0.11.0\nultralytics-thop    ✅ 2.0.14>=2.0.0\n```\n\n### Minimal Reproducible Example\n\n```\nfrom ultralytics import YOLOE\nfrom ultralytics.models.yolo.yoloe import YOLOESegTrainerFromScratch\n\ndata = dict(\n    train=dict(\n        yolo_data=[\"Objects365.yaml\"],\n        grounding_data=[\n            dict(\n                img_path=\"../datasets/flickr/full_images/\",\n                json_file=\"../datasets/flickr/annotations/final_flickr_separateGT_train_segm.json\",\n            ),\n            dict(\n                img_path=\"../datasets/mixed_grounding/gqa/images\",\n                json_file=\"../datasets/mixed_grounding/annotations/final_mixed_train_no_coco_segm.json\",\n            ),\n        ],\n    ),\n    val=dict(yolo_data=[\"lvis.yaml\"]),\n)\n\nmodel = YOLOE(\"yoloe-11n-seg.yaml\")\nmodel.train(\n    data=data,\n    batch=4, # Reduce batch size to account for smaller GPU memory\n    epochs=960, # Increase epochs to compensate\n    close_mosaic=2,\n    optimizer=\"AdamW\",\n    lr0=2e-3,\n    warmup_bias_lr=0.0,\n    weight_decay=0.025,\n    momentum=0.9,\n    workers=4,\n    trainer=YOLOESegTrainerFromScratch,\n    device=\"0\" # Use a single GPU instead of 8\n)\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @xperroni, thank you for your detailed bug report and for providing a thorough minimal reproducible example (MRE) 🚀! This is an automated response to help get you started—an Ultralytics engineer will also review and assist you soon.\n\nFor new users, we recommend checking the [Docs](https://docs.ultralytics.com/) where you'll find helpful [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples. Many common questions are answered there, which might help you troubleshoot while you wait.\n\nIf you have further details or updates to your MRE, please share them to help our team reproduce and resolve the issue efficiently 🛠️.\n\nJoin the Ultralytics community for discussion and support:\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth forums: [Discourse](https://community.ultralytics.com/)\n- Community threads: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease ensure you are using the latest `ultralytics` package and all required dependencies in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). This helps verify the issue isn't resolved in a newer release:\n\n```bash\npip install -U ultralytics\n```\n\nSee all requirements in [pyproject.toml](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml).\n\n## Environments\n\nYOLO models can be run in any of these up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThanks again for your report! An Ultralytics engineer will follow up soon."
      },
      {
        "user": "Y-T-G",
        "body": "Can you try latest version?"
      },
      {
        "user": "xperroni",
        "body": "Upgraded to version 8.3.130, as shown in the environment report below. Still getting same error.\n\n```\nUltralytics 8.3.130 🚀 Python-3.8.10 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24135MiB)\nSetup complete ✅ (20 CPUs, 94.0 GB RAM, 1645.0/1832.7 GB disk)\n\nOS                  Linux-6.8.0-52-generic-x86_64-with-glibc2.29\nEnvironment         Linux\nPython              3.8.10\nInstall             pip\nPath                /home/user/airchair/lib/python3.8/site-packages/ultralytics\nRAM                 93.98 GB\nDisk                1645.0/1832.7 GB\nCPU                 Intel Core(TM) i9-10900X 3.70GHz\nCPU count           20\nGPU                 NVIDIA GeForce RTX 3090, 24135MiB\nGPU count           2\nCUDA                12.1\n\nnumpy               ✅ 1.24.4>=1.23.0\nmatplotlib          ✅ 3.7.5>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 8.4.0>=7.1.2\npyyaml              ✅ 5.3.1>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.10.1>=1.4.1\ntorch               ✅ 2.4.1>=1.8.0\ntorch               ✅ 2.4.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.19.1>=0.9.0\ntqdm                ✅ 4.67.1>=4.64.0\npsutil              ✅ 5.9.8\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.0.3>=1.1.4\nseaborn             ✅ 0.13.2>=0.11.0\nultralytics-thop    ✅ 2.0.14>=2.0.0\n```"
      }
    ]
  },
  {
    "issue_number": 20597,
    "title": "reg_max, request for help",
    "author": "Ange1ika",
    "state": "open",
    "created_at": "2025-05-12T10:35:11Z",
    "updated_at": "2025-06-17T00:23:46Z",
    "labels": [
      "question",
      "Stale",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI want to ask: why I cant use ddp (when I train with yolo.train(device=[0,1,2])), if I change reg_max = 1, and off the dfl module? Maybe I wrong change reg_max? My changers on git@github.com:Ange1ika/ultralytics_fine_tune-reg_max.git. When I train with my version, all weights and train are holding only first gpu, despite adding arg device=[0,1,2]. \n\nwhen I try train with yours repo and without changes reg - picture 1\n\n![Image](https://github.com/user-attachments/assets/e60d06cb-d78c-4f15-87e0-957f26d079a2)\n\non the other hand: when I use my approach\n\n![Image](https://github.com/user-attachments/assets/603ce56a-a876-4463-80bd-adf072ffae91)\n\nCan you help, please: how right change reg_max and how fine-tune (for instance segmentation task), on our custom dataset, where I have very small and very large masks \n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Ange1ika, thank you for reaching out and for providing detailed context with images and your custom repo link! 🚀 This is an automated response to assist you quickly—an Ultralytics engineer will also review your question and provide further help soon.\n\nWe recommend checking out the [Docs](https://docs.ultralytics.com/) for guidance, especially the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples, which may address some common scenarios.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE). This helps us debug your issue efficiently.\n\nFor custom training ❓ questions, it helps to include as much info as possible—dataset image examples, training logs, and confirmation that you’re following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community to discuss your setup! For real-time chat, join us on [Discord](https://discord.com/invite/ultralytics) 🎧. For more detailed threads, visit [Discourse](https://community.ultralytics.com/), or share insights on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nMake sure you’re running the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience!"
      },
      {
        "user": "Y-T-G",
        "body": "Did you install your local repo with `pip install .` (dot at end)?"
      },
      {
        "user": "Ange1ika",
        "body": "> Did you install your local repo with `pip install .` (dot at end)?\n\nYes, I do "
      }
    ]
  },
  {
    "issue_number": 20610,
    "title": "how to solve this gpu problem",
    "author": "Moeus",
    "state": "open",
    "created_at": "2025-05-13T03:36:47Z",
    "updated_at": "2025-06-17T00:23:44Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nThe running error is\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[25], [line 27](vscode-notebook-cell:?execution_count=25&line=27)\n     [24](vscode-notebook-cell:?execution_count=25&line=24) print(f\"GPU 可用: {model.device.type == 'cuda' if hasattr(model, 'device') else False}\")\n     [26](vscode-notebook-cell:?execution_count=25&line=26) # 训练模型\n---> [27](vscode-notebook-cell:?execution_count=25&line=27) model.train(\n     [28](vscode-notebook-cell:?execution_count=25&line=28)     data=\"dataset/data.yaml\",  # 指定数据集配置文件路径\n     [29](vscode-notebook-cell:?execution_count=25&line=29)     epochs=100,\n     [30](vscode-notebook-cell:?execution_count=25&line=30)     imgsz=640,\n     [31](vscode-notebook-cell:?execution_count=25&line=31)     batch=16,\n     [32](vscode-notebook-cell:?execution_count=25&line=32)     workers=8,\n     [33](vscode-notebook-cell:?execution_count=25&line=33)     project=\"runs/detect\",\n     [34](vscode-notebook-cell:?execution_count=25&line=34)     name=\"exp\",\n     [35](vscode-notebook-cell:?execution_count=25&line=35)     device=\"cuda\"\n     [36](vscode-notebook-cell:?execution_count=25&line=36) )\n\nFile d:\\anaconda3\\envs\\img_recongnition\\Lib\\site-packages\\ultralytics\\engine\\model.py:793, in Model.train(self, trainer, **kwargs)\n    [790](file:///D:/anaconda3/envs/img_recongnition/Lib/site-packages/ultralytics/engine/model.py:790)     self.model = self.trainer.model\n    [792](file:///D:/anaconda3/envs/img_recongnition/Lib/site-packages/ultralytics/engine/model.py:792) self.trainer.hub_session = self.session  # attach optional HUB session\n--> [793](file:///D:/anaconda3/envs/img_recongnition/Lib/site-packages/ultralytics/engine/model.py:793) self.trainer.train()\n    [794](file:///D:/anaconda3/envs/img_recongnition/Lib/site-packages/ultralytics/engine/model.py:794) # Update model and cfg after training\n    [795](file:///D:/anaconda3/envs/img_recongnition/Lib/site-packages/ultralytics/engine/model.py:795) if RANK in {-1, 0}:\n\nFile d:\\anaconda3\\envs\\img_recongnition\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:212, in BaseTrainer.train(self)\n    [209](file:///D:/anaconda3/envs/img_recongnition/Lib/site-packages/ultralytics/engine/trainer.py:209)         ddp_cleanup(self, str(file))\n...\n    [102](file:///D:/anaconda3/envs/img_recongnition/Lib/site-packages/torch/utils/_device.py:102) if func in _device_constructors() and kwargs.get('device') is None:\n    [103](file:///D:/anaconda3/envs/img_recongnition/Lib/site-packages/torch/utils/_device.py:103)     kwargs['device'] = self.device\n--> [104](file:///D:/anaconda3/envs/img_recongnition/Lib/site-packages/torch/utils/_device.py:104) return func(*args, **kwargs)\n\nRuntimeError: Expected a 'cuda' device type for generator but found 'cpu'\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Moeus, thank you for reaching out and providing detailed error information! 🤗 This is an automated response to help you get started, and an Ultralytics engineer will follow up with further assistance soon.\n\nWe recommend checking out the [Docs](https://docs.ultralytics.com/) for helpful guidance. If you're new to Ultralytics, the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage pages may answer many common questions.\n\nSince this appears to be a 🐛 Bug Report involving GPU usage, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) if you haven't already. This will help us quickly reproduce and debug the issue.\n\nIf your question relates to custom training, please include extra details like dataset image examples and training logs, and ensure you're following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nYou’re also welcome to join our community:\n- For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧\n- For detailed discussions, try [Discourse](https://community.ultralytics.com/)\n- Or check out our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge\n\n## Upgrade\n\nPlease ensure you’re using the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) installed in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). This helps verify your issue isn’t due to an outdated version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can run YOLO in any of these verified environments (with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "Y-T-G",
        "body": "Can you provide the output after running this command in terminal: `yolo checks`?"
      },
      {
        "user": "Y-T-G",
        "body": "What's the full error?"
      }
    ]
  },
  {
    "issue_number": 20616,
    "title": "How to use multiple graphics cards for knowledge distillation",
    "author": "zys1994",
    "state": "open",
    "created_at": "2025-05-13T10:08:11Z",
    "updated_at": "2025-06-17T00:23:43Z",
    "labels": [
      "enhancement",
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI have completed the single card knowledge distillation training and achieved good improvement. Could you help  me how to adapt to multiple cards?\n##### train_code(key)\n```\nmodel = YOLO(args.model_yaml)  # build a new model from YAML\nmodel_t = YOLO('{}/teacher.pt')\nmodel.train(teacher_model= model_t.model, data=args.dataset_yaml, epochs=args.epoch, batch=args.batch_size, imgsz=560, device=device_lists)\n```\n##### trainer.py\n```\nif 'teacher_model' in overrides:\n            self.model_t = overrides[\"teacher_model\"]\n            #overrides.pop(\"teacher_model\")\n        else:\n            self.model_t = None\n```\nif i pop the \"teacher_model\",  Ultralytics/DDP/_temp_5tlnx78y139710823693952.py could not recognize \"teacher_model\";  \nif i don't pop the \"teacher_model\", It prompts “SyntaxError: 'teacher_model' is not a valid YOLO argument.“ this error.\nif i add \"teacher_model: None \" to /ultralytics/cfg/default.yaml. it seems not to recognize the model_t.\n```\n  self.model_t = self.model_t.to(self.device)\n  AttributeError: 'str' object has no attribute 'to'\n```\nI'm kind of stuck, how can I adjust better?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @zys1994, thank you for reaching out and for your interest in Ultralytics 🚀! This is an automated response to help you get started—an Ultralytics engineer will review your question and assist you soon.\n\nWe recommend checking out our [Docs](https://docs.ultralytics.com/) for extensive guides and usage examples, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) workflows. Many topics around multi-GPU training, knowledge distillation, and custom modifications are covered there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) so we can investigate further.\n\nFor your custom training ❓ question, please include as much detail as possible, such as:\n- your dataset configuration\n- sample training commands\n- full error messages and stack traces\n- any custom code or changes you've made\n\nBe sure to review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) to optimize your workflow.\n\nJoin the Ultralytics community for more support:\n- Chat with us and others on [Discord](https://discord.com/invite/ultralytics) 🎧\n- Post questions or share ideas on [Discourse](https://community.ultralytics.com/)\n- Connect with other users on our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nTo ensure your environment is up to date and your issue is not already resolved, upgrade to the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your detailed description and code snippets—an Ultralytics engineer will review your issue and provide further guidance soon!"
      },
      {
        "user": "glenn-jocher",
        "body": "To use multiple GPUs for knowledge distillation, you need to handle the teacher model differently in distributed mode since DDP creates separate processes. Instead of passing the model directly, try this approach:\n\n```python\n# Save teacher model to a temporary file first\nteacher_path = 'temp_teacher.pt'\nmodel_t.model.save(teacher_path)\n\n# Then in your training code\nmodel = YOLO(args.model_yaml)\nmodel.train(data=args.dataset_yaml, epochs=args.epoch, batch=args.batch_size, \n           imgsz=560, device=device_lists, teacher_path=teacher_path)\n```\n\nThen modify your trainer initialization to load the teacher model in each process:\n\n```python\n# In BaseTrainer.__init__ or similar location\nif hasattr(self.args, 'teacher_path') and self.args.teacher_path:\n    from ultralytics import YOLO\n    teacher = YOLO(self.args.teacher_path)\n    self.model_t = teacher.model.to(self.device)\n```\n\nThis ensures each process gets its own copy of the teacher model properly loaded on its assigned GPU."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20620,
    "title": "Remove ultralytics dependency and run inference file for python version 3.6",
    "author": "Gcc1012",
    "state": "open",
    "created_at": "2025-05-13T12:26:05Z",
    "updated_at": "2025-06-17T00:23:41Z",
    "labels": [
      "question",
      "dependencies",
      "Stale",
      "classify"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n I have trained a classification model using yolo11n , now i have to write its inference script , but the problem is for development and deployment i have python version 3.6, but yolo and ultralytics supports python version 3.8 and above only , i have tried to convert the classification model i.e .pt file into torchscript to remove this dependency issue , but converting into torchscript is lowering the accuracy of the model , is there any way , or any souce i can refer to resolve this issue? My requirement is to use .pt file directly but also by removing the dependency of ultralytics\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Gcc1012, thank you for reaching out and for your interest in Ultralytics 🚀! This is an automated response to help you get started, and an Ultralytics engineer will assist you soon.\n\nWe recommend visiting the [Docs](https://docs.ultralytics.com/) for guidance, where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, including information about model export and inference options. Many common questions are already addressed there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate further.\n\nIf this is a custom training ❓ Question, please include as much detail as possible, such as your dataset image examples, training logs, and confirm you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community wherever you prefer! For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. For in-depth discussions, see [Discourse](https://community.ultralytics.com/). Or join the conversation on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to ensure your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "Y-T-G",
        "body": "You can export to ONNX with `dynamic=True`"
      },
      {
        "user": "Gcc1012",
        "body": "I dont want to convert it in any other format , i just want to use .pt file , my requirement is to load the .pt file using torch , no dependency of ultralytics\n"
      }
    ]
  },
  {
    "issue_number": 20624,
    "title": "TypeError: 'ObjectCounter' object is not callable",
    "author": "KennyTC",
    "state": "open",
    "created_at": "2025-05-13T20:50:05Z",
    "updated_at": "2025-06-17T00:23:40Z",
    "labels": [
      "non-reproducible",
      "Stale",
      "solutions"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nRunning the example code, I got error \n```\n    results = counter(im0)\n              ^^^^^^^^^^^^\nTypeError: 'ObjectCounter' object is not callable\n```\n\n\n\n### Environment\n\n```\nUltralytics 8.3.53 🚀 Python-3.11.12 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11169MiB)\nSetup complete ✅ (8 CPUs, 31.3 GB RAM, 447.2/456.9 GB disk)\n\nOS                  Linux-6.8.0-59-generic-x86_64-with-glibc2.35\nEnvironment         Linux\nPython              3.11.12\nInstall             git\nRAM                 31.31 GB\nDisk                447.2/456.9 GB\nCPU                 Intel Core(TM) i7-7700K 4.20GHz\nCPU count           8\nGPU                 NVIDIA GeForce GTX 1080 Ti, 11169MiB\nGPU count           1\nCUDA                11.8\n\nnumpy               ✅ 1.26.4>=1.23.0\nnumpy               ✅ 1.26.4<2.0.0; sys_platform == \"darwin\"\nmatplotlib          ✅ 3.9.2>=3.3.0\nopencv-python       ✅ 4.10.0.84>=4.6.0\npillow              ✅ 11.0.0>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.13.1>=1.4.1\ntorch               ✅ 2.6.0+cu118>=1.8.0\ntorch               ✅ 2.6.0+cu118!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.21.0+cu118>=0.9.0\ntqdm                ✅ 4.66.5>=4.64.0\npsutil              ✅ 6.1.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.2.3>=1.1.4\nseaborn             ✅ 0.13.2>=0.11.0\nultralytics-thop    ✅ 2.0.9>=2.0.0\n```\n\n### Minimal Reproducible Example\n\nMy code is as in the example [here](https://docs.ultralytics.com/ja/guides/object-counting/#real-world-applications)\n```\nimport cv2\n\nfrom ultralytics import solutions\n\n\ndef count_objects_in_region(video_path, output_video_path, model_path):\n    \"\"\"Count objects in a specific region within a video.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    assert cap.isOpened(), \"Error reading video file\"\n    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n\n    region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]\n    counter = solutions.ObjectCounter(show=True, region=region_points, model=model_path)\n\n    while cap.isOpened():\n        success, im0 = cap.read()\n        if not success:\n            print(\"Video frame is empty or processing is complete.\")\n            break\n        results = counter(im0)\n        video_writer.write(results.plot_im)\n\n    cap.release()\n    video_writer.release()\n    cv2.destroyAllWindows()\n\n\ncount_objects_in_region(\"people-walking.mp4\", \"outputs/counting/pp_counting.mp4\", \"yolo11n.pt\")\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @KennyTC, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it. Thank you for already including sample code and environment details—it’s very helpful!\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response; an Ultralytics engineer will also assist you here soon 😊"
      },
      {
        "user": "RizwanMunawar",
        "body": "Hi @KennyTC, thanks for sharing this! Please try upgrading the Ultralytics Python package using `pip install -U ultralytics`. That should resolve the error. Thanks."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20626,
    "title": "The OBB model performs poorly on triangular samples",
    "author": "lzp0916",
    "state": "open",
    "created_at": "2025-05-14T01:18:20Z",
    "updated_at": "2025-06-17T00:23:39Z",
    "labels": [
      "Stale",
      "OBB"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nThe OBB model performs poorly on triangular samples during testing. I'm not sure what problem caused it. How should this be resolved.\n\n![Image](https://github.com/user-attachments/assets/adb5c10f-3c62-4653-82ff-bcd507705935)\n\n[train.zip](https://github.com/user-attachments/files/20198283/train.zip)\n\n### Environment\n\nUltralytics v8.3.133\nPython 3.12\nWindow 10\n\n### Minimal Reproducible Example\n\n[datasets.zip](https://github.com/user-attachments/files/20198282/datasets.zip)\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @lzp0916, thank you for sharing your experience with Ultralytics 🚀! This is an automated response to help get you started—an Ultralytics engineer will also assist you soon.\n\nWe appreciate the details and attachments you've provided. For 🐛 Bug Reports like this, having a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) is essential for efficient debugging. If possible, please ensure your example is as concise as possible and includes all necessary files and steps to reproduce the issue.\n\nIf you’re exploring custom training or dataset challenges, please also review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) and share any relevant training logs or additional sample images that illustrate the issue.\n\nCheck out our [Docs](https://docs.ultralytics.com/) for more info about [OBB tasks](https://docs.ultralytics.com/tasks/obb/) and model configurations.\n\nJoin the Ultralytics community for more support and collaboration:\n- For real-time help, join our [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth topics, visit [Discourse](https://community.ultralytics.com/)\n- For sharing and discussion, see our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nTo ensure this issue isn’t already resolved in a newer release, please upgrade your `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO runs in several up-to-date, verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nA green badge means all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. These tests verify YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThanks again for your report and for helping improve Ultralytics!"
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @lzp0916,\n\nThis issue likely stems from the nature of OBB models, which are designed for rotated rectangular shapes rather than triangular objects. The YOLO OBB format (x1 y1 x2 y2 x3 y3 x4 y4) and internal xywhr representation both assume rectangular shapes.\n\nFor triangular objects, consider these approaches:\n1. Create a minimum enclosing rectangle for each triangle, accepting some background inclusion\n2. Use instance segmentation models like YOLO11-seg which might better capture the exact triangular shape\n\nIf you want to continue with OBB, you might need to adjust how triangles are annotated - ensure the fourth point is included (even if it means duplicating one vertex) to maintain the required format.\n\nCould you share how your triangular objects are currently annotated in your dataset? That would help diagnose if there's an annotation-specific issue."
      },
      {
        "user": "Y-T-G",
        "body": "You can try this\n\nhttps://github.com/ultralytics/ultralytics/pull/16851"
      }
    ]
  },
  {
    "issue_number": 20628,
    "title": "使用yolo11n训练分割模型，模型大小是预训练模型的2倍",
    "author": "tjpulfn",
    "state": "open",
    "created_at": "2025-05-14T03:18:27Z",
    "updated_at": "2025-06-17T00:23:38Z",
    "labels": [
      "question",
      "Stale",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n我训练了yolo11n-seg的模型，使用中间保存结果best.pt进行预测，发现best.pt模型大小是12M，而官方的yolo11n-seg只有5.9M，请问是什么原因\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @tjpulfn, thank you for your interest in Ultralytics 🚀! We recommend visiting the [Docs](https://docs.ultralytics.com/) where you can find detailed guides and many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, as well as answers to frequently asked questions.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate further.\n\nIf you are asking a custom training ❓ Question, please include as much detail as possible, such as sample images from your dataset, your training logs, and verify that you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best:\n- For real-time chat, join [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussions, visit [Discourse](https://community.ultralytics.com/)\n- For community sharing, check out our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nMake sure you are using the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) met in a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following verified environments (with up-to-date dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response. An Ultralytics engineer will review your issue and assist you soon 😊"
      },
      {
        "user": "Y-T-G",
        "body": "It has extra states for resuming."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20630,
    "title": "I can't reproduce the accuracy of yolo11n.yaml on COCO2017.",
    "author": "ctx289",
    "state": "open",
    "created_at": "2025-05-14T03:39:40Z",
    "updated_at": "2025-06-17T00:23:36Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI can't reproduce the accuracy of yolo11n.yaml on COCO2017 when training with 8 NVIDIA V100 GPUs. Here are my training environment and parameters. I'm wondering if there are any problems that lead to the misalignment of the accuracy. My PyTorch isn't the latest version, but I don't think it will have a significant impact on the training accuracy. I've looked through the documentation, but I can't seem to find the specific training configuration used when we train YOLO11. I hope you can help me solve this problem.Thank you very much for your answer.\n\n<img width=\"310\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f90704c8-e240-4349-8307-aa928b05393a\" />\n\nmodel=YOLO('/fuxi_team3_intern/persons/sweethxchen/ultralytics/ultralytics/cfg/models/11/yolo11n.yaml')\nmodel.train(data='/fuxi_team3_intern/persons/sweethxchen/ultralytics/ultralytics/cfg/datasets/coco.yaml',\n            epochs=500, imgsz=640, batch=128, device=[0,1,2,3,4,5,6,7])\n\npython3 -m torch.distributed.run --nproc_per_node 8 train.py --device 0,1,2,3,4,5,6,7 \n\nepochs: 500\ntime: null\npatience: 100\nbatch: 128\nimgsz: 640\nsave: true\nsave_period: -1\ncache: false\ndevice:\n- 0\n- 1\n- 2\n- 3\n- 4\n- 5\n- 6\n- 7\nworkers: 8\nproject: null\nname: train5\nexist_ok: false\npretrained: true\noptimizer: auto\nverbose: true\nseed: 0\ndeterministic: true\nsingle_cls: false\nrect: false\ncos_lr: false\nclose_mosaic: 10\nresume: false\namp: true\nfraction: 1.0\nprofile: false\nfreeze: null\nmulti_scale: false\noverlap_mask: true\nmask_ratio: 4\ndropout: 0.0\nval: true\nsplit: val\nsave_json: false\nconf: null\niou: 0.7\nmax_det: 300\nhalf: false\ndnn: false\nplots: true\nsource: null\nvid_stride: 1\nstream_buffer: false\nvisualize: false\naugment: false\nagnostic_nms: false\nclasses: null\nretina_masks: false\nembed: null\nshow: false\nsave_frames: false\nsave_txt: false\nsave_conf: false\nsave_crop: false\nshow_labels: true\nshow_conf: true\nshow_boxes: true\nline_width: null\nformat: torchscript\nkeras: false\noptimize: false\nint8: false\ndynamic: false\nsimplify: true\nopset: null\nworkspace: null\nnms: false\nlr0: 0.01\nlrf: 0.01\nmomentum: 0.937\nweight_decay: 0.0005\nwarmup_epochs: 3.0\nwarmup_momentum: 0.8\nwarmup_bias_lr: 0.1\nbox: 7.5\ncls: 0.5\ndfl: 1.5\npose: 12.0\nkobj: 1.0\nnbs: 64\nhsv_h: 0.015\nhsv_s: 0.7\nhsv_v: 0.4\ndegrees: 0.0\ntranslate: 0.1\nscale: 0.5\nshear: 0.0\nperspective: 0.0\nflipud: 0.0\nfliplr: 0.5\nbgr: 0.0\nmosaic: 1.0\nmixup: 0.0\ncutmix: 0.0\ncopy_paste: 0.0\ncopy_paste_mode: flip\nauto_augment: randaugment\nerasing: 0.4\ncrop_fraction: 1.0\ncfg: null\ntracker: botsort.yaml\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @ctx289, thank you for your detailed question and for sharing your training setup with Ultralytics 🚀! This is an automated response to help guide you while an Ultralytics engineer reviews your issue.\n\nWe recommend checking out the [Docs](https://docs.ultralytics.com/) for reference, especially the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, as many common questions are addressed there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) if possible. This helps us understand and debug the issue efficiently.\n\nFor custom training or reproducibility questions, please ensure you:\n- Share your dataset configuration and any relevant dataset samples\n- Attach complete training logs, especially where discrepancies appear\n- Confirm you’re following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)\n\nJoin our community for more support and discussion! For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧. For in-depth topics, check [Discourse](https://community.ultralytics.com/). Or join the conversation on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease ensure you’re using the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to rule out any issues resolved in recent releases:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following up-to-date, verified environments (all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) are preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify the correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nAn Ultralytics engineer will review your issue and follow up soon. Thank you for helping us improve YOLO!"
      },
      {
        "user": "ctx289",
        "body": "I'm using the version of Ultralytics that is later than 8.3.100. The mAP (mean Average Precision) of 0.5:0.95 that I tested is only 38.2, which has a significant difference."
      },
      {
        "user": "Y-T-G",
        "body": "You can read this\n\nhttps://github.com/ultralytics/ultralytics/issues/17048#issuecomment-2424742030"
      }
    ]
  },
  {
    "issue_number": 20631,
    "title": "Training a Cable detection model, which task should be used? Pose?",
    "author": "Haseeeb21",
    "state": "open",
    "created_at": "2025-05-14T05:13:26Z",
    "updated_at": "2025-06-17T00:23:35Z",
    "labels": [
      "question",
      "Stale",
      "detect",
      "pose"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nMy task is to detect cables (Electric Cables) from drone shot videos.\nThe aerial video is available and I have extracted the frames. I want to ask which kind of labeling should be done on the images?\nAs each cable instance only has 2 points (Starting and Ending).\n\nI have labeled it using Label Studio, where only two points are displaying each cable. Then I have exported it in json format and converted it according to YOLO POSE format (to train the model).\n\nThe training images were around 100 and few images as validation. I have trained the model and predicted it on a video.\n\nI have tweaked the code such that a line is drawn between the cable key-points instead of boxes (which by default is returned by predicting from POSE task). Now the issue is the points are not as much accurate sometimes it's away from the cable and I mean it is not fixed the points move as the drone moves forward. \n\nLike as the video goes on the key points are placed near the cables but very rarely on the exact cable and thus the line is tilted and is not on the exact cable area.\n\nIs this a limitation of the POSE model? or this is as expected?\n\n## Suggest other models for this task\n\nAnd what other types of model / task you would suggest to solve this problem (detect cables)? and which kind of labeling should be done? As I think segmentation can't be done as it requires at least 3 points (closed area) or 4, but here I only have 2 points representing each cable. And the cables are also thin (as seen from the drone).\n\n### Additional\n\nIf there is any pre-existing model for such task, or any model that works well for this task can anyone please share it.\nThanks!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Haseeeb21, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) where you can find detailed [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, which may address some of your questions.\n\nIf this is a ❓ custom training or task selection question, please provide as much detail as possible about your dataset, labeling format, and any relevant training logs or model outputs. This helps us and the community offer you the most relevant guidance.\n\nIf this is a 🐛 Bug Report, please include a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us troubleshoot efficiently.\n\nAn Ultralytics engineer will review your issue and provide further assistance soon 🤖.\n\nJoin our vibrant community for real-time support on [Discord](https://discord.com/invite/ultralytics) 🎧, or for in-depth topic discussions, check out [Discourse](https://community.ultralytics.com/), or connect with others on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nBefore proceeding, please ensure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to see if your issue is already resolved in the latest release:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following up-to-date verified environments (with dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for sharing your use case and feedback!"
      },
      {
        "user": "Y-T-G",
        "body": "You can try segmentation. Or if the labeling is too difficult, then segmentation. And use small imgsz if the cable covers most of the image."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20641,
    "title": "Object Detection Yolov8",
    "author": "Denis-Petre",
    "state": "open",
    "created_at": "2025-05-14T09:48:15Z",
    "updated_at": "2025-06-17T00:23:34Z",
    "labels": [
      "enhancement",
      "question",
      "Stale",
      "App",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI’m currently working on a project that detects objects in floorplans. While the detection works, I’m trying to improve the model by training it further using feedback and corrections. However, I’ve encountered the following issues:\n\nThe feedback is not updated on the website immediately.\nThe dropdown menu is not updated after saving the feedback.\nError: AttributeError: module 'streamlit' has no attribute 'experimental_rerun' (This has been resolved).\nThe fine-tuning process is not working.\nThe feedback is not retained for the next time I use the model.\nColors Issue: RGBA instead of RGB (This has also been resolved).\nCould someone please help me address these issues? Thank you!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Denis-Petre, thank you for sharing your experience with Ultralytics 🚀! We appreciate your detailed feedback and your efforts to improve your object detection project. This is an automated response to help you get the most relevant assistance—an Ultralytics engineer will also review your issue and follow up soon.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us understand and debug the problems you’re facing, especially with feedback retention and fine-tuning.\n\nFor questions about custom training or model feedback workflows, please include as much detail as possible (dataset samples, training logs, relevant code snippets) and check that you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community for real-time support and discussion:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for live chat\n- [Discourse](https://community.ultralytics.com/) for in-depth topics\n- [Subreddit](https://reddit.com/r/Ultralytics) to share and learn from others\n\n## Upgrade\n\nMake sure you are using the latest `ultralytics` package and that all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) are installed in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in any of the following verified environments (with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @Denis-Petre, thanks for using Ultralytics YOLO!\n\nIt looks like you're building a custom Streamlit application using our models with a feedback system. For the issues you're experiencing:\n\n1. For feedback not updating immediately and dropdown menu issues - these could be related to Streamlit's session state management. Try using `st.session_state` to persist values between reruns.\n\n2. For feedback retention - you'll need to implement a storage mechanism (database, file system, etc.) to save annotations between sessions.\n\n3. For fine-tuning not working - could you share more details about your implementation? Are you using our [solutions/streamlit_inference.py](https://docs.ultralytics.com/reference/solutions/streamlit_inference/) as a reference?\n\nIf you could share your code implementation (particularly the feedback and fine-tuning parts), we can provide more specific guidance to resolve these issues."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20642,
    "title": "Imx500 export support for pose models",
    "author": "anjith2006",
    "state": "open",
    "created_at": "2025-05-14T10:09:26Z",
    "updated_at": "2025-06-17T00:23:33Z",
    "labels": [
      "enhancement",
      "Stale",
      "pose",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nI see that Imx500 export only supported for detection models for now.\nis there plan to support other models -- pose, segmentation in future.\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @anjith2006, thank you for your interest in Ultralytics and for submitting your feature request 🚀! We recommend checking out our [Docs](https://docs.ultralytics.com/) for the latest updates, supported export formats, and [YOLO Modes](https://docs.ultralytics.com/modes/). Many common questions about model export are covered there as well.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) if you haven't already. This helps us investigate any issues more efficiently.\n\nFor all custom training or feature requests, sharing as much detail as possible about your use case is appreciated. The more context you provide, the better we can understand your needs!\n\nJoin the Ultralytics community for discussion and support:\n- Real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth Q&A on [Discourse](https://community.ultralytics.com/)\n- Community threads on our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nMake sure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in these up-to-date verified environments (all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response to help you get started. An Ultralytics engineer will review your request and assist you soon 😊"
      },
      {
        "user": "glenn-jocher",
        "body": "You're right, currently IMX500 export is limited to detection models as confirmed in our code base. The export function specifically checks for detection tasks and raises an error for other model types.\n\nWhile I don't have specific timeline information on when pose and segmentation model support might be added, this is a good feature request. We're continuously working to expand export capabilities across different model types and platforms.\n\nIf you're interested in contributing to this implementation, it would involve modifying the `export_imx` function in `ultralytics/engine/exporter.py` to properly handle the additional output layers for pose and segmentation models. The main challenge would be adapting the NMS wrapper to correctly process keypoints for pose models or masks for segmentation models.\n\nWould you be interested in collaborating on this feature? We'd be happy to provide guidance if you'd like to submit a PR."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20646,
    "title": "Use of openCV means Geotiffs cannot be read, suggest tiffile instead",
    "author": "robmarkcole",
    "state": "open",
    "created_at": "2025-05-14T14:24:15Z",
    "updated_at": "2025-06-17T00:23:31Z",
    "labels": [
      "enhancement",
      "dependencies",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nAs title, and a script below to validate. FR is to instead use [tiffile](https://pypi.org/project/tifffile/) to read tiffs\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"opencv-rasterio-compatibility.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1FY0oEjORqI83Fg4vg9TcqdJzoLnw07jy\n\"\"\"\n\n# Cell 1 — install needed packages\n# !pip install --quiet opencv-python-headless rasterio\n# !pip install tifffile\n\n# Cell 2 — imports & suppress warning\nimport numpy as np\nimport cv2\nimport rasterio\nimport warnings\nfrom rasterio.errors import NotGeoreferencedWarning\n\n# suppress the “Dataset has no geotransform…” warning\nwarnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)\n\n# Cell 3 — helper functions (updated read_with_cv2)\n\ndef write_cv2_multipage(path, data):\n    \"\"\"Write a (bands, H, W) uint8 array as a multi-page TIFF via OpenCV.\"\"\"\n    pages = [data[i] for i in range(data.shape[0])]\n    ok = cv2.imwritemulti(path, pages)\n    if not ok:\n        raise RuntimeError(f\"cv2.imwritemulti failed for {path}\")\n\ndef read_with_rasterio(path):\n    \"\"\"\n    Read a TIFF written by OpenCV as multiple pages.\n    If src.count > 1 it'll read normally; otherwise\n    it will open each GDAL subdataset (TIFF IFD) as its own band.\n    Returns a (bands, H, W) array.\n    \"\"\"\n    with rasterio.open(path) as src:\n        if src.count > 1:\n            return src.read()\n        elif src.subdatasets:\n            bands = []\n            for sub in src.subdatasets:\n                with rasterio.open(sub) as s:\n                    bands.append(s.read(1))\n            return np.stack(bands, axis=0)\n        else:\n            return src.read()\n\ndef write_rasterio_multiband(path, data):\n    \"\"\"\n    Write a (bands, H, W) uint8 array as a multi-band TIFF via rasterio.\n    This creates one IFD with 'bands' samples per pixel, which OpenCV\n    can still read back via imreadmulti.\n    \"\"\"\n    bands, H, W = data.shape\n    profile = {\n        'driver': 'GTiff',\n        'dtype': data.dtype,\n        'count': bands,          # <— n bands in one IFD\n        'height': H,\n        'width': W,\n        # you can also add compression or tiling here…\n    }\n    with rasterio.open(path, 'w', **profile) as dst:\n        for i in range(bands):\n            dst.write(data[i], i + 1)\n\ndef read_with_cv2(path):\n    \"\"\"\n    Read a multi-page or multi-sample TIFF with OpenCV;\n    returns a (bands, H, W) array.\n    \"\"\"\n    # First, try imreadmulti (multi-page TIFF)\n    ok, mats = cv2.imreadmulti(path, flags=cv2.IMREAD_UNCHANGED)\n    if ok and len(mats) > 1:\n        return np.stack(mats, axis=0)\n\n    # Fallback: read as a single multi-channel image\n    img = cv2.imread(path, flags=cv2.IMREAD_UNCHANGED)\n    if img is None:\n        raise RuntimeError(f\"cv2 failed to read {path} in either mode\")\n    # if grayscale single-band, expand dims\n    if img.ndim == 2:\n        return img[np.newaxis, ...]\n    # img.shape == (H, W, C)\n    return img.transpose(2, 0, 1)\n\n# Cell 4 — generate data + run enhanced round‐trip tests\n\nimport tifffile\nimport traceback\n\n# Parameters\nH, W, n_channels = 64, 80, 10\n\n# Create a test stack: each band is a 0→255 horizontal gradient\ndata = np.zeros((n_channels, H, W), dtype=np.uint8)\nfor i in range(n_channels):\n    ramp = np.linspace(0, 255, W, dtype=np.uint8)\n    data[i] = np.tile(ramp, (H, 1))\n\n# 1️⃣ OpenCV → rasterio → OpenCV\ncv2_path = 'cv2_multipage.tif'\nwrite_cv2_multipage(cv2_path, data)\nrio_read1 = read_with_rasterio(cv2_path)\nprint(\"OpenCV→rasterio:\", rio_read1.shape, rio_read1.dtype)\nassert np.array_equal(rio_read1, data), \"❌ Mismatch OpenCV→rasterio!\"\n\n# 2️⃣ rasterio → OpenCV (expected to FAIL)\nrio_path = 'rasterio_multiband.tif'\nwrite_rasterio_multiband(rio_path, data)\n\nfailed = False\ntry:\n    cv2_read2 = read_with_cv2(rio_path)\n    # If it didn't raise, but returned something, check if shape is wrong\n    if not np.array_equal(cv2_read2, data):\n        print(\"⚠️ OpenCV read, but data mismatch\")\n    else:\n        print(\"⚠️ OpenCV unexpectedly succeeded and matched data\")\n    failed = False\nexcept Exception as e:\n    print(\"✅ OpenCV failed to read rasterio TIFF (as expected):\")\n    traceback.print_exc(limit=1)\n    failed = True\n\nassert failed, \"❌ OpenCV should not be able to open a rasterio‐written GeoTIFF!\"\n\n# 3️⃣ rasterio → tifffile (expected to SUCCEED)\n\ndef read_with_tifffile(path, bands_expected):\n    arr = tifffile.imread(path)\n    # arr might be (bands, H, W) or (H, W, bands)\n    if arr.ndim == 3:\n        if arr.shape[0] == bands_expected:\n            return arr\n        elif arr.shape[2] == bands_expected:\n            return arr.transpose(2, 0, 1)\n    elif arr.ndim == 2:\n        return arr[np.newaxis, ...]\n    raise RuntimeError(f\"Unexpected shape {arr.shape} from tifffile\")\n\ntif_read = read_with_tifffile(rio_path, n_channels)\nprint(\"rasterio→tifffile:\", tif_read.shape, tif_read.dtype)\nassert np.array_equal(tif_read, data), \"❌ Mismatch rasterio→tifffile!\"\n\nprint(\"✅ All enhanced tests passed!\")\n```\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @robmarkcole, thank you for bringing this up and for providing such a detailed script to illustrate your feature request! 🚀 This is an automated response to acknowledge your contribution—an Ultralytics engineer will review your suggestion and assist you soon.\n\nIf your request relates to a 🐛 bug, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) if you haven't already, as it helps us investigate issues efficiently.\n\nFor general questions or custom training inquiries, including dataset examples and training logs ensures we can help more effectively. Make sure to review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community for support or discussion:\n- [Discord](https://discord.com/invite/ultralytics) for live chat 🎧\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) to share and explore with others\n\n## Upgrade\n\nTo ensure your issue isn’t already resolved, please upgrade to the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in these up-to-date verified environments (with all dependencies such as [CUDA](https://developer.nvidia.com/cuda-zone), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM ([GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/))\n- **Amazon** Deep Learning AMI ([AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/))\n- **Docker Image** ([Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf the badge above is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify the correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThanks again for your input and for helping improve Ultralytics!"
      },
      {
        "user": "glenn-jocher",
        "body": "Thank you for reporting this issue and providing a detailed demonstration script. You're correct that OpenCV has limitations when handling GeoTIFF files, particularly with proper georeferencing information. We could modify the `imread` function in `ultralytics/utils/patches.py` to use tifffile as a fallback when OpenCV's `imdecodemulti` fails to properly read a TIFF file. This would maintain compatibility with standard TIFFs while adding support for GeoTIFFs.\n\nWould you be interested in submitting a PR that implements this change? The implementation could conditionally import tifffile when needed and use it as a fallback reader for TIFFs that OpenCV struggles with. This would be a valuable improvement for users working with geospatial imagery."
      },
      {
        "user": "robmarkcole",
        "body": "@glenn-jocher I had a quick play, the issue is that `imdecodemulti` doesn't fail - it returns a single channel only. Therefore you would need to check for the _expected_ number of channels to determine a failure. I'm not sure if you would want to pass a `channels` arg to determine this? Overall it would be preferable to use tiffile IMO, but I will await your suggestions. \n\nOne other issue to highlight is that tiff data is not typically 8-bit, but the current normalisation expects 8-bit 0-255 pixel range. \n\nI attached a 16 bit geotiff incase it is useful for some experimentation #\n\n[s2_test_RGBNIR_cropped.tif.zip](https://github.com/user-attachments/files/20240767/s2_test_RGBNIR_cropped.tif.zip)"
      }
    ]
  },
  {
    "issue_number": 20649,
    "title": "Which packages to export a full_integer_quant INT8 tflite Yolov8 model?",
    "author": "fanchb",
    "state": "open",
    "created_at": "2025-05-14T22:36:46Z",
    "updated_at": "2025-06-17T00:23:30Z",
    "labels": [
      "question",
      "dependencies",
      "Stale",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nTo be able to run an object detection model on an Android DSP, I need a full quantized INT8 tfilte version. I have tried to export my model with YoloV8 and different packages versions with no success (several different errors reported in past issues). I started from YoloV8.3.99 tag with following packages:\n\n- Python 3.8\n- tensorflow 2.13.0\n- tf-keras 2.13.1\n- onnx2tf 1.17.0 and later onnx2tf 1.20.0\n\nAre there known package versions which work for that INT8 export with most recent YoloV8 tag? Are Python 3.10 and onnx2tf 1.26.3 required? \n\nThank you for your help!\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @fanchb, thank you for your interest in Ultralytics 🚀! This is an automated response to help you get started while an Ultralytics engineer reviews your question.\n\nIf this is a 🐛 Bug Report, please provide a minimum reproducible example (MRE) if you haven't already. This will help us understand and debug the issue more effectively.\n\nIf this is a custom training ❓ Question, please include as much detail as possible, such as dataset image examples, training logs, and confirmation that you're following our recommended training tips for best results.\n\nWe also encourage you to join the Ultralytics community for real-time discussions and support. You can connect with others on Discord, Discourse, or the Ultralytics Subreddit to share your experience and get help from fellow users.\n\n## Upgrade\n\nPlease ensure you are using the latest version of the `ultralytics` package and all requirements in a Python 3.8 or later environment with PyTorch 1.8 or newer to verify your issue is not already addressed in the most recent release. Upgrade using:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in a variety of up-to-date verified environments, including cloud notebooks with free GPU, Google Cloud Deep Learning VM, Amazon Deep Learning AMI, and Docker images. Make sure your environment includes all necessary dependencies such as CUDA, CUDNN, Python, and PyTorch.\n\n## Status\n\nUltralytics CI tests run every 24 hours and on every commit to verify correct operation across all YOLO modes and tasks on macOS, Windows, and Ubuntu.\n\nAn Ultralytics engineer will review your issue and assist you soon. Thank you for your patience and for being part of the Ultralytics community!"
      },
      {
        "user": "Y-T-G",
        "body": "Why not use the latest Ultralytics?"
      },
      {
        "user": "fanchb",
        "body": "@Y-T-G , good point !\n\nI wanted to compare between FP16 (running on a GPU but overheating at high inference rate) and INT8 (running on the DSP) performances and I believed I needed to stay on YoloV8 since there was an issue with GPU delegate in YoloV11. But you just fixed it with commits described in #20436. Wonderful!\n\nSo, with latest Ultralytics, no known limitations in the export of a best_full_interger_quant model?\n\nThank you!"
      }
    ]
  },
  {
    "issue_number": 20655,
    "title": "YOLO 11 OBB -> ONNX -> RKNN ERROR AND YOLO 11 OBB -> ONNX NO DETECTION",
    "author": "VohminV",
    "state": "open",
    "created_at": "2025-05-15T07:32:59Z",
    "updated_at": "2025-06-17T00:23:29Z",
    "labels": [
      "Stale",
      "OBB",
      "embedded",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nPredict\n\n### Bug\n\n\n# Load the YOLO11 model\nmodel = YOLO(\"object.pt\")\n\n# Export the model to RKNN format\n# 'name' can be one of rk3588, rk3576, rk3566, rk3568, rk3562, rv1103, rv1106, rv1103b, rv1106b, rk2118\nmodel.export(format=\"rknn\", name=\"rk3588\") ```\n\n[v2.3.2](https://github.com/airockchip/rknn-toolkit2/releases/tag/v2.3.2)\nI rknn-toolkit2 version: 2.3.2\nI Loading : 100%|██████████████████████████████████████████████| 214/214 [00:00<00:00, 16960.13it/s]\nI OpFusing 0: 100%|██████████████████████████████████████████████| 100/100 [00:00<00:00, 121.46it/s]\nI OpFusing 1 : 100%|██████████████████████████████████████████████| 100/100 [00:01<00:00, 80.63it/s]\nI OpFusing 0 : 100%|██████████████████████████████████████████████| 100/100 [00:02<00:00, 43.68it/s]\nI OpFusing 1 : 100%|██████████████████████████████████████████████| 100/100 [00:02<00:00, 39.87it/s]\nI OpFusing 0 : 100%|██████████████████████████████████████████████| 100/100 [00:02<00:00, 33.54it/s]\nI OpFusing 1 : 100%|██████████████████████████████████████████████| 100/100 [00:03<00:00, 32.73it/s]\nI OpFusing 2 : 100%|██████████████████████████████████████████████| 100/100 [00:03<00:00, 27.54it/s]\nI rknn building ...\nE RKNN: [11:27:59.988] Unkown op target: 0\nE RKNN: [11:27:59.990] Unkown op target: 0\nE RKNN: [11:27:59.991] Unkown op target: 0\nE RKNN: [11:27:59.993] Unkown op target: 0\nE RKNN: [11:28:00.002] Unkown op target: 0\nE RKNN: [11:28:00.004] Unkown op target: 0\n\n[v2.3.0](https://github.com/airockchip/rknn-toolkit2/releases/tag/v2.3.0)\n\nONNX: export success ✅ 2.0s, saved as 'object.onnx' (10.2 MB)\nI rknn-toolkit2 version: 2.3.0\nI Loading : 100%|██████████████████████████████████████████████| 214/214 [00:00<00:00, 21788.59it/s]\nI OpFusing 0: 100%|██████████████████████████████████████████████| 100/100 [00:00<00:00, 123.52it/s]\nI OpFusing 1 : 100%|██████████████████████████████████████████████| 100/100 [00:01<00:00, 69.26it/s]\nI OpFusing 0 :  80%|█████████████████████████████████████▌         | 80/100 [00:01<00:00, 46.19it/s]\nW build: Show op fuse match nodes:\nRule: reduce_reshape_op_around_concat\nSubgraph:\nOp type: Concat\nOp name: /model.23/Cos_output_0_concat\n  Input:\n    /model.23/Cos_output_0_rs : [1, 1, 1, 2100]\n    /model.23/Sin_output_0_rs : [1, 1, 1, 2100]\n    /model.23/Cos_output_0_rs : [1, 1, 1, 2100]\n  Output:\n    /model.23/Cos_output_0_concat-rs : [1, 3, 1, 2100]\n  Attribute:\n    axis : 1\n \n\nW You can add disable_rules=['reduce_reshape_op_around_concat'] in rknn.config() to temporarily disable the error rule!\n\nE build: Traceback (most recent call last):\n  File \"rknn/api/rknn_log.py\", line 344, in rknn.api.rknn_log.error_catch_decorator.error_catch_wrapper\n  File \"rknn/api/rknn_base.py\", line 1962, in rknn.api.rknn_base.RKNNBase.build\n  File \"rknn/api/graph_optimizer.py\", line 2108, in rknn.api.graph_optimizer.GraphOptimizer.fuse_ops\n  File \"rknn/api/ir_graph.py\", line 1772, in rknn.api.ir_graph.IRGraph.remove_nodes\n  File \"rknn/api/ir_graph.py\", line 1730, in rknn.api.ir_graph.IRGraph.remove_node\nKeyError: '/model.23/Cos_output_0_rs'\n\nW build: ===================== WARN(2) =====================\nE rknn-toolkit2 version: 2.3.0\nRKNN: export failure ❌ 5.1s: Traceback (most recent call last):\n  File \"rknn/api/rknn_log.py\", line 344, in rknn.api.rknn_log.error_catch_decorator.error_catch_wrapper\n  File \"rknn/api/rknn_base.py\", line 1962, in rknn.api.rknn_base.RKNNBase.build\n  File \"rknn/api/graph_optimizer.py\", line 2108, in rknn.api.graph_optimizer.GraphOptimizer.fuse_ops\n  File \"rknn/api/ir_graph.py\", line 1772, in rknn.api.ir_graph.IRGraph.remove_nodes\n  File \"rknn/api/ir_graph.py\", line 1730, in rknn.api.ir_graph.IRGraph.remove_node\nKeyError: '/model.23/Cos_output_0_rs'\n\nTraceback (most recent call last):\n  File \"rknn/api/rknn_log.py\", line 344, in rknn.api.rknn_log.error_catch_decorator.error_catch_wrapper\n  File \"rknn/api/rknn_base.py\", line 1962, in rknn.api.rknn_base.RKNNBase.build\n  File \"rknn/api/graph_optimizer.py\", line 2108, in rknn.api.graph_optimizer.GraphOptimizer.fuse_ops\n  File \"rknn/api/ir_graph.py\", line 1772, in rknn.api.ir_graph.IRGraph.remove_nodes\n  File \"rknn/api/ir_graph.py\", line 1730, in rknn.api.ir_graph.IRGraph.remove_node\nKeyError: '/model.23/Cos_output_0_rs'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/linuxlite/ONNX.py\", line 8, in <module>\n    model.export(format=\"rknn\", name=\"rk3588\")  # \n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/linuxlite/.local/lib/python3.12/site-packages/ultralytics/engine/model.py\", line 742, in export\n    return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/linuxlite/.local/lib/python3.12/site-packages/ultralytics/engine/exporter.py\", line 456, in __call__\n    f[14], _ = self.export_rknn()\n               ^^^^^^^^^^^^^^^^^^\n  File \"/home/linuxlite/.local/lib/python3.12/site-packages/ultralytics/engine/exporter.py\", line 182, in outer_func\n    raise e\n  File \"/home/linuxlite/.local/lib/python3.12/site-packages/ultralytics/engine/exporter.py\", line 177, in outer_func\n    f, model = inner_func(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/linuxlite/.local/lib/python3.12/site-packages/ultralytics/engine/exporter.py\", line 1206, in export_rknn\n    rknn.build(do_quantization=False)  # TODO: Add quantization support\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/linuxlite/.local/lib/python3.12/site-packages/rknn/api/rknn.py\", line 192, in build\n    return self.rknn_base.build(do_quantization=do_quantization, dataset=dataset, expand_batch_size=rknn_batch_size)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"rknn/api/rknn_log.py\", line 349, in rknn.api.rknn_log.error_catch_decorator.error_catch_wrapper\n  File \"rknn/api/rknn_log.py\", line 95, in rknn.api.rknn_log.RKNNLog.e\nValueError: Traceback (most recent call last):\n  File \"rknn/api/rknn_log.py\", line 344, in rknn.api.rknn_log.error_catch_decorator.error_catch_wrapper\n  File \"rknn/api/rknn_base.py\", line 1962, in rknn.api.rknn_base.RKNNBase.build\n  File \"rknn/api/graph_optimizer.py\", line 2108, in rknn.api.graph_optimizer.GraphOptimizer.fuse_ops\n  File \"rknn/api/ir_graph.py\", line 1772, in rknn.api.ir_graph.IRGraph.remove_nodes\n  File \"rknn/api/ir_graph.py\", line 1730, in rknn.api.ir_graph.IRGraph.remove_node\nKeyError: '/model.23/Cos_output_0_rs'\n\n### Environment\n\nUltralytics Orange Pi 5 Orangepi5_1.1.8_ubuntu_focal_desktop_xfce_linux6.1.43.7z\n\n### Minimal Reproducible Example\n\n```\n# Load the YOLO11 model\nmodel = YOLO(\"object.pt\")\n\n# Export the model to RKNN format\n# 'name' can be one of rk3588, rk3576, rk3566, rk3568, rk3562, rv1103, rv1106, rv1103b, rv1106b, rk2118\nmodel.export(format=\"rknn\", name=\"rk3588\") \n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @VohminV, thank you for reporting this and for sharing detailed logs and your environment info! 🚀 This is an automated response to help guide you while an Ultralytics engineer reviews your issue.\n\nWe recommend checking the [Docs](https://docs.ultralytics.com/) for additional [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, as many common export and deployment questions are covered there.\n\nSince this is a 🐛 Bug Report, thank you for including a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/)—this is very helpful for troubleshooting!\n\nTo further assist us and the community:\n- Please make sure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). You can upgrade with:\n  ```bash\n  pip install -U ultralytics\n  ```\n- If you haven't already, please try your export and conversion steps with the most recent package versions.\n\nJoin the Ultralytics community for more support:\n- For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussions, head over to [Discourse](https://community.ultralytics.com/)\n- Share and discuss on our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Verified Environments\n\nYOLO models can be run in the following environments (with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nAn Ultralytics engineer will review your report and assist you further soon. Thank you for helping us improve YOLO11 and for being part of the Ultralytics community! 🧡"
      },
      {
        "user": "Y-T-G",
        "body": "RKNN export doesn't support OBB yet.\n\n![Image](https://github.com/user-attachments/assets/2cae9160-98c1-41a2-8d4d-bf7290e049eb)"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20656,
    "title": "NON MAX Suppression",
    "author": "omidgh1379",
    "state": "open",
    "created_at": "2025-05-15T08:19:35Z",
    "updated_at": "2025-06-17T00:23:28Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI try to apply Non max suppression on my model output and I used this code to get the first result \n\n``\nmodel = YOLO('best.pt')\n\na = next(iter(valid_loader))\nresult = a['img']\n\nresult =result.to(torch.float32)\n \nresult = model.model(result)[0][... , 0:14  , :]\n\n``\n\nresult.shape  torch.size([1,  14 , 8400 )  where the 14 the first for is my box and I have 10 class - I remove the masks since when I applied them the class column in non max suppression include mask as a index for different classes. \n\n\nmy issue is that each time when I run this NMS \nafter = non_max_suppression(result , conf_thres=0.25, iou_thres=0.5 , nc =10 ,agnostic=True)\n \nI get a diffrent result one time I get 300 output and one time I get 50 output. \n\nI wanted to know why each time I get different result knowing the fact that my result is the same. \n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @omidgh1379, thank you for reaching out and for providing details about your experience with Non-Max Suppression in Ultralytics 🚀! This is an automated response to help you get started. An Ultralytics engineer will review your question and assist you soon.\n\nWe recommend visiting the [Docs](https://docs.ultralytics.com/) for comprehensive guides and examples, including both [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate the issue further.\n\nFor custom training or inference ❓ questions, please include as much information as possible—such as sample images, inference code snippets, or logs—to help us better understand the scenario. Also, review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best! For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧. For in-depth discussions, explore [Discourse](https://community.ultralytics.com/). You can also join conversations on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease upgrade to the latest `ultralytics` package and ensure all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) are installed in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO is verified in several up-to-date environments (with all dependencies, [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM ([GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/))\n- **Amazon** Deep Learning AMI ([AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/))\n- **Docker Image** ([Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu every 24 hours and on each commit.\n\nThank you for your contribution and patience!"
      },
      {
        "user": "Y-T-G",
        "body": "`non_max_suppression` runs inplace so the original result is modified. You can pass `inplace=False`."
      },
      {
        "user": "omidgh1379",
        "body": "Thank you my issue is solved "
      }
    ]
  },
  {
    "issue_number": 20660,
    "title": "The difference between using Python reasoning and C++reasoning",
    "author": "Bigknowledges",
    "state": "open",
    "created_at": "2025-05-15T11:20:47Z",
    "updated_at": "2025-06-17T00:23:27Z",
    "labels": [
      "question",
      "Stale",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nFor the same model and picture. I used Python inference: official PT file inference, onnxruntime inference. I used OpenCV:: dnn, onnxruntime, and libtorch for C++inference. At first, I thought the inconsistent results were due to the loss of accuracy caused by the model export settings and transferring from a. pt file to an. onnx file. It wasn't until the Python onnxruntime inference was completed that it was very similar to the official pt inference and the results were acceptable. But when I use C++reasoning, the jagged edges are magnified to an unacceptable size. I have tried many training parameters but have not been able to achieve satisfactory results. But Python language inference does not have this problem, which is strange. I hope to receive some advice (I didn't find this problem before, until this time when my training target was relatively small).\n\n![Image](https://github.com/user-attachments/assets/3fab91d4-0327-4fae-8abf-85740227bedf)\n\n![Image](https://github.com/user-attachments/assets/212d54f9-c1a9-4f64-8d84-b0b39b3370bc)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Bigknowledges, thank you for reaching out and sharing your detailed question with the Ultralytics team! 🚀 This is an automated response to help you get started—an Ultralytics engineer will also review your issue and assist you soon.\n\nWe recommend checking out the [Docs](https://docs.ultralytics.com/) for more information, including examples for [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage. Many frequently asked questions are already covered there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) so we can better understand and debug the issue. For inference differences between Python and C++ backends, sharing:\n- The exact export commands and parameters you used\n- Sample input images\n- Inference scripts/snippets for both Python and C++\n- Screenshots or outputs highlighting the differences\n\n...will help us assist you more efficiently.\n\nIf your inquiry is about deployment or custom inference, please include as much detail as possible, such as environment, library versions, and any model modifications. Also, review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) to ensure optimal training and inference performance.\n\nYou're welcome to join the Ultralytics community for more interactive support and discussion:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for live chat\n- [Discourse](https://community.ultralytics.com/) for structured discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) to share experiences and tips\n\n## Upgrade\n\nTo rule out any issues already fixed in recent updates, please upgrade to the latest `ultralytics` package and ensure all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) are satisfied in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for using Ultralytics!"
      },
      {
        "user": "Y-T-G",
        "body": "Are you using same processing like \n\nhttps://github.com/ultralytics/ultralytics/tree/main/examples/YOLOv8-Segmentation-ONNXRuntime-Python"
      },
      {
        "user": "Ivan-developer0",
        "body": "**Hi @Bigknowledges 👋**\n\n**Thanks for your detailed question!**\n\nYou're noticing a difference in inference quality between Python and C++ backends when using the same YOLO model:  \n- Python inference with official `.pt` and ONNX Runtime gives acceptable results  \n- C++ inference (OpenCV dnn, ONNX Runtime, libtorch) causes jagged edges and worse output  \n\nThis can happen due to subtle differences in preprocessing, postprocessing, or backend implementation details between Python and C++ environments.\n\nTo help you resolve this:\n\n1. **Ensure identical preprocessing steps** (resizing, normalization, padding) are applied in both Python and C++ inference code. Small differences here can cause large output quality changes.  \n2. **Verify consistent postprocessing** (NMS thresholds, confidence thresholds) is used across both implementations.  \n3. Share your exact **export commands and parameters**, so we can check if any export setting affects C++ compatibility.  \n4. Compare your **inference scripts/snippets** for Python and C++ to spot differences.  \n5. Try running the **official ONNX Runtime Python example** from Ultralytics [here](https://github.com/ultralytics/ultralytics/tree/main/examples/YOLOv8-Segmentation-ONNXRuntime-Python) as a reference.  \n\nAlso:\n\n- Make sure your C++ environment libraries (OpenCV, ONNX Runtime, libtorch) are up-to-date and match the versions recommended by Ultralytics.  \n- Upgrade to the latest `ultralytics` Python package as well:  \n \n`  pip install -U ultralytics`\n\n- Test on simple known datasets to isolate whether the issue is dataset or deployment related.\n\n**Please provide your export commands, sample inputs, and inference code snippets for both Python and C++ if you want more tailored assistance!**\n"
      }
    ]
  },
  {
    "issue_number": 20663,
    "title": "Training Ineffective (mAP ≈ 0) When Number of Training Images ≤ Batch Size",
    "author": "devansh-lodha",
    "state": "open",
    "created_at": "2025-05-15T15:47:31Z",
    "updated_at": "2025-06-17T00:23:25Z",
    "labels": [
      "bug",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nTrain\n\n### Bug\n\nWhen training a YOLO model (e.g.`yolo11s.pt`) on a custom dataset, the model achieves a mean Average Precision (mAP50-95) score that is effectively zero if the number of unique training images (`N_imgs`) is less than or equal to the specified `batch_size`.\nOnce `N_imgs` becomes strictly greater than `batch_size`, the mAP50-95 score shows a significant jump, indicating that learning starts to occur as expected. This behavior has been observed consistently across different `batch_size` values (tested systematically with 32 and 128), with the \"jump point\" for mAP scaling accordingly.\n\nThis suggests a potential issue in the data loading mechanism or the training loop that prevents effective training when the total number of training samples cannot form more than one full batch, or is otherwise mishandled when `N_imgs <= batch_size`.\n\n**Expected Behavior:**\nThe model should demonstrate some level of learning (mAP50-95 > ~0.0 and generally increasing with `N_imgs`) even when `N_imgs <= batch_size`, as long as `N_imgs` is at least 1. While performance is expected to be suboptimal with very few images, it should not be consistently near-zero until `N_imgs` strictly exceeds `batch_size`. The training process should effectively utilize all provided training images.\n\n**Actual Behavior (Summary from more extensive tests):**\nThe mAP50-95 remains near zero until `N_imgs > batch_size`.\n\n**Test 1: `batch_size = 32`, `epochs = 10`, `model = yolo11s.pt` (using Ultralytics `8.3.134`)**\n| Num Train Images | mAP50-95 |\n|------------------|----------|\n| 1                | 0.0006   |\n| 2                | 0.0007   |\n| 4                | 0.0010   |\n| 8                | 0.0007   |\n| 15               | 0.0009   |\n| 16               | 0.0008   |\n| 30               | 0.0007   |\n| 31               | 0.0007   |\n| 32 (== BS)       | 0.0007   |\n| **33 (> BS)**    | **0.0130**   |\n| 34               | 0.0164   |\n| 40               | 0.0526   |\n| 64               | 0.0791   |\n| 96               | 0.2272   |\n| 128              | 0.3715   |\n\n![Image](https://github.com/user-attachments/assets/6a6a95f2-8ae2-4736-a91d-19f9f1c4ea3f)\n\n**Test 2: `batch_size = 128`, `epochs = 10`, `model = yolo11s.pt` (using Ultralytics `8.3.134`)**\n| Num Train Images | mAP50-95 |\n|------------------|----------|\n| 1                | 0.0012   |\n| 16               | 0.0008   |\n| 32               | 0.0007   |\n| 64               | 0.0006   |\n| 96               | 0.0006   |\n| 120              | 0.0006   |\n| 127              | 0.0006   |\n| 128 (== BS)      | 0.0006   |\n| **129 (> BS)**   | **0.0062**   |\n| 130              | 0.0106   |\n| 140              | 0.0560   |\n| 160              | 0.0763   |\n| 192              | 0.0890   |\n| 256              | 0.0704   |\n\n![Image](https://github.com/user-attachments/assets/ae3d4b6f-efda-4206-822b-a1489d828e60)\n\nAs can be seen, there is a distinct \"jump\" in mAP50-95 only when the number of training images exceeds the batch size. Even when `N_imgs == batch_size`, the performance remains at near-zero levels.\n\nThis behavior could lead users with very small datasets (or those performing initial sanity checks with a tiny subset of images) to incorrectly conclude that their setup or data is entirely non-functional.\n\n### Environment\n\n```\nUltralytics 8.3.134 🚀 Python-3.13.2 torch-2.7.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81156MiB)\nSetup complete ✅ (64 CPUs, 503.7 GB RAM, 199.7/430.7 GB disk)\n\nOS                  Linux-5.15.0-139-generic-x86_64-with-glibc2.31\nEnvironment         Linux\nPython              3.13.2\nInstall             pip\nRAM                 503.73 GB\nDisk                199.7/430.7 GB\nCPU                 AMD EPYC 7452 32-Core Processor\nCPU count           64\nGPU                 NVIDIA A100-SXM4-80GB, 81156MiB\nGPU count           4\nCUDA                12.6\n\nnumpy               ✅ 2.2.5>=1.23.0\nmatplotlib          ✅ 3.10.3>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 11.2.1>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.15.3>=1.4.1\ntorch               ✅ 2.7.0>=1.8.0\ntorch               ✅ 2.7.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.22.0>=0.9.0\ntqdm                ✅ 4.67.1>=4.64.0\npsutil              ✅ 7.0.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.2.3>=1.1.4\nseaborn             ✅ 0.13.2>=0.11.0\nultralytics-thop    ✅ 2.0.14>=2.0.0\n```\n\n### Minimal Reproducible Example\n\n**Methodology Overview:**\n1.  **Dataset:** A public object detection dataset from Roboflow (\"liangdianzhong/-qvdww/3\", 911 training images, 23 classes, YOLO format) was used as the base. Any standard object detection dataset could be used. A consistent validation set was used for all training runs.\n2.  **Model:** `yolo11s.pt` (as downloaded by Ultralytics `8.3.134`) was used for fine-tuning. \n3.  **Experiment Loop:**\n    *   A fixed `batch_size` was chosen (e.g., 32, then 128).\n    *   The number of training images (`N_imgs`) was varied across a predefined list (e.g., [1, ..., 31, 32, 33, ..., 128] for `batch_size=32`).\n    *   For each `N_imgs`:\n        *   A random subset of `N_imgs` images and their corresponding label files were copied from the original training set into a temporary directory.\n        *   A temporary `data_subset.yaml` was created, pointing to this subset for `train` and to the original, consistent validation set for `val`.\n        *   The model was trained from the pre-trained weights using the `yolo` CLI:\n            ```bash\n            yolo task=detect mode=train model=yolo11s.pt data=<path_to_subset_yaml> epochs=10 imgsz=640 batch=<current_batch_size> patience=0 name=train_N<N_imgs>_E10_B<batch_size> plots=True exist_ok=True\n            ```\n            (`patience=0` was used to ensure all 10 epochs completed for fair comparison).\n        *   The final `mAP50-95 (Box)` was recorded from the `results.csv` file generated by the training run.\n4.  **Analysis:** The recorded `mAP50-95` values were plotted against `N_imgs` for each `batch_size` tested.\n\n**Link to GitHub Gist with Full Experimental Script and Setup Details:**\n\n```\nhttps://gist.github.com/devansh-lodha/e026368c4bd45b8bda7acaaa6cfcd8b7\n```\n\n**To Reproduce using the Gist:**\n1.  Clone the Gist or copy the Python script.\n2.  Ensure you have the necessary environment (Python, Ultralytics, Roboflow library - see \"Environment\" section of this bug report and `requirements.txt` included in Gist).\n3.  Set up your Roboflow API key if you intend to download the same dataset (or adapt the script's data loading part for your own local dataset).\n4.  Run the Python script. It will:\n    *   Download the dataset (if configured for Roboflow).\n    *   Create the described training subsets.\n    *   Execute the YOLO training commands for different `N_imgs` values for one or more batch sizes.\n    *   Collect and plot the results, which should demonstrate the mAP jump described.\n\nThe script is configured to test with `batch_size = 32` and then `batch_size = 128` (these can be adjusted in the script). The key observation will be the mAP50-95 scores being near zero until `N_imgs` exceeds the `batch_size`.\n\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @devansh-lodha, thank you for the detailed report and for your interest in Ultralytics 🚀! This is an automated response to help streamline the troubleshooting process. An Ultralytics engineer will also assist you here soon.\n\nWe recommend checking the [Docs](https://docs.ultralytics.com/) for additional [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples, as well as answers to many frequently asked questions.\n\nSince this is a 🐛 Bug Report, thank you for including a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/)—that’s extremely helpful for our debugging process!\n\nIf you have any further details, logs, or clarifications to add, please include them here to assist our team. Also, ensure you’re following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) to optimize your experiments.\n\nJoin the Ultralytics community where it suits you best:\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth discussions: [Discourse](https://community.ultralytics.com/)\n- Community threads: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nTo help us rule out issues that may already be resolved, please upgrade to the latest `ultralytics` package along with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your detailed report and willingness to contribute! One of our engineers will review and respond as soon as possible."
      },
      {
        "user": "Y-T-G",
        "body": "Did you try with `nbs=1`?"
      },
      {
        "user": "devansh-lodha",
        "body": "Hi @Y-T-G ,\n\nThank you so much for the suggestion to try `nbs=1`! This made sense to me because with the default `nbs=64` (Nominal Batch Size), the trainer accumulates gradients and only performs an optimizer step after processing 64 images. If the total number of training images (`N_imgs`) in a subset is less than 64, an optimizer step might not be performed within an epoch, potentially leading to no weight updates and thus no learning.\n\nI have re-run the experiments for both `batch_size=32` and `batch_size=128` using the following parameters in the `yolo` training command for each run:\n*   `nbs=1`\n*   `cache=False`\n*   `seed=0`\n*   `epochs=10`\n*   `patience=0`\n*   `model=yolo11s.pt`\n\n**Test Results: `batch_size = 32`, `nbs=1`**\n\n| Num Train Images | mAP50-95   |\n|------------------|------------|\n| 1                | 0.0007     |\n| 2                | 0.0011     |\n| 4                | 0.0010     |\n| 8                | 0.0006     |\n| 15               | 0.0008     |\n| 16               | 0.0008     |\n| 30               | 0.0008     |\n| 31               | 0.0008     |\n| 32 (== BS)       | 0.0008     |\n| **33 (> BS)**    | **0.0233** |\n| 34               | 0.0258     |\n| 40               | 0.0674     |\n| 64               | 0.0796     |\n| 96               | 0.2454     |\n| 128              | 0.3657     |\n\n![Image](https://github.com/user-attachments/assets/b30c431c-04bc-4ffc-b237-5c2937495a0a)\n\n**Test Results: `batch_size = 128`, `nbs=1`**\n\n| Num Train Images | mAP50-95   |\n|------------------|------------|\n| 1                | 0.0007     |\n| 16               | 0.0009     |\n| 32               | 0.0007     |\n| 64               | 0.0006     |\n| 96               | 0.0005     |\n| 120              | 0.0006     |\n| 127              | 0.0005     |\n| 128 (== BS)      | 0.0006     |\n| **129 (> BS)**   | **0.0060** |\n| 130              | 0.0163     |\n| 140              | 0.0651     |\n| 160              | 0.0706     |\n| 192              | 0.0874     |\n| 256              | 0.0914     |\n\n![Image](https://github.com/user-attachments/assets/9868647b-e136-43ac-800c-3261d6abe895)\n\n**Key Observations with `nbs=1`:**\n1.  mAP50-95 remains near-zero when `N_imgs <= batch_size`.\n2.  A distinct and significant mAP jump occurs only when `N_imgs > batch_size`.\n\nThis persistence of the issue, even when forcing optimizer updates after every processed batch (due to `nbs=1`), suggests the problem is not solely related to gradient accumulation tied to `nbs > 1`. "
      }
    ]
  },
  {
    "issue_number": 20676,
    "title": "Lite ONNX-Inference Version of Ultralytics Without Torch Dependency",
    "author": "Flippchen",
    "state": "open",
    "created_at": "2025-05-16T08:56:34Z",
    "updated_at": "2025-06-17T00:23:24Z",
    "labels": [
      "enhancement",
      "dependencies",
      "Stale",
      "embedded"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nCurrently, installing the `ultralytics` package automatically pulls in `torch` as a dependency, even when the one only intends to use ONNX models for inference. Since Ultralytics already has pre- and post-processing implemented for ONNX export and inference, it would be beneficial to provide a lightweight version (e.g.`ultralytics[lite]`) that omits the torch dependency entirely. This version would rely only on packages such as numpy for inference-related operations.\n\n### Use case\n\nThis feature would reduce the installation size and resource requirements for users deploying ONNX models on edge devices, lightweight servers, or in containerized environments where Torch is unnecessary.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Flippchen, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\n🤖 This is an automated response. An Ultralytics engineer will review your feature request and assist you soon!"
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @Flippchen, thanks for this thoughtful feature request! A lightweight `ultralytics[lite]` package for ONNX inference without the torch dependency would indeed be valuable for edge deployments and containerized environments.\n\nCurrently, you can work around this by using `pip install ultralytics --no-deps` and then manually installing only the minimal dependencies needed for ONNX inference (like numpy, opencv, etc.), as outlined in our [custom installation methods](https://docs.ultralytics.com/quickstart/#custom-installation-methods).\n\nWe'll consider adding this as a proper installation option in a future release. This is a reasonable request that aligns with our goals of making YOLO models more accessible for deployment across different environments."
      },
      {
        "user": "Flippchen",
        "body": "Hi @glenn-jocher, Thanks for your quick response. I thought I had tried this before and received an error message saying that the inference code also relies on Torch/Torch functions, even when using ONNX. Am I missing something?"
      }
    ]
  },
  {
    "issue_number": 20684,
    "title": "I can't count objects with YOLO when i increase the screen size",
    "author": "JeyMarsh",
    "state": "open",
    "created_at": "2025-05-17T11:47:10Z",
    "updated_at": "2025-06-17T00:23:23Z",
    "labels": [
      "question",
      "Stale",
      "detect",
      "solutions"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI'm creating a skecth to count objects with Yolo, and it works normally, but when I increase the screen resolution, it stops counting when a toy car passes through the dot region. I've tried several times to increase the dots in the region, but it doesn't work.\n\n### Additional\n\nimport os\nfrom ultralytics import solutions\nimport serial\nimport cv2\n\n\nmodel = r\"Repositório\\runs\\detect\\train14\\weights\\best.pt\"\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n\ncap = cv2.VideoCapture(0)\nporta = \"COM5\"\nvelocidade = 115200\ntry:\n    conecao = serial.Serial(porta, velocidade, timeout=0.5)\n    print(\"Conectado com a porta\", conecao.portstr)\nexcept serial.SerialException:\n    print(\"Não conectada\")\n    pass\n\n\n\n\n\nregion_points = [(20, 400), (1700, 400)]                                      # line counting\n#region_points = [(0, 400), (1920, 400), (1920, 360), (0, 360)]  # rectangle region\n# region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360), (20, 400)]   # polygon region\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 1820)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n# Video writer\nw, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\ncap.set(cv2.CAP_PROP_FPS, 120)\n\n\n# Initialize object counter object\ncounter = solutions.ObjectCounter(\n    show=True,\n    region=region_points,\n    model=r\"Repositório\\runs\\detect\\train14\\weights\\best.pt\",\n    classes=[0],\n     # count specific classes i.e. person and car with COCO pretrained model.\n    #tracker=\"custom_dataset.yaml\",\n)\n\n# Process video\nwhile cap.isOpened():\n    success, im0 = cap.read()\n\n\n\n\n\n\n    if not success:\n        print(\"Video frame is empty or processing is complete.\")\n        break\n    results = counter(im0)\n\n    in_count = results.in_count\n    out_count = results.out_count\n    total_present = in_count - out_count\n   # boxes = results[0].boxes.xyxy.cpu().tolist()\n   # clss = results[0].boxes.clss.cpu().tolist()\n   # confs = results[0].boxes.conf.cpu().tolist()\n\n    #if boxes is not None:\n    #    for box, cls, conf in zip(boxes, clss, confs):\n    #        color = colors(int(cls), True)\n\n    #    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color, 2)\n\n    #    label = f'{model.names[int(cls)]} {conf:.2f}'\n    #    cv2.putText(im0, label, (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    # Mostra o frame processado com as caixas\n   # cv2.imshow(\"Output\", im0)\n   # if cv2.waitKey(1) & 0xFF == ord('q'):  # Sai do loop se apertar a tecla 'q' for pressionada\n   #     cap.release()\n   #     video_writer.release()\n   #     cv2.destroyAllWindows()\n\n\n\n    # print(results)  # access the output\n\n\n\n    print(\" Total de veículos: \", total_present)\n    if total_present > 0:\n        conecao.write (b'1')\n    else:\n         conecao.write(b'0')\n\n\ncap.release()\nvideo_writer.release()\ncv2.destroyAllWindows()  # destroy all opened windows\nconecao.close()",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @JeyMarsh, thank you for reaching out and for your detailed description and code snippet! 🚀 This is an automated response to help you get the fastest and best support. An Ultralytics engineer will also review your issue soon.\n\nWe recommend visiting the [Docs](https://docs.ultralytics.com/) for more information on usage and troubleshooting. You can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it. This should include the smallest amount of code and data needed to reproduce the issue—this helps us help you faster!\n\nIf this is a ❓ Custom Training or Usage Question, please share as much relevant information as possible, such as sample images or videos, your dataset, and full logs. Also, check that you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community for more help and discussions:\n- For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussions, visit [Discourse](https://community.ultralytics.com/)\n- Or share knowledge on our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nMake sure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in these up-to-date, verified environments (with all dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for helping us improve Ultralytics!"
      },
      {
        "user": "RizwanMunawar",
        "body": "@JeyMarsh Hi, thanks for sharing it. You will need to adjust the region coordinates based on your video resolution to get counts properly."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20385,
    "title": "opencv-python-headless install mechanism",
    "author": "Teriks",
    "state": "open",
    "created_at": "2025-04-27T20:33:30Z",
    "updated_at": "2025-06-16T19:32:47Z",
    "labels": [
      "enhancement",
      "dependencies"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nI would be nice for this package to have some sort of mechanism (perhaps using extras) to specify whether you want the dependency python-opencv, or python-opencv-headless\n\nthe headless variant of opencv is configured for use on systems lacking a window manager or various GL related libraries\n\nFull opencv is not really needed if you are not using the GUI dependent features in your project, and requires installing additional packages on headless systems to make it work correctly when it could just work out of the box\n\nAn extra would probably be the best way to do this, however it would probably cause issues with current dependents that do not expect opencv to be a transient dependency by default \n\nSomething to think about maybe\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Teriks, thank you for your suggestion and for engaging with Ultralytics 🚀! Your input regarding the option to install either `opencv-python` or `opencv-python-headless` is appreciated and has been noted for consideration.\n\nThis is an automated response to help guide you while an Ultralytics engineer reviews your feedback and will assist you soon.\n\nIf you have a 🐛 Bug Report or encounter issues with your setup, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate further.\n\nFor general usage and customization questions, check the [Docs](https://docs.ultralytics.com/), including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) guides.\n\nJoin the Ultralytics community in real time on [Discord](https://discord.com/invite/ultralytics) 🎧, explore discussions on [Discourse](https://community.ultralytics.com/), or contribute to threads on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nTo ensure you benefit from the latest features and fixes, upgrade to the newest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO runs seamlessly in these verified environments (with CUDA/CUDNN, Python, and PyTorch preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "Thanks for this thoughtful suggestion, @Teriks. This is a valid feature request that would benefit users running Ultralytics in headless environments. Adding support for `opencv-python-headless` through pip extras could be a clean solution, though as you noted, we need to be careful about backward compatibility.\n\nWe're open to contributions if you'd like to submit a PR implementing this feature. Our [contributing guide](https://docs.ultralytics.com/help/contributing/) outlines the process. In the meantime, users can manually install `opencv-python-headless` before installing Ultralytics as a workaround."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 21063,
    "title": "Using YOLOv9c and YOLOv9e to train the hrsc2016 dataset has the same results",
    "author": "Wei0209",
    "state": "open",
    "created_at": "2025-06-15T14:29:45Z",
    "updated_at": "2025-06-16T18:33:38Z",
    "labels": [
      "question",
      "OBB"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nWhen I use YOLOv9c and YOLOv9e to train the hrsc2016 dataset, the mAP50 values ​​displayed are the same.\nI can provide yolov9e.yaml and yolov9c.yaml\n# YOLOv9\n\n# parameters\nnc: 1  # number of classes\n\n# gelan backbone\nbackbone:\n  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2\n  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4\n  - [-1, 1, RepNCSPELAN4, [256, 128, 64, 1]]  # 2\n  - [-1, 1, ADown, [256]]  # 3-P3/8\n  - [-1, 1, RepNCSPELAN4, [512, 256, 128, 1]]  # 4\n  - [-1, 1, ADown, [512]]  # 5-P4/16\n  - [-1, 1, RepNCSPELAN4, [512, 512, 256, 1]]  # 6\n  - [-1, 1, ADown, [512]]  # 7-P5/32\n  - [-1, 1, RepNCSPELAN4, [512, 512, 256, 1]]  # 8\n  - [-1, 1, SPPELAN, [512, 256]]  # 9\n\nhead:\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4\n  - [-1, 1, RepNCSPELAN4, [512, 512, 256, 1]]  # 12\n\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3\n  - [-1, 1, RepNCSPELAN4, [256, 256, 128, 1]]  # 15 (P3/8-small)\n\n  - [-1, 1, ADown, [256]]\n  - [[-1, 12], 1, Concat, [1]]  # cat head P4\n  - [-1, 1, RepNCSPELAN4, [512, 512, 256, 1]]  # 18 (P4/16-medium)\n\n  - [-1, 1, ADown, [512]]\n  - [[-1, 9], 1, Concat, [1]]  # cat head P5\n  - [-1, 1, RepNCSPELAN4, [512, 512, 256, 1]]  # 21 (P5/32-large)\n\n  - [[15, 18, 21], 1, OBB, [nc, 1]]  # DDetect(P3, P4, P5)\n\n# YOLOv9\n\n# parameters\nnc: 1  # number of classes\n\n# gelan backbone\nbackbone:\n  - [-1, 1, Silence, []]\n  - [-1, 1, Conv, [64, 3, 2]]  # 1-P1/2\n  - [-1, 1, Conv, [128, 3, 2]]  # 2-P2/4\n  - [-1, 1, RepNCSPELAN4, [256, 128, 64, 2]]  # 3\n  - [-1, 1, ADown, [256]]  # 4-P3/8\n  - [-1, 1, RepNCSPELAN4, [512, 256, 128, 2]]  # 5\n  - [-1, 1, ADown, [512]]  # 6-P4/16\n  - [-1, 1, RepNCSPELAN4, [1024, 512, 256, 2]]  # 7\n  - [-1, 1, ADown, [1024]]  # 8-P5/32\n  - [-1, 1, RepNCSPELAN4, [1024, 512, 256, 2]]  # 9\n   \n  - [1, 1, CBLinear, [[64]]] # 10\n  - [3, 1, CBLinear, [[64, 128]]] # 11\n  - [5, 1, CBLinear, [[64, 128, 256]]] # 12\n  - [7, 1, CBLinear, [[64, 128, 256, 512]]] # 13\n  - [9, 1, CBLinear, [[64, 128, 256, 512, 1024]]] # 14\n  \n  - [0, 1, Conv, [64, 3, 2]]  # 15-P1/2\n  - [[10, 11, 12, 13, 14, -1], 1, CBFuse, [[0, 0, 0, 0, 0]]] # 16\n  - [-1, 1, Conv, [128, 3, 2]]  # 17-P2/4\n  - [[11, 12, 13, 14, -1], 1, CBFuse, [[1, 1, 1, 1]]] # 18  \n  - [-1, 1, RepNCSPELAN4, [256, 128, 64, 2]]  # 19\n  - [-1, 1, ADown, [256]]  # 20-P3/8\n  - [[12, 13, 14, -1], 1, CBFuse, [[2, 2, 2]]] # 21  \n  - [-1, 1, RepNCSPELAN4, [512, 256, 128, 2]]  # 22\n  - [-1, 1, ADown, [512]]  # 23-P4/16\n  - [[13, 14, -1], 1, CBFuse, [[3, 3]]] # 24 \n  - [-1, 1, RepNCSPELAN4, [1024, 512, 256, 2]]  # 25\n  - [-1, 1, ADown, [1024]]  # 26-P5/32\n  - [[14, -1], 1, CBFuse, [[4]]] # 27\n  - [-1, 1, RepNCSPELAN4, [1024, 512, 256, 2]]  # 28\n  - [-1, 1, SPPELAN, [512, 256]]  # 29\n\n# gelan head\nhead:\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n  - [[-1, 25], 1, Concat, [1]]  # cat backbone P4\n  - [-1, 1, RepNCSPELAN4, [512, 512, 256, 2]]  # 32\n\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n  - [[-1, 22], 1, Concat, [1]]  # cat backbone P3\n  - [-1, 1, RepNCSPELAN4, [256, 256, 128, 2]]  # 35 (P3/8-small)\n\n  - [-1, 1, ADown, [256]]\n  - [[-1, 32], 1, Concat, [1]]  # cat head P4\n  - [-1, 1, RepNCSPELAN4, [512, 512, 256, 2]]  # 38 (P4/16-medium)\n\n  - [-1, 1, ADown, [512]]\n  - [[-1, 29], 1, Concat, [1]]  # cat head P5\n  - [-1, 1, RepNCSPELAN4, [512, 1024, 512, 2]]  # 41 (P5/32-large)\n\n   # detect\n  - [[35, 38, 41], 1, OBB, [nc,2]]  # Detect(P3, P4, P5)\n\nExecute the command:\ntrain yolov9c:\n\nfrom ultralytics import YOLO\n \ndef main():\n    model = YOLO('/workspace/yolov10/ultralytics/cfg/models/v9/yolov9c.yaml',task=\"obb\").load('/workspace/yolov10/yolov9c.pt')  \n    model.train(data='/workspace/yolov10/ultralytics/cfg/datasets/hrsc.yaml', optimizer=\"SGD\", epochs=150, imgsz=640, batch=32,device=[0,1,2,3])\nif __name__ == '__main__':\n    main()\n\ntrain yolov9e:\n\nfrom ultralytics import YOLO\n \ndef main():\n    model = YOLO('/workspace/yolov10/ultralytics/cfg/models/v9/yolov9e.yaml',task=\"obb\").load('/workspace/yolov10/yolov9e.pt')  \n    model.train(data='/workspace/yolov10/ultralytics/cfg/datasets/hrsc.yaml', optimizer=\"SGD\", epochs=150, imgsz=640, batch=32,device=[0,1,2,3])\nif __name__ == '__main__':\n    main()\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Wei0209, thank you for reaching out and sharing your experience with YOLOv9c and YOLOv9e on the hrsc2016 dataset! 🚀 This is an automated response to help get you started—an Ultralytics engineer will assist you soon.\n\nWe recommend reviewing the [Docs](https://docs.ultralytics.com/) for insights into model configuration and training. If you haven't already, please check out the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage guides for helpful examples.\n\nSince this may be a bug or unexpected behavior, could you please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/)? This will help us investigate further. Key details to include:\n- The exact YAML files you used for both models\n- Any relevant training logs or output\n- Clarification if you meant to use `yolov9e.yaml` but referenced `yolov9c.yaml` in both training scripts\n\nIf you have questions about custom training, please share as much information as possible (such as dataset samples and logs), and review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nFor live support, join our [Discord](https://discord.com/invite/ultralytics) 🎧, or discuss in-depth on [Discourse](https://community.ultralytics.com/). You can also connect with others on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nTo ensure you’re using the latest features and fixes, upgrade `ultralytics` and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in any of these up-to-date, verified environments (with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThanks again for your detailed report! An Ultralytics engineer will review and follow up with you soon."
      },
      {
        "user": "Wei0209",
        "body": "train yolov9c on hrsc:\n```python\nfrom ultralytics import YOLO\n \ndef main():\n    model = YOLO('/workspace/yolov10/ultralytics/cfg/models/v9/yolov9c.yaml',task=\"obb\").load('/workspace/yolov10/yolov9c.pt')\n    model.train(data='/workspace/yolov10/ultralytics/cfg/datasets/hrsc.yaml', optimizer=\"SGD\", epochs=150, imgsz=640, batch=32,device=[0,1,2,3]  )\nif __name__ == '__main__':\n    main()\n```\n\ntrain yolov9e on hrsc:\n```python\nfrom ultralytics import YOLO\n \ndef main():\n    model = YOLO('/workspace/yolov10/ultralytics/cfg/models/v9/yolov9e.yaml',task=\"obb\").load('/workspace/yolov10/yolov9e.pt')\n    model.train(data='/workspace/yolov10/ultralytics/cfg/datasets/hrsc.yaml', optimizer=\"SGD\", epochs=150, imgsz=640, batch=32,device=[0,1,2,3]  )\nif __name__ == '__main__':\n    main()\n```"
      },
      {
        "user": "glenn-jocher",
        "body": "The identical mAP50 results between YOLOv9c and YOLOv9e suggest there may be an issue with model loading or configuration. YOLOv9e should significantly outperform YOLOv9c based on the [performance benchmarks](https://docs.ultralytics.com/models/yolov9/) showing YOLOv9e achieves 55.6 mAP compared to YOLOv9c's 53.0 mAP.\n\nPlease verify that your pretrained weights (`yolov9c.pt` and `yolov9e.pt`) are actually different models and not the same file. You can check this by printing the model architecture or parameter count after loading:\n\n```python\nmodel = YOLO('/workspace/yolov10/ultralytics/cfg/models/v9/yolov9e.yaml',task=\"obb\").load('/workspace/yolov10/yolov9e.pt')\nprint(f\"Model parameters: {sum(p.numel() for p in model.model.parameters())}\")\n```\n\nAlso try training from scratch without loading pretrained weights to see if the architectures produce different results. What specific mAP50 values are you seeing for both models?"
      }
    ]
  },
  {
    "issue_number": 19824,
    "title": "WeightsUnpickler error",
    "author": "Salmankm93",
    "state": "open",
    "created_at": "2025-03-22T13:36:31Z",
    "updated_at": "2025-06-16T17:17:10Z",
    "labels": [
      "non-reproducible",
      "detect"
    ],
    "body": "Hello;\nI am working on a project and using the Yolov8.yaml file to train the module from scratch, everything okay however I have faced this problem, and I am stuck here I have used this repo before, and there were no problems or errors, but when I want to return to the project, I have faced this issue. \nand this is the error message:\n```\nFile \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1470, in load\n    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL ultralytics.nn.tasks.DetectionModel was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DetectionModel])` or the `torch.serialization.safe_globals([DetectionModel])` context manager to allowlist this global if you trust this class/function.\n```\ncould anyone help, please?",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Salmankm93, thank you for reaching out and sharing your issue with Ultralytics 🚀!\n\nWe recommend exploring the [Ultralytics Docs](https://docs.ultralytics.com/) for helpful resources, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples. These resources might address common questions or provide insights into your problem.\n\nIf this is a 🐛 Bug Report, it would be incredibly helpful if you could share a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/). This will allow us to better understand and debug your issue.\n\nFor custom training ❓ Questions, please include as much detail as possible about your setup, such as:\n- Dataset examples\n- Training logs\n- Configuration files\n- Steps to reproduce the issue\n\nPlease also make sure you’re following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Upgrade\n\nEnsure that you are running the latest `ultralytics` package version, as updates may already resolve your issue. Upgrade your environment with:\n```bash\npip install -U ultralytics\n```\nThis should be done in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) installed. You can check the package's [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) for further verification.\n\n## Environments\n\nYou can run YOLO in various verified environments. For convenience, consider using the following platforms:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM ([GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/))\n- **Amazon** Deep Learning AMI ([AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/))\n- **Docker Image** ([Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Community and Support\n\nJoin the Ultralytics community to collaborate and share knowledge:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for real-time discussions\n- [Discourse](https://community.ultralytics.com/) for in-depth conversations\n- [Subreddit](https://reddit.com/r/Ultralytics) for thread-based discussions\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf the CI badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. These tests validate YOLO's functionality across macOS, Windows, and Ubuntu.\n\n⚠️ Note: This is an automated response to assist you quickly. An Ultralytics engineer will review your issue and provide further support soon."
      },
      {
        "user": "Y-T-G",
        "body": "You haven't provided the code or the full traceback of the error. It's difficult to debug without them. Please post the code and full error. And also post the output of running `yolo checks` in the terminal."
      },
      {
        "user": "edt-developer",
        "body": "I have the exact same issue running the example from https://docs.ultralytics.com/models/fast-sam/#usage-examples:\npython Inference.py --model_path ./FastSAM-s.pt --img_path ./src_images/frame_20fps_0.jpg\n\nTraceback (most recent call last):\n  File \"/Users/kellyk/Documents/dev/projects/fast_sam/FastSAM/Inference.py\", line 122, in <module>\n    main(args)\n  File \"/Users/kellyk/Documents/dev/projects/fast_sam/FastSAM/Inference.py\", line 76, in main\n    model = FastSAM(args.model_path)\n            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kellyk/Documents/dev/projects/fast_sam/FastSAM/ultralytics/yolo/engine/model.py\", line 107, in __init__\n    self._load(model, task)\n  File \"/Users/kellyk/Documents/dev/projects/fast_sam/FastSAM/ultralytics/yolo/engine/model.py\", line 156, in _load\n    self.model, self.ckpt = attempt_load_one_weight(weights)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kellyk/Documents/dev/projects/fast_sam/FastSAM/ultralytics/nn/tasks.py\", line 578, in attempt_load_one_weight\n    ckpt, weight = torch_safe_load(weight)  # load ckpt\n                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kellyk/Documents/dev/projects/fast_sam/FastSAM/ultralytics/nn/tasks.py\", line 518, in torch_safe_load\n    return torch.load(file, map_location='cpu'), file  # load\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages/torch/serialization.py\", line 1470, in load\n    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint. \n        (1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n        (2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n        WeightsUnpickler error: Unsupported global: GLOBAL ultralytics.nn.tasks.SegmentationModel was not an allowed global by default. Please use `torch.serialization.add_safe_globals([SegmentationModel])` or the `torch.serialization.safe_globals([SegmentationModel])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n\n$ yolo checks\nUltralytics 8.3.96 🚀 Python-3.11.11 torch-2.6.0 CPU (Apple M2 Max)\nSetup complete ✅ (12 CPUs, 96.0 GB RAM, 780.8/926.4 GB disk)\n\nOS                  macOS-14.7.2-arm64-arm-64bit\nEnvironment         Darwin\nPython              3.11.11\nInstall             pip\nPath                /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages/ultralytics\nRAM                 96.00 GB\nDisk                780.8/926.4 GB\nCPU                 Apple M2 Max\nCPU count           12\nGPU                 None\nGPU count           None\nCUDA                None\n\nnumpy               ✅ 2.1.1<=2.1.1,>=1.23.0\nmatplotlib          ✅ 3.10.1>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 11.1.0>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.15.2>=1.4.1\ntorch               ✅ 2.6.0>=1.8.0\ntorch               ✅ 2.6.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.21.0>=0.9.0\ntqdm                ✅ 4.67.1>=4.64.0\npsutil              ✅ 7.0.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.2.3>=1.1.4\nseaborn             ✅ 0.13.2>=0.11.0\nultralytics-thop    ✅ 2.0.14>=2.0.0\n\n\n(venv) kk@MacBookPro FastSAM % pip install -U ultralytics\n\nRequirement already satisfied: ultralytics in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (8.3.96)\nRequirement already satisfied: numpy<=2.1.1,>=1.23.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (2.1.1)\nRequirement already satisfied: matplotlib>=3.3.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (3.10.1)\nRequirement already satisfied: opencv-python>=4.6.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (11.1.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (1.15.2)\nRequirement already satisfied: torch>=1.8.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (2.6.0)\nRequirement already satisfied: torchvision>=0.9.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (0.21.0)\nRequirement already satisfied: tqdm>=4.64.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (0.13.2)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from ultralytics) (2.0.14)\nRequirement already satisfied: contourpy>=1.0.1 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\nRequirement already satisfied: python-dateutil>=2.7 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: filelock in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: networkx in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\nRequirement already satisfied: sympy==1.13.1 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /Users/kellyk/Documents/dev/projects/fast_sam/venv/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
      }
    ]
  },
  {
    "issue_number": 21026,
    "title": "Yolov8 training parameter \"classes\" does not seem to work as intended.",
    "author": "Til-Widmann",
    "state": "open",
    "created_at": "2025-06-11T20:03:48Z",
    "updated_at": "2025-06-16T16:21:56Z",
    "labels": [
      "bug",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nI've encountered an issue when training a YOLOv8 model using a dataset that contains multiple classes. When I specify a subset of these classes via the `classes` [parameter](https://docs.ultralytics.com/usage/cfg/#train-settings:~:text=Specifies%20a%20list%20of%20class%20IDs%20to%20train%20on.%20Useful%20for%20filtering%20out%20and%20focusing%20only%20on%20certain%20classes%20during%20training.) during training, the validation step subsequently fails if it processes validation samples that exclusively contain classes _not_ included in that specified subset.(Error shown below) This leads me to question if the `classes` parameter is fully implemented or if there's a specific parameter i have to set for such scenarios during validation.\n\n```\nClass Images Instances Box(P R mAP50 mAP50-95): 100%|██████████| 434/434 [04:06<00:00, 1.76it/s]\nTraceback (most recent call last):\nFile \"/home/<user>/run.py\", line 47, in <module>\nmain()\nFile \"/home/<user>/run.py\", line 43, in main\nmodule.main()\nFile \"/home/<user>/modules/yolov8/main.py\", line 21, in main\ncommand(**args)\nFile \"/home/<user>/modules/yolov8/model.py\", line 73, in train\nmodel.train(\nFile \"/home/<user>/.local/lib/python3.10/site-packages/ultralytics/engine/model.py\", line 806, in train\nself.trainer.train()\nFile \"/home/<user>/.local/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 207, in train\nself._do_train(world_size)\nFile \"/home/<user>/.local/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 432, in _do_train\nself.metrics, self.fitness = self.validate(\nFile \"/home/<user>/.local/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 605, in validate\nmetrics = self.validator(self)\nFile \"/home/<user>/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\nreturn func(*args, **kwargs)\nFile \"/home/<user>/.local/lib/python3.10/site-packages/ultralytics/engine/validator.py\", line 197, in __call__\nstats = self.get_stats()\nFile \"/home/<user>/.local/lib/python3.10/site-packages/ultralytics/models/yolo/detect/val.py\", line 181, in get_stats\nstats = {k: torch.cat(v, 0).cpu().numpy() for k, v in self.stats.items()} # to numpy\nFile \"/home/<user>/.local/lib/python3.10/site-packages/ultralytics/models/yolo/detect/val.py\", line 181, in <dictcomp>\nstats = {k: torch.cat(v, 0).cpu().numpy() for k, v in self.stats.items()} # to numpy \nRuntimeError:\ntorch.cat(): expected a non-empty list of Tensors\n```\n\n### Environment\n\nultralytics-thop    ✅ 2.0.13>=2.0.0\nultralytics-thop    ✅ 2.0.13>=2.0.0\nseaborn             ✅ 0.13.2>=0.11.0\nseaborn             ✅ 0.13.2>=0.11.0\npandas              ✅ 2.2.3>=1.1.4\npandas              ✅ 2.2.3>=1.1.4\npy-cpuinfo          ✅ 9.0.0\npy-cpuinfo          ✅ 9.0.0\npsutil              ✅ 6.1.0\npsutil              ✅ 6.1.0\ntqdm                ✅ 4.67.1>=4.64.0\ntqdm                ✅ 4.67.1>=4.64.0\ntorchvision         ✅ 0.20.1>=0.9.0\ntorchvision         ✅ 0.20.1>=0.9.0\ntorch               ✅ 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorch               ✅ 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorch               ✅ 2.5.1>=1.8.0\ntorch               ✅ 2.5.1>=1.8.0\nscipy               ✅ 1.14.1>=1.4.1\nscipy               ✅ 1.14.1>=1.4.1\nrequests            ✅ 2.32.3>=2.23.0\nrequests            ✅ 2.32.3>=2.23.0\npyyaml              ✅ 6.0.2>=5.3.1\npyyaml              ✅ 6.0.2>=5.3.1\npillow              ✅ 11.0.0>=7.1.2\npillow              ✅ 11.0.0>=7.1.2\nopencv-python       ✅ 4.10.0.84>=4.6.0\nopencv-python       ✅ 4.10.0.84>=4.6.0\nmatplotlib          ✅ 3.10.0>=3.3.0\nmatplotlib          ✅ 3.10.0>=3.3.0\nnumpy               ✅ 2.2.0<2.0.0; sys_platform == \"darwin\"\nnumpy               ✅ 2.2.0<2.0.0; sys_platform == \"darwin\"\nnumpy               ✅ 2.2.0>=1.23.0\nnumpy               ✅ 2.2.0>=1.23.0\nCUDA                12.4\nCUDA                12.4\nGPU count           1\nGPU count           1\nGPU                 Tesla T4, 15936MiB\nGPU                 Tesla T4, 15936MiB\nCPU count           4\nCPU count           4\nCPU                 AMD EPYC 7V12 64-Core Processor\nCPU                 AMD EPYC 7V12 64-Core Processor\nDisk                46.5/123.9 GB\nDisk                46.5/123.9 GB\nRAM                 27.41 GB\nRAM                 27.41 GB\nInstall             pip\nInstall             pip\nPython              3.10.12\nPython              3.10.12\nEnvironment         Linux\nEnvironment         Linux\n\nOS                  Linux-5.15.0-1087-azure-x86_64-with-glibc2.35\nOS                  Linux-5.15.0-1087-azure-x86_64-with-glibc2.35\n\nSetup complete ✅ (4 CPUs, 27.4 GB RAM, 46.5/123.9 GB disk)\nSetup complete ✅ (4 CPUs, 27.4 GB RAM, 46.5/123.9 GB disk)\nUltralytics 8.3.50 🚀 Python-3.10.12 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15936MiB)\nUltralytics 8.3.50 🚀 Python-3.10.12 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15936MiB)\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nView Ultralytics Settings with 'yolo settings' or at '/home/aifish/.config/Ultralytics/settings.json'\nView Ultralytics Settings with 'yolo settings' or at '/home/aifish/.config/Ultralytics/settings.json'\nCreating new Ultralytics Settings v0.0.6 file ✅ \nCreating new Ultralytics Settings v0.0.6 file ✅ \nultralytics-thop    ✅ 2.0.13>=2.0.0\nultralytics-thop    ✅ 2.0.13>=2.0.0\nseaborn             ✅ 0.13.2>=0.11.0\n2025-06-11 20:52:05.757\t\npandas              ✅ 2.2.3>=1.1.4\npandas              ✅ 2.2.3>=1.1.4\npy-cpuinfo          ✅ 9.0.0\npy-cpuinfo          ✅ 9.0.0\npsutil              ✅ 6.1.0\npsutil              ✅ 6.1.0\ntqdm                ✅ 4.67.1>=4.64.0\ntqdm                ✅ 4.67.1>=4.64.0\ntorchvision         ✅ 0.20.1>=0.9.0\ntorchvision         ✅ 0.20.1>=0.9.0\ntorch               ✅ 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorch               ✅ 2.5.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorch               ✅ 2.5.1>=1.8.0\ntorch               ✅ 2.5.1>=1.8.0\nscipy               ✅ 1.14.1>=1.4.1\nscipy               ✅ 1.14.1>=1.4.1\nrequests            ✅ 2.32.3>=2.23.0\nrequests            ✅ 2.32.3>=2.23.0\npyyaml              ✅ 6.0.2>=5.3.1\npyyaml              ✅ 6.0.2>=5.3.1\npillow              ✅ 11.0.0>=7.1.2\npillow              ✅ 11.0.0>=7.1.2\nopencv-python       ✅ 4.10.0.84>=4.6.0\nopencv-python       ✅ 4.10.0.84>=4.6.0\nmatplotlib          ✅ 3.10.0>=3.3.0\nmatplotlib          ✅ 3.10.0>=3.3.0\n\n### Minimal Reproducible Example\n\nStart training classes parameter set to a single class. Use a dataset with multiple classes. The dataset needs to have val samples where the selected class is not present.\n\n model.train(\n      data=dataset_with_multiple_classes,\n      classes=[0]\n  )\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Til-Widmann, thank you for reporting this issue and for providing detailed context! 🚀 This is an automated response to help get you the fastest support possible. An Ultralytics engineer will also review your report and respond soon.\n\nWe recommend checking the [Docs](https://docs.ultralytics.com/) for additional guidance, where you can find [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples. Many common training and validation scenarios are addressed there.\n\nSince this is a 🐛 Bug Report, thank you for including a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/)—this helps us investigate efficiently!\n\nIf you have additional details (such as sample data snippets or a link to a minimal dataset that reproduces the issue), please add them here to further assist our team.\n\nJoin the Ultralytics community for real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧, in-depth discussions on [Discourse](https://community.ultralytics.com/), or connect with others via our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease ensure you are running the latest `ultralytics` package (including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml)) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your report!"
      },
      {
        "user": "Y-T-G",
        "body": "Are there any warnings in the training log?"
      },
      {
        "user": "Til-Widmann",
        "body": "I got no prior warnings."
      }
    ]
  },
  {
    "issue_number": 13789,
    "title": "Dataset not found ⚠️, missing path ",
    "author": "casperking666",
    "state": "closed",
    "created_at": "2024-06-19T00:21:13Z",
    "updated_at": "2025-06-16T14:26:16Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI have a file structure like this (The reason why I have a nested \"datasets/datasets\" structure is cus one issue saying this fixed his issue, but I tried it it didn't work. )\r\n<img width=\"166\" alt=\"image\" src=\"https://github.com/ultralytics/ultralytics/assets/74254229/3a565d6b-25dd-4b4c-a37c-75fceefaf3e2\">\r\nwhich I believe is right?\r\nand I have a python script with this training code\r\n<img width=\"597\" alt=\"image\" src=\"https://github.com/ultralytics/ultralytics/assets/74254229/aef049aa-795e-44df-b2ea-59fe73b10344\">\r\nI am on a hpc, so here is the commands in the job\r\n<img width=\"218\" alt=\"image\" src=\"https://github.com/ultralytics/ultralytics/assets/74254229/0b67ccb7-54a5-45c1-a741-026e890d484f\">\r\nIt kept giving me this missing path error, even though the yaml file is just right there, you can see the ls output, it's just there. I tried various combination, absolute path, relative path, absolutely no idea whats going on.\r\n<img width=\"955\" alt=\"image\" src=\"https://github.com/ultralytics/ultralytics/assets/74254229/67b1ab54-29d6-4b2c-9ee1-2d9e7a4b678d\">\r\nHere is the yaml file, I tried absolute path as well, not working too.\r\n<img width=\"225\" alt=\"image\" src=\"https://github.com/ultralytics/ultralytics/assets/74254229/411637ca-db99-42f6-bd63-4a8d800febbe\">\r\n\r\nultralytics               8.2.35\r\n\r\nBeen stuck here for absolutely ages and it's driving me nuts. HELP!!!!\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @casperking666, thank you for your interest in Ultralytics YOLOv8 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@casperking666 hi there! 👋\n\nThank you for reaching out and providing detailed information about your issue. Let's work through this together.\n\nFirstly, it looks like your dataset structure and YAML file paths might be causing the issue. To ensure everything is set up correctly, please follow these steps:\n\n1. **Verify Dataset Structure**: Ensure your dataset follows the required structure. For a classification task, it should look something like this:\n    ```\n    datasets/\n    ├── train/\n    │   ├── class1/\n    │   │   ├── image1.jpg\n    │   │   └── image2.jpg\n    │   ├── class2/\n    │   │   ├── image1.jpg\n    │   │   └── image2.jpg\n    ├── val/ (optional)\n    │   ├── class1/\n    │   │   ├── image1.jpg\n    │   │   └── image2.jpg\n    │   ├── class2/\n    │   │   ├── image1.jpg\n    │   │   └── image2.jpg\n    └── test/\n        ├── class1/\n        │   ├── image1.jpg\n        │   └── image2.jpg\n        ├── class2/\n        │   ├── image1.jpg\n        │   └── image2.jpg\n    ```\n\n2. **Check YAML File**: Ensure your YAML file correctly points to the dataset directories. Here’s an example of how it should look:\n    ```yaml\n    path: /absolute/path/to/datasets  # or relative path\n    train: train\n    val: val  # optional\n    test: test\n    ```\n\n3. **Absolute vs. Relative Paths**: When specifying paths in your YAML file, ensure they are correct. If using relative paths, they should be relative to the location of the YAML file. Absolute paths should be complete and correct.\n\n4. **Verify Python Script**: Ensure your training script correctly references the YAML file. Here’s an example:\n    ```python\n    from ultralytics import YOLO\n\n    # Load a model\n    model = YOLO(\"yolov8n-cls.pt\")  # load a pretrained model\n\n    # Train the model\n    results = model.train(data=\"/absolute/path/to/your/yaml_file.yaml\", epochs=100, imgsz=640)\n    ```\n\n5. **Check Environment**: Ensure you are using the latest versions of `torch` and `ultralytics`. You can update them using:\n    ```bash\n    pip install --upgrade torch ultralytics\n    ```\n\nIf you have verified all the above and the issue persists, please provide a minimum reproducible example of your code and dataset structure. This will help us investigate further. You can refer to our [minimum reproducible example guide](https://docs.ultralytics.com/help/minimum_reproducible_example) for more details.\n\nFeel free to reach out if you have any more questions or need further assistance. We're here to help! 😊"
      },
      {
        "user": "casperking666",
        "body": "Will try that. Thanks!"
      }
    ]
  },
  {
    "issue_number": 16024,
    "title": "RGBD Input for YOLOv8 trying to pass 4 channels",
    "author": "hithereryan",
    "state": "closed",
    "created_at": "2024-09-05T06:00:59Z",
    "updated_at": "2025-06-16T14:06:13Z",
    "labels": [
      "enhancement",
      "question",
      "fixed",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI am trying to customize YOLO architecture to accept 4 channel RGBD input. \r\nI added ch:4 to the .yaml architecture file, and then created a custom dataloader to handle the incoming images (saved as 4-channel numpy arrays). \r\n\r\n```\r\nimport os\r\nimport torch\r\nfrom torch.utils.data import Dataset, DataLoader\r\nfrom PIL import Image\r\nimport numpy as np\r\n\r\nclass CustomYOLO4ChannelDataset(Dataset):\r\n    def __init__(self, rgbd_dir, labels_dir, transform=None):\r\n        self.rgbd_dir = rgbd_dir #Link to the folder that has the combined numpy 4 channel arrays stored \r\n        self.labels_dir = labels_dir\r\n        self.transform = transform\r\n        self.depth_min = 0 #need to update\r\n        self.depth_max = 36000 #need to update\r\n        self.combined_image_files = [f for f in os.listdir(rgbd_dir) if f.endswith('.npy')]  # Adjust extension if necessary\r\n\r\n        # Check if directories exist\r\n        if not os.path.exists(rgbd_dir) or not os.path.exists(labels_dir):\r\n            raise FileNotFoundError(f\"Directories do not exist: {rgbd_dir}, {labels_dir}\")\r\n\r\n        # Check if image files are found\r\n        if len(self.combined_image_files) == 0:\r\n            raise ValueError(f\"No image files found in {rgbd_dir}\")\r\n\r\n    def __len__(self):\r\n        return len(self.combined_image_files)\r\n    \r\n    def normalize_image(self, image, depth_min, depth_max):\r\n        # Assuming `image` is a 4D numpy array with shape (height, width, 4)\r\n        # Separate color channels and depth channel\r\n        color_channels = image[:, :, :3]\r\n        depth_channel = image[:, :, 3]\r\n\r\n        depth_channel_masked = np.where(depth_channel>36000, 0, depth_channel)\r\n\r\n        # Normalize color channels from [0, 225] to [0, 1]\r\n        color_channels_normalized = color_channels / 225.0\r\n        # Normalize depth channel\r\n        depth_channel_normalized = (depth_channel_masked - depth_min) / (depth_max - depth_min)\r\n    # Combine normalized channels back into a single image\r\n        normalized_image = np.dstack((color_channels_normalized, depth_channel_normalized))\r\n        return normalized_image\r\n\r\n    def __getitem__(self, idx):\r\n        combined_img_path = os.path.join(self.rgbd_dir, self.combined_image_files[idx])\r\n        label_path = os.path.join(self.labels_dir, self.combined_image_files[idx].replace('.npy', '.txt'))  # Adjust extension\r\n\r\n        # Load 4-channel image\r\n        #image = Image.open(img_path).convert('RGBA')  # or use 'RGBA' or 'LA' depending on the type of 4-channel data\r\n        combined_image = np.load(combined_img_path)\r\n\r\n        # Convert to numpy array and normalize\r\n        #image = np.array(image).astype(np.float32) / 255.0  # Normalize if needed\r\n        normalized_combined_image = self.normalize_image(combined_image, self.depth_min, self.depth_max)\r\n\r\n        image = torch.from_numpy(normalized_combined_image).permute(2, 0, 1)  # Convert to tensor and rearrange to CxHxW\r\n\r\n        # Load labels\r\n        boxes = []\r\n        with open(label_path, 'r') as file:\r\n            for line in file:\r\n                class_id, x_center, y_center, width, height = map(float, line.split())\r\n                boxes.append([class_id, x_center, y_center, width, height])\r\n\r\n        if (len(boxes) == 0):\r\n            boxes = torch.zeros((1, 5), dtype=torch.float32)\r\n        else:\r\n            boxes = torch.tensor(boxes, dtype=torch.float32)\r\n\r\n        # Apply transformations\r\n        if self.transform:\r\n            image = self.transform(image)\r\n\r\n        return image, boxes\r\n```\r\n\r\nUsed in:\r\n```\r\nfrom ultralytics import YOLO\r\n\r\n# Load YOLOv8 model and adjust for 4-channel input\r\nmodel = YOLO('/path/to/yolov8mcustom.yaml')  # Load modified model configuration\r\n\r\n# Initialize the custom dataset and dataloader\r\nrgbd_dir = '/path/to/merged'\r\nlabels_dir = '/path/to/labels'\r\n\r\ncustom_dataset = CustomYOLO4ChannelDataset(rgbd_dir, labels_dir)\r\ncustom_dataloader = DataLoader(custom_dataset, batch_size=8, shuffle=True)\r\n\r\n# Define a training loop or integrate with YOLO's train function\r\ndef train_one_epoch(model, dataloader, device):\r\n    model.train(imgsz=384)\r\n    for images, labels in dataloader:\r\n        # Move data to the device (e.g., GPU)\r\n        images = [image.to(device) for image in images]\r\n        labels = [label.to(device) for label in labels]\r\n\r\n        # Perform forward pass, loss computation, and backpropagation\r\n        results = model.train(images, labels)  # Replace this with model's specific training call\r\n        loss = results['loss']  # Example access to the loss\r\n        loss.backward()\r\n        # optimizer.step()  # Assuming you have defined an optimizer\r\n        # optimizer.zero_grad()\r\n\r\n# Set device\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nmodel.to(device)\r\n\r\n# Train the model using your custom DataLoader\r\ntrain_one_epoch(model, custom_dataloader, device)\r\n```\r\n\r\nThe dataloader seems to work when tested independently, but when training it's raising an error about the shape of its data passing:\r\n\r\n> Starting training for 100 epochs...\r\n> \r\n>       Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\r\n>   0%|          | 0/1 [00:00<?, ?it/s]\r\n> Traceback (most recent call last):\r\n>   File \"4-channel-yolo.py\", line 40, in <module>\r\n>     train_one_epoch(model, custom_dataloader, device)\r\n>   File \"4-channel-yolo.py\", line 22, in train_one_epoch\r\n>     model.train()\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/ultralytics/engine/model.py\", line 341, in train\r\n>     self.trainer.train()\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/ultralytics/engine/trainer.py\", line 191, in train\r\n>     self._do_train(world_size)\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/ultralytics/engine/trainer.py\", line 342, in _do_train\r\n>     self.loss, self.loss_items = self.model(batch)\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n>     return forward_call(*args, **kwargs)\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py\", line 41, in forward\r\n>     return self.loss(x, *args, **kwargs)\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py\", line 211, in loss\r\n>     preds = self.forward(batch['img']) if preds is None else preds\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py\", line 42, in forward\r\n>     return self.predict(x, *args, **kwargs)\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py\", line 59, in predict\r\n>     return self._predict_once(x, profile, visualize)\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py\", line 79, in _predict_once\r\n>     x = m(x)  # run\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n>     return forward_call(*args, **kwargs)\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/ultralytics/nn/modules/conv.py\", line 36, in forward\r\n>     return self.act(self.bn(self.conv(x)))\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n>     return forward_call(*args, **kwargs)\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 463, in forward\r\n>     return self._conv_forward(input, self.weight, self.bias)\r\n>   File \"/home/smarts/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\r\n>     return F.conv2d(input, weight, bias, self.stride,\r\n> RuntimeError: Given groups=1, weight of size [48, 4, 3, 3], expected input[4, 3, 640, 640] to have 4 channels, but got 3 channels instead\r\n\r\nIt also should only be running one epoch, instead of one image per epoch. \r\nA bit lost on what is being passed through that only has 3 channels instead of 4, and why. \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "Y-T-G",
        "body": "You can disable `transforms`"
      },
      {
        "user": "Apoorvapatil25",
        "body": "@glenn-jocher In the issue raised above, we were able to resolve the channel incompatibility error. We are now having immense trouble creating a custom training class that can accept 4channel input. We have successfully created a 4 channel input tensor. We have created a custom_dataloader and a custom model. But we are unable to create a training class / function. \r\nCould you please help us? "
      },
      {
        "user": "glenn-jocher",
        "body": "I'm glad you resolved the channel issue. For creating a custom training function, ensure your model and dataloader are compatible with the training loop. You can adapt the existing YOLOv8 training loop to handle your custom inputs. If you need specific guidance, please refer to the Ultralytics documentation for training custom models."
      }
    ]
  },
  {
    "issue_number": 20748,
    "title": "Requires caching twice when cache=\"ram\" and batch=-1",
    "author": "dantetemplar",
    "state": "closed",
    "created_at": "2025-05-22T00:05:31Z",
    "updated_at": "2025-06-16T13:36:41Z",
    "labels": [
      "bug",
      "enhancement",
      "fixed"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nFirst it cache while deriving good batch size, and secondly before starting training.\n\n![Image](https://github.com/user-attachments/assets/91cc7ab8-d552-462c-8788-9ea1c2ec1837)\n\nI guess will be better to reuse cache, or to skip cache on batch size estimation.\n\nAlso you can see I've monkey patch NUM_THREADS, I have 80 cores processor and I was able to speed up cache initialization by increasing NUM_THREADS from 8 to 80, may be it will be good to add argument that will be passed to BaseDataloader, but it is uses global variable for now: https://github.com/ultralytics/ultralytics/blob/3b818b32ece7f4545a2cb8ed5adc38b31ade26a9/ultralytics/data/base.py#L265.\n\n\nMy full code:\n```python\n# /// script\n# requires-python = \">=3.12\"\n# dependencies = [\n#     \"ultralytics\",\n#     \"torch\",\n#     \"torchvision\",\n# ]\n# ///\n\nimport os\nimport tempfile\n\nfrom ultralytics import YOLO\n\ncurrent_full_path = os.path.abspath(__file__)\ncurrent_dir = os.path.dirname(current_full_path)\nYAML = f\"\"\"\npath: {current_dir}/HumanRescue\ntrain:\n  - 01_train/\n  - 02_train/\n  - lacmus_train/\nval:\n  - 03_validation-private/\n  - 03_validation-public/\n  - lacmus_validation/\ntest:\n  - 04_test/\nnames:\n  0: person\n\"\"\"\n\nprint(YAML)\n\nmodel = YOLO()\n\n\nwith tempfile.NamedTemporaryFile(delete=True, suffix=\".yaml\") as f:\n    f.write(YAML.encode(\"utf-8\"))\n    f.flush()\n    f.seek(0)\n\n    data_path = f.name\n\n    import ultralytics.data.base\n\n    ultralytics.data.base.NUM_THREADS = 80\n    results = model.train(data=data_path, batch=-1, workers=8, cache=\"ram\")\n    # , deterministic=False (to avoid warning)\n    # batch=96 (to avoid caching twice)\n    print(results)\n```\n\n### Environment\n\n```\nUltralytics 8.3.141 🚀 Python-3.12.9 torch-2.7.0+cu126 CUDA:0 (Tesla V100-SXM2-32GB, 32501MiB)\nSetup complete ✅ (80 CPUs, 503.8 GB RAM, 245.1/438.1 GB disk)\n\nOS                  Linux-4.15.0-161-generic-x86_64-with-glibc2.31\nEnvironment         Docker\nPython              3.12.9\nInstall             git\nPath                /home/robo/resciu-dataset/.venv/lib/python3.12/site-packages/ultralytics\nRAM                 503.79 GB\nDisk                245.1/438.1 GB\nCPU                 Intel Xeon E5-2698 v4 2.20GHz\nCPU count           80\nGPU                 Tesla V100-SXM2-32GB, 32501MiB\nGPU count           1\nCUDA                12.6\n\nnumpy               ✅ 2.2.4>=1.23.0\nmatplotlib          ✅ 3.10.1>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 11.1.0>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.15.2>=1.4.1\ntorch               ✅ 2.7.0>=1.8.0\ntorch               ✅ 2.7.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.22.0>=0.9.0\ntqdm                ✅ 4.67.1>=4.64.0\npsutil              ✅ 7.0.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.2.3>=1.1.4\nultralytics-thop    ✅ 2.0.14>=2.0.0\n```\n\n### Minimal Reproducible Example\n\n```python\nmodel.train(data=\"visdrone.yaml\", batch=-1, workers=8, cache=\"ram\")\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @dantetemplar, thank you for your detailed report and suggestions for Ultralytics 🚀! We appreciate your insights, especially regarding cache reuse and NUM_THREADS optimization.\n\nThis is an automated response to help you get started. An Ultralytics engineer will review your issue and assist you soon.\n\nIf this is a 🐛 Bug Report, thank you for including a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/)! This helps us debug issues more efficiently.\n\nIf you have further details or logs, please add them here. For new users, we recommend visiting the [Docs](https://docs.ultralytics.com/) for useful [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples, as well as the [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best:\n- For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussions, visit [Discourse](https://community.ultralytics.com/)\n- Or join our [Subreddit](https://reddit.com/r/Ultralytics) to share and learn from others\n\n## Upgrade\n\nTo ensure your issue persists with the latest release, please upgrade the `ultralytics` package along with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these verified, up-to-date environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for helping improve Ultralytics!"
      },
      {
        "user": "glenn-jocher",
        "body": "Thanks for reporting this issue @dantetemplar! You've identified a valid optimization opportunity. When using `batch=-1` with `cache=\"ram\"`, the system does indeed cache images twice - first during batch size estimation, then again for the actual training.\n\nThis happens because the `auto_batch` method in `DetectionTrainer` builds a dataset during estimation without considering the cache status. The suggested approaches are good:\n\n1. Skip caching during batch size estimation (preferred)\n2. Reuse the cache from the estimation phase during training\n\nYour observation about `NUM_THREADS` is also valuable. Adding a configurable parameter to control thread count for caching would be beneficial for systems with many cores like yours.\n\nI'll create an issue to track these improvements. As a workaround, you can either:\n- Explicitly set the batch size to avoid the auto-determination\n- Use `cache=\"disk\"` instead which is more memory-efficient (though slower)\n\nWould you be interested in contributing a PR to fix this? It would likely involve modifying the `auto_batch` method to either skip caching or preserve the cache for reuse."
      },
      {
        "user": "XBastille",
        "body": "Hi @glenn-jocher , I would like to work on this issue, I am thinking about skipping caching during batch size estimation and Add NUM_THREADS configuration (as separate smaller feature), Could you assign me??"
      }
    ]
  },
  {
    "issue_number": 21062,
    "title": "run error plot_images() in classify task",
    "author": "lc2313445",
    "state": "closed",
    "created_at": "2025-06-15T09:37:38Z",
    "updated_at": "2025-06-16T13:34:52Z",
    "labels": [
      "bug",
      "fixed",
      "classify"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nTrain\n\n### Bug\n\nIn classify task, when building trainer, it will do self.plot_training_samples(batch, ni) (the path is: ultralytics\\engine\\trainer.py), then it will call plot_images() (the path is :ultralytics\\models\\yolo\\classify\\train.py)\nthe args of plot_images() are (images, batch_idx, cls, fname, on_plot) but the parameters of plot_images() (path: ultralytics\\models\\utils\\plotting.py) not contain \"batch_idx\" and \"cls\", I just deleted these two args, so it can work~\n\n### Environment\n\nUltralytics 8.3.154  Python-3.9.13 torch-2.0.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\nSetup complete  (20 CPUs, 15.7 GB RAM, 134.1/252.2 GB disk)\n\nOS                  Windows-10-10.0.26100-SP0\nEnvironment         Windows\nPython              3.9.13\nInstall             git\nPath                D:\\project_code\\KeratitisYolo8\\ultralytics11\\ultralytics\nRAM                 15.67 GB\nDisk                134.1/252.2 GB\nCPU                 12th Gen Intel Core(TM) i7-12700H\nCPU count           20\nGPU                 NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB\nGPU count           1\nCUDA                11.8\n\n### Minimal Reproducible Example\n\nfrom ultralytics import YOLO\nscript_directory = os.path.dirname(os.path.abspath(__file__))\ncfg_path = os.path.join(script_directory, \"cfg.yaml\")\nyolo_path=os.path.join(script_directory, \"yolo11n-cls.yaml\")\nmodel =  YOLO('yolo11s-cls.yaml',task='classify')\nmodel.train(cfg=cfg_path)# issue happen\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @lc2313445, thank you for bringing this to our attention and for your interest in improving Ultralytics 🚀! This is an automated response to help you get the fastest possible support—an Ultralytics engineer will also review your issue soon.\n\nFor 🐛 Bug Reports like this, please ensure you provide a complete [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/), which you've already included—thank you! This helps us debug and resolve your issue efficiently.\n\nBe sure to check the [Docs](https://docs.ultralytics.com/) for usage details and examples. For classification tasks, you may also find [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) guides helpful.\n\nJoin our community for real-time chat in [Discord](https://discord.com/invite/ultralytics) 🎧, participate in longer discussions on [Discourse](https://community.ultralytics.com/), or share your experiences on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease ensure you are using the latest version of the `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). Many issues are resolved in the latest release:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date, verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for the detailed report and your willingness to submit a PR—it’s contributions like yours that help improve the project! An Ultralytics engineer will follow up with you soon."
      },
      {
        "user": "Y-T-G",
        "body": "Can you provide a usable minimum reproducible example?"
      },
      {
        "user": "lc2313445",
        "body": "> Can you provide a usable minimum reproducible example?\njust do this\n model =  YOLO('yolo11s-cls.yaml',task='classify')\n model.train(cfg=cfg_path)"
      }
    ]
  },
  {
    "issue_number": 21055,
    "title": "Detection speed on single image with YOLO11n.",
    "author": "MD7070B",
    "state": "open",
    "created_at": "2025-06-13T18:00:17Z",
    "updated_at": "2025-06-16T13:29:37Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\nWhat is the expected inference speed when running on a relatively mundane CPU, and are there any tactics for improving the performance of object detection models?\n\n\n### Additional\n\nI feel like this is likely quite a basic question, but I am quite new to YOLO implementations - I have done some reading around the docs, but concluded my best bet would be to ask a question here!\n\nI am running a YOLO11n model from Python, as part of a security / surveillance system I have developed (an extension to an IP camera system, if you like). \n\nMy aim is to run YOLO Detect on a still image, to identify, mainly, humans & vehicles. I *have* successfully managed this, running YOLO from a Python script which is spawned by my application (written in Node.js), but it is quite slow. This is running on my old server, which does not have a GPU, so inference is running on the CPU, which is admittedly quite a slow one. \n\nHowever, I have seen a various pieces of information which seems to reference inference speeds of <500ms, even on a Raspberry Pi, so I am wondering if I am missing something here? Any suggestions, or is this to be expected?\n\nApologies for not providing any exact numbers for current inference speeds, my images sizes, etc - these will follow when I have a chance. ",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @MD7070B, thank you for reaching out and for your interest in Ultralytics 🚀! Welcome to the YOLO community—your project sounds exciting. For new users, we recommend checking out the [Docs](https://docs.ultralytics.com/), where you'll find both [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples that may help answer common questions.\n\nSince you mentioned performance concerns and plan to provide more details, if this becomes a 🐛 Bug Report, please include a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/), including relevant code snippets, input image sizes, and details about your environment. This will help us assist you more efficiently.\n\nFor custom training or usage ❓ Questions, please share as much context as possible—like sample images, logs, and your hardware setup. Also, check out our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) for ideas on improving performance.\n\nIf you’d like to connect with the community in real time, join our [Discord](https://discord.com/invite/ultralytics) 🎧. For deeper discussions, visit [Discourse](https://community.ultralytics.com/), or share and learn on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nTo ensure you’re using the latest features and optimizations, upgrade the `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can run in various verified environments (with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response to help you get started. An Ultralytics engineer will review your issue and assist you soon 😊."
      },
      {
        "user": "Y-T-G",
        "body": "> I have successfully managed this, running YOLO from a Python script which is spawned by my application (written in Node.js), but it is quite slow. This is running on my old server, which does not have a GPU, so inference is running on the CPU, which is admittedly quite a slow one.\n\nIf you mean you're rerunning the script every time which involves reinitializing the model again, then it will be slow. You should not be reloading the model each time. You will either have to natively use the model in JS, or create a Python server that you send query to for inference which keeps the model in memory."
      },
      {
        "user": "MD7070B",
        "body": "Right - I wondered if something like that may be the issue. \n\nSo if I spawn the Python process _once_, and maybe send queries via HTTP JSON requests, it should run a lot quicker?"
      }
    ]
  },
  {
    "issue_number": 21041,
    "title": "which epoch is best.pt in yolo11?",
    "author": "carolineadicahya",
    "state": "open",
    "created_at": "2025-06-12T19:56:33Z",
    "updated_at": "2025-06-16T11:52:15Z",
    "labels": [
      "question",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi, I have a question regarding evaluation results in my segmentation experiment.\n\nWhen I run:\n`model.val(\"best.pt\")`\n\n![Image](https://github.com/user-attachments/assets/6820ef9f-4496-4607-aa85-1e0279a8af13)\nThe result for mAP50 (mask) is 0.897.\n\nHowever, when I checked my results.csv, I found that the highest mAP50 (mask) is 0.9031, which occurs at epoch 214, based on this fitness function and here’s how I compute the fitness value and visualize the graph:\n`df['fitness'] = df['metrics/mAP50(M)'] * 0.1 + df['metrics/mAP50-95(M)'] * 0.9\nplt.plot(df['epoch'], df['metrics/mAP50(M)'], label=label, color=colors[label])\nbest_epoch = df['fitness'].idxmax()\nbest_map = df['metrics/mAP50(M)'][best_epoch]`\n\nWhy is the mAP50 (mask) from model.val(\"best.pt\") different from the one in results.csv?\nIf they're not supposed to be the same, what is the reason?\nI'm asking this for my thesis, so I'd really appreciate any clarification.\n\nThank you!\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @carolineadicahya, thank you for reaching out and for providing detailed information about your segmentation experiment with YOLO11! 📊 Your question is important for understanding model evaluation and checkpoint selection.\n\nPlease note this is an automated response 🤖—an Ultralytics engineer will review your question and assist further soon.\n\nWe recommend checking the [Docs](https://docs.ultralytics.com/) for more details on model validation and training. You can also find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples there.\n\nIf your question is related to a 🐛 bug or if you suspect unexpected behavior, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug the issue. This helps us reproduce your results and offer more targeted support.\n\nIf you have additional training logs, sample data, or more details about your experiments, please share them as they will help us give you the most accurate guidance.\n\nJoin the Ultralytics community where it suits you best:\n- For real-time chat, join [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussions, visit [Discourse](https://community.ultralytics.com/)\n- Or connect with others on our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nBefore proceeding, please upgrade to the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to ensure your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for helping improve Ultralytics HUB and YOLO! 🚀"
      },
      {
        "user": "Y-T-G",
        "body": "It's based on the fitness of both mask and box.\n\nhttps://github.com/ultralytics/ultralytics/blob/6149368391f770dc41c720aa1644c052927bb2ff/ultralytics/utils/metrics.py#L1182\n"
      },
      {
        "user": "carolineadicahya",
        "body": "so what should i define my fitness?"
      }
    ]
  },
  {
    "issue_number": 21017,
    "title": "WARNING ⚠️ NMS time limit 2.050s exceeded",
    "author": "fcu52005505",
    "state": "open",
    "created_at": "2025-06-11T04:03:50Z",
    "updated_at": "2025-06-16T10:36:57Z",
    "labels": [
      "track",
      "detect"
    ],
    "body": "After running boxmot for about three hours, the terminal kept showing:\n\n> WARNING ⚠️ NMS time limit 2.050s exceeded\n> WARNING ⚠️ NMS time limit 2.050s exceeded\n> WARNING ⚠️ NMS time limit 2.050s exceeded\n\nAfter that, the display froze and the FPS dropped to just 0~1. How can I fix this issue? Thanks!\n\n![Image](https://github.com/user-attachments/assets/94d8673c-8e2c-4057-a693-723c40f8dff4)\n\nMy situation is this: when processing a very long video—up to five hours in my tests—I consistently hit a\n“WARNING ⚠️ NMS time limit 2.050s exceeded” around the 3-hour-52-minute mark. But if I split the video into two halves and run them separately, everything completes normally with no errors. I’ve tried increasing the NMS time limit in YOLO (e.g., from 2.050s to 3.100s), but that doesn’t prevent the warning. Plus, each time the warning appears, the FPS drops to zero and the video freezes.\n\nMy application scenario is license plate recognition: I first detect vehicles with YOLOv11 and track them using IDTrack, then crop the license plate region and perform character recognition on the plate. Aside from the vehicle detection step, everything else is done using YOLOv11.\n\n![Image](https://github.com/user-attachments/assets/cb860ca8-6dea-470b-a039-9c67b68d1be6)\n\n",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @fcu52005505, thank you for reaching out and providing details about your workflow and the issue you're experiencing with NMS time limits during long video processing 🚦.\n\nThis is an automated response to help gather all the information our engineering team will need to assist you. An Ultralytics engineer will review your issue and provide further support soon!\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) if possible. This will help us debug the issue more effectively. Examples of what to include:\n- The exact command(s) you ran\n- A sample of your input data (or synthetic data that triggers the problem)\n- Any custom code or settings related to the NMS configuration\n- Full error logs or stack traces, if available\n\nIf this is related to custom training or inference ❓, please provide as much detail as possible about your dataset, configuration, and environment.\n\nFor more information, check out our [Docs](https://docs.ultralytics.com/) for helpful [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples. Many common questions are answered there!\n\nJoin the Ultralytics community for additional support and discussion:\n- [Discord](https://discord.com/invite/ultralytics) for real-time chat 🎧\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) for community knowledge sharing\n\n## Upgrade\n\nPlease ensure you are using the latest version of the `ultralytics` package (and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml)) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/)). You can upgrade with:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following up-to-date, verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for helping us improve Ultralytics!"
      },
      {
        "user": "Y-T-G",
        "body": "This can happen due to high resource load or a bad model with lots of false predictions"
      },
      {
        "user": "Y-T-G",
        "body": "Can you provide the code you are using for inference?"
      }
    ]
  },
  {
    "issue_number": 21066,
    "title": "Inquiry About YOLO Enterprise License – Pricing, Usage, and Integration Options",
    "author": "pornpra",
    "state": "open",
    "created_at": "2025-06-16T05:22:16Z",
    "updated_at": "2025-06-16T10:32:03Z",
    "labels": [
      "question",
      "enterprise"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nDear Ultralytics Team,\n\nI’m interested in learning more about the YOLO Enterprise license and would appreciate clarification on a few key points:\n\n1. Pricing Model: Could you provide details on the pricing structure? Is it subscription-based, one-time, or usage-based? Are there different tiers based on deployment size or feature access?\n\n2. Usage Method: Once licensed, how do we run the YOLO model—do we use it via a Python SDK with license key integration, or is it offered through a graphical interface (e.g., drag-and-drop or no-code UX/UI)?\n\n3. Deployment Options: Can the enterprise version be deployed on our own infrastructure (e.g., on-premise, Docker, Jetson), or does it require running through your platform?\n\n4. Feature Differences: How does the enterprise license differ in features or performance compared to the open-source YOLOv8?\n\n5. Trial or Demo: Is there an option to test the enterprise version before purchasing, such as a trial period or demo walkthrough?\n\n\nLooking forward to your response. 😀\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @pornpra, thank you for your interest in Ultralytics and for your detailed questions about the YOLO Enterprise license! 🚀 This is an automated response to acknowledge your inquiry—an Ultralytics engineer will follow up soon to provide further details.\n\nIn the meantime, we recommend checking our [Docs](https://docs.ultralytics.com/) for general information on YOLO capabilities and deployment options. If you have questions about using YOLO11, YOLO12, or other versions, you may also find helpful usage examples in the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) guides.\n\nFor licensing and enterprise options, our team will be best equipped to answer specifics, but you can also connect with the Ultralytics community for peer discussions:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for real-time chat\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) for community-driven threads\n\n## Upgrade\n\nIf you are considering technical evaluations, ensure you are running the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in these up-to-date verified environments (with dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for reaching out! An Ultralytics engineer will provide further information on enterprise licensing, integration, and feature differences soon."
      },
      {
        "user": "glenn-jocher",
        "body": "Thank you for your interest in the Ultralytics Enterprise License! I can help clarify several of your questions based on our available information:\n\n**Pricing & Structure:** The Enterprise License uses an annual fee model with organization-wide usage rights. Specific pricing is customized based on your organization's size and use case, so you'll need to [request a quote through our license form](https://www.ultralytics.com/license) to get exact pricing details.\n\n**Usage Method:** You'll receive access to the complete Ultralytics YOLO source code portfolio (including YOLOv5, YOLOv8, YOLO11, and future releases) along with proprietary enterprise-exclusive models. This is accessed via Python SDK with your standard development workflow, not through a separate graphical interface.\n\n**Deployment Options:** Yes, the enterprise license allows deployment on your own infrastructure including on-premise, Docker, Jetson devices, and other environments. You have full control over where and how you deploy the models.\n\n**Feature Differences:** Enterprise models are pre-trained on significantly larger datasets with more object classes compared to open-source models, providing a better foundation for fine-tuning. You also get access to proprietary models not available in the open-source version, plus full commercial rights without AGPL-3.0 restrictions.\n\n**Trial/Demo:** For trial availability and detailed feature demonstrations, please mention this when you [submit the license request form](https://www.ultralytics.com/license) - our team can discuss options during the consultation process.\n\nThe license includes basic support, with options for enhanced SLA and consulting services under separate agreements. Would you like to proceed with the license request form to get more specific details tailored to your use case?"
      }
    ]
  },
  {
    "issue_number": 3734,
    "title": "The meaning of yolov8-pose shape",
    "author": "PredyDaddy",
    "state": "closed",
    "created_at": "2023-07-14T07:27:06Z",
    "updated_at": "2025-06-16T10:11:36Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello,\r\n   I am writting an cuda kernel function of post-processing with yolov8-pose. I just want to know if my understanding of the output is correct. The output is 56 x 8400. \r\n\r\n   56 meaning: xywh(4) + confidence(1) + kpts(17)\r\n   The only thing confusing me is the the first result. I am not really sure is the xyxy or xywh. If is xywh, the xy is the center point or left upon corner? \r\n   \r\nCause this is the result with a C++ struct, I just not sure the first 4.  Thanks!\r\n![issue](https://github.com/ultralytics/ultralytics/assets/109494714/c315fb60-4c12-4df3-92c1-867b5d6ad27c)\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@PredyDaddy hi there,\n\nThank you for reaching out about the meaning of the `yolov8-pose` output shape. The output shape of `56 x 8400` indicates that there are 56 detections in total, each represented by 8400 values.\n\nThe breakdown of the 8400 values per detection is as follows: \n- `xywh(4)`: The first 4 values represent the bounding box coordinates in the format of `xywh`, where `xy` refers to the top-left corner of the bounding box.\n- `confidence(1)`: The next value represents the confidence score of the detection.\n- `kpts(17)`: The remaining 17 values represent the keypoints or pose estimation information associated with the detection.\n\nRegarding your specific question about the first result, the `xy` in the `xywh` format refers to the top-left corner of the bounding box.\n\nI hope this clarifies your understanding of the output. If you have any further questions, feel free to ask.\n\nBest regards,\nGlenn Jocher"
      },
      {
        "user": "PredyDaddy",
        "body": "> @PredyDaddy hi there,\r\n> \r\n> Thank you for reaching out about the meaning of the `yolov8-pose` output shape. The output shape of `56 x 8400` indicates that there are 56 detections in total, each represented by 8400 values.\r\n> \r\n> The breakdown of the 8400 values per detection is as follows:\r\n> \r\n> * `xywh(4)`: The first 4 values represent the bounding box coordinates in the format of `xywh`, where `xy` refers to the top-left corner of the bounding box.\r\n> * `confidence(1)`: The next value represents the confidence score of the detection.\r\n> * `kpts(17)`: The remaining 17 values represent the keypoints or pose estimation information associated with the detection.\r\n> \r\n> Regarding your specific question about the first result, the `xy` in the `xywh` format refers to the top-left corner of the bounding box.\r\n> \r\n> I hope this clarifies your understanding of the output. If you have any further questions, feel free to ask.\r\n> \r\n> Best regards, Glenn Jocher\r\n\r\nThanks a lot "
      },
      {
        "user": "glenn-jocher",
        "body": "You're welcome @PredyDaddy! I am happy to know that my explanation was helpful and clarified your doubts. If you have any further questions regarding YOLOv8 or encounter any other issues, feel free to ask. Your contributions to this community are truly appreciated! \n\nGood luck with your development, and thank you for choosing and supporting the Ultralytics YOLOv8!\n\nBest regards,\nGlenn Jocher\n"
      }
    ]
  },
  {
    "issue_number": 1556,
    "title": "TensorRT predict ",
    "author": "Lexiaotian",
    "state": "closed",
    "created_at": "2023-03-22T02:19:30Z",
    "updated_at": "2025-06-16T09:36:39Z",
    "labels": [
      "question",
      "non-reproducible",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nWhy TensorRT Inferential preprocess take so long？\r\n.pt: Speed: 1.4ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 1280, 1280)\r\n.engine: Speed: 30.2ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 1280, 1280)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "Laughing-q",
        "body": "@Lexiaotian it works fine to me.\r\n![PtKfeofzUz](https://user-images.githubusercontent.com/61612323/226788841-66f4ae51-0fa0-4724-b5f1-fe5d9e61c16b.jpg)\r\nBTW I exported model by:\r\n```bash\r\nyolo export model=weights/yolov8s.pt format=engine opset=12 device='0' imgsz=1280 half\r\n```"
      },
      {
        "user": "Lexiaotian",
        "body": "I tried it your way, but the gap in preprocessing is still large，I would like to ask, is there any difference between the pre processing methods of. pt and. engine?"
      },
      {
        "user": "Laughing-q",
        "body": "@Lexiaotian there's no difference between these two."
      }
    ]
  },
  {
    "issue_number": 21049,
    "title": "`convert_segment_masks_to_yolo_seg` get mask (h,w) error",
    "author": "Houvven",
    "state": "open",
    "created_at": "2025-06-13T10:19:31Z",
    "updated_at": "2025-06-16T09:01:15Z",
    "labels": [
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nOther\n\n### Bug\n\nWhen I used convert_segment_masks_to_yolo_seg, I encountered the following error:\n```\nTraceback (most recent call last):\n  File \"C:\\Users\\houvven\\AppData\\Local\\Programs\\PyCharm Professional\\plugins\\python-ce\\helpers\\pydev\\pydevd.py\", line 1570, in _exec\n    pydev_imports.execfile(file, globals, locals)  # execute the script\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\houvven\\AppData\\Local\\Programs\\PyCharm Professional\\plugins\\python-ce\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\n  File \"D:\\Projects\\DeepLearn\\yolo-document-segment\\datasets_init.py\", line 129, in <module>\n    convert_segment_masks_to_yolo_seg(\n  File \"C:\\Users\\houvven\\miniconda3\\envs\\DeepLearn\\Lib\\site-packages\\ultralytics\\data\\converter.py\", line 389, in convert_segment_masks_to_yolo_seg\n    img_height, img_width = mask.shape  # Get image dimensions\n    ^^^^^^^^^^^^^^^^^^^^^\nValueError: too many values to unpack (expected 2)\n```\n\n\nhttps://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/converter.py#L388, mask shape is (H,W,C) but it only accepts two args.\n\n### Environment\n\nUltralytics 8.3.142  Python-3.11.11 torch-2.4.1+cpu CPU (Intel Core(TM) i7-14700KF)\nSetup complete  (28 CPUs, 31.8 GB RAM, 567.7/1907.7 GB disk)\n\nOS                  Windows-10-10.0.26200-SP0\nEnvironment         Windows\nPython              3.11.11\nInstall             pip\nPath                C:\\Users\\houvven\\miniconda3\\envs\\DeepLearn\\Lib\\site-packages\\ultralytics\nRAM                 31.84 GB\nDisk                567.7/1907.7 GB\nCPU                 Intel Core(TM) i7-14700KF\nCPU count           28\nGPU                 None\nGPU count           None\nCUDA                None\n\nnumpy                2.2.5>=1.23.0\nmatplotlib           3.10.1>=3.3.0\nopencv-python        4.11.0.86>=4.6.0\npillow               11.2.1>=7.1.2\npyyaml               6.0.2>=5.3.1\nrequests             2.32.3>=2.23.0\nscipy                1.15.2>=1.4.1\ntorch                2.4.1>=1.8.0\ntorch                2.4.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision          0.19.1>=0.9.0\ntqdm                 4.67.1>=4.64.0\npsutil               5.9.0\npy-cpuinfo           9.0.0\npandas               2.2.3>=1.1.4\nultralytics-thop     2.0.14>=2.0.0\n\n\n### Minimal Reproducible Example\n\n```python\nconvert_segment_masks_to_yolo_seg(\n        masks_dir=preprocessed_masks_dir,\n        output_dir=output_dir,\n        classes=1\n)\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Houvven, thank you for reporting this issue and for providing detailed environment info and a minimal reproducible example 🚀! This is an automated response to help streamline your experience—an Ultralytics engineer will review and assist you soon.\n\nFor reference, if you’re new to Ultralytics or looking for more examples, check out our [Docs](https://docs.ultralytics.com/), where you’ll find [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage guides. Many frequently asked questions are addressed there.\n\nSince this is a 🐛 Bug Report, thank you for including a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/). This greatly helps us debug and resolve issues faster.\n\nYou’re always welcome to join the Ultralytics community for more support and discussion:\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth discussions: [Discourse](https://community.ultralytics.com/)\n- Community knowledge sharing: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease ensure you are running the latest version of the `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). This helps verify if your issue persists with the newest code:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO runs seamlessly in the following environments, all with up-to-date dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests validate all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your detailed report! An Ultralytics engineer will follow up soon to assist further."
      },
      {
        "user": "Y-T-G",
        "body": "It only supports 1 channel masks."
      },
      {
        "user": "Houvven",
        "body": "```python\nimport cv2\n\nimage = cv2.imread('data/tmp/train/segments_processed/image_00001.png', cv2.IMREAD_GRAYSCALE)\nprint(\"Image shape:\", image.shape) #ouput: Image shape: (512, 512)\n```\n\nThe above is correct, but if I import ultralytics, the shape will become (h, w, c).\n\n```python\nfrom ultralytics.data.converter import convert_segment_masks_to_yolo_seg\n# or import ultralytics\nimport cv2\n\nimage = cv2.imread('data/tmp/train/segments_processed/image_00001.png', cv2.IMREAD_GRAYSCALE)\nprint(\"Image shape:\", image.shape) # outut: Image shape: (512, 512, 1)\n```\n"
      }
    ]
  },
  {
    "issue_number": 20517,
    "title": "yolo+tensorboard，no \"events.out.*\"log event file",
    "author": "OrchidMale",
    "state": "closed",
    "created_at": "2025-05-06T12:43:09Z",
    "updated_at": "2025-06-16T07:46:00Z",
    "labels": [
      "question",
      "fixed",
      "classify"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nWhen training the model, the corresponding \"runs/classify/train\" folder does not generate \"events.out.*\" log event file.\nI refer to the documentation for this URL \"https://docs.ultralytics.com/integrations/tensorboard/#installation\".\nFollowing the step-by-step operation of the document, there is no output like \"TensorBoard: Start with 'tensorboard --logdir path_to_your_tensorboard_logs', view at http://localhost:6006/\"\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @OrchidMale, thank you for reaching out and for your interest in Ultralytics 🚀! This is an automated response to help you get started—one of our engineers will also assist you soon.\n\nWe recommend visiting the [Docs](https://docs.ultralytics.com/) for comprehensive guides, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples. Many common questions and integration topics are already covered there.\n\nSince you’re experiencing issues with TensorBoard log files not being generated, if this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/). This helps us to quickly understand and debug your situation.\n\nIf this is a custom training ❓ Question, please include as much detail as possible, such as snippets of your training script, dataset setup, and any logs, and confirm you’ve reviewed our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community to connect with others:\n- For quick chats, join our [Discord](https://discord.com/invite/ultralytics) 🎧\n- For deeper discussions, visit our [Discourse](https://community.ultralytics.com/)\n- To share and learn with others, check our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease ensure you’re running the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) installed in a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO runs seamlessly in these up-to-date verified environments (with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI verifies the correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nWe appreciate your patience and look forward to helping you resolve your issue!"
      },
      {
        "user": "Y-T-G",
        "body": "You need to enable Tensorboard.\n\n`yolo settings tensorboard=True`"
      },
      {
        "user": "OrchidMale",
        "body": "@Y-T-G \n> You need to enable Tensorboard.\n> \n> `yolo settings tensorboard=True`\n\n![Image](https://github.com/user-attachments/assets/95748277-5879-4cc8-991c-e31d75d3d7e9)\n![Image](https://github.com/user-attachments/assets/ab678431-032b-4122-9df3-1c8ffff1e2bf)\n\nhow to enable Tensorboard?\n\n\n\n"
      }
    ]
  },
  {
    "issue_number": 21069,
    "title": "Certain challenges of YOLO11-OBB in document layout analysis scenarios",
    "author": "pearl88",
    "state": "open",
    "created_at": "2025-06-16T07:38:58Z",
    "updated_at": "2025-06-16T07:39:24Z",
    "labels": [
      "question",
      "OBB"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nFirst of all, thank you for creating such an outstanding project! When training on document data using YOLO11-OBB, I've achieved excellent precision with mAP50-95 reaching around 95. However, I'm encountering inaccurate bounding box localization issues for certain objects like text blocks, large tables, and images. Despite adjusting multiple training parameters, the improvements have been minimal.\n\nI'm writing to inquire whether specific network architecture adjustments or configuration optimizations are recommended for document-centric scenarios in YOLO11-OBB. If such modifications exist, I would be deeply grateful for your expert guidance. Thank you immensely for your support!\n\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @pearl88, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response 🤖 to help guide you more quickly! An Ultralytics engineer will also review and assist you soon."
      }
    ]
  },
  {
    "issue_number": 20645,
    "title": "Classification recognition rate issue",
    "author": "monkeycc",
    "state": "open",
    "created_at": "2025-05-14T14:10:52Z",
    "updated_at": "2025-06-16T07:03:23Z",
    "labels": [
      "question",
      "classify"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n```python\n    model = YOLO( \"./yolo11s-cls.pt\") \n\n    results = model.train(data=  \"./yolo\", imgsz=512 , epochs=100, batch = 60 ,   degrees=0.0, translate=0.0, scale=0.0, shear=0.0, perspective=0.0,\n                      hsv_h=0.0,hsv_s=0.0, hsv_v=0.0, flipud=0.0, fliplr=0.0, bgr=0.0, mosaic=0.0, mixup=0.0, copy_paste=0.0, copy_paste_mode='', auto_augment='',\n                      erasing=0.0, crop_fraction=1.0)  \n```\n\n![Image](https://github.com/user-attachments/assets/4f1ebfa3-33da-4749-8454-06b12df07833)\n\nHow many objects are there\nDifferent missing corners\n\nFor the angle\nThe recognition rate is very poor\n\nRequest assistance\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @monkeycc, thank you for reaching out and sharing your experience with Ultralytics 🚀! This is an automated response to help you get started while an Ultralytics engineer reviews your issue.\n\nWe recommend visiting the [Docs](https://docs.ultralytics.com/) for detailed guides and checking out [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, which may address common classification and training questions.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) including a sample of your data and the exact code used. This helps us debug more efficiently.\n\nFor training performance or recognition rate ❓ questions, please share additional details such as:\n- Dataset samples (images and labels)\n- Complete training logs\n- Your data.yaml configuration\n- Clarification on how the \"angle\" is affecting results\n\nYou can also connect with the Ultralytics community in real-time on [Discord](https://discord.com/invite/ultralytics) 🎧, join discussions on [Discourse](https://community.ultralytics.com/), or share insights on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease upgrade to the latest `ultralytics` package and ensure all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) are met in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to check if the issue persists:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in these verified environments (with dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nAn Ultralytics engineer will review your issue and assist you further soon. Thank you for your patience and for helping improve Ultralytics!"
      },
      {
        "user": "glenn-jocher",
        "body": "Based on your training configuration, I can see why you're experiencing poor recognition rates for objects at different angles. You've disabled all data augmentation (`degrees=0.0, translate=0.0, scale=0.0...`), which is likely limiting your model's ability to generalize to different orientations and variations.\n\nFor objects at different angles, I'd recommend:\n\n```python\nmodel.train(data=\"./yolo\", imgsz=512, epochs=100, batch=60,\n            degrees=45.0,  # Enable rotation augmentation\n            fliplr=0.5,    # Add horizontal flips\n            scale=0.5      # Add scaling variation\n            # Keep other augmentations as needed\n           )\n```\n\nSince your dataset contains objects with different orientations and missing corners, these augmentations will help the model learn invariance to these variations. Consider using `perspective` and `shear` parameters as well if you need the model to recognize objects from different viewpoints.\n\nFor more detailed guidance on model evaluation and fine-tuning, you can refer to our [performance metrics guide](https://docs.ultralytics.com/guides/yolo-performance-metrics/) to understand how to interpret and improve your results."
      },
      {
        "user": "monkeycc",
        "body": "![Image](https://github.com/user-attachments/assets/d3d07597-d8fe-4f04-975d-6e825a619118)\n---\n![Image](https://github.com/user-attachments/assets/7d649915-a3da-419c-89a7-aa5065fe8330)\n---\n![Image](https://github.com/user-attachments/assets/508d64f2-7be7-417a-993f-0cefc3d1b699)\n---\n\ntrain_batch0.jpg\ntrain_batch1.jpg\ntrain_batch2.jpg\n\n```python\n(flipud=0.0, fliplr=0.0, mosaic=0.0, mixup=0.0, copy_paste=0.0, copy_paste_mode='', \n                      erasing=0.0, shear=0.0,translate=0.0,auto_augment=None,degrees=0.0)\n```\n\nI found that my image will be cropped\nI need not to crop or move, but I found that it doesn't have much effect\n\nBecause flipping can lead to incorrect direction determination\nMoving and cropping can lead to issues with recognition rate,\nI just need the complete image, changing the color and brightness, and the tilt angle\n\nBut I found that the configuration file is useless and not effective\n\nWhy is it in train_match0 train_match1 train_match2\nI see incomplete images\nIn theory, it should be a complete thumbnail"
      }
    ]
  },
  {
    "issue_number": 21068,
    "title": "Online hard negative mining",
    "author": "yeonhyochoi",
    "state": "open",
    "created_at": "2025-06-16T06:58:41Z",
    "updated_at": "2025-06-16T06:59:08Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nhello, \nHas anyone applied hard negative mining or 'online hard negative mining' to yolo? The current problem is that there is a lot of data, and I want to select difficult data from normal data and learn from them.\n \n1. There is the entire training data (class 0: 440,000, background: 1,200,000)\n2. For the first 10 epochs, learn the entire data, and for the subsequent epochs, evaluate the data each time and select only difficult data in proportion to the data that has the class and learn from them\n\nI wonder if I can reduce false positives by learning in this way.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @yeonhyochoi, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\nThis is an automated response 🦾. An Ultralytics engineer will review your question and assist you soon!\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      }
    ]
  },
  {
    "issue_number": 3528,
    "title": "3d object detection",
    "author": "Maximal4028",
    "state": "closed",
    "created_at": "2023-07-04T13:23:41Z",
    "updated_at": "2025-06-16T06:57:07Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello! I want to train 3d object detection network. Is it possible to do this using Yolov8? I need to detect object bbox, 3d center, orientation and dimensions. I have custom kitti dataset, which contains the following values for each object:\r\n\r\n1 - class\r\n2,3,4,5 - bbox (in yolo format)\r\n6,7 - x,y 3d center coordinates (in yolo format)\r\n8 - alpha (observation angle of object, ranging [-pi..pi])\r\n9, 10, 11 - x,y,z dimensions length\r\n\r\nFor example:\r\n1 0.54648 0.55136 0.03436392 0.08869 0.54553 0.5485032 -1.67 1.41 1.58 4.36\r\n\r\nIf yolov8 can be used for 3d detection, what changes do I need to work correctly? Do I need to change network layers? How to change dataset and dataloader? Do I need to change the loss function for bbox? How to correctly add a loss functions for 3d center, orientation and dimensions?\r\n\r\nThank you in advance for the answer!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @Maximal4028, thank you for your interest in YOLOv8 🚀! We recommend a visit to the [YOLOv8 Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) in a [**Python>=3.7**](https://www.python.org/) environment with [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@Maximal4028 hi there! Thank you for reaching out. While YOLOv8 is primarily designed for 2D object detection, it can be modified to support 3D object detection as well. \n\nTo train YOLOv8 for 3D object detection, you would need to make several changes to the network architecture, the dataset format, the dataloader, and the loss function.\n\nIn terms of network architecture, you would need to modify YOLOv8 to take into account the additional outputs required for 3D object detection, such as the 3D center coordinates, orientation, and dimensions. This would involve adding new layers to the network. \n\nFor the dataset format, you would need to update your KITTI dataset to include the additional values for each object, such as the 3D center coordinates, orientation, and dimensions. These values would need to be converted to the appropriate format that YOLOv8 expects.\n\nIn the dataloader, you would need to modify the code to load and preprocess the 3D object detection data, ensuring that it matches the updated dataset format.\n\nLastly, the loss function would need to be adjusted to include the appropriate terms for the 3D center, orientation, and dimensions. This would involve updating the calculation of the loss based on the predicted and ground truth values for these attributes.\n\nPlease note that modifying YOLOv8 for 3D object detection would require a deeper understanding of the network architecture and the specific requirements of your task. It may also involve additional research and experimentation to achieve satisfactory results.\n\nI hope this helps you get started with training YOLOv8 for 3D object detection. If you have any further questions or need more guidance, please feel free to ask. Good luck with your project!"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 4003,
    "title": "Yolov8 with BEV",
    "author": "bzha5848",
    "state": "closed",
    "created_at": "2023-07-28T02:43:35Z",
    "updated_at": "2025-06-16T06:32:10Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nHello there,\r\n\r\nJust wondering for Yolov8 would that be available to be applied in autonomous vehicle driving areas, like BEV? or be integrated with repos below?\r\nhttps://github.com/OpenDriveLab/UniAD/tree/main\r\nhttps://github.com/OpenDriveLab/Birds-eye-view-Perception\r\n\r\nThanks so much!!\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @bzha5848, thank you for your interest in YOLOv8 🚀! We recommend a visit to the [YOLOv8 Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) in a [**Python>=3.7**](https://www.python.org/) environment with [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@bzha5848 hello,\n\nThanks for showing interest in using YOLOv8 for autonomous vehicle driving scenarios and for your willingness to contribute with a PR. \n\nYes, YOLOv8 can most certainly be applied in autonomous vehicle driving areas such as Bird's Eye View (BEV) detection. It can process images from any perspective, including the bird's-eye view. The success depends largely on the training data: if it contains appropriate bird's-eye view images, YOLOv8 should be well equipped to detect objects from that perspective during inference.\n\nAs for integration with the repositories you've mentioned, yes, technically it should be feasible. YOLOv8 can generate the object detection results that Perception and UniAD to utilize. The modification and integration would mostly involve having the output of YOLOv8 be the input to these other systems.\n\nHope this helps. Good luck with your work and don't hesitate to contribute back with a PR! Your contributions will be beneficial to the whole YOLOv8 community.\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 19607,
    "title": "Problem with calculating mAP on validation dataset with different difficulty levels",
    "author": "Berni11",
    "state": "closed",
    "created_at": "2025-03-10T02:12:37Z",
    "updated_at": "2025-06-16T05:52:11Z",
    "labels": [
      "question",
      "Stale",
      "OBB",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI have trained a yolo11n-obb model on the Kitti Dataset by converting the pointclouds into BEV images. The training worked well and the model also detects correctly.\n\nTo be able to compare the results with other object detection models, I wanted to validate my yolo model using the 3 difficulties specified on the Kitti Dataset website: Easy, Moderate and Hard\nhttps://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=bev\n\nFor this I have created 3 different validation sets. They contain the same images but have their labels filtered according to the guidelines.\n\nNow when I validate my model using the easy, moderate and hard validation set, I get better mAP on the hard dataset than on the easy dataset. I expected it to be the opposite as the easy dataset has fewer objects to detect.\n\nHere are the validation results from the easy validation set:\n`Ultralytics 8.3.86 🚀 Python-3.9.21 torch-2.5.1+cu124 CUDA:0 (Tesla V100-SXM2-32GB, 32494MiB)\nYOLO11n-obb summary (fused): 109 layers, 2,654,308 parameters, 0 gradients, 6.6 GFLOPs\nval: Scanning /home/user/Dataset/Kitti/BEVdata-obb_val_easy_640/validation/labels.cache... 1481 images, 0 backgrounds, 0 corrupt: 100%|███`\n\n                 `Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 93/93 [00:09<00:00, 10.14it/s]\n                   all       1481       2637      0.458      0.779      0.541      0.426\n            Pedestrian        571        821      0.608      0.497      0.463      0.299\n                   Car        977       1679      0.299      0.993      0.569      0.546\n               Cyclist        107        137      0.469      0.847      0.592      0.434\n`Speed: 0.3ms preprocess, 1.4ms inference, 0.0ms loss, 1.3ms postprocess per image\nResults saved to runs/obb/val125`\n\nResults from the moderate validation set:\n\n`Ultralytics 8.3.86 🚀 Python-3.9.21 torch-2.5.1+cu124 CUDA:0 (Tesla V100-SXM2-32GB, 32494MiB)\nval: Scanning /home/user/Dataset/Kitti/BEVdata-obb_val_moderate_640/validation/labels.cache... 1481 images, 13 backgrounds, 0 corrupt: 100`\n\n                 `Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 93/93 [00:09<00:00,  9.73it/s]\n                   all       1481       4684      0.696      0.839      0.796      0.608\n            Pedestrian        375        779      0.764      0.694      0.739      0.465\n                   Car       1235       3695      0.648      0.989      0.859      0.807\n               Cyclist        151        210      0.676      0.833      0.791      0.552\n`Speed: 0.2ms preprocess, 1.3ms inference, 0.0ms loss, 1.3ms postprocess per image\nResults saved to runs/obb/val126`\n\nResults from the hard \n\nvalidation set:\n\n`Ultralytics 8.3.86 🚀 Python-3.9.21 torch-2.5.1+cu124 CUDA:0 (Tesla V100-SXM2-32GB, 32494MiB)\nval: Scanning /home/user/Dataset/Kitti/BEVdata-obb_val_hard_640/validation/labels.cache... 1481 images, 11 backgrounds, 0 corrupt: 100%|██`\n\n                 `Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 93/93 [00:09<00:00,  9.63it/s]\n                   all       1481       6314      0.835      0.838      0.881       0.67\n            Pedestrian        396        932      0.814      0.678      0.774      0.479\n                   Car       1250       5111      0.879      0.981      0.976      0.907\n               Cyclist        193        271      0.811      0.855      0.894      0.624\n`Speed: 0.2ms preprocess, 1.3ms inference, 0.0ms loss, 1.3ms postprocess per image`\n\nImage showing the labels from the easy validation set:\n![Image](https://github.com/user-attachments/assets/9ee430c2-4686-441b-a7d2-e7774d1b7e92)\n\nImage showing the labels from the moderate validation set:\n![Image](https://github.com/user-attachments/assets/1b1b0265-ff77-4030-a302-06e052865800)\n\nImage showing the labels from the hard validation set:\n![Image](https://github.com/user-attachments/assets/a10a8902-d48e-499b-b589-c463a3700c0c)\n\nImage with the detections by the model:\n![Image](https://github.com/user-attachments/assets/f5d93182-ffc5-4a5c-a335-393cc526d6c9)\n\n\nPlease let me know if someone knows why the mAP values are better on the hard validation set than on the easy validation set and if this can be fixed to get correct mAP values.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Berni11, thank you for your detailed report and for using Ultralytics 🚀! We appreciate your effort in providing the validation results and images. Here's how we can assist you:\n\nWe recommend first reviewing our [Docs](https://docs.ultralytics.com/) for guidance, as they contain many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples, which might help clarify some aspects of your validation process.\n\nIf this is a 🐛 Bug Report, we kindly ask you to provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/). This will help us debug the issue effectively. Be sure to include the following:\n1. Your validation command(s) and any custom arguments.\n2. A small subset of your dataset (if possible), or a clear description of how the validation sets were prepared.\n3. Any additional logs or errors not already provided.\n\nIf this is a custom training or ❓ Question, you might want to verify the following:\n1. Are the validation datasets correctly filtered based on the Kitti Dataset's difficulty levels?\n2. Are the validation metrics consistent across different hardware or environments? Sometimes discrepancies can arise from preprocessing or environment configurations.\n3. Are you following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)?\n\nTo ensure your environment is up-to-date, please upgrade to the latest `ultralytics` version along with its dependencies. Run the following:\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can validate your model in any of the following verified environments to ensure consistency:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Community and Support\n\nJoin the Ultralytics community where it suits you best:\n- For real-time assistance, join our [Discord](https://discord.com/invite/ultralytics) 🎧.\n- Prefer in-depth discussions? Visit our [Discourse](https://community.ultralytics.com/).\n- Share knowledge or ask for advice on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf the above badge is green, it means our [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing, and the issue might be specific to your setup.\n\nThis is an automated response to help guide you, but rest assured, an Ultralytics engineer will review your issue and provide further assistance soon. Thank you for your patience! 😊"
      },
      {
        "user": "Y-T-G",
        "body": "The mAP calculation has a bug, which can be fixed by using the suggestion by OP here:\n\nhttps://github.com/ultralytics/ultralytics/issues/17270"
      },
      {
        "user": "Berni11",
        "body": "I have tried the mentioned fix but I still get higher mAP on the hard dataset than on the easy dataset:\n\nEasy:\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 93/93 [00:11<00:00,  8.27it/s]\n                   all       1481       2637      0.458      0.779      0.534      0.418\n            Pedestrian        571        821      0.608      0.497       0.44       0.28\n                   Car        977       1679      0.299      0.993      0.569      0.544\n               Cyclist        107        137      0.469      0.847      0.592       0.43\n\nModerate:\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 93/93 [00:11<00:00,  8.05it/s]\n                   all       1481       4684      0.696      0.839      0.793      0.599\n            Pedestrian        375        779      0.764      0.694      0.729      0.452\n                   Car       1235       3695      0.648      0.989      0.859      0.799\n               Cyclist        151        210      0.676      0.833      0.791      0.545\n\nHard:\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 93/93 [00:12<00:00,  7.69it/s]\n                   all       1481       6314      0.835      0.838      0.877      0.657\n            Pedestrian        396        932      0.814      0.678      0.762      0.463\n                   Car       1250       5111      0.879      0.981      0.976      0.895\n               Cyclist        193        271      0.811      0.855      0.892      0.614\n"
      }
    ]
  },
  {
    "issue_number": 1765,
    "title": "Yolov8 training on KITTI Dataset",
    "author": "Prashambhuta",
    "state": "closed",
    "created_at": "2023-04-02T10:08:21Z",
    "updated_at": "2025-06-16T02:25:48Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nI am attempting to train KITTI Lidar data for Object detection using YOLOv8 architecture. I am struggling with creating the forward pass for training. I am using the following repositories as a reference. I would appreciate it if someone would help me construct the training model.\r\n\r\n* https://github.com/ghimiredhikura/Complex-YOLOv3\r\n* https://github.com/AI-liu/Complex-YOLO\r\n\r\nThere is an issue, which requests an approach to attempt this, but it is also open.\r\n\r\n* https://github.com/ultralytics/ultralytics/issues/1058\n\n### Use case\n\nThe Yolov8 will improve the performance of the KITTI dataset Object detection and would be good to compare the results with existing YOLO implementations.\r\n\r\n\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @Prashambhuta, thank you for your interest in YOLOv8 🚀! We recommend a visit to the [YOLOv8 Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) in a [**Python>=3.7**](https://www.python.org/) environment with [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@Prashambhuta It appears that you're trying to adapt the YOLOv8 architecture for object detection using KITTI Lidar data. While I can't provide a complete solution for this task, I can guide you through the general steps that you might need to follow in order to achieve your goal.\r\n\r\nData preparation: The first step is to convert the KITTI Lidar dataset into a suitable format that can be used with YOLOv8. This might involve converting the point cloud data into a format that can be used as input to the YOLOv8 network.\r\n\r\nModify YOLOv8 architecture: Since Lidar data is inherently different from image data, you may need to modify the YOLOv8 architecture to accommodate the differences. This could involve modifying the input layer to accept Lidar data, and potentially adjusting other layers or adding new ones to better process the data.\r\n\r\nAdapt the loss function: The loss function may also need to be adapted to suit the specific characteristics of Lidar data. You can refer to the loss functions used in the Complex-YOLOv3 and Complex-YOLO repositories you mentioned for inspiration.\r\n\r\nTraining: Once you have made the necessary modifications to the YOLOv8 architecture and loss function, you can proceed with training the model using the prepared KITTI Lidar dataset.\r\n\r\nEvaluation: Finally, you'll need to evaluate the performance of your YOLOv8 model on the KITTI dataset. This will involve running inference on the test dataset and comparing the results to the ground truth labels, as well as comparing the results to other YOLO implementations.\r\n\r\nPlease note that adapting the YOLOv8 architecture for Lidar data is a complex task and may require significant experimentation and fine-tuning. The repositories and issue you mentioned are a good starting point, but you may also want to consult relevant research papers and other resources to gain a deeper understanding of the problem and potential solutions.\r\n\r\nGood luck with your project!"
      },
      {
        "user": "Prashambhuta",
        "body": "Thank you for your input. I created a YOLOv8 architecture and using the same DataLoader from ComplexYOLO, I plan to train the model. I am trying to figure out the Forward step function for the v8 model. "
      }
    ]
  },
  {
    "issue_number": 19498,
    "title": "Support Exporting YOLOv8 to ONNX with IR Version 8",
    "author": "lasindu-codegen",
    "state": "closed",
    "created_at": "2025-03-03T07:30:56Z",
    "updated_at": "2025-06-16T01:35:43Z",
    "labels": [
      "question",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI'm trying to export a YOLOv8 model to ONNX with an intermediate representation (IR) version of 8. By default, the export produces a model with IR version 9, which is incompatible with my deployment environment. I attempted to force opset 8 during export, but this led to errors—specifically with the `upsample_nearest2d` operator.\n\n\n**Steps to Reproduce:**\n1. Run the following code snippet:\n   ```python\n   import torch\n   from ultralytics import YOLO\n\n   # Load the YOLOv8 model\n   model = YOLO(\"yolov8m.pt\")  # use the pretraiend weight \n\n   # Attempt to export to ONNX with opset 8 (expected to target IR version 8)\n   onnx_path = \"yolov8m.onnx\"\n   model.export(format=\"onnx\", opset=8, dynamic=True)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @lasindu-codegen, thank you for reaching out and sharing your question about exporting YOLOv8 to ONNX with IR version 8 🚀! We recommend checking out our [Docs](https://docs.ultralytics.com/) for guidance on YOLO usage, including export formats and troubleshooting tips.\n\nIf this is a 🐛 Bug Report, please ensure you provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) that highlights the issue, including the exact error messages and configuration steps you've taken. This will help us pinpoint the problem more efficiently.\n\nIf your request is a ❓ Question about custom usage or export scenarios, please include additional details about your development environment (e.g., Python version, PyTorch version, ONNX runtime version) as this can have an impact on compatibility. In addition, sharing the full error traceback would be helpful for troubleshooting.\n\nIn the meantime, ensure you are using the latest version of the `ultralytics` package. You can upgrade to the latest release by running:\n\n```bash\npip install -U ultralytics\n```\n\nYou can run YOLOv8 in a variety of environments. If you haven't already, we recommend testing your export code in one of these verified setups:\n\n- **Notebooks** with free GPU access:    \n   <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>  \n- **Google Cloud** Deep Learning VM. Refer to our [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/).\n- **Amazon AWS** Deep Learning AMI. See the [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/).\n- **Docker Image**. You can test using the prebuilt YOLO Docker image. Instructions are available in the [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n### Status  \n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>  \n\nEnsure the latest CI tests are passing, as these validate the correct functionality of the repository. If the badge is green, the repository is in good health for compatibility across tested environments.\n\n💡 Just a friendly note: This is an automated response to assist you with initial troubleshooting and guidance. Rest assured, an Ultralytics engineer will review your issue and provide additional support soon! 😊"
      },
      {
        "user": "Laughing-q",
        "body": "@lasindu-codegen this is because onnx with `opset=8` does not include the `upsample_nearest2d` operator, recommend you to use higher opset version."
      },
      {
        "user": "Y-T-G",
        "body": "IR version is not connected to the opset version. You need to downgrade the `onnx` package to get a lower IR version:\n\nhttps://onnxruntime.ai/docs/reference/compatibility.html\n\nFor IR version 8, you need the onnx version 1.14\n\n\n"
      }
    ]
  },
  {
    "issue_number": 1058,
    "title": "Is it possible to train and detect lidar point cloud data using yolov8? ",
    "author": "safal-305",
    "state": "closed",
    "created_at": "2023-02-20T05:13:53Z",
    "updated_at": "2025-06-16T01:17:18Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "\r\n### Question\r\n\r\n\r\nThis is not an issue , but more like a doubt or clarification. I was wondering if it as possible to detect yolov8 for the detection and training of objects from lidar point cloud data.\r\n\r\n\r\n",
    "comments": [
      {
        "user": "JustasBart",
        "body": "I've an **Intel® RealSense™ LiDAR Camera L515** myself and would love to know also :rocket: "
      },
      {
        "user": "AyushExel",
        "body": "Not currently as we don't have the right data loader and loss functions(I'm not sure what exactly it'll be) in place. But I think it'll be a nice experiment if you decide to embark on that journey. Happy to help!"
      },
      {
        "user": "nguyenlam185",
        "body": "@JustasBart \r\nCheck this out https://github.com/ghimiredhikura/Complex-YOLOv3"
      }
    ]
  },
  {
    "issue_number": 1458,
    "title": "Questions about mask_ratio, mask_overlap, pretrained and augmentation process  ",
    "author": "DP1701",
    "state": "closed",
    "created_at": "2023-03-16T12:39:09Z",
    "updated_at": "2025-06-16T01:05:41Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello all,\r\n\r\nThanks for the release of the new YOLO version. \r\n\r\nI currently have a few questions that the documentation can't answer. \r\n\r\nSegmentation:\r\n\r\nWhat is the function of the parameter \"mask_ratio\"? What does the 4 mean and what changes if I enter a 1?\r\n\r\nWhat is the function of the parameter \"overlap_mask\"? Is there an overlap of the masks when using copy_paste (augmentation)?\r\n\r\nWhat does the parameter \"pretrained\" stand for? Isn't a pre-trained model used during training, for example if I enter model=yolov8n-seg.pt?\r\n\r\nAugmentation:\r\n\r\nIs the current albumentations implementation compatible with training mode \"segment\"? \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @DP1701, thank you for your interest in YOLOv8 🚀! We recommend a visit to the [YOLOv8 Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\n## Install\n\nPip install the `ultralytics` package including all [requirements.txt](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) in a [**Python>=3.7**](https://www.python.org/) environment with [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 modes and tasks on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "AyushExel",
        "body": "mask_ratio: It is used to set the downsampling ratio when training the segmentation mask. 4 means the masks/images will be downsampled by a factor of 4. We're using it by default as it is much faster to train and it doesn't lose much accuracy. Setting it to 1 will train in the original resolution.\r\n\r\nmask_overlap: This trains the segmentation models by overlapping all individual masks as one mask. This also improves speed by a lot (more than doubles the speed sometimes!). Disabling it might give you a slight improvement in map.\r\n\r\nPretrained: Yes this is a new addition for segmentation and detection. Your usage is correct. if you pass model=model.pt, it will be used as a pre-trained model.  now you can use pretrained=model.pt to transfer pre-trained weights to a model.\r\nSee this for usage https://github.com/ultralytics/ultralytics/issues/1406#issuecomment-1471968676\r\n"
      },
      {
        "user": "AyushExel",
        "body": "Please use the yolo command for training.\r\n```\r\nyolo segment train device=\"1,2,3,4\"\r\n```\r\nhttps://docs.ultralytics.com/tasks/segment/"
      }
    ]
  },
  {
    "issue_number": 21039,
    "title": "Transfer weights from detection model",
    "author": "AbdullahOx",
    "state": "closed",
    "created_at": "2025-06-12T12:41:47Z",
    "updated_at": "2025-06-16T00:30:06Z",
    "labels": [
      "question",
      "detect",
      "pose"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi,\nI have trained a detector model to detect a specific object, and now I would like to transfer the weights from this model, specifically the backbone, neck, and detection heads, to a pose estimation model. \n\nNote: Both models have the same size.\n\nCould you please advise on how to proceed?\nThank you.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @AbdullahOx, thank you for reaching out and for your interest in Ultralytics 🚀! This is an automated response to help you get started while an Ultralytics engineer reviews your question.\n\nIf you haven’t already, please check the [Docs](https://docs.ultralytics.com/), where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples. There may be relevant information about transferring weights and customizing models.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug more efficiently.\n\nFor custom training or transfer learning ❓ questions like this, please share as much detail as possible, such as your model configuration, sample code, and any errors you encounter. Also, make sure you’re following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community for more discussion and support:\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth discussions: [Discourse](https://community.ultralytics.com/)\n- Community knowledge sharing: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease ensure you’re using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in a variety of supported, GPU-enabled environments with all dependencies preinstalled:\n\n- **Notebooks**: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing, verifying correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu.\n\nAn Ultralytics engineer will review your request and assist you soon. Thank you for your patience and for contributing to the Ultralytics community! 🙌"
      },
      {
        "user": "Y-T-G",
        "body": "You can use `.load()` to transfer weights.\n\n`model =  YOLO(\"yolo11n-pose.yaml\").load(\"your_model.pt\")`"
      },
      {
        "user": "AbdullahOx",
        "body": "Great, this worked, thank you! @Y-T-G \nI think I was just overcomplicating things with my implementation, while your one-line solution did the trick. :)\n\nOne last question: can I still train/fine-tune the model on keypoints as usual after transferring the weights, or would I need to make any additional adjustments?"
      }
    ]
  },
  {
    "issue_number": 3078,
    "title": "YOLOv8 3D Object detection in WOD (Waymo Open Dataset)",
    "author": "atanasko",
    "state": "closed",
    "created_at": "2023-06-07T12:10:11Z",
    "updated_at": "2025-06-15T23:31:32Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\nHi,\r\n\r\nI would like to play with WOD (Waymo Open Dataset) and to detect object in LIDAR data. I already find #1058 and  #1765, but still have some ambiguity. If I understand well in order to train and model to detect objects in LIDAR data from Waymo Open Dataset, I'll need to train this model to do that with BEV(Bird's Eye View) image. But in order to do that I'll need to write new Dataset class that will load data from WOD, and than also to find the right Loss function. Is this the correct path? Am I missing something here?\r\n\r\nI already look into [Complex-YOLO](https://github.com/AI-liu/Complex-YOLO/tree/master), [Complex-YOLOv3](https://github.com/ghimiredhikura/Complex-YOLOv3) and [Complex-YOLOv4-Pytorch](https://github.com/maudzung/Complex-YOLOv4-Pytorch) repositories.\r\n\r\nWill I need to modify something in the YOLOv8 structure?\r\n\r\nThanks in advance,\r\nAtanasko \r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@atanasko hi Atanasko,\n\nYes, you are correct. In order to train and model to detect objects in LIDAR data from Waymo Open Dataset, you will need to create a new Dataset class that will load data from the dataset and convert it to BEV (Bird's Eye View) image. You will also need to select an appropriate Loss function to train the model.\n\nThere should be no need to modify the YOLOv8 structure itself, just create the appropriate Dataset class to preprocess the data for training.\n\nLet us know if you have any further questions.\n\nBest,\nGlenn"
      },
      {
        "user": "atanasko",
        "body": "@glenn-jocher hi Glenn,\r\n\r\nThank you! I'll try and come back to you if some more questions. I'll create a fork of YOLOv8 repository so my work can be seen there.\r\n\r\nBest regards,\r\nAtanasko"
      },
      {
        "user": "glenn-jocher",
        "body": " @atanasko You're welcome! Please feel free to create a fork of the YOLOv8 repository and work from there. If you have any questions or run into any issues, don't hesitate to ask for help. We're happy to assist you.\r\n\r\nGood luck with your project!\r\n\r\nBest regards,\r\nGlenn"
      }
    ]
  },
  {
    "issue_number": 3360,
    "title": "About the optimizer",
    "author": "Zengyf-CVer",
    "state": "closed",
    "created_at": "2023-06-24T12:01:45Z",
    "updated_at": "2025-06-15T20:26:23Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n@glenn-jocher @Laughing-q \r\nWhen training YOLOv8, the optimizer parameter defaults to auto. How does this work?\r\n![image](https://github.com/ultralytics/ultralytics/assets/41098760/a920b86e-0d34-4d0f-a6ec-2a1953a2f700)\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@Zengyf-CVer I tried to do a bit of a study on optimizers. I trained about 200 models on 4 datasets and noticed that AdamW showed improvement in many cases for transfer learning tasks vs the current default, so the smart_optimizer() function will apply it if the dataset seems small. See for example https://wandb.ai/glenn-jocher/optim-VOC.yaml\r\n\r\n\r\nThe `auto` logic is here:\r\nhttps://github.com/ultralytics/ultralytics/blob/51d8cfa9c37b7b2b98b3d3ec5a6f1a9ff6b38359/ultralytics/yolo/engine/trainer.py#L610-L636"
      },
      {
        "user": "glenn-jocher",
        "body": "@Zengyf-CVer in general though there is a lot of work to create hyps that are tuned for the most common use case (i.e. fine-tuning pretrained weights on a smaller dataset), rather than the current hyps which are fine-tuned for training COCO from scratch."
      },
      {
        "user": "Zengyf-CVer",
        "body": "@glenn-jocher \r\nIt can be seen from this line of code that AdamW is used within 10,000 iterations, and SGD is used more than 10,000 times. Am I right?\r\nhttps://github.com/ultralytics/ultralytics/blob/51d8cfa9c37b7b2b98b3d3ec5a6f1a9ff6b38359/ultralytics/yolo/engine/trainer.py#L634"
      }
    ]
  },
  {
    "issue_number": 21016,
    "title": "can I fix the GPU usage during training？",
    "author": "Monlter",
    "state": "open",
    "created_at": "2025-06-11T03:21:56Z",
    "updated_at": "2025-06-15T17:36:44Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello, can I fix the GPU usage during training? In other words, I can prevent the GPU memory usage from fluctuating. This can prevent my program from failing to apply for GPU resources when other nodes use too much GPU resources during training.\nThe main problem is that when I am at a certain stage of the training process, the GPU memory occupied by the training code suddenly decreases, and other programs may take the opportunity to use more GPU memory. When the training code needs to apply for more GPU memory, the training code crashes due to lack of extra GPU memory. This problem bothers me a lot.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Monlter, thank you for reaching out and using Ultralytics 🚀! This is an automated response to help guide you while an Ultralytics engineer reviews your question.\n\nFor resource management and GPU usage customization, you may find helpful information in our [Docs](https://docs.ultralytics.com/)—especially the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) sections. Many common questions about training and hardware compatibility are addressed there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate further.\n\nIf you are looking for best practices in custom training or troubleshooting, please share as much detail as possible—such as environment specs, training logs, and configuration files. Also verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community for support and discussion:\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth discussions: [Discourse](https://community.ultralytics.com/)\n- Community threads: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nMake sure you are running the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to see if your issue persists:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can run in any of these verified environments (with all dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf the badge above is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. Our CI system verifies the correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nAn Ultralytics engineer will review your question soon. Thank you for your patience and for being part of the Ultralytics community! 🧡"
      },
      {
        "user": "glenn-jocher",
        "body": "You can stabilize GPU memory usage by pre-allocating memory at the start of training. Try setting the `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:False` environment variable before training, or use `torch.cuda.set_per_process_memory_fraction(0.8)` in your script to reserve a fixed portion of GPU memory. Additionally, you can maximize your batch size using `batch=-1` which will automatically determine the optimal batch size for your GPU and help maintain more consistent memory usage throughout training."
      },
      {
        "user": "Monlter",
        "body": "Thank you for your reply. The above solutions do not solve the problem I raised very well. I want to use a fixed GPU memory during my training process (which can be the maximum memory required by the program and data) without any fluctuations, so that other programs will occupy my memory resources due to fluctuations in memory during my training process."
      }
    ]
  },
  {
    "issue_number": 21050,
    "title": "onnx model inference",
    "author": "ankushgowda2026",
    "state": "closed",
    "created_at": "2025-06-13T11:51:53Z",
    "updated_at": "2025-06-15T08:52:12Z",
    "labels": [
      "question",
      "segment",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi Team,\n\nI trained the yolov8n-seg.yaml model on custom data with a single class. For C++ inference, I converted the best.pt model to ONNX format. In the ONNX model, the output shape is [1, 37, 8400]. I'm not sure how to extract the correct class_id from this output. Could you please help clarify this?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @ankushgowda2026, thank you for reaching out and for using Ultralytics 🚀! This is an automated response to help get you started—an Ultralytics engineer will also assist you soon.\n\nWe recommend reviewing the [Docs](https://docs.ultralytics.com/) for guidance on ONNX export and inference. Many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples are available, and you may find relevant information on ONNX output interpretation there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) to help us investigate further, including your export and inference code snippets, sample input data, and any additional logs or errors.\n\nIf this is a ❓ Custom Training or inference question, please include as much detail as possible about your training process, the export steps, and how you are running inference in C++.\n\nYou can also join the Ultralytics community for support and discussion:\n- For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussions, check out [Discourse](https://community.ultralytics.com/)\n- Join threads on our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nMake sure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in any of the following verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your contribution and patience!"
      },
      {
        "user": "Y-T-G",
        "body": "You can check this\n\nhttps://github.com/winxos/yolov8_segment_onnx_in_cpp"
      },
      {
        "user": "ankushgowda2026",
        "body": "thank you @Y-T-G it works for me "
      }
    ]
  },
  {
    "issue_number": 20340,
    "title": "Mismatch between Confusion Matrix and Detection Count in YOLOv11",
    "author": "kkk2705",
    "state": "closed",
    "created_at": "2025-04-25T05:16:34Z",
    "updated_at": "2025-06-14T23:53:01Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi,\n\nI'm working with a YOLOv11 model for fruit detection and I'm encountering a mismatch between the confusion matrix output and the detection count when testing the model on validation images using the trained .pt file.\n\nConfusion Matrix Total: 6473  \n\n\n\nDetected Oranges (from inference on the same validation images): 5348 mature + 332 immature = 5680\n\nI used a confidence threshold of 0.25 (default) for both testing and validation. However, the confusion matrix total (6473) is higher than the number of detected oranges (5680), and I'm unsure why this discrepancy exists.\n\nCould someone explain why this mismatch occurs, and if there's a way to align the total counts between the confusion matrix and the detection output?\n\nThank you!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @kkk2705, thank you for reaching out and providing details about your experience with YOLO11 🚀! We appreciate your thoroughness in searching the issues and discussions beforehand.\n\nThis is an automated response to help guide you while an Ultralytics engineer reviews your question and will assist you soon.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) to help us investigate further. This should include relevant code snippets, a sample of your validation data, the command(s) you used, and any logs or outputs that demonstrate the mismatch.\n\nIf this is a custom training ❓ Question, please include as much information as possible, such as dataset image examples, your training and inference commands, and confirm that you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nFor more information and helpful resources, please visit the [Docs](https://docs.ultralytics.com/), especially [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, where you may find answers to common questions.\n\nJoin the Ultralytics community where it suits you best:\n- For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧\n- Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/)\n- Or join threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share ideas with the community\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "Y-T-G",
        "body": "You can read this\n\nhttps://github.com/ultralytics/ultralytics/issues/2783#issuecomment-2413530537"
      },
      {
        "user": "Y-T-G",
        "body": "Also how did you get 6473?\n\nThe bottom row isn't detections. They are labels that were missed (false negatives). You shouldn't be including those counts."
      }
    ]
  },
  {
    "issue_number": 21056,
    "title": "Inconsistencies between the confusion matrix of the training output and the output of the terminal",
    "author": "jiandaoni",
    "state": "closed",
    "created_at": "2025-06-14T06:08:57Z",
    "updated_at": "2025-06-14T21:51:03Z",
    "labels": [
      "bug",
      "question"
    ],
    "body": "> I see this as well, it stands to reason that the confusion matrix should have some correlation with the box metrics, both are calculated from the predicted results of the dataset, what parameters should I set or what should I do if I want the confusion matrix to be as consistent as possible with the curves of these metrics.\n> https://github.com/ultralytics/ultralytics/issues/17047#issuecomment-2424660435\n> In this comment you just talk about calculations being different, can you be more specific? \n\n _Originally posted by @jiandaoni in [#21007](https://github.com/ultralytics/ultralytics/issues/21007#issuecomment-2972154981)_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @jiandaoni, thank you for your detailed question and for your interest in Ultralytics 🚀! This is an automated response to help you get started—an Ultralytics engineer will review your issue and assist you soon.\n\nWe recommend checking the [Docs](https://docs.ultralytics.com/) for an overview of evaluation metrics and visualization tools, as well as practical [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/)—for example, a snippet of code, sample data, or a specific command—so we can investigate the confusion matrix and box metric discrepancies more effectively.\n\nIf this is a custom training ❓ Question, please attach any relevant dataset samples, training logs, and clarify which YOLO version and settings you are using. Also, review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) to ensure best practices.\n\nJoin the Ultralytics community where it suits you best:\n- For real-time help: [Discord](https://discord.com/invite/ultralytics) 🎧\n- For longer discussions: [Discourse](https://community.ultralytics.com/)\n- For sharing and exploring ideas: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease ensure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can run YOLO in any of these up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "The confusion matrix and box metrics measure fundamentally different aspects of model performance, which explains the inconsistencies you're observing. As detailed in our [troubleshooting guide](https://docs.ultralytics.com/guides/yolo-common-issues/#understanding-precision-metrics-in-yolo11), box precision measures geometric accuracy of bounding boxes using IoU, while confusion matrix precision focuses on classification accuracy across classes without considering geometric accuracy.\n\nA bounding box can be geometrically accurate (correct location) but have the wrong class prediction, creating discrepancies between these metrics. The confusion matrix evaluates pure classification performance, while box metrics evaluate both localization and classification together.\n\nTo minimize discrepancies, you can adjust the confidence threshold (`conf`) and IoU threshold (`iou`) during validation to be more restrictive, but complete alignment isn't possible since they fundamentally measure different performance aspects that are both important for comprehensive model evaluation."
      }
    ]
  },
  {
    "issue_number": 20975,
    "title": "YOLO11m to onnx conversion error",
    "author": "Roxy-Hart",
    "state": "open",
    "created_at": "2025-06-06T21:07:42Z",
    "updated_at": "2025-06-14T19:25:21Z",
    "labels": [
      "detect",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nI have trained a model using: yolo task=detect mode=train model=yolo11m.pt data=data.yaml epochs=8 imgsz=640\nand used this code to convert it into onnx\n\n![Image](https://github.com/user-attachments/assets/570b87ac-b3b2-41c3-94c6-20b1f7395145)\nBut there are error in detection,Further down I have also provided the output and the code usinfg to detect the onnx\n\n![Image](https://github.com/user-attachments/assets/078662d2-d7db-46a6-9562-3406afdae135)\n\n![Image](https://github.com/user-attachments/assets/d9412fdc-bdbf-4785-b8a6-ac206749f7ed)\n\n### Environment\n\nmentioned above\n\n### Minimal Reproducible Example\n\nmentioned above\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Roxy-Hart, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it. If you have already included an MRE, thank you! Please ensure it is as clear and minimal as possible, including the exact code and any relevant sample data.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response. An Ultralytics engineer will also assist you here soon! 😊"
      },
      {
        "user": "Y-T-G",
        "body": "Both the preprocessing and post-processing are incorrect.\n\nhttps://github.com/ultralytics/ultralytics/tree/main/examples/YOLOv8-ONNXRuntime\n\nhttps://www.reddit.com/r/Ultralytics/comments/1jwl6se/ultralytics_postprocessing_guide/"
      },
      {
        "user": "Roxy-Hart",
        "body": "I also wants to export this model to tensflowjs with the same output so that it can be used in deetction in a web app.with original it is getting distorted"
      }
    ]
  },
  {
    "issue_number": 21012,
    "title": "Can't get attribute 'MANet' on <module 'ultralytics.nn.modules.block'",
    "author": "Echo-Nie",
    "state": "closed",
    "created_at": "2025-06-10T17:12:50Z",
    "updated_at": "2025-06-14T12:47:55Z",
    "labels": [
      "bug",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\n# YOLO Checks\n\n```\npy3.9torch200) (base) @user:~/Hyper-YOLO$ yolo checks\nUltralytics 8.3.152 🚀 Python-3.9.21 torch-2.2.0+cu121 CUDA:0 (NVIDIA RTX 5880 Ada Generation, 48647MiB)\nSetup complete ✅ (80 CPUs, 251.5 GB RAM, 132.6/1832.2 GB disk)\n\nOS                  Linux-5.15.0-130-generic-x86_64-with-glibc2.31\nEnvironment         Linux\nPython              3.9.21\nInstall             pip\nPath                /home/songyutong/anaconda3/envs/py3.9torch200/lib/python3.9/site-packages/ultralytics\nRAM                 251.54 GB\nDisk                132.6/1832.2 GB\nCPU                 Intel Xeon Silver 4416+\nCPU count           80\nGPU                 NVIDIA RTX 5880 Ada Generation, 48647MiB\nGPU count           2\nCUDA                12.1\n\nnumpy               ✅ 1.25.2>=1.23.0\nmatplotlib          ✅ 3.9.4>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 11.1.0>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.13.1>=1.4.1\ntorch               ✅ 2.2.0>=1.8.0\ntorch               ✅ 2.2.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.17.0>=0.9.0\ntqdm                ✅ 4.66.2>=4.64.0\npsutil              ✅ 7.0.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.2.3>=1.1.4\nultralytics-thop    ✅ 2.0.14>=2.0.0\n```\n\nAs you can see, my Ultralytics version is the latest one available. However, when I run the following command:\n\n```bash\npython Hyper-YOLO/ultralytics/models/yolo/detect/train.py --data uav.yaml --model Hyper-YOLO/ultralytics/cfg/models/hyper-yolo/hyper-yolon.pt --epochs 5 --batch 20 --imgsz 640 --cfg ultralytics/cfg/default.yaml\n```\n\nI encounter this error:\n\n```\nCan't get attribute 'MANet' on <module 'ultralytics.nn.modules.block'\n```\n\nInterestingly, after checking the latest source code repository, I found that the `MANet` class no longer exists there. This is very confusing because I successfully ran this exact same code just last month — it feels surreal!\n\n---\n\nWhy is this happening?\n\n### Environment\n\npy3.9torch200) (base) @user:~/Hyper-YOLO$ yolo checks\nUltralytics 8.3.152 🚀 Python-3.9.21 torch-2.2.0+cu121 CUDA:0 (NVIDIA RTX 5880 Ada Generation, 48647MiB)\nSetup complete ✅ (80 CPUs, 251.5 GB RAM, 132.6/1832.2 GB disk)\n\nOS                  Linux-5.15.0-130-generic-x86_64-with-glibc2.31\nEnvironment         Linux\nPython              3.9.21\nInstall             pip\nPath                /home/songyutong/anaconda3/envs/py3.9torch200/lib/python3.9/site-packages/ultralytics\nRAM                 251.54 GB\nDisk                132.6/1832.2 GB\nCPU                 Intel Xeon Silver 4416+\nCPU count           80\nGPU                 NVIDIA RTX 5880 Ada Generation, 48647MiB\nGPU count           2\nCUDA                12.1\n\nnumpy               ✅ 1.25.2>=1.23.0\nmatplotlib          ✅ 3.9.4>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 11.1.0>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.13.1>=1.4.1\ntorch               ✅ 2.2.0>=1.8.0\ntorch               ✅ 2.2.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.17.0>=0.9.0\ntqdm                ✅ 4.66.2>=4.64.0\npsutil              ✅ 7.0.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.2.3>=1.1.4\nultralytics-thop    ✅ 2.0.14>=2.0.0\n\n### Minimal Reproducible Example\n\npython Hyper-YOLO/ultralytics/models/yolo/detect/train.py --data uav.yaml --model Hyper-YOLO/ultralytics/cfg/models/hyper-yolo/hyper-yolon.pt --epochs 5 --batch 20 --imgsz 640 --cfg ultralytics/cfg/default.yaml\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Echo-Nie, thank you for your detailed report and for using Ultralytics 🚀! This is an automated response to help you get started. An Ultralytics engineer will review your issue and assist you soon.\n\nWe recommend checking out the [Docs](https://docs.ultralytics.com/) for helpful [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, as many common questions are answered there.\n\nSince this is a 🐛 Bug Report, thank you for providing the command and environment details. If possible, please ensure your [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) includes all necessary code and data snippets so we can reproduce the error.\n\nJoin the Ultralytics community for real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧, in-depth discussion on [Discourse](https://community.ultralytics.com/), or threads on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease ensure you are running the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). You can upgrade using:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in these up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf the badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for being part of the Ultralytics community!"
      },
      {
        "user": "Echo-Nie",
        "body": "Sorry, I was reproducing HyperYOLO, which comes with its own version of Ultralytics. But I accidentally ran pip install ultralytics, which has a higher priority than the local one, causing an error. It's fixed now. 😭"
      },
      {
        "user": "glenn-jocher",
        "body": "Great catch! This is a common issue when working with custom forks that have additional modules. The pip-installed version of Ultralytics indeed takes precedence over your local HyperYOLO version since it lacks the custom `MANet` class. For future reference, you can use `pip uninstall ultralytics` to remove the pip version, or work in a dedicated virtual environment to avoid such conflicts."
      }
    ]
  },
  {
    "issue_number": 20889,
    "title": "set_classes after predict in YOLO-World",
    "author": "ArtemShaputko",
    "state": "closed",
    "created_at": "2025-06-01T00:14:17Z",
    "updated_at": "2025-06-14T11:17:28Z",
    "labels": [
      "question",
      "fixed",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n### Question\n\nI want to set classes in YOLO-World after predict, this is test script:\n\n```\nmodel = YOLO(\"yolov8m-worldv2.pt\")\nmodel.to(\"cuda\") if torch.cuda.is_available() else model.to(\"cpu\")\nmodel.set_classes([\"person\", \"phone\"])\nmodel.predict(\"image.png\")\nmodel.set_classes([\"human\", \"car\"])\nmodel.predict(\"image.png\")\n    \nprint(\"Текущие классы:\", model.names)\n```\n\nBut I get exception \"Inference tensors do not track version counter\":\n```\nTraceback (most recent call last):\n  File \"/home/user/test_file.py\", line 13, in <module>\n    model.set_classes([\"human\", \"car\"])\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n  File \"/home/user/.local/lib/python3.13/site-packages/ultralytics/models/yolo/model.py\", line 186, in set_classes\n    self.model.set_classes(classes)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/home/user/.local/lib/python3.13/site-packages/ultralytics/nn/tasks.py\", line 877, in set_classes\n    self.txt_feats = self.get_text_pe(text, batch=batch, cache_clip_model=cache_clip_model)\n                     ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/.local/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/user/.local/lib/python3.13/site-packages/ultralytics/nn/tasks.py\", line 901, in get_text_pe\n    txt_feats = [model.encode_text(token).detach() for token in text_token.split(batch)]\n                 ~~~~~~~~~~~~~~~~~^^^^^^^\n  File \"/home/user/.local/lib/python3.13/site-packages/ultralytics/nn/text_model.py\", line 132, in encode_text\n    txt_feats = self.model.encode_text(texts).to(dtype)\n                ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n  File \"/home/user/.local/lib/python3.13/site-packages/clip/model.py\", line 422, in encode_text\n    x = self.transformer(x)\n  File \"/home/user/.local/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/user/.local/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/user/.local/lib/python3.13/site-packages/clip/model.py\", line 252, in forward\n    return self.resblocks(x)\n           ~~~~~~~~~~~~~~^^^\n  File \"/home/user/.local/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/user/.local/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/user/.local/lib/python3.13/site-packages/torch/nn/modules/container.py\", line 240, in forward\n    input = module(input)\n  File \"/home/user/.local/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/user/.local/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/user/.local/lib/python3.13/site-packages/clip/model.py\", line 235, in forward\n    x = x + self.attention(self.ln_1(x))\n            ~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/home/user/.local/lib/python3.13/site-packages/clip/model.py\", line 231, in attention\n    return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n           ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/.local/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/user/.local/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/user/.local/lib/python3.13/site-packages/torch/nn/modules/activation.py\", line 1373, in forward\n    attn_output, attn_output_weights = F.multi_head_attention_forward(\n                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        query,\n        ^^^^^^\n    ...<17 lines>...\n        is_causal=is_causal,\n        ^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/artem/.local/lib/python3.13/site-packages/torch/nn/functional.py\", line 6230, in multi_head_attention_forward\n    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n              ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/artem/.local/lib/python3.13/site-packages/torch/nn/functional.py\", line 5614, in _in_projection_packed\n    proj = linear(q, w, b)\nRuntimeError: Inference tensors do not track version counter.\n```\n\n### Additional\n\npytorch: 2.7.0+cu126\nultralytics:  8.3.146",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @ArtemShaputko, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response 🤖. An Ultralytics engineer will review your issue and assist you as soon as possible. Thank you for your patience!"
      },
      {
        "user": "Y-T-G",
        "body": "You can try\n\n`model.model.set_classes([\"person\", \"phone\"], cache_clip_model=False)`\n\nto set classes"
      },
      {
        "user": "furkanmumcu",
        "body": "Even though setting `cache_clip_model=False` solves the problem, it causes the CLIP model to be initialized on every call. This significantly slows things down when working with multiple images and varying labels. There should be a better way to fix this."
      }
    ]
  },
  {
    "issue_number": 21007,
    "title": "Inconsistencies between the confusion matrix of the training output and the output of the terminal",
    "author": "jiandaoni",
    "state": "open",
    "created_at": "2025-06-10T05:54:27Z",
    "updated_at": "2025-06-14T10:23:37Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nComparing the two graphs, it can be seen that in the confusion matrix, the sensitivity (numerically equal to the recall) of the fire and smoke categories is 0.65 and 0.50, respectively, while the recall of the terminal output is 0.635 and 0.497, respectively. How can I make the output of both the same\n![Image](https://github.com/user-attachments/assets/db85aec1-e0a0-4407-aba6-7fb945bf0cf7)\n\n![Image](https://github.com/user-attachments/assets/ac82777f-1651-4ac4-abad-cba80359b318)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @jiandaoni, thank you for bringing this to our attention and for your interest in Ultralytics 🚀! This is an automated response to help guide you while an Ultralytics engineer reviews your issue and will assist you further soon.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) that demonstrates the inconsistencies you observed. This will help us investigate and debug the issue more efficiently.\n\nFor more information on usage and common questions, feel free to check out the [Docs](https://docs.ultralytics.com/), including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) guides.\n\nJoin the Ultralytics community wherever you prefer:\n- For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussions, check out [Discourse](https://community.ultralytics.com/)\n- For community threads, join our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease ensure you are running the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) installed in a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "Y-T-G",
        "body": "You can check this\n\nhttps://github.com/ultralytics/ultralytics/issues/17047#issuecomment-2424660435"
      },
      {
        "user": "jiandaoni",
        "body": "Thank you for the recovery, in fact it is"
      }
    ]
  },
  {
    "issue_number": 21044,
    "title": "Extremely Slow Inference Time With YOLOv11 & YOLOv8 Nano Models On Coral Edge TPU",
    "author": "AlecLozano",
    "state": "open",
    "created_at": "2025-06-13T02:18:52Z",
    "updated_at": "2025-06-14T09:01:46Z",
    "labels": [
      "bug",
      "detect",
      "embedded"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nPredict\n\n### Bug\n\nI'm trying to run a YOLOv11n model that I trained on my raspberry pi that has a coral edge tpu connected to it. The model detects boxes perfectly, however the performance is abysmal! The inference time for the model on my pi is about 1800-2100 ms!\n\nI triple checked if I exported my model in the 'edgetpu' format and even went back to train and export a YOLOv8n model to see if it was just a YOLOv11n problem, all with the same slow inference time.\n\n![Image](https://github.com/user-attachments/assets/30d5b1cd-53b9-4736-9de4-d50fb302932d)\n\nI have followed [ultralytics tutorial for edge tpu on raspberry pi](https://docs.ultralytics.com/guides/coral-edge-tpu-on-raspberry-pi/#how-do-i-run-inference-with-an-exported-yolo11-model-on-a-raspberry-pi-using-the-coral-edge-tpu) multiple times and have exported my models in 'edgetpu' format on google colab.\n\nI even used [googles tflite library](https://ai.google.dev/edge/api/tflite/python/tf/lite/Interpreter) to see if I could use the TPU, however I still get the 2 second inference time.\n\nPlease help me! I plan to use this for my robotics project and lost way too much sleep over this  problem\n\n### Environment\n\n[](\n\n![Image](https://github.com/user-attachments/assets/04528368-1ee4-4022-b79a-dc32201ce267)\n\n)\n\n### Minimal Reproducible Example\n\nThis is the code I use to test my model\n\n[](\n\n![Image](https://github.com/user-attachments/assets/6cca789c-9e01-421b-ad66-cf31e8cf73e4)\n\n)\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @AlecLozano, thank you for reporting this and for your detailed description! This is an automated response to acknowledge your issue and provide some helpful next steps while an Ultralytics engineer reviews your report soon 🛠️.\n\nWe recommend checking the [Docs](https://docs.ultralytics.com/) for guidance, especially the [Coral Edge TPU on Raspberry Pi guide](https://docs.ultralytics.com/guides/coral-edge-tpu-on-raspberry-pi/#how-do-i-run-inference-with-an-exported-yolo11-model-on-a-raspberry-pi-using-the-coral-edge-tpu). For additional reference, you might find [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples useful.\n\nSince this is a 🐛 Bug Report, please ensure you have provided a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/). If your code and environment are already included, thank you! Otherwise, providing a concise script and details about your setup (hardware, OS, model export steps, and inference code) will help us investigate more efficiently.\n\nYou’re welcome to join our community for more interactive support:\n- [Discord](https://discord.com/invite/ultralytics) for real-time chat 🎧\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with others\n\n## Upgrade\n\nPlease ensure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in various verified environments (with dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience while our team looks into your report!"
      },
      {
        "user": "glenn-jocher",
        "body": "The inference times you're experiencing (1800-2100ms) are definitely not normal for Edge TPU - you should be seeing around 22-32ms for YOLOv8n at 320px resolution based on our [benchmarks](https://docs.ultralytics.com/guides/coral-edge-tpu-on-raspberry-pi/#benchmarks).\n\nThis suggests the Edge TPU is not being utilized. Can you run `lsusb` to verify the Coral device is detected and check if your exported model filename ends with `_edgetpu.tflite`? Also, try running `python3 -c \"import tflite_runtime.interpreter as tflite; print(tflite.load_delegate('libedgetpu.so.1'))\"` to verify the Edge TPU runtime is properly installed.\n\nIf the TPU is detected but still slow, the issue is likely with the model export - ensure you're exporting with `format=\"edgetpu\"` on a non-ARM platform and that the resulting model file has the correct `_edgetpu.tflite` suffix."
      },
      {
        "user": "AlecLozano",
        "body": "This is the output when I enter `lsusb`, the TPu is the `Global Unichip Corp`\n\n![Image](https://github.com/user-attachments/assets/1ce2d919-e69f-4bc5-bb0c-e39053708d9e)\n\nThis is the output of the model when I enter `python3 -c \"import tflite_runtime.interpreter as tflite; print(tflite.load_delegate('libedgetpu.so.1'))\"`\n\n![Image](https://github.com/user-attachments/assets/a09715e0-95c8-40c1-85ad-f4d84c50c0c8)\n\nI have exported my model on google colab with the following code\n\n`from ultralytics import YOLO`\n\n`# Loaded my trained model`\n`model = YOLO(\"/content/best.pt\")`\n\n`# Exported my model`\n`model.export(format='edgetpu', task=\"detect\", imgsz=640)`\n\nHere is my exported model\n\n![Image](https://github.com/user-attachments/assets/d11bc150-51e4-4011-8dd8-15d4d22d56e1)"
      }
    ]
  },
  {
    "issue_number": 6240,
    "title": "Speed up inference speeds on Raspberry pi 5 8gb",
    "author": "pythonstuff8",
    "state": "closed",
    "created_at": "2023-11-10T00:49:37Z",
    "updated_at": "2025-06-14T05:21:11Z",
    "labels": [
      "question",
      "fixed"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi, I have a raspberry pi 8gb ram, overclocked at 3GHZ CPU and 900MHZ GPU. When I run my program on inference on the raspberry pi 5 i am getting a stable 310ms. Is there any way for me to make it faster or use other machine learning model to train on a custom dataset?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "pythonstuff8",
        "body": "Also the model I am using is int8 quantized, plus how do I use multithreading to make this faster?"
      },
      {
        "user": "pythonstuff8",
        "body": "plus using tflite format"
      },
      {
        "user": "glenn-jocher",
        "body": "@pythonstuff8 hello, and thank you for reaching out concerning inference performance on a Raspberry Pi.\n\nThe inference speed you are getting with an int8 quantized model on a Raspberry Pi is already quite good. However, there are a few strategies you might consider to further speed up inference:\n\n1. **Model Pruning**: This process involves removing less important connections in the neural network, which can reduce the computational load without significantly sacrificing accuracy.\n\n2. **Smaller Model**: If you are not already using one of the smaller YOLOv8 models designed for edge devices, consider doing so. These models strike a balance between speed and accuracy that may be optimal for your device.\n\n3. **Core Utilization**: Ensure that you are utilizing all available cores on your Raspberry Pi. YOLOv8's inference can utilize multiple threads to parallelize batch processing. Check the number of workers specified in your dataloader and adjust it to the number of CPU cores available in your Raspberry Pi when executing the predict function.\n\n4. **Optimized TFLite Model**: You mentioned using TFLite, which is a great solution for edge devices. Make sure you are using the latest version of TFLite and that your model has been converted and optimized properly for your Raspberry Pi's CPU architecture.\n\n5. **Disable Logging**: Verbose output can slow down inference. Disabling logs or running in a quieter mode may speed up the process slightly.\n\n6. **Power & Thermal Management**: Overclocking can lead to thermal throttling if the device overheats. Make sure your Raspberry Pi is adequately cooled to maintain the increased clock speeds.\n\n7. **Custom Inference Engine**: Depending on your device’s hardware, you may achieve better performance using an inference engine optimized for your specific hardware, such as one leveraging the hardware acceleration available on the Raspberry Pi.\n\nRegarding multi-threading, many deep learning frameworks can automatically take advantage of multiple cores for inference without explicitly programming multi-threading. However, if the framework or model you are using does not do this, using a multi-threaded application would involve running multiple inference jobs in parallel across different threads. Care must be taken with this approach to ensure that the shared resources are managed correctly and that the threads don't interfere with each other's operations.\n\nPlease refer to our documentation for further details on model optimization and to help guide you through these processes.\n\nRemember that improvements in speed may come at the cost of reduced accuracy or increased model size, so it's essential to find a balance that suits your application."
      }
    ]
  },
  {
    "issue_number": 21046,
    "title": "Yolo12 pretrain",
    "author": "czheyutao",
    "state": "open",
    "created_at": "2025-06-13T06:54:40Z",
    "updated_at": "2025-06-14T05:02:35Z",
    "labels": [
      "bug",
      "question",
      "wontfix",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI encountered a problem reproducing YOLO12 on the Coco dataset.Why do the effects obtained by val in these two situations not match.In addition, I was unable to find the training settings through model.ckpt in yolo12.pt\n\n![Image](https://github.com/user-attachments/assets/2a3be4b4-7f86-4d61-a104-00a8c3103846)\n\n![Image](https://github.com/user-attachments/assets/4a3d048e-fe51-4ea5-b280-e35da5a10950)\n\n![Image](https://github.com/user-attachments/assets/d838c048-79db-46a2-8521-234d42bdb437)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @czheyutao, thank you for your interest in Ultralytics and for trying out YOLO12 on the COCO dataset! 🚀 This is an automated response to help guide you while an Ultralytics engineer reviews your issue and will assist you soon.\n\nWe recommend visiting the [Docs](https://docs.ultralytics.com/) where you can find helpful [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples. Many common questions about training, validation, and pretrained weights are addressed there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug the issue. This should include the exact commands used, the dataset configuration, and any environment details that could help us reproduce your results.\n\nIf this is a custom training ❓ Question, please include as much information as possible—training logs, dataset samples, and verify you're following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nFor real-time community chat, join us on [Discord](https://discord.com/invite/ultralytics) 🎧. For in-depth discussions, visit [Discourse](https://community.ultralytics.com/), or connect on the [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nEnsure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date, verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for providing any additional details requested above. An Ultralytics engineer will be with you soon!"
      },
      {
        "user": "Y-T-G",
        "body": "How did you get the first one?"
      },
      {
        "user": "czheyutao",
        "body": "> How did you get the first one?\n\nSorry for the vague expression.I use the code below to get the first result\n\n![Image](https://github.com/user-attachments/assets/85cb8353-fcf6-45e4-a4d1-e79c233fc792)\n\n"
      }
    ]
  },
  {
    "issue_number": 20964,
    "title": "Intel Xpu training support",
    "author": "huahuadeliaoliao",
    "state": "open",
    "created_at": "2025-06-06T07:49:41Z",
    "updated_at": "2025-06-14T01:50:37Z",
    "labels": [
      "enhancement",
      "dependencies"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nThe device parameter can be used directly in train to use xpu, the batch parameter can be filled in as -1 for automatic adjustment, amp and plots can be enabled normally.\n\n### Use case\n\n_No response_\n\n### Additional\n\nCurrently, training can be correctly performed on an XPU by specifying device as `torch.device(\"xpu\")`. However, `AMP` and `plots` must be set to False because they are currently hardcoded to use the CUDA device and will not utilize the XPU. It's worth noting that PyTorch does support AMP on XPU. Additionally, the `batch` parameter cannot be set to `-1`, as this will cause it to call the CUDA device.\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @huahuadeliaoliao, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response to let you know your feature request for Intel XPU support has been received. An Ultralytics engineer will review and assist you soon. Thank you for your willingness to contribute! 🛠️"
      },
      {
        "user": "glenn-jocher",
        "body": "Thanks for the detailed analysis! You're absolutely right about the XPU support limitations. The main areas that need updates for full XPU support are:\n\n1. The [`autocast` function](https://docs.ultralytics.com/reference/utils/torch_utils/#ultralytics.utils.torch_utils.autocast) in `torch_utils.py` currently only handles CUDA and CPU devices\n2. The [`check_amp` function](https://docs.ultralytics.com/reference/utils/checks/#ultralytics.utils.checks.check_amp) returns `False` for non-CUDA devices\n3. The [`select_device` function](https://docs.ultralytics.com/reference/utils/torch_utils/#ultralytics.utils.torch_utils.select_device) needs XPU device handling for auto-batch functionality\n\nYour PR would be very welcome! Please focus on extending these utility functions to properly detect and handle XPU devices, especially for AMP support since PyTorch does indeed support AMP on XPU. Feel free to reference how MPS device support was implemented as a similar pattern."
      },
      {
        "user": "azhuvath",
        "body": "Even I was trying to do something like this, but lot of branches of code for different devices.\n\n(yolo_env) ubuntu@sys-intel-gpu:~/tvc$ python tvcd.py \nUsing device: xpu\nDevice count: <functools._lru_cache_wrapper object at 0x7f42da19d430>\n\n🚦 Starting YOLOv8 training...\nUltralytics 8.3.152 🚀 Python-3.10.12 torch-2.7.1+xpu \nTraceback (most recent call last):\n  File \"/home/ubuntu/tvc/tvcd.py\", line 141, in <module>\n    main()\n  File \"/home/ubuntu/tvc/tvcd.py\", line 120, in main\n    model = train_yolov8_model()\n  File \"/home/ubuntu/tvc/tvcd.py\", line 29, in train_yolov8_model\n    model.train(\n  File \"/home/ubuntu/tvc/yolo_env/lib/python3.10/site-packages/ultralytics/engine/model.py\", line 791, in train\n    self.trainer = (trainer or self._smart_load(\"trainer\"))(overrides=args, _callbacks=self.callbacks)\n  File \"/home/ubuntu/tvc/yolo_env/lib/python3.10/site-packages/ultralytics/engine/trainer.py\", line 121, in __init__\n    self.device = select_device(self.args.device, self.args.batch)\n  File \"/home/ubuntu/tvc/yolo_env/lib/python3.10/site-packages/ultralytics/utils/torch_utils.py\", line 201, in select_device\n    raise ValueError(\nValueError: Invalid CUDA 'device=xpu' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: None\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch."
      }
    ]
  },
  {
    "issue_number": 20878,
    "title": "Why Does YOLOv8n Only Reach 0.361 mAP After 300 Epochs (vs. Official 37.3)",
    "author": "BoringCheese",
    "state": "open",
    "created_at": "2025-05-30T11:35:50Z",
    "updated_at": "2025-06-14T01:28:35Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI'm currently writing a paper on object detection, and I've noticed that many papers and implementations train for 300 epochs.\nI'm using YOLOv8n and trained it for 300 epochs on the COCO dataset with the default settings. However, my final result is:\nmAP@0.5:0.95 = 0.361,\nwhich is a bit lower than the official result of 37.3 reported by Ultralytics.\n\nMy question is:\nIs it expected that YOLOv8n cannot reach 37.3 mAP after 300 epochs with default settings?\n\nOr is it more likely that I have some parameter settings incorrect (e.g., data augmentation, learning rate, batch size, EMA, etc.)?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @BoringCheese, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response. An Ultralytics engineer will also assist you here soon! 👨‍💻"
      },
      {
        "user": "Y-T-G",
        "body": "You can check this\n\nhttps://github.com/ultralytics/ultralytics/issues/17048#issuecomment-2424742030"
      },
      {
        "user": "BoringCheese",
        "body": "I think I’ve found the cause — it was likely due to incorrect parameter settings.  \nInstead of relying on the default parameters, I realized I should retrieve the actual training arguments used by the official model using:\n\n```python\nmodel = YOLO(\"./yolov8n.pt\")  \nprint(model.ckpt[\"train_args\"])\n```\n\nI'm now re-running the training using those parameters, and I’ll share the updated results once I have them.\n"
      }
    ]
  },
  {
    "issue_number": 21054,
    "title": "YOLO Detection Loses Track Mid-Video — Need Help Integrating Tracker to Maintain Consistent sheep ID",
    "author": "RAM1119",
    "state": "open",
    "created_at": "2025-06-13T16:01:37Z",
    "updated_at": "2025-06-13T18:25:35Z",
    "labels": [
      "question",
      "track",
      "detect"
    ],
    "body": "Body:\n\nHi team,\n\nI'm currently using a YOLO model trained on sheep back-view images to detect sheeps in video frames. The setup includes a utility that identifies railings and defines a \"30% gate area\" on the frame. My goal is to stop detection once the sheep crosses out of this gate area and rely on tracking instead.\nProblem\n\nDuring detection:\n\n    YOLO occasionally loses the sheep bounding box for a few frames.\n\n    When it re-detects the sheep, it treats it as a new object, assigning a new unique ID.\n\n    This leads to the same sheep being tracked with different IDs, which breaks consistency.\n\n What I'm Trying to Do\n\nI want to switch from detection to tracking once the sheep exits the 30% gate area:\n\n    Capture the last YOLO bbox for the sheep inside the gate.\n\n    Initialize a tracker (e.g., SORT, Deep SORT, ByteTrack) with this bbox.\n\n    Continue tracking the sheep until it leaves the frame, without assigning a new ID, even if YOLO misses it momentarily.\n\n Where I Need Help\n\n    How can I initialize the tracker properly with the last YOLO detection once the sheep exits the gate?\n\n    How do I ensure that YOLO doesn’t trigger a new tracker ID when detection resumes?\n\n    How to link the tracker to YOLO detections smoothly without duplication?\n\nI'd appreciate any suggestions, code examples, or best practices for implementing this tracking transition effectively.\n\nThank you!\n\n`# YOLO sheep Detection Only\n# Detects sheeps in 30% gate area - NO MIL tracking\n# \"\"\"\n\nimport cv2\nimport numpy as np\nimport torch\nfrom ultralytics import YOLO\nimport time\nfrom pathlib import Path\n\n# Import utility functions\nfrom sheep_tracking_utils import (\n    setup_gate_areas, create_gate_mask, is_detection_in_gate,\n    draw_gate_area, validate_file_paths, get_video_properties\n)\n\n# ========================== CONFIGURATION ==========================\nMODEL_PATH = \"sairam_sheep_detection_model.pt\"\nVIDEO_PATH = \"sheep_test.mp4\"\nOUTPUT_VIDEO_PATH = \"yolo_detection_only_output.mp4\"\n\n# Detection settings\nCONFIDENCE_THRESHOLD = 0.3\nMAX_MISSING_FRAMES = 25  # If sheep missing for 25 frames, consider it new sheep\n\n# Display and save settings\nSHOW_VIDEO = True\nSAVE_OUTPUT_VIDEO = True\n# ===================================================================\n\nclass sheepTracker:\n    def __init__(self):\n        self.sheep_id = 1\n        self.last_seen_frame = 0\n        self.frame_data = []\n        self.missing_frames = 0\n        self.is_active = True\n    \n    def update(self, detection, frame_number):\n        \"\"\"Update sheep tracker with new detection\"\"\"\n        self.last_seen_frame = frame_number\n        self.missing_frames = 0\n        self.frame_data.append(detection)\n    \n    def increment_missing(self):\n        \"\"\"Increment missing frame counter\"\"\"\n        self.missing_frames += 1\n        if self.missing_frames >= MAX_MISSING_FRAMES:\n            self.is_active = False\n\nclass YOLOsheepDetector:\n    def __init__(self, model_path, confidence_threshold=0.3):\n        \"\"\"Initialize YOLO sheep detector only\"\"\"\n        \n        # Load YOLO model\n        self.model = YOLO(model_path)\n        self.conf_threshold = confidence_threshold\n        \n        # Gate area\n        self.gate_area = None\n        self.gate_mask = None\n        \n        # sheep tracking\n        self.sheep_trackers = {}  # {sheep_id: sheepTracker}\n        self.next_sheep_id = 1\n        self.frame_count = 0\n        \n        # Statistics\n        self.total_detections_in_gate = 0\n        \n        print(f\"🎯 YOLO sheep Detector Only\")\n        print(f\"🎯 Confidence threshold: {self.conf_threshold}\")\n        print(f\"📊 Max missing frames: {MAX_MISSING_FRAMES}\")\n    \n    def setup_gate_area_for_video(self, video_path):\n        \"\"\"Setup 30% gate area\"\"\"\n        try:\n            original_gate_area, small_gate_area, gate_mask = setup_gate_areas(video_path)\n            \n            # Handle different data types for gate area\n            print(f\"🔍 Gate area type: {type(small_gate_area)}\")\n            print(f\"🔍 Gate area shape: {small_gate_area.shape if hasattr(small_gate_area, 'shape') else 'no shape'}\")\n            print(f\"🔍 Gate area value: {small_gate_area}\")\n            \n            if isinstance(small_gate_area, np.ndarray):\n                if small_gate_area.shape == (4, 2):\n                    # This is a 4-point polygon, convert to bounding box (x1, y1, x2, y2)\n                    x_coords = small_gate_area[:, 0]\n                    y_coords = small_gate_area[:, 1]\n                    x1, y1 = int(np.min(x_coords)), int(np.min(y_coords))\n                    x2, y2 = int(np.max(x_coords)), int(np.max(y_coords))\n                    self.gate_area = [x1, y1, x2, y2]\n                elif small_gate_area.shape == (4,):\n                    # This is already [x1, y1, x2, y2] format\n                    self.gate_area = [int(coord) for coord in small_gate_area.tolist()]\n                else:\n                    # Flatten and take first 4 elements\n                    flat_area = small_gate_area.flatten()\n                    self.gate_area = [int(coord) for coord in flat_area[:4]]\n            elif isinstance(small_gate_area, (list, tuple)):\n                # Already a list or tuple\n                if len(small_gate_area) >= 4:\n                    self.gate_area = [int(coord) for coord in small_gate_area[:4]]\n                else:\n                    print(f\"⚠️ Gate area has insufficient coordinates: {len(small_gate_area)}\")\n                    self.gate_area = small_gate_area\n            else:\n                print(f\"⚠️ Unexpected gate_area type: {type(small_gate_area)}\")\n                self.gate_area = small_gate_area\n                \n            print(\"🚪 Gate area set to fixed 30% size\")\n            print(f\"📦 Gate area coordinates (x1, y1, x2, y2): {self.gate_area}\")\n        except Exception as e:\n            print(f\"❌ Error setting up gate areas: {e}\")\n            raise\n    \n    def calculate_distance(self, box1, box2):\n        \"\"\"Calculate distance between two bounding boxes\"\"\"\n        center1_x = (box1[0] + box1[2]) / 2\n        center1_y = (box1[1] + box1[3]) / 2\n        center2_x = (box2[0] + box2[2]) / 2\n        center2_y = (box2[1] + box2[3]) / 2\n        \n        return np.sqrt((center1_x - center2_x)**2 + (center1_y - center2_y)**2)\n    \n    def find_closest_sheep(self, detection_box):\n        \"\"\"Find closest active sheep\"\"\"\n        min_distance = float('inf')\n        closest_sheep_id = None\n        \n        for sheep_id, tracker in self.sheep_trackers.items():\n            if tracker.is_active and tracker.frame_data:\n                last_detection = tracker.frame_data[-1]\n                last_box = [last_detection['bbox_x1'], last_detection['bbox_y1'], \n                           last_detection['bbox_x2'], last_detection['bbox_y2']]\n                \n                distance = self.calculate_distance(detection_box, last_box)\n                max_distance = 100\n                \n                if distance < min_distance and distance < max_distance:\n                    min_distance = distance\n                    closest_sheep_id = sheep_id\n        \n        return closest_sheep_id\n    \n    def assign_detection_to_sheep(self, detection, frame_number):\n        \"\"\"Assign detection to sheep tracker\"\"\"\n        detection_box = [detection['bbox_x1'], detection['bbox_y1'], \n                        detection['bbox_x2'], detection['bbox_y2']]\n        \n        closest_sheep_id = self.find_closest_sheep(detection_box)\n        \n        if closest_sheep_id is not None:\n            # Update existing sheep\n            self.sheep_trackers[closest_sheep_id].update(detection, frame_number)\n            detection['sheep_id'] = closest_sheep_id\n            print(f\"🔄 Updated sheep_{closest_sheep_id} at frame {frame_number}\")\n        else:\n            # Create new sheep\n            new_sheep = sheepTracker()\n            new_sheep.update(detection, frame_number)\n            self.sheep_trackers[self.next_sheep_id] = new_sheep\n            detection['sheep_id'] = self.next_sheep_id\n            print(f\"🆕 Created sheep_{self.next_sheep_id} at frame {frame_number}\")\n            self.next_sheep_id += 1\n        \n        return detection['sheep_id']\n    \n    def update_missing_sheeps(self, frame_number):\n        \"\"\"Update missing frame counters\"\"\"\n        detected_sheeps = set()\n        \n        for sheep_id, tracker in self.sheep_trackers.items():\n            if tracker.last_seen_frame == frame_number:\n                detected_sheeps.add(sheep_id)\n        \n        for sheep_id, tracker in self.sheep_trackers.items():\n            if sheep_id not in detected_sheeps and tracker.is_active:\n                tracker.increment_missing()\n                if not tracker.is_active:\n                    print(f\"❌ Lost sheep_{sheep_id} after {MAX_MISSING_FRAMES} missing frames\")\n    \n    def detect_in_frame(self, frame, frame_number):\n        \"\"\"Detect sheeps in gate area only\"\"\"\n        frame_detections = []\n        \n        # Run YOLO detection\n        results = self.model(frame, conf=self.conf_threshold, verbose=False)\n        \n        if results and results[0].boxes is not None and len(results[0].boxes) > 0:\n            boxes = results[0].boxes.xyxy.cpu().numpy()\n            confidences = results[0].boxes.conf.cpu().numpy()\n            \n            for i, (box, conf) in enumerate(zip(boxes, confidences)):\n                x1, y1, x2, y2 = box\n                \n                # Check if detection is in gate area\n                if is_detection_in_gate(box.tolist(), self.gate_mask):\n                    self.total_detections_in_gate += 1\n                    \n                    detection_info = {\n                        'frame_number': frame_number,\n                        'confidence': float(conf),\n                        'bbox_x1': float(x1),\n                        'bbox_y1': float(y1),\n                        'bbox_x2': float(x2),\n                        'bbox_y2': float(y2),\n                        'bbox_width': float(x2 - x1),\n                        'bbox_height': float(y2 - y1),\n                        'center_x': float((x1 + x2) / 2),\n                        'center_y': float((y1 + y2) / 2),\n                        'area': float((x2 - x1) * (y2 - y1)),\n                        'in_gate_area': True\n                    }\n                    \n                    # Assign to sheep tracker\n                    sheep_id = self.assign_detection_to_sheep(detection_info, frame_number)\n                    frame_detections.append(detection_info)\n                    \n                    print(f\"🎯 Frame {frame_number}: sheep_{sheep_id} detected in gate\")\n        \n        # Update missing sheep counters\n        self.update_missing_sheeps(frame_number)\n        \n        return frame_detections\n    \n    def draw_detections_and_gate(self, frame, detections):\n        \"\"\"Draw gate area and YOLO detection boxes\"\"\"\n        annotated_frame = frame.copy()\n        \n        # Draw 30% gate area (always visible)\n        if self.gate_area is not None:\n            try:\n                # Ensure gate_area is in correct format and convert to integers\n                if len(self.gate_area) >= 4:\n                    x1, y1, x2, y2 = [int(coord) for coord in self.gate_area[:4]]\n                else:\n                    print(f\"⚠️ Invalid gate_area format: {self.gate_area}\")\n                    return annotated_frame\n                \n                gate_color = (0, 255, 0) if len(detections) > 0 else (128, 128, 128)\n                \n                # Draw gate area rectangle\n                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), gate_color, 3)\n                \n                # Draw gate area label\n                label = \"30% GATE AREA\"\n                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n                cv2.rectangle(annotated_frame, (x1, y1 - 35), (x1 + label_size[0] + 10, y1 - 5), gate_color, -1)\n                cv2.putText(annotated_frame, label, (x1 + 5, y1 - 15), \n                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n            except Exception as e:\n                print(f\"❌ Error drawing gate area: {e}\")\n                print(f\"Gate area: {self.gate_area}\")\n        \n        # Draw YOLO detection boxes (only in gate area)\n        for i, detection in enumerate(detections):\n            if detection['in_gate_area']:\n                x1 = int(detection['bbox_x1'])\n                y1 = int(detection['bbox_y1'])\n                x2 = int(detection['bbox_x2'])\n                y2 = int(detection['bbox_y2'])\n                conf = detection['confidence']\n                sheep_id = detection.get('sheep_id', 'Unknown')\n                \n                # Green color for YOLO detections\n                detection_color = (0, 255, 0)\n                \n                # Draw YOLO detection box\n                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), detection_color, 2)\n                \n                # Draw detection label with sheep number\n                label = f\"sheep_{sheep_id} ({conf:.2f})\"\n                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n                cv2.rectangle(annotated_frame, (x1, y1 - 25), (x1 + label_size[0] + 10, y1 - 5), detection_color, -1)\n                cv2.putText(annotated_frame, label, (x1 + 5, y1 - 10), \n                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n        \n        return annotated_frame\n    \n    def process_frame(self, frame, frame_number):\n        \"\"\"Process single frame: detect only\"\"\"\n        \n        # YOLO detection only\n        detections = self.detect_in_frame(frame, frame_number)\n        \n        # Draw gate area and YOLO detections\n        result_frame = self.draw_detections_and_gate(frame, detections)\n        \n        return result_frame\n    \n    def get_sheep_dictionary(self):\n        \"\"\"Generate sheep dictionary\"\"\"\n        sheep_dict = {}\n        for sheep_id, tracker in self.sheep_trackers.items():\n            if tracker.frame_data:\n                sheep_dict[f\"sheep{sheep_id}\"] = tracker.frame_data\n        return sheep_dict\n    \n    def process_video(self, video_path, output_path=None, show_video=True):\n        \"\"\"Process entire video\"\"\"\n        \n        # Setup\n        self.setup_gate_area_for_video(video_path)\n        \n        video_props = get_video_properties(video_path)\n        fps = video_props['fps']\n        width = video_props['width']\n        height = video_props['height']\n        total_frames = video_props['total_frames']\n        \n        # Create gate mask manually since we have bounding box coordinates\n        print(f\"🔧 Creating gate mask for area: {self.gate_area}\")\n        if self.gate_area and len(self.gate_area) >= 4:\n            x1, y1, x2, y2 = self.gate_area[:4]\n            self.gate_mask = np.zeros((height, width), dtype=np.uint8)\n            # Ensure coordinates are within bounds\n            x1 = max(0, min(x1, width-1))\n            y1 = max(0, min(y1, height-1))\n            x2 = max(0, min(x2, width-1))\n            y2 = max(0, min(y2, height-1))\n            self.gate_mask[y1:y2, x1:x2] = 255\n            print(f\"✅ Gate mask created successfully\")\n        else:\n            print(f\"❌ Invalid gate area for mask creation: {self.gate_area}\")\n            self.gate_mask = None\n        \n        print(f\"📹 Video: {width}x{height}, {fps} FPS, {total_frames} frames\")\n        \n        # Open video\n        cap = cv2.VideoCapture(video_path)\n        \n        # Video writer\n        out = None\n        if output_path:\n            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n        \n        # Create window\n        if show_video:\n            cv2.namedWindow('YOLO sheep Detection Only', cv2.WINDOW_AUTOSIZE)\n        \n        print(\"🚀 Starting YOLO detection only...\")\n        \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            self.frame_count += 1\n            \n            # Process frame\n            result_frame = self.process_frame(frame, self.frame_count)\n            \n            # Show\n            if show_video:\n                cv2.imshow('YOLO sheep Detection Only', result_frame)\n                if cv2.waitKey(1) & 0xFF == ord('q'):\n                    break\n            \n            # Save\n            if out:\n                out.write(result_frame)\n            \n            # Progress\n            if self.frame_count % 30 == 0:\n                progress = (self.frame_count / total_frames) * 100\n                print(f\"📊 Progress: {progress:.1f}% ({self.frame_count}/{total_frames})\")\n        \n        # Cleanup\n        cap.release()\n        if out:\n            out.release()\n        if show_video:\n            cv2.destroyAllWindows()\n        \n        return self.get_sheep_dictionary()\n\ndef main():\n    \"\"\"Main function\"\"\"\n    print(\"🎯 YOLO sheep DETECTION ONLY\")\n    print(\"=\" * 50)\n    print(\"🔍 YOLO Detection in 30% Gate Area\")\n    print(\"🐄 No MIL tracking - just detection\")\n    print(\"📦 Green boxes for detections\")\n    print(\"=\" * 50)\n    \n    # Validate inputs\n    if not validate_file_paths(MODEL_PATH, VIDEO_PATH):\n        return\n    \n    # Initialize detector\n    detector = YOLOsheepDetector(\n        model_path=MODEL_PATH,\n        confidence_threshold=CONFIDENCE_THRESHOLD\n    )\n    \n    # Process video\n    try:\n        sheep_dict = detector.process_video(\n            video_path=VIDEO_PATH,\n            output_path=OUTPUT_VIDEO_PATH if SAVE_OUTPUT_VIDEO else None,\n            show_video=SHOW_VIDEO\n        )\n        \n        print(\"\\n✅ Processing completed!\")\n        print(f\"📊 Total sheeps tracked: {len(sheep_dict)}\")\n        \n        for sheep_name, detections in sheep_dict.items():\n            print(f\"🐄 {sheep_name}: {len(detections)} detections\")\n        \n        return sheep_dict\n        \n    except KeyboardInterrupt:\n        print(\"\\n🛑 Processing interrupted\")\n        return None\n    except Exception as e:\n        print(f\"❌ Error: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    sheep_dictionary = main()`",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @RAM1119, thank you for reaching out and for providing such a detailed description and code snippet! 🚀 This is an automated response to help get you started—an Ultralytics engineer will also assist you soon.\n\nWe recommend checking the [Ultralytics Docs](https://docs.ultralytics.com/)—especially the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage sections, which cover detection and tracking integrations and may have relevant examples for linking YOLO detection with trackers like SORT, Deep SORT, or ByteTrack.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) if you haven’t already. This helps us debug your issue efficiently.\n\nIf your question is about custom training or pipeline design, please include as much detail as possible—such as dataset samples, logs, and any experiment results—so our team can give the most effective advice. You may also want to review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community in your favorite space:\n- For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧\n- For deeper discussions, check out [Discourse](https://community.ultralytics.com/)\n- Or share your progress and questions on our [Subreddit](https://reddit.com/r/Ultralytics)!\n\n## Upgrade\n\nFirst, please ensure you’re using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these verified environments (with dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM ([GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/))\n- **Amazon** Deep Learning AMI ([AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/))\n- **Docker Image** ([Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience—our team will review your request and provide further guidance soon! 🐑"
      },
      {
        "user": "Y-T-G",
        "body": "> My goal is to stop detection once the sheep crosses out of this gate area and rely on tracking instead.\n\nThe tracker in Ultralytics requires detection. It can't track without detection.\n\nYou can change the tracker parameters to improve tracking.\n\nhttps://docs.ultralytics.com/modes/track/#tracker-arguments"
      }
    ]
  },
  {
    "issue_number": 20879,
    "title": "Incremental Learning with YOLOv8",
    "author": "Toninhooo",
    "state": "open",
    "created_at": "2025-05-30T11:50:12Z",
    "updated_at": "2025-06-13T18:02:01Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello, I have researched far and wide to try and resolve this issue that I am having. For context, I am a beginner, and I am trying to implement LwF with YOLOv8. The issue that I am having is trying to calculate the soft loss for the KD. I have applied the hard loss to the V8Detection class to get the results. As for the soft loss, I am trying to calculate each head of scale one by one (80,40,20), and comparing it with the teacher to try and get the most accurate results. I am doing this for both regression and classification. From what I have understood and researched, the shape that the student outputs in my case is [8, 70, 80, 80], [8, 70, 40, 40], and [8, 70, 20, 20]. As for my teacher, when doing teacher_output [0], the shape is [8,10,8400]. But to compare and not have shape mismatch, I did tecaher_output[1][0] = [8, 70, 80, 80],tecaher_output[1][1] = [8, 70, 40, 40], and tecaher_output[1][2] = [8, 70, 20, 20]. If I am not mistaken, the first 64 of the 70 are related to DFL, and the last 6 are for my classes. With all of this done the code runs well, shows improvement in hard loss and soft loss, but at the end I predict on an image just for testing and the student model does over 500 predictions, with confident score 1.0, and all incorrect. Can you help me in figuring this issue out. A snippet of my code is provided below, and if you would like any print statements, please let me know.\n\n```python\n# CODE:\n## Soft loss function\n    def compute_lwf_loss(student, teacher, alpha, temperature, conf_thresh=0.5):\n      # print(\"\\n=== Entered compute_lwf_loss ===\")\n      # print(f\"\\nStudent bbox shape: {student_bbox.shape}\")\n      # print(f\"Teacher bbox shape: {teacher_bbox.shape}\")\n      # print(f\"Student class shape: {student_class.shape}\")\n      # print(f\"Teacher class shape: {teacher_class.shape}\")\n    \n    \n      B, C, H, W = student.shape\n      student_logits_flat = student.permute(0, 2, 3, 1).reshape(-1, C)\n      teacher_logits_flat = teacher.permute(0, 2, 3, 1).reshape(-1, C)\n    \n      teacher_soft = F.softmax(teacher_logits_flat / temperature, dim=1)\n      student_soft = F.log_softmax(student_logits_flat / temperature, dim=1)\n    \n      soft_loss = F.kl_div(student_soft, teacher_soft, reduction=\"batchmean\") * (temperature**2 )\n      # print(\"---------------Leaving compute_lwf Function-----------------\\n\")\n    \n      return soft_loss\n\n## ---------- Training Function ----------\n      def train_with_lwf(student_model, teacher_model, dataloader, optimizer, device, alpha, temperature, epochs):\n        print(\"2. Starting training with LwF...\\n\")\n        # This ensures the student model is reconfigured for 6 classes\n    \n        student_model.model.train()\n        teacher_model.model.eval()\n        student_hard_loss_calculator = v8DetectionLoss(model=student_model.model.train())\n    \n        for epoch in range(epochs):\n            print(f\"\\n--- Epoch {epoch + 1}/{epochs} ---\")\n    \n            beta = 0.5\n            num_batches = 0\n            total_loss = 0.0\n            warmup_epochs = 3\n            hyp = SimpleNamespace(\n                box=7.5,\n                cls=0.5,\n                dfl=1.5\n            )\n            student_hard_loss_calculator.hyp = hyp\n            student_model.args = hyp\n    \n            for batch in tqdm(dataloader, desc=\"Training\"):\n    \n                images_tuple, targets_tuple = batch\n                if isinstance(images_tuple, tuple) or isinstance(images_tuple, list):\n                    images = torch.stack(images_tuple)  # shape: (B, 3, 640, 640)\n                else:\n                    raise ValueError(f\"Expected tuple/list of images but got {type(images_tuple)}\")\n    \n                images = images.float() / 255.0\n                images = images.float().to(device)\n                images.requires_grad = True\n                # print(f\"images shape: {images.shape}, dtype: {images.dtype}, device: {images.device}\")\n                # print(f\"images min/max: {images.min().item():.3f}/{images.max().item():.3f}\")\n                # print(f\"requires_grad: {images.requires_grad}\\n\\n\\n\")\n    \n                # print(f\"Images shape: {images.shape}\")\n                # for i, single_image_targets_dict in enumerate(targets_tuple):\n                #     labels_for_this_image = single_image_targets_dict['labels']\n                #     print(f\"Labels for image {i} shape: {labels_for_this_image.shape}\")\n                # print(f\"targets_tuple length: {len(targets_tuple)}\")\n                # print(f\"targets_tuple content types: {[type(t) for t in targets_tuple]}\")\n                # print(f\"targte_tuple: {targets_tuple}\")\n    \n                targets = merge_target_dicts(targets_tuple)\n                # print(targets)\n    \n                ### Confirms Merged target are correct#######\n                # print(\"Merged Targets:\")\n                # print(\" - batch_idx shape:\", targets[\"batch_idx\"].shape)\n                # print(\" - cls shape:\", targets[\"cls\"].shape)\n                # print(\" - bboxes shape:\", targets[\"bboxes\"].shape)\n                # print(\" - example bboxes:\", targets[\"bboxes\"][:5])\n                # print(\" - example classes:\", targets[\"cls\"][:5])\n                # print(targets)\n    \n                # Student forward pass\n                student_output = student_model.model(images)\n                # student_output.to(device)\n                student_output = [t.to(device) for t in student_output]\n                # for i, out in enumerate(student_output):\n                #     print(f\"student_output[{i}] shape: {out.shape}\")\n    \n                student_output_0 = student_output[0]\n                # print(student_output[0].shape)  # [B, 70, H, W]\n                # print(targets['cls'].shape, targets['bboxes'].shape, targets['batch_idx'].shape)\n    \n    \n                # These prints of the logits prove that the first 64 are bbox logits and last 6 are for classification logits\n    \n                students_boxes_logits_0 = student_output[0][:,:64,:,:]\n                students_class_logits_0 = student_output[0][:, 64:, :,:]\n                students_boxes_logits_1 = student_output[1][:,:64,:,:]\n                students_class_logits_1 = student_output[1][:, 64:, :,:]\n                students_boxes_logits_2 = student_output[2][:,:64,:,:]\n                students_class_logits_2 = student_output[2][:, 64:, :,:]\n    \n    \n                # print(f\"\\n\\nShape student_boxes[0]: {students_boxes_logits_0.shape}\")\n                # print(f\"Shape student classificaiton[0]: {students_class_logits_0.shape} \")\n                # print(f\"\\nShape student_boxes[1]: {students_boxes_logits_1.shape}\")\n                # print(f\"Shape student classificaiton[1]: {students_class_logits_1.shape} \")\n                # print(f\"\\nShape student_boxes[2]: {students_boxes_logits_2.shape}\")\n                # print(f\"Shape student classificaiton[2]: {students_class_logits_2.shape} \")\n    \n    \n                # Teacher forward pass (eval, no grad, no targets)\n                with torch.no_grad():\n                    teacher_output = teacher_model.model(images)\n    \n                # print(f\"\\n\\nteacher_output[0] shape: {teacher_output[0].shape}\")\n                # print(f\"teacher_output[1][0] shape: {teacher_output[1][0].shape}\")\n                # print(f\"teacher_output[1][1] shape: {teacher_output[1][1].shape}\")\n                # print(f\"teacher_output[1][2] shape: {teacher_output[1][2].shape}\")\n    \n                teacher_boxes_logits_0 = teacher_output[1][0][:,:64,:,:]\n                teacher_class_logits_0 = teacher_output[1][0][:, 64:, :,:]\n                teacher_boxes_logits_1 = teacher_output[1][1][:,:64,:,:]\n                teacher_class_logits_1 = teacher_output[1][1][:, 64:, :,:]\n                teacher_boxes_logits_2 = teacher_output[1][2][:,:64,:,:]\n                teacher_class_logits_2 = teacher_output[1][2][:, 64:, :,:]\n    \n                # print(f\"\\n\\nShape teacher_boxes[1][0]: {teacher_boxes_logits_0.shape}\")\n                # print(f\"Shape teacher classificaiton[1][0]: {teacher_class_logits_0.shape} \")\n                # print(f\"\\nShape teacher_boxes[1][1]: {teacher_boxes_logits_1.shape}\")\n                # print(f\"Shape teacher classificaiton[1][1]: {teacher_class_logits_1.shape} \")\n                # print(f\"\\nShape teacher_boxes[1][2]: {teacher_boxes_logits_2.shape}\")\n                # print(f\"Shape teacher classificaiton[1][2]: {teacher_class_logits_2.shape} \")\n    \n                if epochs < warmup_epochs:\n                    distill_class_loss_0 = 0.0\n                    distill_class_loss_1 = 0.0\n                    distill_class_loss_2 = 0.0\n                    distill_reg_loss_0 = 0.0\n                    distill_reg_loss_1 = 0.0\n                    distill_reg_loss_2 = 0.0\n                else:\n                    #Compute distillation loss\n                    distill_class_loss_0 = compute_lwf_loss(students_class_logits_0, teacher_class_logits_0, alpha=alpha, temperature=temperature)\n                    distill_class_loss_1 = compute_lwf_loss(students_class_logits_1, teacher_class_logits_1, alpha=alpha, temperature=temperature)\n                    distill_class_loss_2 = compute_lwf_loss(students_boxes_logits_2, teacher_boxes_logits_2, alpha=alpha, temperature=temperature)\n                    distill_reg_loss_0 = compute_lwf_loss(students_boxes_logits_0, teacher_boxes_logits_0, alpha=alpha, temperature=temperature) * 0.2\n                    distill_reg_loss_1 = compute_lwf_loss(students_boxes_logits_1, teacher_boxes_logits_1, alpha=alpha, temperature=temperature) * 0.5\n                    distill_reg_loss_2 = compute_lwf_loss(students_boxes_logits_2, teacher_boxes_logits_2, alpha=alpha, temperature=temperature)\n    \n                hard_loss, loss_detached = student_hard_loss_calculator(student_output, targets)\n                print(f\"\\nHard Loss Output: {hard_loss}\")\n                print(f\"Hard Loss Total: {hard_loss.sum().item():.4f}\")\n                print(f\" - Box: {hard_loss[0].item():.4f}\")\n                print(f\" - Cls: {hard_loss[1].item():.4f}\")\n                print(f\" - DFL: {hard_loss[2].item():.4f}\")\n                print(f\"(Detached) Cls Loss for logging: {loss_detached[1].item():.4f}\")\n    \n                print(\"\\nSoft losses\")\n                print(f\"distill_class_loss_0: {distill_class_loss_0:.4f}\")\n                print(f\"distill_class_loss_1: {distill_class_loss_1:.4f}\")\n                print(f\"distill_class_loss_2: {distill_class_loss_2:.4f}\")\n                print(f\"distill_reg_loss_0: {distill_reg_loss_0:.4f}\")\n                print(f\"distill_reg_loss_1: {distill_reg_loss_1:.4f}\")\n                print(f\"distill_reg_loss_2: {distill_reg_loss_2:.4f}\")\n    \n                # Combine losses\n                total_distill_reg_loss = distill_reg_loss_0 + distill_reg_loss_1 + distill_reg_loss_2\n                total_distill_class_loss = distill_class_loss_0 + distill_class_loss_1 + distill_class_loss_2\n                # total_soft_loss = total_distill_class_loss + total_distill_reg_loss\n                # or weighted\n                total_soft_loss = alpha * (total_distill_class_loss + total_distill_reg_loss)\n    \n                print(f\"Total Soft Loss: {total_soft_loss:.4f}\")\n    \n                final_loss = (hard_loss + alpha * total_soft_loss).sum()\n                print(f\"\\nFinal Loss: {final_loss:.4f}\")\n    \n                optimizer.zero_grad()\n                final_loss.backward()\n                optimizer.step()\n    \n                final_loss +=final_loss.item()\n                num_batches +=1\n    \n            avg_loss = final_loss / num_batches if num_batches > 0 else 0.0\n            print(f\">>> Epoch {epoch + 1} complete. Average Loss: {avg_loss:.4f}\\n\\n\\n\")\n \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Toninhooo, thank you for sharing your detailed question and code related to incremental learning and LwF with YOLOv8! 🚀 This is an automated response to help you get started while an Ultralytics engineer reviews your issue and will assist you soon.\n\nWe recommend visiting the [Docs](https://docs.ultralytics.com/) for comprehensive guides and examples, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage. Many common questions and best practices are covered there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it. This includes the smallest amount of code, data, and steps required to reproduce your issue.\n\nFor custom training ❓ questions, please provide as much detail as possible, including example images, your dataset format, and training logs, and ensure you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best! For real-time chat, check out [Discord](https://discord.com/invite/ultralytics) 🎧. For deeper discussions, visit [Discourse](https://community.ultralytics.com/), or join our [Subreddit](https://reddit.com/r/Ultralytics) to share and learn with others.\n\n## Upgrade\n\nTo ensure your issue is not already resolved in the latest version, upgrade to the newest `ultralytics` package (with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml)) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following up-to-date verified environments (with dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for contributing to Ultralytics!"
      },
      {
        "user": "glenn-jocher",
        "body": "Thanks for the detailed question! The issue you're experiencing with overconfident predictions (500+ detections with confidence 1.0) suggests that the knowledge distillation is disrupting the model's calibration. Here are the key problems I see:\n\nThe main issue is in your soft loss calculation - you're applying `F.softmax` to the raw logits from all 70 channels, but YOLO's output structure is different. The first 64 channels are DFL (Distribution Focal Loss) regression values, not classification logits, so they shouldn't be processed with softmax.\n\nFor proper knowledge distillation with YOLO, you need to:\n\n1. **Separate the outputs correctly**: Use the model's built-in decoding rather than manually slicing channels\n2. **Apply temperature scaling only to classification scores**: The bbox regression should use MSE or L1 loss\n3. **Use the model's prediction format**: Work with the decoded outputs from `student_output[0]` which are already processed\n\nHere's a corrected approach for your soft loss:\n\n```python\ndef compute_classification_kd_loss(student_cls, teacher_cls, temperature):\n    # Apply temperature scaling only to classification logits\n    teacher_soft = F.softmax(teacher_cls / temperature, dim=-1)\n    student_soft = F.log_softmax(student_cls / temperature, dim=-1)\n    return F.kl_div(student_soft, teacher_soft, reduction=\"batchmean\") * (temperature ** 2)\n\ndef compute_regression_kd_loss(student_reg, teacher_reg):\n    # Use MSE for regression outputs\n    return F.mse_loss(student_reg, teacher_reg)\n```\n\nThe high confidence predictions suggest the temperature scaling is being applied incorrectly to the regression outputs, causing the model to become overconfident. Consider using the model's native loss functions from the [v8DetectionLoss class](https://docs.ultralytics.com/reference/utils/loss/#ultralytics.utils.loss.v8DetectionLoss) and only applying KD to the final decoded predictions rather than raw feature maps."
      },
      {
        "user": "Toninhooo",
        "body": "Thank you for your feedback! I’ve updated my code based on your suggestions and noticed a difference in the results. However, the student model is still making poor predictions.\n\nCould you please verify if the process I’m following is correct? I suspect the issue lies in the interaction between the student and teacher models—specifically, that the student may not be learning properly from the teacher, which could be leading to the poor predictions.\n\nBelow is the updated code with your suggested changes, along with the output losses over 100 epochs.\n\n[Output_YOLOv8_test_2.log](https://github.com/user-attachments/files/20592336/Output_YOLOv8_test_2.log)\n```python\n##  ---------- Training Function ----------\n\n\ndef train_with_lwf(student_model, teacher_model, dataloader, optimizer, device, alpha, temperature, epochs):\n      print(\"2. Starting training with LwF...\\n\")\n      # Setting up CSV File's\n      log_dir = 'lwf_test_2'\n      os.makedirs(log_dir, exist_ok=True)\n  \n      log_train_file = os.path.join(log_dir, f'training_log.csv')\n      if not os.path.exists(log_train_file):\n          with open(log_train_file, mode='w', newline='') as f:\n              writer = csv.writer(f)\n              writer.writerow([\n                  'epoch',\n                  'train/soft_cls_loss',\n                  'train/soft_reg_loss',\n                  'train/soft_loss',\n                  'train/hard_loss'\n                  'train/total_loss',\n              ])\n  \n      batch_size = 8\n      # For Aim\n      run = Run()\n      hparams_dict = {\n          'learning_rate': 0.001,\n          'epochs': epoch_size,\n          'batch_size':batch_size\n      }\n      context = {\"test_2\": \"trial\"}\n      run['hparams'] = hparams_dict\n  \n      student_model.model.train()\n      teacher_model.model.eval()\n      student_hard_loss_calculator = v8DetectionLoss(model=student_model.model.train())\n      teacher_hard_loss_calculator = v8DetectionLoss(model=teacher_model.model.eval())\n  \n      for epoch in range(epochs):\n          print(f\"\\n--- Epoch {epoch + 1}/{epochs} ---\")\n  \n          beta = 0.5\n          num_batches = 0\n          total_loss_metric = 0.0\n          soft_cls_loss_metric = 0.0\n          soft_reg_loss_metric =0.0\n          hard_loss_metric = 0.0\n          soft_loss_metric = 0.0\n          warmup_epochs = 0\n  \n          hyp = SimpleNamespace(\n              box=7.5,\n              cls=0.5,\n              dfl=1.5\n          )\n          student_hard_loss_calculator.hyp = hyp\n          student_model.args = hyp\n          teacher_hard_loss_calculator.hyp = hyp\n          teacher_model.args = hyp\n  \n          for batch in tqdm(dataloader, desc=\"Training\"):\n  \n              images_tuple, targets_tuple = batch\n              if isinstance(images_tuple, tuple) or isinstance(images_tuple, list):\n                  images = torch.stack(images_tuple)  # shape: (B, 3, 640, 640)\n              else:\n                  raise ValueError(f\"Expected tuple/list of images but got {type(images_tuple)}\")\n  \n              images = images.float() / 255.0\n              images = images.float().to(device)\n              images.requires_grad = True\n  \n              targets = merge_target_dicts(targets_tuple)\n  \n              # Student forward pass\n              student_output = student_model.model(images)\n              student_output = [t.to(device) for t in student_output]\n  \n  \n              # # Teacher forward pass (eval, no grad, no targets)\n              with torch.no_grad():\n                  teacher_output = teacher_model.model(images)\n  \n              hard_loss, loss_detached = student_hard_loss_calculator(student_output, targets)\n              # print(f\"\\nHard Loss Output: {hard_loss}\")\n              # print(f\" - Box: {hard_loss[0].item():.4f}\")\n              # print(f\" - Cls: {hard_loss[1].item():.4f}\")\n              # print(f\" - DFL: {hard_loss[2].item():.4f}\")\n              # print(f\"(Detached) Cls Loss for logging: {loss_detached[1].item():.4f}\")'''\n  \n  \n              ############ STUDENT  ##################\n              # print(f\"\\n no: {student_hard_loss_calculator.no}\")\n              # print(f\" reg_max: {student_hard_loss_calculator.reg_max}\")\n              # print(f\" nc: {student_hard_loss_calculator.nc}\")\n  \n              pred_dist_student, pred_scores_student = torch.cat([xi.view(student_output[0].shape[0], student_hard_loss_calculator.no, -1) for xi in student_output], 2).split((student_hard_loss_calculator.reg_max * 4, student_hard_loss_calculator.nc), 1)\n              # print(f\"\\n pred_dist_student shape: {pred_dist_student.shape}\")\n              # print(f\"\\n pred_scores_student shape: {pred_scores_student.shape}\")\n  \n              pred_dist_student = pred_dist_student.permute(0, 2, 1).contiguous()\n              pred_scores_student = pred_scores_student.permute(0, 2, 1).contiguous()\n              # print(f\"\\n pred_dist_student shape after permute: {pred_dist_student.shape}\")\n              # print(f\"\\n pred_scores_student shape after permute: {pred_scores_student.shape}\")\n  \n              stride = student_model.model.stride\n              anchor_points, _ = make_anchors(student_output, stride, 0.5)\n              # print(anchor_points.shape)\n  \n              pred_bboxes = student_hard_loss_calculator.bbox_decode(anchor_points, pred_dist_student)\n              # print(f\"pred_bboxes shape: {pred_bboxes.shape}\")\n              # print(f\"pred_bboxes: {pred_bboxes}\")\n  \n              ################# TEACHER #######################\n              # print(f\"\\n teacher output shape: {teacher_output[0].shape}\")\n              # print(f\"\\n teacher output: {teacher_output[0]}\")\n              # print(f\"\\n no: {teacher_hard_loss_calculator.no}\")\n              # print(f\" reg_max: {teacher_hard_loss_calculator.reg_max}\")\n              # print(f\" nc: {teacher_hard_loss_calculator.nc}\")\n  \n              # tecaher_feats = teacher_output[1] if isinstance(teacher_output, tuple) else teacher_output\n              # # print(feats)\n  \n              teacher_output = teacher_output[1]\n              pred_dist_teacher, pred_scores_teacher = torch.cat([xi.view(teacher_output[0].shape[0], teacher_hard_loss_calculator.no, -1) for xi in teacher_output], 2).split((teacher_hard_loss_calculator.reg_max * 4, teacher_hard_loss_calculator.nc), 1)\n              # print(f\"pred_dist_teacher shape: {pred_dist_teacher.shape}\")\n              # print(f\"pred_scores_teacher shape: {pred_scores_teacher.shape}\")\n  \n              pred_dist_teacher = pred_dist_teacher.permute(0, 2, 1).contiguous()\n              pred_scores_teacher = pred_scores_teacher.permute(0, 2, 1).contiguous()\n              # print(f\"\\n pred_dist_teacher shape after permute: {pred_dist_teacher.shape}\")\n              # print(f\"\\n pred_scores_teacher shape after permute: {pred_scores_teacher.shape}\")\n  \n              stride_t = teacher_model.model.stride\n              anchor_points_t, _ = make_anchors(teacher_output, stride_t, 0.5)\n              # print(anchor_points.shape)\n  \n              pred_bboxes_t = teacher_hard_loss_calculator.bbox_decode(anchor_points_t, pred_dist_teacher)\n  \n              soft_cls_loss = class_kd_loss(pred_scores_student, pred_scores_teacher, temperature)\n              soft_reg_loss = reg_kd_loss(pred_bboxes, pred_bboxes_t)\n  \n              total_soft_loss = soft_cls_loss + soft_reg_loss\n              final_loss = alpha * total_soft_loss + ((1 - alpha) * hard_loss).sum()\n  \n              optimizer.zero_grad()\n              final_loss.backward()\n              optimizer.step()\n  \n              # hard_loss_metric += hard_loss.item()\n              # soft_loss_metric += soft_loss.item()\n              # final_loss_metric += final_loss.item()\n              num_batches +=1\n              # Accumulate losses\n              soft_cls_loss_metric += soft_cls_loss.detach().cpu().item()\n              soft_reg_loss_metric += soft_reg_loss.detach().cpu().item()\n              hard_loss_metric += hard_loss.detach().cpu().sum().item()\n              soft_loss_metric += total_soft_loss.detach().cpu().item()\n              total_loss_metric += final_loss.detach().cpu().item() \n```\n\n"
      }
    ]
  },
  {
    "issue_number": 21020,
    "title": "Why is my train_loss nan",
    "author": "yangershuai627",
    "state": "open",
    "created_at": "2025-06-11T09:11:33Z",
    "updated_at": "2025-06-13T17:59:34Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nWhen using the custom module A+B alone, the train_loss is not nan. When using the custom module C alone, the train_loss is not nan either. When using module A+B+C, the train_loss is nan, as shown in the following figure.\n![Image](https://github.com/user-attachments/assets/c8e4803a-48d7-4cba-a832-27c197c689d9)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @yangershuai627, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\n_This is an automated response. An Ultralytics engineer will assist you here soon!_"
      },
      {
        "user": "yangershuai627",
        "body": "@Y-T-G Do you know how to solve this problem? "
      },
      {
        "user": "Y-T-G",
        "body": "You can try reducing learning rate"
      }
    ]
  },
  {
    "issue_number": 20605,
    "title": "Can I overwrite shuffle=False when using rect=True if all my images have the same size ?",
    "author": "Loic-76",
    "state": "closed",
    "created_at": "2025-05-12T15:58:28Z",
    "updated_at": "2025-06-13T17:37:43Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI wonder if I can without a risk overwrite shuffle=False with rect=True as all my images have the same shape and aspect ratio.\nI feel like I lose a lot of training time with padding. What do you guys think ?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Loic-76, thank you for your interest in Ultralytics 🚀! We recommend checking out the [Docs](https://docs.ultralytics.com/) for guidance—especially the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, which address many common training questions.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate further.\n\nIf this is a custom training ❓ Question (like yours), please share as much detail as possible, such as dataset image samples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nYou’re also invited to join the Ultralytics community! For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧. For deeper discussions, try [Discourse](https://community.ultralytics.com/), or check out our [Subreddit](https://reddit.com/r/Ultralytics) for community knowledge sharing.\n\n## Upgrade\n\nTo ensure you aren't encountering an issue that has been resolved in the latest release, upgrade the `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can use YOLO in any of these verified, up-to-date environments (with all dependencies such as [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response 🤖. An Ultralytics engineer will review your question and assist you soon!"
      },
      {
        "user": "Y-T-G",
        "body": "If you have `mosaic` enabled, then the padding isn't just an empty area. It's portions of other images. So it's not really a wasted space."
      },
      {
        "user": "Loic-76",
        "body": "> If you have `mosaic` enabled, then the padding isn't just an empty area. It's portions of other images. So it's not really a wasted space.\n\nHello and thank you for your reply, I tested it and I want to add a little something.\n\nAs the center of the generated mosaic is determined using random values (and can either be inside the image or outside of it), the imgsz parameter must be adapted so there is no padding.\nFor example with imgsz=2000 on 1000x2000 images, there was still some padding added."
      }
    ]
  },
  {
    "issue_number": 21043,
    "title": "Where is model config file in YOLO11?",
    "author": "Tian14267",
    "state": "closed",
    "created_at": "2025-06-13T01:55:06Z",
    "updated_at": "2025-06-13T16:14:12Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n**Hello， I want to know where is  model config file in YOLO11? I want to build a new yolo model ,I can I set model config file ?**\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Tian14267, thank you for your interest in Ultralytics 🚀! For new users, we recommend checking out the [Docs](https://docs.ultralytics.com/) where you’ll find guides on [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, which may answer many common questions.\n\nSince this is a ❓ custom model question, please provide as much detail as possible about what you’d like to achieve. If you’re working on custom training, including your intended model architecture or any sample config attempts will help us assist you faster. Also, please review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nThis is an automated response 🤖 — an Ultralytics engineer will also review your issue soon!\n\nJoin the Ultralytics community for more support:\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth discussion: [Discourse](https://community.ultralytics.com/)\n- Community threads: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package and verify all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) are met in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "Tian14267",
        "body": "I find it . Thank you"
      },
      {
        "user": "glenn-jocher",
        "body": "Great to hear you found it! For anyone else looking, YOLO11 model configurations are defined in YAML files like `yolo11n.yaml`, and you can build a new model using `YOLO(\"your_config.yaml\")` or reference the existing configs in the ultralytics repository structure."
      }
    ]
  },
  {
    "issue_number": 21035,
    "title": "input the torch tensor",
    "author": "g2bond",
    "state": "open",
    "created_at": "2025-06-12T08:49:44Z",
    "updated_at": "2025-06-13T15:30:05Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi, can model = YOLO('yolo11s.pt') directly input a pytorch tensor of the shape (B, C, H, W) and output coordinates and depth features? If so, how can I do it? I want to integrate it into my model as a submodule.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @g2bond, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response. An Ultralytics engineer will assist you here soon 😊."
      },
      {
        "user": "Y-T-G",
        "body": "You can get the raw outputs. But it wouldn't be postprocessed:\n\n`output = model.model(input_tensor)`"
      },
      {
        "user": "g2bond",
        "body": "I wrote a simple implementation：\n\nfrom ultralytics import YOLO\nimport cv2\nimport torch\nimport numpy as np\n\nimage = cv2.imread(\"test_p.png\")\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ntensor = torch.from_numpy(image_rgb.astype(np.float32) / 255.0)\ntensor = tensor.permute(2, 0, 1).unsqueeze(0)\nmodel = YOLO('yolo11s.pt')\noutput=model.model(tensor)\nprint(output)\n\nBut it throws the error:\nTraceback (most recent call last):\n  File \"/Users/test.py\", line 22, in <module>\n    output=model.model(tensor)\n...\n  File \"/opt/anaconda3/envs/dover/lib/python3.9/site-packages/ultralytics/nn/modules/conv.py\", line 682, in forward\n    return torch.cat(x, self.d)\nRuntimeError: Sizes of tensors must match except in dimension 1. Expected size 136 but got size 135 for tensor number 1 in the list.\n\nThe image is 1920*1080. How does this error occur? Do I need to do any preprocessing on the input to adapt to the correct shape?"
      }
    ]
  },
  {
    "issue_number": 2973,
    "title": "how to disable wandb?",
    "author": "lucas-mior",
    "state": "closed",
    "created_at": "2023-06-02T16:20:15Z",
    "updated_at": "2025-06-13T15:17:37Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nFormerly, I was able to disable wandb using:\r\n```\r\nimport wandb\r\nwandb.disabled = True\r\n```\r\nor\r\n```\r\n!wandb disabled\r\n```\r\nHowever, now it seems that yolo always asks for wandb login and won't start training if I don't enter it.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@lucas-mior thank you for your question. In order to disable wandb in YOLOv8, please modify the `wandb_logging.py` file in the `utils` folder by commenting out or deleting the line that initializes wandb. This will prevent the program from asking for wandb login and allow you to train without wandb logging. Let me know if you have any further questions or concerns."
      },
      {
        "user": "Hommoner",
        "body": "try add this \r\n`os.environ['WANDB_DISABLED'] = 'true'`"
      },
      {
        "user": "glenn-jocher",
        "body": "Thank you for your suggestion @Hommoner. Setting the environment variable `WANDB_DISABLED` to `'true'` is another way to disable wandb in YOLOv8. Once the variable is set, wandb will not initialize and no wandb logging will occur during training. Please let us know if you have any further questions or if you encounter any issues."
      }
    ]
  },
  {
    "issue_number": 21048,
    "title": "Is the explanation of DFL Loss correct?",
    "author": "DP1701",
    "state": "open",
    "created_at": "2025-06-13T07:58:00Z",
    "updated_at": "2025-06-13T14:07:57Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello everyone,\n\nUnfortunately, I always find it difficult to remember what DFL Loss actually does and what it says. Now I have started to understand it better with ChatGPT and got this explanation. Can anyone confirm whether this is correct?\n\n\n---\n\n###  Distribution Focal Loss (DFL) — Simple Explanation\n\nDFL is a type of loss used in object detection (e.g. YOLOv8) to improve the accuracy of bounding box predictions.\n\nInstead of predicting box coordinates (x, y, w, h) as continuous numbers, the model predicts **discrete probability distributions** over possible positions (e.g. from 0 to 255).  \nDFL helps the model focus more on difficult predictions, inspired by the original Focal Loss.\n\n---\n\n###  Why DFL?\n\n- Small errors in box positions can cause significant issues.\n- DFL encourages the model to give high probability to the correct position and to concentrate probability around it.\n- Hard examples (e.g. small or occluded objects) contribute more to the loss.\n- Like Focal Loss, DFL increases the focus on harder examples during training.\n\n---\n\n###  Example\n\nSuppose the ground truth for the left boundary of a box is at **50.0**.\n\nInstead of directly predicting 50.0, the model predicts:\n\n| Position | Probability |\n|----------|-------------|\n| 49       | 0.2         |\n| 50       | 0.6 ✅       |\n| 51       | 0.2         |\n\n---\n\n###  How is the final predicted value calculated?\n\n- The model does **not** simply take the bin with the highest probability (`argmax`).\n- Instead, it calculates a **weighted average (expected value)** over all bins to get a more precise prediction:\n\n\nFor this example:\n\n49 * 0.2 + 50 * 0.6 + 51* 0.2 = 50.0\n\n\n- This allows sub-pixel precision, even though the predictions are over discrete bins.\n\n---\n\n###  In short:\n\n**DFL = Discretized coordinate prediction + Focal Loss principle to focus on hard-to-predict examples, using weighted average for final prediction.**\n\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @DP1701, thank you for reaching out and for your interest in Ultralytics 🚀! This is an automated response to help get you started—an Ultralytics engineer will also assist you soon.\n\nWe recommend checking out our [Docs](https://docs.ultralytics.com/) for in-depth explanations and usage examples for various YOLO features, including loss functions. Many common questions may already be addressed in the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) guides.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate.\n\nFor custom training ❓ Questions, sharing details like dataset samples and training logs will help us assist you more effectively. Also, please review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nConnect with the Ultralytics community:\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth discussions: [Discourse](https://community.ultralytics.com/)\n- Knowledge sharing: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nMake sure you're using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be used in any of these up-to-date, verified environments (all dependencies included):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your question! An Ultralytics team member will review and provide further guidance on DFL Loss soon."
      },
      {
        "user": "glenn-jocher",
        "body": "Your explanation captures the core concepts of DFL very well! The key points about using discrete probability distributions instead of continuous coordinates and computing weighted averages for final predictions are absolutely correct.\n\nA couple of minor clarifications based on the Ultralytics implementation:\n\nThe default `reg_max` is 16 (not 255), so you'd typically have 16 discrete bins (0-15) rather than 256. Your example with positions 49, 50, 51 and the weighted average calculation (49×0.2 + 50×0.6 + 51×0.2 = 50.0) perfectly demonstrates how the final coordinate is computed.\n\nYou can see this in action in the [DFL module implementation](https://docs.ultralytics.com/reference/nn/modules/block/#ultralytics.nn.modules.block.DFL) where the weighted sum is computed after applying softmax to the distribution, and in the [DFLoss class](https://docs.ultralytics.com/reference/utils/loss/#ultralytics.utils.loss.DFLoss) which handles the loss computation between predicted distributions and target coordinates.\n\nThe \"Focal\" aspect in the name comes from the general principle of focusing training on harder examples, though the actual implementation differs from traditional Focal Loss with its gamma weighting scheme."
      }
    ]
  },
  {
    "issue_number": 21042,
    "title": "I want YOLOv12 to be stronger than YOLOv11.",
    "author": "Taoboan1999",
    "state": "open",
    "created_at": "2025-06-13T00:53:48Z",
    "updated_at": "2025-06-13T09:22:58Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello, I trained and tested the same training parameters on YOLOv12 and YOLOv11, but why is YOLOv12 less effective than YOLOv11? How can I further optimize it? I want YOLOv12 to be stronger than YOLOv11.\n\n\n\n                lr0=0.001, \n                lrf=0.01,  \n                momentum=0.937,  \n                weight_decay=0.0005,  \n                warmup_epochs=3.0,  \n                warmup_momentum=0.8, \n                warmup_bias_lr=0.1, \n                box=5.0,  \n                cls=0.025, \n                dfl=1.0,\n                val=True, \n                plots=True, \n                conf=0.001,\n                iou=0.7,  \n                hsv_h=0.0, \n                hsv_s=0.0,  \n                hsv_v=0.0,  \n                degrees=90.0, \n                translate=0.0,  \n                scale=0.0,  \n                shear=0.0, \n                perspective=0.0,  \n                flipud=0.0, \n                fliplr=0.5,  \n                bgr=0.0, \n                mosaic=1.0, \n                mixup=0.1,  \n                cutmix=0.5, \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Taoboan1999, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response and an Ultralytics engineer will assist you soon 😊"
      },
      {
        "user": "glenn-jocher",
        "body": "Thanks for your question! YOLO12 is actually **not recommended** for any use cases. YOLO12 suffers from training instability, excessive memory consumption, and slow speed (2-3x slower on CPU) due to the complex attention layers it uses. This explains why you're seeing worse performance compared to YOLO11.\n\nWe recommend using [YOLO11](https://docs.ultralytics.com/models/yolo11/) instead, which is our current state-of-the-art model delivering superior performance across all tasks. YOLO11 provides better accuracy, efficiency, and stability compared to YOLO12. For optimal results, stick with YOLO11 and consider using our [hyperparameter tuning guide](https://docs.ultralytics.com/guides/hyperparameter-tuning/) to optimize your training parameters."
      }
    ]
  },
  {
    "issue_number": 20156,
    "title": "The wrong parameter in RepC3 module of RT-DETR",
    "author": "DongBig",
    "state": "open",
    "created_at": "2025-04-14T02:11:55Z",
    "updated_at": "2025-06-13T00:23:44Z",
    "labels": [
      "bug",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nThere is an issue with the second parameter of cv1 and cv2 in the RepC3 module of RT-DETR. The original parameter passed was c2, but the correct one should be c_.\n\n### Environment\n\nOS                  Windows-10-10.0.22631-SP0\nEnvironment         Windows\nPython              3.10.15\nInstall             pip\nRAM                 31.86 GB\nDisk                392.4/551.2 GB\nCPU                 12th Gen Intel Core(TM) i5-12400F\nCPU count           12\nGPU                 NVIDIA GeForce RTX 4060, 8188MiB\nGPU count           1\nCUDA                12.4\n\n### Minimal Reproducible Example\n\n`class RepC3(nn.Module):\n    \"\"\"Rep C3.\"\"\"\n\n    def __init__(self, c1, c2, n=3, e=1.0):\n        \"\"\"Initialize CSP Bottleneck with a single convolution using input channels, output channels, and number.\"\"\"\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c2, 1, 1)\n        self.cv2 = Conv(c1, c2, 1, 1)\n        self.m = nn.Sequential(*[RepConv(c_, c_) for _ in range(n)])\n        self.cv3 = Conv(c_, c2, 1, 1) if c_ != c2 else nn.Identity()\n\n    def forward(self, x):\n        \"\"\"Forward pass of RT-DETR neck layer.\"\"\"\n        return self.cv3(self.m(self.cv1(x)) + self.cv2(x))`\n\nRepC3(512,256,3,0.5)\n\n### Additional\n\nThe right code should be\n`class RepC3(nn.Module):\n    \"\"\"Rep C3.\"\"\"\n\n    def __init__(self, c1, c2, n=3, e=1.0):\n        \"\"\"Initialize CSP Bottleneck with a single convolution using input channels, output channels, and number.\"\"\"\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c1, c_, 1, 1)\n        self.m = nn.Sequential(*[RepConv(c_, c_) for _ in range(n)])\n        self.cv3 = Conv(c_, c2, 1, 1) if c_ != c2 else nn.Identity()\n\n    def forward(self, x):\n        \"\"\"Forward pass of RT-DETR neck layer.\"\"\"\n        return self.cv3(self.m(self.cv1(x)) + self.cv2(x))`\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @DongBig, thank you for bringing this to our attention and for your detailed report 🚀! We appreciate your effort in identifying and suggesting a fix for the issue in the RepC3 module of RT-DETR.\n\nIf this is a 🐛 Bug Report, please verify that you are using the latest version of the `ultralytics` package. To upgrade, run the following command in your environment:\n\n```bash\npip install -U ultralytics\n```\n\nIf the issue persists, please ensure your example is a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it effectively.\n\nOur engineers will review your findings and assist you further soon. In the meantime, you might want to check out the [Docs](https://docs.ultralytics.com/), where you can find helpful resources for [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage.\n\nFor real-time discussions, feel free to join us on [Discord](https://discord.com/invite/ultralytics) 🎧 or explore our [Discourse](https://community.ultralytics.com/) and [Subreddit](https://reddit.com/r/Ultralytics) for broader community engagement.\n\n## Environments\n\nYou can also test your code in verified environments with all dependencies pre-installed:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu environments.\n\nThank you for your contribution, and stay tuned for further updates! 😊"
      },
      {
        "user": "glenn-jocher",
        "body": "You're absolutely right, @DongBig! This is indeed a bug in the RepC3 module implementation. Looking at the [official code in the block.py module](https://docs.ultralytics.com/reference/nn/modules/block/#ultralyticsnnmodulesblockrepC3), the correct implementation should use `c_` (hidden channels) as the second parameter for both `cv1` and `cv2`, not `c2` as currently implemented.\n\nThe correct implementation should be:\n```python\nself.cv1 = Conv(c1, c_, 1, 1)\nself.cv2 = Conv(c1, c_, 1, 1)\n```\n\nThis is important for maintaining the proper architecture design of the RepC3 module. Thank you for catching this issue and for being willing to submit a PR to fix it. We appreciate your contribution to improving Ultralytics YOLO!"
      },
      {
        "user": "ExtReMLapin",
        "body": "Already fixed in \n\nhttps://github.com/ultralytics/ultralytics/pull/17086"
      }
    ]
  },
  {
    "issue_number": 20593,
    "title": "using yolov11l.engine to detect the car, the inference time is not stable",
    "author": "CrazyLearner98",
    "state": "open",
    "created_at": "2025-05-12T06:40:41Z",
    "updated_at": "2025-06-13T00:23:30Z",
    "labels": [
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\n![Image](https://github.com/user-attachments/assets/4b8b687d-fd13-4d8d-865f-966faa57aff7)\nWhen it is only one car in the frame, it still cost thounds of ms, sometimes one car only cost 7-8 ms, I don't know what's wrong here. I use two engines, one for yolov11 detection, another one is for depth estimation. \nI export yolov11 use official yolo.export, and infer it use official format.\n\n### Environment\n\nNONE\n\n### Minimal Reproducible Example\n\nNONE\n\n### Additional\n\nNONE\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @CrazyLearner98, thank you for bringing this to our attention and for your interest in Ultralytics 🚀! This is an automated response—an Ultralytics engineer will review your issue and assist you soon.\n\nFor 🐛 Bug Reports like this, could you please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE)? This will help us investigate the unstable inference time you're experiencing with yolov11l.engine more efficiently.\n\nIn the meantime, you may want to check out our [Docs](https://docs.ultralytics.com/) for further [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples or see if our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) offer any relevant insights.\n\nJoin the Ultralytics community for more support and discussion:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for real-time chat\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) to connect with other users\n\n## Upgrade\n\nPlease ensure you have the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) installed in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following up-to-date verified environments, each with dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests have passed. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThanks again for your report! Please update with an MRE when possible so we can assist you more effectively 😊"
      },
      {
        "user": "Y-T-G",
        "body": "Can you provide the output after running this command in terminal: `yolo checks`?"
      },
      {
        "user": "CrazyLearner98",
        "body": "> Can you provide the output after running this command in terminal: `yolo checks`?\n\nWhen I run larger videos, my GPU memory usage reaches 100%. Is the slowdown caused by the GPU memory being fully occupied?\n\n![Image](https://github.com/user-attachments/assets/7728605f-ff57-4df0-ae2c-6bbe8cbf7c8f)"
      }
    ]
  },
  {
    "issue_number": 20596,
    "title": "How do I run a custom data augmentation method in the yolov8seg task, including making some changes to the labels?",
    "author": "Keven-Don",
    "state": "open",
    "created_at": "2025-05-12T09:53:24Z",
    "updated_at": "2025-06-13T00:23:29Z",
    "labels": [
      "enhancement",
      "question",
      "Stale",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nIn my YOLOv8-seg implementation, I wrote some offline augmentation methods. However, I found that during training, these offline data augmentations cause the model to forget some learned features. So I want to convert them into online data augmentation methods (including some modifications to the labels) and integrate them into Ultralytics. I'd like to ask how to properly integrate them?\n\ndef replace_img_part(\n        img: np.ndarray,\n        instances: List[Dict],\n        bg_color=(255, 255, 255),\n        num_random_shapes=3,\n        min_width=100,\n        max_width=800,\n        min_height=100,\n        max_height=800,\n        aspect_ratio: Optional[float] = None,\n        max_attempts=100\n):\n\n    h, w = img.shape[:2]\n    combined_mask = np.zeros((h, w), dtype=np.uint8)\n\n    for inst in instances:\n        if 'polygon' not in inst:\n            raise ValueError(\"实例缺少多边形参数'polygon'\")\n        polygon = inst['polygon'].astype(np.int32)\n        cv2.fillPoly(combined_mask, [polygon], color=255, lineType=cv2.LINE_AA)\n\n    bg_replaced = img.copy()\n    inverse_mask = cv2.bitwise_not(combined_mask)\n\n    random_mask = np.zeros((h, w), dtype=np.uint8)\n\n    for _ in range(num_random_shapes):\n        attempt = 0\n        while attempt < max_attempts:\n            x = np.random.randint(0, w)\n            y = np.random.randint(0, h)\n            if inverse_mask[y, x] == 255:\n                break\n            attempt += 1\n        else:\n            continue\n\n        max_w = min(max_width, w - x)\n        max_h = min(max_height, h - y)\n        if max_w < min_width or max_h < min_height:\n            continue\n\n        if aspect_ratio:\n            base_size = np.random.randint(min_width, max_w + 1)\n            width = base_size\n            height = int(base_size / aspect_ratio)\n            height = np.clip(height, min_height, max_h)\n        else:\n            width = np.random.randint(min_width, max_w + 1)\n            height = np.random.randint(min_height, max_h + 1)\n\n        x_end = min(x + width, w)\n        y_end = min(y + height, h)\n\n        cv2.rectangle(\n            random_mask,\n            (x, y),\n            (x_end, y_end),\n            color=255,\n            thickness=-1,\n            lineType=cv2.LINE_AA\n        )\n    final_replace_mask = cv2.bitwise_and(random_mask, inverse_mask)\n    bg_replaced[final_replace_mask == 255] = bg_color\n    return bg_replaced, instances\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Keven-Don, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response 🤖. An Ultralytics engineer will review your question and assist you soon!"
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @Keven-Don,\n\nTo integrate your custom data augmentation into YOLOv8's online augmentation pipeline, you'll need to create a custom transform class by extending the `BaseTransform` class from `ultralytics/data/augment.py`. Here's how:\n\n1. Create a custom transform class:\n\n```python\nfrom ultralytics.data.augment import BaseTransform\nimport numpy as np\nimport cv2\n\nclass ReplaceImgPart(BaseTransform):\n    def __init__(self, p=0.5, bg_color=(255, 255, 255), num_random_shapes=3):\n        super().__init__()\n        self.p = p\n        self.bg_color = bg_color\n        self.num_random_shapes = num_random_shapes\n        # Add other parameters as needed\n        \n    def apply_image(self, labels):\n        \"\"\"Apply augmentation to image\"\"\"\n        if np.random.random() > self.p:\n            return labels\n            \n        img = labels[\"img\"]\n        h, w = img.shape[:2]\n        \n        # Create mask from instances\n        combined_mask = np.zeros((h, w), dtype=np.uint8)\n        instances = labels[\"instances\"]\n        \n        # Convert bbox instances to polygons if needed\n        # This part will depend on your specific use case\n        \n        # Rest of your augmentation logic here\n        # Make sure to adapt it to work with the Ultralytics format\n        \n        labels[\"img\"] = augmented_img  # Your augmented image\n        return labels\n```\n\n2. Add your transform to the augmentation pipeline by modifying the `v8_transforms` function or by creating a custom dataloader. The simplest approach is to update your training script:\n\n```python\nfrom ultralytics import YOLO\nfrom ultralytics.data.augment import v8_transforms, Compose\n\n# Create your custom transform\ncustom_transform = ReplaceImgPart(p=0.5)\n\n# Load model\nmodel = YOLO('yolov8s-seg.pt')\n\n# Add your transform to the augmentation pipeline\ndef custom_transforms_adapter(dataset, imgsz, hyp):\n    transform = v8_transforms(dataset, imgsz, hyp)\n    transform.transforms.append(custom_transform)\n    return transform\n\n# Update the model's data loader with your custom transforms\nmodel.add_callback('on_train_start', lambda: setattr(model.trainer.data, 'transforms', custom_transforms_adapter))\n\n# Train as usual\nmodel.train(data='your_dataset.yaml', epochs=100)\n```\n\nThe key is adapting your function to work with the Ultralytics instances format, which includes bounding boxes, segments (polygons), and other attributes in a specific structure.\n\nLet me know if you need more specific guidance on adapting your code to the Ultralytics format!"
      },
      {
        "user": "Keven-Don",
        "body": "Okay, let me try it first to see if it works."
      }
    ]
  },
  {
    "issue_number": 20600,
    "title": "Using the new attention mechanism to replace the original C2SPA in the YOLO11 detection model shows \"ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 16, 1, 1])\"",
    "author": "zhu120",
    "state": "open",
    "created_at": "2025-05-12T12:20:00Z",
    "updated_at": "2025-06-13T00:23:27Z",
    "labels": [
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nUsing the new attention mechanism to replace the original C2SPA in the YOLO11 detection model shows \"ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 16, 1, 1])\"\n\n### Environment\n\nUltralytics 8.3.77 \nPython=3.12.8\nTorch-2.6.0+cu126  gpu\n\n\n### Minimal Reproducible Example\n\nclass SMSA(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(SMSA, self).__init__()\n        self.in_channels = in_channels\n        self.reduction = reduction\n        self.conv1 = Conv(in_channels, in_channels // reduction, 1)\n        self.conv2 = Conv(in_channels // reduction, in_channels, 1)\n\n    def forward(self, x):\n        \"\"\"\n        SMSA: Multi-semantic spatial attention mechanism.\n        \"\"\"\n        # Compute spatial attention\n        attention = F.adaptive_avg_pool2d(x, 1)\n        attention = self.conv1(attention)\n        attention = F.relu(attention)\n        attention = self.conv2(attention)\n        attention = torch.sigmoid(attention)\n        \n        return x * attention  # Apply attention\n\nclass PCSA(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(PCSA, self).__init__()\n        self.in_channels = in_channels\n        self.reduction = reduction\n        self.attention = nn.Sequential(\n            Conv(in_channels, in_channels // reduction, 1),\n            nn.Sigmoid(),\n            Conv(in_channels // reduction, in_channels, 1)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        PCSA: Progressive channel-wise self-attention mechanism.\n        \"\"\"\n        # Compute channel-wise attention\n        attention = self.attention(x)\n        return x * attention  # Apply attention\n\nclass SCSA(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(SCSA, self).__init__()\n        self.smsa = SMSA(in_channels, reduction)  # 使用共享空间注意力\n        self.pcsa = PCSA(in_channels, reduction)  # 渐进通道注意力\n\n    def forward(self, x):\n        x = self.smsa(x)  # 应用空间注意力\n        x = self.pcsa(x)  # 应用通道注意力\n        return x\n\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @zhu120, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users, where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nSince this is a 🐛 Bug Report, thank you for providing your code! To help us debug further, please ensure your [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) includes all the code and steps needed to reproduce the error, as well as details on how you integrated the new attention mechanism into the YOLO11 model.\n\nJoin the Ultralytics community for support and discussion:\n- For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧\n- Prefer in-depth discussions? Visit [Discourse](https://community.ultralytics.com/)\n- Dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community\n\n## Upgrade\n\nPlease upgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response to help you get started. An Ultralytics engineer will also assist you here soon 😊"
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @zhu120, \n\nThe error you're seeing is because BatchNorm2d (likely used within your Conv layers) requires more than one spatial value per channel during training. When you use `F.adaptive_avg_pool2d(x, 1)`, you're reducing the spatial dimensions to 1x1, which causes issues with BatchNorm2d.\n\nLooking at your code, I'd recommend checking out the existing attention mechanisms in Ultralytics such as `ChannelAttention`, `SpatialAttention`, and `CBAM` in `ultralytics/nn/modules/conv.py`. These implementations handle the dimensions properly to avoid BatchNorm issues.\n\nFor example, in the official implementation, channel attention uses:\n```python\nself.pool = nn.AdaptiveAvgPool2d(1)\nself.fc = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)\nself.act = nn.Sigmoid()\n```\n\nAnd then applies it with: `return x * self.act(self.fc(self.pool(x)))`\n\nYou might want to modify your attention implementations to follow this pattern or use `Conv(..., norm=None)` to avoid BatchNorm in the 1x1 convolutions after pooling."
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20615,
    "title": "Why does the Patience Parameter distort the training map scores in validation?",
    "author": "codejoetheduke",
    "state": "open",
    "created_at": "2025-05-13T07:10:46Z",
    "updated_at": "2025-06-13T00:23:25Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI trained a yolo11l model on a custom dataset with 3 classes for 100 epochs and had a score of 0.821. The model stopped improving at the 46th epoch. Now when I added patience, I get a far different result and lower map50 score at validation. Isn't there a way of solving this? I want the same result but shortened to 46 epochs.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @codejoetheduke, thank you for reaching out and for your interest in Ultralytics 🚀! We recommend reviewing our [Docs](https://docs.ultralytics.com/) where you’ll find [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, as well as answers to many common questions.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us understand and debug the issue.\n\nIf you have a custom training ❓ question, please share as much information as possible, such as your training logs, your dataset configuration, and any modifications to default training settings. Make sure to check our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) to optimize your experiments.\n\nJoin the Ultralytics community in any way you prefer! For real-time chat, hop on [Discord](https://discord.com/invite/ultralytics) 🎧. For more detailed discussions, visit [Discourse](https://community.ultralytics.com/). Or join threads on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nUpgrade your `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to make sure you’re using the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can run in any of the following up-to-date verified environments (with dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response. An Ultralytics engineer will review and assist you soon!"
      },
      {
        "user": "Y-T-G",
        "body": "What's the code?"
      },
      {
        "user": "codejoetheduke",
        "body": "```arduion\n# 🔧 Load Pretrained YOLOv11 Model\nmodel1 = YOLO(\"yolo11l.pt\")\n\n# 🚀 Simulate 100-Epoch Training in 46 Epochs\nmodel1.train(data='data.yaml',\n             epochs=100,  # shortened to 46\n             imgsz=640,\n             device=0,\n             batch=16,\n             optimizer='AdamW',\n             lr0=3e-4,           # initial learning rate (same as before)\n             momentum=0.9,\n             weight_decay=1e-2,\n             close_mosaic=30,\n             seed=42,\n             patience=10          # disable early stopping (optional but recommended)\n)\n```"
      }
    ]
  },
  {
    "issue_number": 21013,
    "title": "Does onnx export include NMS post-processing?",
    "author": "SebastianJanampa",
    "state": "closed",
    "created_at": "2025-06-11T01:48:58Z",
    "updated_at": "2025-06-12T22:15:59Z",
    "labels": [
      "question",
      "pose",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello,\n\nI am trying to benchmark yolo11-pose and yolov8-pose. I saw some github repos and they export the model to onnx and then manually add the nms postprocessing as shown below\n```python\ndef yolo_insert_nms(\n    path, score_threshold=0.01, iou_threshold=0.7, max_output_boxes=300, simplify=False\n):\n    \"\"\"\n    http://www.xavierdupre.fr/app/onnxcustom/helpsphinx/api/onnxops/onnx__EfficientNMS_TRT.html\n    https://huggingface.co/spaces/muttalib1326/Punjabi_Character_Detection/blob/3dd1e17054c64e5f6b2254278f96cfa2bf418cd4/utils/add_nms.py\n    \"\"\"\n    onnx_model = onnx.load(path)\n\n    if simplify:\n        from onnxsim import simplify\n\n        onnx_model, _ = simplify(onnx_model, overwrite_input_shapes={\"image\": [1, 3, 640, 640]})\n\n    graph = onnx_graphsurgeon.import_onnx(onnx_model)\n    graph.toposort()\n    graph.fold_constants()\n    graph.cleanup()\n\n    topk = max_output_boxes\n    attrs = OrderedDict(\n        plugin_version=\"1\",\n        background_class=-1,\n        max_output_boxes=topk,\n        score_threshold=score_threshold,\n        iou_threshold=iou_threshold,\n        score_activation=False,\n        box_coding=0,\n    )\n\n    outputs = [\n        onnx_graphsurgeon.Variable(\"num_dets\", np.int32, [-1, 1]),\n        onnx_graphsurgeon.Variable(\"det_boxes\", np.float32, [-1, topk, 4]),\n        onnx_graphsurgeon.Variable(\"det_scores\", np.float32, [-1, topk]),\n        onnx_graphsurgeon.Variable(\"det_classes\", np.int32, [-1, topk]),\n    ]\n\n    graph.layer(\n        op=\"EfficientNMS_TRT\",\n        name=\"batched_nms\",\n        inputs=[graph.outputs[0], graph.outputs[1]],\n        outputs=outputs,\n        attrs=attrs,\n    )\n\n    graph.outputs = outputs\n    graph.cleanup().toposort()\n\n    onnx.save(onnx_graphsurgeon.export_onnx(graph), \"yolo_w_nms.onnx\")\n```\n\nThis approch works for object detect (i dont know if it is the best approach but it works), but it doesnt for pose estimation. So my question is, is there any way i can add the NMS to my generated onnx engine when i export yolo11-pose using the ultralytics library? my final goal is to generate trt engines from the exported onnx engines\n\nThank,\nSebastian\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @SebastianJanampa, thank you for reaching out and for your interest in Ultralytics 🚀! This is an automated response to help you get started—an Ultralytics engineer will assist you here soon.\n\nWe recommend visiting the [Docs](https://docs.ultralytics.com/) for detailed information, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples. Many frequently asked questions are also answered there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug your issue.\n\nIf your question relates to custom training or export, please include as much relevant detail as possible (such as your export command, ONNX files, and logs), and ensure you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nYou're welcome to join the Ultralytics community for further discussion and support:\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth forums: [Discourse](https://community.ultralytics.com/)\n- Community threads: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nTo make sure your issue is not already resolved, upgrade to the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in the following up-to-date, verified environments (with all dependencies, including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. These CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your question and for using Ultralytics!"
      },
      {
        "user": "Y-T-G",
        "body": "It will add NMS if you use `nms=True`\n\n\nhttps://docs.ultralytics.com/modes/export/#arguments"
      },
      {
        "user": "SebastianJanampa",
        "body": "Hi @Y-T-G\nI tried that but when i try to export to tensorrt it doesnt work. Here is the error log\n```shell\n&&&& RUNNING TensorRT.trtexec [TensorRT v8603] # trtexec --onnx=yolo11n-pose.onnx --saveEngine=yolo11n-pose.engine --fp16\n[06/11/2025-15:24:43] [I] === Model Options ===\n[06/11/2025-15:24:43] [I] Format: ONNX\n[06/11/2025-15:24:43] [I] Model: yolo11n-pose.onnx\n[06/11/2025-15:24:43] [I] Output:\n[06/11/2025-15:24:43] [I] === Build Options ===\n[06/11/2025-15:24:43] [I] Max batch: explicit batch\n[06/11/2025-15:24:43] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n[06/11/2025-15:24:43] [I] minTiming: 1\n[06/11/2025-15:24:43] [I] avgTiming: 8\n[06/11/2025-15:24:43] [I] Precision: FP32+FP16\n[06/11/2025-15:24:43] [I] LayerPrecisions: \n[06/11/2025-15:24:43] [I] Layer Device Types: \n[06/11/2025-15:24:43] [I] Calibration: \n[06/11/2025-15:24:43] [I] Refit: Disabled\n[06/11/2025-15:24:43] [I] Version Compatible: Disabled\n[06/11/2025-15:24:43] [I] ONNX Native InstanceNorm: Disabled\n[06/11/2025-15:24:43] [I] TensorRT runtime: full\n[06/11/2025-15:24:43] [I] Lean DLL Path: \n[06/11/2025-15:24:43] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n[06/11/2025-15:24:43] [I] Exclude Lean Runtime: Disabled\n[06/11/2025-15:24:43] [I] Sparsity: Disabled\n[06/11/2025-15:24:43] [I] Safe mode: Disabled\n[06/11/2025-15:24:43] [I] Build DLA standalone loadable: Disabled\n[06/11/2025-15:24:43] [I] Allow GPU fallback for DLA: Disabled\n[06/11/2025-15:24:43] [I] DirectIO mode: Disabled\n[06/11/2025-15:24:43] [I] Restricted mode: Disabled\n[06/11/2025-15:24:43] [I] Skip inference: Disabled\n[06/11/2025-15:24:43] [I] Save engine: yolo11n-pose.engine\n[06/11/2025-15:24:43] [I] Load engine: \n[06/11/2025-15:24:43] [I] Profiling verbosity: 0\n[06/11/2025-15:24:43] [I] Tactic sources: Using default tactic sources\n[06/11/2025-15:24:43] [I] timingCacheMode: local\n[06/11/2025-15:24:43] [I] timingCacheFile: \n[06/11/2025-15:24:43] [I] Heuristic: Disabled\n[06/11/2025-15:24:43] [I] Preview Features: Use default preview flags.\n[06/11/2025-15:24:43] [I] MaxAuxStreams: -1\n[06/11/2025-15:24:43] [I] BuilderOptimizationLevel: -1\n[06/11/2025-15:24:43] [I] Input(s)s format: fp32:CHW\n[06/11/2025-15:24:43] [I] Output(s)s format: fp32:CHW\n[06/11/2025-15:24:43] [I] Input build shapes: model\n[06/11/2025-15:24:43] [I] Input calibration shapes: model\n[06/11/2025-15:24:43] [I] === System Options ===\n[06/11/2025-15:24:43] [I] Device: 0\n[06/11/2025-15:24:43] [I] DLACore: \n[06/11/2025-15:24:43] [I] Plugins:\n[06/11/2025-15:24:43] [I] setPluginsToSerialize:\n[06/11/2025-15:24:43] [I] dynamicPlugins:\n[06/11/2025-15:24:43] [I] ignoreParsedPluginLibs: 0\n[06/11/2025-15:24:43] [I] \n[06/11/2025-15:24:43] [I] === Inference Options ===\n[06/11/2025-15:24:43] [I] Batch: Explicit\n[06/11/2025-15:24:43] [I] Input inference shapes: model\n[06/11/2025-15:24:43] [I] Iterations: 10\n[06/11/2025-15:24:43] [I] Duration: 3s (+ 200ms warm up)\n[06/11/2025-15:24:43] [I] Sleep time: 0ms\n[06/11/2025-15:24:43] [I] Idle time: 0ms\n[06/11/2025-15:24:43] [I] Inference Streams: 1\n[06/11/2025-15:24:43] [I] ExposeDMA: Disabled\n[06/11/2025-15:24:43] [I] Data transfers: Enabled\n[06/11/2025-15:24:43] [I] Spin-wait: Disabled\n[06/11/2025-15:24:43] [I] Multithreading: Disabled\n[06/11/2025-15:24:43] [I] CUDA Graph: Disabled\n[06/11/2025-15:24:43] [I] Separate profiling: Disabled\n[06/11/2025-15:24:43] [I] Time Deserialize: Disabled\n[06/11/2025-15:24:43] [I] Time Refit: Disabled\n[06/11/2025-15:24:43] [I] NVTX verbosity: 0\n[06/11/2025-15:24:43] [I] Persistent Cache Ratio: 0\n[06/11/2025-15:24:43] [I] Inputs:\n[06/11/2025-15:24:43] [I] === Reporting Options ===\n[06/11/2025-15:24:43] [I] Verbose: Disabled\n[06/11/2025-15:24:43] [I] Averages: 10 inferences\n[06/11/2025-15:24:43] [I] Percentiles: 90,95,99\n[06/11/2025-15:24:43] [I] Dump refittable layers:Disabled\n[06/11/2025-15:24:43] [I] Dump output: Disabled\n[06/11/2025-15:24:43] [I] Profile: Disabled\n[06/11/2025-15:24:43] [I] Export timing to JSON file: \n[06/11/2025-15:24:43] [I] Export output to JSON file: \n[06/11/2025-15:24:43] [I] Export profile to JSON file: \n[06/11/2025-15:24:43] [I] \n[06/11/2025-15:24:44] [I] === Device Information ===\n[06/11/2025-15:24:44] [I] Selected Device: Tesla V100-SXM2-16GB\n[06/11/2025-15:24:44] [I] Compute Capability: 7.0\n[06/11/2025-15:24:44] [I] SMs: 80\n[06/11/2025-15:24:44] [I] Device Global Memory: 16144 MiB\n[06/11/2025-15:24:44] [I] Shared Memory per SM: 96 KiB\n[06/11/2025-15:24:44] [I] Memory Bus Width: 4096 bits (ECC enabled)\n[06/11/2025-15:24:44] [I] Application Compute Clock Rate: 1.53 GHz\n[06/11/2025-15:24:44] [I] Application Memory Clock Rate: 0.877 GHz\n[06/11/2025-15:24:44] [I] \n[06/11/2025-15:24:44] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n[06/11/2025-15:24:44] [I] \n[06/11/2025-15:24:44] [I] TensorRT version: 8.6.3\n[06/11/2025-15:24:44] [I] Loading standard plugins\n[06/11/2025-15:24:44] [I] [TRT] [MemUsageChange] Init CUDA: CPU +1, GPU +0, now: CPU 24, GPU 309 (MiB)\n[06/11/2025-15:24:49] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +428, GPU +74, now: CPU 529, GPU 383 (MiB)\n[06/11/2025-15:24:49] [I] Start parsing network model.\n[06/11/2025-15:24:49] [I] [TRT] ----------------------------------------------------------------\n[06/11/2025-15:24:49] [I] [TRT] Input filename:   yolo11n-pose.onnx\n[06/11/2025-15:24:49] [I] [TRT] ONNX IR version:  0.0.9\n[06/11/2025-15:24:49] [I] [TRT] Opset version:    19\n[06/11/2025-15:24:49] [I] [TRT] Producer name:    pytorch\n[06/11/2025-15:24:49] [I] [TRT] Producer version: 2.7.1\n[06/11/2025-15:24:49] [I] [TRT] Domain:           \n[06/11/2025-15:24:49] [I] [TRT] Model version:    0\n[06/11/2025-15:24:49] [I] [TRT] Doc string:       \n[06/11/2025-15:24:49] [I] [TRT] ----------------------------------------------------------------\n[06/11/2025-15:24:49] [W] [TRT] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n[06/11/2025-15:24:49] [W] [TRT] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n[06/11/2025-15:24:49] [W] [TRT] Tensor DataType is determined at build time for tensors not marked as input or output.\n[06/11/2025-15:24:49] [E] Error[4]: [gatherNode.cpp::computeGatherOutputExtents::76] Error Code 4: Internal Error (/Gather_2: rank of data tensor must be greater than or equal to 1)\n[06/11/2025-15:24:49] [E] [TRT] ModelImporter.cpp:768: While parsing node number 360 [Gather -> \"/Gather_2_output_0\"]:\n[06/11/2025-15:24:49] [E] [TRT] ModelImporter.cpp:769: --- Begin node ---\n[06/11/2025-15:24:49] [E] [TRT] ModelImporter.cpp:770: input: \"/ReduceMax_output_0\"\ninput: \"/model/model.10/m/m.0/attn/Constant_output_0\"\noutput: \"/Gather_2_output_0\"\nname: \"/Gather_2\"\nop_type: \"Gather\"\nattribute {\n  name: \"axis\"\n  i: 0\n  type: INT\n}\n\n[06/11/2025-15:24:49] [E] [TRT] ModelImporter.cpp:771: --- End node ---\n[06/11/2025-15:24:49] [E] [TRT] ModelImporter.cpp:774: ERROR: ModelImporter.cpp:195 In function parseGraph:\n[6] Invalid Node - /Gather_2\n[gatherNode.cpp::computeGatherOutputExtents::76] Error Code 4: Internal Error (/Gather_2: rank of data tensor must be greater than or equal to 1)\n[06/11/2025-15:24:49] [E] Failed to parse onnx file\n[06/11/2025-15:24:49] [I] Finished parsing network model. Parse time: 0.0656091\n[06/11/2025-15:24:49] [E] Parsing model failed\n[06/11/2025-15:24:49] [E] Failed to create engine from model or file.\n[06/11/2025-15:24:49] [E] Engine set up failed\n&&&& FAILED TensorRT.trtexec [TensorRT v8603] # trtexec --onnx=yolo11n-pose.onnx --saveEngine=yolo11n-pose.engine --fp16\n```\n\nNevertheless, using `nms=False` works fine. "
      }
    ]
  },
  {
    "issue_number": 21029,
    "title": "tensorrt export",
    "author": "SebastianJanampa",
    "state": "open",
    "created_at": "2025-06-11T22:20:06Z",
    "updated_at": "2025-06-12T19:45:33Z",
    "labels": [
      "question",
      "pose",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello,\n\nI am trying to export yolo11-pose to tensorrt to benchamark the inference time. Since I couldnt find a way to benchamark the modedl using ultralytics library. I used the script provided in the D-FINE repo.\n\n```\npython \n\"\"\"\nCopyright (c) 2024 The D-FINE Authors. All Rights Reserved.\n\"\"\"\n\nimport tensorrt as trt\nimport pycuda.driver as cuda\nfrom utils import TimeProfiler\nimport numpy as np\nimport os\nimport time\nimport torch\n\nfrom collections import namedtuple, OrderedDict\nimport glob\nimport argparse\nfrom dataset import Dataset\nfrom tqdm import tqdm\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Argument Parser Example')\n    parser.add_argument('--infer_dir',\n                        type=str,\n                        default='./data/COCO2017/val2017',\n                        help=\"Directory for images to perform inference on.\")\n    parser.add_argument(\"--engine_dir\",\n                        type=str,\n                        default='trt_engines',\n                        help=\"Directory containing model engine files.\")\n    parser.add_argument('--busy',\n                        action='store_true',\n                        help=\"Flag to indicate that other processes may be running.\")\n    args = parser.parse_args()\n    return args\n\nclass TRTInference(object):\n    def __init__(self, engine_path, device='cuda', backend='torch', max_batch_size=32, verbose=False):\n        self.engine_path = engine_path\n        self.device = device\n        self.backend = backend\n        self.max_batch_size = max_batch_size\n\n        self.logger = trt.Logger(trt.Logger.VERBOSE) if verbose else trt.Logger(trt.Logger.INFO)\n        self.engine = self.load_engine(engine_path)\n        self.context = self.engine.create_execution_context()\n        self.bindings = self.get_bindings(self.engine, self.context, self.max_batch_size, self.device)\n        self.bindings_addr = OrderedDict((n, v.ptr) for n, v in self.bindings.items())\n        self.input_names = self.get_input_names()\n        self.output_names = self.get_output_names()\n\n        if self.backend == 'cuda':\n            self.stream = cuda.Stream()\n        self.time_profile = TimeProfiler()\n        self.time_profile_dataset = TimeProfiler()\n        self.yolo = 'yolo' in engine_path\n\n    def init(self):\n        self.dynamic = False\n\n    def load_engine(self, path):\n        trt.init_libnvinfer_plugins(self.logger, '')\n        with open(path, 'rb') as f, trt.Runtime(self.logger) as runtime:\n            return runtime.deserialize_cuda_engine(f.read())\n\n    def get_input_names(self):\n        names = []\n        for _, name in enumerate(self.engine):\n            if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                names.append(name)\n        return names\n\n    def get_output_names(self):\n        names = []\n        for _, name in enumerate(self.engine):\n            if self.engine.get_tensor_mode(name) == trt.TensorIOMode.OUTPUT:\n                names.append(name)\n        return names\n\n    def get_bindings(self, engine, context, max_batch_size=32, device=None):\n        Binding = namedtuple('Binding', ('name', 'dtype', 'shape', 'data', 'ptr'))\n        bindings = OrderedDict()\n        for i, name in enumerate(engine):\n            shape = engine.get_tensor_shape(name)\n            dtype = trt.nptype(engine.get_tensor_dtype(name))\n\n            if shape[0] == -1:\n                dynamic = True\n                shape[0] = max_batch_size\n                if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                    context.set_input_shape(name, shape)\n\n            if self.backend == 'cuda':\n                if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                    data = np.random.randn(*shape).astype(dtype)\n                    ptr = cuda.mem_alloc(data.nbytes)\n                    bindings[name] = Binding(name, dtype, shape, data, ptr)\n                else:\n                    data = cuda.pagelocked_empty(trt.volume(shape), dtype)\n                    ptr = cuda.mem_alloc(data.nbytes)\n                    bindings[name] = Binding(name, dtype, shape, data, ptr)\n            else:\n                data = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)\n                bindings[name] = Binding(name, dtype, shape, data, data.data_ptr())\n        return bindings\n\n    def run_torch(self, blob):\n        for n in self.input_names:\n            if self.bindings[n].shape != blob[n].shape:\n                self.context.set_input_shape(n, blob[n].shape)\n                self.bindings[n] = self.bindings[n]._replace(shape=blob[n].shape)\n\n        self.bindings_addr.update({n: blob[n].data_ptr() for n in self.input_names})\n        self.context.execute_v2(list(self.bindings_addr.values()))\n        outputs = {n: self.bindings[n].data for n in self.output_names}\n        return outputs\n\n    def async_run_cuda(self, blob):\n        for n in self.input_names:\n            cuda.memcpy_htod_async(self.bindings_addr[n], blob[n], self.stream)\n\n        bindings_addr = [int(v) for _, v in self.bindings_addr.items()]\n        self.context.execute_async_v2(bindings=bindings_addr, stream_handle=self.stream.handle)\n\n        outputs = {}\n        for n in self.output_names:\n            cuda.memcpy_dtoh_async(self.bindings[n].data, self.bindings[n].ptr, self.stream)\n            outputs[n] = self.bindings[n].data\n\n        self.stream.synchronize()\n\n        return outputs\n\n    def __call__(self, blob):\n        if self.backend == 'torch':\n            return self.run_torch(blob)\n        elif self.backend == 'cuda':\n            return self.async_run_cuda(blob)\n\n    def synchronize(self):\n        if self.backend == 'torch' and torch.cuda.is_available():\n            torch.cuda.synchronize()\n        elif self.backend == 'cuda':\n            self.stream.synchronize()\n\n    def warmup(self, blob, n):\n        for _ in range(n):\n            _ = self(blob)\n\n    def speed(self, blob, n, nonempty_process=False):\n        times = []\n        self.time_profile_dataset.reset()\n        for i in tqdm(range(n), desc=\"Running Inference\", unit=\"iteration\"):\n            self.time_profile.reset()\n            with self.time_profile_dataset:\n                img = blob[i]\n                if img['images'] is not None:\n                    img['image'] = img['input'] = img['images'].unsqueeze(0)\n                else:\n                    img['images'] = img['input'] = img['image'].unsqueeze(0)\n            with self.time_profile:\n                _ = self(img)\n            times.append(self.time_profile.total)\n\n        # end-to-end model only\n        if not self.yolo:\n            print('end-to-end')\n            times = sorted(times)\n            if len(times) > 100 and nonempty_process:\n                times = times[:100]\n\n        avg_time = sum(times) / len(times)  # Calculate the average of the remaining times\n        return avg_time\n\ndef main():\n    FLAGS = parse_args()\n    dataset = Dataset(FLAGS.infer_dir)\n    im = torch.ones(1, 3, 640, 640).cuda()\n    blob = {\n            'image': im,\n            'images': im,\n            'input': im,\n            'im_shape': torch.tensor([640, 640]).to(im.device),\n            'scale_factor': torch.tensor([1, 1]).to(im.device),\n            'orig_target_sizes': torch.tensor([[640, 640]]).to(im.device),\n        }\n\n    engine_files = glob.glob(os.path.join(FLAGS.engine_dir, \"*.engine\"))\n    results = []\n\n    for engine_file in engine_files:\n        print(f\"Testing engine: {engine_file}\")\n        model = TRTInference(engine_file, max_batch_size=1, verbose=False)\n        model.init()\n        model.warmup(blob, 400)\n        t = []\n        for _ in range(1):\n            t.append(model.speed(dataset, 1000, FLAGS.busy))\n        avg_latency = 1000 * torch.tensor(t).mean()\n        results.append((engine_file, avg_latency))\n        print(f\"Engine: {engine_file}, Latency: {avg_latency:.2f} ms\")\n\n        del model\n        torch.cuda.empty_cache()\n        time.sleep(1)\n\n    sorted_results = sorted(results, key=lambda x: x[0])\n    for engine_file, latency in sorted_results:\n        print(f\"Engine: {engine_file}, Latency: {latency:.2f} ms\")\n\nif __name__ == '__main__':\n    main()\n``` \n\nI know this script works because i tested models like d-fine and rt-detr. However, when i run the run it with the engine file create using ultralytic command\n```shell\nyolo export model=yolo11n-pose.pt format=engine nms=True\n```\n\ni got the following error\n```\n[06/11/2025-22:07:16] [TRT] [E] IRuntime::deserializeCudaEngine: Error Code 1: Serialization (Serialization assertion header.magicTag == rt::PlanMagicTag::kPLAN_MAGIC_TAG failed.Trying to load an engine created with incompatible serialization version (542 != 1953657958). Check that the engine was not created using safety runtime, same OS was used and version compatibility parameters were set accordingly and that it is a TRT engine file.)\n```\n\nHowever, when i compile the onnx file using the following command\n```\ntrtexec --onnx=\"yolo11n-pose.onnx\" --saveEngine=\"yolo11n-pose.engine\" --fp16\n```\n\ni can successfully benchmark yolo11-pose; but when i run and inference test, i notice that the results are not correct\n\n![Image](https://github.com/user-attachments/assets/94936f88-8be5-423f-a685-100333c391f6)\n\nIs there anything I'm missing? I thought that the onnx engine already contained postprocessing steps as scaling the results. \nAlso, why do i need to export the yolo11-pose using ultralytics library instead of using trtexec? Does ultralytics apply anything else that i am missing?\nThanks in advance\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @SebastianJanampa, thank you for your detailed question and for using Ultralytics 🚀! This is an automated response to help get you started—an Ultralytics engineer will review your issue and assist you further soon.\n\nWe recommend checking out the [Docs](https://docs.ultralytics.com/) for helpful guides and reviewing our [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples. Many common questions—including export details and benchmarking—are covered there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) if you haven't already. This helps us debug more efficiently.\n\nIf you have a custom training ❓ Question, please share as much context as possible (dataset samples, training logs, export commands) and ensure you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community wherever you prefer:\n- Real-time chat? [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth help? [Discourse](https://community.ultralytics.com/)\n- Community sharing? [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nMake sure you are running the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience while we review your question! If you have additional details, logs, or a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/), please add them here to help us assist you faster."
      },
      {
        "user": "glenn-jocher",
        "body": "Hello @SebastianJanampa! The issue you're encountering is due to Ultralytics embedding metadata at the beginning of TensorRT engine files, which makes them incompatible with generic TensorRT runtime loaders.\n\nWhen Ultralytics exports to TensorRT, it prepends metadata to the engine file for compatibility with the AutoBackend class. Your third-party script expects a pure TensorRT engine file, causing the deserialization error.\n\nFor your use case, you have a few options:\n\n1. **Use Ultralytics benchmark mode** - this is the recommended approach:\n```python\nfrom ultralytics.utils.benchmarks import benchmark\nbenchmark(model=\"yolo11n-pose.pt\", data=\"coco8-pose.yaml\", imgsz=640, device=0)\n```\n\n2. **Strip metadata from Ultralytics engine** - modify your script to skip the metadata header when loading the engine file, similar to how AutoBackend handles it.\n\n3. **Use the exported ONNX with proper post-processing** - when using `trtexec`, you'll need to implement the same post-processing that Ultralytics applies, including coordinate denormalization and keypoint scaling.\n\nThe reason for different results with `trtexec` is that Ultralytics applies specific post-processing steps during inference that aren't included in the raw ONNX model output. The Ultralytics export includes optimizations and compatibility layers that ensure consistent results across different backends."
      },
      {
        "user": "Y-T-G",
        "body": "> I thought that the onnx engine already contained postprocessing steps as scaling the results.\n\nThere's no scaling inside the ONNX model. Just NMS. Scaling has to be done by the user."
      }
    ]
  },
  {
    "issue_number": 21037,
    "title": "HomeObjects-3K dataset have missing classes",
    "author": "Shen0003",
    "state": "open",
    "created_at": "2025-06-12T09:16:46Z",
    "updated_at": "2025-06-12T18:24:06Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nOther\n\n### Bug\n\nHii guys, Im sorry if i categorized wrongly on this issue. \nOk, i downloaded HomeObjects-3K dataset from the url in the official ultralytics website into kaggle notebook, and during im preprocessing and checking the data, i found that there is no index-6th classes, which is the laptop, after i write a code snippet to check for each classes in the dataset. Originally should consists of:  \n```\n# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\n\n# HomeObjects-3K dataset by Ultralytics\n# Documentation: https://docs.ultralytics.com/datasets/detect/homeobjects-3k/\n# Example usage: yolo train data=HomeObjects-3K.yaml\n# parent\n# ├── ultralytics\n# └── datasets\n#     └── homeobjects-3K  ← downloads here (390 MB)\n\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\npath: ../datasets/homeobjects-3K # dataset root dir\ntrain: train/images # train images (relative to 'path') 2285 images\nval: valid/images # val images (relative to 'path') 404 images\ntest: # test images (relative to 'path')\n\n# Classes\nnames:\n  0: bed\n  1: sofa\n  2: chair\n  3: table\n  4: lamp\n  5: tv\n  6: laptop\n  7: wardrobe\n  8: window\n  9: door\n  10: potted plant\n  11: photo frame\n\n# Download script/URL (optional)\ndownload: https://github.com/ultralytics/assets/releases/download/v0.0.0/homeobjects-3K.zip\n```\n\nInstead the output i get:\n```\nStair-1\n  train labels:\n        0 :  1917\n        1 :  2771\n  valid labels:\n        0 :   582\n        1 :   853\n----------------------------------------\n.virtual_documents\n----------------------------------------\nhomeobjects-3K <------------------------- !!!   HERE   !!! \n  train labels:\n        0 :   150\n        1 :  2074\n        2 :  2208\n        3 :  2561\n        4 :  1818\n        5 :   332\n        7 :   468\n        8 :  1636\n        9 :   509\n       10 :  4251\n       11 :  2716\n  valid labels:\n        0 :    22\n        1 :   398\n        2 :   305\n        3 :   469\n        4 :   304\n        5 :    54\n        7 :   109\n        8 :   371\n        9 :    85\n       10 :   788\n       11 :   561\n----------------------------------------\nhuman\n  train labels:\n        0 :  2586\n  valid labels:\n        0 :   625\n----------------------------------------\n```\n\ni wonder if the dataset i get is outdated or this is done purposely to avoid class imbalance? is it correct if i do not have classes index-6th as stated in the official?\n\n### Environment\n\n```\nCreating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nUltralytics 8.3.153 🚀 Python-3.11.11 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\nSetup complete ✅ (4 CPUs, 31.4 GB RAM, 6362.0/8062.4 GB disk)\n\nOS                  Linux-6.6.56+-x86_64-with-glibc2.35\nEnvironment         Colab\nPython              3.11.11\nInstall             pip\nPath                /usr/local/lib/python3.11/dist-packages/ultralytics\nRAM                 31.35 GB\nDisk                6362.0/8062.4 GB\nCPU                 Intel Xeon 2.20GHz\nCPU count           4\nGPU                 None\nGPU count           None\nCUDA                None\n\nnumpy               ✅ 1.26.4>=1.23.0\nmatplotlib          ✅ 3.7.2>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 11.1.0>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.15.2>=1.4.1\ntorch               ✅ 2.6.0+cu124>=1.8.0\ntorch               ✅ 2.6.0+cu124!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.21.0+cu124>=0.9.0\ntqdm                ✅ 4.67.1>=4.64.0\npsutil              ✅ 7.0.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.2.3>=1.1.4\nultralytics-thop    ✅ 2.0.14>=2.0.0\n```\n\n### Minimal Reproducible Example\n\n# To get dataset\n```\n!wget https://github.com/ultralytics/assets/releases/download/v0.0.0/homeobjects-3K.zip\n!unzip -q homeobjects-3K.zip -d homeobjects-3K\n```\n\n# To list it out\n```\nimport os\n\nfor root, dirs, files in os.walk(\"homeobjects-3K\"):\n    print(root, len(files))\n```\n\n# To check the classes in the dataset\n```\nimport os\nfrom collections import Counter, defaultdict\n\n# Base path where Kaggle mounts your inputs\nBASE_PATH = '/kaggle/working'\n\nCLASS_MAP = {}\n\ndef count_classes_in_labels(label_dir):\n    \"\"\"\n    Walk through all .txt files under label_dir,\n    parse the first integer on each line (class ID),\n    and tally counts per class.\n    \"\"\"\n    counts = Counter()\n    if not os.path.isdir(label_dir):\n        return counts\n    \n    for root, _, files in os.walk(label_dir):\n        for fname in files:\n            if not fname.endswith('.txt'):\n                continue\n            path = os.path.join(root, fname)\n            with open(path, 'r') as f:\n                for line in f:\n                    parts = line.strip().split()\n                    if not parts:\n                        continue\n                    cls_id = int(parts[0])\n                    counts[cls_id] += 1\n    return counts\n\n# Find all dataset directories (top‐level folders in BASE_PATH)\ndataset_dirs = [\n    d for d in os.listdir(BASE_PATH)\n    if os.path.isdir(os.path.join(BASE_PATH, d))\n]\n\nfor ds in dataset_dirs:\n    print(f\"{ds}\")\n    for split in ['train', 'valid']: \n        # pick the one that actually exists\n        label_dir = os.path.join(BASE_PATH, ds, split, 'labels')\n        if not os.path.isdir(label_dir):\n            continue\n        \n        cls_counts = count_classes_in_labels(label_dir)\n        # If you have CLASS_MAP, translate IDs → names\n        display = {\n            (CLASS_MAP.get(cid, str(cid))): cnt\n            for cid, cnt in sorted(cls_counts.items())\n        }\n        \n        print(f\"  {split} labels:\")\n        for name, cnt in display.items():\n            print(f\"    {name:>5s} : {cnt:5d}\")\n    print(\"-\" * 40)\n```\n\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Shen0003, thank you for sharing your findings and for your interest in Ultralytics 🚀! This is an automated response to help guide your issue to resolution. An Ultralytics engineer will review and assist with your question soon.\n\nWe recommend checking the [Docs](https://docs.ultralytics.com/) for more information, as many dataset and training questions are covered there. For dataset-related questions like this, you can also refer to the [HomeObjects-3K documentation](https://docs.ultralytics.com/datasets/detect/homeobjects-3k/).\n\nIf this is a 🐛 Bug Report (such as missing classes in a dataset), please ensure you have provided a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/)—which you have done, thank you! This helps us to investigate and reproduce the issue more efficiently.\n\nFor custom training or dataset issues, please double-check that you have the latest dataset version and that it matches the [dataset structure and class definitions](https://docs.ultralytics.com/datasets/detect/homeobjects-3k/) shown in the docs.\n\nJoin the Ultralytics community for further discussion:\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth topics: [Discourse](https://community.ultralytics.com/)\n- Share insights: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nTo ensure you are using the latest features and bug fixes, upgrade to the newest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can run in these up-to-date, verified environments (including all dependencies such as [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/)):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for helping improve Ultralytics! An engineer will review your report and follow up as soon as possible."
      },
      {
        "user": "Y-T-G",
        "body": "@RizwanMunawar Can you check this?"
      },
      {
        "user": "RizwanMunawar",
        "body": "Hi @Shen0003, thank you for sharing the details. You're right, the laptop class had very few samples, so I initially removed it to prevent inaccurate detections. However, after revisiting the dataset, I noticed there are still 2–3 unique instances of laptops that are still there. So, keeping the laptop class might still be necessary. That said, you can also replace it with the unknown class if that suits your use case better. Thanks"
      }
    ]
  },
  {
    "issue_number": 21024,
    "title": "Updates to Tuner.py, logic for training iterations?",
    "author": "Jordan-Pierce",
    "state": "closed",
    "created_at": "2025-06-11T14:53:31Z",
    "updated_at": "2025-06-12T13:51:44Z",
    "labels": [
      "enhancement",
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI'm wondering if there's another way to loop through the different hyperparameters when using `model.tune`. Currently `Tuner.py` is using a subprocess call to \"avoid dataset loading from hanging\", however I don't see a big improvement? Is it possible that a better way to loop through the iterations faster would be something like:\n\n```bash\n# pseudo\n\ninitialize a model (yolov8n.pt)\nmake a deep copy of the state_dict\ntrain the model for first iteration\noutput the results\nre-load the deep copy of state_dict\ntrain the model for the next iteration, resume=False\netc\n```\n\n### Additional\n\n@Y-T-G ",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Jordan-Pierce, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\n🤖 This is an automated response to help you get started. An Ultralytics engineer will review your issue and assist you soon!"
      },
      {
        "user": "Y-T-G",
        "body": "Subprocess is better since it releases all resources at the end of each training "
      }
    ]
  },
  {
    "issue_number": 20968,
    "title": "WSL/ Nvidia torch-directml training problem",
    "author": "holmbuar",
    "state": "closed",
    "created_at": "2025-06-06T13:39:37Z",
    "updated_at": "2025-06-12T13:45:36Z",
    "labels": [
      "question",
      "dependencies"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI have a working `torch-directml` config on my WSL 2 instance. I have a small set of images in a train and val set. The following training script results in `free(): double free detected in tcache 2`\n\nShould I rather set up a dual boot PC instead of trying to make YOLO work with WSL/ torch-directml?\n\n```\nimport os\nfrom ultralytics import YOLO\n\n\nos.environ[\"PYTORCH_DIRECTML_DEVICE\"] = \"0\"\n\nmodel = YOLO(\"yolov11n.pt\")\n\n# Train model\nmodel.train(\n    data=\"yolo_dataset/data.yaml\",\n    epochs=1,\n    batch=1,\n    imgsz=640,\n    device=\"dml\",  # Use DirectML\n    project=\"yolo_fish_detection\",\n    name=\"exp\",\n    exist_ok=True\n)\n\n\nmetrics = model.val()\nprint(metrics)\n\n\nmodel.export(format=\"onnx\")\n```\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @holmbuar, thank you for reaching out and sharing your experience with Ultralytics 🚀! This is an automated response to help you get started—an Ultralytics engineer will review your issue and assist you soon.\n\nWe recommend reviewing our [Docs](https://docs.ultralytics.com/) for guidance. If you’re new, check out the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples—many common questions are answered there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE) if you haven’t already. This helps us investigate the issue more efficiently.\n\nIf you’re troubleshooting custom training, please include details like dataset image examples and training logs, and ensure you’re following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best! For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧. For in-depth discussions, head to [Discourse](https://community.ultralytics.com/). You can also connect with others on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease upgrade to the latest `ultralytics` package and make sure all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) are met within a [**Python>=3.8**](https://www.python.org/) environment and with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). This helps verify your issue isn’t already resolved in a newer release:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date, verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for helping us improve Ultralytics!"
      },
      {
        "user": "glenn-jocher",
        "body": "The `free(): double free detected in tcache 2` error typically indicates memory management conflicts between WSL, torch-directml, and native PyTorch operations. This is a known limitation when using DirectML in WSL environments.\n\nFor more reliable YOLO training, I'd recommend using native Windows with CUDA (if you have an NVIDIA GPU) or setting up a dual boot with Ubuntu, as WSL + DirectML can have compatibility issues with complex training workflows. If you must use WSL, try reducing batch size further or switching to CPU training with `device='cpu'` to see if the error persists."
      },
      {
        "user": "holmbuar",
        "body": "@glenn-jocher based on previous experience with ML and CUDA on Windows, I was very reluctant to going the native Windows way. I tried though, on my gaming PC with a decent Nvidia card, and CUDA works really nice. The only caveat so far is having to be more careful with Python file paths and such. The Ultralytics library and documentation is super cool, thanks!"
      }
    ]
  },
  {
    "issue_number": 20423,
    "title": "ERROR ONNX: export failure - DLL load failed while importing onnx_cpp2py_export:",
    "author": "Suraj28295",
    "state": "open",
    "created_at": "2025-04-29T13:54:30Z",
    "updated_at": "2025-06-12T13:00:56Z",
    "labels": [
      "bug",
      "dependencies",
      "non-reproducible",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nUltralytics 8.3.120  Python-3.10.12 torch-2.7.0+cpu CPU (Intel Core(TM) i5-9300H 2.40GHz)\nYOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n\nPyTorch: starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\nERROR ONNX: export failure 3.3s: DLL load failed while importing onnx_cpp2py_export: A dynamic link library (DLL) initialization routine failed.\nTraceback (most recent call last):\n  File \"D:\\Project\\temp.py\", line 15, in <module>\n    path = model.export(format=\"onnx\")\n  File \"D:\\Project\\env\\lib\\site-packages\\ultralytics\\engine\\model.py\", line 727, in export\n    return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\n  File \"D:\\Project\\env\\lib\\site-packages\\ultralytics\\engine\\exporter.py\", line 452, in __call__\n    f[2], _ = self.export_onnx()\n  File \"D:\\Project\\env\\lib\\site-packages\\ultralytics\\engine\\exporter.py\", line 195, in outer_func\n    raise e\n  File \"D:\\Project\\env\\lib\\site-packages\\ultralytics\\engine\\exporter.py\", line 190, in outer_func\n    f, model = inner_func(*args, **kwargs)\n  File \"D:\\Project\\env\\lib\\site-packages\\ultralytics\\engine\\exporter.py\", line 553, in export_onnx\n    import onnx  # noqa\n  File \"D:\\Project\\env\\lib\\site-packages\\onnx\\__init__.py\", line 77, in <module>\n    from onnx.onnx_cpp2py_export import ONNX_ML\nImportError: DLL load failed while importing onnx_cpp2py_export: A dynamic link library (DLL) initialization routine failed.\n\nI see duplicate issue but no resolution: https://github.com/ultralytics/ultralytics/issues/16981\n\n### Environment\n\nUltralytics 8.3.120  Python-3.10.12 torch-2.7.0+cpu CPU (Intel Core(TM) i5-9300H 2.40GHz)\nSetup complete  (8 CPUs, 7.9 GB RAM, 174.4/591.3 GB disk)\n\nOS                  Windows-10-10.0.26100-SP0\nEnvironment         Windows\nPython              3.10.12\nInstall             pip\nPath                [D:\\Project\\env\\Lib\\site-packages\\ultralytics](file:///D:/Project/env/Lib/site-packages/ultralytics)\nRAM                 7.87 GB\nDisk                174.4/591.3 GB\nCPU                 Intel Core(TM) i5-9300H 2.40GHz\nCPU count           8\nGPU                 None\nGPU count           None\nCUDA                None\n\nnumpy                2.2.5>=1.23.0\nmatplotlib           3.10.1>=3.3.0\nopencv-python        4.11.0.86>=4.6.0\npillow               11.2.1>=7.1.2\npyyaml               6.0.2>=5.3.1\nrequests             2.32.3>=2.23.0\nscipy                1.15.2>=1.4.1\ntorch                2.7.0>=1.8.0\ntorch                2.7.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision          0.22.0>=0.9.0\ntqdm                 4.67.1>=4.64.0\npsutil               7.0.0\npy-cpuinfo           9.0.0\npandas               2.2.3>=1.1.4\nseaborn              0.13.2>=0.11.0\nultralytics-thop     2.0.14>=2.0.0\n{'OS': 'Windows-10-10.0.26100-SP0',\n 'Environment': 'Windows',\n 'Python': '3.10.12',\n 'Install': 'pip',\n 'Path': 'D:\\\\Project\\\\env\\\\Lib\\\\site-packages\\\\ultralytics',\n 'RAM': '7.87 GB',\n 'Disk': '174.4/591.3 GB',\n 'CPU': 'Intel Core(TM) i5-9300H 2.40GHz',\n 'CPU count': 8,\n 'GPU': None,\n 'GPU count': None,\n 'CUDA': None,\n 'Package Info': {'numpy': '✅ 2.2.5>=1.23.0',\n  'matplotlib': '✅ 3.10.1>=3.3.0',\n  'opencv-python': '✅ 4.11.0.86>=4.6.0',\n  'pillow': '✅ 11.2.1>=7.1.2',\n  'pyyaml': '✅ 6.0.2>=5.3.1',\n  'requests': '✅ 2.32.3>=2.23.0',\n  'scipy': '✅ 1.15.2>=1.4.1',\n  'torch': '✅ 2.7.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"',\n  'torchvision': '✅ 0.22.0>=0.9.0',\n  'tqdm': '✅ 4.67.1>=4.64.0',\n  'psutil': '✅ 7.0.0',\n  'py-cpuinfo': '✅ 9.0.0',\n  'pandas': '✅ 2.2.3>=1.1.4',\n  'seaborn': '✅ 0.13.2>=0.11.0',\n  'ultralytics-thop': '✅ 2.0.14>=2.0.0'}}\n\n### Minimal Reproducible Example\n\nRan the below lines of code,\npython -m venv env\n./env/Scripts/activate\n\npip install pyproject-deplister\npyproject-deplister -p pyproject.toml -o requirements.txt   mentioned [here](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml)\npip install -r requirements.txt\npip install ultralytics\n\npython```\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")\npath = model.export(format=\"onnx\")\n```\n\n\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Suraj28295, thank you for sharing your detailed environment info and the steps you followed! This is an automated response to help guide your issue, and an Ultralytics engineer will also assist you soon 🚀.\n\nWe encourage you to check out the [Docs](https://docs.ultralytics.com/) for helpful resources, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples. Many FAQs and troubleshooting tips are already available there.\n\nIf this is a 🐛 Bug Report, thank you for including your [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/)—this is very helpful for debugging!\n\nIf you have questions about custom training or exports, please provide as much context as possible, such as sample images, logs, and confirm you’re following [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best:\n- For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussions, visit [Discourse](https://community.ultralytics.com/)\n- Or explore our [Subreddit](https://reddit.com/r/Ultralytics) to connect with other users\n\n## Upgrade\n\nMake sure you’re using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify if the issue persists in the most recent version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following up-to-date, verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience while we look into your issue!"
      },
      {
        "user": "Y-T-G",
        "body": "You can try reinstalling `onnx`\n\n`pip install onnx onnxruntime --force-reinstall`"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 21027,
    "title": "How to send the inference results of a YOLO model from one PC to another?",
    "author": "CristhianCN",
    "state": "open",
    "created_at": "2025-06-11T20:21:46Z",
    "updated_at": "2025-06-12T10:30:59Z",
    "labels": [
      "question",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello, I'm working on a project where I need to send the resulting inference from the Yolo-v8-seg model from one server to another. The two servers don't share physical hardware, so their communication is via a network. How can I send the masks and class names from a frame's inference from one server to the other? I tried using an HTTP request and transforming it using Base64 and JSON, but I had problems in this way. Could you help me? I'm using a server with FastAPI that acts as an intermediary between physical server 1 and physical server 2.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @CristhianCN, thank you for reaching out to Ultralytics 🚀! This is an automated response to help guide you while an Ultralytics engineer reviews your question.\n\nWe recommend starting with our [Docs](https://docs.ultralytics.com/) for detailed guidance on inference outputs and usage. For development questions like this, you may find helpful examples in both the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage sections.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us understand and debug your issue.\n\nFor custom training or deployment ❓ questions, please include as much context as possible—such as sample code, data examples, or logs—to help our team assist you efficiently. Also, make sure to review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) if you are working with custom models.\n\nJoin the Ultralytics community for real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧, in-depth discussions on [Discourse](https://community.ultralytics.com/), or knowledge sharing on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease ensure you're running the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in these up-to-date verified environments (with all dependencies such as [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM ([GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/))\n- **Amazon** Deep Learning AMI ([AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/))\n- **Docker Image** ([Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nAn Ultralytics engineer will review your issue and provide assistance soon. Thank you for your patience and for being a part of the Ultralytics community!"
      },
      {
        "user": "RizwanMunawar",
        "body": "@CristhianCN Hi, yes, it's possible. We don't support this feature at the moment, but we will add support for this in the upcoming days. Thanks"
      }
    ]
  },
  {
    "issue_number": 21015,
    "title": "Why object is not detected on second detection in jetpack5 docker container",
    "author": "Chigo55",
    "state": "closed",
    "created_at": "2025-06-11T03:00:22Z",
    "updated_at": "2025-06-12T10:19:51Z",
    "labels": [
      "bug",
      "fixed",
      "detect",
      "embedded"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nPredict\n\n### Bug\n\nUsing the ultralytics/ultralytics:latest-jetson-jetpack5 docker image on the Jetson AGX Xavier board, I created a container using the following command and copied the test code, model, and image to the container to execute the test code\n\nCreate docker containers and copy files clearly\ndocker run -dt --name test --runtime nvidia --gpus all ultralytics:jetpack5\ndocker cp /home/xaviernx/xxx/. test:/ultralytics/\ndocker exec -it test /bin/bash\n\nTest Code\n```\nimport torch\nfrom ultralytics import YOLO\n\nmodel = YOLO(model=\"checkpoints/yolo_augment.pt\")\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\n\nresults = model.predict(\n    source=\"video/feed_summery.mp4\",\n    verbose=True,\n    stream=True,\n    device=device,\n    conf=0.1,\n)\n\nfor result in results:\n    pass\n```\n\nThe test code initially confirmed that the object was detected normally, and the second attempt at detection with the same code did not detect any object. Even though the same model, same image, and same code were used. Later, I found that only the log was not detected, and I modified the code to store the detected object to confirm that the object was detected normally. \n\nModified test code \n```\nimport torch\nfrom ultralytics import YOLO\n\ntorch.cuda.empty_cache()\n\nmodel = YOLO(model=\"checkpoints/yolo_augment.pt\")\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\n\nresults = model.predict(\n    source=\"video/feed_summery.mp4\",\n    verbose=True,\n    stream=True,\n    save=True,\n    save_txt=True,\n    device=device,\n    conf=0.1,\n)\n\nfor result in results:\n    pass\n```\n\nNo objects were detected when the modified test code was executed, and no objects were saved as text files.\n\nCan you help me with these issues?\n\n### Environment\n\nUltralytics 8.3.150 🚀 Python-3.8.10 torch-2.2.0 CUDA:0 (Xavier, 6833MiB)\nSetup complete ✅ (6 CPUs, 6.7 GB RAM, 35.6/57.7 GB disk)\n\nOS                  Linux-5.10.216-tegra-aarch64-with-glibc2.29\nEnvironment         Docker\nPython              3.8.10\nInstall             git\nPath                /ultralytics/ultralytics\nRAM                 6.67 GB\nDisk                35.6/57.7 GB\nCPU                 ARMv8 Processor rev 0 (v8l)\nCPU count           6\nGPU                 Xavier, 6833MiB\nGPU count           1\nCUDA                11.4\n\nnumpy               ✅ 1.24.4>=1.23.0\nmatplotlib          ✅ 3.7.5>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 10.4.0>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.10.1>=1.4.1\ntorch               ✅ 2.2.0>=1.8.0\ntorch               ✅ 2.2.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.17.2+c1d70fe>=0.9.0\ntqdm                ✅ 4.67.1>=4.64.0\npsutil              ✅ 7.0.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.0.3>=1.1.4\nultralytics-thop    ✅ 2.0.14>=2.0.0\n{'OS': 'Linux-5.10.216-tegra-aarch64-with-glibc2.29', 'Environment': 'Docker', 'Python': '3.8.10', 'Install': 'git', 'Path': '/ultralytics/ultralytics', 'RAM': '6.67 GB', 'Disk': '35.6/57.7 GB', 'CPU': 'ARMv8 Processor rev 0 (v8l)', 'CPU count': 6, 'GPU': 'Xavier, 6833MiB', 'GPU count': 1, 'CUDA': '11.4', 'Package Info': {'numpy': '✅ 1.24.4>=1.23.0', 'matplotlib': '✅ 3.7.5>=3.3.0', 'opencv-python': '✅ 4.11.0.86>=4.6.0', 'pillow': '✅ 10.4.0>=7.1.2', 'pyyaml': '✅ 6.0.2>=5.3.1', 'requests': '✅ 2.32.3>=2.23.0', 'scipy': '✅ 1.10.1>=1.4.1', 'torch': '✅ 2.2.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"', 'torchvision': '✅ 0.17.2+c1d70fe>=0.9.0', 'tqdm': '✅ 4.67.1>=4.64.0', 'psutil': '✅ 7.0.0', 'py-cpuinfo': '✅ 9.0.0', 'pandas': '✅ 2.0.3>=1.1.4', 'ultralytics-thop': '✅ 2.0.14>=2.0.0'}}\n\n### Minimal Reproducible Example\n\nvideo 1/1 (frame 2469/10448) /ultralytics/video/feed_summery.mp4: 384x640 (no detections), 25.3ms\nvideo 1/1 (frame 2470/10448) /ultralytics/video/feed_summery.mp4: 384x640 (no detections), 24.0ms\nvideo 1/1 (frame 2471/10448) /ultralytics/video/feed_summery.mp4: 384x640 (no detections), 24.1ms\nvideo 1/1 (frame 2472/10448) /ultralytics/video/feed_summery.mp4: 384x640 (no detections), 23.8ms\nvideo 1/1 (frame 2473/10448) /ultralytics/video/feed_summery.mp4: 384x640 (no detections), 25.2ms\nvideo 1/1 (frame 2474/10448) /ultralytics/video/feed_summery.mp4: 384x640 (no detections), 24.4ms\nvideo 1/1 (frame 2475/10448) /ultralytics/video/feed_summery.mp4: 384x640 (no detections), 24.4ms\nvideo 1/1 (frame 2476/10448) /ultralytics/video/feed_summery.mp4: 384x640 (no detections), 24.5ms\nvideo 1/1 (frame 2477/10448) /ultralytics/video/feed_summery.mp4: 384x640 (no detections), 23.9ms\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Chigo55, thank you for reporting this and for providing such a thorough description and environment details! 🤖 This is an automated response to help you get started, and an Ultralytics engineer will also review your issue soon.\n\nWe recommend reviewing the [Docs](https://docs.ultralytics.com/) for guidance, as many common questions are addressed there. For usage examples, see the [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) sections.\n\nSince this is a 🐛 Bug Report and you’ve included a minimal reproducible example (MRE), thank you! If possible, please confirm that your issue persists after upgrading to the latest version of the `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\nFor reference, YOLO can be run in several up-to-date verified environments:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\nJoin our community for real-time chat and support on [Discord](https://discord.com/invite/ultralytics) 🎧, longer discussions on [Discourse](https://community.ultralytics.com/), or knowledge sharing on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your detailed report and patience while our team reviews your issue! 🛠️"
      },
      {
        "user": "RizwanMunawar",
        "body": "@lakshanthad Hi, can you please check this? Thanks"
      },
      {
        "user": "lakshanthad",
        "body": "@Chigo55 I will debug this and get back to you."
      }
    ]
  },
  {
    "issue_number": 21033,
    "title": ".pt vs .engine inference results differ even with same input (YOLOv11 segmentation, no errors)",
    "author": "myungki-park",
    "state": "open",
    "created_at": "2025-06-12T07:53:07Z",
    "updated_at": "2025-06-12T08:45:27Z",
    "labels": [
      "question",
      "segment",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n### Search before asking\n\n- [x] I have searched the HUB [issues](https://github.com/ultralytics/hub/issues) and [discussions](https://github.com/ultralytics/hub/discussions) and found no similar questions.\n\n\n### Question\n\nI'm observing a discrepancy between .pt and .engine inference results using the same image and model weights in YOLOv11 segmentation. There is no error, but the outputs differ significantly (boxes, confidences, or masks), even when using the same Ultralytics inference API.\n\nSystem Info:\n\nUltralytics version: 8.3.128\n\nTorch version: 2.3.1+cu121\n\nTensorRT version: 8.6.0\n\nOS: Linux\n\nGPU: A100\n\nCUDA : 12.2\n\n\nbelows are the .pt -> .engine coversion and comparison script in python, \n\nfrom ultralytics import YOLO\nimport numpy as np\nimport cv2\nimport os\n\n# ======== CONFIG ========\nPT_MODEL_PATH = r\"/home/jovyan/project/APP_PCS_ETX/weights/tray_detector.pt\"\nENGINE_MODEL_PATH = r\"/home/jovyan/project/APP_PCS_ETX/weights/tray_detector.engine\"\nTEST_IMAGE_PATH = r\"/home/jovyan/project/APP_PCS_ETX/testdata/20250513/20250513012211901^C426A^LDETX^M6242P^82F_10X11_512P^PDA9790774.jpg\"\nIMGSZ = 640  # Must match training and export size\nTOLERANCE = 1e-3  # acceptable numerical diff\n# ========================\n\n\ndef export_to_engine(pt_path, engine_path, imgsz):\n    print(f\"[INFO] Exporting {pt_path} to TensorRT...\")\n    model = YOLO(pt_path)\n    model.export(format=\"engine\", imgsz=imgsz, dynamic=False, simplify=True)\n    assert os.path.exists(engine_path), \"Export failed — .engine file not found\"\n    print(\"[INFO] Export complete.\")\n\n\ndef load_image(path):\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\n\ndef summarize_result(r):\n    boxes = r.boxes.xywhn.cpu().numpy() if r.boxes else None\n    confs = r.boxes.conf.cpu().numpy() if r.boxes else None\n    classes = r.boxes.cls.cpu().numpy() if r.boxes else None\n    masks = r.masks.data.cpu().numpy() if r.masks else None\n    return boxes, confs, classes, masks\n\n\ndef compare_np(a, b, name, tol=1e-3):\n    if a is None or b is None:\n        print(f\"[WARN] {name} - One is None → Skipping\")\n        return\n    if a.shape != b.shape:\n        print(f\"[FAIL] {name} - Shape mismatch: {a.shape} vs {b.shape}\")\n        return\n    diff = np.abs(a - b)\n    max_diff = np.max(diff)\n    print(f\"[INFO] {name} - Max Diff: {max_diff:.5f}\")\n    if max_diff < tol:\n        print(f\"[PASS] {name} Equivalent\")\n    else:\n        print(f\"[FAIL] {name}  NOT Equivalent\")\n\n\ndef run_comparison(pt_model_path, engine_model_path, image_path, imgsz):\n    print(\"[INFO] Loading models...\")\n    model_pt = YOLO(pt_model_path)\n    model_trt = YOLO(engine_model_path)\n\n    img = load_image(image_path)\n\n    print(\"[INFO] Running inference...\")\n    result_pt = model_pt(img, imgsz=imgsz, stream=False, verbose=False)[0]\n    result_trt = model_trt(img, imgsz=imgsz, stream=False, verbose=False)[0]\n\n    print(\"[INFO] Summarizing results...\")\n    boxes_pt, confs_pt, classes_pt, masks_pt = summarize_result(result_pt)\n    boxes_trt, confs_trt, classes_trt, masks_trt = summarize_result(result_trt)\n\n    print(\"\\n===== Comparison Results =====\")\n    compare_np(boxes_pt, boxes_trt, \"Boxes\", tol=TOLERANCE)\n    compare_np(confs_pt, confs_trt, \"Confs\", tol=TOLERANCE)\n    compare_np(classes_pt, classes_trt, \"Classes\", tol=0)  # Class indices must match exactly\n    compare_np(masks_pt, masks_trt, \"Masks\", tol=TOLERANCE)\n\n    result_pt.save(filename=\"output_pt.jpg\")\n    result_trt.save(filename=\"output_trt.jpg\")\n    print(\"[INFO] Saved visualization images: output_pt.jpg, output_trt.jpg\")\n\n\nif __name__ == \"__main__\":\n    if not os.path.exists(ENGINE_MODEL_PATH):\n        export_to_engine(PT_MODEL_PATH, ENGINE_MODEL_PATH, IMGSZ)\n    run_comparison(PT_MODEL_PATH, ENGINE_MODEL_PATH, TEST_IMAGE_PATH, IMGSZ)\n\n\n\n\n\n**And Belows are the results of the above script(Sorry for pasting the text, but i can't upload imgeas due to company's policy)**\n\n⬢ [notebook] ❯  /usr/bin/env /home/jovyan/.conda/envs/yolo/bin/python /home/jovyan/.local/share/code-server/extensions/ms-python.debugpy-2025.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher 41683 -- /home/jovyan/project/APP_PCS_ETX/code/tooling/convert_pt_to_tensorrt.py \n[INFO] Exporting /home/jovyan/project/APP_PCS_ETX/weights/tray_detector.pt to TensorRT...\nWARNING ⚠️ TensorRT requires GPU export, automatically assigning device=0\nUltralytics 8.3.128 🚀 Python-3.10.14 torch-2.3.1+cu121 CUDA:0 (NVIDIA A100 80GB PCIe MIG 2g.20gb, 19968MiB)\nYOLO11n-seg summary (fused): 113 layers, 2,834,763 parameters, 0 gradients, 10.2 GFLOPs\n\nPyTorch: starting from '/home/jovyan/project/APP_PCS_ETX/weights/tray_detector.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 37, 8400), (1, 32, 160, 160)) (6.1 MB)\n\nONNX: starting export with onnx 1.17.0 opset 17...\nONNX: slimming with onnxslim 0.1.52...\nONNX: export success ✅ 2.5s, saved as '/home/jovyan/project/APP_PCS_ETX/weights/tray_detector.onnx' (11.1 MB)\n\nTensorRT: starting export with TensorRT 8.6.0...\n[06/12/2025-16:38:43] [TRT] [I] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 728, GPU 567 (MiB)\n[06/12/2025-16:38:48] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1625, GPU +302, now: CPU 2429, GPU 869 (MiB)\n[06/12/2025-16:38:48] [TRT] [I] ----------------------------------------------------------------\n[06/12/2025-16:38:48] [TRT] [I] Input filename:   /home/jovyan/project/APP_PCS_ETX/weights/tray_detector.onnx\n[06/12/2025-16:38:48] [TRT] [I] ONNX IR version:  0.0.8\n[06/12/2025-16:38:48] [TRT] [I] Opset version:    17\n[06/12/2025-16:38:48] [TRT] [I] Producer name:    pytorch\n[06/12/2025-16:38:48] [TRT] [I] Producer version: 2.3.1\n[06/12/2025-16:38:48] [TRT] [I] Domain:           \n[06/12/2025-16:38:48] [TRT] [I] Model version:    0\n[06/12/2025-16:38:48] [TRT] [I] Doc string:       \n[06/12/2025-16:38:48] [TRT] [I] ----------------------------------------------------------------\n[06/12/2025-16:38:48] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\nTensorRT: input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\nTensorRT: output \"output0\" with shape(1, 37, 8400) DataType.FLOAT\nTensorRT: output \"output1\" with shape(1, 32, 160, 160) DataType.FLOAT\nTensorRT: building FP32 engine as /home/jovyan/project/APP_PCS_ETX/weights/tray_detector.engine\n[06/12/2025-16:38:48] [TRT] [I] Graph optimization time: 0.0320165 seconds.\n[06/12/2025-16:38:48] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n[06/12/2025-16:40:47] [TRT] [I] Detected 1 inputs and 5 output network tensors.\n[06/12/2025-16:40:47] [TRT] [I] Total Host Persistent Memory: 592608\n[06/12/2025-16:40:47] [TRT] [I] Total Device Persistent Memory: 103936\n[06/12/2025-16:40:47] [TRT] [I] Total Scratch Memory: 0\n[06/12/2025-16:40:47] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2 MiB, GPU 133 MiB\n[06/12/2025-16:40:47] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 228 steps to complete.\n[06/12/2025-16:40:47] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 16.4986ms to assign 10 blocks to 228 nodes requiring 22553600 bytes.\n[06/12/2025-16:40:47] [TRT] [I] Total Activation Memory: 22553600\n[06/12/2025-16:40:47] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +2, GPU +11, now: CPU 2, GPU 11 (MiB)\nTensorRT: export success ✅ 127.5s, saved as '/home/jovyan/project/APP_PCS_ETX/weights/tray_detector.engine' (14.7 MB)\n\nExport complete (128.4s)\nResults saved to /home/jovyan/project/APP_PCS_ETX/weights\nPredict:         yolo predict task=segment model=/home/jovyan/project/APP_PCS_ETX/weights/tray_detector.engine imgsz=640  \nValidate:        yolo val task=segment model=/home/jovyan/project/APP_PCS_ETX/weights/tray_detector.engine imgsz=640 data=/home/jovyan/yolo/data/TrayDetection/YOLODataset/dataset.yaml  \nVisualize:       https://netron.app\n[INFO] Export complete.\n[INFO] Loading models...\nWARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n[INFO] Running inference...\nLoading /home/jovyan/project/APP_PCS_ETX/weights/tray_detector.engine for TensorRT inference...\n[06/12/2025-16:40:49] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n[06/12/2025-16:40:49] [TRT] [I] Loaded engine size: 14 MiB\n[06/12/2025-16:40:49] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +11, now: CPU 0, GPU 11 (MiB)\n[06/12/2025-16:40:49] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +21, now: CPU 0, GPU 32 (MiB)\n[INFO] Summarizing results...\n\n===== 🔍 Comparison Results =====\n[FAIL] Boxes - Shape mismatch: (1, 4) vs (300, 4)\n[FAIL] Confs - Shape mismatch: (1,) vs (300,)\n[FAIL] Classes - Shape mismatch: (1,) vs (300,)\n[WARN] Masks - One is None → Skipping",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @myungki-park, thank you for bringing this up and for providing such a detailed description and script! 🚀 This is an automated response to help you get the fastest and most accurate assistance. An Ultralytics engineer will also review your issue and respond shortly.\n\nWe recommend referencing the [Docs](https://docs.ultralytics.com/) for extensive [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, which may answer common questions about model exports and inference.\n\nSince you are observing discrepancies between .pt and .engine inference results in YOLO11 segmentation, please ensure you provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/). This helps us reproduce and debug the issue efficiently. If the script you included is your MRE, that's perfect! If possible, also include a small example image and any relevant outputs or logs that highlight the differences.\n\nJoin our community for real-time discussion on [Discord](https://discord.com/invite/ultralytics) 🎧, dive deeper on [Discourse](https://community.ultralytics.com/), or share insights on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease verify your issue with the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) installed in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following up-to-date verified environments (all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your detailed report and for using Ultralytics! 🦾"
      },
      {
        "user": "Y-T-G",
        "body": "You shouldn't ignore warnings. They are shown for a reason.\n\n`WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.`\n\nYou need to provide the `task` while loading the model.\n\n`model = YOLO(\"model.engine\", task=\"segment\")`"
      },
      {
        "user": "myungki-park",
        "body": "@Y-T-G yeah you are right. thank you so much!!"
      }
    ]
  },
  {
    "issue_number": 21003,
    "title": "❗ Bug Report: PytorchStreamReader failed reading zip archive when loading model weights on Mac",
    "author": "EtienneDavid",
    "state": "closed",
    "created_at": "2025-06-09T20:28:29Z",
    "updated_at": "2025-06-12T08:35:29Z",
    "labels": [
      "bug",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nPredict\n\n### Bug\n\n**Description**\n\nI'm encountering a RuntimeError when trying to load a YOLOv10 detection model on my Mac using Ultralytics. The weights file was trained and saved in the cloud (on a T4 GPU instance). The same weights file loads fine on the cloud server, but fails locally on my Mac.\n\n**System Information**\n\nOS (Local): macOS (Apple Silicon, CPU-only)\nPython: 3.11\nUltralytics Version: latest (installed via uv pip)\nTorch Version: latest compatible with Ultralytics\nModel Task: detect\nDevice (Local): CPU\n\n**Code Snippet**\n\n\n```\nfrom ultralytics import YOLO\nmodel = YOLO(\"/path/to/latest_model.pt\", task=\"detect\", verbose=True)\nresults = model.predict(\"/path/to/image.png\", save=True)\nresults[0].show()\n```\n\nError Output\n\n```\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[5], line 7\n      3 device = torch.device(\"cpu\")\n      5 from ultralytics import YOLO\n----> 7 model = YOLO(\"/Users/etiennedavid/Documents/projects/ComputerVision/weights/stomata_plantzz_latest_model.pt\",task=\"segment\",verbose=True)\n      9 results = model.predict(\"/Users/etiennedavid/Documents/projects/ComputerVision/toolbox/emergence_rate/emergence_test/maize/early/17.png\", save=True)\n     11 results[0].show()\n\nFile ~/Documents/projects/ComputerVision/.venv/lib/python3.11/site-packages/ultralytics/models/yolo/model.py:53, in YOLO.__init__(self, model, task, verbose)\n     50     self.__dict__ = new_instance.__dict__\n     51 else:\n     52     # Continue with default YOLO initialization\n---> 53     super().__init__(model=model, task=task, verbose=verbose)\n\nFile ~/Documents/projects/ComputerVision/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py:148, in Model.__init__(self, model, task, verbose)\n    146     self._new(model, task=task, verbose=verbose)\n    147 else:\n--> 148     self._load(model, task=task)\n    150 # Delete super().training for accessing self.model.training\n    151 del self.training\n\nFile ~/Documents/projects/ComputerVision/.venv/lib/python3.11/site-packages/ultralytics/engine/model.py:292, in Model._load(self, weights, task)\n    289 weights = checks.check_model_file_from_stem(weights)  # add suffix, i.e. yolo11n -> yolo11n.pt\n    291 if Path(weights).suffix == \".pt\":\n--> 292     self.model, self.ckpt = attempt_load_one_weight(weights)\n    293     self.task = self.model.args[\"task\"]\n    294     self.overrides = self.model.args = self._reset_ckpt_args(self.model.args)\n\nFile ~/Documents/projects/ComputerVision/.venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py:1306, in attempt_load_one_weight(weight, device, inplace, fuse)\n   1293 def attempt_load_one_weight(weight, device=None, inplace=True, fuse=False):\n   1294     \"\"\"\n   1295     Load a single model weights.\n   1296 \n   (...)   1304         (tuple): Tuple containing the model and checkpoint.\n   1305     \"\"\"\n-> 1306     ckpt, weight = torch_safe_load(weight)  # load ckpt\n   1307     args = {**DEFAULT_CFG_DICT, **(ckpt.get(\"train_args\", {}))}  # combine model and default args, preferring model args\n   1308     model = (ckpt.get(\"ema\") or ckpt[\"model\"]).to(device).float()  # FP32 model\n\nFile ~/Documents/projects/ComputerVision/.venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py:1211, in torch_safe_load(weight, safe_only)\n   1209                 ckpt = torch.load(f, pickle_module=safe_pickle)\n   1210         else:\n-> 1211             ckpt = torch.load(file, map_location=\"cpu\")\n   1213 except ModuleNotFoundError as e:  # e.name is missing module name\n   1214     if e.name == \"models\":\n\nFile ~/Documents/projects/ComputerVision/.venv/lib/python3.11/site-packages/ultralytics/utils/patches.py:115, in torch_load(*args, **kwargs)\n    112 if TORCH_1_13 and \"weights_only\" not in kwargs:\n    113     kwargs[\"weights_only\"] = False\n--> 115 return _torch_load(*args, **kwargs)\n\nFile ~/Documents/projects/ComputerVision/.venv/lib/python3.11/site-packages/torch/serialization.py:1326, in load(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\n   1324 orig_position = opened_file.tell()\n   1325 overall_storage = None\n-> 1326 with _open_zipfile_reader(opened_file) as opened_zipfile:\n   1327     if _is_torchscript_zip(opened_zipfile):\n   1328         warnings.warn(\n   1329             \"'torch.load' received a zip file that looks like a TorchScript archive\"\n   1330             \" dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to\"\n   1331             \" silence this warning)\",\n   1332             UserWarning,\n   1333         )\n\nFile ~/Documents/projects/ComputerVision/.venv/lib/python3.11/site-packages/torch/serialization.py:671, in _open_zipfile_reader.__init__(self, name_or_buffer)\n    670 def __init__(self, name_or_buffer) -> None:\n--> 671     super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\n\nRuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory\n\n```\n\nRuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory\n\nWhat I've Tried\n\nVerified that the model loads perfectly on the original training machine (Linux with NVIDIA T4).\n\nRe-downloaded the .pt file multiple times.\n\nChecked file size (seems intact).\n\nEnsured my local Python and Torch environments are compatible.\n\n\n**Questions**\n\nCould the problem stem from CPU vs GPU differences in Torch serialization?\n\nIs there a recommended way to safely download .pt weights for cross-platform use?\n\nThanks in advance! Let me know if logs, model samples, or additional context would be helpful.\n\n### Environment\n\nUltralytics 8.3.129 🚀 Python-3.11.9 torch-2.5.0 CPU (Apple M1 Pro)\nSetup complete ✅ (8 CPUs, 16.0 GB RAM, 376.7/460.4 GB disk)\n\nOS                  macOS-15.5-arm64-arm-64bit\nEnvironment         Darwin\nPython              3.11.9\nInstall             git\nPath                /Users/etiennedavid/Documents/projects/ComputerVision/.venv/lib/python3.11/site-packages/ultralytics\nRAM                 16.00 GB\nDisk                376.7/460.4 GB\nCPU                 Apple M1 Pro\nCPU count           8\nGPU                 None\nGPU count           None\nCUDA                None\n\nnumpy               ✅ 2.3.0>=1.23.0\nmatplotlib          ✅ 3.10.3>=3.3.0\nopencv-python       ✅ 4.11.0.86>=4.6.0\npillow              ✅ 11.2.1>=7.1.2\npyyaml              ✅ 6.0.2>=5.3.1\nrequests            ✅ 2.32.4>=2.23.0\nscipy               ✅ 1.15.3>=1.4.1\ntorch               ✅ 2.5.0>=1.8.0\ntorch               ✅ 2.5.0!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.20.0>=0.9.0\ntqdm                ✅ 4.67.1>=4.64.0\npsutil              ✅ 7.0.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.3.0>=1.1.4\nseaborn             ✅ 0.13.2>=0.11.0\nultralytics-thop    ✅ 2.0.14>=2.0.0\n\n### Minimal Reproducible Example\n\n```\nfrom ultralytics import YOLO\nmodel = YOLO(\"/path/to/latest_model.pt\", task=\"detect\", verbose=True)\nresults = model.predict(\"/path/to/image.png\", save=True)\nresults[0].show()\n```\n\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @EtienneDavid, thank you for reporting this issue and providing such a detailed description! 🚀 This is an automated response to help guide the troubleshooting process. An Ultralytics engineer will review your report and assist you soon.\n\nWe recommend checking the [Docs](https://docs.ultralytics.com/) for additional guidance and reviewing the [Predict](https://docs.ultralytics.com/modes/predict/) documentation for usage examples.\n\nSince this is a 🐛 Bug Report, please ensure you've provided a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/). Your code snippet looks great—if there are any other steps or files (such as a sample model weight file) that may help us reproduce the error, please share them as well.\n\nJoin the Ultralytics community for more support:\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth discussions: [Discourse](https://community.ultralytics.com/)\n- Share knowledge: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nPlease upgrade to the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to ensure your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience! An Ultralytics engineer will follow up with you soon."
      },
      {
        "user": "Y-T-G",
        "body": "The traceback is incomplete/truncated "
      },
      {
        "user": "EtienneDavid",
        "body": "Hmmm no ? It ends with \"RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory\". I fixed the formatting of my question for clarity"
      }
    ]
  },
  {
    "issue_number": 21030,
    "title": "tflite: best_int8.tflite does not generate correct bounding box.",
    "author": "harufumigithub",
    "state": "open",
    "created_at": "2025-06-12T00:15:18Z",
    "updated_at": "2025-06-12T08:35:24Z",
    "labels": [
      "bug",
      "non-reproducible",
      "detect",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nExport\n\n### Bug\n\nyolo export model=yolo11n.pt format=tflite int8=true\ngenerates followings: \nbest_saved_model$ tree\n.\n├── assets\n├── best_float16.tflite\n├── best_float32.tflite\n├── best_full_integer_quant.tflite\n├── best_integer_quant.tflite\n├── blob_detr_int8_v50.tflite\n├── fingerprint.pb\n├── metadata.yaml\n├── saved_model.pb\n\n\nAmong these tflite, running inference works well except best_int8.tflite. \nbest_int8.tflite place wrong bounding box.  When this happens, you need to reset or restart the environment and solve issue. However, this wrong BBox placement comes back time to time.  \n\n### Environment\n\nyolov8$ yolo check\nUltralytics 8.3.68 🚀 Python-3.9.13 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3070 Laptop GPU, 7747MiB)\nSetup complete ✅ (16 CPUs, 62.4 GB RAM, 183.0/245.0 GB disk)\n\nOS                  Linux-6.8.0-60-generic-x86_64-with-glibc2.35\nEnvironment         Linux\nPython              3.9.13\nInstall             pip\nRAM                 62.45 GB\nDisk                183.0/245.0 GB\nCPU                 11th Gen Intel Core(TM) i7-11800H 2.30GHz\nCPU count           16\nGPU                 NVIDIA GeForce RTX 3070 Laptop GPU, 7747MiB\nGPU count           1\nCUDA                12.4\n\nnumpy               ✅ 1.23.5>=1.23.0\nnumpy               ✅ 1.23.5<2.0.0; sys_platform == \"darwin\"\nmatplotlib          ✅ 3.5.2>=3.3.0\nopencv-python       ✅ 4.10.0.84>=4.6.0\npillow              ✅ 9.2.0>=7.1.2\npyyaml              ✅ 6.0>=5.3.1\nrequests            ✅ 2.28.1>=2.23.0\nscipy               ✅ 1.10.1>=1.4.1\ntorch               ✅ 2.5.1+cu124>=1.8.0\ntorch               ✅ 2.5.1+cu124!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.20.1+cu124>=0.9.0\ntqdm                ✅ 4.64.1>=4.64.0\npsutil              ✅ 5.9.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 1.4.4>=1.1.4\nseaborn             ✅ 0.11.2>=0.11.0\nultralytics-thop    ✅ 2.0.10>=2.0.0\n\n\n### Minimal Reproducible Example\n\nyolo export model=yolo11n.pt format=tflite int8=true\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @harufumigithub, thank you for your detailed report and for using Ultralytics 🚀! We appreciate you taking the time to provide your environment details and the steps you've followed.\n\nThis is an automated response to help streamline troubleshooting—an Ultralytics engineer will also review and assist you soon.\n\nIf this is a 🐛 Bug Report, please ensure you've included a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/)—it looks like you have, which is great! If you can share any additional sample input images, code snippets, or logs that demonstrate the incorrect bounding box outputs with `best_int8.tflite`, that will further help us investigate.\n\nYou can find more information and usage examples in our [Docs](https://docs.ultralytics.com/), including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) guides.\n\nJoin the Ultralytics community for real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧, dive into in-depth discussions on [Discourse](https://community.ultralytics.com/), or check out our [Subreddit](https://reddit.com/r/Ultralytics) for more support and knowledge sharing.\n\n## Upgrade\n\nTo help rule out issues already addressed in recent updates, please upgrade to the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following up-to-date verified environments (with dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for reporting this! An Ultralytics engineer will follow up with you soon."
      },
      {
        "user": "Y-T-G",
        "body": "You also need to provide the inference code you're using to reproduce this. Are you using Ultralytics for inference? Or your own custom inference code?"
      }
    ]
  },
  {
    "issue_number": 21034,
    "title": "the command 'yolo benchmark' doesn't support yoloe model ?",
    "author": "xifeng0126",
    "state": "closed",
    "created_at": "2025-06-12T08:01:11Z",
    "updated_at": "2025-06-12T08:33:38Z",
    "labels": [
      "documentation",
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI want to use the 'yolo benchmark' command to compare the performance of yoloe with other model.\n\n1. First, I executed the following command, but received an error.\n  ```\n  yolo benchmark model=yoloe-v8l-seg.pt data='coco8.yaml' imgsz=640 half=False device=0\n  ```\n  ```\n                   Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|     \n  ERROR ❌ Benchmark failure for PyTorch: The shape of the mask [1] at index 0 does not match the shape of the indexed tensor [0, 1] at index 0\n  \n  PyTorch: starting from 'yoloe-v8l-seg.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 116, 8400), (1, 32, 160, 160)) (102.4 MB)\n  ```\n2. I guess it's because the benchmark only supports detect tasks? So I converted the yoloe-v8-l-seg model to get the yoloe-v8-l-seg-det model. \n  ```\n  det_model = YOLOE(\"yoloe-v8l.yaml\")\n  \n  state = torch.load(\"yoloe-v8l-seg.pt\")\n  \n  det_model.load(state[\"model\"])\n  det_model.save(\"yoloe-v8l-seg-det.pt\")\n  \n  ```\nIt works. I'm not sure if my approach is correct, because here only the performance of the detect task is compared. How do you compare models on YOLOE? I mainly focus on ONNX format models, but I couldn't find relevant content in the YOLOE documentation.\n\n  ```\n  Benchmarks complete for yoloe-v8l-seg-det.pt on coco8.yaml at imgsz=640 (28.33s)\n  Benchmarks legend:  - ✅ Success  - ❎ Export passed but validation failed  - ❌️ Export failed\n                     Format Status❔  Size (MB) metrics/mAP50-95(B) Inference time (ms/im)     FPS\n  0                 PyTorch       ✅       98.0               0.681                  39.95   25.03\n  1             TorchScript       ✅      195.9              0.6758                   9.01  110.93\n  2                    ONNX       ✅      168.3              0.6758                  12.94    77.3\n  ```\n \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @xifeng0126, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users, which includes many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and answers to common questions.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please include as much detail as possible, such as dataset examples and training logs, and ensure you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best! For real-time chat, head over to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or join threads on our [Subreddit](https://reddit.com/r/Ultralytics) to connect with others.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to ensure your issue is not already resolved:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response. An Ultralytics engineer will review your question and assist you as soon as possible. Thank you for your patience!"
      },
      {
        "user": "Y-T-G",
        "body": "You're using `data='coco8.yaml'` which is a detection dataset, not a segmentation dataset.\n\nhttps://docs.ultralytics.com/datasets/segment/coco8-seg/"
      }
    ]
  },
  {
    "issue_number": 19261,
    "title": "Assign tracker ID with unique ID",
    "author": "erfansafaie",
    "state": "closed",
    "created_at": "2025-02-15T16:37:33Z",
    "updated_at": "2025-06-12T00:23:30Z",
    "labels": [
      "question",
      "Stale",
      "track"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello. Thanks for create such rich Ultralytics library. \nI want to know Is it possible using unique ID instead of incremental for tracking?\nI did some modifying but I can't get good output and have some bugs. Is there any sample?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @erfansafaie, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom tracking ❓ Question, please share more details about the modifications you've made, the behavior you're expecting, and the exact bugs or errors you're encountering. This might include code snippets, test case examples, log outputs, or any other relevant data to help us better assist you. Be sure to review our [Tracking Documentation](https://docs.ultralytics.com/modes/#track) for insights into how tracking works in YOLO.\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated message, but an Ultralytics engineer will review your question and provide further assistance soon. Thank you for your patience! 😊"
      },
      {
        "user": "Y-T-G",
        "body": "Isn't incremental ID unique?"
      },
      {
        "user": "erfansafaie",
        "body": "Yes. For example using uuid or something like that.\nIn my use case, it's be better to write this unique ID directly on database. If we detect feature's object with more confidence, modify it on database based on the ID. "
      }
    ]
  },
  {
    "issue_number": 19292,
    "title": "Colab default setting could not covert to tflite",
    "author": "kris-himax",
    "state": "closed",
    "created_at": "2025-02-18T09:09:13Z",
    "updated_at": "2025-06-12T00:23:29Z",
    "labels": [
      "bug",
      "non-reproducible",
      "Stale",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nFew weeks ago I can convert the yolo11n.pt to tflite, but now the colab default python setting will be python 3.11.11.\nAt this version, the tflite could not be converted by onnx2tf.\nHow can I fix it?\n\n![Image](https://github.com/user-attachments/assets/60fa0461-5413-4f7d-92c6-4adac4c368d9)\n\n```\nfrom ultralytics import YOLO\nmodel = YOLO('yolo11n.pt')\nmodel.export(format='tflite', imgsz=192, int8=True)\nmodel = YOLO('yolo11n_saved_model/yolo11n_full_integer_quant.tflite')\nres = model.predict(imgsz=192)\nres[0].plot(show=True)\n```\n\n```\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n100%|██████████| 755k/755k [00:00<00:00, 116MB/s]\nScanning /content/datasets/coco8/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 117.40it/s]New cache created: /content/datasets/coco8/labels/val.cache\nTensorFlow SavedModel: WARNING ⚠️ >300 images recommended for INT8 calibration, found 4 images.\n\nTensorFlow SavedModel: starting TFLite export with onnx2tf 1.26.3...\nERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\nERROR: The trace log is below.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/onnx2tf/utils/common_functions.py\", line 312, in print_wrapper_func\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/onnx2tf/utils/common_functions.py\", line 385, in inverted_operation_enable_disable_wrapper_func\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/onnx2tf/utils/common_functions.py\", line 55, in get_replacement_parameter_wrapper_func\n    func(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/onnx2tf/ops/Mul.py\", line 245, in make_node\n    correction_process_for_accuracy_errors(\n  File \"/usr/local/lib/python3.11/dist-packages/onnx2tf/utils/common_functions.py\", line 5894, in correction_process_for_accuracy_errors\n    min_abs_err_perm_1: int = [idx for idx in range(len(validation_data_1.shape))]\n                                                        ^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'shape'\n\nERROR: input_onnx_file_path: yolo11n.onnx\nERROR: onnx_op_name: wa/model.10/m/m.0/attn/Mul\nERROR: Read this and deal with it. https://github.com/PINTO0309/onnx2tf#parameter-replacement\nERROR: Alternatively, if the input OP has a dynamic dimension, use the -b or -ois option to rewrite it to a static shape and try again.\nERROR: If the input OP of ONNX before conversion is NHWC or an irregular channel arrangement other than NCHW, use the -kt or -kat option.\nERROR: Also, for models that include NonMaxSuppression in the post-processing, try the -onwdt option.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/onnx2tf/utils/common_functions.py\", line 312, in print_wrapper_func\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/onnx2tf/utils/common_functions.py\", line 385, in inverted_operation_enable_disable_wrapper_func\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/onnx2tf/utils/common_functions.py\", line 55, in get_replacement_parameter_wrapper_func\n    func(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/onnx2tf/ops/Mul.py\", line 245, in make_node\n    correction_process_for_accuracy_errors(\n  File \"/usr/local/lib/python3.11/dist-packages/onnx2tf/utils/common_functions.py\", line 5894, in correction_process_for_accuracy_errors\n    min_abs_err_perm_1: int = [idx for idx in range(len(validation_data_1.shape))]\n                                                        ^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'shape'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-da2eaec26985>\", line 3, in <cell line: 0>\n    model.export(format='tflite', imgsz=192, int8=True)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\", line 741, in export\n    return Exporter(overrides=args, _callbacks=self.callbacks)(model=self.model)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/exporter.py\", line 418, in __call__\n    f[5], keras_model = self.export_saved_model()\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/exporter.py\", line 175, in outer_func\n    f, model = inner_func(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/exporter.py\", line 1036, in export_saved_model\n    keras_model = onnx2tf.convert(\n                  ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/onnx2tf/onnx2tf.py\", line 1141, in convert\n    op.make_node(\n  File \"/usr/local/lib/python3.11/dist-packages/onnx2tf/utils/common_functions.py\", line 378, in print_wrapper_func\n    sys.exit(1)\nSystemExit: 1\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n    traceback_info = getframeinfo(tb, context)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1671, in getframeinfo\n    lineno = frame.f_lineno\n             ^^^^^^^^^^^^^^\nAttributeError: 'tuple' object has no attribute 'f_lineno'\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n[/usr/local/lib/python3.11/dist-packages/onnx2tf/utils/common_functions.py](https://localhost:8080/#) in print_wrapper_func(*args, **kwargs)\n    311         try:\n--> 312             result = func(*args, **kwargs)\n    313 \n\n18 frames\nAttributeError: 'NoneType' object has no attribute 'shape'\n\nDuring handling of the above exception, another exception occurred:\n\nSystemExit                                Traceback (most recent call last)\n    [... skipping hidden 1 frame]\n\nSystemExit: 1\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\n    [... skipping hidden 1 frame]\n\n[/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py](https://localhost:8080/#) in find_recursion(etype, value, records)\n    380     # first frame (from in to out) that looks different.\n    381     if not is_recursion_error(etype, value, records):\n--> 382         return len(records), 0\n    383 \n    384     # Select filename, lineno, func_name to track frames with\n\nTypeError: object of type 'NoneType' has no len()\n\n```\n\nThanks,\nKris\n\n### Environment\n\ncolab default Environment\n\n### Minimal Reproducible Example\n\nhttps://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @kris-himax, thank you for bringing this to our attention 🚀! We recommend checking out the [Ultralytics Docs](https://docs.ultralytics.com/) for guidance on various features, including exporting models to alternative formats. You can also find helpful [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples there.\n\nTo assist us in debugging this issue 🐛, could you please provide a detailed [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/)? This includes a concise code snippet (if different from your provided one) that reliably reproduces the issue, along with additional environment details if applicable.\n\nIn the meantime, here are a few steps you can try to help resolve the problem:\n1. **Upgrade**: Make sure you're using the latest version of the `ultralytics` package and all its dependencies in a [Python>=3.8](https://www.python.org/) environment. You can upgrade by running:\n\n   ```bash\n   pip install -U ultralytics\n   ```\n\n2. **Environment**: Ensure your environment is configured properly, including installing the required dependencies listed in the [pyproject.toml](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml). Sometimes, updating the environment or running the script in an alternative environment can address unexpected errors.\n\n3. **Platform Options**: If you're encountering challenges in Colab, you can try other verified environments:\n   - Run with free GPUs on <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a>, <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>, or <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>. \n   - Explore cloud-based options like [AWS](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/) or [GCP](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/).\n\n4. **Check Warnings**: The warning regarding the INT8 calibration suggests that more calibration images might be needed for optimal results (>300 images). Testing with additional data could help isolate the problem.\n\nFor a deeper dive into the onnx2tf-specific errors, you may refer to the onnx2tf documentation and troubleshooting tips outlined in their [README](https://github.com/PINTO0309/onnx2tf#parameter-replacement). For instance, their guidance on parameters such as `-b` or `-ois` for static shapes might prove helpful in your case.\n\nLastly, we invite you to join the broader Ultralytics community for further collaboration 💬:\n- Connect in real-time on [Discord](https://discord.com/invite/ultralytics) 🎧.\n- Engage with discussions on [Discourse](https://community.ultralytics.com/) or our [Subreddit](https://reddit.com/r/Ultralytics).\n\nAn Ultralytics engineer will review this shortly and assist further. Thank you for your patience and for helping improve Ultralytics! 🚀"
      },
      {
        "user": "Y-T-G",
        "body": "What's the log before that?"
      },
      {
        "user": "kris-himax",
        "body": "Hi @Y-T-G,\nthe log before that is,\n\n![Image](https://github.com/user-attachments/assets/2d0f8d7f-1659-407f-9d11-013db3b6d1ba)\n\n```\nUltralytics 8.3.53 🚀 Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\nWARNING ⚠️ INT8 export requires a missing 'data' arg for calibration. Using default 'data=coco8.yaml'.\nYOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n\nPyTorch: starting from 'yolo11n.pt' with input shape (1, 3, 192, 192) BCHW and output shape(s) (1, 84, 756) (5.4 MB)\nrequirements: Ultralytics requirements ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'onnx>=1.12.0', 'onnx2tf>1.17.5,<=1.22.3', 'onnxslim>=0.1.31', 'tflite_support', 'onnxruntime-gpu'] not found, attempting AutoUpdate...\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com/\nCollecting sng4onnx>=1.0.1\n  Downloading sng4onnx-1.0.4-py3-none-any.whl.metadata (4.6 kB)\nCollecting onnx_graphsurgeon>=0.3.26\n  Downloading onnx_graphsurgeon-0.5.2-py2.py3-none-any.whl.metadata (8.1 kB)\nCollecting onnx>=1.12.0\n  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nCollecting onnx2tf<=1.22.3,>1.17.5\n  Downloading onnx2tf-1.22.3-py3-none-any.whl.metadata (136 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136.6/136.6 kB 97.7 MB/s eta 0:00:00\nCollecting onnxslim>=0.1.31\n  Downloading onnxslim-0.1.45-py3-none-any.whl.metadata (4.2 kB)\nCollecting tflite_support\n  Downloading tflite_support-0.4.4-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.4 kB)\nCollecting onnxruntime-gpu\n  Downloading onnxruntime_gpu-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx_graphsurgeon>=0.3.26) (1.26.4)\nRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (4.25.5)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxslim>=0.1.31) (1.13.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxslim>=0.1.31) (24.2)\nRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tflite_support) (1.4.0)\nRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tflite_support) (24.3.25)\nCollecting protobuf>=3.20.2 (from onnx>=1.12.0)\n  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\nCollecting sounddevice>=0.4.4 (from tflite_support)\n  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\nCollecting pybind11>=2.6.0 (from tflite_support)\n  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\nCollecting coloredlogs (from onnxruntime-gpu)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->tflite_support) (1.17.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxslim>=0.1.31) (1.3.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->tflite_support) (2.22)\nDownloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\nDownloading onnx_graphsurgeon-0.5.2-py2.py3-none-any.whl (56 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.4/56.4 kB 261.9 MB/s eta 0:00:00\nDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.0/16.0 MB 169.9 MB/s eta 0:00:00\nDownloading onnx2tf-1.22.3-py3-none-any.whl (435 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 435.0/435.0 kB 127.6 MB/s eta 0:00:00\nDownloading onnxslim-0.1.45-py3-none-any.whl (142 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.6/142.6 kB 337.5 MB/s eta 0:00:00\nDownloading tflite_support-0.4.4-cp310-cp310-manylinux2014_x86_64.whl (60.8 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.8/60.8 MB 200.6 MB/s eta 0:00:00\nDownloading onnxruntime_gpu-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (291.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 291.5/291.5 MB 220.7 MB/s eta 0:00:00\nDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 237.7 MB/s eta 0:00:00\nDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 243.3/243.3 kB 216.8 MB/s eta 0:00:00\nDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 190.0 MB/s eta 0:00:00\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 172.3 MB/s eta 0:00:00\nInstalling collected packages: sng4onnx, pybind11, protobuf, onnx2tf, humanfriendly, sounddevice, onnx, coloredlogs, tflite_support, onnxslim, onnxruntime-gpu, onnx_graphsurgeon\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 4.25.5\n    Uninstalling protobuf-4.25.5:\n      Successfully uninstalled protobuf-4.25.5\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnx2tf-1.22.3 onnx_graphsurgeon-0.5.2 onnxruntime-gpu-1.20.1 onnxslim-0.1.45 protobuf-3.20.3 pybind11-2.13.6 sng4onnx-1.0.4 sounddevice-0.5.1 tflite_support-0.4.4\n\nrequirements: AutoUpdate success ✅ 30.7s, installed 7 packages: ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'onnx>=1.12.0', 'onnx2tf>1.17.5,<=1.22.3', 'onnxslim>=0.1.31', 'tflite_support', 'onnxruntime-gpu']\nrequirements: ⚠️ Restart runtime or rerun command for updates to take effect\n\n\nTensorFlow SavedModel: starting export with tensorflow 2.17.1...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/calibration_image_sample_data_20x128x128x3_float32.npy.zip to 'calibration_image_sample_data_20x128x128x3_float32.npy.zip'...\n100%|██████████| 1.11M/1.11M [00:00<00:00, 165MB/s]\nUnzipping calibration_image_sample_data_20x128x128x3_float32.npy.zip to /content/calibration_image_sample_data_20x128x128x3_float32.npy...: 100%|██████████| 1/1 [00:00<00:00, 48.41file/s]\nONNX: starting export with onnx 1.17.0 opset 19...\n\nONNX: slimming with onnxslim 0.1.45...\nONNX: export success ✅ 1.4s, saved as 'yolo11n.onnx' (10.1 MB)\nTensorFlow SavedModel: collecting INT8 calibration images from 'data=coco8.yaml'\n\nDataset 'coco8.yaml' images not found ⚠️, missing path '/content/datasets/coco8/images/val'\nDownloading https://ultralytics.com/assets/coco8.zip to '/content/datasets/coco8.zip'...\n100%|██████████| 433k/433k [00:00<00:00, 64.1MB/s]\nUnzipping /content/datasets/coco8.zip to /content/datasets/coco8...: 100%|██████████| 25/25 [00:00<00:00, 3369.68file/s]Dataset download success ✅ (1.3s), saved to /content/datasets\n\n\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n100%|██████████| 755k/755k [00:00<00:00, 99.6MB/s]\nScanning /content/datasets/coco8/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 104.53it/s]New cache created: /content/datasets/coco8/labels/val.cache\nTensorFlow SavedModel: WARNING ⚠️ >300 images recommended for INT8 calibration, found 4 images.\n\nTensorFlow SavedModel: starting TFLite export with onnx2tf 1.22.3...\nTensorFlow SavedModel: export success ✅ 96.6s, saved as 'yolo11n_saved_model' (35.1 MB)\n\nTensorFlow Lite: starting export with tensorflow 2.17.1...\nTensorFlow Lite: export success ✅ 0.0s, saved as 'yolo11n_saved_model/yolo11n_int8.tflite' (2.7 MB)\n\nExport complete (98.3s)\nResults saved to /content\nPredict:         yolo predict task=detect model=yolo11n_saved_model/yolo11n_int8.tflite imgsz=192 int8 \nValidate:        yolo val task=detect model=yolo11n_saved_model/yolo11n_int8.tflite imgsz=192 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml int8 \nVisualize:       https://netron.app/\n```\n\nThanks,\nKris"
      }
    ]
  },
  {
    "issue_number": 20421,
    "title": "Warning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.",
    "author": "yangershuai627",
    "state": "open",
    "created_at": "2025-04-29T12:37:18Z",
    "updated_at": "2025-06-12T00:23:19Z",
    "labels": [
      "question",
      "dependencies",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nThe first question: I have attempted to add.contiguous() after performing transpose or permute and other transformation operations on the tensor. The result shows that the accuracy after repair is reduced by 0.2 percent compared with that without repair.\nFor example:\n```\n#primary code:\ninput_tensor = ori_tensor.transpose(1, 3)\n#modified code:\ninput_tensor = ori_tensor.transpose(1, 3).contiguous()\n```\nThe second question: I tried training with a single card again, but the accuracy improved instead. Why does the accuracy of single-card training exceed that of multi-card training? What should I do to make the accuracy of multi-card training as high as that of single-card training?\nThis is the module I customized:\n```\nimport torch\nimport torch.nn as nn\nfrom typing import Tuple, Union\n\n\n# https://arxiv.org/pdf/2309.11523\n# https://github.com/qhfan/RMT/blob/main/classfication_release/RMT.py\n\n\ndef rotate_every_two(x):\n    # 对输入张量x在最后一个维度上进行旋转（每两个元素交换位置）\n    x1 = x[:, :, :, :, ::2]  # 取奇数列\n    x2 = x[:, :, :, :, 1::2]  # 取偶数列\n    x = torch.stack([-x2, x1], dim=-1)  # 将奇数列与偶数列交换，并变为负数的偶数列\n    return x.flatten(-2)  # 将倒数第二维度展平\n\n\ndef theta_shift(x, sin, cos):\n    # 对输入x进行旋转变换，其中sin和cos为旋转角度的正弦和余弦值\n    return (x * cos) + (rotate_every_two(x) * sin)\n\n\nclass DWConv2d(nn.Module):\n    # Depthwise Convolution Layer（深度卷积层）\n    def __init__(self, dim, kernel_size, stride, padding):\n        super().__init__()\n        # depthwise卷积，输入和输出的通道数相同，groups=dim表示每个输入通道独立卷积\n        self.conv = nn.Conv2d(dim, dim, kernel_size, stride, padding, groups=dim)\n\n    def forward(self, x: torch.Tensor):\n        '''\n        x: 输入张量，形状为 (b h w c)，\n        b: batch size，h: 高度，w: 宽度，c: 通道数\n        '''\n        x = x.permute(0, 3, 1, 2)  # 将x从 (b h w c) 转换为 (b c h w)，以适应卷积的格式\n        x = self.conv(x)  # 对x进行深度卷积\n        x = x.permute(0, 2, 3, 1)  # 将输出从 (b c h w) 转回 (b h w c)\n        return x\n\n\nclass RetNetRelPos2d(nn.Module):\n    # 用于生成相对位置编码的模块\n    def __init__(self, embed_dim, num_heads, initial_value, heads_range):\n        '''\n        recurrent_chunk_size: (clh clw)\n        num_chunks: (nch ncw)\n        clh * clw == cl\n        nch * ncw == nc\n\n        default: clh==clw, clh != clw is not implemented\n        '''\n        super().__init__()\n        angle = 1.0 / (10000 ** torch.linspace(0, 1, embed_dim // num_heads // 2))  # 计算角度\n        angle = angle.unsqueeze(-1).repeat(1, 2).flatten()  # 将角度展开\n        self.initial_value = initial_value\n        self.heads_range = heads_range\n        self.num_heads = num_heads\n        decay = torch.log(\n            1 - 2 ** (-initial_value - heads_range * torch.arange(num_heads, dtype=torch.float) / num_heads))  # 计算衰减\n        self.register_buffer('angle', angle)  # 注册角度作为模型参数\n        self.register_buffer('decay', decay)  # 注册衰减作为模型参数\n\n    def generate_2d_decay(self, H: int, W: int):\n        '''\n        生成2D衰减掩码，结果形状为 (HW)*(HW)\n        '''\n        index_h = torch.arange(H).to(self.decay)  # 高度方向的索引\n        index_w = torch.arange(W).to(self.decay)  # 宽度方向的索引\n        grid = torch.meshgrid([index_h, index_w])  # 生成网格坐标\n        grid = torch.stack(grid, dim=-1).reshape(H * W, 2)  # 将坐标堆叠并展平\n        mask = grid[:, None, :] - grid[None, :, :]  # 计算网格点之间的差异\n        mask = (mask.abs()).sum(dim=-1)  # 计算距离\n        mask = mask * self.decay[:, None, None]  # 应用衰减\n        return mask\n\n    def generate_1d_decay(self, l: int):\n        '''\n        生成1D衰减掩码，结果形状为 l*l\n        '''\n        index = torch.arange(l).to(self.decay)  # 生成索引\n        mask = index[:, None] - index[None, :]  # 计算差异\n        mask = mask.abs()  # 获取绝对值\n        mask = mask * self.decay[:, None, None]  # 应用衰减\n        return mask\n\n    def forward(self, slen: Tuple[int], activate_recurrent=False, chunkwise_recurrent=False):\n        '''\n        slen: (h, w)\n        h * w == l\n        recurrent is not implemented\n        '''\n        if activate_recurrent:\n            sin = torch.sin(self.angle * (slen[0] * slen[1] - 1))  # 计算正弦值\n            cos = torch.cos(self.angle * (slen[0] * slen[1] - 1))  # 计算余弦值\n            retention_rel_pos = ((sin, cos), self.decay.exp())  # 返回旋转的相对位置编码\n\n        elif chunkwise_recurrent:\n            index = torch.arange(slen[0] * slen[1]).to(self.decay)\n            sin = torch.sin(index[:, None] * self.angle[None, :])  # 计算正弦值\n            sin = sin.reshape(slen[0], slen[1], -1)  # 将其调整为 (h w d1) 形状\n            cos = torch.cos(index[:, None] * self.angle[None, :])  # 计算余弦值\n            cos = cos.reshape(slen[0], slen[1], -1)  # 将其调整为 (h w d1) 形状\n\n            mask_h = self.generate_1d_decay(slen[0])  # 生成高度方向的衰减掩码\n            mask_w = self.generate_1d_decay(slen[1])  # 生成宽度方向的衰减掩码\n\n            retention_rel_pos = ((sin, cos), (mask_h, mask_w))  # 返回包含 sin, cos 和掩码的元组\n\n        else:\n            index = torch.arange(slen[0] * slen[1]).to(self.decay)\n            sin = torch.sin(index[:, None] * self.angle[None, :])  # 计算正弦值\n            sin = sin.reshape(slen[0], slen[1], -1)  # 将其调整为 (h w d1) 形状\n            cos = torch.cos(index[:, None] * self.angle[None, :])  # 计算余弦值\n            cos = cos.reshape(slen[0], slen[1], -1)  # 将其调整为 (h w d1) 形状\n            mask = self.generate_2d_decay(slen[0], slen[1])  # 生成2D衰减掩码\n            retention_rel_pos = ((sin, cos), mask)  # 返回旋转后的相对位置编码及掩码\n\n        return retention_rel_pos\nclass VisionRetentionChunk(nn.Module):\n    # Vision Retention Chunk，利用卷积与注意力机制提取图像特征\n    def __init__(self, embed_dim, num_heads=4, value_factor=1):\n        super().__init__()\n        self.factor = value_factor  # 用于控制v_proj的输出维度\n        self.embed_dim = embed_dim  # 嵌入维度\n        self.num_heads = num_heads  # 注意力头数\n        self.head_dim = self.embed_dim * self.factor // num_heads  # 每个头的维度\n        self.key_dim = self.embed_dim // num_heads  # 键的维度\n        self.scaling = self.key_dim ** -0.5  # 缩放因子，用于归一化注意力分数\n        # 查询、键、值投影层\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n        self.v_proj = nn.Linear(embed_dim, embed_dim * self.factor, bias=True)\n        # 使用深度卷积处理值（v）\n        self.lepe = DWConv2d(embed_dim, 5, 1, 2)\n\n        # 输出投影层\n        self.out_proj = nn.Linear(embed_dim * self.factor, embed_dim, bias=True)\n        self.reset_parameters()\n\n        self.Relpos = RetNetRelPos2d(embed_dim, num_heads, 2, 4)\n\n    def reset_parameters(self):\n        # 对投影层参数进行初始化\n        nn.init.xavier_normal_(self.q_proj.weight, gain=2 ** -2.5)\n        nn.init.xavier_normal_(self.k_proj.weight, gain=2 ** -2.5)\n        nn.init.xavier_normal_(self.v_proj.weight, gain=2 ** -2.5)\n        nn.init.xavier_normal_(self.out_proj.weight)\n        nn.init.constant_(self.out_proj.bias, 0.0)\n\n    def forward(self, x: torch.Tensor):\n        '''\n        x: 输入张量，形状为 (b h w c)\n        rel_pos: 相对位置编码，包含sin和cos的值，以及掩码（mask_h, mask_w）\n        '''\n        x = x.permute(0, 2, 3, 1)\n\n        bsz, h, w, _ = x.size()  # 获取批大小、图像高度、宽度\n\n        rel_pos = self.Relpos((h, w), chunkwise_recurrent=True)\n\n        (sin, cos), (mask_h, mask_w) = rel_pos  # 从输入的相对位置中获取sin、cos和掩码\n\n        # 通过查询、键、值投影层获取q, k, v\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n        lepe = self.lepe(v)  # 通过深度卷积处理值（v）\n\n        k *= self.scaling  # 对键进行缩放\n        # 重塑q和k，准备进行注意力计算\n        q = q.view(bsz, h, w, self.num_heads, self.key_dim).permute(0, 3, 1, 2, 4)  # (b n h w d1)\n        k = k.view(bsz, h, w, self.num_heads, self.key_dim).permute(0, 3, 1, 2, 4)  # (b n h w d1)\n\n        # 对q和k进行旋转变换\n        qr = theta_shift(q, sin, cos)  # (b n h w d1)\n        kr = theta_shift(k, sin, cos)  # (b n h w d1)\n\n        # 计算水平维度的注意力\n        qr_w = qr.transpose(1, 2)  # (b h n w d1)\n        kr_w = kr.transpose(1, 2)  # (b h n w d1)\n        v = v.reshape(bsz, h, w, self.num_heads, -1).permute(0, 1, 3, 2, 4)  # (b h n w d2)\n\n        qk_mat_w = qr_w @ kr_w.transpose(-1, -2)  # (b h n w w)\n        qk_mat_w = qk_mat_w + mask_w  # 加上掩码\n        qk_mat_w = torch.softmax(qk_mat_w, -1)  # 归一化注意力分数\n        v = torch.matmul(qk_mat_w, v)  # (b h n w d2)\n\n        # 计算垂直维度的注意力\n        qr_h = qr.permute(0, 3, 1, 2, 4)  # (b w n h d1)\n        kr_h = kr.permute(0, 3, 1, 2, 4)  # (b w n h d1)\n        v = v.permute(0, 3, 2, 1, 4)  # (b w n h d2)\n\n        qk_mat_h = qr_h @ kr_h.transpose(-1, -2)  # (b w n h h)\n        qk_mat_h = qk_mat_h + mask_h  # 加上掩码\n        qk_mat_h = torch.softmax(qk_mat_h, -1)  # 归一化注意力分数\n        output = torch.matmul(qk_mat_h, v)  # (b w n h d2)\n\n        # 将输出展平并通过最终的投影层\n        output = output.permute(0, 3, 1, 2, 4).flatten(-2, -1)  # (b h w n*d2)\n        output = output + lepe  # 加上深度卷积的输出\n        output = self.out_proj(output)  # 最后的投影层\n\n        output = output.permute(0, 3, 1, 2)\n\n        return output\n\ndef autopad(k, p=None, d=1):  # kernel, padding, dilation\n    \"\"\"Pad to 'same' shape outputs.\"\"\"\n    if d > 1:\n        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n    if p is None:\n        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n    return p\n\nclass Conv(nn.Module):\n    \"\"\"Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).\"\"\"\n\n    default_act = nn.SiLU()  # default activation\n\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n        \"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n\n    def forward(self, x):\n        \"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\n        return self.act(self.bn(self.conv(x)))\n\n    def forward_fuse(self, x):\n        \"\"\"Perform transposed convolution of 2D data.\"\"\"\n        return self.act(self.conv(x))\n\nclass Bottleneck_VRC(nn.Module):\n    \"\"\"Standard bottleneck.\"\"\"\n\n    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):\n        \"\"\"Initializes a standard bottleneck module with optional shortcut connection and configurable parameters.\"\"\"\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, k[0], 1)\n        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n        self.cv3 = VisionRetentionChunk(c2)\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        \"\"\"Applies the YOLO FPN to input data.\"\"\"\n        return x + self.cv3(self.cv2(self.cv1(x))) if self.add else self.cv3(self.cv2(self.cv1(x)))\n\nclass C2f_VRC(nn.Module):\n    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n\n    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\n        \"\"\"Initializes a CSP bottleneck with 2 convolutions and n Bottleneck blocks for faster processing.\"\"\"\n        super().__init__()\n        self.c = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)\n        self.m = nn.ModuleList(\n            Bottleneck_VRC(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))\n\n    def forward(self, x):\n        \"\"\"Forward pass through C2f layer.\"\"\"\n        y = list(self.cv1(x).chunk(2, 1))\n        y.extend(m(y[-1]) for m in self.m)\n        return self.cv2(torch.cat(y, 1))\n\n    def forward_split(self, x):\n        \"\"\"Forward pass using split() instead of chunk().\"\"\"\n        y = list(self.cv1(x).split((self.c, self.c), 1))\n        y.extend(m(y[-1]) for m in self.m)\n        return self.cv2(torch.cat(y, 1))\n\n\n\ndef main():\n\n    # 创建一个假输入张量 (batch_size, height, width, channels)\n    x = torch.randn(8, 128, 64, 64)  # 随机初始化输入\n\n    # 实例化 VisionRetentionChunk 模型\n    model = VisionRetentionChunk(embed_dim=128, num_heads=4, value_factor=1)\n\n    # 前向传播\n    output = model(x)\n\n    # 打印输出张量的形状，确认模型的输出维度\n    print(f\"Output shape: {output.shape}\")\n\n\nif __name__ == \"__main__\":\n    main()\n    \n```\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @yangershuai627, thank you for your detailed question and for sharing your custom module code with the Ultralytics community! 🚀 This is an automated response to help get you started—an Ultralytics engineer will also assist you soon.\n\nWe recommend reviewing the [Docs](https://docs.ultralytics.com/) for guidance on both [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, which might address some common issues and questions.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate further. This helps us quickly reproduce and debug the issue.\n\nFor custom training or accuracy ❓ questions, please share as much relevant information as possible, such as dataset samples and complete training logs, and double-check you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community for real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧, in-depth discussions on [Discourse](https://community.ultralytics.com/), or knowledge sharing on our [Subreddit](https://reddit.com/r/Ultralytics)!\n\n## Upgrade\n\nPlease upgrade to the latest `ultralytics` package and ensure all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) are met in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your report and contribution!"
      },
      {
        "user": "yangershuai627",
        "body": "@glenn-jocher Could you help me?"
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @yangershuai627,\n\nThis warning typically occurs with operations that change tensor memory layout (like transpose, permute, view) without making the tensor contiguous. For your specific implementation:\n\n1. **Selective .contiguous() usage**: Rather than adding it everywhere, only add it at critical points where tensors are passed between modules or at final outputs before gradient computation. Excessive .contiguous() can hurt performance both in speed and potentially accuracy due to different computation ordering.\n\n2. **Multi-card vs single-card accuracy**: This difference is common and can be caused by:\n   - Different effective batch normalization statistics across distributed training\n   - Different gradient accumulation and synchronization patterns\n   - Your module has many tensor reshaping operations which may behave differently in distributed mode\n\nTry these approaches:\n- Set `find_unused_parameters=False` in your DDP wrapper \n- Try SyncBatchNorm for multi-GPU training\n- Implement gradient clipping to stabilize multi-GPU training\n- Consider using torch.compile() if you're on PyTorch 2.0+ to optimize the operations\n\nYou can also profile your model to identify specific operations causing the stride mismatches rather than adding .contiguous() everywhere."
      }
    ]
  },
  {
    "issue_number": 20506,
    "title": "export yolov9-s for imx500",
    "author": "sinais54",
    "state": "open",
    "created_at": "2025-05-06T04:20:35Z",
    "updated_at": "2025-06-12T00:23:14Z",
    "labels": [
      "question",
      "Stale",
      "embedded",
      "exports"
    ],
    "body": "### Search before asking\n\n- [ ] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi, I want to use yolov9-s network on imx500 but unfortunately only yolo11n and yolov8n are supported at the moment. Do you have a solution to do this and can you guide me? Also, do you have any plans to add other networks in the future?\nThanks\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @sinais54, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users, where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, and where many of the most common questions may already be answered.\n\nThis is an automated response to help you get started—an Ultralytics engineer will also review your issue and assist you soon.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @sinais54,\n\nCurrently, IMX500 export is only supported for YOLOv8n and YOLO11n models due to specific memory and layer constraints required by the Sony IMX500 hardware. The exporter checks for specific layer configurations and expects exactly 168 layers for YOLOv8n or 238 layers for YOLO11n.\n\nUnfortunately, YOLOv9-s has a different architecture that isn't compatible with the current IMX500 export implementation. There's no direct workaround to export YOLOv9-s models to IMX500 format at this time.\n\nFor best results with the IMX500 hardware, I recommend using either YOLOv8n or YOLO11n and optimizing your dataset and training parameters to improve performance within these supported model architectures. If you need to use the IMX500 specifically, these are currently your best options.\n\nRegarding future support for additional models, the team is continuously working on expanding compatibility, but I don't have specific timeline information about YOLOv9 support for IMX500."
      },
      {
        "user": "maxmool",
        "body": "Hi, I have also tried to do this recently but have not been successful so far and I will be very happy if your wonderful team completes this task as always. Also, if you have any ideas and solutions to do this, I will be happy to guide me."
      }
    ]
  },
  {
    "issue_number": 20588,
    "title": "yolo11 分类模型类别不均衡，可以怎么优化呢？",
    "author": "Liuzaiyang531",
    "state": "open",
    "created_at": "2025-05-12T02:22:29Z",
    "updated_at": "2025-06-12T00:23:00Z",
    "labels": [
      "question",
      "Stale",
      "classify"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n我的数据集有些分类有上千张样本，有些分类只有几百甚至几十张样本，怎么可以稍微解决这种类别极度不均衡的问题呢\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @Liuzaiyang531, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users, where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples. Many common questions may already be answered there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response. An Ultralytics engineer will assist you here soon 😊"
      },
      {
        "user": "glenn-jocher",
        "body": "您好！类别不均衡是分类任务中常见的问题。您可以尝试以下方法来解决：\n\n1. 数据增强：对样本较少的类别应用更强的数据增强。Ultralytics支持与[Albumentations](https://docs.ultralytics.com/integrations/albumentations/)的集成，可以为少数类创建更多变体。\n\n2. 类权重调整：在训练配置中为少数类分配更高的权重，使模型更关注这些类别。\n\n3. 过采样/合成数据：对少数类进行过采样或使用生成式AI创建合成数据来平衡数据集。\n\n4. 预训练模型：利用预训练的YOLO11分类模型，因为它已经学习了良好的特征表示，可以更好地泛化到数据较少的类别。\n\n如果您需要更多关于数据多样性的信息，可以参考[这篇文章](https://www.ultralytics.com/blog/understanding-ai-bias-and-dataset-bias-in-vision-ai-systems)。"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 21005,
    "title": "Tracking inference takes longer than it says per frame",
    "author": "rtwalz",
    "state": "closed",
    "created_at": "2025-06-10T05:10:26Z",
    "updated_at": "2025-06-11T21:37:25Z",
    "labels": [
      "question",
      "track"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi there! I run people tracking with the `yolo` CLI, like this:\n\n```\nyolo track \\\n  model=yolov8n.pt \\\n  source=PATH_TO_VIDEO_FILE \\\n  vid_stride=2 \\\n  classes=0 \\\n  batch=8 \\\n  save=False \\\n  save_txt=True \\\n  imgsz=320 \\\n  device=0 \\\n  show_labels=False \\\n  show_conf=False \\\n  show_boxes=False \\\n  project=tracking \\\n  name=CLIP_NAME \\\n  save_frames=True\n```\n\nWhen I run it on a GPU (specifically on Modal.com), it says every frame takes somewhere between 1 to 2 ms to process in the logs. But the total time it takes to process is not equal to 2ms * [number of frames]. \n\nFor example, these are my logs:\n\n```\n16:22:35.823 video 1/1 (frame 1/1875) clip.mp4: 320x192, 2 persons, 1.4ms\n...\n16:22:58.891 video 1/1 (frame 1875/1875) clip.mp4: 320x192 (no detections), 1.4ms\n16:22:58.989 Speed 0.7ms preprocess, 1.4ms inference, 1.0ms postprocess per image at shape (8, 3, 320, 192)\n16:22:58.989 Results saved to tracking/20250609T230507\n```\n\nSo this took about 23 seconds. But if every frame takes 3.1ms (0.7+1.4+1.0), then shouldn't this take ~5.8 seconds instead? I assume I am missing something simple. Thank you!\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @rtwalz, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response. An Ultralytics engineer will also assist you here soon to help clarify your tracking inference timing question! ⏱️"
      },
      {
        "user": "glenn-jocher",
        "body": "The speed metrics you're seeing (0.7ms preprocess, 1.4ms inference, 1.0ms postprocess) only capture the core inference pipeline timing, not the complete end-to-end processing time. The additional ~17 seconds comes from overheads not included in these metrics: video decoding/loading, saving frames to disk (`save_frames=True`), writing txt files (`save_txt=True`), and tracker state management between frames. These I/O operations, especially saving 1875 individual frame images, can significantly increase total processing time beyond the pure inference speed."
      },
      {
        "user": "rtwalz",
        "body": "Got it, thanks @glenn-jocher!"
      }
    ]
  },
  {
    "issue_number": 21014,
    "title": "be related to yoloe",
    "author": "luyiqwerrewq",
    "state": "closed",
    "created_at": "2025-06-11T02:37:49Z",
    "updated_at": "2025-06-11T18:57:18Z",
    "labels": [
      "question",
      "track",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello, I would like to use the YOLOE model to detect videos and add tracking. How can I do it? For example, tracking people wearing green clothes in videos\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @luyiqwerrewq, thank you for your interest in Ultralytics 🚀! We recommend checking out the [Docs](https://docs.ultralytics.com/) for guidance—there you’ll find detailed [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples, which cover many frequently asked questions.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate.\n\nIf you have custom training or usage questions (like tracking or video detection), please share as much information as possible, including relevant code snippets, your dataset setup, and any logs. Don’t forget to review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community to get support and share your experience! For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧. For longer discussions, head to [Discourse](https://community.ultralytics.com/). You can also engage with others on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease ensure you’re on the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO models can be run in any of these up-to-date, verified environments (with all dependencies such as [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify proper operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu, running every 24 hours and on every commit.\n\n---\n\nThis is an automated response to help you get started quickly. An Ultralytics engineer will assist you here soon! 😊"
      },
      {
        "user": "luyiqwerrewq",
        "body": "from ultralytics import YOLOE\n\nmodel = YOLOE(\"./yoloe-11l-seg.pt\")\nnames = [\"person\"]\nmodel.set_classes(names, model.get_text_pe(names))\n\nresults = model.track(source=\"./002.mp4\",\n                      tracker=\"./custom_botsort_tracker.yaml\",\n                      save=True,  # \n                      save_txt=False,  # \n                      save_frames=True,  # \n                      project=\"./MP4/\",  # \n                      name=\"tracking_results\")  # \nI use this code,but no track id show in the results--xxx.avi"
      },
      {
        "user": "glenn-jocher",
        "body": "Looking at your code, YOLOE tracking should work, but you need to iterate through the results to access track IDs. The saved video might not automatically display track IDs. Try this to verify tracking is working:\n\n```python\nfrom ultralytics import YOLOE\n\nmodel = YOLOE(\"./yoloe-11l-seg.pt\")\nnames = [\"person\"]\nmodel.set_classes(names, model.get_text_pe(names))\n\nresults = model.track(source=\"./002.mp4\", tracker=\"./custom_botsort_tracker.yaml\", show=True)\nfor r in results:\n    if r.boxes and r.boxes.id is not None:\n        print(f\"Track IDs: {r.boxes.id.int().cpu().tolist()}\")\n```\n\nFor tracking \"people wearing green clothes\" specifically, you'd need a custom-trained model since YOLOE with \"person\" class will detect all people regardless of clothing color. The current approach detects all people and assigns track IDs to them."
      }
    ]
  },
  {
    "issue_number": 20999,
    "title": "RuntimeError: Channel Mismatch in Custom MBConv Module with YOLOv8",
    "author": "KeyzCoder",
    "state": "closed",
    "created_at": "2025-06-09T14:54:30Z",
    "updated_at": "2025-06-11T18:11:29Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello, I am encountering a RuntimeError while integrating a custom MBConv module into YOLOv8. The error occurs during training and seems to be related to a mismatch between the expected input channels and the actual input tensor's channels.\n\nError Message:\n`RuntimeError: Given groups=1, weight of size [1024, 8, 1, 1], expected input[1, 64, 64, 64] to have 8 channels, but got 64 channels instead`\n\nWhat I Have Done:\nCustom MBConv Module: I implemented a custom MBConv module and integrated it into YOLOv8. The module dynamically infers input_filters if not explicitly provided. Here's a snippet of the initialization:\n`class MBConv(nn.Module):\n    def __init__(self, ksize, input_filters=None, output_filters=None, expand_ratio=1, stride=1, image_size=224):\n        super().__init__()\n        self._input_filters = input_filters\n        self._output_filters = output_filters\n        self._expand_ratio = expand_ratio\n        self._kernel_size = ksize\n        self._stride = stride\n        # Debugging prints\n        print(f\"Initializing MBConv: ksize={ksize}, input_filters={input_filters}, output_filters={output_filters}, expand_ratio={expand_ratio}\")`\n\nYAML Configuration: I modified the YOLOv8 YAML file to include the MBConv module:\n`backbone:\n  - [-1, 3, MBConv, [3, 64, 128]]  # ksize=3, input_filters=64, output_filters=128\n  - [-1, 6, MBConv, [256, 256]]  # Layer 4`\n\nDebugging output:\n`Initializing MBConv: ksize=64, input_filters=8, output_filters=64, expand_ratio=128\nInitializing MBConv: ksize=128, input_filters=128, output_filters=256, expand_ratio=1`\n\n\nMy Questions:\n\n- How can I ensure that the input_filters dynamically inferred or passed from the YAML file matches the actual input tensor's channels?\n- Is there a specific way YOLOv8 handles custom modules that I might be missing?\n\n\n\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @KeyzCoder, thank you for reaching out and providing detailed context and code snippets! 🚀 This is an automated response to help you get started—an Ultralytics engineer will review your issue and provide further assistance soon.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug more efficiently. This should include your custom MBConv implementation, the relevant YAML configuration, and steps to reproduce the error.\n\nIf you are working on custom training or module integration, please share as much information as possible, such as dataset samples, training logs, and confirm you’ve reviewed our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nFeel free to check out our [Docs](https://docs.ultralytics.com/) for guidance on [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, as well as examples that may be helpful for your integration.\n\nFor real-time discussion, join our [Discord](https://discord.com/invite/ultralytics) 🎧. For more in-depth technical Q&A, visit [Discourse](https://community.ultralytics.com/), or engage with the community on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease ensure you are running the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in the following verified environments (with all dependencies like [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for helping improve Ultralytics! 💡"
      },
      {
        "user": "glenn-jocher",
        "body": "Looking at your debug output, the arguments are being parsed incorrectly. The issue is that YOLO's `parse_model` function automatically handles input/output channels for built-in modules but not custom ones.\n\nFor custom modules like your MBConv, YOLO passes the exact arguments from your YAML without the automatic channel management. Your `__init__` method should expect the first argument to be the input channels from the previous layer, followed by your custom arguments.\n\nTry restructuring your MBConv `__init__` to match YOLO's expected pattern:\n\n```python\nclass MBConv(nn.Module):\n    def __init__(self, c1, c2, ksize=3, expand_ratio=1, stride=1):\n        super().__init__()\n        # c1 = input channels (automatically passed by YOLO)\n        # c2 = output channels (first arg from YAML)\n        # remaining args follow\n        self._input_filters = c1\n        self._output_filters = c2\n        self._kernel_size = ksize\n        # ... rest of your implementation\n```\n\nThen update your YAML to:\n```yaml\nbackbone:\n  - [-1, 3, MBConv, [128, 3]]  # c2=128, ksize=3\n  - [-1, 6, MBConv, [256, 3]]  # c2=256, ksize=3\n```\n\nThis follows the standard YOLO pattern where the first YAML argument becomes `c2` (output channels) and `c1` (input channels) is automatically inferred from the previous layer."
      },
      {
        "user": "KeyzCoder",
        "body": "Thank you so much for your help! I was completely stuck on this error for days and was about to give up on my project.\n\nI'm so relieved this is finally working for my project. \n\nThanks again for taking the time to explain not just the fix but also WHY it works this way. This helps me understand the whole architecture so much better, and I'll definitely remember this.\nYou're awesome! "
      }
    ]
  },
  {
    "issue_number": 18829,
    "title": "Yolo incompatible with Jetpack 6.2(Jetson Orin Nano Super)",
    "author": "lida2003",
    "state": "closed",
    "created_at": "2025-01-22T22:21:29Z",
    "updated_at": "2025-06-11T15:55:26Z",
    "labels": [
      "bug",
      "dependencies",
      "embedded"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nI just installed Jepack 6.2, and sudo `sudo pip3 install ultralytics`.\n\nIt seems it uses libcudnn.so.8 not cuDNN: 9.3.0.75. \n\nThe issue might be pytorch, as I didn't see any correct version for L4T36.4.3, which is for Super performance: https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048\n\nAny ideas?\n\nSee below link for details:\n\nhttps://forums.developer.nvidia.com/t/yolo-incompatible-with-jetpack-6-2-jetson-orin-nano-super/321078\n\n### Environment\n\n```\nSoftware part of jetson-stats 4.3.1 - (c) 2024, Raffaello Bonghi\nModel: NVIDIA Jetson Orin Nano Developer Kit - Jetpack 6.2 [L4T 36.4.3]\nNV Power Mode[0]: 15W\nSerial Number: [XXX Show with: jetson_release -s XXX]\nHardware:\n - P-Number: p3767-0005\n - Module: NVIDIA Jetson Orin Nano (Developer kit)\nPlatform:\n - Distribution: Ubuntu 22.04 Jammy Jellyfish\n - Release: 5.15.148-tegra\njtop:\n - Version: 4.3.1\n - Service: Active\nLibraries:\n - CUDA: 12.6.68\n - cuDNN: 9.3.0.75\n - TensorRT: 10.3.0.30\n - VPI: 3.2.4\n - Vulkan: 1.3.204\n - OpenCV: 4.11.0 - with CUDA: YES\n```\n\n### Minimal Reproducible Example\n\n\n```\nsudo pip3 install ultralytics\n```\n\n### Additional\n\n\n\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @lida2003, thank you for bringing this issue to our attention 🚀! We suggest checking out the [Docs](https://docs.ultralytics.com/) for helpful information, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples that may address your question.\n\nIf this is a 🐛 Bug Report, which seems to be the case here, please ensure you provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) in your post so we can efficiently debug the issue. From the details you’ve provided, it appears the installation on Jetson Orin Nano with Jetpack 6.2 may have encountered compatibility challenges with the available `libcudnn` and `cuDNN` versions.\n\nIf this issue pertains to custom hardware setups, you might also want to ensure all dependencies, including CUDA, cuDNN, and PyTorch, are correctly aligned with the versions required by Ultralytics. You can verify and install the latest `ultralytics` package in a [**Python>=3.8**](https://www.python.org/) environment with the following command:\n\n```bash\npip install -U ultralytics\n```\n\n---\n\n### Compatibility and Environments\n\nUltralytics YOLO supports various verified environments with preinstalled dependencies:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n- **Jetson devices**: Custom configurations may involve building PyTorch and libraries from source. See the [Ultralytics Forums](https://community.ultralytics.com/) for device-specific assistance.\n\n---\n\n### Community Support\n\nJoin the Ultralytics community for additional insights and support:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for real-time chat.\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions.\n- [Subreddit](https://reddit.com/r/Ultralytics) for knowledge sharing.\n\n---\n\n## Status\n\nPlease also ensure the [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests (badge below) are green, as this confirms compatibility across various operations:\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\n---\n\nAn Ultralytics engineer will review your report and provide further assistance soon. Thank you for your patience! 🚀"
      },
      {
        "user": "Y-T-G",
        "body": "You can install PyTorch 2.5 using the wheel link here:\n\nhttps://developer.download.nvidia.com/compute/redist/jp/v61/pytorch/"
      },
      {
        "user": "lida2003",
        "body": "pytorch libcudnn.so.8 link issue, should use v9 cudnn for jetpack6.2"
      }
    ]
  },
  {
    "issue_number": 20136,
    "title": "Freezing a certain layer during the training process",
    "author": "cainiao123s",
    "state": "open",
    "created_at": "2025-04-11T09:24:43Z",
    "updated_at": "2025-06-11T14:21:10Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello, may I ask if during the training process, I can set to freeze a certain layer only when epochs=10? The layer will not be frozen in other epochs.\n\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @cainiao123s, thank you for your interest in Ultralytics 🚀! We recommend exploring the [Docs](https://docs.ultralytics.com/) for guidance where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples. Many of the most common questions may already be answered there.\n\nIf this is a custom training ❓ Question, please provide as much information as possible to assist us in understanding your use case, such as a description of the model, training logs, and dataset specifics. You can also check out our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/) for optimization advice.\n\nIf this is a 🐛 Bug Report, please include a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to allow us to debug the issue effectively.\n\nJoin the Ultralytics community where it suits you best:\n- Real-time chat? Join us on [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth discussions? Check out [Discourse](https://community.ultralytics.com/)\n- Engaging community threads? Visit our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nEnsure you are using the latest `ultralytics` package and verify your issue is not already resolved. Upgrade in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in these up-to-date verified environments with all dependencies preinstalled:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests validate the correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu.\n\nThis is an automated response 🛠️, but an Ultralytics engineer will assist you shortly! 😊"
      },
      {
        "user": "Y-T-G",
        "body": "You can do that using callbacks.\n\nhttps://github.com/ultralytics/ultralytics/blob/e19f0ef606c67f028a998b23223ead689ad4c8c6/ultralytics/engine/trainer.py#L431"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 5077,
    "title": "Difference between box precision, mask precision and confusion matrix precision?",
    "author": "cozeybozey",
    "state": "closed",
    "created_at": "2023-09-25T10:31:27Z",
    "updated_at": "2025-06-11T03:37:59Z",
    "labels": [
      "question"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi I don't really understand how the 3 precisions mentioned in the title can differ from one another. What does it mean when the box precision differs from the precision seen in the confusion matrix? Here is an example:\r\n\r\nIn the image below you can see that the car precision and recall are higher than zero. But in the confusion matrix there are no true positives for the car, so the recall and precision should be zero. \r\n![image](https://github.com/ultralytics/ultralytics/assets/71214958/a55277c5-1668-4b26-b745-fc16d956f714)\r\n![image](https://github.com/ultralytics/ultralytics/assets/71214958/86220832-15a8-491c-8506-576be386361f)\r\n\r\nAlso the box precisions differs from the mask precisions.\r\n\r\nMany thanks in advance!\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @cozeybozey, thank you for your interest in YOLOv8 🚀! We recommend a visit to the [YOLOv8 Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@cozeybozey hello, thank you for your question and for taking the time to provide detailed examples! This question goes into the core concepts of performance metrics in the field of detection models like YOLOv8. \n\nLet me explain these three kinds of precision:\n\n1. **Box precision** refers to the agreement between the predicted boxes and the ground truth boxes. This is calculated based on bounding box coordinates (x, y, width, height). The evaluation metric used is IoU (Intersection over Union), a common evaluation metric for object detection tasks.\n\n2. **Mask precision** refers to the agreement between the predicted segmentation masks and the ground truth masks. This is applicable if the model supports segmentation (a pixel-wise classification of objects). The mask precision comes into play when the model does not only detect the object but also tries to delineate the extent of the object in question.\n\n3. **Confusion matrix precision** is a metric derived from the confusion matrix, which summarizes the model performance across all classes. Precision here is calculated as the ratio of True Positives to the sum of True Positives and False Positives. This metric does not include the geometric agreement between predictions and ground truths, but focuses only on classification accuracy.\n\nIn your specific example, what might be happening is that the model is predicting the bounding boxes with reasonable accuracy (hence, non-zero box precision), but not classifying them as cars correctly. This could explain why there are no True Positives for cars in your confusion matrix, leading to zero precision as per this metric.\n\nRegarding the difference between box precision and mask precision, it primarily comes down to the tasks. Object detection and semantic/instance segmentation are different tasks, thus their evaluation metrics (like box precision and mask precision) might not agree with each other.\n\nRemember, different tasks require different metrics to properly evaluate model performance, so it's common for those metrics to differ. It ultimately depends on what specific aspect of performance you're trying to evaluate.\n\nHope that helps! Let us know if you have any additional questions.\n"
      },
      {
        "user": "cozeybozey",
        "body": "Thanks for the response, that makes it a lot more clear! I still am a bit confused though when a bounding box is regarded as a true positive. From what I understand from your example it is possible that a model predicts a bounding box around a car, but for example classifies it as person. Doing this will lead to a true positive bounding box, but not a true positive for the confusion matrix. Am I understanding that correctly? "
      }
    ]
  },
  {
    "issue_number": 20332,
    "title": "Training segmentation model on COCO dataset with panoptic annotations - performance issues",
    "author": "xperroni",
    "state": "open",
    "created_at": "2025-04-24T15:37:38Z",
    "updated_at": "2025-06-11T03:01:05Z",
    "labels": [
      "question",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI wrote a [script](https://github.com/xperroni/annotate_for_yolo) to convert the 133-class COCO panoptic segmentation annotations to the [YOLO instance segmentation dataset format](https://docs.ultralytics.com/datasets/segment/). I then used the converted annotations to train the `yolo11n-seg` model with:\n\n    yolo segment train data=coco_panoptic.yaml model=yolo11n-seg.pt epochs=600\n\nHowever, training results were rather underwhelming. By the 300th epoch mAP(box)50-95 has all but converged to around 22.0, and mAP(mask)50-95 to around 18.0. These are way below the mAP(box)50-95 of 38.9 and the mAP(mask)50-95 of 32.0 [reported](https://docs.ultralytics.com/tasks/segment/#models) for the `yolo11n-seg` model trained on COCO with Ultralytics' custom 80-class segmentation annotations.\n\nHow do I improve those numbers? I know there are various [training settings](https://docs.ultralytics.com/modes/train/#resuming-interrupted-trainings) I could play with, but it's hard to know where to start. Is there anywhere I can find the settings used for the Ultralytics pretrained models?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @xperroni, thank you for your detailed question and for using Ultralytics 🚀! This is an automated response to help you get started while an Ultralytics engineer reviews your issue and will assist you further soon.\n\nWe recommend checking out the [Docs](https://docs.ultralytics.com/) for insights into [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples, as well as answers to many common questions.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) if you haven't already. This helps our team debug more effectively.\n\nIf you have a custom training ❓ Question (like this one), please provide as much detail as possible, including sample images from your dataset, training logs, and confirm you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nFor real-time chat and support, join our [Discord](https://discord.com/invite/ultralytics) 🎧. If you prefer more in-depth discussion, visit [Discourse](https://community.ultralytics.com/), or join conversations on our [Subreddit](https://reddit.com/r/Ultralytics) to connect with the community.\n\n## Upgrade\n\nEnsure you’re using the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for reaching out and sharing your experience! An Ultralytics engineer will follow up soon to help you further."
      },
      {
        "user": "Y-T-G",
        "body": "Can you post the plots of the label visualizations generated inside runs folder?"
      },
      {
        "user": "xperroni",
        "body": "Sorry for the delayed response. Do you mean these?\n\n![Image](https://github.com/user-attachments/assets/8860b1b3-8a85-4c1d-afdc-de507dd6c8a6)\n![Image](https://github.com/user-attachments/assets/a2432641-5322-4ae2-a8cb-0da7891b2629)\n![Image](https://github.com/user-attachments/assets/4e604d53-29d6-43a4-b4d4-f9d4def6c714)\n![Image](https://github.com/user-attachments/assets/8796a98c-4c82-41ba-866b-65c43d683f76)\n![Image](https://github.com/user-attachments/assets/71eaf09c-04d0-444b-a22a-e42e64acb80c)"
      }
    ]
  },
  {
    "issue_number": 9356,
    "title": "Resume YOLOv8 Training On GPU",
    "author": "HuzaifaEhsan421",
    "state": "closed",
    "created_at": "2024-03-27T12:03:01Z",
    "updated_at": "2025-06-11T02:56:59Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\r\n\r\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\r\n\r\n\r\n### Question\r\n\r\ncan i resume training yolov8 on gpu, when i start training it is on CPU now I want to shift to Gpu, is there a way doing it?\r\n\r\nI have tried one approach \r\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n\r\n\r\nmodel = YOLO(\"last.pt\")\r\nmodel.to(device)\r\nmodel.train(resume=True)\r\n\r\nBut above approach gives me error on line model.train(resume=True)\r\n\r\nAssertionError: Invalid device id\r\n\r\nCan anyone helpme out in resolving this issue.\r\n\r\n\r\n### Additional\r\n\r\n_No response_",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @HuzaifaEhsan421, thank you for your interest in Ultralytics YOLOv8 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "@HuzaifaEhsan421 hello! 👋 \n\nAbsolutely, you can resume training your YOLOv8 model on a GPU if it started on a CPU. The approach you tried is on the right track but slightly misaligned with how YOLOv8's API is structured for training operations. To resume training on a GPU, you can use the CLI command or adjust your script slightly. Here's a simpler way to achieve this using the CLI, which automatically handles device allocation:\n\n```bash\nyolo train model=last.pt resume=True\n```\n\nThis command will automatically detect and use your GPU if it's available. No need to manually manage the device in your code. If you're using a script and want to explicitly set the device, ensure your CUDA device ID is correct or simply use `'cuda'` if using the first available GPU. Also, `resume` functionality is handled directly by the `train` method without manually moving your model to the device:\n\n```python\nfrom ultralytics import YOLO\n\n# Load your model, specifying device at load time if needed\nmodel = YOLO('last.pt', device='cuda')\n\n# Resume training\nmodel.train(resume=True)\n```\n\nThis should get you back on track without the `AssertionError`. If you continue to experience issues, ensure your CUDA environment is correctly set up and accessible to PyTorch. 🚀\n\nHope this helps! Let us know if you have any more questions."
      },
      {
        "user": "HuzaifaEhsan421",
        "body": "Hi @glenn-jocher \r\nI have also tried this approach but this also raises an error.\r\nTypeError: YOLO.__init__() got an unexpected keyword argument 'device'\r\n"
      }
    ]
  },
  {
    "issue_number": 20992,
    "title": "how to add crop augmentation in current pipline",
    "author": "xuan-xuan6",
    "state": "open",
    "created_at": "2025-06-09T03:12:29Z",
    "updated_at": "2025-06-11T02:20:20Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nhttps://github.com/ultralytics/ultralytics/issues/20675, I had saw it. I also want to add crop method in my train pipline.but this issues provide less detail to reach it. who can help to provide the detail? \n\n\n`from ultralytics import YOLO\n\nmodel = YOLO('./ultralytics/cfg/models/v8/yolov8n.yaml').load('./pre_weights/v8/yolov8n.pt')\n\n# Display model information\nmodel.info()\n\n# Train the model\nresults = model.train(\n    data='./ultralytics/cfg/datasets/weiping-spot-det_origin.yaml',\n    task=\"detect\",\n    epochs=140, \n    batch=8,\n    workers=4, \n    imgsz=1280,\n    scale=0.3, \n    mosaic=1.0,\n    mixup=0.0,  \n    copy_paste=0.7,  \n    flipud=0.5,\n    fliplr=0.5,\n    degrees=0.3,\n    translate=0.1,\n    cutmix = 0.0,\n    device=\"0\",\n    patience=50,\n    close_mosaic=0,\n    name = 'origin_det',\n    cache='disk',\n  )\n`\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @xuan-xuan6, thank you for reaching out and for your interest in Ultralytics 🚀! This is an automated response to help you get started, and an Ultralytics engineer will also assist you soon.\n\nIf you are looking to add a crop augmentation to your current pipeline, we recommend checking out the [Docs](https://docs.ultralytics.com/) for extensive guidance on augmentation options and pipeline customization. For reference, you may want to review general augmentation configuration in both [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) workflows.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf you are seeking custom training advice ❓, please include as much detail as possible, such as dataset image samples and complete training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community for more support and discussion! For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧. For deeper technical topics, see [Discourse](https://community.ultralytics.com/). You can also connect on our [Subreddit](https://reddit.com/r/Ultralytics) to share experiences and solutions.\n\n## Upgrade\n\nPlease ensure you are using the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following up-to-date verified environments, all with dependencies (including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/)) preinstalled:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n😊 Thank you again for your question and your patience!"
      },
      {
        "user": "glenn-jocher",
        "body": "Currently, YOLO training doesn't include a built-in `crop` augmentation parameter in the standard training configuration. The [augmentation settings](https://docs.ultralytics.com/usage/cfg/) include parameters like `scale`, `translate`, `mosaic`, etc., but not random cropping.\n\nHowever, there is a `CenterCrop` class available in `ultralytics.data.augment.CenterCrop`, though it's primarily designed for classification tasks. For detection tasks, the existing `scale` and `translate` parameters you're already using provide similar benefits by scaling images and translating them within the frame.\n\nIf you specifically need random cropping functionality, you would need to implement a custom augmentation transform and integrate it into the training pipeline by modifying the transform composition in the data loading process."
      },
      {
        "user": "shining-love",
        "body": "> Currently, YOLO training doesn't include a built-in `crop` augmentation parameter in the standard training configuration. The [augmentation settings](https://docs.ultralytics.com/usage/cfg/) include parameters like `scale`, `translate`, `mosaic`, etc., but not random cropping.\n> \n> However, there is a `CenterCrop` class available in `ultralytics.data.augment.CenterCrop`, though it's primarily designed for classification tasks. For detection tasks, the existing `scale` and `translate` parameters you're already using provide similar benefits by scaling images and translating them within the frame.\n> \n> If you specifically need random cropping functionality, you would need to implement a custom augmentation transform and integrate it into the training pipeline by modifying the transform composition in the data loading process.\n\nThanks for your reply. how can i modifying the transform composition in the data loading process? can you give me a example to modify? thanks"
      }
    ]
  },
  {
    "issue_number": 20580,
    "title": "using single category detection and the recall rate in the output of YOLOV8's' val 'is not equal to the recall rate in the confusion matrix.",
    "author": "AllenGitHub1",
    "state": "open",
    "created_at": "2025-05-10T19:49:55Z",
    "updated_at": "2025-06-11T00:23:21Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHello, I am using single category detection and the recall rate in the output of YOLOV8's' val 'is not equal to the recall rate in the confusion matrix. Why is that? In theory, the two should be equal.\n\n![Image](https://github.com/user-attachments/assets/7c88206a-56ad-433d-9879-85309cb41076)\n\n![Image](https://github.com/user-attachments/assets/6b07bde0-c8c3-45fd-bf57-b18adbd20251)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @AllenGitHub1, thank you for reaching out and for using Ultralytics 🚀! We appreciate your detailed question and the screenshots provided.\n\nAs this is an automated response, an Ultralytics engineer will review your issue and assist you further soon.\n\nIf this is a 🐛 Bug Report, please help us by providing a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) (MRE). This will help us understand and debug the issue more efficiently.\n\nIf you are seeking clarification about custom training or evaluation results, please provide as much information as possible, such as your dataset structure, relevant code snippets, and full validation logs. Also, double-check that you're following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nYou can find answers to many common questions in the [Docs](https://docs.ultralytics.com/), including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples.\n\nJoin the Ultralytics community for more support:\n- Real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth discussions on [Discourse](https://community.ultralytics.com/)\n- Share knowledge on our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package, including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml), in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to ensure your issue isn’t already resolved:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for helping us improve Ultralytics!"
      },
      {
        "user": "AllenGitHub1",
        "body": "Everyone, please help me answer this question. I need it for my graduation thesis. Thank you very much"
      },
      {
        "user": "Y-T-G",
        "body": "You can read this \n\nhttps://github.com/ultralytics/ultralytics/issues/17047#issuecomment-2424660435"
      }
    ]
  },
  {
    "issue_number": 8837,
    "title": "YOLOv8 Classification Setup",
    "author": "zshar7",
    "state": "closed",
    "created_at": "2024-03-11T02:00:38Z",
    "updated_at": "2025-06-10T21:57:36Z",
    "labels": [
      "question",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI am working on yolov8-cls but I am unsure to configure my files to train a model.\r\n\r\nMy models are like this\r\ndata\r\n-train\r\n--class 1\r\n---image 1\r\n---image 2\r\n...\r\n--class2\r\n...\r\n-test\r\n--class 1\r\n---image 1\r\n...\r\n-val\r\n--class 1\r\n--image 1\r\n\r\netc etc\r\n\r\nIn the data folder, there are three folders, train, test, and val, and in those three folders are folders with the name of the class and the images contained.\r\n\r\nHow would I write a data.yaml for this and reference that in a cli command:\r\nyolo classify train data=data/data.yaml model=yolov8n-cls.yaml epochs=100 imgsz=640\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "glenn-jocher",
        "body": "@zshariff6506 hello! For setting up your `data.yaml` for YOLOv8 classification, you'll want to structure it to reference your dataset's directory structure. Given your setup, your `data.yaml` might look something like this:\n\n```yaml\ntrain: data/train  # path to training data\nval: data/val  # path to validation data\ntest: data/test  # path to test data\n\nnc: 2  # number of classes\nnames: ['class1', 'class2']  # class names\n```\n\nMake sure to adjust `nc` (number of classes) and `names` (list of class names) to match your actual dataset.\n\nThen, your CLI command looks almost correct. Just ensure your `data.yaml` and model configuration file paths are correctly specified. If they're in the same directory from which you're running the command, your command should work as is:\n\n```bash\nyolo classify train data=data.yaml model=yolov8n-cls.yaml epochs=100 imgsz=640\n```\n\nIf your files are in different directories, just provide the relative or absolute paths to them. Hope this helps! 😊"
      },
      {
        "user": "zshar7",
        "body": "So now YOLO is looking inside of the data.yaml for train images:\r\nraise RuntimeError(emojis(f\"Dataset '{clean_url(self.args.data)}' error ❌ {e}\")) from e\r\nRuntimeError: Dataset 'data.yaml' error  [WinError 3] The system cannot find the path specified: 'C:\\\\..\\\\.data.yaml\\\\train'\r\n\r\nThis is data.yaml:\r\ntrain: ../train/  # path to training data\r\nval: ../valid/  # path to validation data\r\ntest: ../test/  # path to test data\r\n\r\nnc: 6  # number of classes\r\nnames: ['chicken post', 'chicken pre', 'liver pre', 'liverpost', 'muscle post', 'muscle pre']  # class names"
      },
      {
        "user": "glenn-jocher",
        "body": "@zshariff6506 hey there! It looks like there might be a small hiccup with the paths in your `data.yaml`. The error suggests that YOLOv8 can't find the specified `train` directory. Let's try adjusting the paths a bit. 🛠️\n\nGiven the error message, it seems like the paths in your `data.yaml` might not be correctly pointing to where your data actually resides. If your `data.yaml` file is located at the root of your project, and your data folders (`train`, `val`, `test`) are also at the root, your paths should not start with `../` (which goes up one directory). Instead, they should be like this:\n\n```yaml\ntrain: train/  # path to training data\nval: valid/  # path to validation data\ntest: test/  # path to test data\n\nnc: 6  # number of classes\nnames: ['chicken post', 'chicken pre', 'liver pre', 'liverpost', 'muscle post', 'muscle pre']  # class names\n```\n\nIf your `data.yaml` is in a different directory, you'll need to adjust the paths accordingly to correctly point to your data directories. Double-check the paths and try running your command again. Hope this helps! 😊"
      }
    ]
  },
  {
    "issue_number": 20915,
    "title": "Multi-Node Multi-GPU training parameter WORLD_SIZE and LOCAL_RANK and RANK setting [SOLVED]",
    "author": "axfbh",
    "state": "open",
    "created_at": "2025-06-03T09:26:27Z",
    "updated_at": "2025-06-10T20:03:13Z",
    "labels": [
      "bug",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nMulti-GPU\n\n### Bug\ni was using to run a group nodes (2 machines with 2 gpu each) training yolo-ultralytics, i came across the issues #7038 and i solved the error code. when i  launching multi-gpu by torchrun, i got an error with \"RuntimeError: CUDA error: invalid device ordinal Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\" \n\ni solved that problem, and it can work with origin ultraltyics framework with not error.\n\n### Environment\n\nCUDA: 12.6\nGPU: 2070\nUbuntu: 24.04.2\nPython: 3.10.0\n\nultralytics: 8.3.146\ntorch: 2.6.0+cu126\n\n### Minimal Reproducible Example\n\n### Solved Step\nActually, torch.device is assigned by a wrong gpu id.\n\ni add a code in **utils/\\__init\\__.py** as follwing\n```\nWORLD_SIZE = int(os.getenv(\"WORLD_SIZE\", -1))\n```\n\ni change the **engine/trainer.py** file as following \n```\nfrom ultralytics.utils import (\n    DEFAULT_CFG,\n    LOCAL_RANK,\n    WORLD_SIZE,\n    LOGGER,\n    RANK,\n    TQDM,\n    YAML,\n    callbacks,\n    clean_url,\n    colorstr,\n    emojis,\n)\n```\n```\n    def train(self):\n        print('WORLD_SIZE', WORLD_SIZE)\n        \"\"\"Allow device='', device=None on Multi-GPU systems to default to device=0.\"\"\"\n        if WORLD_SIZE != -1:\n            world_size = WORLD_SIZE\n        elif isinstance(self.args.device, str) and len(self.args.device):  # i.e. device='0' or device='0,1,2,3'\n            world_size = len(self.args.device.split(\",\"))\n        elif isinstance(self.args.device, (tuple, list)):  # i.e. device=[0, 1, 2, 3] (multi-GPU from CLI is list)\n            world_size = len(self.args.device)\n        elif self.args.device in {\"cpu\", \"mps\"}:  # i.e. device='cpu' or 'mps'\n            world_size = 0\n        elif torch.cuda.is_available():  # i.e. device=None or device='' or device=number\n            world_size = 1  # default to device 0\n        else:  # i.e. device=None or device=''\n            world_size = 0\n\n        # Run subprocess if DDP training, else train normally\n        if world_size > 1 and \"LOCAL_RANK\" not in os.environ:\n            # Argument checks\n            if self.args.rect:\n                LOGGER.warning(\"'rect=True' is incompatible with Multi-GPU training, setting 'rect=False'\")\n                self.args.rect = False\n            if self.args.batch < 1.0:\n                LOGGER.warning(\n                    \"'batch<1' for AutoBatch is incompatible with Multi-GPU training, setting default 'batch=16'\"\n                )\n                self.args.batch = 16\n\n            # Command\n            cmd, file = generate_ddp_command(world_size, self)\n            try:\n                LOGGER.info(f\"{colorstr('DDP:')} debug command {' '.join(cmd)}\")\n                subprocess.run(cmd, check=True)\n            except Exception as e:\n                raise e\n            finally:\n                ddp_cleanup(self, str(file))\n\n        else:\n            self._do_train(world_size)\n\n```\n\n```\n    def _setup_ddp(self, world_size):\n        \"\"\"Initialize and set the DistributedDataParallel parameters for training.\"\"\"\n        print('GPUS',torch.cuda.device_count())\n        print('LOCAL_RANK', LOCAL_RANK,'RANK', RANK)\n        print('Device Name',torch.cuda.get_device_name(LOCAL_RANK))\n        torch.cuda.set_device(LOCAL_RANK)\n        self.device = torch.device(\"cuda\", LOCAL_RANK)\n        \n#        torch.cuda.set_device(RANK)\n#        self.device = torch.device(\"cuda\", RANK)\n        # LOGGER.info(f'DDP info: RANK {RANK}, WORLD_SIZE {world_size}, DEVICE {self.device}')\n        os.environ[\"TORCH_NCCL_BLOCKING_WAIT\"] = \"1\"  # set to enforce timeout\n        dist.init_process_group(\n            backend=\"nccl\" if dist.is_nccl_available() else \"gloo\",\n            timeout=timedelta(seconds=10800),  # 3 hours\n            rank=RANK,\n            world_size=world_size,\n        )\n```\n\nThe train code file is showed below:\n```\nfrom ultralytics import YOLO\nfrom ultralytics.utils import (\n    LOCAL_RANK,\n    WORLD_SIZE,\n    LOGGER,\n    RANK,\n)\nimport os\nos.environ[\"NCCL_DEBUG\"] = \"INFO\"\nos.environ[\"NCCL_SOCKET_NTHREADS\"] = \"2\"\nos.environ[\"NCCL_NSOCKS_PERTHREAD\"] = \"4\"\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n\nif __name__ == '__main__':\n    model = YOLO(\"yolo11s.pt\")  # Load a pretrained model\n    results = model.train(data=\"skinning.yaml\", epochs=100, imgsz=640, batch=24, mosaic=0,\n                          device=[LOCAL_RANK],\n                          amp=False,\n                          translate=0.12431,\n                          scale=0.07643)\n\n```\n\nThe launch script is for node=0 is\n```\ntorchrun --nproc_per_node=2 --nnodes=2 --node_rank=0 --master_addr=192.168.0.60 --master_port=23456 main.py\n```\nfor node=1 is\n```\ntorchrun --nproc_per_node=2 --nnodes=2 --node_rank=1 --master_addr=192.168.0.60 --master_port=23456 main.py\n```\n\ni still got the error \"RuntimeError: CUDA error: invalid device ordinal Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\" \n\ni found that each rank only got one gpus, because select_device set os.environ[CUDA_VISIBLE_DEVICES] = device, and train code we used device=[LOCAL_RANK]. \n\nTherefore we mark  os.environ[CUDA_VISIBLE_DEVICES] = device in select_device function in **utils/torch_uilts.py** file, as follow:\n```\n#os.environ[\"CUDA_VISIBLE_DEVICES\"] = device  # set environment variable - must be before assert is_available()\n```\n\nafter that, i got the other error with \" File \"/home/miniconda3/envs/yolo_p/lib/python3.10/site-packages/torch/distributed/utils.py\"\", line 123, in to_map [rank2]:   return (obj.to(target_device),) \", \n\ni found that nn.parallel.DistributedDataParalle device_ids was set by RANK cause target_device in node=1 is error as target_device='cuda:2' target_device='cuda:3'. \n\nTherefore i change RANK to LOCAL_RANK in _setup_train function in **engine/trainer.py** file, as follow:\n```\nself.model = nn.parallel.DistributedDataParallel(self.model, device_ids=[LOCAL_RANK], find_unused_parameters=True)\n```\n\nALL DOWN\n\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [x] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @axfbh, thank you for sharing your detailed experience and solution with Multi-GPU training on Ultralytics 🚀! We appreciate the effort to document your debugging and the steps you took to resolve the CUDA device mapping issues.\n\nThis is an automated response to help guide your next steps. An Ultralytics engineer will also review your issue and assist you shortly.\n\nIf your report is 🐛 a Bug Report, please ensure you’ve included a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/) so we can replicate and debug the issue as efficiently as possible.\n\nIf you have implementation questions or want to discuss your approach, please provide any additional details, dataset samples, or training logs that might help our team.\n\nFor quick answers or to engage with the community:\n- Real-time chat: [Discord](https://discord.com/invite/ultralytics) 🎧\n- In-depth questions: [Discourse](https://community.ultralytics.com/)\n- Discussions and sharing: [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nBefore proceeding, please make sure you’re using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO runs in the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for your contribution! If you plan to submit a PR with your improvements, please ensure your code is up to date with the latest main branch. Looking forward to your PR and further discussion! 🚀"
      },
      {
        "user": "glenn-jocher",
        "body": "Thank you for the detailed analysis and solution! Your findings are correct - the current implementation has issues with multi-node training where `RANK` is used instead of `LOCAL_RANK` for device assignment, causing the \"invalid device ordinal\" error since `RANK` can exceed the number of GPUs per node.\n\nYour key fixes make sense:\n1. Using `WORLD_SIZE` from environment when available\n2. Using `LOCAL_RANK` for `torch.cuda.set_device()` and device creation\n3. Using `LOCAL_RANK` for `DistributedDataParallel` device_ids\n4. Avoiding `CUDA_VISIBLE_DEVICES` interference in multi-node setups\n\nPlease do submit a PR with these changes - this would be a valuable contribution to improve multi-node training support. Make sure to include your test case and environment details in the PR description."
      },
      {
        "user": "axfbh",
        "body": "Hello, i found a new way to compatible the original framework.\n\nonce you finished, i mentioned before work, you can do this a little change to improve your framework\n\nfirst step\ni **remove** the world_size and **change** the device parameter in the **training code**, as below\n```\nfrom ultralytics import YOLO\nimport os\nos.environ[\"NCCL_DEBUG\"] = \"INFO\"\nos.environ[\"NCCL_SOCKET_NTHREADS\"] = \"2\"\nos.environ[\"NCCL_NSOCKS_PERTHREAD\"] = \"4\"\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n\nif __name__ == '__main__':\n    model = YOLO(\"yolo11s.pt\")  # Load a pretrained model\n    results = model.train(data=\"skinning.yaml\", epochs=100, imgsz=640, batch=24, mosaic=0,\n                          device=[0,1], # which means, what's gpu id you can see in your each node, you have to specific gpus in your each machine.\n                          amp=False,\n                          translate=0.12431,\n                          scale=0.07643)\n```\n\nsecond step\ni **unmark** a code from **select_device**, as below\n\n```\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = device  # set environment variable - must be before assert is_available()\n```\n\nThis is work can run multi-node multi-gpu with less modify origin code\n\n> Thank you for the detailed analysis and solution! Your findings are correct - the current implementation has issues with multi-node training where `RANK` is used instead of `LOCAL_RANK` for device assignment, causing the \"invalid device ordinal\" error since `RANK` can exceed the number of GPUs per node.\n> \n> Your key fixes make sense:\n> \n> 1. Using `WORLD_SIZE` from environment when available\n> 2. Using `LOCAL_RANK` for `torch.cuda.set_device()` and device creation\n> 3. Using `LOCAL_RANK` for `DistributedDataParallel` device_ids\n> 4. Avoiding `CUDA_VISIBLE_DEVICES` interference in multi-node setups\n> \n> Please do submit a PR with these changes - this would be a valuable contribution to improve multi-node training support. Make sure to include your test case and environment details in the PR description.\n\n"
      }
    ]
  },
  {
    "issue_number": 13004,
    "title": "YoloV8 CLI Training not generating Tensorflow events files",
    "author": "nickcharles",
    "state": "closed",
    "created_at": "2024-05-22T01:11:34Z",
    "updated_at": "2025-06-10T17:43:42Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### YOLOv8 Component\n\nTrain, Integrations\n\n### Bug\n\nI'm running some image classification training runs on a VM. I'm trying to monitor the training run with Tensorboard. The training run is completing correctly, but Tensorboard is showing no data. Looking in the `runs` directory, YoloV8 doesn't seem to be generating Tensorflow events files for Tensorboard to read.\r\n\r\nOutput of CLI invocation up until training data.\r\n```\r\n$ yolo task=classify mode=train model=yolov8x-cls.pt data=./quality-obliques-simple epochs=300 batch=-1\r\n\r\nDownloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-cls.pt to 'yolov8x-cls.pt'...                                              \r\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110M/110M [00:00<00:00, 367MB/s]\r\nNew https://pypi.org/project/ultralytics/8.2.18 available 😃 Update with 'pip install -U ultralytics'                                                         \r\nUltralytics YOLOv8.2.8 🚀 Python-3.10.14 torch-2.3.0 CUDA:0 (Tesla T4, 14931MiB)                                                                              \r\nengine/trainer: task=classify, mode=train, model=yolov8x-cls.pt, data=./quality-obliques-simple, epochs=300, time=None, patience=100, batch=-1, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train                  \r\ntrain: /home/nick/quality-obliques-simple/train... found 22029 images in 3 classes ✅                                                                         \r\nval: /home/nick/quality-obliques-simple/val... found 6297 images in 3 classes ✅                                                                              \r\ntest: /home/nick/quality-obliques-simple/test... found 3146 images in 3 classes ✅                                                                            \r\nOverriding model.yaml nc=1000 with nc=3                                                                                                                       \r\n                                                                                                                                                              \r\n                   from  n    params  module                                       arguments                                                                  \r\n  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                                                              \r\n  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]                                                            \r\n  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]                                                        \r\n  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]                                                           \r\n  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]                                                        \r\n  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]                                                           \r\n  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]                                                        \r\n  7                  -1  1   7375360  ultralytics.nn.modules.conv.Conv             [640, 1280, 3, 2]                                                          \r\n  8                  -1  3  27865600  ultralytics.nn.modules.block.C2f             [1280, 1280, 3, True]                                                      \r\n  9                  -1  1   1644803  ultralytics.nn.modules.head.Classify         [1280, 3]                                                                  \r\nYOLOv8x-cls summary: 183 layers, 56145683 parameters, 56145683 gradients                                                                                      \r\nTransferred 300/302 items from pretrained weights                                                                                                             \r\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...                                                                                           \r\nDownloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...                                                      \r\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6.23M/6.23M [00:00<00:00, 126MB/s]\r\n/opt/conda/envs/training-env/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608935911/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)                                                                                                           \r\n  return F.conv2d(input, weight, bias, self.stride,                                                                                                           \r\nAMP: checks passed ✅                                                                                                                                         \r\nAutoBatch: Computing optimal batch size for imgsz=224                                                                                                         \r\nAutoBatch: CUDA:0 (Tesla T4) 14.58G total, 0.50G reserved, 0.46G allocated, 13.61G free                                                                       \r\n      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output                                            \r\n    56145683           0         1.237         45.51           117        (1, 3, 224, 224)                  (1, 3)                                            \r\n    56145683           0         1.252         45.18         54.38        (2, 3, 224, 224)                  (2, 3)                                            \r\n    56145683           0         1.372         40.29         52.06        (4, 3, 224, 224)                  (4, 3)                                            \r\n    56145683           0         1.745         41.48         54.58        (8, 3, 224, 224)                  (8, 3)                                            \r\n    56145683           0         2.158         48.19         68.33       (16, 3, 224, 224)                 (16, 3)                                            \r\nAutoBatch: Using batch-size 108 for CUDA:0 9.07G/14.58G (62%) ✅                                                                                              \r\ntrain: Scanning /home/nick/quality-obliques-simple/train... 22029 images, 0 corrupt: 100%|██████████| 22029/22029 [00:11<00:00, 1960.20it/s]                  \r\ntrain: New cache created: /home/nick/quality-obliques-simple/train.cache                                                                                      \r\nval: Scanning /home/nick/quality-obliques-simple/val... 6297 images, 0 corrupt: 100%|██████████| 6297/6297 [00:02<00:00, 2730.17it/s]                         \r\nval: New cache created: /home/nick/quality-obliques-simple/val.cache                                                                                          \r\noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically...           \r\noptimizer: SGD(lr=0.01, momentum=0.9) with parameter groups 50 weight(decay=0.0), 51 weight(decay=0.00084375), 51 bias(decay=0.0)                             \r\nImage sizes 224 train, 224 val                                                                                                                                \r\nUsing 8 dataloader workers                                                                                                                                    \r\nLogging results to runs/classify/train                                                                                                                        \r\nStarting training for 300 epochs...\r\n```\r\n\r\nOutput of `yolo settings`:\r\n```\r\nyolo settings\r\n💡 Learn about settings at https://docs.ultralytics.com/quickstart/#ultralytics-settings\r\nPrinting '/home/nick/.config/Ultralytics/settings.yaml'\r\n\r\nsettings_version: 0.0.4\r\ndatasets_dir: /home/nick/datasets\r\nweights_dir: weights\r\nruns_dir: runs\r\nuuid: <UUID>\r\nsync: true\r\napi_key: ''\r\nopenai_api_key: ''\r\nclearml: true\r\ncomet: true\r\ndvc: true\r\nhub: true\r\nmlflow: true\r\nneptune: true\r\nraytune: true\r\ntensorboard: true\r\nwandb: true\r\n```\n\n### Environment\n\n```\r\nUltralytics YOLOv8.2.8 🚀 Python-3.10.14 torch-2.3.0 CUDA:0 (Tesla T4, 14931MiB)\r\nSetup complete ✅ (8 CPUs, 51.0 GB RAM, 51.2/98.2 GB disk)\r\n\r\nOS                  Linux-5.10.0-28-cloud-amd64-x86_64-with-glibc2.31\r\nEnvironment         Linux\r\nPython              3.10.14\r\nInstall             pip\r\nRAM                 51.00 GB\r\nCPU                 Intel Xeon 2.20GHz\r\nCUDA                12.1\r\n\r\nmatplotlib          ✅ 3.8.4>=3.3.0\r\nopencv-python       ✅ 4.9.0>=4.6.0\r\npillow              ✅ 10.3.0>=7.1.2\r\npyyaml              ✅ 6.0.1>=5.3.1\r\nrequests            ✅ 2.32.1>=2.23.0\r\nscipy               ✅ 1.13.0>=1.4.1\r\ntorch               ✅ 2.3.0>=1.8.0\r\ntorchvision         ✅ 0.18.0>=0.9.0\r\ntqdm                ✅ 4.66.4>=4.64.0\r\npsutil              ✅ 5.9.8\r\npy-cpuinfo          ✅ 9.0.0\r\nthop                ✅ 0.1.1-2209072238>=0.1.1\r\npandas              ✅ 2.2.2>=1.1.4\r\nseaborn             ✅ 0.13.2>=0.11.0\r\n```\n\n### Minimal Reproducible Example\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [X] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @nickcharles, thank you for your interest in Ultralytics YOLOv8 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "Hello! Thanks for providing detailed information about your issue with TensorBoard integration during YOLOv8 training.\n\nFrom your description, it seems that TensorBoard is enabled in your settings, but it's not generating the expected event files. Here are a couple of things you might want to check or try:\n\n1. **Ensure TensorBoard Logging is Active**: Make sure that the `tensorboard` flag is set to `True` in your training command or settings. It seems to be enabled in your `yolo settings`, but double-checking can help.\n\n2. **Directory Check**: Verify that the `runs` directory path in your training command points to where TensorBoard expects to find the logs. Sometimes, path discrepancies can cause issues.\n\n3. **Permissions**: Ensure that the directory where TensorBoard is trying to write has the correct permissions. Sometimes, permission issues on VMs can prevent file creation.\n\n4. **Manual TensorBoard Start**: Try manually starting TensorBoard pointing to your logs directory to see if it can pick up any data:\n   ```bash\n   tensorboard --logdir=runs/classify/train\n   ```\n\nIf these steps don't resolve the issue, it might be helpful to look into the logs for any errors related to TensorBoard or file writing. If you continue to experience problems, please feel free to submit a PR with a fix or further details, and we can look into this together!"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 20673,
    "title": "YOLOv10-heatmap",
    "author": "TryHard-LL",
    "state": "open",
    "created_at": "2025-05-16T07:57:22Z",
    "updated_at": "2025-06-10T15:19:59Z",
    "labels": [
      "question",
      "detect",
      "solutions"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi, how to implement the heatmap in yolov10? \nThanks.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @TryHard-LL, thank you for your interest in Ultralytics 🚀! We recommend checking out the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples—many common questions are answered there already.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate the issue.\n\nIf this is a custom training ❓ Question, please include as much detail as possible, such as dataset image examples, training logs, and confirm you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best! For real-time chat, visit [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Explore [Discourse](https://community.ultralytics.com/). Or connect with others on our [Subreddit](https://reddit.com/r/Ultralytics) to share insights.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to ensure your issue isn’t already resolved:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO runs in the following up-to-date verified environments with all dependencies (including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/)) preinstalled:\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThis is an automated response. An Ultralytics engineer will also assist you soon!"
      },
      {
        "user": "Y-T-G",
        "body": "You can use the same guide for YOLOv10\n\nhttps://docs.ultralytics.com/guides/heatmaps/"
      },
      {
        "user": "TryHard-LL",
        "body": "> You can use the same guide for YOLOv10\n> \n> https://docs.ultralytics.com/guides/heatmaps/\n\nThe document is for yolov11. Is it also applicable to yolov10? "
      }
    ]
  },
  {
    "issue_number": 21011,
    "title": "how to modify mask proto shape?",
    "author": "SoulProficiency",
    "state": "open",
    "created_at": "2025-06-10T09:18:27Z",
    "updated_at": "2025-06-10T11:46:30Z",
    "labels": [
      "question",
      "segment",
      "exports"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\ndefault input shape is [640,640] and mask proto shape is [batch,32,160,160],i just want to modify the proto shape from [batch,32,160,160] to [batch,32,640,640],how can we do it?i try use [mask_ratio] = 1 during training but it export to onnx is still 160*160 format \n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @SoulProficiency, thank you for reaching out and your interest in Ultralytics 🚀! This is an automated response to help guide you while an Ultralytics engineer reviews your question.\n\nWe recommend checking the [Docs](https://docs.ultralytics.com/) for detailed guidance, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples. Many common customization and export questions are covered there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) that demonstrates the issue, including the code/configuration you used to change the mask proto shape and the ONNX export command. This will help us debug more efficiently.\n\nIf this is a custom training ❓ Question, please include as much detail as possible—such as your training command, relevant config modifications, and any training/export logs. Make sure you’re following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nYou can also join the Ultralytics community for real-time support and discussion:\n- [Discord](https://discord.com/invite/ultralytics) 🎧\n- [Discourse](https://community.ultralytics.com/)\n- [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nEnsure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date verified environments (with all required dependencies preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM ([GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/))\n- **Amazon** Deep Learning AMI ([AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/))\n- **Docker Image** ([Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf the badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing, verifying correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nAn Ultralytics engineer will review your question and assist you further soon. Thank you for your patience and for contributing to the Ultralytics community!"
      },
      {
        "user": "Y-T-G",
        "body": "You can check this\n\nhttps://github.com/ultralytics/ultralytics/issues/20200"
      }
    ]
  },
  {
    "issue_number": 21010,
    "title": "The problem of coordinate offset in result.masks.data",
    "author": "OrangeFish-KSL",
    "state": "closed",
    "created_at": "2025-06-10T09:13:13Z",
    "updated_at": "2025-06-10T11:43:31Z",
    "labels": [
      "question",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nThe segmentation map generated by yolo11 itself is very accurate, but the segmentation map generated by reading result.masks.data is inaccurate\n```\nfrom ultralytics import YOLO\nimport torch\nimport cv2\nimport os\nimport numpy as np\n\n# Initialize the model\nmodel = YOLO('best.pt')\ninput_image_path = ‘frame_00411_left_enhance.jpg’\n# Function to resize mask to the original image size\ndef resize_mask(mask, original_size):\n    h, w = original_size\n    resized_mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n    return resized_mask\n\n# Function to composite masks onto a white background via positional alignment\ndef composite_masks_onto_white(masks, original_image, class_ids_to_extract):\n    height, width = original_image.shape[:2]\n    white_background = np.ones((height, width, 3), dtype=np.uint8) * 255  # Create a white canvas with the original image's dimensions\n\n    for i in range(masks.shape[0]):  # Iterate over each mask layer\n        if clss[i].item() in class_ids_to_extract:\n            mask = masks[i].cpu().numpy()  # Convert to numpy array\n            binary_mask = resize_mask(mask, (height, width))  # Resize the mask to match the original image size\n            binary_mask = binary_mask.astype(np.uint8) * 255  # Convert to a binary mask\n\n            # Transfer the areas from the original image to the white background using the mask\n            for c in range(3):  # Assuming a 3-channel image\n                white_background[:, :, c][binary_mask > 0] = original_image[:, :, c][binary_mask > 0]\n\n    return white_background\n\n# Execute prediction\nresults = model.predict(\n    source=input_image_path,\n    imgsz=1280,\n    project='runs/segment',\n    name='test',\n    save=False,\n    line_width=1,\n    show_conf=False,\n    show_labels=False,\n    save_txt=False,\n    save_crop=False,\n    show_boxes=False,\n)\n\n# Specify which classes to extract\nclass_ids_to_extract = [0, 2]  # Example: 0 (cane), 2 (node)\n\n# Read the original image\noriginal_image = cv2.imread(input_image_path)\n\nfor i, result in enumerate(results):\n    masks = result.masks.data  # The masks predicted by the model\n    boxes = result.boxes.data\n    clss = boxes[:, 5].int()\n\n    # Composite masks onto a white background\n    composited_image = composite_masks_onto_white(masks, original_image, class_ids_to_extract)\n\n    # Save the composited result\n    output_image_path = 'composited_cane_node.jpg'\n    cv2.imwrite(output_image_path, composited_image)\nprint(f\"Composited image saved to {output_image_path}\")\n```\n\n![Image](https://github.com/user-attachments/assets/93abd599-d61e-4453-ad5d-cdd0420b1b5e)\n\n![Image](https://github.com/user-attachments/assets/a9db1ae1-9cb9-4980-8de3-ee109961d1e1)\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @OrangeFish-KSL, thank you for your interest in Ultralytics 🚀! This is an automated response to help you get started—an Ultralytics engineer will review your issue and assist you soon.\n\nIf this is a 🐛 Bug Report, please provide a minimum reproducible example (MRE) if you haven't already. This helps us debug more efficiently.\n\nIf this is a custom training ❓ Question, please share as much detail as possible, including dataset image samples and training logs. Make sure you're following our tips for best training results.\n\nUpgrade to the latest `ultralytics` package with all requirements in a Python>=3.8 environment and with PyTorch>=1.8 to ensure your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\nYou can run YOLO in several verified environments, including Jupyter notebooks with free GPU, Google Cloud Deep Learning VM, Amazon Deep Learning AMI, or using our official Docker image.\n\nOur continuous integration (CI) tests verify all YOLO modes and tasks on macOS, Windows, and Ubuntu every 24 hours and on every commit. If the CI badge is green, tests are passing and the core functionality is working as expected.\n\nFeel free to join our community for real-time chat, discussions, or to share knowledge with other users."
      },
      {
        "user": "Y-T-G",
        "body": "You can try using `retina_masks=True` with `model.predict`"
      }
    ]
  },
  {
    "issue_number": 20864,
    "title": "TensorBoard logging error of last epoch.",
    "author": "karolbadowski",
    "state": "open",
    "created_at": "2025-05-29T09:57:37Z",
    "updated_at": "2025-06-10T10:42:37Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\n_No response_\n\n### Bug\n\nDuring the detailed validation that happens after the training, TensorBoard log of the last epoch mAP50-95 and last epoch mAP50 get overwritten by the best checkpoint value (instead of last checkpoint value).\n\nI have reported this issue to Giovanni (giovanni.dalzilio@ultralytics.com).\nIt has already beed confirmed as a bug.\n\nHere is a portion of the original email regarding a 100 epochs training:\n\n> I have noticed an bug in TensorBoard logging of the last epoch (epoch number 100) in ultralytics pipeline (such issue did not occur in yolov5 pipeline).\n> In all training histories in the console output and in the csv file, I can see that the mAP50-95 continues dropping down in the last epoch.\n> Just before the final validation happens, it is displayed in TensorBoard correctly for a moment....\n> However when the summary validation happens in the end of training, the result of epoch 100 is replaced in TensorBoard to value of best checkpoint and value of epoch 100 disappears.\n> It seems like there is a bug in implementation.\n> In yolov5 repository there was no such issue. Also epochs were indexed from 0 to 99 instead of 1 to 100.\n> My hypothesis is that it is:\n> - either completely unintentional overwriting (in such scenario it should not be logged in training),\n> - or someone wanted to visualise the top value in the end after the last epoch as a feature for easier comparisons of top scores of many trainings - here it replaces the last epoch instead of being placed after it.\n\n\n\n### Environment\n\nUltralytics 8.3.145  Python-3.12.3 torch-2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24217MiB)\nSetup complete ✅ (32 CPUs, 124.9 GB RAM, 1229.9/1829.7 GB disk)\n\nOS                  Linux-6.11.0-25-generic-x86_64-with-glibc2.39\nEnvironment         Linux\nPython              3.12.3\nInstall             git\nPath                /home/karol-badowski/Development/ultralytics/ultralytics\nRAM                 124.90 GB\nDisk                1229.9/1829.7 GB\nCPU                 AMD Ryzen 9 7950X3D 16-Core Processor\nCPU count           32\nGPU                 NVIDIA GeForce RTX 4090, 24217MiB\nGPU count           1\nCUDA                12.1\n\nnumpy               ✅ 1.26.4>=1.23.0\nmatplotlib          ✅ 3.9.1>=3.3.0\nopencv-python       ✅ 4.10.0.84>=4.6.0\npillow              ✅ 10.4.0>=7.1.2\npyyaml              ✅ 6.0.1>=5.3.1\nrequests            ✅ 2.32.3>=2.23.0\nscipy               ✅ 1.14.0>=1.4.1\ntorch               ✅ 2.4.1>=1.8.0\ntorch               ✅ 2.4.1!=2.4.0,>=1.8.0; sys_platform == \"win32\"\ntorchvision         ✅ 0.19.0>=0.9.0\ntqdm                ✅ 4.66.4>=4.64.0\npsutil              ✅ 6.0.0\npy-cpuinfo          ✅ 9.0.0\npandas              ✅ 2.2.2>=1.1.4\nseaborn             ✅ 0.13.2>=0.11.0\nultralytics-thop    ✅ 2.0.0>=2.0.0\n\n\n### Minimal Reproducible Example\n\nJust run a default training.\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @karolbadowski, thank you for reporting this and for your detailed description! 🚀 This is an automated response to help streamline your experience—an Ultralytics engineer will also review your issue and assist you soon.\n\nWe recommend reviewing the [Docs](https://docs.ultralytics.com/) for general guidance and checking the many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples. Many common questions are answered there.\n\nAs this is a 🐛 Bug Report, thanks for including a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/)—this is extremely helpful for debugging. If you have any additional logs, screenshots, or specific TensorBoard outputs that further illustrate the issue, please feel free to add them.\n\nJoin the Ultralytics community for real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧, in-depth discussions on [Discourse](https://community.ultralytics.com/), or share and learn on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease ensure you are using the latest `ultralytics` package with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment and [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/). This will help verify the issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date, verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/), and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for helping improve Ultralytics!"
      },
      {
        "user": "Y-T-G",
        "body": "You can check this\n\nhttps://github.com/ultralytics/ultralytics/pull/18804"
      },
      {
        "user": "karolbadowski",
        "body": "I have checked the thread and the issue seems to not have been resolved because someone on 22'nd of January wanted this error to persist because they like the side-effect of that error (seeing the best score). \n\nTo sum up:\n- Someone was logging the best mAP at the end of the reports (or at least they thought they do this).\n- What actually happens is: the best score is not added at the end after the last epoch.\n- It instead replaces the log of scores of last epoch and overwrites them.\n\nFirst of all, logging best result pretending to be the last result is not the default purpose of Tensorboard Logs and it is a wrong usage of tensorboard logs - they are supposed to show the training history.\nSecond of all, it works incorrectly and is not appended at the end like the custom user thought it works, but replaces the last log instead.\n\nThird important detail: \nIn yolov5 repository, tensorboard logs have their epochs indexed from 0 to epochs_count-1 and generally this is the common practice.\nHere in the ultralytics repository, tensorboard logs have their epochs indexed from 1 to epochs_count.\nIf at any time in the past your change worked as someone intended (to put the best score after the last epoch...) in a moment when indexing changed from [0 , epochs_count-1] to [1, epochs_count], **now the index [epochs_count] overwrites the last epoch instead of placing the max score after the last epoch.**\n\n- Kindly please make the default solution correct for all users in a way how tensorboards are supposed to work according to their default correct purpose and usage.\n- I propose that if someone wants a custom feature to log the max score at the end, allow it for them by an optional flag (as it is a custom change and not the correct usage). \n- When someone wants to display max value after the training, please fix the logging of best score to be after last epoch (not overwriting the last epoch). \n\nPlease beware of the non-standard indexing that happens at the moment, which is a risk of this and other similar errors in the future.\n\nI assume someone did not know about changed indexing and placed the top score in [epochs_count] instead of [epochs_count+1]. "
      }
    ]
  },
  {
    "issue_number": 18326,
    "title": "Segmentation weird straight lines",
    "author": "rob-safi",
    "state": "closed",
    "created_at": "2024-12-20T09:52:44Z",
    "updated_at": "2025-06-10T10:09:14Z",
    "labels": [
      "question",
      "Stale",
      "segment"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/ultralytics/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi,\r\nI recently trained a yolo11m-seg model and training looks normal and in most images it does really well but in some cases I get weird straight lines. I checked the training data pre and post augmentations and didn't notice anything off.\r\nI have tried with and without retina masks but it doesn't change anything.\r\n\r\nAny idea how to mitigate this?\r\n\r\nHere are two examples:\r\n\r\n![gdrive_inspection_data__IMAGE__40390_IMG_6489](https://github.com/user-attachments/assets/148a22df-f7e5-44d8-9d7a-7ad2bbf95eaa)\r\n![loose_materials_2024_11_20_alec_bywaters_pet__IMAGE__IMG_5467](https://github.com/user-attachments/assets/42d7e794-c604-43bf-b82e-f9ca8cd1fd22)\r\n\r\n\r\nHere is the full args in case you notice something weird:\r\n```\r\ntask: segment\r\nmode: train\r\nmodel: yolo11m-seg.pt\r\ndata: .../v1.yaml\r\nepochs: 100\r\ntime: null\r\npatience: 50\r\nbatch: -1\r\nimgsz: 600\r\nsave: true\r\nsave_period: -1\r\ncache: false\r\ndevice: null\r\nworkers: 6\r\nproject: .../yolo11m-seg\r\nname: train\r\nexist_ok: false\r\npretrained: true\r\noptimizer: auto\r\nverbose: true\r\nseed: 0\r\ndeterministic: true\r\nsingle_cls: true\r\nrect: false\r\ncos_lr: false\r\nclose_mosaic: 10\r\nresume: false\r\namp: true\r\nfraction: 1.0\r\nprofile: false\r\nfreeze: null\r\nmulti_scale: false\r\noverlap_mask: false\r\nmask_ratio: 1\r\ndropout: 0.0\r\nval: true\r\nsplit: val\r\nsave_json: false\r\nsave_hybrid: false\r\nconf: null\r\niou: 0.7\r\nmax_det: 300\r\nhalf: false\r\ndnn: false\r\nplots: false\r\nsource: null\r\nvid_stride: 1\r\nstream_buffer: false\r\nvisualize: false\r\naugment: false\r\nagnostic_nms: false\r\nclasses: null\r\nretina_masks: false\r\nembed: null\r\nshow: false\r\nsave_frames: false\r\nsave_txt: false\r\nsave_conf: false\r\nsave_crop: false\r\nshow_labels: true\r\nshow_conf: true\r\nshow_boxes: true\r\nline_width: null\r\nformat: torchscript\r\nkeras: false\r\noptimize: false\r\nint8: false\r\ndynamic: false\r\nsimplify: true\r\nopset: null\r\nworkspace: null\r\nnms: false\r\nlr0: 0.01\r\nlrf: 0.01\r\nmomentum: 0.937\r\nweight_decay: 0.0005\r\nwarmup_epochs: 3.0\r\nwarmup_momentum: 0.8\r\nwarmup_bias_lr: 0.1\r\nbox: 7.5\r\ncls: 0.5\r\ndfl: 1.5\r\npose: 12.0\r\nkobj: 1.0\r\nnbs: 64\r\nhsv_h: 0.015\r\nhsv_s: 0.4\r\nhsv_v: 0.4\r\ndegrees: 0.0\r\ntranslate: 0.05\r\nscale: 0.1\r\nshear: 0.0\r\nperspective: 0.0001\r\nflipud: 0.5\r\nfliplr: 0.5\r\nbgr: 0.0\r\nmosaic: 0.0\r\nmixup: 0.0\r\ncopy_paste: 0.0\r\ncopy_paste_mode: flip\r\nauto_augment: randaugment\r\nerasing: 0.4\r\ncrop_fraction: 1.0\r\ncfg: null\r\ntracker: botsort.yaml\r\nsave_dir: .../train\r\n```\r\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @rob-safi, thank you for sharing the details of your issue and for your interest in Ultralytics 🚀! This is an automated response to help you get started — an Ultralytics engineer will also assist you soon.\n\nWe recommend visiting the [Docs](https://docs.ultralytics.com) for guidance, where you can find detailed explanations, including the [segmentation task documentation](https://docs.ultralytics.com/tasks/segment/) and training tips for improving results.\n\nIf this is a 🐛 Bug Report, please provide a concise [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) that clearly demonstrates the issue. This helps us quickly identify the root cause and debug more effectively.\n\nIf this is a custom training ❓ Question, please provide additional context, such as:\n- Example images from your dataset showing the problem cases (if permissible and not already included)\n- Any unique preprocessing steps or augmentations applied\n- Training logs or metrics that may provide useful insight\n- Confirm whether you’ve reviewed our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)\n\n## Troubleshooting\n\nTo ensure you're working with the latest updates and features, please upgrade your `ultralytics` package to the latest version:\n\n```bash\npip install -U ultralytics\n```\n\nAs a general checklist:\n1. Confirm that your environment meets all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) (e.g., Python>=3.8 and PyTorch>=1.8).\n2. If the issue persists, try running your training process in a verified environment such as the following (each includes pre-installed dependencies):\n   - **Notebooks**: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n   - **Cloud VMs**: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/), [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n   - **Docker**: See the [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Community Support\n\nConnect with the Ultralytics community for additional guidance and support:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for real-time discussion\n- [Discourse](https://community.ultralytics.com) for in-depth technical Q&A\n- [Subreddit](https://reddit.com/r/Ultralytics) to share insights and discuss with other users\n\n## Next Steps\n\nPlease let us know if any specific part of your configuration is unclear or causing concern. If you've already tried specific remedies (e.g., using retina masks), sharing those details helps us fine-tune our assistance. 🚀\n\nThank you for helping us improve, and we’ll begin investigating further with the provided details!"
      },
      {
        "user": "Y-T-G",
        "body": "What lines are you referring to?"
      },
      {
        "user": "rob-safi",
        "body": "@Y-T-G on the right of the detection there is an artificial corner that gets created in the first image and in the second image the right of the segmentation is a straight line which seems a bit weird considering it cuts through objects\r\n![image](https://github.com/user-attachments/assets/56d44d4c-b9ac-486f-98a4-65452c58d391)\r\n![image](https://github.com/user-attachments/assets/caa7103b-6311-4748-92de-525703972097)\r\n\r\nThe training data does not having artifacts like this\r\n"
      }
    ]
  },
  {
    "issue_number": 19937,
    "title": "Ensemble average of two YOLOv11 model predictions and mAP@(0.5-0.95) computation",
    "author": "sivaramakrishnan-rajaraman",
    "state": "open",
    "created_at": "2025-03-30T14:42:45Z",
    "updated_at": "2025-06-10T08:06:55Z",
    "labels": [
      "enhancement",
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\n@glenn-jocher and @Y-T-G: The recent YOLOv11 object detection models are very efficient compared to their predecessors. I want to combine two variants of YOLOv11 models custom-trained on my data. Both models gave very good detection performance on my custom sing-object detection task and I want to see if their averaged predictions improve performance further. I want to combine the predictions of these two models as shown below in CLI: \n\n`yolo val model=\"runs/detect/yolo11n/train_yolo11n/weights/best,pt\" model=\"runs/detect/yolo11s/train_yolo11s/weights/best,pt\" data=custom_data_test.yaml batch=16 conf=0.25 save_txt=True save_conf=True`\n\nThe reason why I want to do it in the above-mentioned way is I want to save the labels and the confidence scores resulting from this ensemble average for further visualization. The reason I want to do this is to make use of Ultralytics' efficient way of computing mAP@0.5-0.95 and make a comparison of the ensemble performance with individual constituent models for an apple-to-apple comparison. If such an ensemble option is not available, I request that you help me with a custom code (at the CLI or python level) to accomplish this task of combining the Ultralytics model predictions, ultimately making use of Ultralytics' way of computing mAP@0.5-0.95 for a fair comparison. \n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @sivaramakrishnan-rajaraman, thank you for reaching out and for your interest in Ultralytics 🚀! \n\nWe recommend checking out the [Ultralytics Docs](https://docs.ultralytics.com/) for detailed guidance on YOLO usage, including [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples. These resources may provide insights into your current query.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us investigate and debug the issue.\n\nIf this is a ❓ Question or Feature Request, please describe your use case in more detail. For example, include all relevant commands, dataset details, and logs to help clarify your request. You might also want to consult our [Model Training Tips](https://docs.ultralytics.com/guides/model-training-tips/) for best practices.\n\nTo compare model performances or explore ensemble techniques, you could start by experimenting with the CLI or Python API. While an automated ensemble feature is currently unavailable, we encourage you to share any custom scripts or insights with the community!\n\nMeanwhile, an Ultralytics engineer will review your query soon to provide additional assistance. In the meantime, feel free to connect with the Ultralytics community for more support:  \n- 💬 [Discord](https://discord.com/invite/ultralytics) for real-time discussions  \n- 🗨️ [Discourse](https://community.ultralytics.com/) for detailed Q&A  \n- 🔗 [Subreddit](https://reddit.com/r/Ultralytics) for community updates  \n\n## Upgrade  \n\nEnsure you are using the latest `ultralytics` package along with all required dependencies to rule out any issues that might already be resolved. Use the following command in a [Python>=3.8](https://www.python.org/) environment with [PyTorch>=1.8](https://pytorch.org/get-started/locally/):  \n\n```bash\npip install -U ultralytics\n```\n\n## Verified Environments  \n\nYOLO models can be run in any of the following environments with all dependencies preinstalled:\n\n- **Notebooks** with free GPU:  \n  <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>  \n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)  \n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)  \n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/)  \n  <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>  \n\n## Status  \n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>  \n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. This ensures YOLO modes and tasks are verified daily on macOS, Windows, and Ubuntu environments.  \n\nWe appreciate your patience and look forward to assisting you further! 😊"
      },
      {
        "user": "Y-T-G",
        "body": "For detection models, you can use this:\n\n```python\nfrom ultralytics import YOLO, ASSETS\nimport cv2\nimport torch\n\nmodel1 = YOLO(\"yolo11n.pt\")\nmodel2 = YOLO(\"yolo11s.pt\")\n\nmodel1(verbose=False)  # this is required to initialize the predictor\n\ndef ensemble_predict(img):\n    prep = model1.predictor.preprocess([img])\n    preds1 = model1.predictor.inference(prep)[0]\n    preds2 = model2.predictor.inference(prep)[0]\n    preds = torch.cat([preds1, preds2], dim=-1)\n    return model1.predictor.postprocess(preds, prep, [img])\n\nimg = cv2.imread(str(ASSETS / \"bus.jpg\"))\nresults = ensemble_predict(img)\n\nfor result in results:\n  print(result.boxes)\n```"
      },
      {
        "user": "Y-T-G",
        "body": "This also works. And you can run val with it.\n\n```python\nfrom ultralytics.nn.autobackend import AutoBackend\nfrom ultralytics import YOLO\nimport torch\n\nensemble = YOLO(\"yolo11n.pt\")  # Dummy model. Not used.\nmodel = AutoBackend([\"yolo11n.pt\", \"yolo11s.pt\"])  # Update this with the list of models.\nensemble.model = model\n\ndef forward(self, x, embed=False, **kwargs):\n    return f(x, **kwargs)\n\nf = model.model.forward\nmodel.stride = torch.tensor([32])\nmodel.fuse = lambda verbose: model\nmodel.model.forward = forward.__get__(model.model, type(model.model))\n\nresults = ensemble.val(data=\"coco128.yaml\")\n```"
      }
    ]
  },
  {
    "issue_number": 21008,
    "title": "\"close_mosaic\" as a custom setting or a custom flag, not a default feature",
    "author": "karolbadowski",
    "state": "open",
    "created_at": "2025-06-10T07:04:57Z",
    "updated_at": "2025-06-10T07:42:39Z",
    "labels": [
      "enhancement",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nHello.\n\nIn all our trainigns, the disabling of the mosaic augmentation in the end is a source of huge drops of quality (se veral percentage points).\n\nTo think of it, it seems like a vary custom and problem-specific / handcrafted optimisation that might be beneficial for one problem (like COCO valset) and not for others. Enabling and disabling of any other augmentations in the end might be beneficial or not beneficial depending on the valset and on the highest frequency of objects sizes/blurryness/rotations/any other features.\n\nAlso the moment when this switch-off happens seems handcrafted, as a \"magic number\" to happen 10 epochs before the end of training.\n\nI have talked with Giovanni and he proposed to create this feature request.\n\nKindly pelase implement a setting in configuration setting file (optionally also a training flag, but the setting will be the least invasive for starts), where the default behavior could be set / switched on or off.\n\n\"close mosaic\" could be defined by a number saying how many epochs from the end, this operation happens... and user could set whether it is 10 or 0 or any other number. In case of 0 - that would be for projects where this custom augmentations modification is not to be activated at all (like for us - it is harmful and drops the mAP50-95 by several percentage points).\n\n\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @karolbadowski, thank you for your thoughtful feature request and for sharing your experiences with mosaic augmentation in training! 🧩 We appreciate your detailed feedback and suggestions for configurability.\n\nThis is an automated response to let you know an Ultralytics engineer will review your request and provide further assistance soon.\n\nIn the meantime, we recommend checking out the [Docs](https://docs.ultralytics.com/) for more details on current augmentation options, configuration, and [Python](https://docs.ultralytics.com/usage/python/) or [CLI](https://docs.ultralytics.com/usage/cli/) usage examples.\n\nIf you have relevant logs, configuration files, or example scripts that illustrate your use case, please consider sharing them here to help us understand your workflow and requirements.\n\nJoin our community for discussion and support:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for real-time chat\n- [Discourse](https://community.ultralytics.com/) for in-depth discussions\n- [Subreddit](https://reddit.com/r/Ultralytics) for community knowledge sharing\n\n## Upgrade\n\nTo make sure you’re testing with the latest features and fixes, please upgrade to the latest `ultralytics` package (and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml)) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in these up-to-date, verified environments:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) across macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you again for contributing to Ultralytics! 🚀"
      },
      {
        "user": "Y-T-G",
        "body": "There's a `close_mosaic` argument which you can set to 0 to disable it:\n\nhttps://docs.ultralytics.com/usage/cfg/#train-settings"
      }
    ]
  },
  {
    "issue_number": 20675,
    "title": "yolov8 customdataset",
    "author": "ohgyeongsu",
    "state": "closed",
    "created_at": "2025-05-16T08:55:42Z",
    "updated_at": "2025-06-10T06:14:38Z",
    "labels": [
      "question",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI want to apply cropping to the original image in memory before any other preprocessing steps, but I would like to continue using the existing Ultralytics YOLOv8 pipeline for the rest of the preprocessing and augmentation. Is there a way to insert a custom cropping operation at the very beginning of the pipeline, so that the crop is applied to the raw image in memory before any resizing, normalization, or augmentation, without fully rewriting the dataset class?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @ohgyeongsu, thank you for your interest in Ultralytics 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com/) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples. Many common questions, including pipeline customization, are already answered there.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best. For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Check out [Discourse](https://community.ultralytics.com/). Or dive into threads on our [Subreddit](https://reddit.com/r/Ultralytics) to share knowledge with the community.\n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) to verify your issue is not already resolved in the latest version:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\n---\n\nThis is an automated response. An Ultralytics engineer will also assist you here soon! 🛠️"
      },
      {
        "user": "glenn-jocher",
        "body": "You can extend the existing dataset class and override the `__getitem__` method to add your custom cropping before passing the image through the regular pipeline. Here's how you might approach it:\n\n```python\nfrom ultralytics.data.dataset import YOLODataset\n\nclass CustomCropDataset(YOLODataset):\n    def __getitem__(self, index):\n        # Get image and labels\n        img = self.get_image(index)\n        \n        # Apply your custom cropping here\n        # Example: img = your_custom_crop_function(img)\n        \n        # Then continue with the regular pipeline\n        img, labels = self.get_label_info(index, img)\n        \n        # Get the rest of the processing from parent class\n        img, labels = self.transforms(img, labels)\n        return img, labels\n```\n\nYou would then need to override the dataset initialization in your training setup. If you need more specific implementation details, let me know what type of cropping you're trying to achieve. \n\nYou could also use the `CenterCrop` transform from `ultralytics/data/augment.py` and apply it before other transforms if a center crop is what you need."
      },
      {
        "user": "ohgyeongsu",
        "body": "> You can extend the existing dataset class and override the `__getitem__` method to add your custom cropping before passing the image through the regular pipeline. Here's how you might approach it:\n> \n> from ultralytics.data.dataset import YOLODataset\n> \n> class CustomCropDataset(YOLODataset):\n>     def __getitem__(self, index):\n>         # Get image and labels\n>         img = self.get_image(index)\n>         \n>         # Apply your custom cropping here\n>         # Example: img = your_custom_crop_function(img)\n>         \n>         # Then continue with the regular pipeline\n>         img, labels = self.get_label_info(index, img)\n>         \n>         # Get the rest of the processing from parent class\n>         img, labels = self.transforms(img, labels)\n>         return img, labels\n> You would then need to override the dataset initialization in your training setup. If you need more specific implementation details, let me know what type of cropping you're trying to achieve.\n> \n> You could also use the `CenterCrop` transform from `ultralytics/data/augment.py` and apply it before other transforms if a center crop is what you need.\n\n@glenn-jocher \nI understand that in order to apply the `CustomCropDataset(YOLODataset)` class to the model, I need to customize `DetectionTrainer` and override the` get_dataset()` method. Could you confirm if this is correct?\n"
      }
    ]
  },
  {
    "issue_number": 13576,
    "title": "Having only one trace target causes an IndexError.",
    "author": "paradoxjun",
    "state": "open",
    "created_at": "2024-06-14T05:55:46Z",
    "updated_at": "2025-06-10T00:24:04Z",
    "labels": [
      "bug"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the YOLOv8 [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### YOLOv8 Component\n\nPredict\n\n### Bug\n\n    File \"./ultralytics_YOLOv8/ultralytics/trackers/byte_tracker.py\", line 273, in update\r\n        dets = bboxes[remain_inds]\r\nIndexError: index 1 is out of bounds for axis 0 with size 1\n\n### Environment\n\nnumpy==1.24.4\r\ntorch==2.1.0+cu118\n\n### Minimal Reproducible Example\n\n    class BYTETracker:    \r\n        def update(self, results, img=None):\r\n                \"\"\"Updates object tracker with new detections and returns tracked object bounding boxes.\"\"\"\r\n                self.frame_id += 1\r\n                activated_stracks = []\r\n                refind_stracks = []\r\n                lost_stracks = []\r\n                removed_stracks = []\r\n        \r\n                scores = results.conf\r\n                bboxes = results.xywhr if hasattr(results, \"xywhr\") else results.xywh\r\n                # Add index\r\n                bboxes = np.concatenate([bboxes, np.arange(len(bboxes)).reshape(-1, 1)], axis=-1)    # Line 264\r\n                cls = results.cls\r\n        \r\n                remain_inds = scores >= self.args.track_high_thresh\r\n                inds_low = scores > self.args.track_low_thresh\r\n                inds_high = scores < self.args.track_high_thresh\r\n        \r\n                inds_second = inds_low & inds_high\r\n                dets_second = bboxes[inds_second]\r\n                dets = bboxes[remain_inds]\r\n                scores_keep = scores[remain_inds]\r\n                scores_second = scores[inds_second]\r\n                cls_keep = cls[remain_inds]\r\n                cls_second = cls[inds_second]\n\n### Additional\n\nLine 264 of byte_tracker.py:\r\n\r\n    bboxes = np.concatenate([bboxes, np.arange(len(bboxes)).reshape(-1, 1)], axis=-1)\r\n\r\nConverts 'torch.tensor' to 'numpy.ndarray'. The index obtained later is still 'torch.tensor'.\r\nWhen the index length is 1, the index results of 'numpy.adarray' and 'torch.tensor' are different, for example:\r\n\r\n    import torch\r\n    import numpy as np\r\n    \r\n    \r\n    remain_inds = torch.tensor([False])\r\n    bboxes_tensor = torch.tensor([[380.98, 178.21, 43.959, 50.217, 0]])\r\n    bboxes_np = np.array([[380.98, 178.21, 43.959, 50.217, 0]])\r\n    \r\n    print(f\"remain_inds: {remain_inds}\")\r\n    print(f\"bboxes[remain_inds]: {bboxes_tensor[remain_inds]}\")\r\n    print(f\"bboxes[remain_inds]: {bboxes_np[remain_inds]}\")\r\n\r\n    \"\"\"\r\n    The console output results:\r\n    remain_inds: tensor([False])\r\n    bboxes[remain_inds]: tensor([], size=(0, 5))\r\n    bboxes[remain_inds]: [380.98  178.21   43.959  50.217   0.   ]\r\n    \"\"\"\r\n\r\n\r\n\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello @paradoxjun, thank you for your interest in Ultralytics YOLOv8 🚀! We recommend a visit to the [Docs](https://docs.ultralytics.com) for new users where you can find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage examples and where many of the most common questions may already be answered.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us debug it.\n\nIf this is a custom training ❓ Question, please provide as much information as possible, including dataset image examples and training logs, and verify you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips//).\n\nJoin the vibrant [Ultralytics Discord](https://ultralytics.com/discord) 🎧 community for real-time conversations and collaborations. This platform offers a perfect space to inquire, showcase your work, and connect with fellow Ultralytics users.\n\n## Install\n\nPip install the `ultralytics` package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n```bash\npip install ultralytics\n```\n\n## Environments\n\nYOLOv8 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLOv8 [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "paradoxjun",
        "body": "这是来自QQ邮箱的假期自动回复邮件。您好，我最近正在休假中，无法亲自回复您的邮件。我将在假期结束后，尽快给您回复。"
      },
      {
        "user": "glenn-jocher",
        "body": "@paradoxjun hello,\n\nThank you for reporting this issue and providing a detailed explanation along with a minimal reproducible example. It appears that the discrepancy between indexing behavior in `numpy` and `torch` tensors is causing the `IndexError`.\n\nTo address this, you can convert the `torch` tensor indices to `numpy` before using them to index the `numpy` array. Here's a potential fix for the relevant part of your code:\n\n```python\nremain_inds = scores.numpy() >= self.args.track_high_thresh\ninds_low = scores.numpy() > self.args.track_low_thresh\ninds_high = scores.numpy() < self.args.track_high_thresh\n\ninds_second = inds_low & inds_high\ndets_second = bboxes[inds_second]\ndets = bboxes[remain_inds]\nscores_keep = scores[remain_inds]\nscores_second = scores[inds_second]\ncls_keep = cls[remain_inds]\ncls_second = cls[inds_second]\n```\n\nThis should ensure that the indexing is consistent and prevent the `IndexError`.\n\nPlease ensure you are using the latest versions of `torch` and `ultralytics` packages. If the issue persists after applying this fix and updating your packages, feel free to let us know, and we can further investigate."
      }
    ]
  },
  {
    "issue_number": 18797,
    "title": "multi-label train",
    "author": "m66505196",
    "state": "closed",
    "created_at": "2025-01-21T11:22:02Z",
    "updated_at": "2025-06-10T00:23:41Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nhello, how to train YOLOv11 with <Class_ID> <x_center> <y_center> <width> <height> <feature_1> <feature_2> ... <feature_n> format?\nnc: <count features>\nnames: ['glasses', 'hat', 'old', 'young'] ?\nDo we need to make changes to the code? YOLOv5 or ... support? my task is detection with muti-label\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @m66505196, thank you for your interest in Ultralytics 🚀! We recommend checking out our [Docs](https://docs.ultralytics.com/) for in-depth guidance, including examples for [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, which may address your question.\n\nIf this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug it effectively.\n\nIf this is a custom training ❓ Question, such as yours, we ask for more details to better assist you:\n- Any modifications you’ve attempted in the code\n- The exact format of your dataset annotations\n- A snippet of your configurations (e.g., `data.yaml`)\n- Training logs or error messages (if you’ve encountered any)\n\nFor multi-label applications or custom feature-based tasks, exploring our [Model Training Tips](https://docs.ultralytics.com/guides/model-training-tips/) might also offer some useful insights.\n\nWe’d like to highlight that you’re running on the open-source project YOLOv11 😉, which is outside the usual single-object detection scope, so some code modifications may indeed be necessary for custom tasks. Please ensure you’ve reviewed and are familiar with the [codebase](https://github.com/ultralytics/ultralytics). \n\n## Upgrade\n\nUpgrade to the latest `ultralytics` package along with all dependencies in a [**Python>=3.8**](https://www.python.org/) environment to ensure you’re leveraging the latest features and fixes. You can do so with:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be executed across multiple environments. Feel free to choose one of the following to fit your needs:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** VM: See our [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: Learn more in our [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: [Quickstart guide here](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) and official image <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Community Support\n\nGot more questions? You’re invited to join and contribute to discussions in our Ultralytics community:\n- Check out our [Discord server](https://discord.com/invite/ultralytics) 🎧 for real-time interaction\n- Browse and contribute to discussions on [Discourse](https://community.ultralytics.com/)\n- Head to our [Subreddit](https://reddit.com/r/Ultralytics) to share ideas with users globally\n\n## Status\n\nThe Ultralytics CI tests monitor the health of the project. As of the last 24-hour cycle, the current status is:  \n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nAn Ultralytics engineer will review and assist you further soon. Thanks for reaching out and happy training! 🚀"
      },
      {
        "user": "glenn-jocher",
        "body": "@m66505196 thank you for your question! YOLO11 supports multi-label training when using additional features or attributes in the dataset. However, the standard YOLO format (`<Class_ID> <x_center> <y_center> <width> <height>`) doesn't natively support extra features like `<feature_1> <feature_2> ...`. To include such features, you would need to modify the dataset parsing and the model's training pipeline in the code.\n\nFor multi-label classification tasks, consider using the **classification mode** instead. You can define multiple labels for each class in the dataset YAML file (`names`), and train accordingly. Refer to the [Ultralytics Classification Docs](https://docs.ultralytics.com/tasks/classify) for more details.\n\nIf your task requires detection with additional labels, you might need to customize the code. For support beyond YOLO11's standard features, you could adapt the codebase or explore other YOLO versions like YOLOv5, which supports modifications."
      },
      {
        "user": "FiksII",
        "body": "\n> For multi-label classification tasks, consider using the **classification mode** instead. You can define multiple labels for each class in the dataset YAML file (`names`), and train accordingly. Refer to the [Ultralytics Classification Docs](https://docs.ultralytics.com/tasks/classify) for more details.\n\nExcuse me, but I can't find anything related to the multi-label classification. Only video contains some \"clue\", that same images might be in a different folders (but, I think it's just a mistake in the presentation)."
      }
    ]
  },
  {
    "issue_number": 19494,
    "title": "Using Sahi with instance segmentation model",
    "author": "facundot",
    "state": "closed",
    "created_at": "2025-03-02T22:45:53Z",
    "updated_at": "2025-06-10T00:23:37Z",
    "labels": [
      "question",
      "Stale",
      "segment"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nHi i have a yolov8 instance segmentation model, iam having trouble doing validation post running the detections with sahi , can someone point me in the right direction\ni have tested the model on images before and it works properly\nfound some export config that says to use show_masks = True , but i get an error saying that parameter doesnt exist\n\n```\nTraceback (most recent call last):\n  File \"/home/facundo/Desktop/ultralytics/examples/YOLOv8-SAHI-Inference-Video/slice-valid.py\", line 31, in <module>\n    result = get_sliced_prediction(\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/facundo/anaconda3/envs/video_yolo/lib/python3.12/site-packages/sahi/predict.py\", line 283, in get_sliced_prediction\n    object_prediction_list = postprocess(object_prediction_list)\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/facundo/anaconda3/envs/video_yolo/lib/python3.12/site-packages/sahi/postprocess/combine.py\", line 555, in __call__\n    object_prediction_list[keep_ind] = merge_object_prediction_pair(\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/facundo/anaconda3/envs/video_yolo/lib/python3.12/site-packages/sahi/postprocess/utils.py\", line 220, in merge_object_prediction_pair\n    return ObjectPrediction(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/facundo/anaconda3/envs/video_yolo/lib/python3.12/site-packages/sahi/prediction.py\", line 79, in __init__\n    super().__init__(\n  File \"/home/facundo/anaconda3/envs/video_yolo/lib/python3.12/site-packages/sahi/annotation.py\", line 525, in __init__\n    raise ValueError(\"Invalid segmentation mask.\")\nValueError: Invalid segmentation mask.\n```\n\n\n```import os\nimport cv2\nfrom sahi import AutoDetectionModel\nfrom sahi.predict import get_sliced_prediction\n\n\ninput_dir = \"dataset/valid/images\"\noutput_dir = \"dataset/valid_output\"  \n\nyolov8_model_path = \"best.pt\"\n\n\ndetection_model = AutoDetectionModel.from_pretrained(\n    model_type='yolov8',\n    model_path=yolov8_model_path,\n    confidence_threshold=0.5,\n    device=\"cuda:0\",\n)\n\n\nos.makedirs(output_dir, exist_ok=True)\n\n\nfor i in os.listdir(input_dir):\n    if i.endswith(\".jpg\"):  \n        image = cv2.imread(f\"{input_dir}/{i}\")[:, :, ::-1]\n        \n        result = get_sliced_prediction(\n            image,\n            detection_model,\n            slice_height=1280,\n            slice_width=1280,\n            overlap_height_ratio=0.2,\n            overlap_width_ratio=0.2,\n            postprocess_class_agnostic=True\n        )\n\n        result.export_visuals(export_dir=output_dir, hide_labels=True, hide_conf=True)\n\n        object_prediction_list = result.object_prediction_list\n\n        coco_annotations = result.to_coco_annotations()[:3]\n        coco_predictions = result.to_coco_predictions(image_id=1)[:3]\n\n        print(f\"COCO Annotations: {coco_annotations}\")\n        print(f\"COCO Predictions: {coco_predictions}\")\n\nprint(\"Processing complete!\")\n```\n\n\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @facundot, thank you for reaching out to Ultralytics 🚀! We appreciate your detailed report and are here to help.\n\nTo assist you further, we recommend that you:\n\n1. Verify you are using the latest `ultralytics` package and all dependencies. Upgrade with:\n```bash\npip install -U ultralytics\n```\nMake sure to check that your environment meets the [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) (e.g., Python>=3.8, PyTorch>=1.8).\n\n2. If this is a 🐛 Bug Report, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) using a simplified script to recreate the problem. The error traceback you’ve shared is helpful, but an MRE allows us to debug more effectively.\n\n3. If you’re encountering an issue with integrating SAHI with YOLOv8 instance segmentation, confirm you are following the proper instructions for SAHI integration and post-processing steps. These may include ensuring that the mask outputs from YOLOv8 models are compatible with SAHI’s expected input structure.\n\nWe encourage you to explore the following resources for additional troubleshooting:\n- [YOLOv8 Segmentation Task Docs](https://docs.ultralytics.com/tasks/segmentation/)\n- [Tips for Custom Training and Best Results](https://docs.ultralytics.com/guides/model-training-tips/)\n\nIf needed, join the Ultralytics community for more direct help and peer discussions:\n- [Discord](https://discord.com/invite/ultralytics) 🎧 for real-time support\n- [Discourse](https://community.ultralytics.com/) for in-depth Q&A\n- [Subreddit](https://reddit.com/r/Ultralytics) for knowledge exchange\n\n## Run in Verified Environments\n\nYou can test this issue in any of the available environments where dependencies are pre-installed:\n\n- <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a>\n- <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n- <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n\nFinally, this is an automated response 🤖. An Ultralytics engineer will review your issue and respond promptly. Thank you for your patience, and we hope to resolve this for you soon! 🚀"
      },
      {
        "user": "glenn-jocher",
        "body": "@facundot for SAHI integration with YOLOv8 segmentation models, ensure you're using the latest versions of both `ultralytics` and `sahi` libraries. The error often occurs when masks aren't properly handled during sliced prediction merging. \n\nTwo solutions to try:\n\n1. **Update Dependencies**:\n```bash\npip install -U ultralytics sahi\n```\n\n2. **Modify Model Initialization**:\n```python\ndetection_model = AutoDetectionModel.from_pretrained(\n    model_type='ultralytics',  # Use 'ultralytics' instead of 'yolov8'\n    model_path=yolov8_model_path,\n    model_device='cuda:0',\n    model_confidence_threshold=0.5,\n)\n```\n\nFor full implementation details, see the SAHI Tiled Inference guide in the Ultralytics documentation. If issues persist, please create a minimal reproducible example and share it via GitHub Discussions."
      },
      {
        "user": "facundot",
        "body": "\n@glenn-jocher \nThe dependencies are updated , task_type doesnt seem to be a valid argument any other suggestions?"
      }
    ]
  },
  {
    "issue_number": 19661,
    "title": "yolov11 how to improve large object performace?",
    "author": "sjtu-cz",
    "state": "open",
    "created_at": "2025-03-12T08:40:05Z",
    "updated_at": "2025-06-10T00:23:33Z",
    "labels": [
      "question",
      "OBB"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\n<img width=\"466\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d64bf551-e496-472d-b436-769c2792ad0a\" />\n\nI use yolo-obb train the doclayout model, But the performance on large targets is relatively poor. How can this be improved?\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @sjtu-cz, thank you for your interest in Ultralytics 🚀! We recommend exploring the [Docs](https://docs.ultralytics.com/) for guidance, where you'll find many [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) examples and answers to common questions.\n\nIf this is a custom training ❓ Question, please provide more details, such as:\n- Training settings (hyperparameters, epochs, batch size, etc.)\n- Dataset information, including image examples and annotations\n- Training logs and performance metrics\n\nFor 🐛 Bug Reports, please include a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) to help us debug effectively.\n\nJoin the Ultralytics community where it suits you best:\n- Real-time chat on [Discord](https://discord.com/invite/ultralytics) 🎧  \n- In-depth discussions on [Discourse](https://community.ultralytics.com/)  \n- Share and learn on our [Subreddit](https://reddit.com/r/Ultralytics)  \n\n## Upgrade\n\nEnsure you’re using the latest release of `ultralytics` with all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) installed in a **Python>=3.8** environment:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in these verified environments (all dependencies preinstalled):\n- **Notebooks** with free GPUs: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM: [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI: [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**: [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>  \n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are passing. These tests verify YOLO functionality across macOS, Windows, and Ubuntu daily.\n\nThis is an automated response to guide you in the right direction. An Ultralytics engineer will review your issue and assist further soon. Thank you for your patience! 🙏"
      },
      {
        "user": "Y-T-G",
        "body": "You can try\n\nhttps://github.com/ultralytics/ultralytics/issues/11634#issuecomment-2367471426"
      },
      {
        "user": "sjtu-cz",
        "body": "> You can try\n> \n> [#11634 (comment)](https://github.com/ultralytics/ultralytics/issues/11634#issuecomment-2367471426)\n\nI changed the reg_max from 16 to 84, but there still have problem to predict high and huge object. while seg model don't have the problem. \nIs there a flaw in the prob_iou loss of rotation object detection?"
      }
    ]
  },
  {
    "issue_number": 20444,
    "title": "Changes in head of YOLO11",
    "author": "huma-96",
    "state": "open",
    "created_at": "2025-05-01T13:16:37Z",
    "updated_at": "2025-06-10T00:23:29Z",
    "labels": [
      "question",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and [discussions](https://github.com/orgs/ultralytics/discussions) and found no similar questions.\n\n\n### Question\n\nI want to modify the architecture of YOLO11 by incorporating the spherical awareness due to 360-degree dataset. But the issue is that I could n't understand that if I replace the head.py in nn/modules and import it in .-init-.py file as well, why its still saying the module not found. I don't know the correct way of modifying the architecture. help me out.\n\n### Additional\n\n_No response_",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @huma-96, thank you for reaching out and for your interest in Ultralytics 🚀! This is an automated response to help guide you, and an Ultralytics engineer will also assist you here soon.\n\nWe recommend checking out the [Docs](https://docs.ultralytics.com/) for helpful resources on [Python](https://docs.ultralytics.com/usage/python/) and [CLI](https://docs.ultralytics.com/usage/cli/) usage, as well as architectural customization tips.\n\nIf your question is about modifying YOLO11's architecture, please include as much detail as possible:\n- The specific code changes you made (code snippets or files)\n- The full error message and traceback\n- Your project directory structure (especially where your custom `head.py` is placed)\n- How you are importing your custom module\n\nThis will help us understand and reproduce your issue. If you believe this is a 🐛 Bug, please provide a [minimum reproducible example](https://docs.ultralytics.com/help/minimum-reproducible-example/) for efficient debugging.\n\nFor custom training or architectural changes, you can also review our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nJoin the Ultralytics community where it suits you best! For real-time chat, head to [Discord](https://discord.com/invite/ultralytics) 🎧. Prefer in-depth discussions? Try [Discourse](https://community.ultralytics.com/). Or visit our [Subreddit](https://reddit.com/r/Ultralytics) to connect and share knowledge.\n\n## Upgrade\n\nEnsure you are using the latest `ultralytics` package (and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml)) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of these up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI covers all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit."
      },
      {
        "user": "glenn-jocher",
        "body": "Hi @huma-96, modifying the YOLO11 architecture can be tricky due to its modular structure. When replacing head.py, ensure you're maintaining all the required imports and class definitions from the original file. The \"module not found\" error likely occurs because:\n\n1. The import statement in __init__.py doesn't match your custom module name\n2. Your modified module relies on classes/functions that aren't properly imported\n3. Python can't find your module in its search path\n\nInstead of directly replacing files, I'd recommend:\n```python\n# Create a custom head that inherits from the original\nfrom ultralytics.nn.modules.head import Detect\n\nclass SphereDetect(Detect):\n    # Override methods to add spherical awareness\n    def forward(self, x):\n        # Your spherical-aware implementation\n        # ...\n```\n\nThen use this custom head when initializing your model. If you're still having issues, sharing your specific error message and modified code would help pinpoint the problem."
      },
      {
        "user": "huma-96",
        "body": "Hi @glenn-jocher, thanks for your suggestions. I've created DSConv.py at the same location as that of Conv.py, define it in .__init__, tasks.py but don't know why getting this error \"Traceback (most recent call last):\n  File \"/home/549/hh8526/.local/bin/yolo\", line 8, in <module>\n    sys.exit(entrypoint())\n  File \"/scratch/ve11/hh8526/yolov11/yolov11_custom/ultralytics/ultralytics/cfg/__init__.py\", line 959, in entrypoint\n    model = YOLO(model, task=task)\n  File \"/scratch/ve11/hh8526/yolov11/yolov11_custom/ultralytics/ultralytics/models/yolo/model.py\", line 54, in __init__\n    super().__init__(model=model, task=task, verbose=verbose)\n  File \"/scratch/ve11/hh8526/yolov11/yolov11_custom/ultralytics/ultralytics/engine/model.py\", line 146, in __init__\n    self._new(model, task=task, verbose=verbose)\n  File \"/scratch/ve11/hh8526/yolov11/yolov11_custom/ultralytics/ultralytics/engine/model.py\", line 256, in _new\n    self.model = (model or self._smart_load(\"model\"))(cfg_dict, verbose=verbose and RANK == -1)  # build model\n  File \"/scratch/ve11/hh8526/yolov11/yolov11_custom/ultralytics/ultralytics/nn/tasks.py\", line 334, in __init__\n    self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelist\n  File \"/scratch/ve11/hh8526/yolov11/yolov11_custom/ultralytics/ultralytics/nn/tasks.py\", line 1427, in parse_model\n    else globals()[m]\nKeyError: 'DSConv'\n\""
      }
    ]
  },
  {
    "issue_number": 20527,
    "title": "Problem when exporting YOLO11s to TF-Lite models",
    "author": "moak4ever",
    "state": "open",
    "created_at": "2025-05-07T00:54:07Z",
    "updated_at": "2025-06-10T00:23:23Z",
    "labels": [
      "bug",
      "Stale",
      "exports"
    ],
    "body": "1. Model\n    - YOLO11s model trained with a custom dataset\n    - Input dimension : 1 x 3 x 960 x 960\n    - Output dimension : 1 x 5 x 18900\n2. H/W Environment\n    - CPU: Intel Xeon Gold 6130 2.10GHz\n    - Memory: 256GB\n3. S/W Environment\n    - Ubuntu           20.04.5 LTS\n    - Python            3.11.12\n    - ultralytics        8.3.123\n    - torch               2.5.1+cu121\n    - tensorflow       2.17.0\n    - tflite-support    0.4.4\n    - onnx                1.16.1\n    - onnx2tf            1.26.3\n    - onnxslim          0.1.50\n4. Python Codes\n    - trained_model_path = 'yolo11s_best.pt'\n    - model = YOLO(trained_model_path)\n    - model.export(format='tflite', imgsz=(960,960), int8=True, data='calibration_dataset.yaml')\n5. Logs\n    - Ultralytics 8.3.123 🚀 Python-3.11.12 torch-2.5.1+cu121 CPU (Intel Xeon Gold 6130 2.10GHz)\n    - YOLO11s summary (fused): 100 layers, 9,413,187 parameters, 0 gradients, 21.3 GFLOPs\n    - PyTorch: starting from 'yolo11s_best.pt' with input shape (1, 3, 960, 960) BCHW and output shape(s) (1, 5, 18900) (18.4 MB)\n    - TensorFlow SavedModel: starting export with tensorflow 2.17.0...\n    - ONNX: starting export with onnx 1.16.1 opset 19...\n    - ONNX: slimming with onnxslim 0.1.50...\n    - ONNX: export success ✅ 1.7s, saved as 'yolo11s_best.onnx' (36.6 MB)\n    - TensorFlow SavedModel: collecting INT8 calibration images from 'data=calibration_dataset.yaml'\n    - Fast image access ✅ (ping: 0.0±0.0 ms, read: 3321.0±1067.9 MB/s, size: 690.6 KB)\n    - Scanning /home/etri/Works/repository/Models/yolo11/export/calibration_dataset/labels/val.cache... 231 images, 168 backgrounds, 0 corrupt: 100%|██████████| 399/399 [00:00<?, ?it/s]\n    - TensorFlow SavedModel: starting TFLite export with onnx2tf 1.26.3...\n    - Killed\n6. Problem\n    - Memory consumption rate gradually increases.\n    - After all, it goes up to 100%.\n    - Consequently, the python process is killed. (may be due to an out-of-memory problem)\n7. On the other hand,\n    - The problem does not occur when exporting a YOLOv8s model.",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @moak4ever, thank you for sharing these detailed logs and system specifications with us 🚀! This is an automated response to help guide your issue to resolution. An Ultralytics engineer will review your report and provide further assistance soon.\n\nIf this is a 🐛 Bug Report, we kindly ask you to provide a [minimum reproducible example (MRE)](https://docs.ultralytics.com/help/minimum-reproducible-example/) if possible. This helps us quickly pinpoint and resolve the issue. Please include a concise script, relevant model files (if shareable), and any necessary calibration data or configuration files.\n\nIf this is a custom training ❓ Question, please share as much information as possible—such as representative dataset images, relevant training or export logs, and confirm you are following our [Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/).\n\nFor immediate community support, you can join us on [Discord](https://discord.com/invite/ultralytics) 🎧, engage in deeper discussions on [Discourse](https://community.ultralytics.com/), or participate in conversations on our [Subreddit](https://reddit.com/r/Ultralytics).\n\n## Upgrade\n\nPlease ensure you are using the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/), as issues may be resolved in newer releases:\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYOLO can be run in any of the following up-to-date, verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda-zone)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nThank you for your patience and for helping us improve Ultralytics HUB and YOLO11! 🧡"
      },
      {
        "user": "Y-T-G",
        "body": "You can try latest onnx2tf."
      },
      {
        "user": "moak4ever",
        "body": "Even if I use the latest version, the same problem occurs.\nMemory consumption rate gradually increases.\nIt goes up to 100%.\nAfter all, the python process is killed.\n\nThe log is as follows\n\nUltralytics 8.3.123 🚀 Python-3.11.12 torch-2.5.1+cu121 CPU (Intel Xeon Gold 6130 2.10GHz)\nYOLO11s summary (fused): 100 layers, 9,413,187 parameters, 0 gradients, 21.3 GFLOPs\n\nPyTorch: starting from 'yolo11s_best.pt' with input shape (1, 3, 960, 960) BCHW and output shape(s) (1, 5, 18900) (18.4 MB)\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746665098.569639    8251 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746665098.577276    8251 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1746665098.598121    8251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1746665098.598152    8251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1746665098.598157    8251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1746665098.598161    8251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n\nTensorFlow SavedModel: starting export with tensorflow 2.19.0...\n\nONNX: starting export with onnx 1.17.0 opset 19...\nONNX: slimming with onnxslim 0.1.50...\nONNX: export success ✅ 2.6s, saved as 'yolo11s_best.onnx' (36.6 MB)\nTensorFlow SavedModel: collecting INT8 calibration images from 'data=calibration_dataset.yaml'\nFast image access ✅ (ping: 0.1±0.1 ms, read: 277.5±210.3 MB/s, size: 690.6 KB)\nScanning /home/etri/Works/repository/Models/yolo11/export/calibration_dataset/labels/val.cache... 231 images, 168 backgrounds, 0 corrupt: 100%|██████████| 399/399 [00:00<?, ?it/s]\nTensorFlow SavedModel: starting TFLite export with onnx2tf 1.27.2...\nKilled\n"
      }
    ]
  },
  {
    "issue_number": 20576,
    "title": "New ideas in the field of robot grasping",
    "author": "CVAE1",
    "state": "open",
    "created_at": "2025-05-10T09:51:37Z",
    "updated_at": "2025-06-10T00:23:17Z",
    "labels": [
      "enhancement",
      "Stale",
      "detect"
    ],
    "body": "### Search before asking\n\n- [x] I have searched the Ultralytics [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar feature requests.\n\n\n### Description\n\nHello, I have designed a feature in the field of robot grasping using YOLOv8 called \"Operation Relationship Reasoning\", which can simultaneously complete object detection tasks and predict the grasping order of multiple objects. If you are interested in this feature, I hope to become a contributor to this repository.\n\n### Use case\n\n_No response_\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @CVAE1, thank you for your interest in Ultralytics and for sharing your ideas for robot grasping with YOLO 🚀! Your proposal for \"Operation Relationship Reasoning\" sounds intriguing and aligns well with the innovative work happening in this community.\n\nThis is an automated response to help guide your contribution and ensure it receives the attention it deserves. An Ultralytics engineer will also review your suggestion and provide feedback soon.\n\nIf you would like to contribute, we recommend reviewing our [Contributing Guidelines](https://github.com/ultralytics/ultralytics/blob/main/CONTRIBUTING.md) and checking out the [Docs](https://docs.ultralytics.com/) for information on the current architecture and workflows. Sharing more details, such as example code, pseudo-code, or a prototype implementation, will help us better understand your approach and how it could be integrated.\n\nJoin the Ultralytics community to share and discuss your ideas further:\n- For real-time chat, join our [Discord](https://discord.com/invite/ultralytics) 🎧\n- For in-depth discussions, visit [Discourse](https://community.ultralytics.com/)\n- Or connect with others on our [Subreddit](https://reddit.com/r/Ultralytics)\n\n## Upgrade\n\nTo ensure compatibility for your developments, upgrade to the latest `ultralytics` package and all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/):\n\n```bash\npip install -U ultralytics\n```\n\n## Environments\n\nYou can develop and test your features in any of these verified environments:\n- **Notebooks** with free GPU: <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\n\n## Status\n\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n\nIf this badge is green, all [Ultralytics CI](https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml?query=event%3Aschedule) tests are currently passing. CI tests verify correct operation of all YOLO [Modes](https://docs.ultralytics.com/modes/) and [Tasks](https://docs.ultralytics.com/tasks/) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n\nWe look forward to your contribution and collaboration!"
      },
      {
        "user": "glenn-jocher",
        "body": "Thanks for your interest in contributing to Ultralytics! Your \"Operation Relationship Reasoning\" feature for robot grasping sounds intriguing and potentially valuable for robotics applications. We're always open to new contributions that extend YOLO's capabilities.\n\nTo get started, I'd recommend following our [Contributing Guide](https://docs.ultralytics.com/help/contributing/) which outlines our process for pull requests. You could create a more detailed proposal with some implementation details, examples, or even a prototype in a fork. This would help our team better understand your feature's implementation and potential integration.\n\nFor robotics-specific integrations, you might also find our [ROS Quickstart Guide](https://docs.ultralytics.com/guides/ros-quickstart/) helpful if your feature relates to ROS environments. We look forward to learning more about your work!"
      },
      {
        "user": "github-actions[bot]",
        "body": "👋 Hello there! We wanted to give you a friendly reminder that this issue has not had any recent activity and may be closed soon, but don't worry - you can always reopen it if needed. If you still have any questions or concerns, please feel free to let us know how we can help.\n\nFor additional resources and information, please see the links below:\n\n- **Docs**: https://docs.ultralytics.com\n- **HUB**: https://hub.ultralytics.com\n- **Community**: https://community.ultralytics.com\n\nFeel free to inform us of any other **issues** you discover or **feature requests** that come to mind in the future. Pull Requests (PRs) are also always welcomed!\n\nThank you for your contributions to YOLO 🚀 and Vision AI ⭐"
      }
    ]
  },
  {
    "issue_number": 18522,
    "title": "exported YOLO11 tflite model crashes in Android (GPU)",
    "author": "adriansacchi",
    "state": "closed",
    "created_at": "2025-01-04T08:24:58Z",
    "updated_at": "2025-06-09T23:52:19Z",
    "labels": [
      "bug",
      "fixed",
      "exports"
    ],
    "body": "### Search before asking\n\n- [X] I have searched the Ultralytics YOLO [issues](https://github.com/ultralytics/ultralytics/issues) and found no similar bug report.\n\n\n### Ultralytics YOLO Component\n\nExport\n\n### Bug\n\nUsing the exported YOLO11 tflite model in Android with a GPU delegate crashes with an exception.\r\n(Same behavior for both exported tflite model files `yolo11n_float16.tflite` and `yolo11n_float16.tflite`)\r\n\r\nThe exported model works in Android on CPU, but then obviously the performance (speed) is not satisfying.  \r\n\r\nUsing the exported YOLO8 tflite model with the same setup works just fine.\r\nOther people have previously reported the same problem in issue #17837, which was closed without an actual solution.\r\n\r\n## Warning and exception\r\n```\r\nW  Got a deoptimization request on un-deoptimizable method long org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(long, long, int, boolean, java.util.List)\r\n\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Failed to build program executable - Out of host memoryError: Program not built!\r\n    Falling back to OpenGL\r\n    TfLiteGpuDelegate Init: Batch size mismatch, expected 1 but got 2\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n    Node number 802 (TfLiteGpuDelegateV2) failed to prepare.\r\n    Restored original execution plan after delegate application failure.\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:110)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:73)\r\n        at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:36)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:232)\r\n```\r\n\r\n\n\n### Environment\n\nUltralytics 8.3.57 🚀 Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\r\nSetup complete ✅ (2 CPUs, 12.7 GB RAM, 33.1/107.7 GB disk)\r\n\r\nOS                  Linux-6.1.85+-x86_64-with-glibc2.35\r\nEnvironment         Colab\r\nPython              3.10.12\r\nInstall             pip\r\nRAM                 12.67 GB\r\nDisk                33.1/107.7 GB\r\nCPU                 Intel Xeon 2.20GHz\r\nCPU count           2\r\nGPU                 None\r\nGPU count           None\r\nCUDA                None\r\n\r\nnumpy               ✅ 1.26.4>=1.23.0\r\nnumpy               ✅ 1.26.4<2.0.0; sys_platform == \"darwin\"\r\nmatplotlib          ✅ 3.8.0>=3.3.0\r\nopencv-python       ✅ 4.10.0.84>=4.6.0\r\npillow              ✅ 11.0.0>=7.1.2\r\npyyaml              ✅ 6.0.2>=5.3.1\r\nrequests            ✅ 2.32.3>=2.23.0\r\nscipy               ✅ 1.13.1>=1.4.1\r\ntorch               ✅ 2.5.1+cu121>=1.8.0\r\ntorch               ✅ 2.5.1+cu121!=2.4.0,>=1.8.0; sys_platform == \"win32\"\r\ntorchvision         ✅ 0.20.1+cu121>=0.9.0\r\ntqdm                ✅ 4.67.1>=4.64.0\r\npsutil              ✅ 5.9.5\r\npy-cpuinfo          ✅ 9.0.0\r\npandas              ✅ 2.2.2>=1.1.4\r\nseaborn             ✅ 0.13.2>=0.11.0\r\nultralytics-thop    ✅ 2.0.13>=2.0.0\r\n{'OS': 'Linux-6.1.85+-x86_64-with-glibc2.35',\r\n 'Environment': 'Colab',\r\n 'Python': '3.10.12',\r\n 'Install': 'pip',\r\n 'RAM': '12.67 GB',\r\n 'Disk': '33.1/107.7 GB',\r\n 'CPU': 'Intel Xeon 2.20GHz',\r\n 'CPU count': 2,\r\n 'GPU': None,\r\n 'GPU count': None,\r\n 'CUDA': None,\r\n 'Package Info': {'numpy': '✅ 1.26.4<2.0.0; sys_platform == \"darwin\"',\r\n  'matplotlib': '✅ 3.8.0>=3.3.0',\r\n  'opencv-python': '✅ 4.10.0.84>=4.6.0',\r\n  'pillow': '✅ 11.0.0>=7.1.2',\r\n  'pyyaml': '✅ 6.0.2>=5.3.1',\r\n  'requests': '✅ 2.32.3>=2.23.0',\r\n  'scipy': '✅ 1.13.1>=1.4.1',\r\n  'torch': '✅ 2.5.1+cu121!=2.4.0,>=1.8.0; sys_platform == \"win32\"',\r\n  'torchvision': '✅ 0.20.1+cu121>=0.9.0',\r\n  'tqdm': '✅ 4.67.1>=4.64.0',\r\n  'psutil': '✅ 5.9.5',\r\n  'py-cpuinfo': '✅ 9.0.0',\r\n  'pandas': '✅ 2.2.2>=1.1.4',\r\n  'seaborn': '✅ 0.13.2>=0.11.0',\r\n  'ultralytics-thop': '✅ 2.0.13>=2.0.0'}}\n\n### Minimal Reproducible Example\n\n## Exporting the models\r\n```\r\n!pip install ultralytics==8.3.57\r\nfrom ultralytics import YOLO\r\n\r\nmodel = YOLO(\"yolo11n.pt\")\r\nmodel.export(format=\"tflite\")\r\n\r\nmodel = YOLO(\"yolov8n.pt\")\r\nmodel.export(format=\"tflite\")\r\n```\r\n\r\n## Android dependencies\r\n```\r\nlitert = \"1.0.1\"\r\n[libraries]\r\nlitert = { module = \"com.google.ai.edge.litert:litert\", version.ref = \"litert\" }\r\nlitert-gpu = { module = \"com.google.ai.edge.litert:litert-gpu\", version.ref = \"litert\" }\r\n```\r\n\r\n## Creating the interpreter\r\n```\r\ninterpreter = Interpreter(model, Interpreter.Options().apply {\r\n    addDelegate(GpuDelegateFactory().create(RuntimeFlavor.APPLICATION))\r\n})\r\n```\n\n### Additional\n\n_No response_\n\n### Are you willing to submit a PR?\n\n- [ ] Yes I'd like to help by submitting a PR!",
    "comments": [
      {
        "user": "UltralyticsAssistant",
        "body": "👋 Hello @adriansacchi, thank you for reporting this issue and for your interest in Ultralytics 🚀! We appreciate your detailed explanation and reproducible steps—it really helps us understand the problem. An Ultralytics engineer will review and provide further assistance soon.\n\nIn the meantime, here are some resources and suggestions to help troubleshoot:\n\n### Docs and Help\nYou can visit our [Docs](https://docs.ultralytics.com) for general guidance, including exporting models to other formats such as TFLite, and common troubleshooting tips for deployments.\n\nSince this appears to be a 🐛 **Bug Report**, could you ensure you've provided a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) (MRE)? This would allow us to further isolate and understand the issue.\n\n### Upgrade Instructions\nFirst, please verify the issue persists in the latest release. Upgrade to the latest stable `ultralytics` version along with all necessary [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a Python>=3.8 environment:\n\n```bash\npip install -U ultralytics\n```\n\n### Verified Environments\nFor testing and running YOLO models, try using one of our recommended and verified environments:\n- **Notebooks with free GPU**:  \n  <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a> <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud**: See our [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/).\n- **AWS DLAMI**: See our [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/).\n- **Docker Image**: <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/).\n\n### Additional Notes\n- Ensure compatibility between your exported TFLite model and your Android device's GPU delegate. In some cases, changes in TFLite interpreter options or reducing the batch size could help resolve errors like `Batch size mismatch`.\n- If you're seeing success with YOLOv8 but issues with YOLOv11 in the exact same environment, it might indicate differences in the export pipeline or the underlying model architecture. We will investigate further.\n\n### Community Support\nYou can also engage with the Ultralytics community for more help or guidance:\n- Join our [Discord](https://discord.com/invite/ultralytics) for real-time discussions 🎧.\n- Check out the [Discourse forum](https://community.ultralytics.com) for detailed threads.\n- Share insights or seek advice in our [Subreddit](https://reddit.com/r/Ultralytics).\n\n### Status\n<a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml?query=event%3Aschedule\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>  \nTo verify whether this issue could already be resolved, check that our nightly **Continuous Integration (CI)** tests pass successfully for the latest updates.\n\nLet us know if there's anything else you can clarify, and thank you again for your detailed report. 🙏"
      },
      {
        "user": "Petros626",
        "body": "does this help https://github.com/ultralytics/ultralytics/issues/17837 ?"
      },
      {
        "user": "adriansacchi",
        "body": "> does this help #17837 ?\r\n\r\nNo, I opened this issue because #17837 got closed without an actual solution."
      }
    ]
  }
]