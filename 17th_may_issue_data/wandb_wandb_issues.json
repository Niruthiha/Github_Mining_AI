[
  {
    "issue_number": 10044,
    "title": "[Q]: Wandb: Network error (ConnectionError), entering retry loop.",
    "author": "MichealgodJordan",
    "state": "open",
    "created_at": "2025-06-17T10:46:42Z",
    "updated_at": "2025-06-17T11:02:42Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\nI began to use wandb last year, and it always ran well. But about a week ago, when I ran my codes, including wandb, I got this error:\nwandb: Appending key for api.wandb.ai to your netrc file: /date2/zhang_h/.netrc\nwandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\nwandb: Network error (ConnectTimeout), entering retry loop.\nwandb: ERROR Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.\nTraceback (most recent call last):\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/asyncio/locks.py\", line 214, in wait\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/asyncio/tasks.py\", line 456, in wait_for\n    return fut.result()\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/response_handle.py\", line 105, in wait_async\n    await asyncio.wait_for(evt.wait(), timeout=timeout)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/asyncio/tasks.py\", line 458, in wait_for\n    raise exceptions.TimeoutError() from exc\nasyncio.exceptions.TimeoutError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 999, in init\n    result = wait_with_progress(\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py\", line 24, in wait_with_progress\n    return wait_all_with_progress(\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py\", line 87, in wait_all_with_progress\n    return asyncio_compat.run(progress_loop_with_timeout)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_compat.py\", line 30, in run\n    return future.result()\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/concurrent/futures/_base.py\", line 446, in result\n    return self.__get_result()\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/concurrent/futures/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_compat.py\", line 74, in run\n    return asyncio.run(self._run_or_cancel(fn))\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/asyncio/base_events.py\", line 646, in run_until_complete\n    return future.result()\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_compat.py\", line 98, in _run_or_cancel\n    return fn_task.result()\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py\", line 82, in progress_loop_with_timeout\n    return await _wait_handles_async(\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py\", line 130, in _wait_handles_async\n    async with asyncio_compat.open_task_group() as task_group:\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/contextlib.py\", line 206, in __aexit__\n    await anext(self.gen)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_compat.py\", line 190, in open_task_group\n    await task_group._wait_all()\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_compat.py\", line 159, in _wait_all\n    raise exc\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py\", line 128, in wait_single\n    results[index] = await handle.wait_async(timeout=timeout)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/mailbox_handle.py\", line 122, in wait_async\n    response = await self._handle.wait_async(timeout=timeout)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/response_handle.py\", line 114, in wait_async\n    raise TimeoutError(\nTimeoutError: Timed out waiting for response on zvkcfk5hxvpu\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/date2/zhang_h/D/BreastCa/TANGLE/intra_fine_tune_q.py\", line 344, in <module>\n    wandb.init(project=\"Q\",\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 1623, in init\n    wandb._sentry.reraise(e)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/analytics/sentry.py\", line 156, in reraise\n    raise exc.with_traceback(sys.exc_info()[2])\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 1609, in init\n    return wi.init(run_settings, run_config, run_printer)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 1012, in init\n    raise CommError(\nwandb.errors.errors.CommError: Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.\nTraceback (most recent call last):\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/asyncio/locks.py\", line 214, in wait\n    await fut\nasyncio.exceptions.CancelledError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/asyncio/tasks.py\", line 456, in wait_for\n    return fut.result()\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/response_handle.py\", line 105, in wait_async\n    await asyncio.wait_for(evt.wait(), timeout=timeout)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/asyncio/tasks.py\", line 458, in wait_for\n    raise exceptions.TimeoutError() from exc\nasyncio.exceptions.TimeoutError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 999, in init\n    result = wait_with_progress(\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py\", line 24, in wait_with_progress\n    return wait_all_with_progress(\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py\", line 87, in wait_all_with_progress\n    return asyncio_compat.run(progress_loop_with_timeout)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_compat.py\", line 30, in run\n    return future.result()\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/concurrent/futures/_base.py\", line 446, in result\n    return self.__get_result()\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/concurrent/futures/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_compat.py\", line 74, in run\n    return asyncio.run(self._run_or_cancel(fn))\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/asyncio/base_events.py\", line 646, in run_until_complete\n    return future.result()\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_compat.py\", line 98, in _run_or_cancel\n    return fn_task.result()\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py\", line 82, in progress_loop_with_timeout\n    return await _wait_handles_async(\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py\", line 130, in _wait_handles_async\n    async with asyncio_compat.open_task_group() as task_group:\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/contextlib.py\", line 206, in __aexit__\n    await anext(self.gen)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_compat.py\", line 190, in open_task_group\n    await task_group._wait_all()\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_compat.py\", line 159, in _wait_all\n    raise exc\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/wait_with_progress.py\", line 128, in wait_single\n    results[index] = await handle.wait_async(timeout=timeout)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/mailbox_handle.py\", line 122, in wait_async\n    response = await self._handle.wait_async(timeout=timeout)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/mailbox/response_handle.py\", line 114, in wait_async\n    raise TimeoutError(\nTimeoutError: Timed out waiting for response on zvkcfk5hxvpu\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/date2/zhang_h/D/BreastCa/TANGLE/intra_fine_tune_q.py\", line 344, in <module>\n    wandb.init(project=\"Q\",\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 1623, in init\n    wandb._sentry.reraise(e)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/analytics/sentry.py\", line 156, in reraise\n    raise exc.with_traceback(sys.exc_info()[2])\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 1609, in init\n    return wi.init(run_settings, run_config, run_printer)\n  File \"/date2/zhang_h/anaconda3/envs/myenv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 1012, in init\n    raise CommError(\nwandb.errors.errors.CommError: Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.\n\n\nI tried versions including wandb==0.12.18 and wandb==0.20.1, and none of them work.\n\nI also tried to ping the www.wandb.ai:\nPING www.wandb.ai (151.101.65.195) 56(84) bytes of data.\n64 bytes from 151.101.65.195 (151.101.65.195): icmp_seq=2 ttl=47 time=152 ms\n64 bytes from 151.101.65.195 (151.101.65.195): icmp_seq=4 ttl=47 time=154 ms\n64 bytes from 151.101.65.195 (151.101.65.195): icmp_seq=5 ttl=47 time=159 ms\n64 bytes from 151.101.65.195 (151.101.65.195): icmp_seq=6 ttl=47 time=122 ms\n64 bytes from 151.101.65.195 (151.101.65.195): icmp_seq=7 ttl=47 time=157 ms\n64 bytes from 151.101.65.195 (151.101.65.195): icmp_seq=13 ttl=47 time=140 ms\n64 bytes from 151.101.65.195 (151.101.65.195): icmp_seq=14 ttl=47 time=153 ms\n64 bytes from 151.101.65.195 (151.101.65.195): icmp_seq=15 ttl=47 time=109 ms\n^C\n--- www.wandb.ai ping statistics ---\n16 packets transmitted, 8 received, 50% packet loss, time 15168ms\nrtt min/avg/max/mdev = 108.765/143.247/158.811/17.203 ms\n\n\nThese are debug files:\n\n[debug.log](https://github.com/user-attachments/files/20773711/debug.log)\n\n[debug-internal.log](https://github.com/user-attachments/files/20773748/debug-internal.log)\n\n[debug-core.log](https://github.com/user-attachments/files/20773714/debug-core.log)\n\nWould anyone be able to help me fix it? Thanks a lot.\n",
    "comments": []
  },
  {
    "issue_number": 7470,
    "title": "[App]: hardware logging in multi-node setting",
    "author": "BramVanroy",
    "state": "open",
    "created_at": "2024-04-24T15:04:00Z",
    "updated_at": "2025-06-17T09:32:06Z",
    "labels": [
      "ty:feature",
      "a:sdk",
      "c:sdk:system-metrics",
      "c:sdk:distributed-training"
    ],
    "body": "### Current Behavior\n\nCurrently, in th e run overview, we can get an idea of the system hardware, specifically GPU count and CPU count. However, as far as I can tell this does not account for multi-node settings and only reports what the current node is equipped with. While I understand why that is the case, it may be confusing because it is not \"correct\".\n\n### Expected Behavior\n\nCorrect hardware information. To be honest I am not sure how feasible it is to collect this information without integration with distributed communication frameworks or something else custom.\n\n### Steps To Reproduce\n\n_No response_\n\n### Screenshots\n\n_No response_\n\n### Environment\n\nOS: Linux\r\n\r\nBrowsers: Edge\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "thanos-wandb",
        "body": "Hi @BramVanroy thank you for reporting this. May I please ask some more context, what's your current compute infra? and which ML frameworks are you mostly using?"
      },
      {
        "user": "thanos-wandb",
        "body": "Hi @BramVanroy just following up on this, to see if you could provide us with some additional information on your current multinode infrastructure so as to include those in a feature request for our engineers? thank you!"
      },
      {
        "user": "BramVanroy",
        "body": "Hi Thanos\r\n\r\nI am running jobs on between 1 node, 1 GPU up to 10 nodes, 4 GPUs each. It seems to me that wandb does not correctly log hardware when it comes to multi-node settings."
      }
    ]
  },
  {
    "issue_number": 9563,
    "title": "[Bug-App]: Plotly figure rendered empty",
    "author": "jonasjuerss",
    "state": "closed",
    "created_at": "2025-03-06T00:04:04Z",
    "updated_at": "2025-06-17T08:56:43Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\n\nI am trying to log a simple plotly bar plot to wandb like shown in the minimal example below. Similar to https://github.com/wandb/wandb/issues/2191, the plot is rendered correctly when logged as an HTML but shows up empty when logged as plotly.\n\n```python\nimport numpy as np\nimport plotly\nimport wandb\nimport plotly.graph_objects as go\n\nif __name__ == \"__main__\":\n    wandb.init(project=..., entity=..., job_type=...)\n\n    bins = np.arange(4) / 2\n    counts = np.arange(4) + 1\n\n    fig = go.Figure(data=[go.Bar(\n        x=bins,\n        y=counts,\n    )])\n\n    wandb.log({\"test_bar\": wandb.Plotly(fig),\n               \"test_bar_html\": wandb.Html(plotly.io.to_html(fig))})\n```\n\n![Image](https://github.com/user-attachments/assets/e124e80f-8649-4477-9db3-022998aa27d5)\n\nI am not sure if the underlying issue is the same as in https://github.com/wandb/wandb/issues/2191. If this is the case and there really is nothing to be done on the wandb side, I think that at the very least, the documentation at https://docs.wandb.ai/guides/track/log/plots/#matplotlib-and-plotly-plots should contain a comment indicating that there have been known issues with this functionality for what is now almost 4 years. It would also be good to have some guidance on what exactly the issue identified in https://github.com/wandb/wandb/issues/2191 was and how to determine if one is facing the same issue.\n\nAdditionally, it would be good to know whether there is a workaround e.g. by pinning plotly to a specific version. Note that logging to HTML can easily lead to a 1000-fold increase in the size of the logged files and is not a satisfactory solution (nor is logging as image, losing the interactive functionality).\n\nRelevant installed versions might be `wandb==0.19.3` and `plotly==6.0.0`.\n\n<details>\n\n<summary>.plotly.json logged to media/plotly</summary>\n\n````json\n{\n  \"data\": [\n    {\n      \"x\": {\n        \"dtype\": \"f8\",\n        \"bdata\": \"AAAAAAAAAAAAAAAAAADgPwAAAAAAAPA/AAAAAAAA+D8=\"\n      },\n      \"y\": {\n        \"dtype\": \"i4\",\n        \"bdata\": \"AQAAAAIAAAADAAAABAAAAA==\"\n      },\n      \"type\": \"bar\"\n    }\n  ],\n  \"layout\": {\n    \"template\": {\n      \"data\": {\n        \"histogram2dcontour\": [\n          {\n            \"type\": \"histogram2dcontour\",\n            \"colorbar\": {\n              \"outlinewidth\": 0,\n              \"ticks\": \"\"\n            },\n            \"colorscale\": [\n              [\n                0,\n                \"#0d0887\"\n              ],\n              [\n                0.1111111111111111,\n                \"#46039f\"\n              ],\n              [\n                0.2222222222222222,\n                \"#7201a8\"\n              ],\n              [\n                0.3333333333333333,\n                \"#9c179e\"\n              ],\n              [\n                0.4444444444444444,\n                \"#bd3786\"\n              ],\n              [\n                0.5555555555555556,\n                \"#d8576b\"\n              ],\n              [\n                0.6666666666666666,\n                \"#ed7953\"\n              ],\n              [\n                0.7777777777777778,\n                \"#fb9f3a\"\n              ],\n              [\n                0.8888888888888888,\n                \"#fdca26\"\n              ],\n              [\n                1,\n                \"#f0f921\"\n              ]\n            ]\n          }\n        ],\n        \"choropleth\": [\n          {\n            \"type\": \"choropleth\",\n            \"colorbar\": {\n              \"outlinewidth\": 0,\n              \"ticks\": \"\"\n            }\n          }\n        ],\n        \"histogram2d\": [\n          {\n            \"type\": \"histogram2d\",\n            \"colorbar\": {\n              \"outlinewidth\": 0,\n              \"ticks\": \"\"\n            },\n            \"colorscale\": [\n              [\n                0,\n                \"#0d0887\"\n              ],\n              [\n                0.1111111111111111,\n                \"#46039f\"\n              ],\n              [\n                0.2222222222222222,\n                \"#7201a8\"\n              ],\n              [\n                0.3333333333333333,\n                \"#9c179e\"\n              ],\n              [\n                0.4444444444444444,\n                \"#bd3786\"\n              ],\n              [\n                0.5555555555555556,\n                \"#d8576b\"\n              ],\n              [\n                0.6666666666666666,\n                \"#ed7953\"\n              ],\n              [\n                0.7777777777777778,\n                \"#fb9f3a\"\n              ],\n              [\n                0.8888888888888888,\n                \"#fdca26\"\n              ],\n              [\n                1,\n                \"#f0f921\"\n              ]\n            ]\n          }\n        ],\n        \"heatmap\": [\n          {\n            \"type\": \"heatmap\",\n            \"colorbar\": {\n              \"outlinewidth\": 0,\n              \"ticks\": \"\"\n            },\n            \"colorscale\": [\n              [\n                0,\n                \"#0d0887\"\n              ],\n              [\n                0.1111111111111111,\n                \"#46039f\"\n              ],\n              [\n                0.2222222222222222,\n                \"#7201a8\"\n              ],\n              [\n                0.3333333333333333,\n                \"#9c179e\"\n              ],\n              [\n                0.4444444444444444,\n                \"#bd3786\"\n              ],\n              [\n                0.5555555555555556,\n                \"#d8576b\"\n              ],\n              [\n                0.6666666666666666,\n                \"#ed7953\"\n              ],\n              [\n                0.7777777777777778,\n                \"#fb9f3a\"\n              ],\n              [\n                0.8888888888888888,\n                \"#fdca26\"\n              ],\n              [\n                1,\n                \"#f0f921\"\n              ]\n            ]\n          }\n        ],\n        \"contourcarpet\": [\n          {\n            \"type\": \"contourcarpet\",\n            \"colorbar\": {\n              \"outlinewidth\": 0,\n              \"ticks\": \"\"\n            }\n          }\n        ],\n        \"contour\": [\n          {\n            \"type\": \"contour\",\n            \"colorbar\": {\n              \"outlinewidth\": 0,\n              \"ticks\": \"\"\n            },\n            \"colorscale\": [\n              [\n                0,\n                \"#0d0887\"\n              ],\n              [\n                0.1111111111111111,\n                \"#46039f\"\n              ],\n              [\n                0.2222222222222222,\n                \"#7201a8\"\n              ],\n              [\n                0.3333333333333333,\n                \"#9c179e\"\n              ],\n              [\n                0.4444444444444444,\n                \"#bd3786\"\n              ],\n              [\n                0.5555555555555556,\n                \"#d8576b\"\n              ],\n              [\n                0.6666666666666666,\n                \"#ed7953\"\n              ],\n              [\n                0.7777777777777778,\n                \"#fb9f3a\"\n              ],\n              [\n                0.8888888888888888,\n                \"#fdca26\"\n              ],\n              [\n                1,\n                \"#f0f921\"\n              ]\n            ]\n          }\n        ],\n        \"surface\": [\n          {\n            \"type\": \"surface\",\n            \"colorbar\": {\n              \"outlinewidth\": 0,\n              \"ticks\": \"\"\n            },\n            \"colorscale\": [\n              [\n                0,\n                \"#0d0887\"\n              ],\n              [\n                0.1111111111111111,\n                \"#46039f\"\n              ],\n              [\n                0.2222222222222222,\n                \"#7201a8\"\n              ],\n              [\n                0.3333333333333333,\n                \"#9c179e\"\n              ],\n              [\n                0.4444444444444444,\n                \"#bd3786\"\n              ],\n              [\n                0.5555555555555556,\n                \"#d8576b\"\n              ],\n              [\n                0.6666666666666666,\n                \"#ed7953\"\n              ],\n              [\n                0.7777777777777778,\n                \"#fb9f3a\"\n              ],\n              [\n                0.8888888888888888,\n                \"#fdca26\"\n              ],\n              [\n                1,\n                \"#f0f921\"\n              ]\n            ]\n          }\n        ],\n        \"mesh3d\": [\n          {\n            \"type\": \"mesh3d\",\n            \"colorbar\": {\n              \"outlinewidth\": 0,\n              \"ticks\": \"\"\n            }\n          }\n        ],\n        \"scatter\": [\n          {\n            \"fillpattern\": {\n              \"fillmode\": \"overlay\",\n              \"size\": 10,\n              \"solidity\": 0.2\n            },\n            \"type\": \"scatter\"\n          }\n        ],\n        \"parcoords\": [\n          {\n            \"type\": \"parcoords\",\n            \"line\": {\n              \"colorbar\": {\n                \"outlinewidth\": 0,\n                \"ticks\": \"\"\n              }\n            }\n          }\n        ],\n        \"scatterpolargl\": [\n          {\n            \"type\": \"scatterpolargl\",\n            \"marker\": {\n              \"colorbar\": {\n                \"outlinewidth\": 0,\n                \"ticks\": \"\"\n              }\n            }\n          }\n        ],\n        \"bar\": [\n          {\n            \"error_x\": {\n              \"color\": \"#2a3f5f\"\n            },\n            \"error_y\": {\n              \"color\": \"#2a3f5f\"\n            },\n            \"marker\": {\n              \"line\": {\n                \"color\": \"#E5ECF6\",\n                \"width\": 0.5\n              },\n              \"pattern\": {\n                \"fillmode\": \"overlay\",\n                \"size\": 10,\n                \"solidity\": 0.2\n              }\n            },\n            \"type\": \"bar\"\n          }\n        ],\n        \"scattergeo\": [\n          {\n            \"type\": \"scattergeo\",\n            \"marker\": {\n              \"colorbar\": {\n                \"outlinewidth\": 0,\n                \"ticks\": \"\"\n              }\n            }\n          }\n        ],\n        \"scatterpolar\": [\n          {\n            \"type\": \"scatterpolar\",\n            \"marker\": {\n              \"colorbar\": {\n                \"outlinewidth\": 0,\n                \"ticks\": \"\"\n              }\n            }\n          }\n        ],\n        \"histogram\": [\n          {\n            \"marker\": {\n              \"pattern\": {\n                \"fillmode\": \"overlay\",\n                \"size\": 10,\n                \"solidity\": 0.2\n              }\n            },\n            \"type\": \"histogram\"\n          }\n        ],\n        \"scattergl\": [\n          {\n            \"type\": \"scattergl\",\n            \"marker\": {\n              \"colorbar\": {\n                \"outlinewidth\": 0,\n                \"ticks\": \"\"\n              }\n            }\n          }\n        ],\n        \"scatter3d\": [\n          {\n            \"type\": \"scatter3d\",\n            \"line\": {\n              \"colorbar\": {\n                \"outlinewidth\": 0,\n                \"ticks\": \"\"\n              }\n            },\n            \"marker\": {\n              \"colorbar\": {\n                \"outlinewidth\": 0,\n                \"ticks\": \"\"\n              }\n            }\n          }\n        ],\n        \"scattermap\": [\n          {\n            \"type\": \"scattermap\",\n            \"marker\": {\n              \"colorbar\": {\n                \"outlinewidth\": 0,\n                \"ticks\": \"\"\n              }\n            }\n          }\n        ],\n        \"scattermapbox\": [\n          {\n            \"type\": \"scattermapbox\",\n            \"marker\": {\n              \"colorbar\": {\n                \"outlinewidth\": 0,\n                \"ticks\": \"\"\n              }\n            }\n          }\n        ],\n        \"scatterternary\": [\n          {\n            \"type\": \"scatterternary\",\n            \"marker\": {\n              \"colorbar\": {\n                \"outlinewidth\": 0,\n                \"ticks\": \"\"\n              }\n            }\n          }\n        ],\n        \"scattercarpet\": [\n          {\n            \"type\": \"scattercarpet\",\n            \"marker\": {\n              \"colorbar\": {\n                \"outlinewidth\": 0,\n                \"ticks\": \"\"\n              }\n            }\n          }\n        ],\n        \"carpet\": [\n          {\n            \"aaxis\": {\n              \"endlinecolor\": \"#2a3f5f\",\n              \"gridcolor\": \"white\",\n              \"linecolor\": \"white\",\n              \"minorgridcolor\": \"white\",\n              \"startlinecolor\": \"#2a3f5f\"\n            },\n            \"baxis\": {\n              \"endlinecolor\": \"#2a3f5f\",\n              \"gridcolor\": \"white\",\n              \"linecolor\": \"white\",\n              \"minorgridcolor\": \"white\",\n              \"startlinecolor\": \"#2a3f5f\"\n            },\n            \"type\": \"carpet\"\n          }\n        ],\n        \"table\": [\n          {\n            \"cells\": {\n              \"fill\": {\n                \"color\": \"#EBF0F8\"\n              },\n              \"line\": {\n                \"color\": \"white\"\n              }\n            },\n            \"header\": {\n              \"fill\": {\n                \"color\": \"#C8D4E3\"\n              },\n              \"line\": {\n                \"color\": \"white\"\n              }\n            },\n            \"type\": \"table\"\n          }\n        ],\n        \"barpolar\": [\n          {\n            \"marker\": {\n              \"line\": {\n                \"color\": \"#E5ECF6\",\n                \"width\": 0.5\n              },\n              \"pattern\": {\n                \"fillmode\": \"overlay\",\n                \"size\": 10,\n                \"solidity\": 0.2\n              }\n            },\n            \"type\": \"barpolar\"\n          }\n        ],\n        \"pie\": [\n          {\n            \"automargin\": true,\n            \"type\": \"pie\"\n          }\n        ]\n      },\n      \"layout\": {\n        \"autotypenumbers\": \"strict\",\n        \"colorway\": [\n          \"#636efa\",\n          \"#EF553B\",\n          \"#00cc96\",\n          \"#ab63fa\",\n          \"#FFA15A\",\n          \"#19d3f3\",\n          \"#FF6692\",\n          \"#B6E880\",\n          \"#FF97FF\",\n          \"#FECB52\"\n        ],\n        \"font\": {\n          \"color\": \"#2a3f5f\"\n        },\n        \"hovermode\": \"closest\",\n        \"hoverlabel\": {\n          \"align\": \"left\"\n        },\n        \"paper_bgcolor\": \"white\",\n        \"plot_bgcolor\": \"#E5ECF6\",\n        \"polar\": {\n          \"bgcolor\": \"#E5ECF6\",\n          \"angularaxis\": {\n            \"gridcolor\": \"white\",\n            \"linecolor\": \"white\",\n            \"ticks\": \"\"\n          },\n          \"radialaxis\": {\n            \"gridcolor\": \"white\",\n            \"linecolor\": \"white\",\n            \"ticks\": \"\"\n          }\n        },\n        \"ternary\": {\n          \"bgcolor\": \"#E5ECF6\",\n          \"aaxis\": {\n            \"gridcolor\": \"white\",\n            \"linecolor\": \"white\",\n            \"ticks\": \"\"\n          },\n          \"baxis\": {\n            \"gridcolor\": \"white\",\n            \"linecolor\": \"white\",\n            \"ticks\": \"\"\n          },\n          \"caxis\": {\n            \"gridcolor\": \"white\",\n            \"linecolor\": \"white\",\n            \"ticks\": \"\"\n          }\n        },\n        \"coloraxis\": {\n          \"colorbar\": {\n            \"outlinewidth\": 0,\n            \"ticks\": \"\"\n          }\n        },\n        \"colorscale\": {\n          \"sequential\": [\n            [\n              0,\n              \"#0d0887\"\n            ],\n            [\n              0.1111111111111111,\n              \"#46039f\"\n            ],\n            [\n              0.2222222222222222,\n              \"#7201a8\"\n            ],\n            [\n              0.3333333333333333,\n              \"#9c179e\"\n            ],\n            [\n              0.4444444444444444,\n              \"#bd3786\"\n            ],\n            [\n              0.5555555555555556,\n              \"#d8576b\"\n            ],\n            [\n              0.6666666666666666,\n              \"#ed7953\"\n            ],\n            [\n              0.7777777777777778,\n              \"#fb9f3a\"\n            ],\n            [\n              0.8888888888888888,\n              \"#fdca26\"\n            ],\n            [\n              1,\n              \"#f0f921\"\n            ]\n          ],\n          \"sequentialminus\": [\n            [\n              0,\n              \"#0d0887\"\n            ],\n            [\n              0.1111111111111111,\n              \"#46039f\"\n            ],\n            [\n              0.2222222222222222,\n              \"#7201a8\"\n            ],\n            [\n              0.3333333333333333,\n              \"#9c179e\"\n            ],\n            [\n              0.4444444444444444,\n              \"#bd3786\"\n            ],\n            [\n              0.5555555555555556,\n              \"#d8576b\"\n            ],\n            [\n              0.6666666666666666,\n              \"#ed7953\"\n            ],\n            [\n              0.7777777777777778,\n              \"#fb9f3a\"\n            ],\n            [\n              0.8888888888888888,\n              \"#fdca26\"\n            ],\n            [\n              1,\n              \"#f0f921\"\n            ]\n          ],\n          \"diverging\": [\n            [\n              0,\n              \"#8e0152\"\n            ],\n            [\n              0.1,\n              \"#c51b7d\"\n            ],\n            [\n              0.2,\n              \"#de77ae\"\n            ],\n            [\n              0.3,\n              \"#f1b6da\"\n            ],\n            [\n              0.4,\n              \"#fde0ef\"\n            ],\n            [\n              0.5,\n              \"#f7f7f7\"\n            ],\n            [\n              0.6,\n              \"#e6f5d0\"\n            ],\n            [\n              0.7,\n              \"#b8e186\"\n            ],\n            [\n              0.8,\n              \"#7fbc41\"\n            ],\n            [\n              0.9,\n              \"#4d9221\"\n            ],\n            [\n              1,\n              \"#276419\"\n            ]\n          ]\n        },\n        \"xaxis\": {\n          \"gridcolor\": \"white\",\n          \"linecolor\": \"white\",\n          \"ticks\": \"\",\n          \"title\": {\n            \"standoff\": 15\n          },\n          \"zerolinecolor\": \"white\",\n          \"automargin\": true,\n          \"zerolinewidth\": 2\n        },\n        \"yaxis\": {\n          \"gridcolor\": \"white\",\n          \"linecolor\": \"white\",\n          \"ticks\": \"\",\n          \"title\": {\n            \"standoff\": 15\n          },\n          \"zerolinecolor\": \"white\",\n          \"automargin\": true,\n          \"zerolinewidth\": 2\n        },\n        \"scene\": {\n          \"xaxis\": {\n            \"backgroundcolor\": \"#E5ECF6\",\n            \"gridcolor\": \"white\",\n            \"linecolor\": \"white\",\n            \"showbackground\": true,\n            \"ticks\": \"\",\n            \"zerolinecolor\": \"white\",\n            \"gridwidth\": 2\n          },\n          \"yaxis\": {\n            \"backgroundcolor\": \"#E5ECF6\",\n            \"gridcolor\": \"white\",\n            \"linecolor\": \"white\",\n            \"showbackground\": true,\n            \"ticks\": \"\",\n            \"zerolinecolor\": \"white\",\n            \"gridwidth\": 2\n          },\n          \"zaxis\": {\n            \"backgroundcolor\": \"#E5ECF6\",\n            \"gridcolor\": \"white\",\n            \"linecolor\": \"white\",\n            \"showbackground\": true,\n            \"ticks\": \"\",\n            \"zerolinecolor\": \"white\",\n            \"gridwidth\": 2\n          }\n        },\n        \"shapedefaults\": {\n          \"line\": {\n            \"color\": \"#2a3f5f\"\n          }\n        },\n        \"annotationdefaults\": {\n          \"arrowcolor\": \"#2a3f5f\",\n          \"arrowhead\": 0,\n          \"arrowwidth\": 1\n        },\n        \"geo\": {\n          \"bgcolor\": \"white\",\n          \"landcolor\": \"#E5ECF6\",\n          \"subunitcolor\": \"white\",\n          \"showland\": true,\n          \"showlakes\": true,\n          \"lakecolor\": \"white\"\n        },\n        \"title\": {\n          \"x\": 0.05\n        },\n        \"mapbox\": {\n          \"style\": \"light\"\n        }\n      }\n    }\n  }\n}\n```\n\n</details>",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hi @jonasjuerss, thanks for flagging this! I'll raise your great feedback with the team, and will let you know what's the decision on this "
      },
      {
        "user": "luisbergua",
        "body": "@jonasjuerss actually tested and it seems `plotly==5.24.1` works fine in case you want to downgrade"
      },
      {
        "user": "jonasjuerss",
        "body": "> [@jonasjuerss](https://github.com/jonasjuerss) actually tested and it seems `plotly==5.24.1` works fine in case you want to downgrade\n\nThanks, I can confirm that this worked for me as a temporary workaround."
      }
    ]
  },
  {
    "issue_number": 4624,
    "title": "[App]: Offline resuming issue",
    "author": "GabPrato",
    "state": "open",
    "created_at": "2022-12-14T00:11:15Z",
    "updated_at": "2025-06-17T00:56:42Z",
    "labels": [
      "ty:feature",
      "a:sdk",
      "c:sdk:resume"
    ],
    "body": "### Current Behavior\n\nIf I run a job in offline mode and stop the job, then resume, I get two different results depending on when I stop the job, shouldn't happen and is very easy to reproduce.\r\nIf I log more steps in the first part than in the second, I only see the first part being plotted on wandb. The logs contain everything but the second part is first in the logs and the first part is after.\r\nIf I log more steps in the second part, I see all the steps in the plot on wandb. The logs only contain the second part though.\n\n### Expected Behavior\n\nAll steps logged should be visible and the logs should contain all logs and in order.\n\n### Steps To Reproduce\n\nHere's how to reproduce:\r\n\r\nPython code, make sure to set a value for project and entity in `wandb.init()`:\r\n```python\r\nimport argparse\r\nimport numpy as np\r\nimport wandb\r\nimport time\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--wandb_experiment_name\", type=str)\r\nparser.add_argument(\"--wandb_experiment_id\", type=str)\r\nparser.add_argument(\"--step_to_start_at\", type=int, default=0)\r\nargs = parser.parse_args()\r\n\r\n\r\nwandb.init(project='undefined', entity='undefined', name=args.wandb_experiment_name, config=args,\r\n    id=args.wandb_experiment_id, resume=\"allow\")\r\n\r\n\r\nfor i in range(args.step_to_start_at, 100):\r\n    print(f'{i}/100')\r\n    time.sleep(1)\r\n    wandb.log({'Train Loss': np.random.rand(1).item()}, step=i)\r\n```\r\n\r\nThen in your console run the following commands:\r\n```bash\r\nWANDB_EXPERIMENT_NAME=\"debug-wandb23\" # non unique name\r\nWANDB_EXPERIMENT_ID=\"2022-12-13_18-45-00\" # unique wandb id, max 64 characters\r\n\r\nwandb offline\r\nexport WANDB_DIR=\"./wandb/$WANDB_EXPERIMENT_ID\"\r\nmkdir -p $WANDB_DIR\r\n\r\npython -u debug_wandb.py \\\r\n    --wandb_experiment_name=$WANDB_EXPERIMENT_NAME \\\r\n    --wandb_experiment_id=$WANDB_EXPERIMENT_ID \\\r\n    --step_to_start_at=0 \r\n```\r\nAnd kill the job at around 50 steps. Then restart the job:\r\n```bash\r\npython -u debug_wandb.py \\\r\n    --wandb_experiment_name=$WANDB_EXPERIMENT_NAME \\\r\n    --wandb_experiment_id=$WANDB_EXPERIMENT_ID \\\r\n    --step_to_start_at=51\r\n\r\n```\r\nAnd kill it once it gets to step 70. Then sync:\r\n```bash\r\nwandb sync wandb/$WANDB_EXPERIMENT_ID/wandb/off*\r\n```\r\nI get the following plot:\r\n![Screenshot from 2022-12-13 18-53-09](https://user-images.githubusercontent.com/25964820/207471618-2352accf-6ad3-4d40-9288-2b1d977d3ce4.png)\r\n\r\n​As you can see, it logs up to about where we killed the job. If I look at the logs on wandb, they are in the wrong order:\r\n> 50/100\r\n> 51/100\r\n> 52/100\r\n> 53/100\r\n> 54/100\r\n> 55/100\r\n> 56/100\r\n> 57/100\r\n> 58/100\r\n> 59/100\r\n> 60/100\r\n> 61/100\r\n> 62/100\r\n> 63/100\r\n> 64/100\r\n> 65/100\r\n> 66/100\r\n> 67/100\r\n> 68/100\r\n> 69/100\r\n> Traceback (most recent call last):\r\n>   File \"/home/pratogab/Projects/efficient_video_transformer/debug_wandb.py\", line 21, in <module>\r\n>     time.sleep(1)\r\n> KeyboardInterrupt\r\n> 24/100\r\n> 25/100\r\n> 26/100\r\n> 27/100\r\n> 28/100\r\n> 29/100\r\n> 30/100\r\n> 31/100\r\n> 32/100\r\n> 33/100\r\n> 34/100\r\n> 35/100\r\n> 36/100\r\n> 37/100\r\n> 38/100\r\n> 39/100\r\n> 40/100\r\n> 41/100\r\n> 42/100\r\n> 43/100\r\n> 44/100\r\n> 45/100\r\n> 46/100\r\n> 47/100\r\n> 48/100\r\n> 49/100\r\n> Traceback (most recent call last):\r\n>   File \"/home/pratogab/Projects/efficient_video_transformer/debug_wandb.py\", line 21, in <module>\r\n>     time.sleep(1)\r\n> KeyboardInterrupt\r\n\r\nIf instead I kill the job at step around 20 and then run the second one for longer, I see all the logs:\r\n```bash\r\nWANDB_EXPERIMENT_NAME=\"debug-wandb24\" # non unique name\r\nWANDB_EXPERIMENT_ID=\"2022-12-13_18-46-00\" # unique wandb id, max 64 characters\r\n\r\nwandb offline\r\nexport WANDB_DIR=\"./wandb/$WANDB_EXPERIMENT_ID\"\r\nmkdir -p $WANDB_DIR\r\n\r\npython -u debug_wandb.py \\\r\n    --wandb_experiment_name=$WANDB_EXPERIMENT_NAME \\\r\n    --wandb_experiment_id=$WANDB_EXPERIMENT_ID \\\r\n    --step_to_start_at=0 \r\n```\r\n(I kill the job at around 20 steps. Then restart the job:)\r\n```bash\r\npython -u debug_wandb.py \\\r\n    --wandb_experiment_name=$WANDB_EXPERIMENT_NAME \\\r\n    --wandb_experiment_id=$WANDB_EXPERIMENT_ID \\\r\n    --step_to_start_at=21\r\n```\r\n(And kill it once it gets to step 90. Then sync:)\r\n```bash\r\nwandb sync wandb/$WANDB_EXPERIMENT_ID/wandb/off*\r\n```\r\nI get this:\r\n![Screenshot from 2022-12-13 18-56-32](https://user-images.githubusercontent.com/25964820/207471722-7e87feea-cd85-43e6-add3-67e6adeebef3.png)\r\n\r\nIf I look at the logs, they are missing the first part:\r\n> 37/100\r\n> 38/100\r\n> 39/100\r\n> 40/100\r\n> 41/100\r\n> 42/100\r\n> 43/100\r\n> 44/100\r\n> 45/100\r\n> 46/100\r\n> 47/100\r\n> 48/100\r\n> 49/100\r\n> 50/100\r\n> 51/100\r\n> 52/100\r\n> 53/100\r\n> 54/100\r\n> 55/100\r\n> 56/100\r\n> 57/100\r\n> 58/100\r\n> 59/100\r\n> 60/100\r\n> 61/100\r\n> 62/100\r\n> 63/100\r\n> 64/100\r\n> 65/100\r\n> 66/100\r\n> 67/100\r\n> 68/100\r\n> 69/100\r\n> 70/100\r\n> 71/100\r\n> 72/100\r\n> 73/100\r\n> 74/100\r\n> 75/100\r\n> 76/100\r\n> 77/100\r\n> 78/100\r\n> 79/100\r\n> 80/100\r\n> 81/100\r\n> 82/100\r\n> 83/100\r\n> 84/100\r\n> 85/100\r\n> 86/100\r\n> 87/100\r\n> 88/100\r\n> 89/100\r\n> 90/100\r\n> Traceback (most recent call last):\r\n>   File \"/home/pratogab/Projects/efficient_video_transformer/debug_wandb.py\", line 21, in <module>\r\n>     time.sleep(1)\r\n> KeyboardInterrupt\n\n### Screenshots\n\n![Screenshot from 2022-12-13 18-53-09](https://user-images.githubusercontent.com/25964820/207472361-79732dad-b392-4b73-800e-ab80704d5bff.png)\r\n![Screenshot from 2022-12-13 18-56-32](https://user-images.githubusercontent.com/25964820/207472362-e0b6c249-e2f5-43d7-89af-5b3a15ed5a4d.png)\r\n\n\n### Environment\n\nOS: Linux, ubuntu 20\r\n\r\nBrowsers: Chrome\r\n\r\nVersion: wandb 0.12.14 & 0.13.6\r\n\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "thanos-wandb",
        "body": "Hi @pratogab thanks so much for reporting this, and all the detailed information to reproduce this error. Does the issue pop up only when you kill the process? Have you tried for instance running this to a certain step and properly terminate the job with `wandb.finish()` and then continue from there with `resume='must'`?"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Frida de Sigley commented: \nRequest #37450 \"Wandb sync issue\" was closed and merged into this request. Last comment in request #37450:\r\n\r\nPosted on the github:\r\nhttps://github.com/wandb/wandb/issues/4624\n"
      },
      {
        "user": "GabPrato",
        "body": "@thanos-wandb I tried what you asked, but same result.\r\n```python\r\nimport argparse\r\nimport numpy as np\r\nimport wandb\r\nimport time\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--wandb_experiment_name\", type=str)\r\nparser.add_argument(\"--wandb_experiment_id\", type=str)\r\nparser.add_argument(\"--wandb_resume\", action='store_true')\r\nparser.add_argument(\"--step_to_start_at\", type=int, default=0)\r\nparser.add_argument(\"--step_to_end_at\", type=int, default=100)\r\nargs = parser.parse_args()\r\n\r\n\r\nif args.wandb_resume:\r\n    wandb.init(project='undefined', entity='undefined', name=args.wandb_experiment_name, config=args, \r\n        id=args.wandb_experiment_id, resume=\"must\")\r\nelse:\r\n    wandb.init(project='undefined', entity='undefined', name=args.wandb_experiment_name, config=args, \r\n        id=args.wandb_experiment_id)\r\n\r\n\r\nfor i in range(args.step_to_start_at, args.step_to_end_at + 1):\r\n    print(f'{i}/{args.step_to_end_at}')\r\n    time.sleep(1)\r\n    wandb.log({'Train Loss': np.random.rand(1).item()}, step=i)\r\n\r\nwandb.finish()\r\n```\r\n\r\n```bash\r\nWANDB_EXPERIMENT_NAME=\"debug-wandb25\" # non unique name\r\nWANDB_EXPERIMENT_ID=\"2022-12-17_10-29-00\" # unique wandb id, max 64 characters\r\n\r\nwandb offline\r\nexport WANDB_DIR=\"./wandb/$WANDB_EXPERIMENT_ID\"\r\nmkdir -p $WANDB_DIR\r\n\r\npython -u debug_wandb.py \\\r\n    --wandb_experiment_name=$WANDB_EXPERIMENT_NAME \\\r\n    --wandb_experiment_id=$WANDB_EXPERIMENT_ID \\\r\n    --step_to_start_at=0 \\\r\n    --step_to_end_at=50\r\n\r\npython -u debug_wandb.py \\\r\n    --wandb_experiment_name=$WANDB_EXPERIMENT_NAME \\\r\n    --wandb_experiment_id=$WANDB_EXPERIMENT_ID \\\r\n    --wandb_resume \\\r\n    --step_to_start_at=50 \\\r\n    --step_to_end_at=71\r\n\r\nwandb sync wandb/$WANDB_EXPERIMENT_ID/wandb/off*\r\n```\r\n\r\nHere's the output of both runs (truncated the print(i) calls):\r\n\r\n> wandb: Tracking run with wandb version 0.12.14\r\n> wandb: W&B syncing is set to `offline` in this directory.  \r\n> wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\r\n> 0/50\r\n> 1/50\r\n> .\r\n> .\r\n> .\r\n> 49/50\r\n> \r\n> wandb: Waiting for W&B process to finish... (success).\r\n> wandb:                                                                                \r\n> wandb: \r\n> wandb: Run history:\r\n> wandb: Train Loss ▂▇▇▃▃▄▂▇▅▂▁▇▃▆▇▁▅▅▇▂▆▃▃▂▂▆▇▂▆▇▅▇▅▃▆▂█▃▅▃\r\n> wandb: \r\n> wandb: Run summary:\r\n> wandb: Train Loss 0.29312\r\n> wandb: \r\n> wandb: You can sync this run to the cloud by running:\r\n> wandb: wandb sync ./wandb/2022-12-17_10-29-00/wandb/offline-run-20221217_103128-2022-12-17_10-29-00\r\n> wandb: Find logs at: ./wandb/2022-12-17_10-29-00/wandb/offline-run-20221217_103128-2022-12-17_10-29-00/logs\r\n\r\n> wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 2022-12-17_10-29-00.\r\n> wandb: Tracking run with wandb version 0.12.14\r\n> wandb: W&B syncing is set to `offline` in this directory.  \r\n> wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\r\n> 51/71\r\n> 52/71\r\n.\r\n.\r\n.\r\n> 70/71\r\n> \r\n> wandb: Waiting for W&B process to finish... (success).\r\n> wandb:                                                                                \r\n> wandb: \r\n> wandb: Run history:\r\n> wandb: Train Loss ▄▂▆▃▁▆▆▅▂▆█▃▃▄▃▅▅█▆▄\r\n> wandb: \r\n> wandb: Run summary:\r\n> wandb: Train Loss 0.53598\r\n> wandb: \r\n> wandb: You can sync this run to the cloud by running:\r\n> wandb: wandb sync ./wandb/2022-12-17_10-29-00/wandb/offline-run-20221217_103231-2022-12-17_10-29-00\r\n> wandb: Find logs at: ./wandb/2022-12-17_10-29-00/wandb/offline-run-20221217_103231-2022-12-17_10-29-00/logs\r\n\r\nAnd I only get the first steps again:\r\n![Screenshot from 2022-12-17 10-36-20](https://user-images.githubusercontent.com/25964820/208249733-fee467d0-0e45-4103-936e-c89111d6c3c0.png)\r\n"
      }
    ]
  },
  {
    "issue_number": 10006,
    "title": "Request for ARM64 Docker Image or Build Instructions",
    "author": "GGGsk",
    "state": "open",
    "created_at": "2025-06-09T01:34:52Z",
    "updated_at": "2025-06-16T17:39:29Z",
    "labels": [],
    "body": "I'm trying to run wandb on an ARM-based device, but it appears that the current official Docker images only support x86_64 architecture.\n\nTo better support ARM users, I'd like to request one of the following:\n\nOfficially built and published Docker images for ARM64 architectures;\nDetailed build instructions or a Dockerfile so that community members can build ARM-compatible images themselves;\nIf such resources already exist, please update the documentation to clearly state how to obtain them.\nThis would help improve the deployment experience of W&B on ARM devices (e.g., Raspberry Pi, AWS Graviton instances, etc.).\n\n✅ Expected Outcome\nUsers should be able to run the W&B server components smoothly on ARM-based systems.\nProvide build scripts or documentation to ensure the build process is reproducible.\nIf multi-architecture images are available, they should be pullable via Docker.\n🔧 Optional Suggestion\nIf ARM builds are already supported internally but not publicly released, please consider publishing them on Docker Hub with manifest support for multi-arch tags.",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @GGGsk! thank you for writing in!\n\n> I'm trying to run wandb on an ARM-based device, but it appears that the current official Docker images only support x86_64 architecture.\n\nCould you please point me to where you are seeing that on your side? We do support ARM with Apple's Arm-based macbooks"
      },
      {
        "user": "GGGsk",
        "body": "> Hey [@GGGsk](https://github.com/GGGsk)! thank you for writing in!\n> \n> > I'm trying to run wandb on an ARM-based device, but it appears that the current official Docker images only support x86_64 architecture.\n> \n> Could you please point me to where you are seeing that on your side? We do support ARM with Apple's Arm-based macbooks\n\nHi @ArtsiomWB, thanks for your reply!\n\nI encountered this issue when trying to pull the official W&B Docker images (e.g., wandb/local) on an ARM64-based device (Ubuntu 20.04 running on AWS Graviton). The image pull failed with a manifest not found error, indicating that the image doesn't have support for the ARM64 architecture.\n\nFor example:\n\ndocker pull wandb/local\n...\nmanifest for wandb/local:latest not found: manifest unknown: manifest unknown\n\nThis suggests that while WandB may work on ARM-based MacBooks via Rosetta or multi-arch support in Docker Desktop, the official images on Docker Hub don’t yet include native ARM64 builds.\n\nTo be honest, I'm quite new to programming. I tried to build a Docker image (wandb/local) for ARM architecture using the source code of this project, in order to deploy it locally on an ARM-based device. Unfortunately, I failed. Therefore, I really hope that you could provide an officially built ARM-compatible image — I would truly appreciate it. \n\nThank you again for your reply!😊\n\nSorry for the confusion — I didn’t make myself clear in the original message. What I meant by \"ARM architecture support\" was not the wandb python library, but the wandb/local Docker image itself. I’m not sure if that caused any misunderstanding on your side."
      },
      {
        "user": "ArtsiomWB",
        "body": "> Sorry for the confusion — I didn’t make myself clear in the original message. What I meant by \"ARM architecture support\" was not the wandb python library, but the wandb/local Docker image itself. I’m not sure if that caused any misunderstanding on your side.\n\nNo worries what-so-ever, so you are referring to W&B Server? \n\nJust to confirm, when you go to deploy.wandb.ai, and try following the installation steps:\n\nhttps://github.com/user-attachments/assets/145b1892-c011-48af-9f70-a7888a93431f\n\nYou are not able to run them on arm-based machines (that are not MacOS-based systems), correct? :) "
      }
    ]
  },
  {
    "issue_number": 10018,
    "title": "[Q]: How to get console logs to appear in Logs tab with Isaac Lab",
    "author": "Creampelt",
    "state": "open",
    "created_at": "2025-06-11T18:22:54Z",
    "updated_at": "2025-06-16T16:43:45Z",
    "labels": [
      "ty:question"
    ],
    "body": "I'm currently using W&B for reinforcement learning with Isaac Lab. Previously, I was using an old version of the SDK (0.12) required by rl-games, but have since updated. Since then, the console output in the Logs tab of a run has not reflected the actual console output.\n\n**With default settings/console=\"wrap\"**: Both the Logs tab and output.log file only contain wandb logs/warnings and errors that end the training runs.\n\n**With console=\"redirect\"**: The Logs tab is empty, but the output.log file contains all of the console logs.\n\nIn both cases, all output is also printed to the terminal.\n\n**Environment**:\nOS: Ubuntu 22.04\npython: 3.10.16\nwandb: 0.19.11\nisaacsim: 4.5.0\nIsaac Lab: 2.1.0\n\nI initialize my wandb run like so:\n```python\nself.run = wandb.init(project=project, entity=entity, settings=wandb.Settings({\"console\": \"redirect\"}))\n```\n\nI couldn't find much documentation on configuring console output, so I'm unsure if this is expected behavior or a bug. Any help is greatly appreciated!",
    "comments": [
      {
        "user": "paulosabile-wb",
        "body": "Hi @Creampelt Good day and thank you for reporting this behavior to us. Happy to review this for you.\n\nWould you mind sharing a minimal code snippet so I can try to reproduce the behavior and further investigate? Thanks!"
      },
      {
        "user": "Creampelt",
        "body": "Yes, here you go: https://github.com/Creampelt/wandb_example\n\nFor installing IsaacLab, I used [pip installation](https://isaac-sim.github.io/IsaacLab/main/source/setup/installation/pip_installation.html#) and only installed the rsl-rl framework (i.e. `./isaaclab.sh -i rsl-rl`). To run, use the command:\n```bash\npython scripts/rsl_rl/train.py --task Isaac-Velocity-Flat-Spot-v0 --headless --max_iterations 10\n```\n\nThe configuration for console settings can be found [here](https://github.com/Creampelt/wandb_example/blob/e93d9f61ede2985d679c03162662dd856a44fa24/source/wandb_example/wandb_example/frameworks/robot_rl/utils/wandb_utils.py#L35C38-L35C57). Let me know if you need anything else!"
      }
    ]
  },
  {
    "issue_number": 10035,
    "title": "[Bug]: segmentation masks and config parameters are not synced from offline run",
    "author": "nickl1234567",
    "state": "open",
    "created_at": "2025-06-16T15:51:33Z",
    "updated_at": "2025-06-16T16:03:17Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nI usually run my wandb in online mode. Now, I switched it to offline. After syncing, I noticed that the config parameters and the segmentation masks are not uploaded.\nThe segmentation masks are stored in the wandb_dir, but not uploaded.\nI wasn't able to locate the config parameters in a JSON or something. However, they appear in the run-<id>.wandb file.\nI would assume that the segmentation masks are just forgotten in the code responsible for sync.\n\nPlease let me know if you need further information.\n\nThe same issue was previously mentioned in the following issues, but never fixed:\n[After wandb sync, There no summary value.](https://github.com/wandb/wandb/issues/7175)\n[Segmentation mask not appearing](https://github.com/wandb/wandb/issues/3090)\n",
    "comments": []
  },
  {
    "issue_number": 10033,
    "title": "[Bug]: Using the `wandb` Package (The Sweep Feature) in VS Code Interactive Mode on Windows",
    "author": "RoyiAvital",
    "state": "open",
    "created_at": "2025-06-16T13:17:20Z",
    "updated_at": "2025-06-16T14:03:29Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nI work with `wandb` on Windows.  \nWhen using the _sweep_ feature I can see the `agent()` function is adjusting itself to work on Jupyter.  \n\nWhen I use the `agent()` for sweep using [VS Code Interactive Window](https://code.visualstudio.com/docs/python/jupyter-support-py) I get many errors on runs which fail due to trying to open new process while the previous is not done.  \nIt seems many forks are executed, far beyond the `count` defined in `agent()`.\n\nCould it be the code does not treat the Interactive Window as it was a Jupyter notebook?",
    "comments": []
  },
  {
    "issue_number": 10002,
    "title": "[Bug]: Can't run tests because of missing authorization for test docker image (tools/local_wandb_server.py)",
    "author": "Giuspepe",
    "state": "closed",
    "created_at": "2025-06-08T01:18:46Z",
    "updated_at": "2025-06-16T09:13:45Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nI wanted to write a PR to fix https://github.com/wandb/wandb/issues/9654, but I can't get running the tests to work.\n\nAfter following the [documentation](https://github.com/wandb/wandb/blob/0bd64133e9046177ef14539be5a892cecc522728/CONTRIBUTING.md#using-pytest) and installing all dependencies, I ran `pytest -s -vv tests/system_tests/test_sweep/test_wandb_agent_full.py`. I got ```AssertionError: `python tools/local_wandb_server.py connect` failed. See stderr. Did you run `python tools/local_wandb_server.py start`?```. So then I ran that command and got the following error:\n\n\n```\n(wandb) ➜  wandb git:(main) ✗ python tools/local_wandb_server.py start                                 \n\nRegistry [us-central1-docker.pkg.dev]: \nRepository [wandb-production/images/local-testcontainer]: \nTag [master]: \n--pull (always, never, missing) [always]: \nlocal_wandb_server.py: Running command: docker run --rm --detach --pull always -e WANDB_ENABLE_TEST_CONTAINER=true --name wandb-local-testcontainer --volume wandb-local-testcontainer-vol:/vol --publish 8080 --publish 9015 --platform linux/amd64 us-central1-docker.pkg.dev/wandb-production/images/local-testcontainer:master\ndocker: Error response from daemon: failed to resolve reference \"us-central1-docker.pkg.dev/wandb-production/images/local-testcontainer:master\": failed to authorize: failed to fetch anonymous token: unexpected status from GET request to https://us-central1-docker.pkg.dev/v2/token?scope=repository%3Awandb-production%2Fimages%2Flocal-testcontainer%3Apull&service=us-central1-docker.pkg.dev: 403 Forbidden\n\nRun 'docker run --help' for more information\nTraceback (most recent call last):\n  File \"/Users/pepe/workspace/wandb/tools/local_wandb_server.py\", line 456, in <module>\n    main()\n  File \"/Users/pepe/workspace/wandb/.venv/lib/python3.9/site-packages/click/core.py\", line 1161, in __call__\n    return self.main(*args, **kwargs)\n  File \"/Users/pepe/workspace/wandb/.venv/lib/python3.9/site-packages/click/core.py\", line 1082, in main\n    rv = self.invoke(ctx)\n  File \"/Users/pepe/workspace/wandb/.venv/lib/python3.9/site-packages/click/core.py\", line 1697, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/Users/pepe/workspace/wandb/.venv/lib/python3.9/site-packages/click/core.py\", line 1443, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/Users/pepe/workspace/wandb/.venv/lib/python3.9/site-packages/click/core.py\", line 788, in invoke\n    return __callback(*args, **kwargs)\n  File \"/Users/pepe/workspace/wandb/tools/local_wandb_server.py\", line 69, in start\n    _start_interactively(name=name)\n  File \"/Users/pepe/workspace/wandb/tools/local_wandb_server.py\", line 149, in _start_interactively\n    _start_container(name=name).apply_ports(server)\n  File \"/Users/pepe/workspace/wandb/tools/local_wandb_server.py\", line 375, in _start_container\n    subprocess.check_call(command, stdout=sys.stderr)\n  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/subprocess.py\", line 373, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command '['docker', 'run', '--rm', '--detach', '--pull', 'always', '-e', 'WANDB_ENABLE_TEST_CONTAINER=true', '--name', 'wandb-local-testcontainer', '--volume', 'wandb-local-testcontainer-vol:/vol', '--publish', '8080', '--publish', '9015', '--platform', 'linux/amd64', 'us-central1-docker.pkg.dev/wandb-production/images/local-testcontainer:master']' returned non-zero exit status 125.\n```\n\nIt seems like it's trying to download a docker image from a private registry. \n\nWhat is the correct way for external contributors to run these tests then?",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bonnie Shen commented: \nHello @Giuspepe! Thank you for reaching out to us and your interest to contribute! Are you following this guide for your PR?\n"
      },
      {
        "user": "Giuspepe",
        "body": "> Bonnie Shen commented: Hello [@Giuspepe](https://github.com/Giuspepe)! Thank you for reaching out to us and your interest to contribute! Are you following this guide for your PR?\n\nHi Bonnie, yes I am following this [guide](https://github.com/wandb/wandb/blob/0bd64133e9046177ef14539be5a892cecc522728/CONTRIBUTING.md#using-pytest), but I can't get the tests to run because of this issue."
      },
      {
        "user": "thanos-wandb",
        "body": "Hi @Giuspepe thank you very much for contributing! Have you already opened a PR in our SDK? for external contributors you don’t need to pull our private “local-testcontainer” image, our CI will do that for you when you open a PR"
      }
    ]
  },
  {
    "issue_number": 9948,
    "title": "[Q]: UsageError: Unable to attach to run ...",
    "author": "Sascha-Roe",
    "state": "open",
    "created_at": "2025-06-01T10:52:53Z",
    "updated_at": "2025-06-15T20:30:36Z",
    "labels": [
      "ty:question"
    ],
    "body": "Hey everyone,\n\nI have trained a TFT-Model using WandB, which worked just fine. But when i try to predict using the trained model i get this error\n\n```python\nWandbAttachFailedError: Failed to attach because the run does not belong to the current service process, or because the service process is busy (unlikely)\n```\n<b />\n\n```python\nUsageError: Unable to attach to run g43vyyi7\n``` \n\n\n\nHas anyone encountered a similar error or knows how to fix this?\nI am using wandb version 0.19.11 with python version 3.12.10.\n\nA small example on how I try to make predictions:\n\n```python\nfrom darts.models import TFTModel\nmodel = TFTModel\nmodel_best = model.load_from_checkpoint(work_dir=work_dir, model_name=model_name, best=True)\n``` \n\nI then prepare my data for the predictions and try to make the predictions using:\n\n```python\npred_series = model_best.predict(n=pred_size,\n                        series=ts_ttest_temp[pred_idxs[0]:pred_idxs[1]],\n                        future_covariates= tcox_test_future[pred_idxs[0]:pred_idxs[3]],\n                        past_covariates=tcov_test[pred_idxs[0]:pred_idxs[1]],\n                        num_samples=1,   \n                        n_jobs=-1)\n``` \n\nThe entire Traceback looks the following:\n\n```python\n---------------------------------------------------------------------------\nWandbAttachFailedError                    Traceback (most recent call last)\nFile /srv/jupyterhub/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:1186, in _attach(attach_id, run_id, run)\n   1185 try:\n-> 1186     attach_settings = service.inform_attach(attach_id=attach_id)\n   1187 except Exception as e:\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/wandb/sdk/lib/service_connection.py:182, in ServiceConnection.inform_attach(self, attach_id)\n    181 except TimeoutError:\n--> 182     raise WandbAttachFailedError(\n    183         \"Failed to attach because the run does not belong to\"\n    184         \" the current service process, or because the service\"\n    185         \" process is busy (unlikely).\"\n    186     ) from None\n\nWandbAttachFailedError: Failed to attach because the run does not belong to the current service process, or because the service process is busy (unlikely).\n\nThe above exception was the direct cause of the following exception:\n\nUsageError                                Traceback (most recent call last)\nCell In[19], line 28\n     25 print('pred_idx: ',pred_idxs)\n     26 #print(tcov_test.start_time())\n     27 #print(tcov_test.end_time())\n---> 28 pred_t = evalTFT.pred_multi(model_best, pred_size, pred_idxs, ts_ttest_temp, tcov_test, tcov_test_future)\n     29 print(\"PREDICTED!\")\n     30 print(pred_t.start_time().weekday())\n\nFile ~/Documents/Code/Giaco/Evaluation/evalTFThelper.py:63, in pred_multi(model, pred_size, pred_idxs, ts_ttest_temp, tcov_test, tcox_test_future)\n     62 def pred_multi(model, pred_size, pred_idxs, ts_ttest_temp, tcov_test, tcox_test_future):\n---> 63     pred_series = model.predict(n=pred_size,\n     64                             series=ts_ttest_temp[pred_idxs[0]:pred_idxs[1]],\n     65                             future_covariates= tcox_test_future[pred_idxs[0]:pred_idxs[3]],\n     66                             past_covariates=tcov_test[pred_idxs[0]:pred_idxs[1]],\n     67                             num_samples=1,   \n     68                             n_jobs=-1)\n     69     return pred_series\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/darts/utils/torch.py:80, in random_method.<locals>.decorator(self, *args, **kwargs)\n     78 with fork_rng():\n     79     manual_seed(self._random_instance.randint(0, high=MAX_TORCH_SEED_VALUE))\n---> 80     return decorated(self, *args, **kwargs)\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/darts/models/forecasting/torch_forecasting_model.py:1530, in TorchForecastingModel.predict(self, n, series, past_covariates, future_covariates, trainer, batch_size, verbose, n_jobs, roll_size, num_samples, dataloader_kwargs, mc_dropout, predict_likelihood_parameters, show_warnings)\n   1511 super().predict(\n   1512     n,\n   1513     series,\n   (...)   1518     show_warnings=show_warnings,\n   1519 )\n   1521 dataset = self._build_inference_dataset(\n   1522     target=series,\n   1523     n=n,\n   (...)   1527     bounds=None,\n   1528 )\n-> 1530 predictions = self.predict_from_dataset(\n   1531     n,\n   1532     dataset,\n   1533     trainer=trainer,\n   1534     verbose=verbose,\n   1535     batch_size=batch_size,\n   1536     n_jobs=n_jobs,\n   1537     roll_size=roll_size,\n   1538     num_samples=num_samples,\n   1539     dataloader_kwargs=dataloader_kwargs,\n   1540     mc_dropout=mc_dropout,\n   1541     predict_likelihood_parameters=predict_likelihood_parameters,\n   1542 )\n   1544 return predictions[0] if called_with_single_series else predictions\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/darts/utils/torch.py:80, in random_method.<locals>.decorator(self, *args, **kwargs)\n     78 with fork_rng():\n     79     manual_seed(self._random_instance.randint(0, high=MAX_TORCH_SEED_VALUE))\n---> 80     return decorated(self, *args, **kwargs)\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/darts/models/forecasting/torch_forecasting_model.py:1679, in TorchForecastingModel.predict_from_dataset(self, n, input_series_dataset, trainer, batch_size, verbose, n_jobs, roll_size, num_samples, dataloader_kwargs, mc_dropout, predict_likelihood_parameters)\n   1674 self.trainer = self._setup_trainer(\n   1675     trainer=trainer, model=self.model, verbose=verbose, epochs=self.n_epochs\n   1676 )\n   1678 # prediction output comes as nested list: list of predicted `TimeSeries` for each batch.\n-> 1679 predictions = self.trainer.predict(model=self.model, dataloaders=pred_loader)\n   1680 # flatten and return\n   1681 return [ts for batch in predictions for ts in batch]\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:887, in Trainer.predict(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\n    885 self.state.status = TrainerStatus.RUNNING\n    886 self.predicting = True\n--> 887 return call._call_and_handle_interrupt(\n    888     self, self._predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\n    889 )\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:48, in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)\n     46     if trainer.strategy.launcher is not None:\n     47         return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n---> 48     return trainer_fn(*args, **kwargs)\n     50 except _TunerExitException:\n     51     _call_teardown_hook(trainer)\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:928, in Trainer._predict_impl(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\n    924     download_model_from_registry(ckpt_path, self)\n    925 ckpt_path = self._checkpoint_connector._select_ckpt_path(\n    926     self.state.fn, ckpt_path, model_provided=model_provided, model_connected=self.lightning_module is not None\n    927 )\n--> 928 results = self._run(model, ckpt_path=ckpt_path)\n    930 assert self.state.stopped\n    931 self.predicting = False\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:974, in Trainer._run(self, model, ckpt_path)\n    971 log.debug(f\"{self.__class__.__name__}: preparing data\")\n    972 self._data_connector.prepare_data()\n--> 974 call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment\n    975 log.debug(f\"{self.__class__.__name__}: configuring model\")\n    976 call._call_configure_model(self)\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:101, in _call_setup_hook(trainer)\n     99 # Trigger lazy creation of experiment in loggers so loggers have their metadata available\n    100 for logger in loggers:\n--> 101     if hasattr(logger, \"experiment\"):\n    102         _ = logger.experiment\n    104 trainer.strategy.barrier(\"pre_setup\")\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/lightning_fabric/loggers/logger.py:118, in rank_zero_experiment.<locals>.experiment(self)\n    116 if rank_zero_only.rank > 0:\n    117     return _DummyExperiment()\n--> 118 return fn(self)\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:404, in WandbLogger.experiment(self)\n    401     self._experiment = wandb.run\n    402 elif attach_id is not None and hasattr(wandb, \"_attach\"):\n    403     # attach to wandb process referenced\n--> 404     self._experiment = wandb._attach(attach_id)\n    405 else:\n    406     # create new wandb process\n    407     self._experiment = wandb.init(**self._wandb_init)\n\nFile /srv/jupyterhub/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:1188, in _attach(attach_id, run_id, run)\n   1186     attach_settings = service.inform_attach(attach_id=attach_id)\n   1187 except Exception as e:\n-> 1188     raise UsageError(f\"Unable to attach to run {attach_id}\") from e\n   1190 settings: Settings = copy.copy(_wl._settings)\n   1192 settings.update_from_dict(\n   1193     {\n   1194         \"run_id\": attach_id,\n   (...)   1197     }\n   1198 )\n\nUsageError: Unable to attach to run g43vyyi7\n```\n\n",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Thomas Drayton commented: \nHi @Sascha-Roe,\n\nThanks for reaching out! I appreciate the detail you've provided regarding this issue that you're having.\n\nBased on the traceback, it looks like our service is trying to re-attach to run `g43vyyi7` but can’t because it is attempting to connect to a run that doesn’t match the workspace/run-id.\n\nIf you don't object, would you also mind sharing:\n\n- How the original training run was configured? The link to the run in your workspace would be great i.e. exact run ID and workspace (project/entity) you’re trying to attach to?\n- run-id you are passing to your prediction script\n- What PyTorch Lightning version and Darts version you used?\n- A minimal working example of your prediction script, if possible?\n\nThanks in advance!\n\nBest,\nThomas\n"
      },
      {
        "user": "Sascha-Roe",
        "body": "Hey Thomas,\n\nI am using darts 0.35.0 and PyTorch Lightning 2.5.1.post0.\nThe workspace is private, but the run that I'm trying to attach to is visible in my workspace. \nIn my prediction script i load the model using\n```python\nmodel_best = model.load_from_checkpoint(work_dir=work_dir, model_name=model_name, best=True)\n```\nwhere \n```python\nwork_dir = \"./models/first_runs/\"\nmodel_name = \"warm-waterfall-26\"\n```\nThe warm-waterfall-26 model is actually located in ./models/first_runs/\nA minimal example for the prediction can be found in the original post.\n\nThe training is configured the following:\n\nFirst all arguments are read in, then the training is started using:\n\n```python\nrun_name = wandb_go(args)\nSAVE = '/models/first_runs/' + run_name +'.pth.tar'\nmodel = define_model(args, run_name)\n\nmodel.fit(ts_ttrain_list, \n                future_covariates=[tcov_train_future] * num_knoten, \n                past_covariates=[tcov_train] * num_knoten,\n                verbose=True,\n                val_series=ts_ttest_list,\n                val_future_covariates=[tcov_test_future] * num_knoten,\n                val_past_covariates=[tcov_test] * num_knoten\n            )\nwandb.finish()\n\n```\n\n\n```python\ndef wandb_go(args):\n    '''Start wandb session with parameters'''\n    wandb.init(project=args.project_name, entity=\"MY_ENTITY\", sync_tensorboard=True, config=args)\n    name = wandb.run.name\n    print(\"Name of run for wandb: \", name)\n    return name\n```\n```python\ndef define_model(args, model_name):\n    wandb_logger = WandbLogger() \n    lr_monitor = LearningRateMonitor(logging_interval='step')\n    n_categories = 70  # how many nodes exist\n    embedding_size = 70  # embed the categorical variable into a numeric vector of size 2\n    categorical_embedding_sizes = {\"Knoten\": (n_categories, embedding_size)}\n\n    model = TFTModel(input_chunk_length=args.back_window,\n                output_chunk_length=args.horizon,\n                hidden_size=args.hidden,\n                lstm_layers=args.lstm_layers,\n                num_attention_heads=args.att_heads,\n                full_attention=args.full_att,\n                dropout=args.dropout,\n                batch_size=args.batch_size,\n                n_epochs=args.epochs,\n                likelihood=args.likelihood, \n                loss_fn=args.loss,\n                lr_scheduler_cls=args.decay_lr_class,\n                lr_scheduler_kwargs={\"gamma\":0.1},\n                random_state=args.rand, \n                force_reset=True,\n                log_tensorboard=True,\n                save_checkpoints=True,\n                model_name=model_name,\n                categorical_embedding_sizes=categorical_embedding_sizes,\n                work_dir = \"./models/first_runs\",\n                pl_trainer_kwargs={\n                    \"accelerator\": \"gpu\",\n                    \"devices\": -1, \n                    \"logger\":[wandb_logger],\n                    \"callbacks\":[lr_monitor]\n                }) \n    return model\n```\n\nI hope this helps.\n\nThank you for your assistance.\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Thomas Drayton commented: \nHi @Sascha-Roe,\n\nThanks for your patience!\n\nIt looks like the reason your train and predict scripts weren't working was because you were mixing PyTorch Lightning's automatic model logging with Darts' custom checkpoint format.\n\n\n1. `WandbLogger(log_model=\"all\")` automatically saved PyTorch Lightning `.ckpt` files to W&B\n2. But `TFTModel.load_from_checkpoint()` expects Darts' own format with `_model.pth.tar` files\n3. When loading, PyTorch Lightning tried to \"attach\" to the original W&B run, which failed with the attachment error\n\nAs a workaround, you can do the following to train a darts model, where you train the model using something like:\n\n\n- Set `WandbLogger(log_model=False)` to disable automatic model logging\n- Let Darts save models in its own format using `save_checkpoints=True`, `work_dir`, and `model_name`\n- Manually upload the Darts model directory as a W&B artifact after training\n- For prediction, create a new W&B inference run, download the artifact, and load using Darts' standard method\n\nA working example is provided for you below:\n\n\n    import argparseimport osimport pandas as pdimport numpy as npimport torchimport wandbfrom darts import TimeSeriesfrom darts.models import TFTModelfrom pytorch_lightning.callbacks import LearningRateMonitorfrom pytorch_lightning.loggers import WandbLoggerfrom darts.utils.timeseries_generation import sine_timeseriesfrom darts.dataprocessing.transformers import Scalerdef generate_data(num_series, n_samples, n_categories, input_chunk_length, output_chunk_length): \"\"\"Generates synthetic data for training the TFT model.\"\"\" all_series = [] for i in range(num_series): freq = 0.01 + 0.05 * (i / num_series) series = sine_timeseries( length=n_samples,  value_frequency=freq,  value_y_offset=i,  column_name=f\"series_{i}\",  dtype=np.float32 ) all_series.append(series) # Create a single multivariate TimeSeries from the list of series series_multivariate = all_series[0] for i in range(1, num_series): series_multivariate = series_multivariate.stack(all_series[i]) series_multivariate = series_multivariate.astype(np.float32) # Create static covariates static_covariates = pd.DataFrame( data={'Knoten': np.random.randint(0, n_categories, num_series)}, index=series_multivariate.components ) # Scale the target series scaler = Scaler() scaled_series = scaler.fit_transform(series_multivariate) scaled_series = scaled_series.astype(np.float32) # Add static covariates to the scaled series scaled_series = scaled_series.with_static_covariates(static_covariates) # Create past and future covariates past_covariates = scaled_series.copy().astype(np.float32) future_covariates = scaled_series.copy().astype(np.float32) # Split target series train_target, val_target = scaled_series.split_before(0.8) return ( train_target.astype(np.float32),  val_target.astype(np.float32),  past_covariates,  future_covariates, scaler )def main(): parser = argparse.ArgumentParser(description=\"\") parser.add_argument(\"--project_name\", type=str, default=\"tft-wandb\", help=\"W&B project name\") parser.add_argument(\"--entity\", type=str, default=None, help=\"W&B entity (username or team)\") parser.add_argument(\"--run_name\", type=str, default=None, help=\"W&B run name (auto-generated if not provided)\") parser.add_argument(\"--input_chunk_length\", type=int, default=24, help=\"Input sequence length\") parser.add_argument(\"--output_chunk_length\", type=int, default=12, help=\"Output sequence length\") parser.add_argument(\"--hidden_size\", type=int, default=32, help=\"Hidden size\") parser.add_argument(\"--lstm_layers\", type=int, default=2, help=\"Number of LSTM layers\") parser.add_argument(\"--num_attention_heads\", type=int, default=4, help=\"Number of attention heads\") parser.add_argument(\"--dropout\", type=float, default=0.1, help=\"Dropout rate\") parser.add_argument(\"--batch_size\", type=int, default=16, help=\"Batch size\") parser.add_argument(\"--n_epochs\", type=int, default=5, help=\"Number of epochs\") parser.add_argument(\"--random_state\", type=int, default=42, help=\"Random state\") args = parser.parse_args() # Initialize run wandb.init( project=args.project_name, entity=args.entity, name=args.run_name, config=vars(args), tags=[\"training\", \"tft\", \"darts\"] ) # Create logger - don't use log_model=\"all\" because Darts has its own format wandb_logger = WandbLogger( project=args.project_name, name=wandb.run.name, log_model=False # We'll manually upload Darts model artifacts ) # Learning rate monitor (don't add ModelCheckpoint - Darts handles its own checkpointing) lr_monitor = LearningRateMonitor(logging_interval='step') # Generate training data print(\"Generating training data...\") num_series = 10 n_samples = 200 n_categories = 70 (train_target, val_target, past_covariates, future_covariates, scaler) = generate_data( num_series, n_samples, n_categories, args.input_chunk_length, args.output_chunk_length ) # Define categorical embeddings embedding_size = 10 categorical_embedding_sizes = {\"Knoten\": (n_categories, embedding_size)} # Create TFT model model = TFTModel( input_chunk_length=args.input_chunk_length, output_chunk_length=args.output_chunk_length, hidden_size=args.hidden_size, lstm_layers=args.lstm_layers, num_attention_heads=args.num_attention_heads, dropout=args.dropout, batch_size=args.batch_size, n_epochs=args.n_epochs, categorical_embedding_sizes=categorical_embedding_sizes, lr_scheduler_cls=torch.optim.lr_scheduler.ExponentialLR, lr_scheduler_kwargs={\"gamma\": 0.95}, random_state=args.random_state, force_reset=True, # Darts model saving configuration save_checkpoints=True, work_dir=\"./models/tft_runs\", model_name=wandb.run.name, # PyTorch Lightning trainer arguments pl_trainer_kwargs={ \"accelerator\": \"auto\", \"logger\": wandb_logger, # W&B logger for metrics only \"callbacks\": [lr_monitor], \"max_epochs\": args.n_epochs, \"enable_checkpointing\": True } ) print(\"Starting training...\") # Train the model model.fit( series=train_target, val_series=val_target, past_covariates=past_covariates, future_covariates=future_covariates, val_past_covariates=past_covariates, val_future_covariates=future_covariates, verbose=True, ) print(\"Training completed. Now uploading model...\") # Manually upload Darts model as artifact # Darts saves models in work_dir/model_name/ directory model_dir = os.path.join(\"./models/tft_runs\", wandb.run.name) if os.path.exists(model_dir): # Create artifact model_artifact = wandb.Artifact( name=f\"tft-model-{wandb.run.name}\", type=\"model\", description=f\"TFT model trained on {num_series} time series\", metadata={ \"input_chunk_length\": args.input_chunk_length, \"output_chunk_length\": args.output_chunk_length, \"hidden_size\": args.hidden_size, \"num_series\": num_series, \"n_categories\": n_categories } ) # Add model files to artifact model_artifact.add_dir(model_dir, name=\"model\") # Log artifact wandb.log_artifact(model_artifact) print(f\"Model artifact uploaded: {model_artifact.name}\") else: print(f\"Model directory not found at {model_dir}\") print(f\"Training completed! Model artifacts saved to project: {args.project_name}\") print(f\"Run name: {wandb.run.name}\") print(f\"Run URL: {wandb.run.url}\") # Log final metrics and model info wandb.log({ \"final_train_loss\": model.trainer.callback_metrics.get(\"train_loss\", 0), \"final_val_loss\": model.trainer.callback_metrics.get(\"val_loss\", 0), \"model_parameters\": sum(p.numel() for p in model.model.parameters()), \"input_chunk_length\": args.input_chunk_length, \"output_chunk_length\": args.output_chunk_length }) # Save training configuration for prediction script wandb.config.update({ \"num_series\": num_series, \"n_samples\": n_samples, \"n_categories\": n_categories }) # Finish run wandb.finish()if __name__ == '__main__': main()\n\n\n  Inference with `python predict.py --project_name \"<you-project-name>\" --run_name \"<the-run-name>\"`\n\n\n    import argparseimport osimport pandas as pdimport numpy as npimport wandbfrom darts.models import TFTModelfrom darts.utils.timeseries_generation import sine_timeseriesfrom darts.dataprocessing.transformers import Scalerdef generate_prediction_data(num_series, n_samples, n_categories, input_chunk_length, output_chunk_length): \"\"\"Generates synthetic data for prediction - same structure as training data.\"\"\" all_series = [] for i in range(num_series): freq = 0.01 + 0.05 * (i / num_series) series = sine_timeseries( length=n_samples,  value_frequency=freq,  value_y_offset=i,  column_name=f\"series_{i}\",  dtype=np.float32 ) all_series.append(series) # Create a single multivariate TimeSeries from the list of series series_multivariate = all_series[0] for i in range(1, num_series): series_multivariate = series_multivariate.stack(all_series[i]) series_multivariate = series_multivariate.astype(np.float32) # Create static covariates (same structure as training) static_covariates = pd.DataFrame( data={'Knoten': np.random.randint(0, n_categories, num_series)}, index=series_multivariate.components ) # Scale the target series scaler = Scaler() scaled_series = scaler.fit_transform(series_multivariate) scaled_series = scaled_series.astype(np.float32) # Add static covariates to the scaled series scaled_series = scaled_series.with_static_covariates(static_covariates) # Create past and future covariates past_covariates = scaled_series.copy().astype(np.float32) future_covariates = scaled_series.copy().astype(np.float32) # Split for prediction input train_data, val_data = scaled_series.split_before(0.8) return train_data, val_data, past_covariates, future_covariates, scalerdef download_model_from_wandb(project_name, run_name, entity=None): \"\"\"Download model artifact from W&B.\"\"\" # Initialize API api = wandb.Api() # Search for the run runs = api.runs(f\"{entity or api.default_entity}/{project_name}\") training_run = None for run in runs: if run.name == run_name: training_run = run break if training_run is None: raise ValueError(f\"Run '{run_name}' not found in project '{project_name}'\") print(f\"Found training run: {training_run.name} ({training_run.id})\") # Get model artifacts from the run artifacts = training_run.logged_artifacts() model_artifact = None for artifact in artifacts: if artifact.type == \"model\": model_artifact = artifact break if model_artifact is None: raise ValueError(f\"No model artifact found in run '{run_name}'\") print(f\"Downloading model artifact: {model_artifact.name}\") # Download the artifact artifact_dir = model_artifact.download() # The artifact contains a 'model' directory with Darts model files model_dir = os.path.join(artifact_dir, \"model\") if not os.path.exists(model_dir): raise ValueError(f\"Model directory not found in artifact: {model_dir}\") # Check for required Darts model files required_files = [\"_model.pth.tar\", \"checkpoints\"] for req_file in required_files: file_path = os.path.join(model_dir, req_file) if not os.path.exists(file_path): print(f\"Warning: {req_file} not found in {model_dir}\") print(f\"Using model directory: {model_dir}\") return model_dir, training_run.configdef main(): parser = argparse.ArgumentParser(description=\"\") parser.add_argument(\"--project_name\", type=str, required=True, help=\"W&B project name\") parser.add_argument(\"--run_name\", type=str, required=True, help=\"Name of the training run to load model from\") parser.add_argument(\"--entity\", type=str, default=None, help=\"W&B entity (username or team)\") parser.add_argument(\"--pred_size\", type=int, default=12, help=\"Prediction horizon\") parser.add_argument(\"--num_samples\", type=int, default=1, help=\"Number of samples for probabilistic forecast\") args = parser.parse_args() # Initialize run for inference print(\"Initializing inference run...\") wandb.init( project=args.project_name, entity=args.entity, job_type=\"inference\", tags=[\"prediction\", \"tft\", \"inference\"], config={ \"training_run\": args.run_name, \"pred_size\": args.pred_size, \"num_samples\": args.num_samples } ) try: # Download model print(f\"Downloading model from training run: {args.run_name}\") model_dir, training_config = download_model_from_wandb( project_name=args.project_name, run_name=args.run_name, entity=args.entity ) # Load model from downloaded directory using Darts format print(\"Loading model from Darts model directory...\") # Extract the parent directory and model name for Darts loading parent_dir = os.path.dirname(model_dir) model_name = os.path.basename(model_dir) model = TFTModel.load_from_checkpoint( model_name=model_name, work_dir=parent_dir, best=True ) print(\"Model loaded successfully!\") # Get training configuration for data generation num_series = training_config.get(\"num_series\", 10) n_samples = training_config.get(\"n_samples\", 200) n_categories = training_config.get(\"n_categories\", 70) print(f\"Generating prediction data (series: {num_series}, samples: {n_samples})\") # Generate prediction data with same parameters as training train_data, val_data, past_covariates, future_covariates, scaler = generate_prediction_data( num_series=num_series, n_samples=n_samples, n_categories=n_categories, input_chunk_length=model.input_chunk_length, output_chunk_length=model.output_chunk_length ) # Make predictions print(f\"Making predictions (horizon: {args.pred_size})...\") predictions = model.predict( n=args.pred_size, series=train_data, # Use training data as input past_covariates=past_covariates, future_covariates=future_covariates, num_samples=args.num_samples, n_jobs=1 # Use single job for clearer error messages ) print(\"Predictions completed successfully!\") # Log prediction results  # Convert predictions to numpy for logging pred_values = predictions[0].values() if hasattr(predictions, '__len__') and len(predictions) > 0 else predictions.values() wandb.log({ \"prediction_horizon\": args.pred_size, \"prediction_shape\": pred_values.shape, \"prediction_mean\": np.mean(pred_values), \"prediction_std\": np.std(pred_values), \"num_components\": len(predictions.components) if hasattr(predictions, 'components') else 1 }) # Log some sample predictions if len(pred_values.shape) > 1: for i in range(min(3, pred_values.shape[1])): # Log first 3 components component_name = predictions.components[i] if hasattr(predictions, 'components') else f\"component_{i}\" wandb.log({f\"predictions_{component_name}\": pred_values[:, i].tolist()}) else: wandb.log({\"predictions\": pred_values.tolist()}) print(f\"Prediction summary:\") print(f\" Shape: {pred_values.shape}\") print(f\" Mean: {np.mean(pred_values):.4f}\") print(f\" Std: {np.std(pred_values):.4f}\") # Show first few predictions print(f\" First 5 predictions: {pred_values[:5].flatten()}\") except Exception as e: print(f\"Error during prediction: {e}\") wandb.log({\"error\": str(e)}) raise finally: wandb.finish() print(\"Inference run completed.\")if __name__ == '__main__': main()\n\n\nHopefully this works for you!\n\nPlease let us know if you have any additional questions!\n\nBest,\nThomas\n"
      }
    ]
  },
  {
    "issue_number": 1370,
    "title": "wandb 10.x creates symlinks, which require administrator privileges on Windows",
    "author": "maria-korosteleva",
    "state": "closed",
    "created_at": "2020-10-16T08:31:33Z",
    "updated_at": "2025-06-15T08:00:50Z",
    "labels": [
      "ty:bug"
    ],
    "body": "wandb version: 10.7\r\nPython version: 3.8.5\r\nOS: Win 10\r\n\r\n## Problem\r\nSince the upgrade to wandb10.x it started to create symlinks. This is inconvenient for Windows users, as requires to run my IDE (VS Code) with administrator privileges -- few more clicks to do so.  \r\n\r\n## Traceback\r\n```\r\nwandb: WARNING Symlinked 0 file into the W&B run directory, call wandb.save again to sync new files.\r\nTraceback (most recent call last):\r\n  File \"c:/Users/Asus/Desktop/Garment-Pattern-Estimation/nn/train.py\", line 129, in <module>\r\n    trainer.fit(model)  # Magic happens here\r\n  File \"c:\\Users\\Asus\\Desktop\\Garment-Pattern-Estimation\\nn\\trainer.py\", line 88, in fit\r\n    start_epoch = self._start_experiment(model)\r\n  File \"c:\\Users\\Asus\\Desktop\\Garment-Pattern-Estimation\\nn\\trainer.py\", line 154, in _start_experiment\r\n    self.experiment.init_run(self.setup)\r\n  File \"c:\\Users\\Asus\\Desktop\\Garment-Pattern-Estimation\\nn\\experiment.py\", line 48, in init_run\r\n    wb.save(os.path.join(wb.run.dir, '*.json'))\r\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\Garments\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 919, in save\r\n    os.symlink(abs_path, wandb_path)\r\nOSError: [WinError 1314] A required privilege is not held by the client: 'C:\\\\Users\\\\Asus\\\\Desktop\\\\Garment-Pattern-Estimation\\\\wandb\\\\run-20201016_172608-2v7x9f8u\\\\files\\\\wandb-metadata.json' -> 'wandb\\\\run-20201016_172608-2v7x9f8u\\\\files\\\\wandb\\\\run-20201016_172608-2v7x9f8u\\\\files\\\\wandb-metadata.json'\r\n```\r\n\r\n## Additional comment\r\n\r\nI'm not entirely sure, but I have a suspicion that those symlinks are related to another issue -- there is a strange subdirectory in the run directory files with a path like this `MyProject/wandb/<rundirname>/files/wandb/<rundirname>/files` (duplicate path??). This is also visible in the traceback final paths.",
    "comments": [
      {
        "user": "issue-label-bot[bot]",
        "body": "Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.91. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/wandb/client) and [code](https://github.com/hamelsmu/MLapp) for this bot."
      },
      {
        "user": "vanpelt",
        "body": "A quick fix is to write the file a file directly into `wandb.run.dir` and then call save which will bypass the symlink.  In a future release we'll switch to copying instead of symlinking."
      },
      {
        "user": "mohamedr002",
        "body": "I have a similar problem can you please clarify more about this fix of wandb.run.dir\r\n"
      }
    ]
  },
  {
    "issue_number": 10025,
    "title": "[Bug-App]: wandb: network error (connecttimeout), entering retry loop.",
    "author": "Delin-Ouyang",
    "state": "open",
    "created_at": "2025-06-12T09:28:06Z",
    "updated_at": "2025-06-13T15:28:57Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\nI tried to run wandb online but it report \"wandb: network error (connecttimeout), entering retry loop.\"\nHere are the logs:\n\n2025-06-12 17:12:43,788 INFO    MainThread:33179 [wandb_setup.py:_flush():79] Current SDK version is 0.18.5\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_setup.py:_flush():79] Configure stats pid to 33179\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_setup.py:_flush():79] Loading settings from /home/oydl/.config/wandb/settings\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_setup.py:_flush():79] Loading settings from /home1/OYDL/gpudrive/wandb/settings\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': 'online', '_disable_service': None}\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': 'baselines/ppo/ppo_pufferlib.py', 'program_abspath': '/home1/OYDL/gpudrive/baselines/ppo/ppo_pufferlib.py', 'program': '/home1/OYDL/gpudrive/baselines/ppo/ppo_pufferlib.py'}\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_setup.py:_flush():79] Applying login settings: {}\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_init.py:_log_setup():534] Logging user logs to /home1/OYDL/gpudrive/wandb/run-20250612_171243-PPO__C__S_100__06_12_17_11_52_027/logs/debug.log\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_init.py:_log_setup():535] Logging internal logs to /home1/OYDL/gpudrive/wandb/run-20250612_171243-PPO__C__S_100__06_12_17_11_52_027/logs/debug-internal.log\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_init.py:init():621] calling init triggers\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_init.py:init():628] wandb.init called with sweep_config: {}\nconfig: {'environment': {'name': 'gpudrive', 'num_worlds': 100, 'k_unique_scenes': 100, 'max_controlled_agents': 64, 'ego_state': True, 'road_map_obs': True, 'partner_obs': True, 'norm_obs': True, 'remove_non_vehicles': True, 'lidar_obs': False, 'reward_type': 'weighted_combination', 'collision_weight': -0.75, 'off_road_weight': -0.75, 'goal_achieved_weight': 1.0, 'dynamics_model': 'classic', 'collision_behavior': 'ignore', 'dist_to_goal_threshold': 2.0, 'polyline_reduction_threshold': 0.1, 'sampling_seed': 42, 'obs_radius': 50.0, 'action_space_steer_disc': 13, 'action_space_accel_disc': 7, 'use_vbd': False, 'vbd_model_path': 'gpudrive/integrations/vbd/weights/epoch=18.ckpt', 'init_steps': 0, 'vbd_trajectory_weight': 0.1, 'vbd_in_obs': False, 'dataset_size': '100'}, 'train': {'exp_id': 'PPO__C__S_100__06_12_17_11_52_027', 'seed': 42, 'cpu_offload': False, 'device': 'cuda', 'bptt_horizon': 1, 'compile': False, 'compile_mode': 'reduce-overhead', 'resample_scenes': False, 'resample_dataset_size': 10000, 'resample_interval': 2000000, 'sample_with_replacement': True, 'shuffle_dataset': False, 'torch_deterministic': False, 'total_timesteps': 1000000, 'batch_size': 131072, 'minibatch_size': 8192, 'learning_rate': '3e-4', 'anneal_lr': False, 'gamma': 0.99, 'gae_lambda': 0.95, 'update_epochs': 4, 'norm_adv': True, 'clip_coef': 0.2, 'clip_vloss': False, 'vf_clip_coef': 0.2, 'ent_coef': 0.0001, 'vf_coef': 0.3, 'max_grad_norm': 0.5, 'target_kl': None, 'log_window': 1000, 'network': Box({'input_dim': 64, 'hidden_dim': 128, 'dropout': 0.01, 'class_name': 'NeuralNet', 'num_parameters': 51228}), 'checkpoint_interval': 400, 'checkpoint_path': './runs', 'render': False, 'render_3d': True, 'render_interval': 1, 'render_k_scenarios': 10, 'render_format': 'mp4', 'render_fps': 15, 'zoom_radius': 50, 'env': 'gpudrive'}, 'vec': {'backend': 'native', 'num_workers': 1, 'env_batch_size': 1, 'zero_copy': False}}\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_init.py:init():671] starting backend\n2025-06-12 17:12:43,789 INFO    MainThread:33179 [wandb_init.py:init():675] sending inform_init request\n2025-06-12 17:12:43,790 INFO    MainThread:33179 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=fork,spawn,forkserver, using: spawn\n2025-06-12 17:12:43,791 INFO    MainThread:33179 [wandb_init.py:init():688] backend started and connected\n2025-06-12 17:12:43,794 INFO    MainThread:33179 [wandb_init.py:init():783] updated telemetry\n2025-06-12 17:12:43,800 INFO    MainThread:33179 [wandb_init.py:init():816] communicating run to backend with 120.0 second timeout\n2025-06-12 17:13:42,379 INFO    Thread-1 (wrapped_target):33179 [retry.py:__call__():172] Retry attempt failed:\nTraceback (most recent call last):\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/urllib3/connection.py\", line 198, in _new_conn\n    sock = connection.create_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n    raise err\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n    sock.connect(sa)\nTimeoutError: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 488, in _make_request\n    raise new_e\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 464, in _make_request\n    self._validate_conn(conn)\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 1093, in _validate_conn\n    conn.connect()\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/urllib3/connection.py\", line 704, in connect\n    self.sock = sock = self._new_conn()\n                       ^^^^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/urllib3/connection.py\", line 207, in _new_conn\n    raise ConnectTimeoutError(\nurllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x7f546440e2d0>, 'Connection to api.wandb.ai timed out. (connect timeout=20)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/urllib3/connectionpool.py\", line 841, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/urllib3/util/retry.py\", line 519, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: /graphql (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f546440e2d0>, 'Connection to api.wandb.ai timed out. (connect timeout=20)'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/wandb/sdk/lib/retry.py\", line 131, in __call__\n    result = self._call_fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/wandb/sdk/internal/internal_api.py\", line 354, in execute\n    return self.client.execute(*args, **kwargs)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/wandb/sdk/lib/gql_request.py\", line 58, in execute\n    request = self.session.post(self.url, **post_args)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/requests/sessions.py\", line 637, in post\n    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/requests/adapters.py\", line 688, in send\n    raise ConnectTimeout(e, request=request)\nrequests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: /graphql (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f546440e2d0>, 'Connection to api.wandb.ai timed out. (connect timeout=20)'))\n2025-06-12 17:14:43,833 ERROR   MainThread:33179 [wandb_init.py:init():842] encountered error: Run initialization has timed out after 120.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-\n2025-06-12 17:14:43,833 ERROR   MainThread:33179 [wandb_init.py:init():1266] error in wandb.init()\nTraceback (most recent call last):\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/wandb/sdk/wandb_init.py\", line 1256, in init\n    return wi.init()\n           ^^^^^^^^^\n  File \"/home/oydl/.pyenv/versions/gpudrive/lib/python3.11/site-packages/wandb/sdk/wandb_init.py\", line 848, in init\n    raise error\nwandb.errors.errors.CommError: Run initialization has timed out after 120.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-\n2025-06-12 17:14:44,423 WARNING MsgRouterThr:33179 [router.py:message_loop():77] message_loop has been closed\n\n\n{\"time\":\"2025-06-12T17:12:43.792817552+08:00\",\"level\":\"INFO\",\"msg\":\"using version\",\"core version\":\"0.18.5\"}\n{\"time\":\"2025-06-12T17:12:43.792841172+08:00\",\"level\":\"INFO\",\"msg\":\"created symlink\",\"path\":\"/home1/OYDL/gpudrive/wandb/run-20250612_171243-PPO__C__S_100__06_12_17_11_52_027/logs/debug-core.log\"}\n{\"time\":\"2025-06-12T17:12:43.899845609+08:00\",\"level\":\"INFO\",\"msg\":\"created new stream\",\"id\":\"PPO__C__S_100__06_12_17_11_52_027\"}\n{\"time\":\"2025-06-12T17:12:43.8999069+08:00\",\"level\":\"INFO\",\"msg\":\"stream: started\",\"id\":\"PPO__C__S_100__06_12_17_11_52_027\"}\n{\"time\":\"2025-06-12T17:12:43.900036173+08:00\",\"level\":\"INFO\",\"msg\":\"sender: started\",\"stream_id\":\"PPO__C__S_100__06_12_17_11_52_027\"}\n{\"time\":\"2025-06-12T17:12:43.899960941+08:00\",\"level\":\"INFO\",\"msg\":\"writer: Do: started\",\"stream_id\":{\"value\":\"PPO__C__S_100__06_12_17_11_52_027\"}}\n{\"time\":\"2025-06-12T17:12:43.899964641+08:00\",\"level\":\"INFO\",\"msg\":\"handler: started\",\"stream_id\":{\"value\":\"PPO__C__S_100__06_12_17_11_52_027\"}}\n{\"time\":\"2025-06-12T17:13:13.903195711+08:00\",\"level\":\"INFO\",\"msg\":\"api: retrying error\",\"error\":\"Post \\\"https://api.wandb.ai/graphql\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"}\n{\"time\":\"2025-06-12T17:13:46.151674407+08:00\",\"level\":\"INFO\",\"msg\":\"api: retrying error\",\"error\":\"Post \\\"https://api.wandb.ai/graphql\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"}\n{\"time\":\"2025-06-12T17:14:20.410541683+08:00\",\"level\":\"INFO\",\"msg\":\"api: retrying error\",\"error\":\"Post \\\"https://api.wandb.ai/graphql\\\": context deadline exceeded\"}\n{\"time\":\"2025-06-12T17:14:44.423007516+08:00\",\"level\":\"INFO\",\"msg\":\"stream: closing\",\"id\":\"PPO__C__S_100__06_12_17_11_52_027\"}\n{\"time\":\"2025-06-12T17:14:44.423270347+08:00\",\"level\":\"INFO\",\"msg\":\"writer: Close: closed\",\"stream_id\":{\"value\":\"PPO__C__S_100__06_12_17_11_52_027\"}}\n{\"time\":\"2025-06-12T17:14:44.423231656+08:00\",\"level\":\"INFO\",\"msg\":\"handler: closed\",\"stream_id\":{\"value\":\"PPO__C__S_100__06_12_17_11_52_027\"}}\n{\"time\":\"2025-06-12T17:14:44.423299639+08:00\",\"level\":\"ERROR\",\"msg\":\"sender: upsertRun:\",\"error\":\"failed to upsert bucket: api: failed sending: POST https://api.wandb.ai/graphql giving up after 4 attempt(s): context canceled\"}\n{\"time\":\"2025-06-12T17:14:44.423410893+08:00\",\"level\":\"ERROR\",\"msg\":\"runwork: ignoring record after close\",\"work\":{\"Record\":{\"RecordType\":{\"Request\":{\"RequestType\":{\"Defer\":{}}}},\"control\":{\"always_send\":true}}}}\n{\"time\":\"2025-06-12T17:14:44.424442048+08:00\",\"level\":\"INFO\",\"msg\":\"sender: closed\",\"stream_id\":\"PPO__C__S_100__06_12_17_11_52_027\"}\n{\"time\":\"2025-06-12T17:14:44.424461219+08:00\",\"level\":\"INFO\",\"msg\":\"stream: closed\",\"id\":\"PPO__C__S_100__06_12_17_11_52_027\"}\n\n\n\n\nThen I tried to run offline and used \"wandb sync\" to upload. But the same network problem occured. I also tried to switch to another network or other machine, or upgrade the version of wandb. All the tries failed. Could you give me some help?\n",
    "comments": [
      {
        "user": "Delin-Ouyang",
        "body": "Or could you please tell me how to visualize the offline data as shown online? Thanks!"
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi @Delin-Ouyang, we did run into an outage last night because of the problems GCP was experiencing. Are you still seeing this on your side?\n\nAre you trying to run sweeps in the offline mode?"
      }
    ]
  },
  {
    "issue_number": 10027,
    "title": "[Q]: Network error although setted to offline?",
    "author": "dustlightt",
    "state": "open",
    "created_at": "2025-06-12T16:01:18Z",
    "updated_at": "2025-06-13T15:11:48Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\n<!--- Ask your question here --->\nWhy prompting for a network connection failure when set to offline?\n\nwandb: Tracking run with wandb version 0.19.1\nwandb: W&B syncing is set to `offline` in this directory.  \nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\nwandb: Network error (ConnectTimeout), entering retry loop.\n\nI haven't been able to connect to the server for a long time, but I was able to a month ago.",
    "comments": [
      {
        "user": "dustlightt",
        "body": "Probably because wandb.sweep requires a connection to the server"
      },
      {
        "user": "dustlightt",
        "body": "sry, I've given up on using wandb and don't have the details to share"
      },
      {
        "user": "ArtsiomWB",
        "body": "Hey @dustlightt! You are correct, you do need the internet connection to run your sweeps, and sweeps in offline modeare currently not available"
      }
    ]
  },
  {
    "issue_number": 8834,
    "title": "[Feature]: Set custom artifact name in WandbModelCheckpoint",
    "author": "GabrielGosden",
    "state": "open",
    "created_at": "2024-11-12T14:45:14Z",
    "updated_at": "2025-06-13T09:37:38Z",
    "labels": [
      "ty:feature",
      "c:artifacts"
    ],
    "body": "### Description\n\n<!--- Describe your feature here --->\n\nHi,\n\nI've noticed that when using [WandbModelCheckpoint's](https://docs.wandb.ai/ref/python/integrations/keras/wandbmodelcheckpoint/) it's currently not possible to define a custom name for the model artifact. The name is automatically set to  `run_{wandb.run.id}_model` as specified in the [_log_ckpt_as_artifact](https://github.com/wandb/wandb/blob/6bc2440183e02633c11fdd291550c5eb9c0b4634/wandb/integration/keras/callbacks/model_checkpoint.py#L154) function.\n\nThis presents a challenge when saving models for multiple metrics within a single run. For instance, consider having a WandbModelCheckpoint for different metrics such as loss, recall, precision, F1 score, IoU, Dice, etc. Since the artifact name remains the same across all metrics, the model versioning becomes ineffective. This results in every model for a specific metric being stored under the same artifact name, leading to many versions that aren't metric-specific.\n\nConsequently, identifying the best-performing model for a particular metric can become quite cumbersome. It often requires sifting through log files or exploring the latest model versions.\n\nHere's an example of what the end result looks like:\n![Image](https://github.com/user-attachments/assets/b539506d-028a-4e6a-9cb3-198570423d47)\n\n\n\n\n\n\n\n### Suggested Solution\n\n<!--- Describe your solution here --->\n\nI would like to be able to define the models artifact name when specifying the callbacks for [WandbModelCheckpoint's](https://docs.wandb.ai/ref/python/integrations/keras/wandbmodelcheckpoint/).\n\n\n\n\n\n\n\n",
    "comments": [
      {
        "user": "GabrielGosden",
        "body": "Slightly related issues:\n\nhttps://github.com/wandb/wandb/issues/7413\nhttps://github.com/wandb/wandb/issues/6278\n\nPeople are having the same issue with `run.log_artifact`"
      },
      {
        "user": "paulosabile-wb",
        "body": "Hey @GabrielGosden Good day and thank you for writing in!\n\nHappy to review this for you! I would like to know the context for this so we can review and identify the next steps to suggest. Would you mind sharing a code snippet so we can review how you are currently executing your callbacks for `WandbModelCheckpoint`? We will then analyze your current setup and discuss this with the team.\n\nIt would also help if you can share your current SDK version. You can get this by running `wandb --version`. Thanks!"
      },
      {
        "user": "GabrielGosden",
        "body": "Hey @paulosabile-wb,\n\nI have created a minimal example using `wandb==0.19.0` \n\nThis minimal example creates a `WandbModelCheckpoint` for `loss`,  `val_loss`, `accuracy` and `val_accuracy`.  This could be recall, precision, F1 score, IoU, Dice, etc.\n\n```\nimport wandb\nfrom wandb.integration.keras import WandbModelCheckpoint\nimport os \nimport numpy as np\nimport tensorflow as tf\n\nwandb.init(project=\"feature_request_model_checkpoint\")\n\n# Create a simple model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(10, input_dim=10, activation='relu'),\n    tf.keras.layers.Dense(1, activation='linear')\n])\n\n# Compile arbitrary model.\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Define multiple WandbModelCheckpoint callbacks\ncallbacks = [\n    WandbModelCheckpoint(\n        os.path.join(\"models/\", wandb.run.name + \"_min_loss.keras\"),\n        monitor=\"loss\",\n        verbose=1,\n        mode=\"min\",\n        save_best_only=False,\n        save_weights_only=False,\n    ),\n    WandbModelCheckpoint(\n        os.path.join(\"models/\", wandb.run.name + \"_min_val_loss.keras\"),\n        monitor=\"val_loss\",\n        verbose=1,\n        mode=\"min\",\n        save_best_only=False,\n        save_weights_only=False,\n    ),\n    WandbModelCheckpoint(\n        os.path.join(\"models/\", wandb.run.name + \"max_accuracy.keras\"),\n        monitor=\"accuracy\",\n        verbose=1,\n        mode=\"max\",\n        save_best_only=False,\n        save_weights_only=False,\n    ),\n    WandbModelCheckpoint(\n        os.path.join(\"models/\", wandb.run.name + \"max_val_accuracy.keras\"),\n        monitor=\"val_accuracy\",\n        verbose=1,\n        mode=\"min\",\n        save_best_only=False,\n        save_weights_only=False,\n    )\n]\n\n# Train the model\nmodel.fit(\n    x=np.random.rand(100, 10),\n    y=np.random.rand(100, 1),\n    epochs=100,\n    validation_data=(np.random.rand(20, 10), np.random.rand(20, 1)),\n    callbacks=callbacks\n)\n```\nThis creates >100 model artifacts all with the same name. Finding the best performing model on a given metric is now quite difficult.\n\n![Image](https://github.com/user-attachments/assets/7cef8284-d943-4f22-bc2d-5b007b94d773)\n\n"
      }
    ]
  },
  {
    "issue_number": 10017,
    "title": "[Bug-App]: Metric not visible in table view",
    "author": "raphaelschwinger",
    "state": "open",
    "created_at": "2025-06-11T09:25:47Z",
    "updated_at": "2025-06-13T08:20:05Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\n## Current Behavior\nWhen I search a specific metric (that is being logged) in the 'Manage Columns' dialog box in the web app Table view, the metric does not show up. \n\nIn my case, there is a \"similar metric\" `test/cmap` instead of `test/cmAP` showing up, the value is `null` instead of the expected `0.47466`.\n\n## Expected Behavior\nMetrics and configs matching the search term should appear in the search boxes.\n\n## Screenshots\n\n<img width=\"951\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/c8063369-4b6b-4530-b06d-7d3741c3f0c1\" />\n\n<img width=\"862\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/3562e6f3-e41a-4500-b670-38a5b2f815dd\" />\n",
    "comments": [
      {
        "user": "paulosabile-wb",
        "body": "Hi @raphaelschwinger Good day and thank you for reporting this behavior to us. Happy to review this for you.\n\nCould you please share the project URL where you are seeing this behavior so I can further investigate? Thanks!\n"
      },
      {
        "user": "raphaelschwinger",
        "body": "@paulosabile-wb \nhttps://wandb.ai/deepbirddetect/BioFoundation\nAlso visible in reports: https://wandb.ai/deepbirddetect/BioFoundation/reports/Birdset-Experiments--VmlldzoxMTQ5MTAwNw"
      }
    ]
  },
  {
    "issue_number": 3098,
    "title": "[Q]Sync offline metrics on another machine",
    "author": "THU-syh",
    "state": "open",
    "created_at": "2022-01-03T13:13:56Z",
    "updated_at": "2025-06-12T21:18:38Z",
    "labels": [
      "c:sdk:sync",
      "c:artifacts"
    ],
    "body": "If I train on a gpu machine that cannot be connected to the Internet and use offline wandb to record metrics, can I move the generated offline folder to another machine and synchronize it to the cloud? Which files must I save and move?",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Leslie commented: \nHi! You can use `wandb sync <run_path>` on an offline run on another machine\n"
      },
      {
        "user": "THU-syh",
        "body": "> Leslie commented: Hi! You can use `wandb sync <run_path>` on an offline run on another machine\r\n\r\nI tried to do this, but got this error：\r\n`wandb: ERROR Error uploading \"***/.cache/wandb/artifacts/obj/md5/ee/cd3b3cf65f625269a2b7377b89***\": FileNotFoundError,`\r\n`wandb: ERROR Uploading artifact file failed. Artifact won't be committed.`\r\nDoes it seem that I still need to move some files to another machine?"
      },
      {
        "user": "vanpelt",
        "body": "@THU-syh because you logged an artifact in the original run, we must sync from the same machine where that artifact existed.  Were the rest of the metrics synced or did this error cause the entire sync process to crash?  If it crashed, it's a bug and we should handle this error in a future release."
      }
    ]
  },
  {
    "issue_number": 8458,
    "title": "[Feature]: API: Fetching Config Files For Multiple Runs",
    "author": "MoH-assan",
    "state": "open",
    "created_at": "2024-09-21T13:44:12Z",
    "updated_at": "2025-06-12T18:44:42Z",
    "labels": [
      "ty:feature",
      "c:sdk:public-api",
      "a:sdk"
    ],
    "body": "### Description\r\n\r\n<!--- Describe your feature here --->\r\n**Objective:** A way to get the configuration of all runs in a project into a data frame through the API. \r\n\r\nI guess one way is \r\n\r\n\r\n```\r\nruns = api.runs(path=\"entiry/project\")\r\nconfigs = []\r\nfor run in tqdm(runs):\r\n    run_config = run.config\r\n    run_config_df = pd.DataFrame.from_dict(run_config, orient='index').T\r\n    configs.append(run_config_df)\r\nconfig_df = pd.concat(configs,axis=0)\r\n```\r\n\r\nThis is a bit bulky and slow for large projects.\r\n\r\n### Suggested Solution\r\n\r\n<!--- Describe your solution here --->\r\nIt would be interesting to have something like this \r\n\r\n`configs = runs.config`\r\n\r\nMimicking fetching the histories from multiple runs as offered in the API method below. https://github.com/wandb/wandb/blob/defd0f24114c23d5b1d9c34f342ccca096ccffe3/wandb/apis/public/runs.py#L170-L266",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bonnie Shen commented: \nHello @MoH-assan:\nI'm happy to file a feature request for this! Thank you for the suggestion!\n"
      },
      {
        "user": "simonMoisselin",
        "body": "same !"
      }
    ]
  },
  {
    "issue_number": 10026,
    "title": "[Bug]: `TorchGraph` incompatible with nested tensors",
    "author": "PierreGtch",
    "state": "open",
    "created_at": "2025-06-12T12:11:55Z",
    "updated_at": "2025-06-12T14:45:24Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n`TorchGraph` is not compatible with models that have [nested tensors](https://docs.pytorch.org/docs/stable/nested.html) as intermediate outputs. \nSo `wandb.watch(..., log_graph=True)` fails with such models. See minimal example:\n\n\n## Minimal example\n```python\nimport wandb\nimport torch\nfrom torch import nn\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = nn.Linear(10, 10)\n\n    def forward(self, x):\n        x = self.layer(x)\n        return x.sum()\nmodel = MyModel()\n\nx = torch.nested.nested_tensor(\n    [torch.randn(4, 10), torch.randn(5, 10)], layout=torch.jagged,\n    )\nprint(f\"{x.shape=}\\n{type(x.shape[1])=}\")\n\nwandb.init()\nwandb.watch(model, log_graph=True, log_freq=1)\n\nloss = model(x)\nwandb.log({\"loss\": loss})\n```\n\n## Result\n```\nx.shape=torch.Size([2, j1, 10])\ntype(x.shape[1])=<class 'torch.SymInt'>\nwandb: Currently logged in as: pierregtch. Use `wandb login --relogin` to force relogin\nwandb: wandb version 0.20.1 is available!  To upgrade, please run:\nwandb:  $ pip install wandb --upgrade\nwandb: Tracking run with wandb version 0.15.12\nwandb: Run data is saved locally in /Users/Pierre.Guetschel/Projects/chan_inv_clf/wandb/run-20250612_140942-u6sw2g9z\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run super-monkey-1\nwandb: ⭐️ View project at https://wandb.ai/pierregtch/chan_inv_clf\nwandb: 🚀 View run at https://wandb.ai/pierregtch/chan_inv_clf/runs/u6sw2g9z\nwandb: logging graph, to disable use `wandb.watch(log_graph=False)`\nTraceback (most recent call last):\n  File \"/Users/Pierre.Guetschel/Desktop/report_wandb_nested.py\", line 24, in <module>\n    loss = model(x)\n           ^^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/Pierre.Guetschel/Desktop/report_wandb_nested.py\", line 11, in forward\n    x = self.layer(x)\n        ^^^^^^^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n    return inner()\n           ^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1818, in inner\n    hook_result = hook(self, args, result)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/wandb_torch.py\", line 349, in after_forward_hook\n    wandb.run.summary[\"graph_%i\" % graph_idx] = self\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/sdk/wandb_summary.py\", line 52, in __setitem__\n    self.update({key: val})\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/sdk/wandb_summary.py\", line 74, in update\n    self._update(record)\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/sdk/wandb_summary.py\", line 128, in _update\n    self._update_callback(record)\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 370, in wrapper_fn\n    return func(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/sdk/wandb_run.py\", line 1370, in _summary_update_callback\n    self._backend.interface.publish_summary(summary_record)\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/sdk/interface/interface.py\", line 258, in publish_summary\n    pb_summary_record = self._make_summary(summary_record)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/sdk/interface/interface.py\", line 236, in _make_summary\n    json_value = self._summary_encode(item.value, path_from_root)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/sdk/interface/interface.py\", line 209, in _summary_encode\n    val_to_json(self._run, path_from_root, value, namespace=\"summary\")\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/sdk/data_types/utils.py\", line 164, in val_to_json\n    val.bind_to_run(run, key, namespace)\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/data_types.py\", line 1448, in bind_to_run\n    util.json_dump_safer(data, fp)\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/util.py\", line 823, in json_dump_safer\n    return dump(obj, fp, cls=WandBJSONEncoder, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/json/__init__.py\", line 179, in dump\n    for chunk in iterable:\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/json/encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/json/encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/json/encoder.py\", line 326, in _iterencode_list\n    yield from chunks\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/json/encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/json/encoder.py\", line 326, in _iterencode_list\n    yield from chunks\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/json/encoder.py\", line 326, in _iterencode_list\n    yield from chunks\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/json/encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/site-packages/wandb/util.py\", line 779, in default\n    return json.JSONEncoder.default(self, obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/Pierre.Guetschel/miniforge3/envs/chan_inv_clf/lib/python3.11/json/encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type SymInt is not JSON serializable\nwandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.\nwandb: 🚀 View run super-monkey-1 at: https://wandb.ai/pierregtch/chan_inv_clf/runs/u6sw2g9z\nwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nwandb: Find logs at: ./wandb/run-20250612_140942-u6sw2g9z/logs\n```",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hi @PierreGtch! Thank you for reporting this and the reproduction. I just sent this bug report to our engineering team. "
      }
    ]
  },
  {
    "issue_number": 5009,
    "title": "[App]: Can't log table - won't show up in Run Tables",
    "author": "michaelabuchanan",
    "state": "closed",
    "created_at": "2023-02-21T21:08:40Z",
    "updated_at": "2025-06-12T14:42:32Z",
    "labels": [
      "a:app",
      "c:table"
    ],
    "body": "### Current Behavior\n\nI am trying to log a dataset from a Jupyter notebook as a Table. When I run the code it creates a Table in my Workspace but says \"No rows to display\" instead of showing the data I tried to log. However, I can tell that the data is passed to WandB because if I navigate to Files/media/table and download the `.json` file located there I can see the data I am trying to pass in the `.json` file. To make sure it wasn't my code causing issues I tried running this sample notebook https://colab.research.google.com/github/wandb/examples/blob/master/colabs/tables/W%26B_Tables_Quickstart.ipynb and still was not able to see the table populated for the runs I did.\r\n![tableissues](https://user-images.githubusercontent.com/32206926/220457687-9379ec84-c844-417f-885a-3e4b16839505.PNG)\r\n\n\n### Expected Behavior\n\nI am expecting to see the Iris dataset from the example notebook linked above populated into the Table created by running this code.\n\n### Steps To Reproduce\n\n1. Run the notebook linked in Current Behavior\r\n2. Check if the table is created as expected in the WandB web application\n\n### Screenshots\n\n.json file found under Files:\r\n![jsonfile](https://user-images.githubusercontent.com/32206926/220458501-08eea77e-ee40-47a2-858c-efd1b8ede284.PNG)\r\n\r\nScreenshot of notebook used:\r\n![notebook](https://user-images.githubusercontent.com/32206926/220458704-078ba480-b81d-4fd8-9073-a1ea7f8f41c5.PNG)\r\n\n\n### Environment\n\nOS: Windows 10\r\n\r\nBrowsers: Google Chrome\r\n\r\nVersion: 0.13.10\r\n\n\n### Additional Context\n\nI am using Weights and Biases through Run.AI so perhaps something about this setup is causing this issue. Normal Keras training logging is working as expected. So far it's only the Tables that I have seen not work.",
    "comments": [
      {
        "user": "MBakirWB",
        "body": "Hi @michaelabuchanan, in the screenshot provided the dataframe is not being logged correctly to a wandb table. The call must be `wandb.log({\"iris\":  wandb.Table(dataframe=iris_dataframe})`\r\n\r\nYou can test the correct behavior via the following. table 1 will not appear, but table 2 will. \r\n\r\n```\r\nrun1 = wandb.init(project=\"test-plot-table\")\r\nids = np.arange(10)\r\n\r\ndf1 = pd.DataFrame({\"id\": ids, \"X\": np.random.randint(0, 5, size=10)})\r\ntable1 = run1.log({\"table1\": df1})\r\n\r\ndf2 = pd.DataFrame({\"id\": ids, \"Y\": np.random.randint(0, 5, size=10)})\r\ntable2 = run1.log({\"table2\": wandb.Table(dataframe=df2)})\r\nwandb.finish()\r\n```\r\n"
      },
      {
        "user": "michaelabuchanan",
        "body": "Thank you for the quick reply! I tried out both changing the code as mentioned and plugging in the sample code you provided and still don't see output from any of the tables. I can once again see the data in the .json files though..."
      },
      {
        "user": "MBakirWB",
        "body": "Hi @michaelabuchanan , I accessed the latest version of your jupyter notebook and successfully logged the tables, see [here](https://wandb.ai/mohammadbakir/Tables-Quickstart/artifacts/code/source-Tables-Quickstart-None/v0/files/_session_history.ipynb). I noticed you are using a private wandb instance, could you run `wandb verify` from your terminal to check the state of your installation. Additionally, when you could, provide me a screen shot of your browser console from from the main page where you are not seeing the data. Thanks"
      }
    ]
  },
  {
    "issue_number": 9949,
    "title": "[Bug-App]: System metrics graphs change when refreshing",
    "author": "spfrommer",
    "state": "open",
    "created_at": "2025-06-02T00:35:28Z",
    "updated_at": "2025-06-12T13:55:14Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\nhttps://github.com/user-attachments/assets/554cb7f4-8288-4c22-93fb-bedba13c329c\n\nBrowser: Chrome 136.0.7103.114 (Official Build) (arm64) \nOS: macOS 15.5 (24F74)\n\nIssue is pretty clear from the above video. I full-screen the metrics graph, and it changes. Exit full screen, and it changes again. This happens for all system metrics graphs, and I haven't noticed it for other non-system graphs. I have my point aggregation method set to \"full fidelity\" so there shouldn't be any random sampling going on as far as I'm aware.",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @spfrommer, thanks for flagging this! Could you share a link to an example plot where you’re seeing the issue? I’ll also try to reproduce it on my end in the meantime."
      },
      {
        "user": "spfrommer",
        "body": "@aajais For example: https://wandb.ai/spfrom_team/flowmatchrl/runs/xsgqnf6m"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Aman Atman commented: \nHi, thanks for reporting this—our engineering team is already looking into it. We’ll update this thread as soon as we have a fix!\n"
      }
    ]
  },
  {
    "issue_number": 9924,
    "title": "[Bug]: Sweeps not working together with PyTorch under Python 3.13",
    "author": "Leon0402",
    "state": "open",
    "created_at": "2025-05-28T22:52:58Z",
    "updated_at": "2025-06-12T13:27:39Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "It seem that importing pytorch break wandb sweeps. \n\nSmall example\n```\n# train.py\nimport random\nimport time\n\n# import torch\nimport tqdm\n\nimport wandb\n\n\ndef train_one_epoch(epoch, lr, bs):\n    acc = 0.25 + ((epoch / 30) + (random.random() / 10))\n    loss = 0.2 + (1 - ((epoch - 1) / 10 + random.random() / 5))\n    return acc, loss\n\n\ndef evaluate_one_epoch(epoch):\n    acc = 0.1 + ((epoch / 20) + (random.random() / 10))\n    loss = 0.25 + (1 - ((epoch - 1) / 10 + random.random() / 6))\n    return acc, loss\n\n\ndef main():\n    wandb.init()\n\n    lr = wandb.config.lr\n    bs = wandb.config.batch_size\n    epochs = wandb.config.epochs\n\n    for epoch in tqdm.tqdm(range(epochs)):\n        train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n        val_acc, val_loss = evaluate_one_epoch(epoch)\n        time.sleep(1)\n\n        wandb.log(\n            {\n                \"epoch\": epoch,\n                \"train_acc\": train_acc,\n                \"train_loss\": train_loss,\n                \"val_acc\": val_acc,\n                \"val_loss\": val_loss,\n            }\n        )\n\n\nmain()\n```\n\n```\n# config.yaml\nprogram: train.py\nmethod: random\nname: sweep\nmetric:\n  goal: maximize\n  name: val_acc\nparameters:\n  batch_size:\n    values: [16, 32, 64]\n  lr:\n    min: 0.0001\n    max: 0.1\n  epochs:\n    values: [5, 10, 15]\n```\n\nStart sweep:\n wandb sweep --project sweep-demo-cli config.yaml\n\nAnd then start agent:\nwandb agent <id>\n\n\nObserve that it is working:\n```\nwandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\nwandb: Starting wandb agent 🕵️\n2025-05-28 22:47:16,585 - wandb.wandb_agent - INFO - Running runs: []\n2025-05-28 22:47:16,837 - wandb.wandb_agent - INFO - Agent received command: run\n2025-05-28 22:47:16,838 - wandb.wandb_agent - INFO - Agent starting run with config:\n        batch_size: 32\n        epochs: 15\n        lr: 0.01778725577107571\n2025-05-28 22:47:16,840 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --batch_size=32 --epochs=15 --lr=0.01778725577107571\nwandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n<Here all the output of the script>\n...\n```\n\n\nBut if you comment in the pytorch import in the training script, suddently the output is:\n```\nwandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\nwandb: Starting wandb agent 🕵️\n2025-05-28 22:52:16,774 - wandb.wandb_agent - INFO - Running runs: []\n2025-05-28 22:52:17,476 - wandb.wandb_agent - INFO - Agent received command: run\n2025-05-28 22:52:17,476 - wandb.wandb_agent - INFO - Agent starting run with config:\n        batch_size: 16\n        epochs: 15\n        lr: 0.0013364305378862206\n2025-05-28 22:52:17,478 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --batch_size=16 --epochs=15 --lr=0.0013364305378862206\n2025-05-28 22:52:22,491 - wandb.wandb_agent - INFO - Running runs: ['440mqklf']\n```\nAnd it will hang forever without any progress. Not great :(\n\n\n\nEdit: I just found a similar issue here https://github.com/wandb/wandb/issues/9616\n\nBut I am using python 3.13 with pytorch 2.7.0 (from pypi). Have been using this combination for quite some time and never had any issues. So in contrast to the mentioned issue, these versions should work together. Does not work with pytorch 2.6 either.",
    "comments": [
      {
        "user": "paulosabile-wb",
        "body": "Hi @Leon0402 Good day and thanks for reporting this to us. We're sorry to hear that you are experiencing issues with your experiments. Could you please share your current SDK version so I can reproduce this issue and further investigate? You can get this by running `wandb --version`. Thanks!"
      },
      {
        "user": "Leon0402",
        "body": "@paulosabile-wb Happens with 0.19.11, but also earlier version (I had 0.19.5 previously I think). The issue is resolved when downgrading to python 3.12. But obviously that is only a quick fix, not a permanent solution."
      },
      {
        "user": "Leon0402",
        "body": "@paulosabile-wb Hey! Is there any progress on reproducing / investigating the issue? Thanks! :)"
      }
    ]
  },
  {
    "issue_number": 9938,
    "title": "[Q]: Cannot login wandb.",
    "author": "waltonfuture",
    "state": "closed",
    "created_at": "2025-05-30T14:58:04Z",
    "updated_at": "2025-06-12T09:05:30Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\n```\nwandb: WARNING Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\nCurrent Settings\n{\n  \"_extra_http_headers\": null,\n  \"_proxies\": null,\n  \"api_key\": null,\n  \"base_url\": \"https://api.wandb.ai\",\n  \"entity\": null,\n  \"git_remote\": \"origin\",\n  \"ignore_globs\": [],\n  \"organization\": null,\n  \"project\": null,\n  \"root_dir\": null,\n  \"section\": \"default\"\n}\n```\nAfter wandb login and input the key, it still shows that the api_key is null",
    "comments": [
      {
        "user": "shuxiaobo",
        "body": "same problem"
      },
      {
        "user": "Shaofei-Li",
        "body": "same problem\n"
      },
      {
        "user": "Jordinia",
        "body": "same problem here!"
      }
    ]
  },
  {
    "issue_number": 9969,
    "title": "[Bug-App]: Run initialization has timed out after 120.0 sec.",
    "author": "YWJ-Best",
    "state": "closed",
    "created_at": "2025-06-03T06:09:26Z",
    "updated_at": "2025-06-12T04:28:06Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\nI cannot start a project. whenever i call wandb.init, it runs for 90 seconds and then it automatically stops with an error “ commerror: run initialization has timed out after 90.0 sec.”\n\nThen I extended the duration to 120 seconds, but the problem still existed.  At this point, the version of wandb was 0.19.11.  Meanwhile, I also attempted to revert the version of wandb to 0.17.8, but the problem still couldn't be solved.\n\nMoreover, my browser also fails to access wandb. ai properly.  It shows\"\nUm...   This page cannot be accessed\n\nThe response time of wandb. ai is too long\n\"",
    "comments": [
      {
        "user": "YWJ-Best",
        "body": "Also, what I want to say is that this situation didn't exist five days ago. I don't know what happened in these five days. I didn't use wandb, so logically there shouldn't be any changes.\n\nWhat should i do， please.\nv v\n ^ (cry)"
      },
      {
        "user": "bucky527",
        "body": "I also encountered the same problem"
      },
      {
        "user": "YWJ-Best",
        "body": "> 我也遇到了同样的问题\n\nI thought it was a problem with the website, and then many people would encounter it. But when I searched online, I found that no one had sent such a request for help recently"
      }
    ]
  },
  {
    "issue_number": 4812,
    "title": "[Q] Does define_metric allow a summary based on the point in time where another metric performs best?",
    "author": "AndersMollgaard",
    "state": "open",
    "created_at": "2023-01-19T08:39:28Z",
    "updated_at": "2025-06-11T18:53:30Z",
    "labels": [
      "ty:feature",
      "c:sdk:custom-charts",
      "a:sdk"
    ],
    "body": "A fair summary of a model should be specified based on the same checkpoint and not different checkpoints where the individual metrics are performing at their best. \r\n\r\nI can see that define_metric has the summary = 'best' option along with a goal = 'minimize' / 'maximize', but as I understand it, I cannot let the goal be based on another metric. Is that so?\r\n\r\nFor instance, I would like to have a summary including the loss and the accuracy at the point in time, where the accuracy is highest.",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hi @AndersMollgaard! [Is this](https://docs.wandb.ai/guides/track/log#customize-axes-and-summaries-with-define_metric) somewhat similar to what you are looking for? Our docs have an example of how to customize the summary. \r\n\r\nIn the example linked above, `the user is capturing the lowest value of loss and the maximum value of accuracy in the summary, instead of the default summary behavior, which uses the final value from history.`"
      },
      {
        "user": "AndersMollgaard",
        "body": "> Hi @AndersMollgaard! [Is this](https://docs.wandb.ai/guides/track/log#customize-axes-and-summaries-with-define_metric) somewhat similar to what you are looking for? Our docs have an example of how to customize the summary.\r\n\r\nYeah, I was looking at your documentation, but did not find what I was looking for. If I understand it correctly, then the above will create a summary where the values of the metrics in the summary are potentially evaluated at different points in time during the training. \r\n\r\nI could have a model classifying \"dog or cat\" which at one point during training says \"everything is a dog\" and at another point says \"everything is a cat\". My summary would then say: {\"fraction_of_dogs_found\": 100%, \"fraction_of_cats_found\": 100%, \"accuracy\": ...}.\r\n\r\nI would like the summary to contain those two metrics at the point in time where the accuracy was largest. In code it would be something like:\r\n\r\n```\r\nwandb.define_metric(\"accuracy\", summary=\"best\", goal=\"maximize\")\r\nwandb.define_metric(\"fraction_of_dogs_found\", summary=\"value_when_accuracy_best\")\r\nwandb.define_metric(\"fraction_of_cats_found\", summary=\"value_when_accuracy_best\")\r\n```"
      },
      {
        "user": "ArtsiomWB",
        "body": "I have asked around internally and we currently do not have this feature. It does sound like a great feature request I could ticket for you though! I think the workaround for this would be to use Pandas data frame and then log it that way. "
      }
    ]
  },
  {
    "issue_number": 9937,
    "title": "[Bug-App]: WandB Network Error: ProxyError and Re-Login Prompt",
    "author": "zsn2021",
    "state": "closed",
    "created_at": "2025-05-30T13:05:28Z",
    "updated_at": "2025-06-11T16:10:00Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\nHello everyone,\nI’ve recently encountered a network issue while using Weights & Biases (WandB), and I thought I’d share it here to see if anyone else has experienced something similar or can offer some solutions.\nIssue Description\nWhen running WandB, I received the following error message in the console:\nwandb: Network error (ProxyError), entering retry loop.\nwandb: W&B API key is configured. Use `wandb login --relogin` to force relogin.",
    "comments": [
      {
        "user": "Necolizer",
        "body": "Same problem. Any progress?"
      },
      {
        "user": "zhouczcz",
        "body": "Same problem"
      },
      {
        "user": "shuxiaobo",
        "body": "+1, Same problem"
      }
    ]
  },
  {
    "issue_number": 9946,
    "title": "[Q]: wandb: Network error (ConnectTimeout), entering retry loop.",
    "author": "xzy-xzy",
    "state": "closed",
    "created_at": "2025-05-31T06:30:54Z",
    "updated_at": "2025-06-11T16:09:03Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\nThe problem started about 18 hours ago. Does anyone have the same problem? ",
    "comments": [
      {
        "user": "XinyiZhang0724",
        "body": "Yes, I also met this problem yesterday."
      },
      {
        "user": "gxwawgw",
        "body": "me too............"
      },
      {
        "user": "asyu17",
        "body": "Yes, I encountered this problem too, and it still hasn't recovered."
      }
    ]
  },
  {
    "issue_number": 3255,
    "title": "[Q] passing environement variables to sweep command arguments",
    "author": "JADGardner",
    "state": "closed",
    "created_at": "2022-02-18T10:53:35Z",
    "updated_at": "2025-06-10T21:50:17Z",
    "labels": [],
    "body": "I have a sweep .yaml config file that looks something like this:\r\n\r\n```javascript\r\nprogram:\r\n  some_code.py\r\nentity: foo\r\nproject: bar\r\nparameters:\r\n  batch_size:\r\n    value: 30\r\n  lr:\r\n    distribution: uniform\r\n    max: 0.01\r\n    min: 0.0001\r\ncommand:\r\n  - torchrun\r\n  - \"--nproc_per_node\" \r\n  - ${WOLRD_SIZE}\r\n  - \"--nnodes\"\r\n  - 1\r\n  - \"--node_rank\"\r\n  - 0\r\n  - ${program}\r\n```\r\n\r\nI use the 'command' section in the config above to have the sweep agent launch the run using torchrun for multi-gpu training. However I want to be able to specify the \"--nproc_per_node\" argument  via an environment variable WOLRD_SIZE that I set in a bash script such as that below:\r\n\r\n```bash\r\n#! /bin/bash\r\nexport CUDA_VISIBLE_DEVICES=0,1,3,4,6,7\r\n\r\nexport WOLRD_SIZE=6\r\n\r\nNUM=10\r\nENTITY=\"foo\"\r\nPROJECT=\"bar\"\r\nSWEEPID=\"s8shs8f\"\r\n\r\nwandb agent --count $NUM --entity $ENTITY --project $PROJECT $SWEEPID\r\n```\r\n\r\nBut when I do the value of WORLD_SIZE for the \"--nproc_per_node\" argument is always blank as shown in the information about the run printed in the console:\r\n\r\n```bash\r\n2022-02-18 10:24:37,346 - wandb.wandb_agent - INFO - About to run command: torchrun --nproc_per_node --nnodes 1 --node_rank 0 some_code.py\r\n```\r\nIs there a way to pass environment variables to the command arguments?",
    "comments": [
      {
        "user": "vanpelt",
        "body": "@JADGardner unfortunately we don't currently support resolving env variables in the `command` section.  The best workaround I could propose today is to make your sweep config command actually execute another bash script, i.e.\r\n\r\n```yaml\r\ncommand:\r\n  - runner.sh\r\n  - ${program}\r\n```\r\n\r\nThen in a script named runner.sh:\r\n\r\n```bash\r\n#!/bin/bash\r\n\r\ntorchrun --nproc_per_node $WOLRD_SIZE --nnodes 1 --node_rank 0 $1\r\n```\r\n\r\nThe environment that the agent is running in should be passed down to all sub processes."
      },
      {
        "user": "robertavram-md",
        "body": "Ty this was helpful for me."
      },
      {
        "user": "listaction",
        "body": "hi super team! \nwhat sort of shell does wandb agent run (when running) - if we use a bash script for the runner and have things like $(date) or `date` it throws errors when running on Ubuntu 22.04.4 LTS \\n \\l bash shell."
      }
    ]
  },
  {
    "issue_number": 10003,
    "title": "[Q]: What would it take to update the experimental Rust api?",
    "author": "VirxEC",
    "state": "open",
    "created_at": "2025-06-09T00:29:13Z",
    "updated_at": "2025-06-10T18:57:49Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\nHello, having a Rust API for wandb would be perfect for my project that utilizes Burn. The following works: \n```toml\nwandb = { git = \"https://github.com/wandb/wandb.git\", tag=\"v0.18.7\" }\n```\nBut I'm worried about for how long wandb.ai will keep accepting this old version of the API.\n\nWhat needs to be done to update the Rust api? I'm willing to do it, if the needed API updates aren't super complicated and my PR might be accepted.",
    "comments": [
      {
        "user": "aajais",
        "body": "Hey @VirxEC, really appreciate you bringing this up! I’ll go ahead and log a feature request for a Rust API and make sure to keep you in the loop on any updates or movement from our side.\n\nIn the meantime, there’s a community-maintained Rust implementation that could be worth checking out: [NousResearch/wandb-rs](https://github.com/NousResearch/wandb-rs). While it’s not officially supported by us, it might help bridge the gap for your work."
      }
    ]
  },
  {
    "issue_number": 9994,
    "title": "[Bug-App]: Exported PNG hides continuous zero-valued segments (horizontal line at 0 disappears)",
    "author": "songyuc",
    "state": "open",
    "created_at": "2025-06-06T11:10:32Z",
    "updated_at": "2025-06-10T14:31:36Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\nWhen a tracked metric (e.g., `eval/success_once`) remains exactly 0 for many consecutive steps, the WandB web UI correctly displays a thin horizontal line at Y=0. However, clicking **Download PNG** produces an image where that entire zero-valued segment is effectively “invisible” (it is compressed into the bottom of the plot or cropped out entirely), giving the impression that the curve abruptly ends rather than remaining at 0. This can confuse users into thinking their metric has stopped logging or that data is missing.\n\n![Image](https://github.com/user-attachments/assets/7f6c768a-b177-4a55-ac59-c05ae94aca2b)\n\n---\n\n**Steps to Reproduce:**\n\n1. Create a simple WandB run that logs a nonzero value for a while, then logs 0 continuously. For example:\n\n   ```python\n   import wandb\n\n   run = wandb.init(project=\"zero-line-bug-demo\")\n   for step in range(50_000_000):\n       if step < 30_000_000:\n           value = 1.0\n       else:\n           value = 0.0\n       wandb.log({\"eval/success_once\": value}, step=step)\n   ```\n2. In the WandB web interface, open the run and observe the plotted line:\n\n   * From step 0 to 30 million, `eval/success_once` oscillates around 1.0 (or remains at 1.0 in this example).\n   * After step 30 million, the metric is logged as 0.0 for all subsequent steps.\n     You will see a thin horizontal line “stuck” at Y=0 in the browser chart.\n3. Click the **Download PNG** button in the top-right corner of the chart.\n4. Open the downloaded PNG file. Notice that the zero-valued segment is either compressed against the x-axis or not visible at all; it looks as if the curve abruptly disappeared at step 30 million rather than remaining at 0.\n\n---\n\n**Expected Behavior:**\n\n* The exported PNG should retain a visible horizontal line at Y=0 for all steps where the metric is exactly 0.\n* Ideally, WandB would automatically leave a small bottom‐margin (e.g., extend the y-axis lower bound slightly below 0) when it detects a long constant segment at 0, so that users can still see that the metric is holding at zero.\n* Alternatively, an option in the chart editor (e.g., “Always display zero baseline”) could ensure that a flat line at 0 is never cropped out in the exported image.\n\n---\n\n**Actual Behavior:**\n\n* In the web UI, the flat zero segment is visible (it is drawn as a thin line at Y=0).\n* In the downloaded PNG, that same segment is essentially “invisible” (compressed onto the x-axis or clipped), making it appear as if the metric disappeared instead of staying at zero.\n\n---\n\n**Screenshots:**\n\n1. **Web UI view (zero segment visible):**\n   \n\n![Image](https://github.com/user-attachments/assets/b78ddf28-bc1c-4228-811f-c2e817b04a3c)\n\n   *In the browser, you can see a thin line at Y=0 for steps > 30 million.*\n\n2. **Exported PNG (zero segment hidden):**\n   \n![Image](https://github.com/user-attachments/assets/8cb688ed-166c-4098-8fe7-094fb191b6b7)\n\n   *In the downloaded PNG, the flat line at Y=0 is not visible or is merged with the axis.*\n\n\n---\n\n**Environment:**\n* **Browser:** Chrome 114.0.5735.199 (Ubuntu 20.04)\n* **Operating System:** Ubuntu 20.04 LTS\n\n\n---\n\n**Additional Context:**\n\n* This issue can mislead new users into thinking that their metric has stopped logging once it reaches 0, since the chart appears to “end.”\n* In many reinforcement‐learning environments or evaluation runs, a metric can plateau at 0 (e.g., “success rate” for an untrained policy). If the baseline disappears, it can cause confusion, especially when inspecting long training runs.\n* If WandB could detect “long constant segments” and automatically pad the y-axis by a small fraction (e.g., 5 pixels or 5% below the minimum value), that would solve the problem without requiring manual intervention.\n* Alternatively, adding a simple option in the chart editor like “Always show zero line” or “Include constant baselines” would allow users to opt in to preserving these flat 0 segments in their exports.\n\n---\n\nThank you for considering this enhancement/bug fix. If you need any more details (e.g., a minimal reproduction script or raw CSV data), please let me know!\n",
    "comments": [
      {
        "user": "fmamberti-wandb",
        "body": "Hi @songyuc , thank you for raising this issue and providing all the details to reproduce on our end - I will raise this with our engineering team and will keep you posted.\n\nIn the meantime, as a workaround, you could manually set the y-axis Min to a value lower than 0, and the line will display in the exported png:\n\n![Image](https://github.com/user-attachments/assets/7520cba0-219a-423a-9da9-dad9c68ebf0c)\n\nOn a side note, to make sure performance on your workspaces is not impacted I would suggest reducing the logging frequency for you metrics, as following our recommendations we suggest not to log more than 100k steps per metric (see [here](https://docs.wandb.ai/guides/track/limits/#metric-frequency)) and while this is not a hard-limit, having ~50M steps per metric could impact the workspace performance. \n "
      }
    ]
  },
  {
    "issue_number": 9993,
    "title": "[Q]: Is there a way to silence \"wandb: Encoding video...\" message?",
    "author": "omaralvarez",
    "state": "open",
    "created_at": "2025-06-06T07:47:59Z",
    "updated_at": "2025-06-10T12:15:15Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\nHow can the `wandb: Encoding video...` messages when logging a video be silenced?\n\nI tried switching the verbosity of the logging but to no avail.\n\n",
    "comments": [
      {
        "user": "fmamberti-wandb",
        "body": "Hi @omaralvarez , Currently it is only possible to silnce those messages by setting the env variable `WANDB_SILENT` to True. This however, will turn off all terminal output for the run being logged - would this work for your case?\n\nOtherwise, I'd be happy to raise a feature request to be able to silence the encoding video messaging."
      },
      {
        "user": "omaralvarez",
        "body": "I would like to see the rest of the messages and just reduce the verbosity of those messages. It would be great if you could raise a feature request. Thanks!!"
      },
      {
        "user": "fmamberti-wandb",
        "body": "No Problem. I have now raised this for our product team to review."
      }
    ]
  },
  {
    "issue_number": 9847,
    "title": "[Q]: Possible to postpone all patching to explicit `wandb.init(...)` call?",
    "author": "vadimkantorov",
    "state": "closed",
    "created_at": "2025-05-13T10:16:50Z",
    "updated_at": "2025-06-09T18:19:47Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\nAnd especially the patching of stdout/stderr related behavior: \n\nhttps://github.com/wandb/wandb/blob/e35e545afd28aab70ee9e2a9dcc5ec7cfb95b1a1/wandb/sdk/lib/console_capture.py#L167-L172",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hey @vadimkantorov, thanks for writing in! Would you mind sharing what's your use case here, so we can have better understanding of the request and advice further?"
      },
      {
        "user": "vadimkantorov",
        "body": "This is mainly for debugging problems with stdout output. When there are many layers of interception/meddling of stdout/stderr (tqdm, ray, tmux, wandb, maybe sth else) with multi-gpu single machine setups (which try to prevent repeated on interleaved messages from several processes):\n\n- https://github.com/tqdm/tqdm/issues/760\n\nFor this it's useful to be able to disable any meddling with stdout/stderr (and making such meddling explicit)"
      },
      {
        "user": "luisbergua",
        "body": "Thanks @vadimkantorov! Am I right in understanding that you're seeing stdout output being tracked before `wandb.init` is called? If so, would you mind sharing a code snippet showing this?"
      }
    ]
  },
  {
    "issue_number": 7237,
    "title": "[Feature]: Delete an org",
    "author": "Hritaban02",
    "state": "closed",
    "created_at": "2024-03-27T08:17:41Z",
    "updated_at": "2025-06-09T04:44:55Z",
    "labels": [
      "a:app"
    ],
    "body": "### Description\r\n\r\nI would like to be able to delete an org and all its teams. \r\n\r\n### Suggested Solution\r\n\r\nAn option in the Usage and Billing Tab to delete the organization as a whole by the owners or admins.\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "MBakirWB",
        "body": "Hi @Hritaban02 thank you for the feature request. It's been logged with our product team. "
      },
      {
        "user": "haonan-li",
        "body": "Hi @MBakirWB \n\nIs there any update on this feature? I want to delete an org, but still don't know how to do it."
      },
      {
        "user": "JoanaMarieL",
        "body": "Hi @haonan-li , we can help you with deleting the ORG as the feature is still in queue. May we ask for the requested ORG?"
      }
    ]
  },
  {
    "issue_number": 507,
    "title": "log_uniform min max unintuitive",
    "author": "dreamflasher",
    "state": "closed",
    "created_at": "2019-08-28T13:12:43Z",
    "updated_at": "2025-06-08T17:21:18Z",
    "labels": [
      "ty:feature"
    ],
    "body": "wandb, version 0.8.9\r\nPython 3.7.4\r\nLinux\r\n\r\n### Description\r\n\r\nCurrently ```Log uniform. Number between exp(min) and exp(max) so that the logarithm of the return value is uniformly distributed.```\r\nWhile it should rather be distributed between min and max (keeping the meaning of the terms min and max) – my expectation would have been, if I want my learning rate between 1e-8 and 1, I can just tell it to be between 1e-8 and 1, but I actually have to convert it by taking the log of my desired values, passing min: -8 and max: 0, so that I get the scaling between 1e-8 and 1. \r\n\r\nSolution: In the parsing of the function take the log of the min and max params.",
    "comments": [
      {
        "user": "issue-label-bot[bot]",
        "body": "Issue-Label Bot is automatically applying the label `enhancement` to this issue, with a confidence of 0.81. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/wandb/client) and [code](https://github.com/hamelsmu/MLapp) for this bot."
      },
      {
        "user": "danmou",
        "body": "Note that you need to take the natural logarithm, so 1e-8 actually becomes -18.42. This is highly unintuitive!"
      },
      {
        "user": "mboratko",
        "body": "In addition, it changes the interpreted value of the parameter simply by changing the distribution, which is very strange. For example, if I have a script expecting `learning_rate` then changing a sweep script with\r\n```\r\nlearning-rate:\r\n  min: 1e-5\r\n  max: 1e-1\r\n```\r\nto\r\n```\r\nlearning-rate:\r\n  distribution: log_uniform\r\n  min: 1e-5\r\n  max: 1e-1\r\n```\r\nwould require me to change my training program.\r\n\r\n----\r\n\r\nEdit: I see that the above comments point out that, actually, the workaround is to enter the log of the values you wanted as min and max. I agree with the other commenters that this is very unintuitive."
      }
    ]
  },
  {
    "issue_number": 9654,
    "title": "[Bug]: Using Python Pool/Multiprocessing causes WandbServiceNotOwnedError",
    "author": "nzkj",
    "state": "open",
    "created_at": "2025-03-28T21:13:07Z",
    "updated_at": "2025-06-08T11:21:26Z",
    "labels": [
      "ty:bug",
      "c:sweeps"
    ],
    "body": "### Describe the bug\n\nI am attempting to use Sklearn with Wandb for perform a sweep on a model. I would like the runs of this sweep to be parallelized, akin to `GridSearchCV`'s `n_jobs=-1` [parameter](https://scikit-learn.org/stable/glossary.html#term-n_jobs).\n\nMy understanding of Wandb: for maximum parallelization, I should create an agent for each CPU. This agent will then receive a hyperparameter combination from the sweep server and enact the training function. When there are no longer any hyperparameter combinations left to run, the agent process will close. Please correct my understanding if this is wrong.\n\nI've written a Python script that spawns a number of agents in parallel, but when a `wandb.agent()` is invoked, I receive an exception (for each agent spawned):\n\n```\nwandb: ERROR Run eqj8has6 errored:\nwandb: ERROR Traceback (most recent call last):\nwandb: ERROR   File \"/home/my_username/projects/my-predictor/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 300, in _run_job\nwandb: ERROR     wandb.teardown()\nwandb: ERROR   File \"/home/my_username/projects/my-predictor/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_setup.py\", line 402, in teardown\nwandb: ERROR     orig_singleton._teardown(exit_code=exit_code)\nwandb: ERROR   File \"/home/my_username/projects/my-predictor/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_setup.py\", line 249, in _teardown\nwandb: ERROR     internal_exit_code = self._connection.teardown(exit_code or 0)\nwandb: ERROR   File \"/home/my_username/projects/my-predictor/.venv/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py\", line 212, in teardown\nwandb: ERROR     raise WandbServiceNotOwnedError(\nwandb: ERROR wandb.sdk.lib.service_connection.WandbServiceNotOwnedError: Cannot tear down service started by different process\nwandb: ERROR\n```\n\nEventually, the invoked agent will print `wandb: Sweep Agent: Waiting for job.` and it will receive a hyperparameter combination to run. Some of these runs actually succeed and output to the user interface, however, there are also a large number of \"crashed\" runs. As far as I know, this means that the sweep doesn't actually include the full sweep of hyperparameter combinations, which isn't useful for model development.\n\nI've included an example script that reproduces the error below with some dummy data. However, note that the data doesn't really make any difference, since the exception is raised on `wandb.agent(...)` inside `run_wandb_agent()`.\n\n```Python\nimport os\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport wandb\nimport numpy as np\nimport multiprocessing as mp\nfrom sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nPROJECT_NAME = \"my-predictor\"\n\n\ndef get_run_params(sweep_config):\n    \"\"\"\n    This function extracts the parameters that are specific to the run from the sweep configuration.\n    In other words, it removes things like the method, metric, and parameters sections from the YAML,\n    and returns only the specific run's parameters (e.g. n_estimators, learning_rate, etc.)\n    \"\"\"\n    config = wandb.config\n    run_keys = set(config.keys()) - set(sweep_config.keys())\n    return {key: config[key] for key in run_keys}\n\n\ndef run_wandb_agent(args):\n    \"\"\"\n    Function to run a single Weights and Biases agent using the sweep_id.\n    \"\"\"\n    sweep_id, model, sweep_config, X_train, y_train, groups_train = args\n    wandb.agent(\n        sweep_id=sweep_id,\n        function=lambda: train_classifier(\n            model=model,\n            sweep_config=sweep_config,\n            X=X_train,\n            y=y_train,\n            groups=groups_train,\n        ),\n    )\n\n\ndef train_classifier(model, sweep_config, X, y, groups):\n    wandb.init(config=sweep_config, project=PROJECT_NAME)\n\n    model.set_params(**get_run_params(sweep_config))\n    print(f\"Model config: {model.get_params()}\")\n\n    NUM_FOLDS = 5\n    sgkf = StratifiedGroupKFold(n_splits=NUM_FOLDS)\n\n    fold_accuracies = []\n    fold_mccs = []\n    fold_f1s = []\n    for i, (train_index, val_index) in enumerate(sgkf.split(X, y, groups)):\n        print(f\"{os.getpid()}: [CV {i}/{NUM_FOLDS}] START\")\n\n        X_train, X_val = X[train_index], X[val_index]\n        y_train, y_val = y[train_index], y[val_index]\n        model.fit(X_train, y_train)\n\n        y_pred = model.predict(X_val)\n        fold_accuracy = accuracy_score(y_val, y_pred)\n        fold_mcc = matthews_corrcoef(y_val, y_pred)\n        fold_f1 = f1_score(y_val, y_pred)\n\n        fold_accuracies.append(fold_accuracy)\n        fold_mccs.append(fold_mcc)\n        fold_f1s.append(fold_f1)\n\n        wandb.log(\n            {\"fold\": i, \"accuracy\": fold_accuracy, \"mcc\": fold_mcc, \"f1\": fold_f1}\n        )\n\n        print(f\"{os.getpid()}: [CV {i}/{NUM_FOLDS}] END\")\n\n    wandb.log(\n        {\n            \"mean_accuracy\": np.mean(fold_accuracies),\n            \"mean_mcc\": np.mean(fold_mccs),\n            \"mean_f1\": np.mean(fold_f1s),\n        }\n    )\n\n    wandb.finish()\n\n\nif __name__ == \"__main__\":\n    # Generate a synthetic dataset\n    X, y = make_classification(\n        n_samples=1000,\n        n_features=20,\n        n_informative=15,\n        n_redundant=5,\n        n_classes=2,\n        random_state=42,\n    )\n\n    # Split the dataset into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n\n    # Create dummy groups for StratifiedGroupKFold\n    groups_train = [i // 10 for i in range(len(y_train))]\n\n    sweep_params = {\n        \"method\": \"grid\",\n        \"metric\": {\"name\": \"mean_mcc\", \"goal\": \"maximize\"},\n    }\n\n    model = GradientBoostingClassifier()\n    model_params = {\n        \"parameters\": {\n            \"n_estimators\": {\"values\": [10, 50, 100, 200]},\n            \"learning_rate\": {\"values\": [0.01, 0.1, 0.5, 1.0]},\n            \"max_depth\": {\"values\": [3, 5, 10, 15]},\n            \"subsample\": {\"values\": [0.5, 0.75, 1.0]},\n        }\n    }\n\n    sweep_config = {**sweep_params, **model_params}\n\n    n_processes = 20\n    sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME)\n\n    # Create arguments for each process\n    process_args = [\n        (sweep_id, model, sweep_config, X_train, y_train, groups_train)\n        for _ in range(n_processes)\n    ]\n\n    # Run the agents in parallel\n    wandb.setup()\n    pool = mp.Pool(processes=n_processes)\n    pool.map(run_wandb_agent, process_args)\n    pool.close()\n    pool.join()\n```\n\nSystem description:\n- Reproducible on `Ubuntu 22.04.5 LTS` and `Linux Mint 22`\n- Reproducible on Wandb versions `v0.19.8` and `v0.18.7`. \n- Reproducible on Python `3.10.12` and `3.12.3` \n\nI've searched online for similar bug reports but can't find anything. There are a couple examples of people trying similar approaches to running multiple agents but they don't seem to encounter this error. This surprises me, since parallelizing your model training isn't exactly a novel idea...\n\nI'm pretty lost at this point, so any help would be greatly appreciated! Thanks in advance.",
    "comments": [
      {
        "user": "nzkj",
        "body": "Here's a less representative but far more minimal example that reproduces the error (obviously won't train any classifier).\n\n```Python\nimport wandb\nimport multiprocessing as mp\n\ndef train_classifier():\n    pass\n\ndef run_wandb_agent(sweep_id):\n    wandb.agent(sweep_id, function=train_classifier)\n\nif __name__ == \"__main__\":\n    sweep_config = {\n        \"method\": \"grid\",\n        \"parameters\": {\n            \"n_estimators\": {\"values\": [10, 50]},\n            \"learning_rate\": {\"values\": [0.01, 0.1]},\n        },\n    }\n\n    n_processes = 20\n    sweep_id = wandb.sweep(sweep_config, project=\"my-predictor\")\n\n    with mp.Pool(processes=n_processes) as pool:\n        pool.map(run_wandb_agent, [sweep_id] * n_processes)\n\n```"
      },
      {
        "user": "fmamberti-wandb",
        "body": "Hi @nzkj , thank you for reaching out and sharing your use case. Currently running multiple agents from the same parent process is not supported (as it's not supported running multiple runs from the same process).\n\nYou can see in our docs the current solutions to [parallelise multiple agents](https://docs.wandb.ai/guides/sweeps/parallelize-agents/)\n\nI will be raising a feature request on your behalf with our product team to support this going forward - I will also mention the suggested solutoin."
      },
      {
        "user": "Giuspepe",
        "body": "I found a neat solution for this after debugging this for a while. You just have to delete the environment variable `WANDB_SERVICE` in each process, so a new service connection is created in each process.\n\nWhen you run `wandb.sweep` in the parent process, `WANDB_SERVICE` gets set. Then you call `wandb.agent` in the subprocesses - they see the `WANDB_SERVICE` environment variable is set (because they inherit it from the parent process) and so when the indivudal subprocesses finish, they try to close the connection, causing the error because they do not own the connection (the parent process does).\n\nSo the solution is pretty easy: before calling `wandb.agent` in the subprocesses, delete the `WANDB_SERVICE` environment variable so each process gets its own service connection.\n\n\nHere's a minimal example with this fix applied:\n\n\n```python\nimport wandb\nimport multiprocessing as mp\nimport os\nimport numpy as np\nimport logging\n\ndef train_classifier():\n    wandb.init()\n    print(\"hello from train_classifier.\", [(k, v) for k, v in os.environ.items() if k.startswith(\"WANDB_\")])\n    wandb.log(\n        {\n            \"message\": \"hello\",\n            \"env vars\": [(k, v) for k, v in os.environ.items() if k.startswith(\"WANDB_\")],\n        }\n    )\n    wandb.summary[\"accuracy\"] = np.random.random()\n    wandb.summary[\"loss\"] = np.random.random()\n    wandb.finish()\n\n\ndef run_wandb_agent(sweep_id):\n    del os.environ[\"WANDB_SERVICE\"]\n    wandb.agent(sweep_id, function=train_classifier)\n\n\nif __name__ == \"__main__\":\n    sweep_config = {\n        \"method\": \"bayes\",\n        \"parameters\": {\n            \"n_estimators\": {\"values\": [10, 50]},\n            \"learning_rate\": {\n                \"min\": 1e-6,\n                \"max\": 1e-2,\n                \"distribution\": \"log_uniform_values\",\n            },\n        },\n        \"run_cap\": 10,\n        \"metric\": {\"goal\": \"maximize\", \"name\": \"accuracy\"},\n    }\n\n    n_processes = 5\n    sweep_id = wandb.sweep(sweep_config, project=\"my-predictor\")\n\n    with mp.Pool(processes=n_processes) as pool:\n        pool.map(run_wandb_agent, [sweep_id] * n_processes)\n```\n\nI hope this helps!\n@fmamberti-wandb, maybe you can add this to the [documentation on how to parallelise agents](https://docs.wandb.ai/guides/sweeps/parallelize-agents/). I'm sure this will save other people plenty of time figuring this out. "
      }
    ]
  },
  {
    "issue_number": 2981,
    "title": "[App] Table not updating it at each call of log",
    "author": "oumarkaba",
    "state": "closed",
    "created_at": "2021-12-03T21:26:43Z",
    "updated_at": "2025-06-06T16:10:46Z",
    "labels": [
      "a:app",
      "c:table"
    ],
    "body": "I have a code for which I want to log some metadata at each step. I thought I could log this metadata in a Table, unfortunately the table is not updated on the app after the first call to log.\r\n\r\nMy code is something like\r\n\r\n```\r\nmetadata = [[\"I love my phone\", \"1\", \"1\"]]\r\ntable = wandb.Table(data=metadata, columns=[\"Text\", \"Predicted Label\", \"True Label\"])\r\nwandb.log({\"metadata\": table})\r\n\r\nrun = wandb.init(**config)\r\nwith run:\r\n    for step in steps:\r\n        ## some code\r\n        table.add_data(*metadata)\r\n        run.log({\"examples\": table})\r\n```\r\n\r\nOn the app, I can only see the initial table with `data` no rows are added to it.\r\nNote that the corresponding artifact is also not updated. Maybe this is due to wandb assigning the same identity to initial and updated table and not logging it again?\r\n",
    "comments": [
      {
        "user": "aidanjdonohue",
        "body": "Hi @oumarkaba, can you share a link to the artifact where this is happening? \r\n"
      },
      {
        "user": "oumarkaba",
        "body": "Here it is : https://wandb.ai/movie-recs-team3/movie-recs/artifacts/run_table/run-19tmgyz5-metadata/2c18e2476e1d0e420924"
      },
      {
        "user": "amitkparekh",
        "body": "A work around I've just found is creating a new instance of a Table which contains the same columns and data as the old one, and logging that.\r\n\r\n```python\r\nnew_table = wandb.Table(\r\n    columns=self.wandb_table.columns, data=self.wandb_table.data\r\n)\r\nself.wandb.log({\"predictions\": new_table}, commit=False)\r\n```\r\n\r\nBefore doing this, it would only create a single json for the table in the `RUN_DIR/files/media/table` folder, but now it's making one for each time this command is run. \r\n\r\nMaybe this can help narrowing down the problem? "
      }
    ]
  },
  {
    "issue_number": 9683,
    "title": "[Bug-App]: error listing projects -  invalid kind. expected organization",
    "author": "tommykoctur",
    "state": "closed",
    "created_at": "2025-04-04T06:13:12Z",
    "updated_at": "2025-06-05T20:34:21Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\nHi when I try to list projects in my account it shows an error:\n```\n\nInvalid kind . Expected Organization\nAn application error occurred.\n\n Click to refresh the page.\n```\nBefore it was working well.\n\nconsole output\n```config.ts:17 Unable to get configuration from server, using defaults\n(anonymous) @ config.ts:17\ninstallServiceWorker.ts:35 Service worker active\nmanifest.json:1 Manifest: property 'start_url' ignored, should be same origin as document.\ncookie.ts:65 Invalid cookie BL\n(anonymous) @ instrument.js:111\ncookieStrToKeyVal @ cookie.ts:65\ngetCookieBool @ cookie.ts:22\n(anonymous) @ useAnonUserSession.tsx:28\nAk @ react-dom.profiling.min.js:264\ntl$1 @ react-dom.profiling.min.js:313\nql @ react-dom.profiling.min.js:296\nxg @ react-dom.profiling.min.js:142\ntl$1 @ react-dom.profiling.min.js:315\n(anonymous) @ react-dom.profiling.min.js:308\nwt @ scheduler.production.min.js:13\nFt @ scheduler.production.min.js:14\nframe-modern.5dcb4e73.js:1 [Intercom] The App ID in your code snippet has not been set. Set it to your App ID found in settings to complete installation: https://app.intercom.com/a/apps/_/settings/web\ni @ frame-modern.5dcb4e73.js:1\nn @ frame-modern.5dcb4e73.js:1\nboot @ frame-modern.5dcb4e73.js:1\n(anonymous) @ frame-modern.5dcb4e73.js:1\n12369 @ frame-modern.5dcb4e73.js:1\na @ frame-modern.5dcb4e73.js:1\n22732 @ frame-modern.5dcb4e73.js:1\na @ frame-modern.5dcb4e73.js:1\n(anonymous) @ frame-modern.5dcb4e73.js:1\na.O @ frame-modern.5dcb4e73.js:1\n(anonymous) @ frame-modern.5dcb4e73.js:1\n(anonymous) @ frame-modern.5dcb4e73.js:1\nframe-modern.5dcb4e73.js:1 [Intercom] Launcher is disabled in settings or current page does not match display conditions\nbeamer-embed.js:13 Initializing Beamer. [Update and engage users effortlessly - https://getbeamer.com]\nindex.js:87 \n            \n            \n           POST https://api.wandb.ai/graphql 400 (Bad Request)\n(anonymous) @ instrument.js:142\nft @ instrumentMethod.js:67\n(anonymous) @ index.js:87\nfetch @ apollo.ts:57\n(anonymous) @ bundle.esm.js:69\net @ Observable.js:197\nvalue @ Observable.js:279\n(anonymous) @ apollo.ts:122\net @ Observable.js:197\nvalue @ Observable.js:279\n(anonymous) @ Observable.js:319\net @ Observable.js:197\nvalue @ Observable.js:279\net.try @ bundle.esm.js:122\net.start @ bundle.esm.js:110\ntt.request @ bundle.esm.js:153\n(anonymous) @ bundle.esm.js:161\n(anonymous) @ apollo.ts:287\n(anonymous) @ bundle.esm.js:161\n(anonymous) @ bundle.esm.js:11\net @ Observable.js:197\nvalue @ Observable.js:279\n(anonymous) @ apollo.ts:166\nPromise.then\n(anonymous) @ apollo.ts:130\net @ Observable.js:197\nvalue @ Observable.js:279\n(anonymous) @ bundle.esm.js:864\net @ Observable.js:197\nvalue @ Observable.js:279\net.getObservableFromLink @ bundle.esm.js:1595\n(anonymous) @ bundle.esm.js:1630\net.fetchRequest @ bundle.esm.js:1629\n(anonymous) @ bundle.esm.js:1146\nft @ tslib.es6.js:100\n(anonymous) @ tslib.es6.js:81\n(anonymous) @ tslib.es6.js:74\n__awaiter$3 @ tslib.es6.js:70\net.fetchQuery @ bundle.esm.js:1092\net.observeQuery @ bundle.esm.js:1460\ntt.setUpQuery @ bundle.esm.js:388\ntt.onSubscribe @ bundle.esm.js:366\n(anonymous) @ bundle.esm.js:96\net @ Observable.js:197\nvalue @ Observable.js:279\ntt.startQuerySubscription @ react-hooks.esm.js:215\ntt.execute @ react-hooks.esm.js:105\ntt.executeLazy @ react-hooks.esm.js:119\n(anonymous) @ react-hooks.esm.js:383\nuseDeepMemo$3 @ react-hooks.esm.js:349\nuseBaseQuery$1 @ react-hooks.esm.js:383\nuseLazyQuery$1 @ react-hooks.esm.js:407\n_0 @ PaginatedList.tsx:139\nai @ react-dom.profiling.min.js:175\nGj @ react-dom.profiling.min.js:213\nIl @ react-dom.profiling.min.js:321\nHl @ react-dom.profiling.min.js:305\nGl @ react-dom.profiling.min.js:304\nul @ react-dom.profiling.min.js:304\nql @ react-dom.profiling.min.js:296\nxg @ react-dom.profiling.min.js:142\nui @ react-dom.profiling.min.js:289\nNi @ react-dom.profiling.min.js:188\nPromise.then\nonNewData @ react-hooks.esm.js:366\nut.runLazyQuery @ react-hooks.esm.js:70\n(anonymous) @ PaginatedList.tsx:174\nAk @ react-dom.profiling.min.js:264\ntl$1 @ react-dom.profiling.min.js:313\nql @ react-dom.profiling.min.js:296\nxg @ react-dom.profiling.min.js:142\nui @ react-dom.profiling.min.js:289\nNi @ react-dom.profiling.min.js:188\nonNewData @ react-hooks.esm.js:378\nnext @ react-hooks.esm.js:234\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\n(anonymous) @ bundle.esm.js:435\niterateObserversSafely @ bundle.esm.js:435\nnext @ bundle.esm.js:410\nst @ bundle.esm.js:1209\n(anonymous) @ bundle.esm.js:1294\n(anonymous) @ bundle.esm.js:1559\n(anonymous) @ bundle.esm.js:1557\net.broadcastQueries @ bundle.esm.js:1555\n(anonymous) @ bundle.esm.js:1646\nnext @ Observable.js:322\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\n(anonymous) @ bundle.esm.js:866\nnext @ bundle.esm.js:866\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\nnext @ bundle.esm.js:29\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\nnext @ apollo.ts:296\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\nonNext @ bundle.esm.js:44\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\nnext @ Observable.js:327\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\n(anonymous) @ bundle.esm.js:76\nPromise.then\n(anonymous) @ bundle.esm.js:75\net @ Observable.js:197\nvalue @ Observable.js:279\n(anonymous) @ apollo.ts:122\net @ Observable.js:197\nvalue @ Observable.js:279\n(anonymous) @ Observable.js:319\net @ Observable.js:197\nvalue @ Observable.js:279\net.try @ bundle.esm.js:122\net.start @ bundle.esm.js:110\ntt.request @ bundle.esm.js:153\n(anonymous) @ bundle.esm.js:161\n(anonymous) @ apollo.ts:287\n(anonymous) @ bundle.esm.js:161\n(anonymous) @ bundle.esm.js:11\net @ Observable.js:197\nvalue @ Observable.js:279\n(anonymous) @ apollo.ts:166\nPromise.then\n(anonymous) @ apollo.ts:130\net @ Observable.js:197\nvalue @ Observable.js:279\n(anonymous) @ bundle.esm.js:864\net @ Observable.js:197\nvalue @ Observable.js:279\net.getObservableFromLink @ bundle.esm.js:1595\n(anonymous) @ bundle.esm.js:1630\net.fetchRequest @ bundle.esm.js:1629\n(anonymous) @ bundle.esm.js:1146\nft @ tslib.es6.js:100\n(anonymous) @ tslib.es6.js:81\n(anonymous) @ tslib.es6.js:74\n__awaiter$3 @ tslib.es6.js:70\net.fetchQuery @ bundle.esm.js:1092\net.observeQuery @ bundle.esm.js:1460\ntt.setUpQuery @ bundle.esm.js:388\ntt.onSubscribe @ bundle.esm.js:366\n(anonymous) @ bundle.esm.js:96\net @ Observable.js:197\nvalue @ Observable.js:279\ntt.startQuerySubscription @ react-hooks.esm.js:224\nut.getExecuteResult @ react-hooks.esm.js:74\ntt.execute @ react-hooks.esm.js:106\n(anonymous) @ react-hooks.esm.js:392\nuseDeepMemo$2 @ react-hooks.esm.js:356\nuseBaseQuery @ react-hooks.esm.js:392\nuseQuery$1 @ react-hooks.esm.js:419\nuseGalleryQuery @ graphql.tsx:53195\nuseGallerySpec @ index.tsx:106\n(anonymous) @ EntityPageContextProvider.tsx:73\nai @ react-dom.profiling.min.js:175\nGj @ react-dom.profiling.min.js:213\nFj @ react-dom.profiling.min.js:211\nDj @ react-dom.profiling.min.js:210\nIl @ react-dom.profiling.min.js:326\nHl @ react-dom.profiling.min.js:305\nGl @ react-dom.profiling.min.js:304\nul @ react-dom.profiling.min.js:304\nql @ react-dom.profiling.min.js:296\nxg @ react-dom.profiling.min.js:142\nui @ react-dom.profiling.min.js:289\nNi @ react-dom.profiling.min.js:188\nonNewData @ react-hooks.esm.js:378\nnext @ react-hooks.esm.js:234\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\n(anonymous) @ bundle.esm.js:435\niterateObserversSafely @ bundle.esm.js:435\nnext @ bundle.esm.js:410\nst @ bundle.esm.js:1209\n(anonymous) @ bundle.esm.js:1294\n(anonymous) @ bundle.esm.js:1559\n(anonymous) @ bundle.esm.js:1557\net.broadcastQueries @ bundle.esm.js:1555\n(anonymous) @ bundle.esm.js:1646\nnext @ Observable.js:322\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\n(anonymous) @ bundle.esm.js:866\nnext @ bundle.esm.js:866\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\nnext @ bundle.esm.js:29\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\nnext @ apollo.ts:296\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\nonNext @ bundle.esm.js:44\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\nnext @ Observable.js:327\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\nnotifySubscription$1 @ Observable.js:135\nonNotify$1 @ Observable.js:179\nvalue @ Observable.js:235\n(anonymous) @ bundle.esm.js:76\nPromise.then\n(anonymous) @ bundle.esm.js:75\net @ Observable.js:197\nvalue @ Observable.js:279\n(anonymous) @ apollo.ts:122\net @ Observable.js:197\nvalue @ Observable.js:279\n(anonymous) @ Observable.js:319\net @ Observable.js:197\nvalue @ Observable.js:279\net.try @ bundle.esm.js:122\net.start @ bundle.esm.js:110\ntt.request @ bundle.esm.js:153\n(anonymous) @ bundle.esm.js:161\n(anonymous) @ apollo.ts:287\n(anonymous) @ bundle.esm.js:161\n(anonymous) @ bundle.esm.js:11\net @ Observable.js:197\nvalue @ Observable.js:279\n(anonymous) @ apollo.ts:166\nPromise.then\n(anonymous) @ apollo.ts:130\net @ Observable.js:197\nvalue @ Observable.js:279\n(anonymous) @ bundle.esm.js:864\net @ Observable.js:197\nvalue @ Observable.js:279\net.getObservableFromLink @ bundle.esm.js:1595\n(anonymous) @ bundle.esm.js:1630\net.fetchRequest @ bundle.esm.js:1629\n(anonymous) @ bundle.esm.js:1146\nft @ tslib.es6.js:100\n(anonymous) @ tslib.es6.js:81\n(anonymous) @ tslib.es6.js:74\n__awaiter$3 @ tslib.es6.js:70\net.fetchQuery @ bundle.esm.js:1092\net.observeQuery @ bundle.esm.js:1460\ntt.setUpQuery @ bundle.esm.js:388\ntt.onSubscribe @ bundle.esm.js:366\n(anonymous) @ bundle.esm.js:96\net @ Observable.js:197\nvalue @ Observable.js:279\ntt.startQuerySubscription @ react-hooks.esm.js:224\nut.getExecuteResult @ react-hooks.esm.js:74\ntt.execute @ react-hooks.esm.js:106\n(anonymous) @ react-hooks.esm.js:392\nuseDeepMemo$2 @ react-hooks.esm.js:356\nuseBaseQuery @ react-hooks.esm.js:392\nuseQuery$1 @ react-hooks.esm.js:419\nuseEntitySettingsQuery @ graphql.tsx:32962\nu @ entitySettings.ts:19\n(anonymous) @ EntityPage.tsx:120\nai @ react-dom.profiling.min.js:175\nGj @ react-dom.profiling.min.js:213\nFj @ react-dom.profiling.min.js:211\nDj @ react-dom.profiling.min.js:210\nIl @ react-dom.profiling.min.js:320\nHl @ react-dom.profiling.min.js:305\nGl @ react-dom.profiling.min.js:304\nul @ react-dom.profiling.min.js:304\nql @ react-dom.profiling.min.js:296\nxg @ react-dom.profiling.min.js:142\n(anonymous) @ react-dom.profiling.min.js:290\nfs.js:4 Apollo NetworkErrors: undefined\nfs.js:4 Apollo GraphQLErrors: [{…}]\nfs.js:4 Apollo NetworkErrors: ServerError: Response not successful: Received status code 400\n    at throwServerError (bundle.esm.js:22:17)\n    at bundle.esm.js:47:13\nfs.js:4 Apollo GraphQLErrors: [{…}]\n```",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @tommykoctur! What is your wandb username? \n\nWhat page are you trying to get the list of projects on?"
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved. "
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!\n"
      }
    ]
  },
  {
    "issue_number": 8769,
    "title": "[Bug-App]: Histogram render not working",
    "author": "ddonatien",
    "state": "open",
    "created_at": "2024-11-05T08:08:57Z",
    "updated_at": "2025-06-05T06:49:53Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\nThe web app doesn't display the beautiful histogram plots like it used to and renders rather ugly-looking teal rectangles instead.\n![Example](https://github.com/user-attachments/assets/6e8d9eaf-52ae-402e-9baf-739c7801d66a)\n\nOS: macOS Sequoia 15.1\nBrowser: Safari 18.1 and firefox 131.0.3 (tested both)",
    "comments": [
      {
        "user": "JoanaMarieL",
        "body": "hello @ddonatien , thank you for writing in and happy to help. Could you please try to Edit Panel this specific chart. From Data tab, look for Point Aggregation Method, and see if it is defaulted to Full Fidelity. Then try to change to Random Sampling and try to adjust the Smoothing option. ![Image](https://github.com/user-attachments/assets/a2b5504c-02b3-41ae-bad9-fade0116c6f6)\nLet us know the result~"
      },
      {
        "user": "ddonatien",
        "body": "Hello, thank you for your help. \nI have tried different smoothing options with random sampling but to no avail.\nHere are a few examples:\nRandom sampling w/ TW EMA 0.49:\n![Image](https://github.com/user-attachments/assets/b4ac8445-c182-44f2-86ef-0dc4ce3ec62a)\nRandom sampling w/ Gaussian 45:\n![Image](https://github.com/user-attachments/assets/ff1325cf-4915-412e-a892-185aabb1b369)\nFull fidelity, min/max `Always`:\n![Image](https://github.com/user-attachments/assets/eb4befd0-9ee3-43ff-8483-20fbdb8623aa)\nFull fidelity, min/max `On Hover`:\n![Image](https://github.com/user-attachments/assets/d1667aca-1e29-454f-befa-9ea670010e11)\n"
      },
      {
        "user": "nikonikolov",
        "body": "Can confirm I am experiencing the same issue. I really want to see the histogram color coding that existed previously based on the bin weights."
      }
    ]
  },
  {
    "issue_number": 9963,
    "title": "[Bug]: wandb-core signal handling behavior",
    "author": "Unturned3",
    "state": "open",
    "created_at": "2025-06-02T19:39:14Z",
    "updated_at": "2025-06-04T20:44:34Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\n\nW&B version: `0.19.9`\nPython version: `3.13.3`\nOS: `AlmaLinux (RHEL) 9.5`\n\nWhat should the correct signal handling behavior be for `wandb-core`?\n\nWhen we create a `wandb` run in a Python script via `wandb.init(...)`, we get the following process hierarchy:\n```\nPID     PPID    USER  COMMAND\n1581284 1580143 user  python3 script.py\n1581651 1581284 user      /usr/lib/python3.13/site-packages/wandb/bin/wandb-core ...\n1581670 1581651 user          /usr/lib/python3.13/site-packages/wandb/bin/gpu_stats ...\n```\n\nIn high-performance computing environments that uses SLURM, cancelling a job using `scancel <jobid>` will result in `SIGINT/SIGTERM` being sent to __all__ processes (and their children) within the job. When `wandb-core` receives `SIGINT` it exits immediately without finishing the run. When the parent `script.py` finally reaches `run.finish()` in the clean-up logic in reaction to the `SIGINT`, we get a `BrokenPipeError` since the `wandb-core` child process already died. The run then becomes a \"zombie\" that appears as \"Running\" on the W&B Web UI, then \"Crashed\" after a while.\n\nCan `wandb-core` gracefully terminate and mark the run as \"Killed\" or some appropriate status, upon receiving a signal? Or can we have `wandb-core` ignore signals such as `SIGINT`, `SIGTERM`, `SIGUSR1` or `SIGUSR2`?",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @Unturned3, thanks for flagging this! What you’re seeing is likely due to how `scancel` sends termination signals, it delivers `SIGINT` to all processes in the job, including both your Python script and the `wandb-core` subprocess, and as you shared currently, `wandb-core` exits immediately on receiving `SIGINT`. \n\nAs a workaround, could you try using the Stop button in the UI instead of `scancel`? That should allow `wandb-core` to shut down more gracefully, especially if it receives the signal first.\n\nAlso, if you’re able to, it would be really helpful to set `WANDB_DEBUG` and `WANDB_CORE_DEBUG` env vars to `True` before rerunning your script. Then, trigger the `scancel` and share the three SDK debug logs for the run that ran into this issue. You can find them in the directory `./wandb/run-{date_time}-{run_id}/logs` on the machine where you ran your code. "
      },
      {
        "user": "Unturned3",
        "body": "Hi @aajais , thanks for the quick reply! I believe the Stop button on the W&B Web UI isn't a \"full solution\" to this problem:\n\n1. The Stop button only generates a `SIGINT` in the main thread of the master process (which spawned `wandb-core`). To inform all other processes (which may be on other machines in a distributed training setup) to stop, `scancel <jobid>` seems to be the only reliable / non-hacky method to do so.\n2. __More importantly__, when SLURM preempts a job, it blasts `SIGINT/TERM` to all processes in the job. There seems to be no way to modify this behavior. Although my script can catch these signals, it is no use if `wandb-core` dies immediately in reaction to them, leaving my script unable to gracefully mark a run as \"Finished\".\n\nEager to hear what you think about these problems! It would be great if `wandb-core` (or W&B in general) can be made robust against these signals and behave in a helpful way.\n\n(I will try to get the debug logs soon.)"
      },
      {
        "user": "Unturned3",
        "body": "Upon further thought, I believe the correct behavior for `wandb-core` is to simply ignore signals such as `INT`, `TERM`, `USR1`, `USR2`, etc. rather than terminating.\n1. From an application logic point-of-view, it makes no sense that `wandb-core` can exit on its own and leave the parent dangling with a broken pipe. The lifetime of the `wandb-core` process should be entirely controlled by the parent (aside from uncatchable events like `SIGKILL`).\n2. It's not possible to modify SLURM's behavior of blasting every process with `SIGINT/TERM` upon job preemption."
      }
    ]
  },
  {
    "issue_number": 9947,
    "title": "[Bug]: 'RecordVideo' object has no attribute 'enabled'",
    "author": "glitchyordis",
    "state": "open",
    "created_at": "2025-05-31T10:36:25Z",
    "updated_at": "2025-06-04T19:59:11Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n# System and packages\nPython: 3.10.16\nSystem: Ubuntu 22.04\ngymnasium: 1.1.1\nwandb: 0.19.11\n\n# Issue\nI was running gymnasium environment and the following error showed up\n\n```\nTraceback (most recent call last):\n  File \"/home/polaris/Desktop/glitchy/task/custom_gym_env.py\", line 962, in <module>\n    envs.close()\n  File \"/home/polaris/miniconda3/envs/task/lib/python3.10/site-packages/gymnasium/vector/vector_env.py\", line 234, in close\n    self.close_extras(**kwargs)\n  File \"/home/polaris/miniconda3/envs/task/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 375, in close_extras\n    [env.close() for env in self.envs]\n  File \"/home/polaris/miniconda3/envs/task/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 375, in <listcomp>\n    [env.close() for env in self.envs]\n  File \"/home/polaris/miniconda3/envs/task/lib/python3.10/site-packages/gymnasium/core.py\", line 341, in close\n    return self.env.close()\n  File \"/home/polaris/miniconda3/envs/task/lib/python3.10/site-packages/gymnasium/core.py\", line 341, in close\n    return self.env.close()\n  File \"/home/polaris/miniconda3/envs/task/lib/python3.10/site-packages/gymnasium/core.py\", line 341, in close\n    return self.env.close()\n  File \"/home/polaris/miniconda3/envs/task/lib/python3.10/site-packages/wandb/integration/gym/__init__.py\", line 75, in close\n    if not self.enabled:\nAttributeError: 'RecordVideo' object has no attribute 'enabled'\n```",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bonnie Shen commented: \nHello @glitchyordis! Thanks for writing to us! Hope you are having a lovely day!\n\nCould you please share me your MINIMAL code snippet of where you are running into this error?\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bonnie Shen commented: \nHello @glitchyordis. Since we haven’t heard back, I’ll go ahead and close this ticket for now. If you experience this issue again, please feel free to ping me here and I’ll be more than happy to keep investigating. Thanks!\n"
      }
    ]
  },
  {
    "issue_number": 7377,
    "title": "[Feature]: Better documentation for what W&B does when logging a PyTorch tensor as a PIL Image",
    "author": "RylanSchaeffer",
    "state": "closed",
    "created_at": "2024-04-14T04:17:49Z",
    "updated_at": "2025-06-03T23:00:32Z",
    "labels": [
      "c:docs",
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Description\r\n\r\nThis is W&B's documentation for Images: https://docs.wandb.ai/ref/python/data-types/image\r\n\r\nThere are two cryptic sentences: \"Note : When logging a torch.Tensor as a wandb.Image, images are normalized. If you do not want to normalize your images, please convert your tensors to a PIL Image.\"\r\n\r\n\r\n### Suggested Solution\r\n\r\nPlease say a little more about what this means. When you say images are normalized, what specifically is meant? Subtract the mean or divide by the maximum? Is the mean/maximum calculated per channel or over all three channels? Do we also divided by the standard deviations i.e. is this really standardization? Additionally, if we pass a numpy array instead of a torch tensor, is this normalization still applied?\r\n\r\nMore documentation about this point would be appreciated! I just wasted a lot of compute saving images that weren't the images I thought they were :cry: \r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "JoanaMarieL",
        "body": "Hello @RylanSchaeffer , thank you for your suggestion we appreciate this. I already created a feature request and tagged our Docs team to check it. We'll provide an update for any progress about it."
      },
      {
        "user": "TonyPolich",
        "body": "Agreed!  What is particularly unclear is if the images are normalised on a per image basis or per batch.  "
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "WandB Internal User commented: \nJoanaMarieL commented: \nHello @RylanSchaeffer , thank you for your suggestion we appreciate this. I already created a feature request and tagged our Docs team to check it. We'll provide an update for any progress about it.\n"
      }
    ]
  },
  {
    "issue_number": 9952,
    "title": "[Feature]: Accessibility Improvement: Option to disable tooltips",
    "author": "pstahlhofen",
    "state": "open",
    "created_at": "2025-06-02T07:52:04Z",
    "updated_at": "2025-06-03T08:33:23Z",
    "labels": [
      "ty:feature"
    ],
    "body": "### Description\n\nAs long as tooltips are automatically shown directly at the cursor when hovering over a plot, charts can be very difficult to parse for people who use screen magnification software. The reason for this is that the hovering tooltip gets in the way of the curve one is actually trying to see. I would like to view tensorbaord logs using screen-zoom without having to worry about tooltips getting in my way\n\n### Suggested Solution\n\nPlease implement an option to disable tooltips, or alternatively an option that will only show the tooltip window once the user *clicks* on the corresponding point in the curve. This would make the software much easier to use for visually impaired people. Thank you :)\n\n",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @pstahlhofen, Thanks for flagging this, definitely seems like a useful feature, as you pointed out. I’ve gone ahead and submitted a request internally and will keep you in the loop as things progress.\n\nIn the meantime, if you’re able to share a quick screen recording that helps us visualize the issue, that would be super helpful!"
      },
      {
        "user": "pstahlhofen",
        "body": "Thanks for the quick replay @aajais\nPlease find a short screen recording attached. In terms of priority, fixing #9953 is a little more pressing for me, personally. I think the video snippet also illustrates how the automatic hover-highlight does not play very nicely with zoom\n\nhttps://github.com/user-attachments/assets/29104a38-2070-4a16-a5bc-27ef707cd2ac"
      }
    ]
  },
  {
    "issue_number": 736,
    "title": "Feature request: min/max values for logged metrics",
    "author": "depthwise",
    "state": "closed",
    "created_at": "2020-01-04T01:15:11Z",
    "updated_at": "2025-06-03T07:59:44Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "Thank you for developing such a useful service. \r\n\r\nAs a practitioner I care disproportionately about the _peak_ metrics in any given run. I.e. max mAP50 for object detection (and min classification/localization losses) or peak mIoU for segmentation. I rarely, if ever, care what the metrics are at the last step of the training run, per se. Things begin to overfit by then anyway.\r\n\r\nW&B doesn't seem to offer a way to derive such min/max in the experiments table, or indeed anywhere its UI as far as I can tell. It'd be great (and probably not too complicated) to add this feature.\r\n\r\nThanks!",
    "comments": [
      {
        "user": "issue-label-bot[bot]",
        "body": "Issue-Label Bot is automatically applying the label `enhancement` to this issue, with a confidence of 0.87. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/wandb/client) and [code](https://github.com/hamelsmu/MLapp) for this bot."
      },
      {
        "user": "vanpelt",
        "body": "Hey @depthwise, thanks for the feature request.  One thing you can do immediately is log a separate metric that only updates when your metric increases or decreases, i.e. \"min_loss\".  What framework are you currently using?"
      },
      {
        "user": "ayushjaiswal",
        "body": "Yes, this feature would be very helpful! The parallel coordinate and parameter importance plots do not make much sense if \"best\" values are not used to generate them. So, it would be nice to have or be able to generate simple derived metrics like min, mean, max, etc."
      }
    ]
  },
  {
    "issue_number": 9954,
    "title": "[Q]: Unable to log TPU system metrics",
    "author": "nguyenhoanglienson",
    "state": "open",
    "created_at": "2025-06-02T11:14:32Z",
    "updated_at": "2025-06-03T07:50:59Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\nHi!\nI'm trying to train a model on TPU using JAX. Everything works fine, no errors, and logging works flawlessly. However, I couldn't see the devices' metrics being logged (memory usage, utilization,...), although the docs say WandB will automatically log those information in the system section. Any idea?\nThanks!\n\nI'm using Kaggle's TPU v3-8 VM, here's my WandB system section:\n\n<img width=\"880\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0c1095ef-38bb-4983-9a14-9fe96b0afab5\" />\n\n<img width=\"877\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/2878625c-c403-40aa-8f6e-1a271a3a0d12\" />",
    "comments": [
      {
        "user": "dmitryduev",
        "body": "Hey @nguyenhoanglienson, what version of wandb and jax are you using?"
      },
      {
        "user": "nguyenhoanglienson",
        "body": "I installed the latest packages with\n`pip install jax[tpu] wandb`\nSo I believe jax's version is 0.6.1 and wandb's 0.19.11, according to the PyPI pages."
      },
      {
        "user": "dmitryduev",
        "body": "Do you have an example run you can share? Can you provide the logs (preferably running with WANDB_DEBUG=true)?\nWhat is the version of libtpu? Have you tried running the tpu-info tool? Can you share its output?\n\nAs far as I can tell, things seem to work OK with the latest jax==0.6.1 jaxlib==0.6.1 & libtpu==0.0.15, tried in Colab with wandb==0.19.11, albeit on v2-8's."
      }
    ]
  },
  {
    "issue_number": 4929,
    "title": "[CLI]: wandb.finish() stuck when uploaded all data",
    "author": "imoneoi",
    "state": "closed",
    "created_at": "2023-02-08T08:25:39Z",
    "updated_at": "2025-06-03T04:20:56Z",
    "labels": [
      "c:sdk:sync",
      "c:sdk:internal-process",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n<!--- Description of the issue below  -->\r\n\r\nWhen running a training loop multiple times and calling `wandb.finish()` after each run, although it shows that all data is uploaded, the program is still stuck for a very long time.\r\n\r\n<!--- A minimal code snippet between the quotes below  -->\r\n```python\r\ndef run_multiple_times():\r\n    while True:\r\n        wandb.init(reinit=True, ...)\r\n        # training code...\r\n        wandb.finish()\r\n\r\n\r\n```\r\n\r\n<!--- A full traceback of the exception in the quotes below -->\r\n```shell\r\nwandb: Waiting for W&B process to finish... (success).\r\nwandb: | 20.180 MB of 20.180 MB uploaded (0.000 MB deduped)\r\n```\r\n\n\n### Additional Files\n\n_No response_\n\n### Environment\n\nWandB version: `0.13.9`\r\n\r\nOS: `5.4.0-135-generic #152-Ubuntu`\r\n\r\nPython version: `3.10.9`\r\n\r\nVersions of relevant libraries:\r\n\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "MBakirWB",
        "body": "Hi @imoneoi, happy to help. Could you please provide the following:\r\n- `debug.log` and `debug-internal.log` for the handing runs\r\n- How many runs are you executing? \r\n- Are you uploading any files/creating artifacts with these runs? If yes what is their approximate size?"
      },
      {
        "user": "tfriedel",
        "body": "I'm also seeing this issue. It doesn't happen everytime, but quite frequently. I have to kill wandb-service to make it disappear. \r\nI was running about 20 runs sequentially and it would happen in maybe 20% of them?\r\nFiles being uploaded are rather small, like 250kb i.e. only code diffs, no models. Network connection is also pretty good (datacenter with gigabit+). This happened with the timm training pipeline when just enabling the wandb feature. \r\nI can send you log files privately if you tell me where to. "
      },
      {
        "user": "imoneoi",
        "body": "I've found a temporary solution by using `wandb.init(reinit=True, ...)` and not calling `wandb.finish()` after a run.\r\n\r\n```python\r\ndef run_multiple_times():\r\n    while True:\r\n        wandb.init(reinit=True, ...)\r\n        # training code...\r\n        # wandb.finish()\r\n```\r\n\r\nMy observation is similar to @tfriedel , about 20% of the runs will stuck.\r\n\r\n- I started 10 runs in parallel (by launching different processes from terminal)\r\n- Each run will upload 20 files with ~1MB each"
      }
    ]
  },
  {
    "issue_number": 9953,
    "title": "[Feature]: Accessibility Improvement: Option to disable hover-highlighting",
    "author": "pstahlhofen",
    "state": "open",
    "created_at": "2025-06-02T08:05:34Z",
    "updated_at": "2025-06-02T20:38:07Z",
    "labels": [
      "ty:feature"
    ],
    "body": "### Description\n\nWhen hovering over a plot, the curve that is currently highlighted can change with tiny movements of the cursor. This behaviour can be stressful to the eye, in particular for people with visual impairments.\n\n### Suggested Solution\n\nPlease implement an option to disable hover-highlighting. Alternatively, it would be helpful to have the option of temporarily \"pinning\" one of the curves, by clicking on it in order to make it highlighted until one clicks elsewhere.\n",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @pstahlhofen, Thanks for flagging this, definitely seems like a useful feature, as you pointed out. I’ve gone ahead and submitted a request internally and will keep you in the loop as things progress.\n\nIn the meantime, if you’re able to share a quick screen recording that helps us visualize the issue, that would be super helpful!"
      }
    ]
  },
  {
    "issue_number": 9587,
    "title": "[Bug]: wandb save not working",
    "author": "nikonikolov",
    "state": "open",
    "created_at": "2025-03-17T08:13:56Z",
    "updated_at": "2025-06-02T19:14:32Z",
    "labels": [
      "ty:bug",
      "a:sdk",
      "c:sdk:save"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\n`wandb.save` doesn't seem to upload files for any of my runs. My code looks like (for several different files)\n```python\nwandb.save(str(self.session_directory.full_config_path), policy='now')\n```\nbut none of them seem to be uploaded for any of my runs.\n\nwandb version: 0.17.9\nOS: Ubuntu 22.04\nPython: 3.10",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @nikonikolov, Thanks for reaching out! Could you share a link to an example run where you encountered this issue? It would also be really helpful if you could provide the three SDK debug logs, which you can find in `./wandb/run-{date_time}-{run_id}/logs` on the machine where you ran your code. I noticed you’re using an older version of our SDK. Can you try upgrading to the latest version?\n\n"
      },
      {
        "user": "aajais",
        "body": "@nikonikolov, just wanted to follow up on this Thanks!"
      },
      {
        "user": "nikonikolov",
        "body": "Hey, sorry for late reply. Link to a run https://wandb.ai/nbnikolov/diffusion_vlams/runs/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio\n\n`debug.log`:\n```\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_setup.py:_flush():77] Current SDK version is 0.17.9\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_setup.py:_flush():77] Configure stats pid to 1035367\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_setup.py:_flush():77] Loading settings from /home/niko/.config/wandb/settings\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_setup.py:_flush():77] Loading settings from /home/niko/Workspace/barrel/wandb/settings\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_setup.py:_flush():77] Loading settings from environment variables: {'api_key': '***REDACTED***'}\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_setup.py:_flush():77] Applying setup settings: {'_disable_service': False}\n2025-03-21 00:29:07,203 WARNING MainThread:1035367 [wandb_setup.py:_flush():77] Could not save program above cwd: /scratch/niko/.cache/bazel/_bazel_niko/bd560e0defb87030a1fbbd7d7fbab0b7/execroot/barrel/bazel-out/k8-opt/bin/barrel/pipes/vlams/torchrun.runfiles/barrel/barrel/components/scripts/train/train.py\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_setup.py:_flush():77] Inferring run settings from compute environment: {'program_relpath': None, 'program_abspath': '/scratch/niko/.cache/bazel/_bazel_niko/bd560e0defb87030a1fbbd7d7fbab0b7/execroot/barrel/bazel-out/k8-opt/bin/barrel/pipes/vlams/torchrun.runfiles/barrel/barrel/components/scripts/train/train.py', 'program': '/scratch/niko/.cache/bazel/_bazel_niko/bd560e0defb87030a1fbbd7d7fbab0b7/execroot/barrel/bazel-out/k8-opt/bin/barrel/pipes/vlams/torchrun.runfiles/barrel/barrel/components/scripts/train/train.py'}\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_setup.py:_flush():77] Applying login settings: {}\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_init.py:_log_setup():524] Logging user logs to /home/niko/experiments/vlams/control/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/wandb/run-20250321_002907-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/logs/debug.log\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_init.py:_log_setup():525] Logging internal logs to /home/niko/experiments/vlams/control/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/wandb/run-20250321_002907-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/logs/debug-internal.log\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_init.py:init():608] calling init triggers\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_init.py:init():615] wandb.init called with sweep_config: {}\nconfig: {}\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_init.py:init():658] starting backend\n2025-03-21 00:29:07,203 INFO    MainThread:1035367 [wandb_init.py:init():662] setting up manager\n2025-03-21 00:29:07,205 INFO    MainThread:1035367 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=fork,spawn,forkserver, using: spawn\n2025-03-21 00:29:07,208 INFO    MainThread:1035367 [wandb_init.py:init():670] backend started and connected\n2025-03-21 00:29:07,216 INFO    MainThread:1035367 [wandb_init.py:init():768] updated telemetry\n2025-03-21 00:29:07,377 INFO    MainThread:1035367 [wandb_init.py:init():801] communicating run to backend with 90.0 second timeout\n2025-03-21 00:29:07,838 INFO    MainThread:1035367 [wandb_init.py:init():852] starting run threads in backend\n2025-03-21 00:29:09,640 INFO    MainThread:1035367 [wandb_run.py:_console_start():2465] atexit reg\n2025-03-21 00:29:09,640 INFO    MainThread:1035367 [wandb_run.py:_redirect():2311] redirect: wrap_raw\n2025-03-21 00:29:09,640 INFO    MainThread:1035367 [wandb_run.py:_redirect():2376] Wrapping output streams.\n2025-03-21 00:29:09,640 INFO    MainThread:1035367 [wandb_run.py:_redirect():2401] Redirects installed.\n2025-03-21 00:29:09,642 INFO    MainThread:1035367 [wandb_init.py:init():895] run started, returning control to user process\n2025-03-21 14:12:22,908 INFO    MainThread:1035367 [wandb_run.py:_finish():2162] finishing run nbnikolov/diffusion_vlams/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio\n2025-03-21 14:12:22,909 INFO    MainThread:1035367 [wandb_run.py:_atexit_cleanup():2426] got exitcode: 0\n2025-03-21 14:12:22,910 INFO    MainThread:1035367 [wandb_run.py:_restore():2408] restore\n2025-03-21 14:12:22,910 INFO    MainThread:1035367 [wandb_run.py:_restore():2414] restore done\n2025-03-21 14:12:22,910 INFO    MainThread:1035367 [wandb_run.py:_on_finish():2679] communicating current version\n2025-03-21 14:12:22,950 INFO    MainThread:1035367 [wandb_run.py:_on_finish():2688] got version response upgrade_message: \"wandb version 0.19.8 is available!  To upgrade, please run:\\n $ pip install wandb --upgrade\"\n\n2025-03-21 14:12:30,199 INFO    MainThread:1035367 [wandb_run.py:_footer_history_summary_info():4088] rendering history\n2025-03-21 14:12:30,201 INFO    MainThread:1035367 [wandb_run.py:_footer_history_summary_info():4120] rendering summary\n2025-03-21 14:12:30,206 INFO    MainThread:1035367 [wandb_run.py:_footer_sync_info():4047] logging synced files\n```\n\nRelevant part of `debug-internal.log`. It says some files should be uploaded, but they aren't present at the link\n```\n2025-03-21 00:29:09,679 DEBUG   SenderThread:1036780 [sender.py:send():391] send: telemetry\n2025-03-21 00:29:09,685 DEBUG   SenderThread:1036780 [sender.py:send():391] send: telemetry\n2025-03-21 00:29:09,686 DEBUG   SenderThread:1036780 [sender.py:send():391] send: files\n2025-03-21 00:29:09,686 INFO    SenderThread:1036780 [sender.py:_save_file():1466] saving file sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/full_config.yaml with policy now\n2025-03-21 00:29:09,686 DEBUG   SenderThread:1036780 [sender.py:send():391] send: files\n2025-03-21 00:29:09,686 INFO    SenderThread:1036780 [sender.py:_save_file():1466] saving file sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/config.yaml with policy now\n2025-03-21 00:29:09,686 DEBUG   SenderThread:1036780 [sender.py:send():391] send: files\n2025-03-21 00:29:09,686 INFO    SenderThread:1036780 [sender.py:_save_file():1466] saving file sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/git.diff with policy now\n2025-03-21 00:29:09,686 DEBUG   SenderThread:1036780 [sender.py:send():391] send: files\n2025-03-21 00:29:09,686 INFO    SenderThread:1036780 [sender.py:_save_file():1466] saving file sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/git.hash with policy now\n2025-03-21 00:29:09,687 DEBUG   SenderThread:1036780 [sender.py:send():391] send: files\n2025-03-21 00:29:09,687 INFO    SenderThread:1036780 [sender.py:_save_file():1466] saving file sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/git.branch with policy now\n2025-03-21 00:29:09,850 INFO    Thread-12 :1036780 [dir_watcher.py:_on_file_created():271] file/dir created: /home/niko/experiments/vlams/control/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/wandb/run-202\n50321_002907-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/files/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/full_config.yaml\n2025-03-21 00:29:09,851 INFO    Thread-12 :1036780 [dir_watcher.py:_on_file_created():271] file/dir created: /home/niko/experiments/vlams/control/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/wandb/run-202\n50321_002907-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/files/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/git.branch\n2025-03-21 00:29:09,851 INFO    Thread-12 :1036780 [dir_watcher.py:_on_file_created():271] file/dir created: /home/niko/experiments/vlams/control/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/wandb/run-202\n50321_002907-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/files/output.log\n2025-03-21 00:29:09,851 INFO    Thread-12 :1036780 [dir_watcher.py:_on_file_created():271] file/dir created: /home/niko/experiments/vlams/control/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/wandb/run-202\n50321_002907-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/files/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/config.yaml\n2025-03-21 00:29:09,851 INFO    Thread-12 :1036780 [dir_watcher.py:_on_file_created():271] file/dir created: /home/niko/experiments/vlams/control/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/wandb/run-202\n50321_002907-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/files/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/git.hash\n2025-03-21 00:29:09,851 INFO    Thread-12 :1036780 [dir_watcher.py:_on_file_created():271] file/dir created: /home/niko/experiments/vlams/control/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/wandb/run-202\n50321_002907-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/files/requirements.txt\n2025-03-21 00:29:09,851 INFO    Thread-12 :1036780 [dir_watcher.py:_on_file_created():271] file/dir created: /home/niko/experiments/vlams/control/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/wandb/run-202\n50321_002907-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/files/wandb-metadata.json\n2025-03-21 00:29:09,851 INFO    Thread-12 :1036780 [dir_watcher.py:_on_file_created():271] file/dir created: /home/niko/experiments/vlams/control/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/wandb/run-202\n50321_002907-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/files/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/git.diff\n2025-03-21 00:29:09,851 INFO    Thread-12 :1036780 [dir_watcher.py:_on_file_created():271] file/dir created: /home/niko/experiments/vlams/control/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/wandb/run-202\n50321_002907-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/files/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio\n2025-03-21 00:29:09,862 INFO    wandb-upload_0:1036780 [upload_job.py:push():130] Uploaded file /tmp/tmpej9_m80uwandb/hjnxv20w-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/full_config.yaml\n2025-03-21 00:29:09,939 INFO    wandb-upload_3:1036780 [upload_job.py:push():130] Uploaded file /tmp/tmpej9_m80uwandb/fur2c7kw-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/git.hash\n2025-03-21 00:29:09,946 INFO    wandb-upload_1:1036780 [upload_job.py:push():130] Uploaded file /tmp/tmpej9_m80uwandb/y71tap8z-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/config.yaml\n2025-03-21 00:29:09,951 INFO    wandb-upload_2:1036780 [upload_job.py:push():130] Uploaded file /tmp/tmpej9_m80uwandb/fypn2g1f-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/git.diff\n2025-03-21 00:29:09,966 INFO    wandb-upload_4:1036780 [upload_job.py:push():130] Uploaded file /tmp/tmpej9_m80uwandb/9k03585f-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/git.branch\n2025-03-21 00:29:11,858 INFO    Thread-12 :1036780 [dir_watcher.py:_on_file_modified():288] file/dir modified: /home/niko/experiments/vlams/control/sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/wandb/run-2\n0250321_002907-sess_2025_03_20_23_04_06_8f48e83b-05_niko_pizero_fm_1s_quat_depth_norm_ratio/files/output.log\n```"
      }
    ]
  },
  {
    "issue_number": 9865,
    "title": "[Bug]: `wandb.init(mode=\"offline\")` times out after a failed online `init` attempt",
    "author": "andrewendlinger",
    "state": "open",
    "created_at": "2025-05-16T00:02:44Z",
    "updated_at": "2025-06-02T18:16:59Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nIf `wandb.init()` in online mode fails (e.g., due to network issues), a following attempt to initialize wandb with `wandb.init(mode=\"offline\")` also results in a timeout. This prevents a reliable fallback to offline logging.\n\n\n#### Minimal Reproducible Example:\n```python\nimport random\nimport time\n\nimport wandb\n\n# --- Configuration ---\nWANDB_PROJECT_NAME = \"my-robust-project\"\nWANDB_ENTITY = None \n\n# --- Attempt Online Initialization, Fallback to Offline (defaul timeout is 90s) ---\nwandb_run = None\ntry:\n    print(\"Attempting to initialize W&B in online mode...\")\n    wandb_run = wandb.init(\n        project=WANDB_PROJECT_NAME,\n        entity=WANDB_ENTITY,\n        config={\"learning_rate\": 0.01, \"epochs\": 5, \"architecture\": \"CNN_robust\"},\n    )\nexcept Exception as e:\n    print(f\"Failed to initialize W&B in online mode: {e}\")\n    wandb.finish() # does not help...\n    print(\"Falling back to W&B offline mode.\")\n    try:\n        wandb_run = wandb.init(\n            mode=\"offline\",\n            project=WANDB_PROJECT_NAME,\n            entity=WANDB_ENTITY,\n            config={\"learning_rate\": 0.01, \"epochs\": 5, \"architecture\": \"CNN_robust\"},\n        )\n    except Exception as offline_e:\n        print(f\"Failed to initialize W&B even in offline mode: {offline_e}\")\n        print(\"W&B logging will be disabled for this run.\")\n        wandb_run = None  # Ensure wandb_run is None if all attempts fail\n\n\n# --- Some Training Logic ---\nif wandb_run:\n    print(\n        f\"Starting training with W&B run: {wandb_run.name if wandb_run.name else 'Offline Run'}\"\n    )\n    for epoch in range(wandb.config.epochs):\n        time.sleep(0.5)  # Simulate work\n        loss = 0.2 + (0.8 - 0.2) * (1 - epoch / wandb.config.epochs) * random.random()\n        acc = 0.7 + (0.7 - 0.3) * (1 - epoch / wandb.config.epochs) * random.random()\n\n        wandb.log({\"epoch\": epoch, \"loss\": loss, \"accuracy\": acc})\n        print(\n            f\"Epoch {epoch + 1}/{wandb.config.epochs} | Loss: {loss:.4f} | Accuracy: {acc:.4f}\"\n        )\n\n    print(\"Training finished.\")\n    wandb_run.finish()\n    print(\"W&B run finished.\")\nelse:\n    print(\"W&B was not initialized. Proceeding without W&B logging.\")\n    for epoch in range(5):\n        time.sleep(0.5)\n        loss = 0.2 + (0.8 - 0.2) * (1 - epoch / 5) * random.random()\n        acc = 0.7 + (0.7 - 0.3) * (1 - epoch / 5) * random.random()\n        print(\n            f\"Epoch {epoch + 1}/5 | Loss: {loss:.4f} | Accuracy: {acc:.4f} (W&B Disabled)\"\n        )\n    print(\"Training finished (W&B Disabled).\")\n```\n\n#### Output:\n_Note: Interestingly, I have to `KeyboardInterrupt` to get the script to exit._\n```bash\n❯ python ./tests/test_wandb.py\nAttempting to initialize W&B in online mode...\nwandb: Network error (ConnectionError), entering retry loop.\nwandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\nwandb: Network error (ConnectionError), entering retry loop.\nFailed to initialize W&B in online mode: Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.\nFalling back to W&B offline mode.\nFailed to initialize W&B even in offline mode: Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.\nW&B logging will be disabled for this run.\nW&B was not initialized. Proceeding without W&B logging.\nEpoch 1/5 | Loss: 0.7459 | Accuracy: 0.7102 (W&B Disabled)\nEpoch 2/5 | Loss: 0.3245 | Accuracy: 0.7120 (W&B Disabled)\nEpoch 3/5 | Loss: 0.4811 | Accuracy: 0.7656 (W&B Disabled)\nEpoch 4/5 | Loss: 0.3539 | Accuracy: 0.7960 (W&B Disabled)\nEpoch 5/5 | Loss: 0.2675 | Accuracy: 0.7355 (W&B Disabled)\nTraining finished (W&B Disabled).\n^CException ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x110da4310>\nTraceback (most recent call last):\n  File \"/Users/andre/PhD/courses/ULF-EnC/.venv/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py\", line 94, in teardown_atexit\n    conn.teardown(hooks.exit_code)\n  File \"/Users/andre/PhD/courses/ULF-EnC/.venv/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py\", line 236, in teardown\n    return self._proc.join()\n  File \"/Users/andre/PhD/courses/ULF-EnC/.venv/lib/python3.10/site-packages/wandb/sdk/service/service.py\", line 251, in join\n    ret = self._internal_proc.wait()\n  File \"/Users/andre/.local/share/uv/python/cpython-3.10.16-macos-aarch64-none/lib/python3.10/subprocess.py\", line 1209, in wait\n    return self._wait(timeout=timeout)\n  File \"/Users/andre/.local/share/uv/python/cpython-3.10.16-macos-aarch64-none/lib/python3.10/subprocess.py\", line 1959, in _wait\n    (pid, sts) = self._try_wait(0)\n  File \"/Users/andre/.local/share/uv/python/cpython-3.10.16-macos-aarch64-none/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n    (pid, sts) = os.waitpid(self.pid, wait_flags)\nKeyboardInterrupt: \n```\n\n\n| wandb  | python |     OS               |\n|--------|--------|---------------|\n| 0.19.11 | 3.10.16 | macOS 14.7.5 |",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nHi there! Thanks so much for reporting this. Let me dig into this and get it in front of the team. I'll let keep you posted here!\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nHi there! Thanks for the detailed bug report and incredible patience. I've reviewed your issue and the reproduction code, and this definitely looks like a legitimate bug. The behavior you're experiencing - where an offline `wandb.init()` times out after a failed online attempt - shouldn't happen. Offline mode should work independently of any network connectivity issues.\n\nBefore I escalate this to our development team, I'd like to gather a bit more information to help them reproduce and fix this more efficiently:\n\n1. **Consistency**: Does this happen every time you run the script, or does it only occur sometimes?\n2. **Fresh vs. same session**: When you encounter this issue, are you:\n  - Running the script fresh from the command line each time (`python script.py`), or\n  - Running it multiple times within the same Python session (like in a Jupyter notebook or IPython shell)?\n\nYour reproduction examples are super helpful, I just want to make sure we capture the full scope of when this occurs so our engineers can tackle it effectively.\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nHi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved.\n"
      }
    ]
  },
  {
    "issue_number": 9774,
    "title": "[Bug]: Wandb grad logger does not support DTensor parameters",
    "author": "dest1n1s",
    "state": "open",
    "created_at": "2025-04-27T07:34:41Z",
    "updated_at": "2025-06-02T18:16:09Z",
    "labels": [
      "ty:feature",
      "a:sdk",
      "c:sdk:watch"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\nWhen using\n\n```python\nwandb_logger.watch(model, log=\"all\")\n```\n\nwith leaf tensors being `torch.DTensor` (e.g. using `torch.distributed.tensor.parallel.parallelize_module`), it comes out with the following error:\n\n```plaintext\nNotImplementedError: Operator aten.histc.default does not have a sharding strategy registered.\n```\n\nI'm using the following environment:\n\n```plaintext\nPython version: 3.12\nWandb version: 0.19.10\nOperating System: Ubuntu 22.04 LTS\n```",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nHi there! Thanks so much for reporting. Let me rerpo this on my end. I'll keep you posted here if we end up making an internal ticket for this or with whatever I find out 🙂\n"
      },
      {
        "user": "NileZhou",
        "body": "That's a big problem, I am using fsdp2 then meet error because this issue"
      }
    ]
  },
  {
    "issue_number": 9646,
    "title": "[Bug]: No wandb-summary.json generated and empty output.log after offline run",
    "author": "chufanchen",
    "state": "open",
    "created_at": "2025-03-27T08:18:35Z",
    "updated_at": "2025-06-02T16:39:56Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n- Wandb verison: 0.19.8\n- Python verison: 3.9\n\nMinimal example:\n\n```python\nimport os\nimport time\nimport wandb\n\nrun = wandb.init(\n    mode=\"offline\",\n    project=\"test-offline-project\",\n    name=\"offline-debug-run\",\n)\n\n# Log some dummy metrics\nfor step in range(5):\n    wandb.log({\n        \"loss\": 1.0 / (step + 1),\n        \"accuracy\": step * 0.1\n    })\n    print(f\"Logged step {step}\")\n    time.sleep(0.1)\n\nwandb.finish()\n```\nAfter run this code:\n\n```console\npython test.py\n```\n\nI got empty `output.log` in `wandb/offline-run-xxxxx/`. Also `wandb-summary.json` is not generated like online mode.",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @chufanchen! Thank you for writing in. \n\nVery interesting that you are seeing this. I just ran your code, and I am seeing the output and the summary files. \n\nCould you please provide the debug.log and debug-internal.log files associated with the run where you are running into this issue? These files should be located in the wandb folder relative to your working directory. "
      },
      {
        "user": "chufanchen",
        "body": "> Hey [@chufanchen](https://github.com/chufanchen)! Thank you for writing in.\n> \n> Very interesting that you are seeing this. I just ran your code, and I am seeing the output and the summary files.\n> \n> Could you please provide the debug.log and debug-internal.log files associated with the run where you are running into this issue? These files should be located in the wandb folder relative to your working directory.\n\n\n\nThanks for the quick reply, @ArtsiomWB.\n \nI’ve attached the debug.log and debug-internal.log files from the run where I encountered the issue.\n\nI wonder if it's possible that there's a difference in how WandB behaves when running in offline mode on an online server versus on a completely offline server. It's just a possibility I'm considering.\n \n[offline-run-20250328_052127-dis3duzl.zip](https://github.com/user-attachments/files/19499376/offline-run-20250328_052127-dis3duzl.zip)\n\n### debug.log\n\n```\n2025-03-28 05:21:27,271 INFO    MainThread:1242465 [wandb_setup.py:_flush():67] Current SDK version is 0.19.8\n2025-03-28 05:21:27,271 INFO    MainThread:1242465 [wandb_setup.py:_flush():67] Configure stats pid to 1242465\n2025-03-28 05:21:27,271 INFO    MainThread:1242465 [wandb_setup.py:_flush():67] Loading settings from /home/zju/.config/wandb/settings\n2025-03-28 05:21:27,271 INFO    MainThread:1242465 [wandb_setup.py:_flush():67] Loading settings from /home/zju/QT-main/wandb/settings\n2025-03-28 05:21:27,272 INFO    MainThread:1242465 [wandb_setup.py:_flush():67] Loading settings from environment variables\n2025-03-28 05:21:27,272 INFO    MainThread:1242465 [wandb_init.py:setup_run_log_directory():647] Logging user logs to /home/zju/QT-main/wandb/offline-run-20250328_052127-dis3duzl/logs/debug.log\n2025-03-28 05:21:27,272 INFO    MainThread:1242465 [wandb_init.py:setup_run_log_directory():648] Logging internal logs to /home/zju/QT-main/wandb/offline-run-20250328_052127-dis3duzl/logs/debug-internal.log\n2025-03-28 05:21:27,272 INFO    MainThread:1242465 [wandb_init.py:init():761] calling init triggers\n2025-03-28 05:21:27,272 INFO    MainThread:1242465 [wandb_init.py:init():766] wandb.init called with sweep_config: {}\nconfig: {'_wandb': {}}\n2025-03-28 05:21:27,272 INFO    MainThread:1242465 [wandb_init.py:init():784] starting backend\n2025-03-28 05:21:27,484 INFO    MainThread:1242465 [wandb_init.py:init():788] sending inform_init request\n2025-03-28 05:21:27,494 INFO    MainThread:1242465 [backend.py:_multiprocessing_setup():101] multiprocessing start_methods=fork,spawn,forkserver, using: spawn\n2025-03-28 05:21:27,494 INFO    MainThread:1242465 [wandb_init.py:init():798] backend started and connected\n2025-03-28 05:21:27,496 INFO    MainThread:1242465 [wandb_init.py:init():891] updated telemetry\n2025-03-28 05:21:27,496 INFO    MainThread:1242465 [wandb_init.py:init():915] communicating run to backend with 90.0 second timeout\n2025-03-28 05:21:27,608 INFO    MainThread:1242465 [wandb_init.py:init():990] starting run threads in backend\n2025-03-28 05:21:27,709 INFO    MainThread:1242465 [wandb_run.py:_console_start():2375] atexit reg\n2025-03-28 05:21:27,709 INFO    MainThread:1242465 [wandb_run.py:_redirect():2227] redirect: wrap_raw\n2025-03-28 05:21:27,709 INFO    MainThread:1242465 [wandb_run.py:_redirect():2292] Wrapping output streams.\n2025-03-28 05:21:27,709 INFO    MainThread:1242465 [wandb_run.py:_redirect():2315] Redirects installed.\n2025-03-28 05:21:27,710 INFO    MainThread:1242465 [wandb_init.py:init():1032] run started, returning control to user process\n2025-03-28 05:21:28,214 INFO    MainThread:1242465 [wandb_run.py:_finish():2112] finishing run test-offline-project/dis3duzl\n2025-03-28 05:21:28,214 INFO    MainThread:1242465 [wandb_run.py:_atexit_cleanup():2340] got exitcode: 0\n2025-03-28 05:21:28,214 INFO    MainThread:1242465 [wandb_run.py:_restore():2322] restore\n2025-03-28 05:21:28,215 INFO    MainThread:1242465 [wandb_run.py:_restore():2328] restore done\n2025-03-28 05:21:28,219 INFO    MainThread:1242465 [wandb_run.py:_footer_history_summary_info():3956] rendering history\n2025-03-28 05:21:28,219 INFO    MainThread:1242465 [wandb_run.py:_footer_history_summary_info():3988] rendering summary\n```\n\n### debug-internal.log\n```\n{\"time\":\"2025-03-28T05:21:27.496291626Z\",\"level\":\"INFO\",\"msg\":\"stream: starting\",\"core version\":\"0.19.8\",\"symlink path\":\"/home/zju/QT-main/wandb/offline-run-20250328_052127-dis3duzl/logs/debug-core.log\"}\n{\"time\":\"2025-03-28T05:21:27.606365417Z\",\"level\":\"INFO\",\"msg\":\"created new stream\",\"id\":\"dis3duzl\"}\n{\"time\":\"2025-03-28T05:21:27.606406643Z\",\"level\":\"INFO\",\"msg\":\"stream: started\",\"id\":\"dis3duzl\"}\n{\"time\":\"2025-03-28T05:21:27.606435737Z\",\"level\":\"INFO\",\"msg\":\"writer: Do: started\",\"stream_id\":\"dis3duzl\"}\n{\"time\":\"2025-03-28T05:21:27.606451005Z\",\"level\":\"INFO\",\"msg\":\"handler: started\",\"stream_id\":\"dis3duzl\"}\n{\"time\":\"2025-03-28T05:21:27.606453169Z\",\"level\":\"INFO\",\"msg\":\"sender: started\",\"stream_id\":\"dis3duzl\"}\n{\"time\":\"2025-03-28T05:21:27.612953498Z\",\"level\":\"INFO\",\"msg\":\"Starting system monitor\"}\n{\"time\":\"2025-03-28T05:21:28.215373809Z\",\"level\":\"INFO\",\"msg\":\"Stopping system monitor\"}\n{\"time\":\"2025-03-28T05:21:28.215971364Z\",\"level\":\"INFO\",\"msg\":\"Stopped system monitor\"}\n{\"time\":\"2025-03-28T05:21:28.216665657Z\",\"level\":\"INFO\",\"msg\":\"handler: operation stats\",\"stats\":{}}\n{\"time\":\"2025-03-28T05:21:28.220270623Z\",\"level\":\"INFO\",\"msg\":\"stream: closing\",\"id\":\"dis3duzl\"}\n{\"time\":\"2025-03-28T05:21:28.22029105Z\",\"level\":\"INFO\",\"msg\":\"handler: closed\",\"stream_id\":\"dis3duzl\"}\n{\"time\":\"2025-03-28T05:21:28.220301259Z\",\"level\":\"INFO\",\"msg\":\"writer: Close: closed\",\"stream_id\":\"dis3duzl\"}\n{\"time\":\"2025-03-28T05:21:28.220309965Z\",\"level\":\"INFO\",\"msg\":\"sender: closed\",\"stream_id\":\"dis3duzl\"}\n{\"time\":\"2025-03-28T05:21:28.220352744Z\",\"level\":\"INFO\",\"msg\":\"stream: closed\",\"id\":\"dis3duzl\"}\n```\n"
      },
      {
        "user": "laozhanger",
        "body": "I encountered the same problem as you, have you solved it"
      }
    ]
  },
  {
    "issue_number": 9936,
    "title": "[Feature]: Add 'first' to define_metric summary options",
    "author": "alexhh-inceptive",
    "state": "open",
    "created_at": "2025-05-30T12:01:29Z",
    "updated_at": "2025-06-02T14:45:14Z",
    "labels": [
      "ty:feature"
    ],
    "body": "### Description\n\nSometimes (e.g. in finetuning) it's useful to compare performance at step 0 (pretrained model) with later steps. A First summary metric would support this\n\n### Suggested Solution\n\nAdd first to the list of supported summaries in define_metric",
    "comments": [
      {
        "user": "fmamberti-wandb",
        "body": "Hi @alexhh-inceptive , thank you for reaching out with your feature request.\n\nI've now raised this with our product team. \n\nIn the meantime, you could log the metrics at step 0 as separate summary metrics, for example with `wandb.summary['<metric_name>_at_step_0'] = <your_metric_value>` so that you can store the metrics as step 0 and compare them with the metric after finetuning."
      },
      {
        "user": "alexhh-inceptive",
        "body": "sounds good, thanks!"
      }
    ]
  },
  {
    "issue_number": 6526,
    "title": "[CLI]: Problems running Docker image \"wandb/local\"",
    "author": "WilberRojas",
    "state": "closed",
    "created_at": "2023-10-31T17:17:52Z",
    "updated_at": "2025-06-02T12:52:06Z",
    "labels": [
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nHi, I'm having error when I try to upload data to wandb, I can upload to wandb.ai but I get the following error when I change to wandb local:\n<!--- Description of the issue below  -->\n\n<!--- A minimal code snippet between the quotes below  -->\n```Command\npython train.py --data data/dataset_pistol.yaml --weights models/yolov5s.pt --epochs 5 --imgsz 224 --device 0\n```\n\n<!--- A full traceback of the exception in the quotes below -->\n```shell\nroot@80a37e5c44bd:/usr/src/app# python train.py --data data/dataset_pistol.yaml --weights models/yolov5s.pt --epochs 5 --imgsz 224 --device 0\nwandb: W&B API key is configured (use `wandb login --relogin` to force relogin)\ntrain: weights=models/yolov5s.pt, cfg=, data=data/dataset_pistol.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=5, batch_size=16, imgsz=224, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\ngithub: skipping check (Docker image), for updates see https://github.com/ultralytics/yolov5\nYOLOv5 🚀 v6.1-0-g3752807 torch 1.10.2+cu113 CUDA:0 (NVIDIA GeForce RTX 3060, 12044MiB)\n\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\nTensorBoard: Start with 'tensorboard --logdir runs/train', view at http://localhost:6006/\nwandb: wandb version 0.15.12 is available!  To upgrade, please run:\nwandb:  $ pip install wandb --upgrade\nwandb: Tracking run with wandb version 0.12.10\nwandb: Syncing run devilish-sorcery-2\nwandb: ⭐️ View project at http://wandb.cloud.example.com:8080/project123/YOLOv5\nwandb: 🚀 View run at http://wandb.cloud.example.com:8080/project123/YOLOv5/runs/goecjzh1\nwandb: Run data is saved locally in /usr/src/app/wandb/run-20231031_165408-goecjzh1\nwandb: Run `wandb offline` to turn off syncing.\n\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to models/yolov5s.pt...\nwandb: ERROR Error uploading \"wandb-metadata.json\": CommError, <Response [403]>\n 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 13.6M/14.1M [00:02<00:00, 6.57MB/s]\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14.1M/14.1M [00:02<00:00, 6.58MB/s]\nOverriding model.yaml nc=80 with nc=1\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\nModel Summary: 270 layers, 7022326 parameters, 7022326 gradients, 15.8 GFLOPs\n\nTransferred 343/349 items from models/yolov5s.pt\nScaled weight_decay = 0.0005\noptimizer: SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\nalbumentations: Blur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\ntrain: Scanning '/usr/src/app/datasets/dataset-singleClass/labels/train' images and labels...2376 found, 0 missing, 0 empty, 0 corrupt: 100%|███████████| 2376/2376 [00:00<00:00, 23929.58it/s]\ntrain: New cache created: /usr/src/app/datasets/dataset-singleClass/labels/train.cache\nval: Scanning '/usr/src/app/datasets/dataset-singleClass/labels/val' images and labels...595 found, 0 missing, 0 empty, 0 corrupt: 100%|██████████████████| 595/595 [00:00<00:00, 10077.05it/s]\nval: New cache created: /usr/src/app/datasets/dataset-singleClass/labels/val.cache\nPlotting labels to runs/train/exp/labels.jpg... \n\nAutoAnchor: 5.07 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\nImage sizes 224 train, 224 val\nUsing 8 dataloader workers\nLogging results to runs/train/exp\nStarting training for 5 epochs...\n\n     Epoch   gpu_mem       box       obj       cls    labels  img_size\n       0/4    0.487G    0.0987   0.01729         0        22       224: 100%|██████████| 149/149 [00:09<00:00, 15.37it/s]                                                                      \n               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100%|██████████| 19/19 [00:01<00:00, 10.51it/s]                                                        \n                 all        595        639       0.49      0.444      0.385      0.195\n\n     Epoch   gpu_mem       box       obj       cls    labels  img_size\n       1/4    0.606G   0.07626   0.01982         0        36       224:   8%|▊         | 12/149 [00:00<00:07, 17.24it/s]                                                                       wandb: ERROR Error uploading \"media/metadata/boxes2D/BoundingBoxDebugger_0_60.boxes2D.json\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/images/BoundingBoxDebugger_0_2.png\": CommError, <Response [403]>\n       1/4    0.606G   0.07633   0.02017         0        38       224:   9%|▉         | 14/149 [00:00<00:07, 17.09it/s]                                                                       wandb: ERROR Error uploading \"media/images/Mosaics_0_2.jpg\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/metadata/boxes2D/BoundingBoxDebugger_0_40.boxes2D.json\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/images/BoundingBoxDebugger_0_4.png\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/metadata/boxes2D/BoundingBoxDebugger_0_00.boxes2D.json\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/metadata/boxes2D/BoundingBoxDebugger_0_10.boxes2D.json\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/images/BoundingBoxDebugger_0_7.png\": CommError, <Response [403]>\n       1/4    0.606G   0.07609   0.02003         0        33       224:   9%|▉         | 14/149 [00:00<00:07, 17.09it/s]                                                                       wandb: ERROR Error uploading \"media/images/BoundingBoxDebugger_0_0.png\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/images/BoundingBoxDebugger_0_3.png\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/metadata/boxes2D/BoundingBoxDebugger_0_20.boxes2D.json\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/images/BoundingBoxDebugger_0_5.png\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/images/BoundingBoxDebugger_0_6.png\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/metadata/boxes2D/BoundingBoxDebugger_0_50.boxes2D.json\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/metadata/boxes2D/BoundingBoxDebugger_0_70.boxes2D.json\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/images/BoundingBoxDebugger_0_1.png\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/metadata/boxes2D/BoundingBoxDebugger_0_30.boxes2D.json\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/images/BoundingBoxDebugger_0_12.png\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/metadata/boxes2D/BoundingBoxDebugger_0_150.boxes2D.json\": CommError, <Response [403]>\n       1/4    0.606G   0.07593   0.02005         0        39       224:  11%|█         | 16/149 [00:00<00:08, 16.59it/s]                                                                       wandb: ERROR Error uploading \"media/images/BoundingBoxDebugger_0_9.png\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/images/BoundingBoxDebugger_0_15.png\": CommError, <Response [403]>\nwandb: ERROR Error uploading \"media/metadata/boxes2D/BoundingBoxDebugger_0_140.boxes2D.json\": CommError, <Response [403]>\n       1/4    0.606G   0.07575   0.01985         0        36       224:  12%|█▏        | 18/149 [00:01<00:07, 16.52it/s]\n```\n\n\n### Additional Files\n\n_No response_\n\n### Environment\n\nIm using the docker image: ultralytics/yolov5:v6.1\n- OS: Ubuntu 20.04.3\n- Python version: 3.8.12\n- Versions of relevant libraries:\nwandb==0.12.10\ntorch==1.10.2+cu113\ntorchvision==0.11.3+cu113\n\n\n\n### Additional Context\n\nI always use Yolov5's docker image because it has all the environment already configured. I have no problems with uploading the data to https://wandb.ai/. But now I am trying to run local wandb and but I get errors in running the training.\nI already run the command:\nwandb login --relogin --host=http://{URL}:8080",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hi @WilberRojas! Thank you for writing in! Could you please provide the debug.log and debug-internal.log files associated with the run where you are running into this issue? These files should be located in the wandb folder relative to your working directory. \r\n\r\n"
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi @WilberRojas, took a peek at your debug logs and I do see many:\r\n`requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: http://{URL}/local-files/redev/YOLOv5/l0vuxdui/wandb-metadata.json..`\r\nErrors which lines up every time you try to upload a file in your console logs. \r\nIt does seem like your URL is either not being read properly, or hasn't been added to the code properly. \r\n\r\nCould you please send your training code in here?"
      },
      {
        "user": "ArtsiomWB",
        "body": "Apologies for the misunderstanding! I see you are currently on wandb 0.12.10. Would it be possible for you to upgrade? The issue could potentially be coming from you using an older version of wandb. "
      }
    ]
  },
  {
    "issue_number": 7778,
    "title": "[Q] How to download a lot of histories?",
    "author": "mbacvanski",
    "state": "open",
    "created_at": "2024-06-09T17:11:54Z",
    "updated_at": "2025-05-29T21:55:47Z",
    "labels": [
      "c:sdk:public-api",
      "a:sdk"
    ],
    "body": "I have several thousand runs in a project, and I'd like to download all their histories together. Manually looping over all the runs and querying their history with `run.history(...)` takes a very long time (hours), and it looks like the implementation of [`runs.histories(...)`](https://github.com/wandb/wandb/blob/v0.17.1/wandb/apis/public/runs.py#L171) does the same thing. \r\n\r\nIf I query the runs in parallel, I get a `requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api.wandb.ai/graphql`. Any suggestions on what to do? ",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nWhen dealing with a large number of runs, running into rate limits (HTTP 429 errors) is a common issue. Here are some strategies to handle this more efficiently:\n\n\n1. Batch Requests: Instead of querying histories sequentially, use batching to minimize the number of API calls.\n2. Retry Logic with Exponential Backoff: Implement a retry mechanism that waits for progressively longer periods before retrying a request.\n3. Throttle Requests: Implement a throttle mechanism to ensure you stay within the API rate limits.\n\nHere’s an example implementation:\n\n\n    import wandbimport timeimport pandas as pdfrom wandb.apis.public import Apifrom requests.exceptions import HTTPErrorInitialize W&B APIapi = Api()Function to fetch history of a single run with retries and exponential backoffdef fetch_run_history(run, max_retries=5, backoff_factor=1):for attempt in range(max_retries):try:return run.history()except HTTPError as e:if e.response.status_code == 429:# Too many requests, wait before retryingwait = backoff_factor * (2 ** attempt)print(f\"Rate limit exceeded. Retrying in {wait} seconds…\")time.sleep(wait)else:raise eraise Exception(\"Max retries exceeded\")Function to fetch histories of all runs in a projectdef fetch_all_histories(project_name, max_retries=5, backoff_factor=1, batch_size=100):runs = api.runs(project_name)histories = []for i in range(0, len(runs), batch_size): batch = runs[i:i + batch_size] for run in batch: try: history = fetch_run_history(run, max_retries, backoff_factor) histories.append((run.name, history)) except Exception as e: print(f\"Failed to fetch history for run {run.name}: {e}\")return historiesFetch all histories for the projectproject_name = \"your_project_name\"histories = fetch_all_histories(project_name)Combine histories into a single DataFramecombined_histories = []for run_name, history in histories:history['run_name'] = run_namecombined_histories.append(history)df_combined = pd.concat(combined_histories, ignore_index=True)Save to CSV or handle as neededdf_combined.to_csv(\"combined_histories.csv\", index=False)\n\n\n\nThe fetch_run_history function includes a retry mechanism with exponential backoff. If a rate limit error (HTTP 429) occurs, it waits for a progressively longer time before retrying. The `fetch_all_histories` function processes runs in batches to reduce the number of API calls made simultaneously. After fetching histories, they are combined into a single DataFrame.\n\nThis approach should help you download run histories more efficiently without excessively hitting API rate limits.\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nHi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved.\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nHi Internal, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!\n"
      }
    ]
  },
  {
    "issue_number": 9685,
    "title": "[Feature]: Edit Job Type from API",
    "author": "collinmccarthy",
    "state": "open",
    "created_at": "2025-04-04T15:52:13Z",
    "updated_at": "2025-05-29T21:54:46Z",
    "labels": [
      "c:sdk:public-api",
      "a:sdk"
    ],
    "body": "### Description\n\nHi, we use job types to group our runs. Right now if I programmatically assign the wrong job type, I can't fix it in any way, making the results in the UI wrong. \n\nI see [#3160](https://github.com/wandb/wandb/issues/3160) that asks how to edit the job type, but that's been closed so I'm turning this into a feature request. I see a comment at the end of that issue that a few others have liked showing that other people are interested in this as well. \n\nThank you\n\n### Suggested Solution\n\n<!--- Describe your solution here --->\n",
    "comments": [
      {
        "user": "fmamberti-wandb",
        "body": "Hi @collinmccarthy , thank you for reaching out about your feature request - I will raise it with our product team.\n\nIn the meantime, you should be able to update the `job_type` for a Run by resuming it and setting a new job type, with:\n\n```\nrun = wandb.init(project=project, id='<RUN_ID>', resume='must', job_type='<new_job_type>')\nrun.finish()\n```"
      },
      {
        "user": "collinmccarthy",
        "body": "Hi @fmamberti-wandb. This is a great workaround for now, thank you for letting me know!"
      }
    ]
  },
  {
    "issue_number": 9908,
    "title": "[Bug]:",
    "author": "anna2313",
    "state": "open",
    "created_at": "2025-05-24T11:40:32Z",
    "updated_at": "2025-05-29T20:49:16Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\n**I tried to run my code on an offline cluster and sync my logs afterwards, but the logs are not properly synced.** \n\nI made a simpler example of this (it is attached). Then I synced with this: `wandb sync <place of log file path>` (the path I used was done one suggested by the wandb output after the run of my code).\nAfterwards, the run showed up on the online platform, but the test_loss and train_loss didn't. I checked the summary json file and it doesn't have the correct information in it.\n\nWhen the same code is run in online mode everything works just fine.\n\n[bug.txt](https://github.com/user-attachments/files/20422789/bug.txt)",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bonnie Shen commented: \nHello Anna. Thanks for writing to us! Hope you are having a lovely day!\nCould you please share me the project link you are experiencing this? Could you please send debug.log and debug-internal.log? These files are under your local folder wandb/run-<date-time>-<run-id>/logs in the same directory where you’re running your code. These will help me a lot to investigate. Thank you!\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bonnie Shen commented: \nHello Anna. Since we haven’t heard back, I’ll go ahead and close this ticket for now. If you experience this issue again, please feel free to ping me here and I’ll be more than happy to keep investigating. Thanks!\n"
      }
    ]
  },
  {
    "issue_number": 9910,
    "title": "[Q]: Academic email verification",
    "author": "Irisaka",
    "state": "closed",
    "created_at": "2025-05-25T02:20:23Z",
    "updated_at": "2025-05-29T13:37:49Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\n<!--- Ask your question here --->\nI try to add a verified academic email，but I cannot receive email.\nI tried several times but it ended up showing rate limit exceeded An application error occurred.\nAfter more than 12 hours, I have not received the email, maybe not the problem of my school mail system.\n\nWhat else can I do to verify my academic email? My email is 3190100439@zju.edu.cn. My username is irisaka. Thank you!",
    "comments": [
      {
        "user": "JoanaMarieL",
        "body": "Hi @Irisaka , thank you for writing in and happy to help. May we ask if you also check your Spam folder for the Verification email?"
      },
      {
        "user": "Irisaka",
        "body": "Hi @JoanaMarieL, thank you for reply. I have checked my spam folder for the verification email, there is nothing."
      },
      {
        "user": "Irisaka",
        "body": "Any one can help? Using my zju.edu.cn email, I can not get any verification email, either in spam folder."
      }
    ]
  },
  {
    "issue_number": 4790,
    "title": "[Feature]: copy a run to another project",
    "author": "hankyul2",
    "state": "open",
    "created_at": "2023-01-14T06:25:20Z",
    "updated_at": "2025-05-29T11:11:06Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "### Description\n\ncopy a run named `my_resnet50` from project `cnn-a` to another project `cnn-b`.\n\n### Suggested Solution\n\nCreate a new `Copy` button in this bar.\r\n\r\n![image](https://user-images.githubusercontent.com/31476895/212459005-df9345b7-dccb-4161-8299-fc8740950de1.png)\r\n\n\n### Alternatives\n\nUse `move` instead of `copy`.\n\n### Additional Context\n\nThank you.",
    "comments": [
      {
        "user": "lesliewandb",
        "body": "Hi @hankyul2, thank you for writing in about this feature request! I'll create a ticket regarding this for you. But before I do, can you tell me more about why you would want this feature and the use case for it? "
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "WandB Internal User commented: \nlesliewandb commented: \nHi @hankyul2, thank you for writing in about this feature request! I'll create a ticket regarding this for you. But before I do, can you tell me more about why you would want this feature and the use case for it?\n"
      },
      {
        "user": "hankyul2",
        "body": "Hi @lesliewandb \r\n\r\nThank you for your reply. I briefly describe the motivation for this feature and its use case.\r\n\r\nMotivation: \r\n\r\n1. to share my experiment results saved in my private project with my coworkers via copying it into the public project. Of course, I can do it by using `move`, but sometimes I need to save my experimental run in both public and private projects to compare it with my other private run and share it with my coworkers at the same time. The `copy` will be very helpful, especially in case you couldn't lose your run in a private project but, want to share it with others.\r\n2. to save my experiment results belonging to more than one project. I sometimes use more than one wandb project to save my experimental logs. Whenever I need to compare my baseline with other runs saved in both projects, I need to move the baseline result between different projects several times or run the baseline experiment several times, which is annoying.  I think the `copy` feature will be very helpful for saving time from running unnecessary multiple runs by enabling storing one baseline experiment in more than one project at the same time.\r\n"
      }
    ]
  },
  {
    "issue_number": 9862,
    "title": "[Q]: Line plot sampling method",
    "author": "dong-zeyu",
    "state": "closed",
    "created_at": "2025-05-15T15:42:48Z",
    "updated_at": "2025-05-28T15:41:19Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\nI would like to use the full fidelity point sample method without plotting the min/max as shadows, because it will make the graph too complicated when there are multiple runs. I followed the [official document](https://docs.wandb.ai/guides/app/features/panels/line-plot/sampling/#configure-how-minimum-and-maximum-points-render) to remove the min/max shadows. However, in my wandb web UI, I cannot see the dropdown menu \"Show min/max values as a shaded area\" as suggested by the official document. Here is the picture:\n\n![Image](https://github.com/user-attachments/assets/5080234a-eaa5-4904-9c3d-de950594c3de)",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @dong-zeyu!\n\nThank you for writing in. Could you please send me a link to your workspace where you are seeing this behavior? I would love to take a look. "
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved."
      },
      {
        "user": "dong-zeyu",
        "body": "Here is the example workspace: https://wandb.ai/zydong/testing?nw=nwuserzydong\n\nYou can see the shaded region in the loss curve. I would like to disable the shaded region.\n\n![Image](https://github.com/user-attachments/assets/7dac2b60-f8b1-4f57-88ed-8579b55aaca8)\n\nI am not able to find the setting to disable the shadow following the documentation\n\n![Image](https://github.com/user-attachments/assets/0ec44ccd-b0e4-46ca-ab42-747206f3bb09)"
      }
    ]
  },
  {
    "issue_number": 9805,
    "title": "[Q]: If I want to filter by creation time, it seems that I can't set a precise time here. The finest granularity is by day.",
    "author": "isCopyman",
    "state": "open",
    "created_at": "2025-05-03T13:58:28Z",
    "updated_at": "2025-05-28T07:21:21Z",
    "labels": [
      "ty:question",
      "a:app"
    ],
    "body": "### Ask your question\n\n<!--- Ask your question here --->\n\n![Image](https://github.com/user-attachments/assets/ad04e29b-e60c-4cbd-9d73-19d05814b11b)\n",
    "comments": [
      {
        "user": "isCopyman",
        "body": "![Image](https://github.com/user-attachments/assets/cf559e6c-60d7-4fdb-8b93-533ad777fcbb)\n\n![Image](https://github.com/user-attachments/assets/6cac249a-863d-4bf9-91a4-3751c150d296)\nIn addition, some columns seem to be unsearchable in the filter."
      },
      {
        "user": "JoanaMarieL",
        "body": "Hello @isCopyman , thank you for raisingt this and happy to help. May we ask of the following to fully understand your concern and possibly look for a workaround:\n\n- Kindly share your specific use case in having a granular search\n- How granular the search will be, by seconds or by hours, etc?\n- How many runs or estimated no. of runs will be included in the filter\n\nThank you,\nJoana Marie"
      },
      {
        "user": "oleg-kachan",
        "body": "Second that, filtering runs up to minutes would be great."
      }
    ]
  },
  {
    "issue_number": 9134,
    "title": "[Bug]: wandb offline causes error",
    "author": "zankner",
    "state": "closed",
    "created_at": "2024-12-20T05:09:02Z",
    "updated_at": "2025-05-28T06:16:48Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\n\nIf I set `wandb offline` and then run:\n\n```\nimport wandb\nwandb.init(project=\"my-project\", name=\"my-name\")\n```\n\nI get the following error:\n\n```\nValidationError                           Traceback (most recent call last)\nCell In[2], line 1\n----> 1 wandb.init(project=\"my-project\", name=\"my-name\")\n\nFile /usr/lib/python3/dist-packages/wandb/sdk/wandb_init.py:1319, in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\n   1315     logger.exception(\"error in wandb.init()\", exc_info=e)\n   1317 # Need to build delay into this sentry capture because our exit hooks\n   1318 # mess with sentry's ability to send out errors before the program ends.\n-> 1319 wandb._sentry.reraise(e)\n   1320 raise AssertionError()\n\nFile /usr/lib/python3/dist-packages/wandb/analytics/sentry.py:156, in Sentry.reraise(self, exc)\n    153 self.exception(exc)\n    154 # this will messily add this \"reraise\" function to the stack trace,\n    155 # but hopefully it's not too bad\n--> 156 raise exc.with_traceback(sys.exc_info()[2])\n\nFile /usr/lib/python3/dist-packages/wandb/sdk/wandb_init.py:1297, in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\n   1295 try:\n   1296     wi = _WandbInit()\n-> 1297     wi.setup(\n   1298         init_settings=init_settings,\n   1299         config=config,\n   1300         config_exclude_keys=config_exclude_keys,\n   1301         config_include_keys=config_include_keys,\n   1302         allow_val_change=allow_val_change,\n   1303         monitor_gym=monitor_gym,\n   1304     )\n   1305     return wi.init()\n   1307 except KeyboardInterrupt as e:\n\nFile /usr/lib/python3/dist-packages/wandb/sdk/wandb_init.py:196, in _WandbInit.setup(self, init_settings, config, config_exclude_keys, config_include_keys, allow_val_change, monitor_gym)\n    191     setup_settings_dict[\"x_disable_service\"] = init_settings.x_disable_service\n    192 setup_settings = (\n    193     wandb.Settings(**setup_settings_dict) if setup_settings_dict else None\n    194 )\n--> 196 self._wl = wandb_setup.setup(settings=setup_settings)\n    198 assert self._wl is not None\n    199 _set_logger(self._wl._get_logger())\n\nFile /usr/lib/python3/dist-packages/wandb/sdk/wandb_setup.py:383, in setup(settings)\n    327 def setup(settings: Settings | None = None) -> _WandbSetup | None:\n    328     \"\"\"Prepares W&B for use in the current process and its children.\n    329 \n    330     You can usually ignore this as it is implicitly called by `wandb.init()`.\n   (...)\n    381         ```\n    382     \"\"\"\n--> 383     ret = _setup(settings=settings)\n    384     return ret\n\nFile /usr/lib/python3/dist-packages/wandb/sdk/wandb_setup.py:323, in _setup(settings, _reset)\n    320     teardown()\n    321     return None\n--> 323 wl = _WandbSetup(settings=settings)\n    324 return wl\n\nFile /usr/lib/python3/dist-packages/wandb/sdk/wandb_setup.py:301, in _WandbSetup.__init__(self, settings)\n    299     _WandbSetup._instance._update(settings=settings)\n    300     return\n--> 301 _WandbSetup._instance = _WandbSetup__WandbSetup(settings=settings, pid=pid)\n\nFile /usr/lib/python3/dist-packages/wandb/sdk/wandb_setup.py:110, in _WandbSetup__WandbSetup.__init__(self, pid, settings, environ)\n    107 self._early_logger = _EarlyLogger()\n    108 _set_logger(self._early_logger)\n--> 110 self._settings = self._settings_setup(settings, self._early_logger)\n    112 wandb.termsetup(self._settings, logger)\n    114 self._check()\n\nFile /usr/lib/python3/dist-packages/wandb/sdk/wandb_setup.py:139, in _WandbSetup__WandbSetup._settings_setup(self, settings, early_logger)\n    137 if s.settings_workspace and early_logger:\n    138     early_logger.info(f\"Loading settings from {s.settings_workspace}\")\n--> 139 s.update_from_workspace_config_file()\n    141 # load settings from the environment variables\n    142 if early_logger:\n\nFile /usr/lib/python3/dist-packages/wandb/sdk/wandb_settings.py:883, in Settings.update_from_workspace_config_file(self)\n    881 for key, value in self._load_config_file(self.settings_workspace).items():\n    882     if value is not None:\n--> 883         setattr(self, key, value)\n\nFile /usr/lib/python3/dist-packages/pydantic/main.py:923, in BaseModel.__setattr__(self, name, value)\n    921     self.__dict__[name] = value\n    922 elif self.model_config.get('validate_assignment', None):\n--> 923     self.__pydantic_validator__.validate_assignment(self, name, value)\n    924 elif self.model_config.get('extra') != 'allow' and name not in self.__pydantic_fields__:\n    925     # TODO - matching error\n    926     raise ValueError(f'\"{self.__class__.__name__}\" object has no field \"{name}\"')\n\nValidationError: 1 validation error for Settings\ndisabled\n  Object has no attribute 'disabled' [type=no_such_attribute, input_value='true', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/no_such_attribute\n```\n\nThis happens for wandb versions 0.19.1 and 0.19.0 but does not happen for version 0.18.7 and below.",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @zankner! Thank you for writing in. \n\nI see the same behavior on my end. I'll report it to our eng team. Thank you!"
      },
      {
        "user": "Limingxing00",
        "body": "So how do we resolve it?"
      },
      {
        "user": "gxy-gxy",
        "body": "> So how do we resolve it?\n\npip install wandb==0.18.7 is useful for me"
      }
    ]
  },
  {
    "issue_number": 4063,
    "title": "[Q] Is it possible to log a custom timestamp and use it as the \"x-axis\" for plots?",
    "author": "desktable",
    "state": "closed",
    "created_at": "2022-08-06T04:44:51Z",
    "updated_at": "2025-05-27T17:50:30Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "Context: my validation dataset spans over a period of time, and I want view the model's performance at different time points.\r\n\r\nRight now I can do the following:\r\n\r\n```\r\nwandb.log({\"dataset_date\": seconds_since_epoch, \"metric\": metric})\r\n```\r\n\r\nBut in the UI the `dataset_date` is shown as numbers instead of dates. Is there a way to make the UI show this x-axis as time instead?\r\n",
    "comments": [
      {
        "user": "MBakirWB",
        "body": "Hi @desktable, select edit from the panel, and change the x-axis from **Data** to your intended metric, **Relative Time (Wall)**. see more [here](https://docs.wandb.ai/ref/app/features/panels/line-plot#settings)"
      },
      {
        "user": "desktable",
        "body": "> Hi @desktable, select edit from the panel, and change the x-axis from **Data** to your intended metric, **Relative Time (Wall)**. see more [here](https://docs.wandb.ai/ref/app/features/panels/line-plot#settings)\r\n\r\nThis is not what I need. I want to use a custom timestamp field, not the default \"Relative Time (Wall)\" field. The custom timestamp field represents the time when the data was collected (which spans over several months), and has nothing to do with the how much time the model has been trained for."
      },
      {
        "user": "MBakirWB",
        "body": "Thank you for the clarification @desktable . At this time we do not support date and time on the x-axis for charts. There is an active feature request for this that is in the design backlog. I added your details to the request and will update you when there is feedback from the team. "
      }
    ]
  },
  {
    "issue_number": 9873,
    "title": "[Q]: Request for upgrade to an academic account",
    "author": "kim4375731",
    "state": "open",
    "created_at": "2025-05-19T05:09:04Z",
    "updated_at": "2025-05-27T01:18:40Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\n<!--- Ask your question here --->\nHi, I registered to wandb account as kim4375731@gmail.com as a normal user, but I recently realized wandb provides plan for academic account.\nAs I'm a graduate student in KAIST (Korea Advanced Institute of Science and Technology), I think I'm eligible for an academic account. \nCan I change my current account status to an academic account with the new e-mail address or what else can I do to achieve one? My academic e-mail address is kim4375731@kaist.ac.kr\n\nPlus, I found my local deployment license is about to expire. Could you kindly renew it? cuz I couldn't receive any feedback from support@wandb.com yet.\n\nThank you in advance",
    "comments": [
      {
        "user": "JoanaMarieL",
        "body": "Hi @kim4375731 thanks for writing in and happy to help.\n\nKindly apply for an academic account [here](https://wandb.ai/academic_application), and link your university email to your User Settings. Then we'll review your application let you know once approved.\n\nAs for your local deployment request, may I please ask for the ticket number?\n\nThank you,\nJoana Marie"
      },
      {
        "user": "kim4375731",
        "body": "Thank you for the reply.\n\nUnfortunately, it seems I cannot reach to the link you provided (cannot connect):\n![Image](https://github.com/user-attachments/assets/79436872-24cc-4e63-a860-f83a923a8061)\nActually I found the link somewhere else by googling, which I also cannot connect. I don't think this is an internet quality issue.\n\nAs for the ticket number of my local deployment session, maybe this is what you're looking for:\ne8227726-d965-47ee-92f2-9303bd3b6c75\n\nThank you!"
      },
      {
        "user": "paulosabile-wb",
        "body": "Hi,\n\n \nCould you please share a screenshot of your [Deploy Manager](https://deploy.wandb.ai/) screen that shows the organization name and the license ID so I can review the renewal request?\n \n \nThanks,\nPaulo"
      }
    ]
  },
  {
    "issue_number": 9674,
    "title": "[Bug]: wandb doesn't work with protobuf==6.30.2",
    "author": "deep1401",
    "state": "closed",
    "created_at": "2025-04-02T20:03:00Z",
    "updated_at": "2025-05-26T16:21:29Z",
    "labels": [
      "ty:bug",
      "a:sdk",
      "c:sdk:code-quality"
    ],
    "body": "### Describe the bug\n\nTrying to import wandb when I have protobuf 6.30.2 gives this error. I need to be on this protobuf version due to dependencies of other packages.\n\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/deep.gandhi/.transformerlab/envs/transformerlab/lib/python3.11/site-packages/wandb/__init__.py\", line 22, in <module>\n    from wandb.sdk.lib import wb_logging as _wb_logging\n  File \"/Users/deep.gandhi/.transformerlab/envs/transformerlab/lib/python3.11/site-packages/wandb/sdk/__init__.py\", line 24, in <module>\n    from . import wandb_helper as helper\n  File \"/Users/deep.gandhi/.transformerlab/envs/transformerlab/lib/python3.11/site-packages/wandb/sdk/wandb_helper.py\", line 6, in <module>\n    from .lib import config_util\n  File \"/Users/deep.gandhi/.transformerlab/envs/transformerlab/lib/python3.11/site-packages/wandb/sdk/lib/__init__.py\", line 2, in <module>\n    from .disabled import RunDisabled, SummaryDisabled\n  File \"/Users/deep.gandhi/.transformerlab/envs/transformerlab/lib/python3.11/site-packages/wandb/sdk/lib/disabled.py\", line 3, in <module>\n    from wandb.sdk.lib import deprecate\n  File \"/Users/deep.gandhi/.transformerlab/envs/transformerlab/lib/python3.11/site-packages/wandb/sdk/lib/deprecate.py\", line 7, in <module>\n    from wandb.proto.wandb_telemetry_pb2 import Deprecated as TelemetryDeprecated\nImportError: cannot import name 'Deprecated' from 'wandb.proto.wandb_telemetry_pb2' (/Users/deep.gandhi/.transformerlab/envs/transformerlab/lib/python3.11/site-packages/wandb/proto/wandb_telemetry_pb2.py)\n```\n\n\n",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hey @deep1401, thanks for flagging this! I tested in a fresh environment with `wandb==0.19.8` and `protobuf==6.30.2` but seeing a different error:\n```\nTypeError: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower). \n```\nSetting `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python` resolves the problem, so would you mind sharing if this works on your end, and your `wandb --version`?"
      },
      {
        "user": "deep1401",
        "body": "Hey @luisbergua,\nThanks for responding!\nI am on `wandb=0.19.8` and `protofbuf==6.30.2`.\nHowever I could see these with `pip show wandb` because `wandb --version` also gives the same error I posted earlier. \nSetting `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python` did not solve this\n\nPlease let me know if you need more details about some other packages"
      },
      {
        "user": "luisbergua",
        "body": "Hey @deep1401, thanks for your answer! I was able to reproduce this with `wandb --version`, will raise with our engineers "
      }
    ]
  },
  {
    "issue_number": 2939,
    "title": "List as inputs in sweep yaml ",
    "author": "rogelioamancisidor",
    "state": "closed",
    "created_at": "2021-11-19T09:59:26Z",
    "updated_at": "2025-05-25T15:45:04Z",
    "labels": [
      "ty:feature",
      "c:sweeps"
    ],
    "body": "**Python version: 3.6.9**\r\n**wandb version 0.12.5**\r\n**OS: Linux**\r\n\r\n**Summary:** In my code `run.py` I use `argparse` for specifying the model's hyperparameters. One of those parameter is the number of hidden layers and their size, which is specified by a list i.e.\r\n\r\n`parser.add_argument(\"--layers_size_prior\", default=[500, 500, 500], nargs='+', help=\"No of units in then hidden layers\", type=int)`\r\n\r\nThen, I can specify the size of `--layers_size_prior` simply as `python run.py --layers_size_prior 100 100` for example. \r\n\r\nI want to use `sweep` for parameter optimization for `--layers_size_prior`  and I create the following `yaml` file\r\n\r\n    project: my_project\r\n    program: ./run.py\r\n    method: grid\r\n    metric:\r\n      name: auc\r\n      goal: maximize\r\n    parameters:\r\n      layers_size_prior:\r\n         values: [500 500 500, 1000 1000 1000]\r\n\r\nbut I get the following error message\r\n\r\n    ValueError: invalid literal for int() with base 10: ' '\r\n\r\nWhich makes me think that there is no `nargs='+'` functionality in `sweep`. \r\n\r\nAs a workaround, I did something similar as in [using tuple in sweeps](https://github.com/wandb/client/issues/1493)\r\n",
    "comments": [
      {
        "user": "dannygoldstein",
        "body": "hi @rogelioamancisidor, i think the issue here is that the values `500 500 500` and `1000 1000 1000` do not get parsed as lists in yaml. if you want them passed to your `--layers_size_prior` arg as a list, try specifying them as a list in yaml:\r\n\r\n```yaml\r\nproject: my_project\r\nprogram: ./run.py\r\nmethod: grid\r\nmetric:\r\n  name: auc\r\n  goal: maximize\r\nparameters:\r\n  layers_size_prior:\r\n     values: [[500, 500, 500], [1000, 1000, 1000]]\r\n```"
      },
      {
        "user": "rogelioamancisidor",
        "body": "It passes a string, i.e. `['1000 1000 1000']`, so it crashes. \r\n    "
      },
      {
        "user": "dannygoldstein",
        "body": "hi @rogelioamancisidor, as a workaround, you can do:\r\n\r\nsweep.yaml:\r\n```yaml\r\nproject: my_project\r\nprogram: ./run.py\r\nmethod: grid\r\nmetric:\r\n  name: auc\r\n  goal: maximize\r\nparameters:\r\n  layers_size_prior:\r\n     values: [\"500 500 500\", \"1000 1000 1000\"]\r\n```\r\n\r\nrun.py:\r\n```python\r\n#!/usr/bin/env python\r\n\r\nimport argparse\r\nclass ParseAction(argparse.Action):\r\n    def __call__(self, parser, namespace, values, option_string=None):\r\n        print('%r %r %r' % (namespace, values, option_string))\r\n        values = list(map(int, values.split()))\r\n        setattr(namespace, self.dest, values)\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--layers_size_prior\", default=[500, 500, 500], help=\"No of units in then hidden layers\", action=ParseAction)\r\nargs = parser.parse_args()\r\n\r\nimport wandb\r\nwandb.init()\r\nprint(wandb.config)\r\n```\r\n\r\ni verified locally that this works. can you let me know if this fixes things for you? \r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 9535,
    "title": "[Feature]: Adding mmCIF file support for `wandb.Molecule`",
    "author": "amorehead",
    "state": "open",
    "created_at": "2025-02-28T01:35:49Z",
    "updated_at": "2025-05-24T00:11:25Z",
    "labels": [
      "ty:feature",
      "a:app",
      "c:table"
    ],
    "body": "### Description\n\nThe following error message displayed in the WandB UI after logging a table containing a `wandb.Molecule` object constructed from a `.cif` (i.e., mmCIF) file suggests that mmCIF files aren't supported, even though documentation suggests otherwise.\n\nLog: `Tried to render media/molecule/1a335c7f618260e68830/sample.cif, but only .pdb files are currently supported.`\n\n### Suggested Solution\n\nIt'd be great for mmCIF file support to be added for `wandb.Molecule` objects, as the PDB file format has long since been deprecated in favor of mmCIF files (according to the RCSB PDB).",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @amorehead, thank you for writing in. \n\nCould you please send me an example script of how you are logging your molecule to wandb? I am trying to get the exact same error. "
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved. "
      },
      {
        "user": "amorehead",
        "body": "Hi, @ArtsiomWB. Attached is one such mmCIF file (renamed to .txt to upload to GitHub) that failed to display properly in the WandB UI. Let me know if you have any follow-up questions.\n\n[sample.cif.txt](https://github.com/user-attachments/files/19099265/sample.cif.txt)"
      }
    ]
  },
  {
    "issue_number": 9871,
    "title": "[Bug]: Wandb not initing properly across ranks in distributed training",
    "author": "alex-infinite",
    "state": "open",
    "created_at": "2025-05-17T02:01:47Z",
    "updated_at": "2025-05-23T15:17:33Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nI want to init wandb on each rank in a parallel training to watch the params from each (I'm doing FSDP not DDP). I'm only logging loss etc from primary rank.\n\nBut it seems that something is going wrong and I'm only getting the parameters logged from a single rank. FWIW, which rank seems to vary across inits/runs of the script and is not necessarily the primary rank which does in fact log the loss etc every time.\n\nThe relevant resources I found on this were these and AFAICT I followed the suggestions and still have this issue\n- https://docs.wandb.ai/guides/track/log/distributed-training/#track-all-processes-to-a-single-run\n- https://wandb.ai/dimaduev/simple-cnn-ddp/reports/Distributed-Training-with-Shared-Mode--VmlldzoxMTI0NTE1NA#training-script \n\nThe operative error seems to be: \n`wandb: 409 encountered ({\"errors\":[{\"message\":\"Error 1062 (23000): Duplicate entry '40991226-1uqaxcfq' for key 'runs.PRIMARY'\",\"path\":[\"upsertBucket\"]}],\"data\":{\"upsertBucket\":null}}), retrying request`\n\nPlease advise!\n\n### Versions\nPython 3.10.15\nwandb, version 0.19.11\nUbuntu 22.04.5\n\n\n### Snippet of logging dump\n```\n[rank 0] Init-ing wandb (run id: 1uqaxcfq)...\n[rank 2] Init-ing wandb (run id: 1uqaxcfq)...\n[rank 3] Init-ing wandb (run id: 1uqaxcfq)...\n[rank 1] Init-ing wandb (run id: 1uqaxcfq)...\nwandb: Currently logged in as: alex-infinite to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Currently logged in as: alex-infinite to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Currently logged in as: alex-infinite to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Currently logged in as: alex-infinite to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: WARNING The `shared` mode feature is experimental and may change. Please contact support@wandb.com for guidance and to report any issues.\nwandb: WARNING The `shared` mode feature is experimental and may change. Please contact support@wandb.com for guidance and to report any issues.\nwandb: WARNING The `shared` mode feature is experimental and may change. Please contact support@wandb.com for guidance and to report any issues.\nwandb: WARNING The `shared` mode feature is experimental and may change. Please contact support@wandb.com for guidance and to report any issues.\nwandb: Tracking run with wandb version 0.19.11\nwandb: Run data is saved locally in /root/home/wandb/run-20250517_003843-1uqaxcfq\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run my-test-run\nwandb: ⭐️ View project at...\nwandb: 🚀 View run at...\n[rank 1] Wandb init-ed with run id: 1uqaxcfq\n[rank 1] Starting train loop from step 0...\nwandb: creating run\nwandb: creating run\nwandb: creating run\nwandb: Tracking run with wandb version 0.19.11\nwandb: Run data is saved locally in /root/home/wandb/run-20250517_003843-1uqaxcfq\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run my-test-run\nwandb: ⭐️ View project at [url]...\nwandb: 🚀 View run at [url]...\nwandb: Tracking run with wandb version 0.19.11\nwandb: Run data is saved locally in /root/home/wandb/run-20250517_003843-1uqaxcfq\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run my-test-run\nwandb: ⭐️ View project at [url]...\nwandb: 🚀 View run at [url]...\n[rank 0] Wandb init-ed with run id: 1uqaxcfq\n[rank 0] Starting train loop from step 0...\n[rank 2] Wandb init-ed with run id: 1uqaxcfq\n[rank 2] Starting train loop from step 0...\nwandb: 409 encountered ({\"errors\":[{\"message\":\"Error 1062 (23000): Duplicate entry '40991226-1uqaxcfq' for key 'runs.PRIMARY'\",\"path\":[\"upsertBucket\"]}],\"data\":{\"upsertBucket\":null}}), retrying request\nwandb: Tracking run with wandb version 0.19.11\nwandb: Run data is saved locally in /root/home/wandb/run-20250517_003843-1uqaxcfq\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run my-test-run\nwandb: ⭐️ View project at [url]...\nwandb: 🚀 View run at [url]...\nwandb: 409 encountered ({\"errors\":[{\"message\":\"Error 1062 (23000): Duplicate entry '40991226-1uqaxcfq' for key 'runs.PRIMARY'\",\"path\":[\"upsertBucket\"]}],\"data\":{\"upsertBucket\":null}}), retrying request\n[rank 3] Wandb init-ed with run id: 1uqaxcfq\n[rank 3] Starting train loop from step 0...\nwandb: 409 encountered ({\"errors\":[{\"message\":\"Error 1062 (23000): Duplicate entry '40991226-1uqaxcfq' for key 'runs.PRIMARY'\",\"path\":[\"upsertBucket\"]}],\"data\":{\"upsertBucket\":null}}), retrying request\n```\n\n\n### Code snippet for init-ing wandb\n```\n        if self.rank == self.PRIMARY_RANK:\n            # Maybe this would've been easier: wandb.util.generate_id()\n            wandb_api = wandb.Api()\n            extant_run_ids = [\n                run.id for run in wandb_api.runs(f\"{wandb_api.default_entity}/{self.wandb_project}\", per_page=1000)\n            ]\n            generate_id = lambda: \"\".join(  # noqa: E731\n                random.choice(string.ascii_lowercase + string.digits) for _ in range(len_wandb_run_id)\n            )\n            run_id = generate_id()\n            while run_id in extant_run_ids:\n                run_id = generate_id()\n            self.wandb_run_id = run_id\n\n        # Then pass it to all ranks.\n        run_id = [self.wandb_run_id]\n        dist.broadcast_object_list(run_id, src=self.PRIMARY_RANK)\n        self.wandb_run_id = run_id[0]\n\n        # Now that we know we have a run id on all ranks, just init all simultaneously.\n        wandb_dir = os.environ.get(\"WANDB_DIR\", os.getcwd())  # Defaults pwd\n        if self.rank == self.PRIMARY_RANK:\n            os.makedirs(wandb_dir, exist_ok=True)\n\n        # Inspired by:\n        # - https://docs.wandb.ai/guides/track/log/distributed-training/#track-all-processes-to-a-single-run\n        # - https://wandb.ai/dimaduev/simple-cnn-ddp/reports/Distributed-Training-with-Shared-Mode--VmlldzoxMTI0NTE1NA#training-script  # noqa: E501\n        wandb_settings = wandb.Settings(\n            mode=\"shared\",\n            x_primary=self.rank == self.PRIMARY_RANK,\n            x_update_finish_state=self.rank == self.PRIMARY_RANK,  # Only allow one rank to finish the run.\n            x_label=f\"rank-{self.rank}\",\n            x_stats_gpu_device_ids=[self.rank],\n        )\n        print_(f\"Init-ing wandb (run id: {self.wandb_run_id})...\")\n        dist.barrier()  # Likely unnecessary but really want to init all at the same time.\n        self.wandb_run = wandb.init(\n            dir=wandb_dir,\n            project=self.wandb_project,\n            name=wandb_run_name,\n            id=self.wandb_run_id,\n            resume=\"must\" if run_id_passed else \"never\",\n            settings=wandb_settings,\n            config={\n                \"model_cfg\": self.model_cfg,\n                \"dataset_cfg\": self.dataset_cfg,\n                \"trainer_args\": asdict(self.trainer_args),\n            },\n        )\n        \n        self.wandb_run.watch((self.fsdp_model, self.conditioner), log=\"all\", log_freq=100)\n```",
    "comments": [
      {
        "user": "fmamberti-wandb",
        "body": "HI @alex-infinite , thank you for reaching out and sharing all the details related to the issue you are sharing.\n\nThe error you shared:\n\n```\nwandb: 409 encountered ({\"errors\":[{\"message\":\"Error 1062 (23000): Duplicate entry '40991226-1uqaxcfq' for key 'runs.PRIMARY'\",\"path\":[\"upsertBucket\"]}],\"data\":{\"upsertBucket\":null}}), retrying request\n```\n\nis one our engineering team is aware of and is looking into. I will add your report to our internal ticket so we can keep you posted with any updates on this.\n\nRegarding the parameters:\n\n> I'm only getting the parameters logged from a single rank\n\ncould you please clarify if you are referring to the model parameters and gradients logged via `self.wandb_run.watch((self.fsdp_model, self.conditioner), log=\"all\", log_freq=100)` or to other parameters as well?\n\nIt would also be very helpful if you could share the logs files for one of your affected runs (these should be in `./wandb/run-<id>-<date_time>` as well as the URL to the run for us to have a look."
      },
      {
        "user": "alex-infinite",
        "body": "Yup, I'm referring to the parameters for just those models. The gradients are only coming through for the conditioner, but it sounds like that's expected as [wandb doesn't support logging gradients from FSDP-wrapped models](https://github.com/wandb/wandb/issues/9866). \n\nThis was run on an ephemeral machine and so I don't have the wandb logs. Here are some logs from something I just ran and ctrl-ced now (this one happen not to be logging gradients/parameters -- don't work with torch's FSDP2/DTensor, which this model uses), but the same init error is thrown.\n\nLogs below. If you pass your email, I'll send you the url.\n\n### `debug.core`\n```\n{\"time\":\"2025-05-20T17:13:12.546284777Z\",\"level\":\"INFO\",\"msg\":\"main: starting server\",\"port-filename\":\"/var/tmp/tmpq8o6kjl8/port-21633.txt\",\"pid\":21633,\"log-level\":0,\"disable-analytics\":false,\"shutdown-on-parent-exit\":false,\"enable-dcgm-profiling\":false}\n{\"time\":\"2025-05-20T17:13:12.546840472Z\",\"level\":\"INFO\",\"msg\":\"main: starting server\",\"port-filename\":\"/var/tmp/tmplzshlx52/port-21631.txt\",\"pid\":21631,\"log-level\":0,\"disable-analytics\":false,\"shutdown-on-parent-exit\":false,\"enable-dcgm-profiling\":false}\n{\"time\":\"2025-05-20T17:13:12.54715041Z\",\"level\":\"INFO\",\"msg\":\"main: starting server\",\"port-filename\":\"/var/tmp/tmppk1_23sx/port-21630.txt\",\"pid\":21630,\"log-level\":0,\"disable-analytics\":false,\"shutdown-on-parent-exit\":false,\"enable-dcgm-profiling\":false}\n{\"time\":\"2025-05-20T17:13:12.549309748Z\",\"level\":\"INFO\",\"msg\":\"main: starting server\",\"port-filename\":\"/var/tmp/tmp2xxs5z0q/port-21632.txt\",\"pid\":21632,\"log-level\":0,\"disable-analytics\":false,\"shutdown-on-parent-exit\":false,\"enable-dcgm-profiling\":false}\n{\"time\":\"2025-05-20T17:13:12.553027726Z\",\"level\":\"INFO\",\"msg\":\"Will exit if parent process dies.\",\"ppid\":21633}\n{\"time\":\"2025-05-20T17:13:12.553001244Z\",\"level\":\"INFO\",\"msg\":\"server is running\",\"addr\":{\"IP\":\"127.0.0.1\",\"Port\":45889,\"Zone\":\"\"}}\n{\"time\":\"2025-05-20T17:13:12.554056652Z\",\"level\":\"INFO\",\"msg\":\"Will exit if parent process dies.\",\"ppid\":21631}\n{\"time\":\"2025-05-20T17:13:12.554046541Z\",\"level\":\"INFO\",\"msg\":\"server is running\",\"addr\":{\"IP\":\"127.0.0.1\",\"Port\":42417,\"Zone\":\"\"}}\n{\"time\":\"2025-05-20T17:13:12.554492085Z\",\"level\":\"INFO\",\"msg\":\"Will exit if parent process dies.\",\"ppid\":21630}\n{\"time\":\"2025-05-20T17:13:12.554493461Z\",\"level\":\"INFO\",\"msg\":\"Will exit if parent process dies.\",\"ppid\":21632}\n{\"time\":\"2025-05-20T17:13:12.554475477Z\",\"level\":\"INFO\",\"msg\":\"server is running\",\"addr\":{\"IP\":\"127.0.0.1\",\"Port\":45599,\"Zone\":\"\"}}\n{\"time\":\"2025-05-20T17:13:12.554474937Z\",\"level\":\"INFO\",\"msg\":\"server is running\",\"addr\":{\"IP\":\"127.0.0.1\",\"Port\":34765,\"Zone\":\"\"}}\n{\"time\":\"2025-05-20T17:13:12.733143966Z\",\"level\":\"INFO\",\"msg\":\"connection: ManageConnectionData: new connection created\",\"id\":\"127.0.0.1:38562\"}\n{\"time\":\"2025-05-20T17:13:12.733813679Z\",\"level\":\"INFO\",\"msg\":\"connection: ManageConnectionData: new connection created\",\"id\":\"127.0.0.1:34038\"}\n{\"time\":\"2025-05-20T17:13:12.733944946Z\",\"level\":\"INFO\",\"msg\":\"connection: ManageConnectionData: new connection created\",\"id\":\"127.0.0.1:34248\"}\n{\"time\":\"2025-05-20T17:13:12.734051184Z\",\"level\":\"INFO\",\"msg\":\"connection: ManageConnectionData: new connection created\",\"id\":\"127.0.0.1:41944\"}\n{\"time\":\"2025-05-20T17:13:12.741479462Z\",\"level\":\"INFO\",\"msg\":\"handleInformInit: received\",\"streamId\":\"61auyx53\",\"id\":\"127.0.0.1:41944\"}\n{\"time\":\"2025-05-20T17:13:12.742503047Z\",\"level\":\"INFO\",\"msg\":\"handleInformInit: received\",\"streamId\":\"61auyx53\",\"id\":\"127.0.0.1:34248\"}\n{\"time\":\"2025-05-20T17:13:12.742504348Z\",\"level\":\"INFO\",\"msg\":\"handleInformInit: received\",\"streamId\":\"61auyx53\",\"id\":\"127.0.0.1:34038\"}\n{\"time\":\"2025-05-20T17:13:12.742551584Z\",\"level\":\"ERROR\",\"msg\":\"error creating symlink\",\"error\":\"symlink /home/alex/.cache/wandb/logs/core-debug-20250520_171312.log /home/alex/wandb/run-20250520_171312-61auyx53/logs/debug-core.log: file exists\"}\n{\"time\":\"2025-05-20T17:13:12.742561064Z\",\"level\":\"ERROR\",\"msg\":\"error creating symlink\",\"error\":\"symlink /home/alex/.cache/wandb/logs/core-debug-20250520_171312.log /home/alex/wandb/run-20250520_171312-61auyx53/logs/debug-core.log: file exists\"}\n{\"time\":\"2025-05-20T17:13:12.74318089Z\",\"level\":\"INFO\",\"msg\":\"handleInformInit: received\",\"streamId\":\"61auyx53\",\"id\":\"127.0.0.1:38562\"}\n{\"time\":\"2025-05-20T17:13:12.743253703Z\",\"level\":\"ERROR\",\"msg\":\"error creating symlink\",\"error\":\"symlink /home/alex/.cache/wandb/logs/core-debug-20250520_171312.log /home/alex/wandb/run-20250520_171312-61auyx53/logs/debug-core.log: file exists\"}\n{\"time\":\"2025-05-20T17:13:12.873638316Z\",\"level\":\"INFO\",\"msg\":\"handleInformInit: stream started\",\"streamId\":\"61auyx53\",\"id\":\"127.0.0.1:34248\"}\n{\"time\":\"2025-05-20T17:13:12.875314383Z\",\"level\":\"INFO\",\"msg\":\"handleInformInit: stream started\",\"streamId\":\"61auyx53\",\"id\":\"127.0.0.1:41944\"}\n{\"time\":\"2025-05-20T17:13:12.877849755Z\",\"level\":\"INFO\",\"msg\":\"handleInformInit: stream started\",\"streamId\":\"61auyx53\",\"id\":\"127.0.0.1:34038\"}\n{\"time\":\"2025-05-20T17:13:12.982052974Z\",\"level\":\"INFO\",\"msg\":\"handleInformInit: stream started\",\"streamId\":\"61auyx53\",\"id\":\"127.0.0.1:38562\"}\n{\"time\":\"2025-05-20T17:13:42.81241196Z\",\"level\":\"INFO\",\"msg\":\"handleInformTeardown: server teardown initiated\",\"id\":\"127.0.0.1:34038\"}\n{\"time\":\"2025-05-20T17:13:42.81248614Z\",\"level\":\"INFO\",\"msg\":\"connection: closing\",\"id\":\"127.0.0.1:34038\"}\n{\"time\":\"2025-05-20T17:13:42.812512925Z\",\"level\":\"INFO\",\"msg\":\"server is shutting down\"}\n{\"time\":\"2025-05-20T17:13:42.812607739Z\",\"level\":\"INFO\",\"msg\":\"connection: closed successfully\",\"id\":\"127.0.0.1:34038\"}\n{\"time\":\"2025-05-20T17:13:42.871313348Z\",\"level\":\"INFO\",\"msg\":\"handleInformTeardown: server teardown initiated\",\"id\":\"127.0.0.1:41944\"}\n{\"time\":\"2025-05-20T17:13:42.871382233Z\",\"level\":\"INFO\",\"msg\":\"connection: closing\",\"id\":\"127.0.0.1:41944\"}\n{\"time\":\"2025-05-20T17:13:42.871411934Z\",\"level\":\"INFO\",\"msg\":\"server is shutting down\"}\n{\"time\":\"2025-05-20T17:13:42.871478792Z\",\"level\":\"INFO\",\"msg\":\"connection: closed successfully\",\"id\":\"127.0.0.1:41944\"}\n{\"time\":\"2025-05-20T17:13:43.3348318Z\",\"level\":\"INFO\",\"msg\":\"handleInformTeardown: server teardown initiated\",\"id\":\"127.0.0.1:34248\"}\n{\"time\":\"2025-05-20T17:13:43.334932292Z\",\"level\":\"INFO\",\"msg\":\"server is shutting down\"}\n{\"time\":\"2025-05-20T17:13:43.334925864Z\",\"level\":\"INFO\",\"msg\":\"connection: closing\",\"id\":\"127.0.0.1:34248\"}\n{\"time\":\"2025-05-20T17:13:43.334994672Z\",\"level\":\"ERROR\",\"msg\":\"processOutgoingData: flush error\",\"error\":\"write tcp 127.0.0.1:42417->127.0.0.1:34248: use of closed network connection\",\"id\":\"127.0.0.1:34248\"}\n{\"time\":\"2025-05-20T17:13:43.335047611Z\",\"level\":\"INFO\",\"msg\":\"connection: closed successfully\",\"id\":\"127.0.0.1:34248\"}\n{\"time\":\"2025-05-20T17:13:43.453962609Z\",\"level\":\"INFO\",\"msg\":\"handleInformTeardown: server teardown initiated\",\"id\":\"127.0.0.1:38562\"}\n{\"time\":\"2025-05-20T17:13:43.454034825Z\",\"level\":\"INFO\",\"msg\":\"connection: closing\",\"id\":\"127.0.0.1:38562\"}\n{\"time\":\"2025-05-20T17:13:43.454061385Z\",\"level\":\"INFO\",\"msg\":\"server is shutting down\"}\n{\"time\":\"2025-05-20T17:13:43.454135062Z\",\"level\":\"INFO\",\"msg\":\"connection: closed successfully\",\"id\":\"127.0.0.1:38562\"}\n```\n\n### `debug-internal.log`\n```\n{\"time\":\"2025-05-20T17:13:12.741568652Z\",\"level\":\"INFO\",\"msg\":\"stream: starting\",\"core version\":\"0.19.11\",\"symlink path\":\"/home/alex/wandb/run-20250520_171312-61auyx53/logs/debug-core.log\"}\n{\"time\":\"2025-05-20T17:13:12.742586198Z\",\"level\":\"INFO\",\"msg\":\"stream: starting\",\"core version\":\"0.19.11\",\"symlink path\":\"/home/alex/wandb/run-20250520_171312-61auyx53/logs/debug-core.log\"}\n{\"time\":\"2025-05-20T17:13:12.742594965Z\",\"level\":\"INFO\",\"msg\":\"stream: starting\",\"core version\":\"0.19.11\",\"symlink path\":\"/home/alex/wandb/run-20250520_171312-61auyx53/logs/debug-core.log\"}\n{\"time\":\"2025-05-20T17:13:12.743320358Z\",\"level\":\"INFO\",\"msg\":\"stream: starting\",\"core version\":\"0.19.11\",\"symlink path\":\"/home/alex/wandb/run-20250520_171312-61auyx53/logs/debug-core.log\"}\n{\"time\":\"2025-05-20T17:13:12.873586992Z\",\"level\":\"INFO\",\"msg\":\"created new stream\",\"id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.873632588Z\",\"level\":\"INFO\",\"msg\":\"stream: started\",\"id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.873689357Z\",\"level\":\"INFO\",\"msg\":\"sender: started\",\"stream_id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.873700065Z\",\"level\":\"INFO\",\"msg\":\"writer: Do: started\",\"stream_id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.873746719Z\",\"level\":\"INFO\",\"msg\":\"handler: started\",\"stream_id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.875266829Z\",\"level\":\"INFO\",\"msg\":\"created new stream\",\"id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.875306197Z\",\"level\":\"INFO\",\"msg\":\"stream: started\",\"id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.875333498Z\",\"level\":\"INFO\",\"msg\":\"sender: started\",\"stream_id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.875342226Z\",\"level\":\"INFO\",\"msg\":\"handler: started\",\"stream_id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.875354523Z\",\"level\":\"INFO\",\"msg\":\"writer: Do: started\",\"stream_id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.877822852Z\",\"level\":\"INFO\",\"msg\":\"created new stream\",\"id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.8778449Z\",\"level\":\"INFO\",\"msg\":\"stream: started\",\"id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.877856291Z\",\"level\":\"INFO\",\"msg\":\"writer: Do: started\",\"stream_id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.877877574Z\",\"level\":\"INFO\",\"msg\":\"handler: started\",\"stream_id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.877896242Z\",\"level\":\"INFO\",\"msg\":\"sender: started\",\"stream_id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.982021775Z\",\"level\":\"INFO\",\"msg\":\"created new stream\",\"id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.982047646Z\",\"level\":\"INFO\",\"msg\":\"stream: started\",\"id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.982072166Z\",\"level\":\"INFO\",\"msg\":\"sender: started\",\"stream_id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.982065986Z\",\"level\":\"INFO\",\"msg\":\"handler: started\",\"stream_id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:12.982094421Z\",\"level\":\"INFO\",\"msg\":\"writer: Do: started\",\"stream_id\":\"61auyx53\"}\n{\"time\":\"2025-05-20T17:13:13.091963569Z\",\"level\":\"INFO\",\"msg\":\"Starting system monitor\"}\n{\"time\":\"2025-05-20T17:13:13.093389786Z\",\"level\":\"INFO\",\"msg\":\"api: retrying HTTP error\",\"status\":409,\"url\":\"https://api.wandb.ai/graphql\",\"body\":\"{\\\"errors\\\":[{\\\"message\\\":\\\"Error 1062 (23000): Duplicate entry '40991226-61auyx53' for key 'runs.PRIMARY'\\\",\\\"path\\\":[\\\"upsertBucket\\\"]}],\\\"data\\\":{\\\"upsertBucket\\\":null}}\"}\n{\"time\":\"2025-05-20T17:13:13.107747269Z\",\"level\":\"INFO\",\"msg\":\"api: retrying HTTP error\",\"status\":409,\"url\":\"https://api.wandb.ai/graphql\",\"body\":\"{\\\"errors\\\":[{\\\"message\\\":\\\"Error 1062 (23000): Duplicate entry '40991226-61auyx53' for key 'runs.PRIMARY'\\\",\\\"path\\\":[\\\"upsertBucket\\\"]}],\\\"data\\\":{\\\"upsertBucket\\\":null}}\"}\n{\"time\":\"2025-05-20T17:13:13.1352274Z\",\"level\":\"INFO\",\"msg\":\"api: retrying HTTP error\",\"status\":409,\"url\":\"https://api.wandb.ai/graphql\",\"body\":\"{\\\"errors\\\":[{\\\"message\\\":\\\"Error 1062 (23000): Duplicate entry '40991226-61auyx53' for key 'runs.PRIMARY'\\\",\\\"path\\\":[\\\"upsertBucket\\\"]}],\\\"data\\\":{\\\"upsertBucket\\\":null}}\"}\n{\"time\":\"2025-05-20T17:13:15.243357787Z\",\"level\":\"INFO\",\"msg\":\"Starting system monitor\"}\n{\"time\":\"2025-05-20T17:13:15.594902505Z\",\"level\":\"INFO\",\"msg\":\"Starting system monitor\"}\n{\"time\":\"2025-05-20T17:13:15.654906053Z\",\"level\":\"INFO\",\"msg\":\"Starting system monitor\"}\n....\n```\n\n### `debug.log`\n```\n2025-05-20 17:13:12,735 INFO    MainThread:21630 [wandb_setup.py:_flush():70] Current SDK version is 0.19.11\n2025-05-20 17:13:12,735 INFO    MainThread:21630 [wandb_setup.py:_flush():70] Configure stats pid to 21630\n2025-05-20 17:13:12,735 INFO    MainThread:21630 [wandb_setup.py:_flush():70] Loading settings from /home/alex/.config/wandb/settings\n2025-05-20 17:13:12,735 INFO    MainThread:21630 [wandb_setup.py:_flush():70] Loading settings from /home/alex/repo/wandb/settings\n2025-05-20 17:13:12,735 INFO    MainThread:21630 [wandb_setup.py:_flush():70] Loading settings from environment variables\n2025-05-20 17:13:12,735 INFO    MainThread:21630 [wandb_init.py:setup_run_log_directory():724] Logging user logs to /home/alex/wandb/run-20250520_171312-61auyx53/logs/debug.log\n2025-05-20 17:13:12,735 INFO    MainThread:21630 [wandb_init.py:setup_run_log_directory():725] Logging internal logs to /home/alex/wandb/run-20250520_171312-61auyx53/logs/debug-internal.log\n2025-05-20 17:13:12,735 INFO    MainThread:21630 [wandb_init.py:init():852] calling init triggers\n2025-05-20 17:13:12,736 INFO    MainThread:21630 [wandb_init.py:init():857] wandb.init called with sweep_config: {}\nconfig: {...my config stuff..., '_wandb': {}}\n2025-05-20 17:13:12,736 INFO    MainThread:21630 [wandb_init.py:init():893] starting backend\n2025-05-20 17:13:12,736 INFO    MainThread:21630 [wandb_init.py:init():897] sending inform_init request\n2025-05-20 17:13:12,736 INFO    MainThread:21633 [wandb_setup.py:_flush():70] Current SDK version is 0.19.11\n2025-05-20 17:13:12,736 INFO    MainThread:21633 [wandb_setup.py:_flush():70] Configure stats pid to 21633\n2025-05-20 17:13:12,736 INFO    MainThread:21633 [wandb_setup.py:_flush():70] Loading settings from /home/alex/.config/wandb/settings\n2025-05-20 17:13:12,736 INFO    MainThread:21633 [wandb_setup.py:_flush():70] Loading settings from /home/alex/repo/wandb/settings\n2025-05-20 17:13:12,736 INFO    MainThread:21633 [wandb_setup.py:_flush():70] Loading settings from environment variables\n2025-05-20 17:13:12,736 INFO    MainThread:21631 [wandb_setup.py:_flush():70] Current SDK version is 0.19.11\n2025-05-20 17:13:12,736 INFO    MainThread:21633 [wandb_init.py:setup_run_log_directory():724] Logging user logs to /home/alex/wandb/run-20250520_171312-61auyx53/logs/debug.log\n2025-05-20 17:13:12,736 INFO    MainThread:21631 [wandb_setup.py:_flush():70] Configure stats pid to 21631\n2025-05-20 17:13:12,736 INFO    MainThread:21631 [wandb_setup.py:_flush():70] Loading settings from /home/alex/.config/wandb/settings\n2025-05-20 17:13:12,736 INFO    MainThread:21633 [wandb_init.py:setup_run_log_directory():725] Logging internal logs to /home/alex/wandb/run-20250520_171312-61auyx53/logs/debug-internal.log\n2025-05-20 17:13:12,736 INFO    MainThread:21631 [wandb_setup.py:_flush():70] Loading settings from /home/alex/repo/wandb/settings\n2025-05-20 17:13:12,736 INFO    MainThread:21632 [wandb_setup.py:_flush():70] Current SDK version is 0.19.11\n2025-05-20 17:13:12,736 INFO    MainThread:21631 [wandb_setup.py:_flush():70] Loading settings from environment variables\n2025-05-20 17:13:12,736 INFO    MainThread:21633 [wandb_init.py:init():852] calling init triggers\n2025-05-20 17:13:12,736 INFO    MainThread:21632 [wandb_setup.py:_flush():70] Configure stats pid to 21632\n2025-05-20 17:13:12,736 INFO    MainThread:21632 [wandb_setup.py:_flush():70] Loading settings from /home/alex/.config/wandb/settings\n2025-05-20 17:13:12,736 INFO    MainThread:21631 [wandb_init.py:setup_run_log_directory():724] Logging user logs to /home/alex/wandb/run-20250520_171312-61auyx53/logs/debug.log\n2025-05-20 17:13:12,736 INFO    MainThread:21632 [wandb_setup.py:_flush():70] Loading settings from /home/alex/repo/wandb/settings\n2025-05-20 17:13:12,736 INFO    MainThread:21632 [wandb_setup.py:_flush():70] Loading settings from environment variables\n2025-05-20 17:13:12,736 INFO    MainThread:21631 [wandb_init.py:setup_run_log_directory():725] Logging internal logs to /home/alex/wandb/run-20250520_171312-61auyx53/logs/debug-internal.log\n2025-05-20 17:13:12,736 INFO    MainThread:21633 [wandb_init.py:init():857] wandb.init called with sweep_config: {}\nconfig: {...my config stuff..., '_wandb': {}}\n2025-05-20 17:13:12,736 INFO    MainThread:21633 [wandb_init.py:init():893] starting backend\n2025-05-20 17:13:12,736 INFO    MainThread:21631 [wandb_init.py:init():852] calling init triggers\n2025-05-20 17:13:12,736 INFO    MainThread:21632 [wandb_init.py:setup_run_log_directory():724] Logging user logs to /home/alex/wandb/run-20250520_171312-61auyx53/logs/debug.log\n2025-05-20 17:13:12,736 INFO    MainThread:21633 [wandb_init.py:init():897] sending inform_init request\n2025-05-20 17:13:12,736 INFO    MainThread:21632 [wandb_init.py:setup_run_log_directory():725] Logging internal logs to /home/alex/wandb/run-20250520_171312-61auyx53/logs/debug-internal.log\n2025-05-20 17:13:12,736 INFO    MainThread:21632 [wandb_init.py:init():852] calling init triggers\n2025-05-20 17:13:12,736 INFO    MainThread:21631 [wandb_init.py:init():857] wandb.init called with sweep_config: {}\nconfig: {...my config stuff..., '_wandb': {}}\n2025-05-20 17:13:12,736 INFO    MainThread:21631 [wandb_init.py:init():893] starting backend\n2025-05-20 17:13:12,736 INFO    MainThread:21631 [wandb_init.py:init():897] sending inform_init request\n2025-05-20 17:13:12,736 INFO    MainThread:21632 [wandb_init.py:init():857] wandb.init called with sweep_config: {}\nconfig: {...my config stuff..., '_wandb': {}}\n2025-05-20 17:13:12,737 INFO    MainThread:21632 [wandb_init.py:init():893] starting backend\n2025-05-20 17:13:12,737 INFO    MainThread:21632 [wandb_init.py:init():897] sending inform_init request\n2025-05-20 17:13:12,739 INFO    MainThread:21630 [backend.py:_multiprocessing_setup():101] multiprocessing start_methods=fork,spawn,forkserver, using: spawn\n2025-05-20 17:13:12,739 INFO    MainThread:21630 [wandb_init.py:init():907] backend started and connected\n2025-05-20 17:13:12,740 INFO    MainThread:21633 [backend.py:_multiprocessing_setup():101] multiprocessing start_methods=fork,spawn,forkserver, using: spawn\n2025-05-20 17:13:12,740 INFO    MainThread:21633 [wandb_init.py:init():907] backend started and connected\n2025-05-20 17:13:12,740 INFO    MainThread:21631 [backend.py:_multiprocessing_setup():101] multiprocessing start_methods=fork,spawn,forkserver, using: spawn\n2025-05-20 17:13:12,740 INFO    MainThread:21631 [wandb_init.py:init():907] backend started and connected\n2025-05-20 17:13:12,740 INFO    MainThread:21632 [backend.py:_multiprocessing_setup():101] multiprocessing start_methods=fork,spawn,forkserver, using: spawn\n2025-05-20 17:13:12,740 INFO    MainThread:21632 [wandb_init.py:init():907] backend started and connected\n2025-05-20 17:13:12,742 INFO    MainThread:21630 [wandb_init.py:init():1005] updated telemetry\n2025-05-20 17:13:12,744 INFO    MainThread:21633 [wandb_init.py:init():1005] updated telemetry\n2025-05-20 17:13:12,745 INFO    MainThread:21631 [wandb_init.py:init():1005] updated telemetry\n2025-05-20 17:13:12,745 INFO    MainThread:21632 [wandb_init.py:init():1005] updated telemetry\n2025-05-20 17:13:12,747 INFO    MainThread:21630 [wandb_init.py:init():1029] communicating run to backend with 90.0 second timeout\n2025-05-20 17:13:12,749 INFO    MainThread:21633 [wandb_init.py:init():1029] communicating run to backend with 90.0 second timeout\n2025-05-20 17:13:12,749 INFO    MainThread:21631 [wandb_init.py:init():1029] communicating run to backend with 90.0 second timeout\n2025-05-20 17:13:12,750 INFO    MainThread:21632 [wandb_init.py:init():1029] communicating run to backend with 90.0 second timeout\n2025-05-20 17:13:13,090 INFO    MainThread:21631 [wandb_init.py:init():1104] starting run threads in backend\n2025-05-20 17:13:13,291 INFO    MainThread:21631 [wandb_run.py:_console_start():2573] atexit reg\n2025-05-20 17:13:13,292 INFO    MainThread:21631 [wandb_run.py:_redirect():2421] redirect: wrap_raw\n2025-05-20 17:13:13,293 INFO    MainThread:21631 [wandb_run.py:_redirect():2490] Wrapping output streams.\n2025-05-20 17:13:13,293 INFO    MainThread:21631 [wandb_run.py:_redirect():2513] Redirects installed.\n2025-05-20 17:13:13,296 INFO    MainThread:21631 [wandb_init.py:init():1150] run started, returning control to user process\n2025-05-20 17:13:15,200 INFO    MainThread:21630 [wandb_init.py:init():1104] starting run threads in backend\n2025-05-20 17:13:15,444 INFO    MainThread:21630 [wandb_run.py:_console_start():2573] atexit reg\n2025-05-20 17:13:15,444 INFO    MainThread:21630 [wandb_run.py:_redirect():2421] redirect: wrap_raw\n2025-05-20 17:13:15,444 INFO    MainThread:21630 [wandb_run.py:_redirect():2490] Wrapping output streams.\n2025-05-20 17:13:15,444 INFO    MainThread:21630 [wandb_run.py:_redirect():2513] Redirects installed.\n2025-05-20 17:13:15,447 INFO    MainThread:21630 [wandb_init.py:init():1150] run started, returning control to user process\n2025-05-20 17:13:15,546 INFO    MainThread:21632 [wandb_init.py:init():1104] starting run threads in backend\n2025-05-20 17:13:15,612 INFO    MainThread:21633 [wandb_init.py:init():1104] starting run threads in backend\n2025-05-20 17:13:15,802 INFO    MainThread:21632 [wandb_run.py:_console_start():2573] atexit reg\n2025-05-20 17:13:15,803 INFO    MainThread:21632 [wandb_run.py:_redirect():2421] redirect: wrap_raw\n2025-05-20 17:13:15,803 INFO    MainThread:21632 [wandb_run.py:_redirect():2490] Wrapping output streams.\n2025-05-20 17:13:15,803 INFO    MainThread:21632 [wandb_run.py:_redirect():2513] Redirects installed.\n2025-05-20 17:13:15,807 INFO    MainThread:21632 [wandb_init.py:init():1150] run started, returning control to user process\n2025-05-20 17:13:15,860 INFO    MainThread:21633 [wandb_run.py:_console_start():2573] atexit reg\n2025-05-20 17:13:15,860 INFO    MainThread:21633 [wandb_run.py:_redirect():2421] redirect: wrap_raw\n2025-05-20 17:13:15,861 INFO    MainThread:21633 [wandb_run.py:_redirect():2490] Wrapping output streams.\n2025-05-20 17:13:15,861 INFO    MainThread:21633 [wandb_run.py:_redirect():2513] Redirects installed.\n2025-05-20 17:13:15,865 INFO    MainThread:21633 [wandb_init.py:init():1150] run started, returning control to user process\n2025-05-20 17:13:42,811 INFO    MsgRouterThr:21632 [mailbox.py:close():129] [no run ID] Closing mailbox, abandoning 1 handles.\n2025-05-20 17:13:42,870 INFO    MsgRouterThr:21633 [mailbox.py:close():129] [no run ID] Closing mailbox, abandoning 1 handles.\n2025-05-20 17:13:43,302 INFO    MsgRouterThr:21631 [mailbox.py:close():129] [no run ID] Closing mailbox, abandoning 3 handles.\n2025-05-20 17:13:43,453 INFO    MsgRouterThr:21630 [mailbox.py:close():129] [no run ID] Closing mailbox, abandoning 1 handles.\n...\n```"
      },
      {
        "user": "fmamberti-wandb",
        "body": "> Yup, I'm referring to the parameters for just those models. The gradients are only coming through for the conditioner, but it sounds like that's expected as https://github.com/wandb/wandb/issues/9866.\n\nYep, as my colleague mentioned, logging gradients is not supported for FSDP models.\n\n> the same init error is thrown\n\nThis is currently known and happens if multiple init requests for distributed processes hit the server at the same time. We are currently working on a fix and will keep you posted for any updates.\n\nRegarding sharing more sensitive info such as the URL you can use the `support@wandb.com` email address"
      }
    ]
  },
  {
    "issue_number": 4387,
    "title": "creating wandb.Audio from pathlib.Path raises error",
    "author": "sammlapp",
    "state": "closed",
    "created_at": "2022-10-19T18:38:57Z",
    "updated_at": "2025-05-23T05:50:10Z",
    "labels": [
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n<!--- Description of the issue below  -->\r\nTrying to create a wandb.Audio object by passing a pathlib.Path instead of a string raises \"ValueError: Argument \"sample_rate\" is required when instantiating wandb.Audio with raw data.\"\r\n\r\nBecause the class accepts either a path or raw audio data, I would expect it to also handle a pathlib.Path object as if it were a string. \r\n\r\n<!--- A minimal code snippet between the quotes below  -->\r\n```python\r\nfrom pathlib import path\r\nimport wandb\r\nwandb.Audio(Path('file.wav'))\r\n```\r\n\r\n<!--- A full traceback of the exception in the quotes below -->\r\n```shell\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n----> 1 wandb.Audio(Path('audio.wav'))\r\n\r\n~/miniconda3/envs/opso_dev/lib/python3.9/site-packages/wandb/data_types.py in __init__(self, data_or_path, sample_rate, caption)\r\n   1055         else:\r\n   1056             if sample_rate is None:\r\n-> 1057                 raise ValueError(\r\n   1058                     'Argument \"sample_rate\" is required when instantiating wandb.Audio with raw data.'\r\n   1059                 )\r\n\r\nValueError: Argument \"sample_rate\" is required when instantiating wandb.Audio with raw data.\r\n```\r\n\n\n### Additional Files\n\n_No response_\n\n### Environment\n\nWandB version: '0.13.4'\r\n\r\nOS: Mac OS 12.6\r\n\r\nPython version: Python 3.9.13\r\n\r\nVersions of relevant libraries:  \r\n\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "ramit-wandb",
        "body": "Hi @sammlapp,\r\n\r\nAs the error message suggests, `sample_rate` is a required argument when reading from a file. Please pass in the sample rate to your `wandb.Audio` object. You can read more about the arguments on our docs here : https://docs.wandb.ai/ref/python/data-types/audio\r\n\r\nThanks,\r\nRamit"
      },
      {
        "user": "kptkin",
        "body": "@sammlapp Thanks for reporting this issue. You are right we are currently don't handle `pathlib.Path` (see the documentation linked by @ramit-wandb) in the code. In the meantime you could convert the `Path` instance to a string and it should work for the current version. We are going to add support for `Path` in on of the future releases."
      }
    ]
  },
  {
    "issue_number": 9874,
    "title": "[Bug]: Deprecation Warning when logging gif using wandb.Video",
    "author": "gianlucageraci",
    "state": "closed",
    "created_at": "2025-05-19T15:07:16Z",
    "updated_at": "2025-05-23T00:53:17Z",
    "labels": [
      "ty:bug",
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nWhen attempting to log a video in GIF format using `wandb.Video`, the following deprecation warning appears:\n``` .../miniconda3/envs/dreamer/lib/python3.10/site-packages/imageio/plugins/pillow.py:409: DeprecationWarning: The keyword `fps` is no longer supported. Use `duration`(in ms) instead, e.g. `fps=50` == `duration=20` (1000 * 1/50). ```\n\nThe warning can be traced back to the method `write_gif_with_image_io` in [video.py](https://github.com/wandb/wandb/blob/a77e7c56b72a6fa52acc8c574eba23e946cb7897/wandb/sdk/data_types/video.py#L42)\n\n```python\ndef write_gif_with_image_io(\n    clip: Any, filename: str, fps: Optional[int] = None\n) -> None:\n    imageio = util.get_module(\n        \"imageio\",\n        required='wandb.Video requires imageio when passing raw data. Install with \"pip install wandb[media]\"',\n    )\n\n    writer = imageio.save(filename, fps=clip.fps, quantizer=0, palettesize=256, loop=0)\n\n    for frame in clip.iter_frames(fps=fps, dtype=\"uint8\"):\n        writer.append_data(frame)\n\n    writer.close()\n```\nThe warning stems from the use of the now-deprecated `fps` keyword in `imageio.save`. According to the warning message, this should be replaced with the `duration` keyword (in milliseconds). The updated line could be:\n```python imageio.save(filename,  duration=round(1000/ clip.fps), quantizer=0, palettesize=256, loop=0)``` \n\nThere was a recent [merge request](https://github.com/wandb/wandb/pull/6887) that reverted from `duration` back to `fps` due to a bug in GIF creation. However, it seems the issue likely stemmed from using `duration=1.0 / clip.fps` (in seconds), rather than `duration=1000 / clip.fps` (in milliseconds) as required by `imageio`. This would also explain why the \nIt should be possible to fix the warning and maintain correct functionality by switching to `duration=1000 / clip.fps` and avoiding the deprecated `fps` keyword.\n\nNOTE:  So far this is a theoretical fix based on the documentation and warning message, i have not yet evaluated the proposed changes. If desired, i can do so and open a merge request.\n\n",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nThanks so much for reporting this! We are reviewing and will update you here of any progress 🙂\n"
      }
    ]
  },
  {
    "issue_number": 4449,
    "title": "[Feature]: Display two curves in one graph?",
    "author": "Freed-Wu",
    "state": "open",
    "created_at": "2022-11-04T14:45:22Z",
    "updated_at": "2025-05-22T18:20:30Z",
    "labels": [
      "ty:feature",
      "c:sdk:tensorboard",
      "a:sdk"
    ],
    "body": "### Description\r\n\r\n```python\r\n#!/usr/bin/env python\r\n\"\"\"main.\"\"\"\r\nimport wandb\r\nfrom torch.utils.tensorboard.writer import SummaryWriter\r\n\r\nmodel = image_models[\"mbt2018\"](pretrained=True, quality=6)\r\n\r\nlog_dir = '/output/logs'\r\n\r\nname = '5'\r\nwandb.init(project=\"my-test-project-2\", dir=log_dir, name=name, id=name,\r\n           config={\"learning_rate\": 0.001, \"epochs\": 100, \"batch_size\": 128},\r\n           sync_tensorboard=True)\r\n\r\nwriter = SummaryWriter(log_dir=log_dir)\r\n\r\nwriter.add_scalars('metrics/loss', {'train': 20, 'val': 100}, 0)\r\nwriter.add_scalars('metrics/loss', {'train': 10, 'val':  50}, 1)\r\n\r\n# Optional\r\nwandb.finish()\r\n```\r\n\r\nCan it display curves of same tag to one graph?\r\n\r\nThis is wandb:\r\n\r\n![screen-2022-11-04-22-44-28](https://user-images.githubusercontent.com/32936898/200003372-763ad8bf-0cf8-457a-baa9-ab46b3f246da.jpg)\r\n\r\nThis is tensorboard:\r\n\r\n![screen-2022-11-04-22-44-06](https://user-images.githubusercontent.com/32936898/200003355-37555673-8831-4938-8c53-2db58679cfba.jpg)\r\n\r\n\r\n### Suggested Solution\r\n\r\n/\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "ramit-wandb",
        "body": "Hi @Freed-Wu,\r\n\r\nYup, this is already possible. You can read about it in our docs here : https://docs.wandb.ai/ref/app/features/panels/line-plot#compare-two-metrics-on-one-chart"
      },
      {
        "user": "Freed-Wu",
        "body": "Can it support tensorboard's `add_scalars`?"
      },
      {
        "user": "ramit-wandb",
        "body": "Hey @Freed-Wu,\r\n\r\nYes, `add_scalar` from tensorboard is supported."
      }
    ]
  },
  {
    "issue_number": 9830,
    "title": "[Feature]: adding docs and option to kill a specific agent",
    "author": "YanivDorGalron",
    "state": "closed",
    "created_at": "2025-05-08T16:20:46Z",
    "updated_at": "2025-05-22T17:50:46Z",
    "labels": [
      "ty:feature"
    ],
    "body": "### Description\n\n<!--- Describe your feature here --->\nI couldn't find any documentation about impute argument when using bayes sweep. also - frequently i try to kill a specific agent without success\nits also not clear if crashed, failed or killed runs affect bayes sweep optimization even after deleting it via the gui.\n###  Suggested Solution\n\n<!--- Describe your solution here --->\n",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @YanivDorGalron, thank you for writing in. \n\nYou're right that this isn't currently documented in the public docs. The impute argument is an internal option used during Bayesian optimization to handle missing or failed runs in the sweep history. It helps the optimizer estimate values for missing metrics. If you're seeing unexpected behavior, feel free to share your sweep config and any relevant logs/code so we can investigate further. \n\nYes, failed, crashed, or manually killed runs can still influence the sweep optimization, since W&B tracks their outcomes unless they are explicitly removed. Deleting them via the UI does not currently purge them from the optimizer’s internal history. "
      },
      {
        "user": "YanivDorGalron",
        "body": "Hey @ArtsiomWB , thank you for answering.\n\nI would like know:\n1. How can I kill a specific agent given a series of agents from multiple sweeps on a single server?\n2. How can I delete failed runs from the optimizer/prevent the optimizer to take into account failed runs?\n3. Given a metric I log multiple times and i want to maximize. The sweep will treat each run according to the maximum logged values instead of the last logged value - is there a way to change this?\n\nI will note that failed runs effecting the sweeps is a major concert on my end as sweeps frequently fails for me especially in the first tries. This demands me to recreate sweeps everytime."
      },
      {
        "user": "YanivDorGalron",
        "body": "@ArtsiomWB another thing i would like to know is how can i launch sweep from a specific server and apply changes to the code without it taking an affect on the next launched runs. currently if i would just change branch the code that will be excecuted next time the agent will launch a run will be changed as well."
      }
    ]
  },
  {
    "issue_number": 9842,
    "title": "[Bug-App]: Unable to remove a run from a group",
    "author": "oleg-kachan",
    "state": "open",
    "created_at": "2025-05-11T11:18:06Z",
    "updated_at": "2025-05-21T18:38:30Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\nI do not see how to remove a run from a group, to not belong to any group at all.\n\nSo, I can move a run into a group, but can not move out a run from a group, if the destination is not assigning a run to a group at all.\n\nTried both\n\n- \"Runs\" screen, https://wandb.ai/ENTITY/PROJECT/table\n- \"Overview\" screen, https://wandb.ai/ENTITY/PROJECT/runs/RUN/overview\n",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nHi there,\n\nThanks for reaching out! You're absolutely right—while you can currently move a run into a group, there isn't a direct way to remove a run from a group entirely (i.e., to make it not belong to _any_ group) via the W&B UI.\n\nWe've seen this come up occasionally and understand how having more flexible control over run grouping could be useful—especially for organizing or cleaning up experiments. I've gone ahead and logged this internally as a **feature request** so our product and engineering teams can review it for consideration in future updates.\n\nIn the meantime, a potential workaround is to assign such runs to a neutral placeholder group (e.g., `ungrouped-temp`) so they’re visually separated. It’s not ideal, but it may help distinguish runs you don’t want actively grouped.\n\nAppreciate your feedback—and let us know if we can help with anything else in the meantime!\n\nBest,\nJason\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nHi Internal,\n\nWe wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.\n\nBest,\nWeights & Biases\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nHi Internal, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!\n"
      }
    ]
  },
  {
    "issue_number": 9869,
    "title": "[Bug]: Wandb causes UninitializedParameter (lazy-loaded parameters) to crash",
    "author": "Vectorrent",
    "state": "closed",
    "created_at": "2025-05-16T04:21:00Z",
    "updated_at": "2025-05-21T15:54:07Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nwandb version: 0.19.9\npython version: 3.13.3\noperating system: Arch Linux\n\nWhen I try to use the wandb logger with a model that has any `UninitializedParameter` modules, the model crashes during bootstrap. It fails when I run this command:\n```py\nwandb_logger.watch(model, log=\"all\", log_freq=100, log_graph=False)\n```\nHere is the crash dump:\n```\nwandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: vectorrent (unsafe) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.19.9\nwandb: Run data is saved locally in data/wandb/run-20250515_230830-c5i30m97\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run fearless-sun-1738\nwandb: ⭐ View project at https://wandb.ai/unsafe/praxis\nwandb: 🚀 View run at https://wandb.ai/unsafe/praxis/runs/c5i30m97\nTraceback (most recent call last):\n  File \"/home/crow/repos/praxis/run.py\", line 1233, in <module>\n    wandb_logger.watch(model, log=\"all\", log_freq=100, log_graph=False)\n    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/crow/repos/praxis/.venv/lib/python3.13/site-packages/lightning/pytorch/loggers/wandb.py\", line 424, in watch\n    self.experiment.watch(model, log=log, log_freq=log_freq, log_graph=log_graph)\n    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/crow/repos/praxis/.venv/lib/python3.13/site-packages/wandb/sdk/wandb_run.py\", line 387, in wrapper\n    return func(self, *args, **kwargs)\n  File \"/home/crow/repos/praxis/.venv/lib/python3.13/site-packages/wandb/sdk/wandb_run.py\", line 425, in wrapper\n    return func(self, *args, **kwargs)\n  File \"/home/crow/repos/praxis/.venv/lib/python3.13/site-packages/wandb/sdk/wandb_run.py\", line 2919, in watch\n    wandb.sdk._watch(self, models, criterion, log, log_freq, idx, log_graph)\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/crow/repos/praxis/.venv/lib/python3.13/site-packages/wandb/sdk/wandb_watch.py\", line 113, in _watch\n    run._torch.add_log_gradients_hook(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        model,\n        ^^^^^^\n        prefix=prefix,\n        ^^^^^^^^^^^^^^\n        log_freq=log_freq,\n        ^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/crow/repos/praxis/.venv/lib/python3.13/site-packages/wandb/integration/torch/wandb_torch.py\", line 149, in add_log_gradients_hook\n    self._hook_variable_gradient_stats(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        parameter, \"gradients/\" + prefix + name, log_track_grad\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/crow/repos/praxis/.venv/lib/python3.13/site-packages/wandb/integration/torch/wandb_torch.py\", line 276, in _hook_variable_gradient_stats\n    handle = var.register_hook(lambda grad: _callback(grad, log_track))\n  File \"/home/crow/repos/praxis/.venv/lib/python3.13/site-packages/torch/_tensor.py\", line 664, in register_hook\n    return handle_torch_function(Tensor.register_hook, (self,), self, hook)\n  File \"/home/crow/repos/praxis/.venv/lib/python3.13/site-packages/torch/overrides.py\", line 1742, in handle_torch_function\n    result = torch_func_method(public_api, types, args, kwargs)\n  File \"/home/crow/repos/praxis/.venv/lib/python3.13/site-packages/torch/nn/parameter.py\", line 168, in __torch_function__\n    raise ValueError(\n    ...<5 lines>...\n    )\nValueError: Attempted to use an uninitialized parameter in <function Tensor.register_hook at 0x7fe3a110b380>. This error happens when you are using a `LazyModule` or explicitly manipulating `torch.nn.parameter.UninitializedParameter` objects. When using LazyModules Call `forward` with a dummy batch to initialize the parameters before calling torch functions\nwandb: \nwandb: 🚀 View run fearless-sun-1738 at: https://wandb.ai/unsafe/praxis/runs/c5i30m97\n```\n**However,** if I set `log=\"parameters\"` or `log=None` here, the model does NOT crash. It only fails when hooking into gradients.\n\nElsewhere in my code, I ran into the exact same kind of crash, when trying to use `numel()` with uninitialized parameters. I was able to resolve that error by skipping uninitialized modules:\n```py\ndef count_initialized_params(model):\n    \"\"\"\n    Count only initialized parameters in a model, skipping UninitializedParameters.\n    \"\"\"\n    from torch.nn.parameter import UninitializedParameter\n\n    total = 0\n    for p in model.parameters():\n        # We have to skip uninitialized parameters, because we don't\n        # know how many parameters they will have and it crashes.\n        if not isinstance(p, UninitializedParameter):\n            total += p.numel()\n    return total\n\ntotal_params = count_initialized_params(model)\n```\nI think the strategy could be similar here. Let me know if you need any additional information.",
    "comments": []
  },
  {
    "issue_number": 9854,
    "title": "[Q]:     The run of the newly - created experiment can be seen on the official website of Weights & Biases (wandb), but the data cannot be synchronized.",
    "author": "1310837268",
    "state": "open",
    "created_at": "2025-05-14T16:37:14Z",
    "updated_at": "2025-05-21T01:03:38Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\n<!--- Ask your question here --->\nHow should I solve this problem? Here are the screenshots of the error log.\n\n![Image](https://github.com/user-attachments/assets/dd6a3120-da43-4d00-a77b-3a86cbe7d544)",
    "comments": [
      {
        "user": "JoanaMarieL",
        "body": "Hi @1310837268 , thank you for writing in and happy to help. May we ask for the following to better assess the issue:\n\n-  briefly describe your specific experiment or use case when you encounter this error\n- code snippet for us to try to reproduce\n- environment where you are running your code\n"
      },
      {
        "user": "JoanaMarieL",
        "body": "Hi @1310837268 , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!\n"
      }
    ]
  },
  {
    "issue_number": 9877,
    "title": "[Q]: Fine-tuning best practices",
    "author": "echen1214",
    "state": "closed",
    "created_at": "2025-05-19T22:47:21Z",
    "updated_at": "2025-05-20T22:22:29Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\nHi all, What are some best practices for using wandb to log multiple rounds of fine-tuning? For example, let's say I have one run logged for 100 epoch. I want to spawn 3 fine-tuning runs with different hyperparameters or data that start from the last epoch of the initial run. I've used `wandb.init(\"id\":<id>,\"resume\":\"must\")` to resume runs, but this doesn't allow me multiple fine-tuning runs. \n\nThanks for the help!",
    "comments": [
      {
        "user": "JasonArkens17",
        "body": "Hi @echen1214,\n\nThanks for reaching out! I understand you're looking to create multiple fine-tuning runs that start from the same checkpoint of an initial run.\n\nBased on your description, you're trying to:\n1. Complete an initial run for 100 epochs\n2. Create 3 different fine-tuning runs starting from that 100th epoch\n3. Use different hyperparameters or data for each of these new runs\n\nThe \"resume\" functionality you've been using (`wandb.init(\"id\":<id>,\"resume\":\"must\")`) is designed to continue the exact same run, which is why it doesn't support multiple branching runs. What you're trying to do is actually a perfect use case for our \"fork\" feature!\n\nThe forking feature allows you to create new runs that branch off from a specific point in an existing run. This lets you explore different parameters or models without affecting the original run - exactly what you need!\n\nHere's how you would implement it:\n\n\n```\nimport wandb\n\n# Assuming your initial run is complete:\noriginal_run_id = \"your_initial_run_id\"\n\n# Create your first fine-tuning run branching from epoch 100\nfine_tune_run1 = wandb.init(\n    project=\"your_project_name\",\n    fork_from=f\"{original_run_id}?_step=100\",  # Specify step/epoch to fork from\n    # Add your new hyperparameters for this specific fine-tuning run\n    config={\"learning_rate\": 0.001, \"batch_size\": 32}\n)\n\n# Continue training with new hyperparameters\n# ...\n\n# Similarly for your other fine-tuning runs with different configs\n```\n\n\n**The forking feature is currently in private preview at the moment.**\n\n**Alternative Approach**\nIf you need an immediate solution while waiting for fork access, you could:\n1. Save model checkpoints at epoch 100 of your initial run\n2. Start completely new runs that load this checkpoint\n3. Use W&B's Groups feature to organize these related runs\n\nLet me know if you have any questions or if you'd like more details on either approach!\n\nBest,\nJason"
      },
      {
        "user": "echen1214",
        "body": "Hi Jason, thanks for the tip. I've implemented the alternative approach and will take keep a lookout on the forking feature."
      }
    ]
  },
  {
    "issue_number": 3507,
    "title": "[Q] Possibility to archive projects or dumb data",
    "author": "phrasenmaeher",
    "state": "closed",
    "created_at": "2022-04-11T13:10:11Z",
    "updated_at": "2025-05-20T21:25:42Z",
    "labels": [],
    "body": "For a project, I have used the Weights & Biases logging library. Over time, I have logged a couple thousand experiments. Because this clogs my W&B disk space, I wanted to know if there's a way to either a) archive the data, or b) download a dump that can (somehow) be viewed offline.\r\n\r\nOf the second solution, I am not sure how to achieve this, maybe some kind of lean W&B only for viewing dumps? With that in mind, I am generally looking for a solution that lets me access and also nicely view the data at a later point (around two years).\r\n\r\nDownloading the project via wget or curl does not seem to be the right solution; the dynamic dashboards are what makes `wandb` so unique and outstanding.",
    "comments": [
      {
        "user": "armanhar",
        "body": "Hey @phrasenmaeher, at the moment archiving is not possible. We have an internal ticket for your request. I'll bump up its priority and let you know when there are updates."
      },
      {
        "user": "phrasenmaeher",
        "body": "@armanhar Thanks for our answer. I'll close the issue then."
      },
      {
        "user": "ashok-arora",
        "body": "Hey @armanhar, is there any update on this? I wish to download and share the dump so that others can use it too. "
      }
    ]
  },
  {
    "issue_number": 9349,
    "title": "[Bug]: Cannot run wandb with ray on a drive that is not C: on Windows 11",
    "author": "harryseely",
    "state": "closed",
    "created_at": "2025-01-28T18:32:06Z",
    "updated_at": "2025-05-20T21:01:42Z",
    "labels": [
      "ty:bug",
      "a:sdk",
      "c:sdk:settings"
    ],
    "body": "### Describe the bug\n\nI am running into an issue previously described [https://github.com/wandb/wandb/issues/1991](https://github.com/wandb/wandb/issues/1991), but that I do not have permission to re-open. This prevents me from using wandb 😞 \n\n### Setup\n\n**Windows 11 (OS Build = 22631.4751)**\n\n**Virtual Environment created using [uv package manager](https://docs.astral.sh/uv/)**\n\n**Python = 3.10**\n\n**CUDA 12.4**\n\n**Packages:**\n\"torch==2.5.1\",\n\"lightning==2.5.0\",\n\"wandb==0.19.4\",\n\"ray[data,train,tune]==2.41\"\n\n### Reproducible example\n\n```\nimport pytorch_lightning as pl\nimport torch\nimport os\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split, TensorDataset\nfrom pytorch_lightning.loggers import WandbLogger, CSVLogger\nfrom ray import train, tune\n\nUSE_WANDB = True\n\n#Turn off ray logging\nos.environ[\"TUNE_DISABLE_AUTO_CALLBACK_LOGGERS\"] = \"1\"\n\ndef create_dataset():\n    x = torch.randn(100, 1)\n    y = 3 * x + torch.randn(100, 1)\n    dataset = TensorDataset(x, y)\n    train_dataset, val_dataset = random_split(dataset, [80, 20])\n    train_loader = DataLoader(train_dataset, batch_size=16)\n    val_loader = DataLoader(val_dataset, batch_size=16)\n\n    return train_loader, val_loader\n\n\nclass SimpleModel(pl.LightningModule):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.layer = nn.Linear(1, 1)\n\n    def forward(self, x):\n        return self.layer(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = nn.functional.mse_loss(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.parameters(), lr=0.01)\n\n\ndef train_model(cfg):\n    model = SimpleModel()\n\n    if USE_WANDB:\n        logger = WandbLogger(project='misc')\n    else:\n        logger = CSVLogger('misc')\n\n    trainer = pl.Trainer(max_epochs=cfg['n_epochs'], logger=logger)\n\n    train_loader, val_loader = create_dataset()\n\n    trainer.fit(model, train_loader, val_loader)\n\n\nresources_per_trial = {\"cpu\": 1, \"gpu\": 1}\n\nsearch_space = {'n_epochs': tune.choice([1, 2])}\n\ntuner = tune.Tuner(\n    tune.with_resources(train_model, resources=resources_per_trial),\n    tune_config=tune.TuneConfig(num_samples=3),\n    param_space=search_space,\n)\n\ntuner.fit()\n```\n\n\n### Traceback\n\n```\nTrial train_model_cd6d2_00002 errored after 0 iterations at 2025-01-28 10:15:24. Total running time: 17s\nError file: C:/Users/hseely/AppData/Local/Temp/ray/session_2025-01-28_10-14-57_757185_38132/artifacts/2025-01-28_10-15-06/train_model_2025-01-28_10-14-54/driver_artifacts/train_model_cd6d2_00002_2_n_epochs=2_2025-01-28_10-15-06/error.txt\n2025-01-28 10:15:24,453 ERROR tune_controller.py:1331 -- Trial task failed for trial train_model_cd6d2_00000\nTraceback (most recent call last):\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n    result = ray.get(future)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 21, in auto_init_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n    return func(*args, **kwargs)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 2772, in get\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 919, in get_objects\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(ValueError): ray::ImplicitFunc.train() (pid=42668, ip=127.0.0.1, actor_id=9b402c5556939abc2982653201000000, repr=train_model)\n  File \"python\\ray\\_raylet.pyx\", line 1883, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 1824, in ray._raylet.execute_task.function_executor\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 696, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 463, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 331, in train\n    raise skipped from exception_cause(skipped)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\ray\\air\\_internal\\util.py\", line 107, in run\n    self._ret = self._target(*self._args, **self._kwargs)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\ray\\tune\\trainable\\function_trainable.py\", line 44, in <lambda>\n    training_func=lambda: self._trainable_func(self.config),\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 463, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\ray\\tune\\trainable\\function_trainable.py\", line 249, in _trainable_func\n    output = fn()\n  File \"D:\\Sync\\RQ3\\Analysis\\wandb_bug_reprex.py\", line 52, in train_model\n    trainer.fit(model, train_loader, val_loader)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 539, in fit\n    call._call_and_handle_interrupt(\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py\", line 47, in _call_and_handle_interrupt\n    return trainer_fn(*args, **kwargs)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 575, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 944, in _run\n    call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py\", line 96, in _call_setup_hook\n    if hasattr(logger, \"experiment\"):\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\lightning_fabric\\loggers\\logger.py\", line 118, in experiment\n    return fn(self)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py\", line 407, in experiment\n    self._experiment = wandb.init(**self._wandb_init)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 1458, in init\n    wandb._sentry.reraise(e)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\wandb\\analytics\\sentry.py\", line 156, in reraise\n    raise exc.with_traceback(sys.exc_info()[2])\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 1402, in init\n    wl = wandb.setup()\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py\", line 383, in setup\n    return _setup(settings=settings)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py\", line 323, in _setup\n    _singleton = _WandbSetup(settings=settings, pid=pid)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py\", line 100, in __init__\n    self._settings = self._settings_setup(settings)\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py\", line 134, in _settings_setup\n    s.update_from_system_environment()\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\wandb\\sdk\\wandb_settings.py\", line 1077, in update_from_system_environment\n    self.program_relpath = self.program_relpath or self._get_program_relpath(\n  File \"D:\\Sync\\RQ3\\analysis\\.venv\\lib\\site-packages\\wandb\\sdk\\wandb_settings.py\", line 1209, in _get_program_relpath\n    relative_path = os.path.relpath(full_path_to_program, start=root)\n  File \"C:\\Users\\hseely\\AppData\\Roaming\\uv\\python\\cpython-3.10.4-windows-x86_64-none\\lib\\ntpath.py\", line 718, in relpath\n    raise ValueError(\"path is on mount %r, start on mount %r\" % (\nValueError: path is on mount 'D:', start on mount 'C:'\n```\n\n\n",
    "comments": [
      {
        "user": "fmamberti-wandb",
        "body": "Hi @harryseely , thank you for reporting this issue.\n\nI'd like to gather some further information to reproduce this example on our end:\n\n- Is your virtual environment only installed on D, or is the script being run as well on the same volume?\n- If you set USE_WANDB=False, does the script run as expected?\n- Is a wandb/run-<date>-<id> folder created in the current working directory when starting the script? If so, could you share the logs from the logs folder?\n\nThese will help reproduce the issue on our end"
      },
      {
        "user": "harryseely",
        "body": "Hi @fmamberti-wandb,\n\n1. The virtual environment is installed on the D: drive. The script is also being run from the D: drive directory containing the venv.\n2. If I set USE_WANDB=False, the script runs successfully and the Pytorch Lightning CSV Logger is used\n3. A wandb/run-- folder is not created in the directory after running the script.\n\n**ADDITIONAL CONTEXT**\n\nI expanded my example script to include situation where ray is not used. In the case where `USE_WANDB= True` and `USE_RAY = False`, the script runs without error, and a wandb dir is created. So this issue only occurs when wandb and ray are used together."
      },
      {
        "user": "fmamberti-wandb",
        "body": "Hi @harryseely , thank you for sharing the additional info. We now raised this as a bug with our engineering team and will keep you posted "
      }
    ]
  },
  {
    "issue_number": 3042,
    "title": "[Feature] enable change of x-axis of the slider in images",
    "author": "MoayedHajiAli",
    "state": "open",
    "created_at": "2021-12-14T18:03:57Z",
    "updated_at": "2025-05-20T11:29:27Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "## **Is your feature request related to a problem? Please describe.**\r\nWhen using WandB with pytorch lightning, all logged metrics have an option to change the x axis to step, epoch ... etc. However, The logged images has by default `step` as the x-axis of the slider in the UI. This gives unnatural control over the slider of the images in the UI, in case images are not logged at every step. When specifying a particular invalid step in the slider ( a step where no image was logged), the shown image does not change and stays at the last step that was shown in the UI. \r\n\r\nToy Example: \r\nIn the following code, I am only logging images at every epoch, where each epoch consists of 2 steps. That means I am logging at even steps. \r\n\r\n![image](https://user-images.githubusercontent.com/52598644/146054446-9f13bf35-3479-479c-95fe-690fb735cda1.png)\r\n**The logged image on the last step (18)**\r\n\r\n![image](https://user-images.githubusercontent.com/52598644/146053492-282e984d-792e-4a85-a09d-934be9308f47.png)\r\n**When trying to see the logged image at step 5, where there is no logged image. the shown image does not change and the UI keeps showing the image at step 18**\r\n\r\n## **Describe the solution you'd like**\r\n**Solution 1 (preferred):** gives the ability to change the x-axis of the slider in the image, so that images can be aligned with epochs, in case the value of `epoch` is given.\r\n\r\n**Solution 2:** When the user puts an invalid step size (i.e a step where no image is logged) in the slider, the slider should go to the nearest valid step (i.e nearest step where there is a logged image) rather than staying at the current step. \r\n",
    "comments": [
      {
        "user": "armanhar",
        "body": "Hey there, thanks for writing in! We already have a ticket filed for your request. I'll bump up its priority and let you know once  the option is available"
      },
      {
        "user": "awaelchli",
        "body": "> Solution 1 (preferred): gives the ability to change the x-axis of the slider in the image, so that images can be aligned with epochs, in case the value of epoch is given.\r\n\r\nHey hey! I have a suggestion here. You can log the epoch (or any increasing value) as a metric. Since it will be monotonically increasing, in your plot settings on wandb dashboard you will be able to select this metric as the x-axis. This should remap the slider on your images and plots. Plus, Lightning logs the epoch by default, so you should be able to go the plot settings and select that directly. \r\n\r\ncheers and happy logging \r\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open 60 days with no activity."
      }
    ]
  },
  {
    "issue_number": 1879,
    "title": "[Q] Mechanism for rerunning failed runs in a sweep?",
    "author": "kaiwenw",
    "state": "open",
    "created_at": "2021-02-22T22:46:50Z",
    "updated_at": "2025-05-19T21:59:23Z",
    "labels": [
      "ty:feature",
      "c:sweeps"
    ],
    "body": "Is it possible for wandb agent's to rerun failed runs in a sweep? Currently what I do is I just create a new sweep, but then I would have to rerun the successful runs as well if I want to compare.",
    "comments": [
      {
        "user": "ariG23498",
        "body": "Hey @kaiwenw \r\nWe do not support this for all the sweep types, but for the Grid search, a broken run when deleted will be reattempted. Here is the doc link to help you with this: https://docs.wandb.ai/sweeps/faq#rerun-grid-search\r\n\r\nYou can also go through the detailed walk-through as mentioned in this [ticket](https://github.com/wandb/client/issues/1787)."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open 60 days with no activity."
      },
      {
        "user": "bolak92",
        "body": "commenting to show interest in this feature"
      }
    ]
  },
  {
    "issue_number": 6287,
    "title": "[CLI]: wandb.Image gives blanks for large or small values",
    "author": "mcgibbon",
    "state": "closed",
    "created_at": "2023-09-13T16:15:41Z",
    "updated_at": "2025-05-19T20:56:59Z",
    "labels": [
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n<!--- Description of the issue below  -->\r\nAccording to the [wandb.Image documentation](https://docs.wandb.ai/ref/python/data-types/image), the type accepts \"numpy array of image data\", where it \"attempts to infer the data format and converts it\". This works for float-valued raw data, but not if the values are very large or very small in scale. [The code doing the conversion](https://github.com/wandb/wandb/blob/main/wandb/sdk/data_types/image.py#L269) has different pathways for numpy arrays and tensors. Torch tensors work properly for large values, but not for small values.\r\n\r\nIf an array of values of scale 1e-5 shows as white-to-black noise, it doesn't make much sense that an array of 1e-8 values would show as full-black.\r\n\r\n<!--- A minimal code snippet between the quotes below  -->\r\n```python\r\n>>> import numpy as np\r\n>>> import wandb\r\n>>> wandb.Image(np.random.uniform(size=[100, 200]) * 1e-8).image.save(\"preview_e-8.png\")\r\n>>> wandb.Image(np.random.uniform(size=[100, 200])).image.save(\"preview_e0.png\")\r\n>>> wandb.Image(np.random.uniform(size=[100, 200]) * 1e8).image.save(\"preview_e8.png\")\r\n>>> wandb.Image(torch.rand(size=[100, 200]) * 1e-8).image.save(\"torch_preview_e-8.png\")\r\n>>> wandb.Image(torch.rand(size=[100, 200])* 1e-5).image.save(\"preview_e-5.png\")\r\n>>> wandb.Image(torch.rand(size=[100, 200]) * 1e8).image.save(\"torch_preview_e8.png\")\r\n```\r\n\r\n<!--- A full traceback of the exception in the quotes below -->\r\nNo exception, but the images look very different even though they should look qualitatively similar. `preview_e-8.png` is a fully black image, `preview_e0.png` is as expected (noise), and `preview_e8.png` is a fully white image. `preview_e-5.png` is the first scale where we get noise, at e-6 we don't get black but the white values become dark gray. The conversion works for large torch tensors, and `torch_preview_e8.png` shows noise, but `torch_preview_e-8.png` is a black image.\r\n\n\n### Additional Files\n\n_No response_\n\n### Environment\n\nWandB version: 0.15.0\r\n\r\nOS: macOS 13.0 Ventura\r\n\r\nPython version: 3.10.10\r\n\r\nVersions of relevant libraries:\r\n\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "thanos-wandb",
        "body": "Hi @mcgibbon thank you for raising this issue. Please note that the pixels when using `numpy` should be in range [0,255], while when using `torch.Tensor` the images are normalized. Would it be please possible that you convert your tensors to PILimage instead ([docs example](https://docs.wandb.ai/ref/python/data-types/image#create-a-wandbimage-from-a-pilimage))?"
      },
      {
        "user": "mcgibbon",
        "body": "Hi @thanos-wandb , I have already worked around this bug on my end. However there are clearly undocumented behaviors here. For example, there's no documented reason the behavior should change so drastically for uniform random values in the range [0, 1e-5] versus uniform random values in [0, 1e-8]. This seems like a bug in `wandb.Image`.\r\n\r\nMy team went without several of our important plots in our runs for weeks until I had time to diagnose this bug and write a work-around. This will affect others."
      },
      {
        "user": "thanos-wandb",
        "body": "Thanks for your feedback, @mcgibbon and sorry for any trouble this issue has caused you. There's a Note below `wandb.Image` arguments in [our Docs here](https://docs.wandb.ai/ref/python/data-types/image). However, I will raise this with our Docs team to make sure we highlight this. May I please ask how you've worked around the issue on your end? "
      }
    ]
  },
  {
    "issue_number": 9461,
    "title": "[Bug]: wandb.Image supports torch.tensor, so should mask",
    "author": "MaKaNu",
    "state": "closed",
    "created_at": "2025-02-13T16:28:43Z",
    "updated_at": "2025-05-19T20:35:44Z",
    "labels": [
      "ty:feature",
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\npython version: 3.12.5\nwandb version: 0.19.6\nos version: ubuntu 20.04\n\nif `image` and `mask` are `torch.tensor` the following works to create an `wandb.Image`:\n\n```python\njust_image = wandb.Image(image, caption=f\"{dataset} image {idx}\")\n```\n\nwhile this not:\n\n```python\nimage_with_pred = wandb.Image(\n            image,\n            masks={\"predictions\": {\"mask_data\": pred_mask, \"class_labels\": class_labels}},\n            caption=f\"{dataset} image with predictions {idx}\",\n        )\n```\ncausing following issue:\n\n```bash\nFile \"/projects/test/.venv/lib/python3.12/site-packages/wandb/sdk/data_types/helper_types/image_mask.py\", line 161, in __init__\n    image = pil_image.fromarray(val[\"mask_data\"].astype(np.int8), mode=\"L\")\n                                ^^^^^^^^^^^^^^^^^^^^^^^\n AttributeError: 'Tensor' object has no attribute 'astype'. Did you mean: 'dtype'?\n```\n\nWhich is reasonable, since the input is a `torch.tensor` not a `numpy.array`.\n\n**What I expect**\n`wandb.Image`'s attribute `mask`should also handle `torch.tensor`, if the image accepts `torch.tensor`. \n",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Thank you for reporting this!\n\nI have gone ahead and brought this up to our engineering team. "
      }
    ]
  },
  {
    "issue_number": 5269,
    "title": "[Feature]: In Tables UI, add option to make column names wrap around",
    "author": "nikita6187",
    "state": "closed",
    "created_at": "2023-03-31T07:11:49Z",
    "updated_at": "2025-05-19T08:39:31Z",
    "labels": [
      "ty:feature"
    ],
    "body": "### Description\n\nHello, the column names in the Tables UI are automatically cut off. When manually resizing them (which also takes a lot of time), a lot of whitespace is needed to see the entire column, if the name is a bit longer.\r\n\r\n![image](https://user-images.githubusercontent.com/15629332/229049140-118fd6a1-0d60-4b3c-9d4e-8efa2fc40f9a.png)\r\n\r\n\n\n### Suggested Solution\n\nIt would be amazing if we could have the option to wrap around the column names (or alternatively automatically adjust the width to the name).\r\n\r\nThank you!\n\n### Alternatives\n\n_No response_\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hi @nikita6187, thanks for writing in and suggesting this! I'll create a new feature request and share it with our Product Team!"
      }
    ]
  },
  {
    "issue_number": 5709,
    "title": "[Q] Calculate metrics from charts",
    "author": "pablo2909",
    "state": "closed",
    "created_at": "2023-06-13T08:41:50Z",
    "updated_at": "2025-05-19T08:38:30Z",
    "labels": [],
    "body": "Hi,\r\n\r\nI have five training loss curves, that I grouped into a mean curve and its standard deviation. I would like to compute the average of the standard deviation across the training steps. Can this be done ? \r\n\r\nThank you ",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hi @pablo2909, thanks for your question! You can add expressions to your plot as explained [here](https://docs.wandb.ai/guides/app/features/panels/line-plot#settings), would this be useful? If not, could you please share a link to your Workspace so I can have a look at this? "
      },
      {
        "user": "pablo2909",
        "body": "Hi @luisbergua , thank you for getting back to me. I can invite you to look at one of the plots. Would that allow you to help me ? If yes, which address should I use ?\r\n\r\n"
      },
      {
        "user": "luisbergua",
        "body": "That would be useful @pablo2909! Would you mind sharing the page url and the name of the plot so I can see it? "
      }
    ]
  },
  {
    "issue_number": 9546,
    "title": "[Bug-App]: Scalars synced through `sync_tensorboard=True` cannot be used as alternate x-axes",
    "author": "ringohoffman",
    "state": "open",
    "created_at": "2025-03-02T20:44:56Z",
    "updated_at": "2025-05-18T07:57:47Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\nhttps://github.com/user-attachments/assets/7fbaac81-c527-489e-8eb3-355b54860223\n\nIt looks like the UI is trying to join the 2 graphs on `Step`, not tensorboard's `global_step`. It seems like one `SummaryWritter` step = one wandb `Step`. So `batch_idx` only has even `Step` values (since it is logged first), and `percent_done` only has odd `Step` values, so naturally they cannot be joined on `Step`.\n\nCan we specifically support joining on tensorboard's `global_step` if `sync_tensorboard=True` and joining on `Step` doesn't work?\n\n```python\n\nimport torch\nimport torch.utils.tensorboard\nimport wandb\n\nlog_dir = \"saved\"\n\nwandb.init(\n    project=\"my_project\",\n    dir=log_dir,\n    job_type=\"dryrun\",\n    sync_tensorboard=True,\n)\n\ntb_writer = torch.utils.tensorboard.SummaryWriter(log_dir=log_dir)\n\n\nfor batch_idx, percent_done in enumerate(torch.linspace(0, 1, 20)):\n    tb_writer.add_scalar(\"batch_idx\", batch_idx, global_step=batch_idx)\n    tb_writer.add_scalar(\"percent_done\", percent_done.item(), global_step=batch_idx)\n\nwandb.finish()\n\n```\n\n",
    "comments": [
      {
        "user": "fmamberti-wandb",
        "body": "Hi @ringohoffman , thank you for reaching out with your request.\n\nYou can use the `define_metric` method to set the `global_step` as the `step_metric` so that is used on the x-axis.\n\nIn your case for example you could add:\n\n```\nwandb.define_metric(\"batch_idx\", step_metric=\"global_step\")\nwandb.define_metric(\"percent_done\", step_metric=\"global_step\")\n```\n\nbefore starting to log your data and the metrics will be plotted against `global_step` by default:\n<img width=\"1244\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fecfe7e8-b574-4800-bc88-c92ce25df9b7\" />\n\nPlease let us know if you have any questions on the above.\n"
      },
      {
        "user": "ringohoffman",
        "body": "@fmamberti-wandb thanks, I didn't know you could use `define_metrics` like that. But I am dynamically creating my metrics, and all my logged metrics start with a different prefix, and I don't want to have to call this for every single metric I create. Can you also check out this other issue I created to support glob prefixes better support this?\n\n* https://github.com/wandb/wandb/issues/9549\n"
      },
      {
        "user": "fmamberti-wandb",
        "body": "Hi @ringohoffman , thanks for sharing this - I've raised a feature request for allowing to support prefix for define metrics. and will keep you posted."
      }
    ]
  },
  {
    "issue_number": 9840,
    "title": "[Q]: Using python logging module for console output",
    "author": "relativityhd",
    "state": "closed",
    "created_at": "2025-05-10T15:06:22Z",
    "updated_at": "2025-05-17T11:09:29Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\nI want to move the wandb prints to my logger (RichHandler and FileHandler) but am unable to do so via \n\n```py\nlogging.getLogger(\"wandb\").addHandler(...)\nlogging.getLogger(\"wandb\").setLevel(logging.INFO)\n```\n\nThis instead adds a lot of unwanted information to my logs (seem to me that this is debug information), while still normally printing the \"nice\" stuff to std err.\n\n![Image](https://github.com/user-attachments/assets/6a513919-0060-4f8e-9d9a-d4b2831a6789)\n\nIn above example: I don't want the `restore`, `restore done` etc. logs in my log files, but I do want the Run history ands similar `wandb: ...` prints in my logs.\n\nI tried to dig a little deeper into the workings of the SDK, but I am quite confused about how to setup the term[log, warn, err] stuff correctly.\nI already found a PR #6982 where ther termlog etc. is replaced with the python logging equivalents, but it was closed.\n\nHow can I achieve this behavior? Is it even possible?",
    "comments": [
      {
        "user": "fmamberti-wandb",
        "body": "Hi @relativityhd , W&B uses its own functions (termlog, termwarn,termerror) to output information on the terminal so you would have to leverage those to write to your own log files.\n\nSee the example snippet below on how to achieve this:\n\n```python\nimport wandb\nimport logging\nfrom rich.logging import RichHandler\n\n# Set up your custom logger\ncustom_logger = logging.getLogger(\"wandb_terminal\")\ncustom_logger.setLevel(logging.INFO)\ncustom_logger.propagate = False\n\n# Add your RichHandler\nrich_handler = RichHandler()\ncustom_logger.addHandler(rich_handler)\n\n# Add your FileHandler\nfile_handler = logging.FileHandler(\"wandb_redirected_terminal.log\")\nformatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nfile_handler.setFormatter(formatter)\ncustom_logger.addHandler(file_handler)\n\n# Override wandb's terminal output functions\noriginal_termlog = wandb.termlog\noriginal_termwarn = wandb.termwarn\noriginal_termerror = wandb.termerror\n\ndef custom_termlog(string=\"\", newline=True, repeat=True):\n    # Still output to terminal via the original function\n    original_termlog(string)\n    # Log to your custom logger\n    if string:\n        custom_logger.info(string)\n        \ndef custom_termwarn(string=\"\", newline=True, repeat=True):\n    original_termwarn(string)\n    if string:\n        custom_logger.warning(string)\n        \ndef custom_termerror(string=\"\", newline=True, repeat=True):\n    original_termerror(string)\n    if string:\n        custom_logger.error(string)\n\n# Replace wandb's terminal output functions with our custom versions\nwandb.termlog = custom_termlog\nwandb.termwarn = custom_termwarn\nwandb.termerror = custom_termerror\n# Now initialize wandb\nwandb.init(project=\"misc-test\")\n\nfor i in range(10):\n    wandb.log({\"step\": i}\n              )\nwandb.finish()\n```\n\nPlease let me know if you have any further questions. "
      },
      {
        "user": "relativityhd",
        "body": "Thank you for your answer! This helps a lot in understanding how to use your sdk.\n"
      }
    ]
  },
  {
    "issue_number": 9866,
    "title": "Does `wandb.watch()` not log gradients in FSDP?",
    "author": "alex-infinite",
    "state": "closed",
    "created_at": "2025-05-16T00:44:03Z",
    "updated_at": "2025-05-17T00:55:03Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "I'm using wandb with FSDP in torch. My model is sharded with FSDP, but my conditioners are not sharded (they're DDP-ed). When I call `wandb.watch()` I only get the gradients for the DDP-ed module, not the FSDP module.\n\nDoes wandb support watching gradients in FSDP? I've poked around and haven't found clear information on this.\n\n\n`self.wandb_run.watch((self.fsdp_model, self.conditioner), log=\"all\", log_freq=100)`\n\n\nPython 3.10.15\nwandb, version 0.19.11\nUbuntu 22.04.5",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nHi there,\n\nThank you for reaching out about the gradient tracking issue with FSDP and `wandb.watch()`. I can confirm that what you're experiencing is indeed a known limitation with wandb's current implementation.\n\nYou're correct that wandb.watch() successfully captures parameters for both your FSDP and DDP modules, but only tracks gradients for the DDP modules. This happens because FSDP handles gradients differently than DDP - while DDP keeps full gradient copies on each process, FSDP shards gradients across processes and only materializes them temporarily during the backward pass. Unfortunately, wandb's gradient tracking mechanism isn't fully compatible with this sharded approach yet.\n\nI'd be happy to submit a feature request internally to prioritize FSDP gradient tracking support if that would be helpful. Let me know if you have any other questions!\n\nBest,\nJason\n"
      },
      {
        "user": "alex-infinite",
        "body": "Thanks for the quick response, Jason. Yeah, that's what I figured. And yeah it certainly would be useful!"
      }
    ]
  },
  {
    "issue_number": 9009,
    "title": "[Feature]: Add compression to Object3D (Point Clouds)",
    "author": "mokrueger",
    "state": "open",
    "created_at": "2024-12-04T10:48:40Z",
    "updated_at": "2025-05-16T20:00:21Z",
    "labels": [
      "ty:feature",
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Description\n\nHi there I am currently working on a project that involves lots of point cloud data. I noticed that the current implementation for Point Clouds a rather simple json format to store the data.\n\nThe following is for the case of using the from_numpy function:\n```python\n            # file: object_3d.py\n            tmp_path = os.path.join(MEDIA_TMP.name, runid.generate_id() + \".pts.json\")\n            with codecs.open(tmp_path, \"w\", encoding=\"utf-8\") as fp:\n                json.dump(\n                    data,\n                    fp,\n                    separators=(\",\", \":\"),\n                    sort_keys=True,\n                    indent=4,\n                )\n            self._set_file(tmp_path, is_tmp=True, extension=\".pts.json\")\n```\n\nFor my purposes this yields file sizes >50Mb which really slows down the visualizations as it takes a while to load. This could easily be fixed within python by adding an optional compression argument. However, I am unsure how the front-end would implement this. \n\n### Suggested Solution\n\nFor my example using bz2 compression of the json data would yield a file size of 7.6MB (53.6MB original). \nGzip yields around 10MB.\nFor reference, compressing the raw numpy data (so no json formatting) could achieve the smallest compression 4,1MB\n\n\nAny sort of compression would probably be beneficial since the Json / whatever format should not be required to be human readable.\n\n\nKind regards ;)",
    "comments": [
      {
        "user": "paulosabile-wb",
        "body": "Hey @mokrueger Thanks for sharing this as well. Let me also review this and get back to you.\n\nWhen you mentioned that file sized that are greater than 5 mb slows down the visualizations in the UI, could you please tell us how long did it take for the workspace to render the object that you were trying to log?\n\nIt will also help if you can share more context about the experiment that you are trying to setup here so we can further review and identify the next steps. Thanks!"
      },
      {
        "user": "mokrueger",
        "body": "Hi @paulosabile-wb, the visualization speed in terms of simply being able to show the point cloud is mostly dependent on bandwidth, so mainly just file size, since the visualization after loading is almost instant. The nice thing in wandb is the ability to use steps in conjunction with point cloud logging, so in many cases such as mine, one might like to compare 2 point clouds and how they develop over time. \n\nIn my example I work with experiments in image depth estimation, so for given 512x384 images I have depth values I would like to plot. Given the large file size the browser not only has to load all of the point clouds but also store them in RAM, so if I want to compare the point clouds on different steps, it takes really long.\n\nOne reason is because of the file size of each point cloud and the second reason is the memory cluttering up the browser, which causes a significant slow down of the browser or sometimes even a crash because of memory issues.\n\nI am working with 512x384 images and mostly compare values with their ground truth, so it requires around 390k points per cloud.\n\nKind regards! :)"
      },
      {
        "user": "fmamberti-wandb",
        "body": "Hi @mokrueger - thank you for sharing the details for your use case and a potential solution. I have now raised this as a feature request for our product team and will keep you posted with any updates."
      }
    ]
  },
  {
    "issue_number": 9571,
    "title": "[Feature]: Allow monitoring all GPUs",
    "author": "AndreasMadsen",
    "state": "open",
    "created_at": "2025-03-07T23:28:27Z",
    "updated_at": "2025-05-16T18:02:41Z",
    "labels": [
      "ty:feature",
      "a:sdk",
      "c:sdk:system-metrics"
    ],
    "body": "### Description\n\n<!--- Describe your feature here --->\nCurrent Wandb only monitors the GPUs that have the `wandb.init()` process's pid (or one of its children) associated with that GPU. However, in my case I'm running an inference service on the same node, but it don't share process tree. Therefore wandb doesn't pick up the GPU's.\n\n### Suggested Solution\n\n<!--- Describe your solution here --->\nAdd a `wandb.init()` flag which enables monitoring all GPUs. ",
    "comments": [
      {
        "user": "fmamberti-wandb",
        "body": "Hi @AndreasMadsen , thank you for reaching out with your request. logging GPU Usage for all GPUs is something we are currently looking into. I am checking internally if we do have any experimental feature currently available for this which we can share and will keep you posted."
      },
      {
        "user": "noaleetz",
        "body": "Hi @AndreasMadsen! Nice to meet you, I'm the PM for our SDK team. We're working on a new set of experimental features to allow tracking system metrics across multiple processes. \n\n- Shared Mode: Multiple compute nodes can now simultaneously log metrics to a single run without conflicts - console logs and system metrics are collected from each node you call wandb.init\n- Node/Process Labels (x_label): Each node/process logs data tagged distinctly (e.g., node-0, node-1), preserving individual context and simplifying debugging.\n\nThese are passed in via the run settings. Again, with the caveat that they are in public preview:\n### run settings `x_label`\n- Def: Label to assign to system metrics and console logs collected for the run to group by on the frontend. Can be used to distinguish data from different nodes in a distributed training job.\n- Purpose: When logging to the **same run id**, will assigns a label (e.g., rank-0, rank-1) to uniquely identify logs and system metrics in distributed training setups. This label is used to distinguish logs and metrics (for example - from different ranks), while enabling aggregated visualization of logged metrics under a single run.\n- This is defined as a run setting [here](https://github.com/wandb/wandb/blob/8841ebdb0a47b9e8b7d7f3886fd1b5b52db69ba5/wandb/sdk/wandb_settings.py#L288-L291)\n\n```\n# spinning up a run inside an individual node with rank-{global_rank}\nsettings = wandb.Settings(\n        mode=\"shared\",\n        x_label=f\"rank-{global_rank}\",\n)\n\nrun = wandb.init(\n        id=SHARED_RUN_ID,\n        settings=settings,\n    )\n```\n\n- Outcome - you can log metrics from different processes/writers to a single run, and you'll see that now console logs and system metrics will be grouped by the relevant label\n\nHere are a few other relevant run settings for a multi-process run:\n- [x_primary_node](https://github.com/wandb/wandb/blob/8841ebdb0a47b9e8b7d7f3886fd1b5b52db69ba5/wandb/sdk/wandb_settings.py#L296-L299)\n- [x_update_finish_state](https://github.com/wandb/wandb/blob/8841ebdb0a47b9e8b7d7f3886fd1b5b52db69ba5/wandb/sdk/wandb_settings.py#L344-L346)\n- [x_stats_gpu_device_ids](https://github.com/wandb/wandb/blob/8841ebdb0a47b9e8b7d7f3886fd1b5b52db69ba5/wandb/sdk/wandb_settings.py#L335-L338)\n\nTo see the full workflow - checkout [this](https://wandb.ai/dimaduev/simple-cnn-ddp/reports/Distributed-Training-with-Shared-Mode--VmlldzoxMTI0NTE1NA) demo report. \n\nLet me know if you have any time this week - would love to talk through some of the features above and get your feedback. Since this is in a preview mode we are getting feedback from users on the workflow so that we can improve it. You can email me at noa.schwartz@wandb.com. \n\n"
      },
      {
        "user": "AndreasMadsen",
        "body": "Hi @noaleetz,\n\nThanks for your message. While I can certainly see the relevance of what you are talking about, and it's a feature I have been wanting for a while, it's unrelated to the feature requested mentioned here.\n\nIn this case, I'm asking about monitoring GPUs on the same node but where the GPUs aren't used by wandb's process tree. At https://github.com/wandb/wandb/blob/cfc0ab112bd9f19e26eebd3699166d44cdf5705c/wandb/sdk/internal/system/assets/gpu.py#L28C5-L28C31 the GPU statistics are filtered to only include wandb's process tree. Is there a setting which would skip this filter?\n\n"
      }
    ]
  },
  {
    "issue_number": 3532,
    "title": "[CLI]: wandb.Video takes a very long time",
    "author": "eliahuhorwitz",
    "state": "closed",
    "created_at": "2022-04-19T09:47:13Z",
    "updated_at": "2025-05-15T16:03:57Z",
    "labels": [
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nRunning wandb.Video when providing a numpy array takes a very long time. Using the profiler I found that the issue is the quantization of the frames for the GIF that is created by default. I suggest changing the format to be mp4 by default, this avoids this quantization and runs much faster. Also, it might be wise to change the default FPS to something other than 4. \r\n\r\nIf the above changes are not desired for some reason, I suggest you update the documentation on the website to inform of this behavior and maybe also add an FAQ item for this.\r\n\r\nThanks!\n\n### Additional Files\n\n_No response_\n\n### Environment\n\nWandB version:\r\n0.12.14\r\n\r\nOS:\r\nFedora 8.5 server\r\nPython version:\r\n3.7.13\r\nVersions of relevant libraries:\r\nimageio=2.17.0\r\nimageio-ffmpeg=0.4.7\r\nmoviepy=1.0.3\r\nPillow=9.0.1\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "ramit-wandb",
        "body": "Hi @eliahuhorwitz,\r\n\r\nThank you for the request! I'll pass it along to our engineering team for review. Could you share a link to the page where you see this slowness?\r\n\r\nThanks,\r\nRamit"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "WandB Internal User commented: \nramit-wandb commented: \nHi @eliahuhorwitz,\r\n\r\nThank you for the request! I'll pass it along to our engineering team for review. Could you share a link to the page where you see this slowness?\r\n\r\nThanks,\r\nRamit\n"
      },
      {
        "user": "eliahuhorwitz",
        "body": "> Hi @eliahuhorwitz,\r\n> \r\n> Thank you for the request! I'll pass it along to our engineering team for review. Could you share a link to the page where you see this slowness?\r\n> \r\n> Thanks, Ramit\r\n\r\nHey @ramit-wandb,\r\nThanks for the prompt answer. I am seeing the slowness in the python library, not the website. As for the documentation, I followed the code snippet found here https://docs.wandb.ai/ref/python/log#video-from-numpy and the documentation found here https://docs.wandb.ai/guides/track/log/media#other-media, here https://docs.wandb.ai/guides/track/log/media#how-do-i-log-a-video and here https://docs.wandb.ai/ref/python/data-types/video. Nowhere does it hint to the impact of gifs vs. mp4s on performance. \r\n\r\nCheers,\r\nEliahu\r\n"
      }
    ]
  },
  {
    "issue_number": 6637,
    "title": "[Feature]: Custom class colors for image overlays",
    "author": "AndrewAnnex",
    "state": "open",
    "created_at": "2023-11-27T22:36:37Z",
    "updated_at": "2025-05-14T22:01:09Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "### Description\n\nCurrently, the web UI for w&b does not allow the user to adjust the selected colors for classes. \r\n![image](https://github.com/wandb/wandb/assets/2126916/6039f549-9d87-4cea-bb56-78cd73b20f0b)\n\n### Suggested Solution\n\nI would like to be able to adjust the color for classes by clicking the class in the UI (currently only supports adjusting opactity) and adjusting it using a standard color wheel/hex color input box tool common to web UIs, and then see this change in the image overlay.\n\n### Alternatives\n\n_No response_\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "JoanaMarieL",
        "body": "Hi @AndrewAnnex ,\r\nThis feature is not available yet but I can create a new request for it if you feel this is useful. Could you please give me some details on your use-case and if this is blocking your workflow in any way and so we can try to find a workaround if needed?"
      },
      {
        "user": "sydholl",
        "body": "WandB Internal User commented: \nJoanaMarieL commented: \nHi @AndrewAnnex ,\r\nThis feature is not available yet but I can create a new request for it if you feel this is useful. Could you please give me some details on your use-case and if this is blocking your workflow in any way and so we can try to find a workaround if needed?\n"
      },
      {
        "user": "AndrewAnnex",
        "body": "@JoanaMarieL It's not a major blocker for me at the moment as I ended up just downloading all the media files for my wandb run and then writing some python to do the plotting, but I still think this is a useful idea. The detail of my use case is that I am training a model that performs binary semantic segmentation. I am using the media logging functionality to capture image overlays of true positives, true negatives, false positives, and false negatives as four different classes (https://docs.wandb.ai/guides/track/log/media#image-overlays). For these four classes, it's helpful to decide on the hue as the default first four colors are red blue purple and green, but I wanted to pick different colors with more opposing hues. Currently there is no user control over how the wandb website decides to color the different classes (what colors are shown in the image I include above). I think it would make sense to add a color picker for the classes via a drop down option once you click the class in the website UI. Currently in the website UI, if you click the class you are presented with an opacity slider only.\r\n\r\nAn alternative option would be to allow this to be performed within the python api in some way, but I don't see an option to do this other than storing the actual image overlay as an RGB image which is not as data efficient as storing a single band 8bit image. "
      }
    ]
  },
  {
    "issue_number": 6500,
    "title": "[App]: define_metrics does not work with media type keys",
    "author": "aria1th",
    "state": "open",
    "created_at": "2023-10-26T05:50:07Z",
    "updated_at": "2025-05-14T22:01:00Z",
    "labels": [
      "c:sdk:media",
      "c:sdk:custom-charts",
      "a:app"
    ],
    "body": "### Current Behavior\n\nAssume the following code:\r\n```\r\n        wandb.define_metric(\"custom_step\")\r\n        wandb.define_metric(\"image_0\", step_metric='custom_step')\r\n        wandb.log(\r\n            {\r\n                'custom_step' : steps,\r\n                'image_0': wandb.Image(image),\r\n            }\r\n        )\r\n```\r\n\r\nThen wandb can't show the custom metric vs media type plot, it will show with default step (current wandb step) instead.\n\n### Expected Behavior\n\n```\r\n        wandb.define_metric(\"custom_step\")\r\n        wandb.define_metric(\"metric_0\", step_metric='custom_step')\r\n        wandb.log(\r\n            {\r\n                'custom_step' : steps,\r\n                'metric_0' : 0,\r\n            }\r\n        )\r\n```\r\n\r\n\r\nthis works well.\n\n### Steps To Reproduce\n\nDefine the custom metric with the code above, then try to log the images, to see plotted image against custom metrics.\n\n### Screenshots\n\n_No response_\n\n### Environment\n\nOS: Linux / Windows *all\r\n\r\nBrowsers: Chrome / Microsoft Edge\r\n\r\nVersion: 0.15.12\r\n\n\n### Additional Context\n\nRelated : https://github.com/wandb/wandb/issues/6077\r\n",
    "comments": [
      {
        "user": "JoanaMarieL",
        "body": "Hi @aria1th ,\r\n\r\nThank you for writing in, we will review the codes you provided and will get back to you."
      },
      {
        "user": "JoanaMarieL",
        "body": "Hello @aria1th ,\r\n\r\nYes, it's true. Weights & Biases uses the internal step counter as the default x-axis for your plots. If you want to use a custom metric as the x-axis, you can use the wandb.define_metric function to specify the metric you want to use as the x-axis. However, this only applies to scalar metrics and not media types.\r\n\r\nWe can also create a feature request for this case if you would like."
      },
      {
        "user": "aria1th",
        "body": "@JoanaMarieL Thank you for the reply!\n Sure, the feature would be great. We can think about some asynchronous inference while training is running background. \n\n(Also, inference can go slow, so it may end after actual training is done... but currently if we end the training, there is no way to attach media results. Are there way to bypass this?)"
      }
    ]
  },
  {
    "issue_number": 6455,
    "title": "[Feature]: Installing tensorboard plugin profile for tensorflow",
    "author": "Graham-Broughton",
    "state": "open",
    "created_at": "2023-10-14T10:13:05Z",
    "updated_at": "2025-05-14T22:00:39Z",
    "labels": [
      "ty:feature",
      "a:sdk",
      "c:sdk:system-metrics"
    ],
    "body": "### Description\n\nWhen attempting to visualize the profile data on tensorboard on the wandb website, it requires the installation of \"tensorboard-plugin-profile\". It would be nice to visualize the profiling results particularly when you are trying to optimize your accelerator especially if using sweeps. \n\n### Suggested Solution\n\nOn the server that is hosting tensorboard: pip install -U tensorboard-plugin-profile\n\n### Alternatives\n\nNot aware of any\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "MBakirWB",
        "body": "Hi @Graham-Broughton , thank you for writing in. Currently the plugin is not supported and it's up for consideration by our team. There is an active feature request for it and I've added your details to the ticket and we will keep you update once there's been movement. Thanks"
      }
    ]
  },
  {
    "issue_number": 5632,
    "title": "[CLI]: wandb resume can not log if `step` is specified",
    "author": "qsh-zh",
    "state": "closed",
    "created_at": "2023-05-30T06:27:38Z",
    "updated_at": "2025-05-14T21:58:50Z",
    "labels": [
      "a:sdk"
    ],
    "body": "### Describe the bug\r\n\r\n<!--- Description of the issue below  -->\r\n\r\n<!--- A minimal code snippet between the quotes below  -->\r\n```python\r\nimport wandb\r\nrun = wandb.init(project='resume_runs')\r\nid = run.id\r\nfor i in range(5):\r\n    run.log({'metric_A':i}, step=i)\r\nrun.finish()\r\n\r\n# ! the following `metric_b` will not be logged\r\nrun_1 = wandb.init(project='resume_runs', id=id, resume='must')\r\nfor i in range(5):\r\n    run_1.log({'metric_b':5+i}, step=i)\r\nrun_1.finish()\r\n```\r\n\r\n<!--- A full traceback of the exception in the quotes below -->\r\n```shell\r\n\r\n```\r\n\r\n\r\n### Additional Files\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nWandB version: 0.14.1 and 0.15.3\r\n\r\nOS: ubuntu 20.04\r\n\r\nPython version: python3.8 / python3.9\r\n\r\nVersions of relevant libraries: No\r\n\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "qsh-zh",
        "body": "The pattern is quite common in training and evaluating ML-models, where 'metric_A' may be for training loss and 'metric_b' mey be some evaluation metric after training, and the 'step' is the iteration number for the trained model."
      },
      {
        "user": "thanos-wandb",
        "body": "Hi @qsh-zh thanks for reporting this issue, and the additional context. What's currently allowed is to log the resumed run metrics in a monotonically increasing time step, such as:\r\n\r\n```\r\nrun_1 = wandb.init(project='resume_runs', id=id, resume='must')\r\nfor i in range(5):\r\n    run_1.log({'metric_b':5+i}, step=5+i)\r\nrun_1.finish()\r\n```\r\n\r\nor, to only update summary metrics (instead of history), as follows:\r\n\r\n```\r\nrun_1 = wandb.init(project='resume_runs', id=id, resume='must')\r\nrun_1.log({'metric_b': value})\r\nrun_1.finish()\r\n```\r\n\r\nMay I please ask if it would help here to log the training and evaluations metrics in two different runs? \r\n"
      },
      {
        "user": "qsh-zh",
        "body": "@thanos-wandb , I want to make sure the evaluation metric is aligned with the training step~(saved checkpoints). It does not make sense that I upload evaluation results with step $5+i$  while the training iteration of the checkpoint is actually $i$.\r\n\r\nNotice we do not overwrite an existing metric, e.g. 'metric_A', but add a metric that does not exist before."
      }
    ]
  },
  {
    "issue_number": 5451,
    "title": "[Q] Can team member adds notes to runs by others?",
    "author": "xibeisiber",
    "state": "closed",
    "created_at": "2023-04-29T15:14:00Z",
    "updated_at": "2025-05-14T21:58:46Z",
    "labels": [],
    "body": "Hi,\r\nIn my project, when other team member tries to add notes to my runs, the page says:\r\n_No Access\r\nYou do not have permission to view this page_\r\n\r\nCan team member adds notes to runs by others?\r\n\r\nThanks!",
    "comments": [
      {
        "user": "thanos-wandb",
        "body": "Hi @xibeisiber thanks for reporting this issue. Is this a project logged in a team or your personal entity? Could you please check in your project's Overview page what are the Privacy settings? "
      },
      {
        "user": "xibeisiber",
        "body": "> Hi @xibeisiber thanks for reporting this issue. Is this a project logged in a team or your personal entity? Could you please check in your project's Overview page what are the Privacy settings?\r\n\r\nThe project is logged in a team entity. \r\n\r\nPrivacy setting in project's Overview page. \r\n![image](https://user-images.githubusercontent.com/5679317/235334386-8eeb0731-812e-4e1b-a560-1c7829245e05.png)\r\n\r\nPrivacy in team settings.\r\n![image](https://user-images.githubusercontent.com/5679317/235334335-b51516bb-7cbe-42d2-bbad-87db7369e28d.png)\r\n"
      },
      {
        "user": "thanos-wandb",
        "body": "Hi @xibeisiber sorry for late response on this issue, this shouldn't happen based on your team settings. Could you please provide me with your username to investigate this further? "
      }
    ]
  },
  {
    "issue_number": 5415,
    "title": "[Feature]: Delete Button for Panel Grids",
    "author": "MartinMichajlow",
    "state": "open",
    "created_at": "2023-04-24T14:52:24Z",
    "updated_at": "2025-05-14T21:58:41Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "### Description\n\nThere is a delete button for tabs within panel grids, but not one for the whole panel grid, once only one tab is left, which would be intuitive in my opinion\r\n![Screenshot 2023-04-24 at 16 47 58](https://user-images.githubusercontent.com/83063125/234033753-c02215df-50d8-4ab4-aea3-c1f2eb03d3dd.png)\r\n![Screenshot 2023-04-24 at 16 42 37](https://user-images.githubusercontent.com/83063125/234033769-da7be69c-6bb0-4c1c-ace0-f77309b77682.png)\r\n\n\n### Suggested Solution\n\nAdd a delete button in the same drop down menu.\n\n### Alternatives\n\n_No response_\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hi @MarMichaj! I can definitely submit this feature request for you. \r\n\r\nJust as a heads up, we do have a grid deletion button on the left of the whole grid.<img width=\"1387\" alt=\"image\" src=\"https://user-images.githubusercontent.com/115654725/234311985-85c8a091-dacc-46ad-a003-06da16ccaa03.png\">\r\n\r\nBut it does not seem to be directly what you want though. "
      },
      {
        "user": "ArtsiomWB",
        "body": "Your feature request has been submitted!"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "WandB Internal User commented: \nArtsiomWB commented: \nYour feature request has been submitted!\n"
      }
    ]
  },
  {
    "issue_number": 5374,
    "title": "[Feature]: Manually override Git-related values for a run",
    "author": "nicjac",
    "state": "closed",
    "created_at": "2023-04-19T09:48:47Z",
    "updated_at": "2025-05-14T21:58:15Z",
    "labels": [
      "ty:feature"
    ],
    "body": "### Description\n\nWhen calling `init()`, the client automatically gathers information regarding the Git environment, if one exists. This works well in many cases. However, there is currently no way (as far as I know) to manually override Git-related values saved to the experiment. \r\n\r\nFor example, if one is to use SageMaker training jobs, there is no Git environment where `init()` is called, even if the original code was indeed versioned. \n\n### Suggested Solution\n\nIdeally, one would be able to provide all the information required to log Git-related data to a run as parameters to `init()` or as environment variables. This way, Git information can be passed onto the remote job execution agent and properly logged in wandb.\n\n### Alternatives\n\n_No response_\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hi @nicjac, thanks for your suggestion! I'll create a feature request for this and share it with our Product Team! "
      }
    ]
  },
  {
    "issue_number": 5349,
    "title": "[App]: Strange `__MIN` & `__MAX` in csv exported from the web",
    "author": "fyqqyf",
    "state": "closed",
    "created_at": "2023-04-17T07:47:49Z",
    "updated_at": "2025-05-14T21:58:12Z",
    "labels": [
      "a:app"
    ],
    "body": "### Current Behavior\r\n\r\nI have the same problem as https://github.com/wandb/wandb/issues/5222#issue-1637710551.\r\n\r\n\r\n### Expected Behavior\r\n\r\nOnly one column from the plot.\r\n<img width=\"361\" alt=\"image\" src=\"https://user-images.githubusercontent.com/48092144/232418684-9b3d426d-4007-4715-951d-fdf954cd35d1.png\">\r\n\r\n\r\n### Steps To Reproduce\r\n\r\n_No response_\r\n\r\n### Screenshots\r\n\r\nThere are three strange columns (the last two are `__MIN` & `__MAX`):\r\n<img width=\"942\" alt=\"image\" src=\"https://user-images.githubusercontent.com/48092144/232419288-a670a47f-0bb7-4f45-a4d3-722f2ddb64b3.png\">\r\n\r\n\r\n### Environment\r\n\r\nOS:\r\n\r\nBrowsers:\r\n\r\nVersion:\r\n\r\n\r\n### Additional Context\r\n\r\nThanks!",
    "comments": [
      {
        "user": "fyqqyf",
        "body": "And another problem: I can't export two curves into csv. I want to use csv to plot my figure.\r\n<img width=\"374\" alt=\"image\" src=\"https://user-images.githubusercontent.com/48092144/232428759-58b454a1-d425-4dde-932d-e6ba2f5b09a1.png\">\r\n"
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi @fyqqyf! In the first post, you are expecting all 3 columns to be different right?"
      },
      {
        "user": "fyqqyf",
        "body": "> Hi @fyqqyf! In the first post, you are expecting all 3 columns to be different right?\r\n\r\nThanks for your reply! I expect only one real value column without the `min` and `max` values."
      }
    ]
  },
  {
    "issue_number": 5197,
    "title": "[Feature]: Restore scheduling of deleted runs from grid search",
    "author": "carloalbertobarbano",
    "state": "open",
    "created_at": "2023-03-20T09:57:15Z",
    "updated_at": "2025-05-14T21:58:02Z",
    "labels": [
      "ty:feature",
      "c:sweeps"
    ],
    "body": "### Description\n\nWhen performing a grid search, sometimes I have to delete some runs due to crashes. However, once deleted, that specific hyperparameters configuration is not scheduled again. \r\nThe only solution that I found is to create a new identical sweep importing the successful runs, and let the missing one be scheduled again.\n\n### Suggested Solution\n\nRe-run all of the deleted runs until the sweep is completed. This used to be the case in previous version of wandb, and it would avoid the hustle of creating new sweeps from the successful runs in order to re-schedule the missing ones.\n\n### Alternatives\n\n_No response_\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hi @carloalbertobarbano! Your feature request has been submitted! Thank you so much for reporting this and we'll keep you posted on any possible updates we know :)"
      },
      {
        "user": "puzzler10",
        "body": "this used to work. what happened? This isn't a feature request, but a bug fix."
      }
    ]
  },
  {
    "issue_number": 5189,
    "title": "[Feature]: Add ability to pass directory name to wandb.agent",
    "author": "jxtngx",
    "state": "open",
    "created_at": "2023-03-17T17:28:35Z",
    "updated_at": "2025-05-14T21:57:54Z",
    "labels": [
      "ty:feature",
      "c:sweeps"
    ],
    "body": "### Description\r\n\r\nAdd ability to pass a directory name to wandb.agent, as it is with wandb.init's dir keyword argument.\r\n\r\nAn explanation of the request, with code:\r\n\r\n```python\r\ndef sweep_runner(func: Callable, count: int, sweep_config: dict, project: str, dir: str):\r\n    wandb.init(project=project, dir=dir)\r\n    sweep_id = wandb.sweep(sweep=sweep_config, project=project_name)\r\n    wandb.agent(sweep_id, function=func, count=count, dir=dir)\r\n\r\n\r\nsweep_runner(...)\r\n```\r\n\r\n### Suggested Solution\r\n\r\nThe proposed change looks like\r\n\r\n```python\r\nwandb.agent(sweep_id, function=some_objective, count=trial_count, dir: Optional[str | Path] = None)\r\n```\r\n\r\n```mermaid\r\nflowchart LR\r\nA[agent.dir=somepath] --> B(pyagent.dir=agent.dir) --> C(Agent.dir=pyagent.dir) \r\n```\r\n\r\nin Agent, the following change would be made:\r\n\r\n```python\r\nos.environ[wandb.env.DIR] = dir or os.path.abspath(os.getcwd())\r\n```\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional Context\r\n\r\nCurrently, one must set an environment variable, WANDB_DIR, for agent. If done incorrectly, this may push logs for a user to an unintended location.\r\n\r\nWhen using Sweeps without the environment variable set, Sweep logs will be created in the current working directory, and run logs will be created in the path provided to wandb.init's `dir`.",
    "comments": [
      {
        "user": "jxtngx",
        "body": "If accepted, I'd like to be assigned to this issue, and will open a PR."
      },
      {
        "user": "luisbergua",
        "body": "Hi @JustinGoheen, thanks for sharing this feedback, it's really interesting case! I'll share internally with our Product team but you can also contribute to `wandb` and open the PR (instructions [here](https://github.com/wandb/wandb/blob/main/CONTRIBUTING.md)) "
      },
      {
        "user": "jxtngx",
        "body": "I'm still tracking this and intend to submit a PR."
      }
    ]
  },
  {
    "issue_number": 9841,
    "title": "[Q]:  Compatibility of wandb with System Libraries for Debian Packaging",
    "author": "Arian-Ott",
    "state": "closed",
    "created_at": "2025-05-10T22:33:21Z",
    "updated_at": "2025-05-14T15:17:23Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\nHi team,\n\nI'm currently working on packaging wandb for Debian and noticed that the wandb source package includes vendored copies of several libraries, notably gql, graphql-core, and others.\n\nDebian's packaging policy strongly advises against using vendored libraries, as outlined in [§4.13 Embedded code copies](https://www.debian.org/doc/debian-policy/ch-source.html#embedded-code-copies). This policy aims to improve security, maintainability, and consistency within the Debian ecosystem by utilising shared system libraries.\n\nTo ensure compliance with Debian policy and maintain a robust package, I would appreciate clarification on a few points regarding these dependencies:\n\nDoes wandb strictly depend on specific pinned versions of these vendored libraries?\n\nAlternatively, is wandb compatible with a range of library versions (for example, gql>=3.4,<4.0)?\n\nClarifying these points will greatly help in accurately setting dependencies and ensuring compatibility within Debian's packaging framework.\n\nThank you very much for your assistance and guidance!\n\nBest regards,\nArian Ott",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hi @Arian-Ott, thank you for writing in. \n\nWe do have the wandb required libraries here:\nhttps://github.com/wandb/wandb/blob/main/pyproject.toml\n\nWhich, for the most part, are all ranges of libraries that are accepted and not a specific library. "
      },
      {
        "user": "Arian-Ott",
        "body": "\nThanks for the quick response.\n\nTurns out the issue was local. The `wandb/vendor` directory had been unintentionally removed in my working tree, which led to some confusion.\n\nI was able to build and install the package via `apt`and a basic import confirms functionality:\n```python\n>>> import wandb\n>>> wandb.__version__\n'0.19.12.dev1'\n```\nThe package is generally buildable for Debian.  (see the CLI output) That said, there are a few remaining policy issues on my end, primarily around replacing vendored libraries with system-provided ones as required by Debian Policy §4.13 (Embedded code copies). I’ll be patching the relevant imports accordingly.\n\nAppreciate the quick response. \n\nCheers,\nArian Ott\n"
      },
      {
        "user": "ArtsiomWB",
        "body": "Wonderful!\n\nThank you for the quick reply as well, Arian. That's a great catch. \n\nIf you're all set for now, I will close this ticket for tracking purposes. If you have any follow-up questions, feel free to add them here, and it will reopen the thread. "
      }
    ]
  },
  {
    "issue_number": 9848,
    "title": "[Q]: Potential Inconsistencies Between Repo and Model License",
    "author": "yueyangchen1",
    "state": "open",
    "created_at": "2025-05-13T13:34:48Z",
    "updated_at": "2025-05-14T13:07:24Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\nHi, while reviewing the licenses for this repository and the model it depends on, I noticed a potential inconsistency that could cause confusion or legal risks in some situations.\n\nYour repository uses the model runwayml/stable-diffusion-v1-5 at landfill/functional_tests/diffusers/t3_sd_batched_generation.py, which is licensed under creativeml-openrail-m.This license includes the requirement: \"You must cause any modified files to carry prominent notices stating that You changed the files;\"However, your repository license mit does not mention this, and therefore has no such requirement.\n\nAs a result, developers might assume no action is needed when modifying the model, which could unintentionally lead to non-compliance with the model license.\n\nSuggested Actions:\n1.You might consider adding a brief note in the README, LICENSE, or a separate NOTICE file to clarify the model’s license requirements (e.g., stating changes).\n2.It could be helpful to include a reference to the model license and summarize any key obligations, so developers are aware of them.\n3.You may want to gently remind users that, in some cases, they should check both the repository license and the model license, especially when redistributing or modifying the model.\n",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hey @cyy12345649, thanks for flagging this! I'll raise it with our team. Please let us know if you notice any other third-parties with similar license differences"
      },
      {
        "user": "yueyangchen1",
        "body": "Thank you for your reply!\nIf you later confirm whether there is indeed a license inconsistency, I’d appreciate it if you could let me know.\nAlso, I’d be interested to hear your thoughts on how you might consider addressing it, if applicable.\nPlease feel free to reach out anytime—I’m happy to assist if needed."
      }
    ]
  },
  {
    "issue_number": 7406,
    "title": "AttributeError: module 'wandb.proto.wandb_internal_pb2' has no attribute 'Result'",
    "author": "yangboz",
    "state": "closed",
    "created_at": "2024-04-18T02:40:38Z",
    "updated_at": "2025-05-14T09:06:06Z",
    "labels": [
      "a:sdk",
      "s:nexus-fix"
    ],
    "body": "### Describe the bug\r\n\r\n<!--- Description of the issue below  -->\r\n\r\n<!--- A minimal code snippet between the quotes below  -->\r\n```python\r\n\r\n```\r\n\r\n<!--- A full traceback of the exception in the quotes below -->\r\n```shell\r\nWandb.login()\r\n```\r\n\r\n\r\n### Additional Files\r\n\r\nimport wandb\r\nimport random\r\n\r\nwandb.login()\r\n#ndb. start a new wandb run to track this script\r\nwandb.init(\r\n    # set the wandb project where this run will be logged\r\n    project=\"anna-awesome-project\",\r\n\r\n    # track hyperparameters and run metadata\r\n    config={\r\n    \"learning_rate\": 0.02,\r\n    \"architecture\": \"CNN\",\r\n    \"dataset\": \"CIFAR-100\",\r\n    \"epochs\": 10,\r\n    }\r\n)\r\n\r\n# simulate training\r\nepochs = 10\r\noffset = random.random() / 5\r\nfor epoch in range(2, epochs):\r\n    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\r\n    loss = 2 ** -epoch + random.random() / epoch + offset\r\n\r\n    # log metrics to wandb\r\n    wandb.log({\"acc\": acc, \"loss\": loss})\r\n\r\n# [optional] finish the wandb run, necessary in notebooks\r\nwandb.finish()\r\n\r\nquick tutorial from Wandb website.\r\n\r\n### Environment\r\n\r\nWandB version:latest\r\n\r\nOS:Macosx\r\n\r\nPython version:3.9\r\n\r\nVersions of relevant libraries: \r\n\r\n\r\n### Additional Context\r\n\r\n[_No response_](https://docs.wandb.ai/quickstart?_gl=1*f4bqgb*_ga*MTc0OTQ1ODc4Ni4xNzEzNDAyODcy*_ga_JH1SJHJQXJ*MTcxMzQwMjg3MS4xLjEuMTcxMzQwNzQ5Mi42MC4wLjA.)",
    "comments": [
      {
        "user": "yangboz",
        "body": "official quick start  https://docs.wandb.ai/quickstart?_gl=1*f4bqgb*_ga*MTc0OTQ1ODc4Ni4xNzEzNDAyODcy*_ga_JH1SJHJQXJ*MTcxMzQwMjg3MS4xLjEuMTcxMzQwNzQ5Mi42MC4wLjA."
      },
      {
        "user": "SarthakNikhal",
        "body": "@kptkin @yangboz Can I help with this issue?"
      },
      {
        "user": "kptkin",
        "body": "@SarthakNikhal sure go for it :)"
      }
    ]
  },
  {
    "issue_number": 8279,
    "title": "[Bug-App]: Published reports are deleted after discarding draft",
    "author": "smanolloff",
    "state": "closed",
    "created_at": "2024-09-05T13:28:05Z",
    "updated_at": "2025-05-14T00:04:54Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\r\n\r\nPublishing the report, then loading the URL of its draft (in my case it was an old bookmarked URL) loads the report in edit mode. Discarding the draft leads to a misleading message saying that the **changes** will be discarded: instead, the **entire report** is deleted forever.\r\n \r\nSteps to reproduce:\r\n\r\n1. Create a new report and copy the draft URL\r\n2. Publish the report\r\n3. Open the copied draft URL (or press \"back\" twice to go back to the draft)\r\n4. Do a dummy change, press \"Discard draft\"\r\n5. Observe the misleading message and confirm\r\n6. Observe the missing report from the \"Reports\" page. The report is forever lost.\r\n\r\nI lost many hours of work this way. Please, if you know how to recover the deleted report, let me know.\r\n\r\nhttps://github.com/user-attachments/assets/65f46e8b-2b6a-44cc-8acc-81cc0497bbf3\r\n\r\n\r\n",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hi @smanolloff! Thank you very much for writing in. \r\n\r\nI have been able to reproduce this on my end as well. Apologies for the inconvenience. I will go ahead and report this to my engineering team as a bug. "
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi @smanolloff! \n\nI'll be closing this thread out because this behavior should now be fixed. Thank you for bringing it up. "
      }
    ]
  },
  {
    "issue_number": 8937,
    "title": "'wandb.tensorboard.unpatch()' missing in documentation",
    "author": "daniel-bogdoll",
    "state": "open",
    "created_at": "2024-11-22T22:43:09Z",
    "updated_at": "2025-05-13T18:22:00Z",
    "labels": [
      "c:docs"
    ],
    "body": "Hey everyone,\n\nI log my experiments with Tensorboard and have multiple experiments per run. Thus, I need to run:\n\n```\nwandb.tensorboard.patch(root_logdir=log_directory)\nwand.init()\nwriter = SummaryWriter(log_dir=log_directory)\n\n...\nMy experiment\n...\n\nwandb.finish()\nwriter.close()\nwandb.tensorboard.unpatch()\n```\n\nHowever, `unpatch()` is not documented, I only found it because your code is open-source :) If it is not called, an error occurs:\n\n```\n\"Tensorboard already patched, remove `sync_tensorboard=True` \"\n            \"from `wandb.init` or only call `wandb.tensorboard.patch` once.\"\n```\n\nThis is the function: https://github.com/wandb/wandb/blob/0bf2ea43770e7349a57fc776aeb16d3035ce4dbf/wandb/integration/tensorboard/monkeypatch.py#L18\n\nThis is the documentation: https://docs.wandb.ai/guides/integrations/tensorboard/. Here, only `patch()` is explained. In addition, the error messsage could be improved by also mentioning that `unpatch()` is a valid option.\n\nHope that helps! :)",
    "comments": [
      {
        "user": "fdsig",
        "body": "Hey @daniel-bogdoll,\n\nThanks so much for bringing this to our attention. In line with what you have helpfully highlighted I will submit feedback here for our documentation team to include details on the `unpatch()` method. \n\nRegarding the error itself, I wonder if you would object to sharing:\n1. The `debug.log` and `debug-internal.log` for located in your wandb dir of your training directory and the full stack trace.\n2. The `wandb --version` of SDK that you are using.\n3. A fuller code snippet and or detail of the use case.\n\nTo provide some helpful context, I am able to successfully run (no `unpatch()`) on our most recent SDK version :\n\n```python\nimport wandb\nfrom torch.utils.tensorboard import SummaryWriter\n\nlog_directory = \"./tb_logs\"\nwandb.tensorboard.patch(root_logdir=log_directory)\nrun = wandb.init(entity=\"demonstrations\", project=\"dis\")\nwriter = SummaryWriter(log_dir=log_directory)\nfor step in range(10):\n    writer.add_scalar(\"metric\", step, step)\n\nrun.finish()\nwriter.close()\n```\n\nTherefore we would love to understand what about your workflow might be causing a crash here."
      },
      {
        "user": "daniel-bogdoll",
        "body": "Hey, this works, but the issue is if you put it into a loop:\n\n```\nfor i in range(10):\n        log_directory = NEW_LOG_DIR_FOR_EACH_LOOP\n        wandb.tensorboard.patch(root_logdir=log_directory)\n        run = wandb.init(entity=\"demonstrations\", project=\"dis\")\n        writer = SummaryWriter(log_dir=log_directory)\n        for step in range(10):\n            writer.add_scalar(\"metric\", step, step)\n        \n        run.finish()\n        writer.close()\n```\n\nThis should also fail on your side :)\n"
      },
      {
        "user": "fdsig",
        "body": "Hey, @daniel-bogdoll -- thanks for providing clarifying details here so quickly. To confirm I can reproduce the error message that you observe when this is inside a for loop. I will request a docs update with the `unpatch()` method. So that we can ensure alignment of understanding would you mind sharing what the use case is here -- I.e are you initializing multiple training runs in series (10 in your example above) with new corresponding `writer` and `run` objects?\n\n"
      }
    ]
  },
  {
    "issue_number": 7350,
    "title": "[CLI]: Large numbers in parameter section of sweep config breaks sweep by repeating runs",
    "author": "HannesStagge",
    "state": "closed",
    "created_at": "2024-04-10T15:00:23Z",
    "updated_at": "2025-05-13T16:50:39Z",
    "labels": [
      "c:sweeps"
    ],
    "body": "### Describe the bug\n\nI encountered some repeated runs when running grid hyperparameter studies. See \r\n[here](https://github.com/wandb/wandb/issues/7173) for another example. \r\n@luisbergua and @QianqianF this might possibly be interesting for you too?\r\n\r\nI tracked it to large exponential numbers (1-1e15) in my sweep configs parameter section. \r\n\r\nThe config that reproduces the error:\r\n\r\n```python\r\nsweep_config = {\r\n        \"method\": \"grid\",\r\n        \"name\": this_file.stem,\r\n        \"metric\": {\"goal\": \"minimize\", \"name\": \"test/loss\"},\r\n        \"parameters\": {\r\n            \"param_A\": {\"values\": [1, 3, 4, 5]},\r\n            \"param_B\": {\r\n                \"values\": [\r\n                    1,\r\n                    10,\r\n                    100,\r\n                    1e3,\r\n                    1e4,\r\n                    1e5,\r\n                    1e6,\r\n                    1e7,\r\n                    1e8,\r\n                    1e9,\r\n                    1e10,\r\n                    1e11,\r\n                    1e12,\r\n                    1e13,\r\n                    1e14,\r\n                    1e15,\r\n                ]\r\n            },\r\n        },\r\n    }\r\n```\r\nas opposed to\r\n```python\r\nsweep_config = {\r\n        \"method\": \"grid\",\r\n        \"name\": this_file.stem,\r\n        \"metric\": {\"goal\": \"minimize\", \"name\": \"test/loss\"},\r\n        \"parameters\": {\r\n            \"param_A\": {\"values\": [1, 3, 4, 5]},\r\n            \"param_B\": {\r\n                \"values\": [\r\n                    0,\r\n                    1,\r\n                    2,\r\n                    3,\r\n                    4,\r\n                    5,\r\n                    6,\r\n                    7,\r\n                    8,\r\n                    9,\r\n                    10,\r\n                    11,\r\n                    12,\r\n                    13,\r\n                    14,\r\n                    15,\r\n                ]\r\n            },\r\n        },\r\n    }\r\n```\r\nworking fine. \r\n\r\nParallel coordinates for the [faulty case](https://wandb.ai/ice_ulm/wandb_test/sweeps/uq26kl7o): \r\n![grafik](https://github.com/wandb/wandb/assets/53195185/4fd138e5-2949-4c55-8d39-baedd40d04a3)\r\nNote the missing runs for param_A=[3,4,5]\r\nAnd for the [correct case](https://wandb.ai/ice_ulm/wandb_test/sweeps/iaee157q):\r\n![grafik](https://github.com/wandb/wandb/assets/53195185/fccc51d3-884a-4ed8-816c-c77d0e9b85c0)\r\n\r\nHeres the complete code for the [faulty](https://gist.github.com/HannesStagge/9b7501769939af96ee9c9c2d5b9132a0) and [correct](https://gist.github.com/HannesStagge/4b167380873bc1e099d08e2033e56d60) case\r\n\r\n\n\n### Additional Files\n\n_No response_\n\n### Environment\n\nWandB version: 0.16.2\r\n\r\nOS: Windows-10-10.0.22631-SP0\r\n\r\nPython version: 3.10.12\r\n\r\nVersions of relevant libraries: \r\ntorch: 2.0.0\r\nlightning: 2.0.9\r\n\n\n### Additional Context\n\nI am running the agents on multiple cores with multiprocessing, but that does not seem to influence the workings from my experience.",
    "comments": [
      {
        "user": "thanos-wandb",
        "body": "Hi @HannesStagge thank you so much for the detailed report. Investigating this further and will get back to you with any updates. "
      },
      {
        "user": "thanos-wandb",
        "body": "@HannesStagge we've reproduced this issue on our end, and reported to our engineering teams. We will keep you posted with any updates. Thanks for reporting!"
      },
      {
        "user": "thanos-wandb",
        "body": "Hi @HannesStagge apologies for the late followup here, but wanted to share a quick update to let you know that we're now closing this ticket as it's been fixed with this PR: https://github.com/wandb/sweeps/pull/109. Please feel free to reopen it if you're still running into this issue, and we will be more than happy to keep investigating.  "
      }
    ]
  },
  {
    "issue_number": 9844,
    "title": "[Bug]: Nested wandb.config updates are not synced to Online UI",
    "author": "Trezorro",
    "state": "open",
    "created_at": "2025-05-12T15:13:24Z",
    "updated_at": "2025-05-13T13:26:47Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "# Nested wandb.config updates are Not Synced or Reflected to Online UI\n\n## Description\n\nWhen updating nested keys in `wandb.config` (e.g., `config[\"root_key\"][\"nested_key\"] = value`), the change appears locally but is **not** synchronized with the W&B web UI. This leads to discrepancies between local config values and what is shown in the online interface.\n\nThe docs don't explicitly mention this behaviour. See: https://docs.wandb.ai/guides/track/config/#set-the-configuration-throughout-your-script\n\n## Steps to Reproduce\n\nHere’s a simplified reproduction using the W&B API:\n\n```python\nimport wandb\n\nconf = {\n    \"model\": \"flow\",\n    \"data\": {\"key1\": 1}\n}\n\nrun = wandb.init(\n    name=\"RUNNAME\",\n    project=\"YOUR_PROJECT\",\n    config=conf,\n)\n##  During run: \n# Direct sub-key update – does not sync to UI\nrun.config['data']['key2'] = 2\n\n# Inspect local config – change is visible locally\nprint(run.config['data'])\n# Output: {'key1': 1, 'key2': 2}\n# But only 'key1' appears in the W&B UI\n\n# Overwriting the full nested dictionary – works\nrun.config.update({'data': {'new': 42}}, allow_val_change=True)  # ❌ works but deletes previous information\n```\n\n### More examples of current behavior:\n```python\nimport wandb\n\nconf = {\n    \"model\": \"flow\",\n    \"data\": {\"key1\": 1}\n}\n\nrun = wandb.init(\n    name=\"RUNNAME\",\n    project=\"YOUR_PROJECT\",\n    config=conf,\n)\n# New top-level key – syncs fine\nrun.config.update({'top_level': 123})  # ✅ reflected online instantly\n\n# Nested update via dict method – not reflected online\nrun.config['data'].update({'nested_key': 42})  # ❌ not synced\n\n# Still does not reflect additional updates to sub-keys\nrun.config['data']['another'] = 99  # ❌ not synced\n# Data online shows {'key1': 1}\n\n# Using |= triggers synchronization\nrun.config['data'] |= {'good_new_nested_key': 777}  # ✅ now everything is reflected\n\n# Verify final local state\nprint(run.config['data'])\n# Output includes all changes, and now visible in UI:\n# data contains: {'key1': 1, 'nested_key': 42, 'another': 99, 'good_new_nested_key': 777}\n```\n\n\n## Workaround\nUse the |= operator to update nested dictionaries so W&B triggers a sync:\n\n```python\n# Correct method to ensure sync\nrun.config['data'] |= {'key2': 2}\n```\nAvoid using:\n\n```python\n# This looks correct but does NOT sync with the W&B UI\nrun.config['data']['key2'] = 2\n```\n\n## Expected Behavior\nAny update to wandb.config, including nested sub-keys, should either:\n\n- Automatically sync with the online UI, or\n- Raise a warning that sub-key changes won’t be tracked unless the full root key is reassigned.\n\n## Actual Behavior\nDirect updates to sub-keys of nested dictionaries (e.g., config[\"nested\"][\"key\"] = val) are not synchronized with the online UI. Changes remain local only and are not visible in the run’s configuration panel.\n\n### Reason: \nThe root `wandb.config` object is a custom `Config` class that handles synchronization and tracking. However, its **nested dictionary values are standard `dict` instances**. The W&B documentation includes a note that supports this:\n\n> \"Use the dictionary access syntax [\"key\"][\"value\"] instead of the attribute access syntax config.key.value if your script accesses run.config keys below the root.\"\n\nThis explains the behaviour.\n\n## Impact\nNested configs are highly useful for organizing run configurations, and are regarded as standard good practice. Libraries such as Omega and Hydra specialize in such configs. Sweeps also support them via keys with dots. \n\nCurrent behavior is counterintuitive because the W&B documentation seems to suggests that .update() or direct dictionary manipulation should work on nested config objects. However, without full re-assignment, these updates are silently ignored by the sync process. This makes reproducibility and run tracking less reliable.\n \n# Proposed Fix\nClearly document that nested keys require full-dict updates using `update()` or `|=` assignment to sync properly.  \nOR   \nEnable automatic detection and sync of nested sub-key updates, if feasible.  \n\nFurther, the error given when reassigning an existing key with multiple nested items could suggest the `|=| workaround. The current error just gives the message:\n```\nwandb.sdk.lib.config_util.ConfigError: Attempted to change value of key \"data\" from  .... to ...\nIf you really want to do this, pass allow_val_change=True to config.update()\n```\n### Docs Suggestion\nClarify in the W&B documentation that:\n\n- Only top-level keys in wandb.config are tracked for synchronization\n-  Nested dictionaries inside wandb.config are not themselves tracked, so direct mutation (e.g., config[\"x\"][\"y\"] = val) will not sync\n- To ensure synchronization, always update the root-level key (e.g., config[\"x\"] |= {...} or config.update({...}))\n\nOptionally, raise a runtime warning when nested dictionaries are accessed or mutated directly (if feasible), or provide a utility wrapper (e.g., config.update_nested(key_path, val)).\n\n## Environment\nW&B Version: 0.19.9\n\nPython Version: 3.11.7\n\nOS: e.g. macOS 14.7.2\n\n\n\n",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @Trezorro, Thanks so much for flagging this and for the detailed report. I’ve logged the issue internally, and we’ll keep this thread updated as soon as we have a fix. Appreciate your patience in the meantime!"
      }
    ]
  },
  {
    "issue_number": 7311,
    "title": "[Q] Questions about freeing up storage.",
    "author": "nzl-thu",
    "state": "closed",
    "created_at": "2024-04-05T03:53:46Z",
    "updated_at": "2025-05-13T12:24:28Z",
    "labels": [
      "a:app"
    ],
    "body": "Hello,\r\n\r\nI am seeking to optimize the storage usage in my wandb account by removing media files and have two questions regarding the storage management UI:\r\n\r\n1. When I choose a run and hit the delete button like this:\r\n<img width=\"1859\" alt=\"image\" src=\"https://github.com/wandb/wandb/assets/36226201/31df311d-3adb-4381-9520-1ac252f046ba\">\r\nwhich of the following is the expected outcome?\r\n\r\na) the entire run is removed, including the logged data (e.g., training loss).\r\nb) only the media/artifacts associated with the run are deleted, while the logged data remains intact.\r\nI am inclined to believe it is the latter (b), as the run directory appears to contain only a \"media\" folder. However, as the exact storage location of the logged data is not unclear to me, I am seeking confirmation here.\r\n\r\n\r\n2. After I deleted all files in a run (`dm6frnej` for example):\r\n<img width=\"1862\" alt=\"image\" src=\"https://github.com/wandb/wandb/assets/36226201/f0df81e6-1573-4598-ab4b-09b9656c92f7\">\r\nwhy the storage utilization of this run remains changed?\r\n<img width=\"1868\" alt=\"image\" src=\"https://github.com/wandb/wandb/assets/36226201/230d4e73-8556-49b4-9d76-20c68689b40d\">\r\nI have read that wandb has some \"time-to-live (TTL)\" mechanism for artifacts management. However, I have not set the TTL policy for my project so I suppose this may not be the reason.\r\n\r\n\r\nThank you!",
    "comments": [
      {
        "user": "Adamits",
        "body": "I am seeing exactly the same behavior. I would have thought that some caching mechanism could be causing this, but several days have passed and the app still says I have no more storage space.\r\n\r\nThis is a pretty large problem as it makes my account unusable."
      },
      {
        "user": "nzl-thu",
        "body": "> I am seeing exactly the same behavior. I would have thought that some caching mechanism could be causing this, but several days have passed and the app still says I have no more storage space.\r\n> \r\n> This is a pretty large problem as it makes my account unusable.\r\n\r\nYes! This is frustrating..."
      },
      {
        "user": "Riccorl",
        "body": "I've been deleting files through the Python API for a week but still see no changes in the web UI. I want to access my runs at some point..."
      }
    ]
  },
  {
    "issue_number": 9199,
    "title": "[Q]: Setting the maximum value of y axis in a line plot programmatically (Python)",
    "author": "ardarslan",
    "state": "closed",
    "created_at": "2025-01-07T14:45:10Z",
    "updated_at": "2025-05-13T12:19:06Z",
    "labels": [
      "ty:question",
      "a:app"
    ],
    "body": "### Ask your question\n\nHello,\n\nI was wondering if there is a way to set the maximum value of the y axis in a line plot programmatically (asking for Python). I can set it manually on the wandb website, however it would be great if I could also set it programmatically. For example, I set this value to 1 in the following line plot:\n\n<img width=\"606\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e3a70ba0-a35e-48f3-8779-9c40d98ac406\" />",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hey @ardarslan, thanks for your question! This is possible with programmatic workspaces (see docs [here](https://docs.wandb.ai/tutorials/workspaces/#3-programmatic-workspace-examples)). Specifically, you can define the y_range of a line plot like:\n```\nwr.LinePlot(range_y=(min, max))\n```\nPlease let me know if this is helpful!"
      },
      {
        "user": "luisbergua",
        "body": "Hi @ardarslan, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!\n"
      },
      {
        "user": "ardarslan",
        "body": "Hi @luisbergua, thanks a lot for your reply! I made only a few changes in the code [here](https://docs.wandb.ai/tutorials/workspaces/#3-programmatic-workspace-examples) (please see the modified version of the code below). However, on the wandb website, the maximum value of y is still not 1.0. Could you please let me know if I'm missing something?\n\n```\nimport os\nimport sys\nimport wandb\nimport wandb_workspaces.workspaces as ws\nimport wandb_workspaces.reports.v2 as wr # We use the Reports API for adding panels\nfrom datetime import datetime, timezone\n\ndatetime_str = datetime.now(timezone.utc).strftime(\"%Y_%m_%d_%H_%M_%S\")\n\n# Improve output formatting\n%load_ext rich\n\n# Initialize Weights & Biases and Login\nwandb.login()\n\n# Function to create a new project and log sample data\ndef create_project_and_log_data():\n    project = \"ProjectName\"  # Default project name\n\n    # Initialize a run to log some sample data\n    with wandb.init(project=project, name=datetime_str) as run:\n        for step in range(100):\n            wandb.log({\n                \"Step\": step,\n                \"val_loss\": 1.0 / (step + 1),\n                \"val_accuracy\": step / 100.0,\n                \"train_loss\": 1.0 / (step + 2),\n                \"train_accuracy\": step / 110.0,\n            })\n    return project\n\n# Create a new project and log data\nproject = create_project_and_log_data()\nentity = wandb.Api().default_entity\n\n# Function to create and configure a workspace with custom settings\ndef custom_settings_example(entity: str, project: str) -> None:\n    workspace = ws.Workspace(name=\"An example workspace\", entity=entity, project=project)\n    workspace.sections = [\n        ws.Section(\n            name=\"Loss\",\n            panels=[\n                wr.LinePlot(x=\"Step\", y=[\"train_loss\"], range_y=(0.0, 1.0)),\n                wr.LinePlot(x=\"Step\", y=[\"val_loss\"], range_y=(0.0, 1.0)),\n            ],\n            is_open=False,\n        ),\n        ws.Section(\n            name=\"Accuracy\",\n            panels=[\n                wr.LinePlot(x=\"Step\", y=[\"train_accuracy\"], range_y=(0.0, 1.0)),\n                wr.LinePlot(x=\"Step\", y=[\"val_accuracy\"], range_y=(0.0, 1.0)),\n            ],\n            is_open=False,\n        ),\n    ]\n\n    workspace.save()\n    print(\"Workspace with custom settings saved.\")\n\n# Run the function to create and configure the workspace\ncustom_settings_example(entity, project)\n```\n"
      }
    ]
  },
  {
    "issue_number": 5580,
    "title": "[CLI]: @wandb_log failed integration with metaflow - wandb.errors.AuthenticationError",
    "author": "erichhhhho",
    "state": "closed",
    "created_at": "2023-05-19T06:31:03Z",
    "updated_at": "2025-05-13T10:23:05Z",
    "labels": [
      "a:app"
    ],
    "body": "### Current Behavior\r\n\r\nWhen creating metaflow flow, following https://docs.wandb.ai/guides/integrations/metaflow:\r\n```\r\nexport WANDB_API_KEY=<YOUR KEY>\r\n```\r\n\r\n```python\r\nfrom metaflow import FlowSpec, Parameter, step, batch, environment\r\nfrom wandb.integration.metaflow import wandb_log\r\nimport wandb\r\n\r\n@wandb_log(datasets=True, models=True, settings=wandb.Settings(api_key=os.getenv(\"WANDB_API_KEY\"))\r\nclass WandbExampleFlow(FlowSpec):\r\n  @step\r\n  def start(self):\r\n    self.raw_df = pd.read_csv(...).    # pd.DataFrame -> upload as dataset\r\n    self.model_file = torch.load(...)  # nn.Module    -> upload as model\r\n    self.next(self.mid)\r\n\r\n  @environment(\r\n        vars={\r\n            \"WANDB_API_KEY\": os.getenv(\"WANDB_API_KEY\"),\r\n        }\r\n    )\r\n  @batch(gpu=1)\r\n  @retry(times=2)\r\n  @step\r\n  def mid(self):\r\n    self.raw_df = pd.read_csv(...).    # pd.DataFrame -> upload as dataset\r\n    self.model_file = torch.load(...)  # nn.Module    -> upload as model\r\n    self.next(self.end)\r\n\r\n  @step\r\n  def end(self):\r\n    self.raw_df = pd.read_csv(...).    \r\n    self.model_file = torch.load(...)\r\n```\r\n\r\nMet the following error when it is running the step with @batch:\r\n```bash\r\nwandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\r\nwandb: ERROR Error while calling W&B API: user is not logged in (<Response [401]>)\r\n<flow XXX step train> failed:\r\n    Internal error\r\nTraceback (most recent call last):\r\n  File \"/home/code/metaflow/metaflow/cli.py\", line 1171, in main\r\n    start(auto_envvar_prefix=\"METAFLOW\", obj=state)\r\n  File \"/home/code/metaflow/metaflow/_vendor/click/core.py\", line 829, in __call__\r\n    return self.main(args, kwargs)\r\n  File \"/home/code/metaflow/metaflow/_vendor/click/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/code/metaflow/metaflow/_vendor/click/core.py\", line 1259, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/code/metaflow/metaflow/_vendor/click/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, ctx.params)\r\n  File \"/home/code/metaflow/metaflow/_vendor/click/core.py\", line 610, in invoke\r\n    return callback(args, kwargs)\r\n  File \"/home/code/metaflow/metaflow/_vendor/click/decorators.py\", line 21, in new_func\r\n    return f(get_current_context(), args, kwargs)\r\n  File \"/home/code/metaflow/metaflow/cli.py\", line 580, in step\r\n    task.run_step(\r\n  File \"/home/code/metaflow/metaflow/task.py\", line 587, in run_step\r\n    self._exec_step_function(step_func)\r\n  File \"/home/code/metaflow/metaflow/task.py\", line 61, in _exec_step_function\r\n    step_function()\r\n  File \"/usr/local/lib/python3.9/site-packages/wandb/integration/metaflow/metaflow.py\", line 307, in wrapper\r\n    with wandb.init(settings=settings) as run:\r\n  File \"/usr/local/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1169, in init\r\n    raise e\r\n  File \"/usr/local/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1150, in init\r\n    run = wi.init()\r\n  File \"/usr/local/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 769, in init\r\n    raise error\r\nwandb.errors.AuthenticationError: The API key you provided is either invalid or missing.  If the `WANDB_API_KEY` environment variable is set, make sure it is correct. Otherwise, to resolve this issue, you may try running the 'wandb login --relogin' command. If you are using a local server, make sure that you're using the correct hostname. If you're not sure, you can try logging in again using the 'wandb login --relogin --host [hostname]' command.(Error 401: Unauthorized)\r\n\r\n    AWS Batch error:\r\n    Essential container in task exited This could be a transient error. Use @retry to retry.\r\n\r\nTask failed.\r\n```\r\n\r\n\r\n\r\n### Expected Behavior\r\n\r\n@wandb_log should be able to integrate metaflow step with @batch and pass in the api key\r\n\r\n### Steps To Reproduce\r\n\r\n1. Create Flow mentioned above\r\n2. Run the flow\r\n\r\n### Screenshots\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nOS:\r\nUbuntu 22.04.1 LTS\r\n\r\nVersion:\r\nmetaflow-2.7.14\r\nwandb-0.15.2\r\n\r\n### Additional Context\r\n\r\nRelated issue: \r\n#2864 \r\n#2962 \r\nhttps://github.com/Netflix/metaflow/issues/814\r\n@andrewtruong \r\n",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hi @erichhhhho, thanks for reporting this! I have tested with the [code from the docs](https://docs.wandb.ai/guides/integrations/metaflow#decorate-your-flows-and-steps) and it works properly for me. Could you try setting your api key in the script as `os.environ[\"WANDB_API_KEY\"] = <your_key>` and see if you get the same error?"
      },
      {
        "user": "luisbergua",
        "body": "Hi @erichhhhho , I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved."
      },
      {
        "user": "erichhhhho",
        "body": "> Hi @erichhhhho, thanks for reporting this! I have tested with the [code from the docs](https://docs.wandb.ai/guides/integrations/metaflow#decorate-your-flows-and-steps) and it works properly for me. Could you try setting your api key in the script as `os.environ[\"WANDB_API_KEY\"] = <your_key>` and see if you get the same error?\r\n\r\nYes, I got the same error. Adding WANDB_API_KEY in local environment variable will not be able to resolve this issue. Basically, it might need @wandb_log to pass the local WANDB_API_KEY into the metaflow step with `@batch`, since the step is running in a different container. Could you try adding step with `@batch`?"
      }
    ]
  },
  {
    "issue_number": 7151,
    "title": "[Feature]: enable nested sections in wandb workspace",
    "author": "oliverwm1",
    "state": "closed",
    "created_at": "2024-03-12T20:48:48Z",
    "updated_at": "2025-05-12T22:27:12Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "### Description\n\nIn the wandb workspace, metrics are currently grouped according to everything prior to the last `/` in the metric name. It would be useful to have nested grouping/sections in order to allow easier exploration of wandb runs with many metrics.\n\n### Suggested Solution\n\nGroup according to all `/`s in the metric names. For example, if a run logs\r\n```\r\ntrain/loss/foo\r\ntrain/loss/bar\r\nvalid/loss/foo\r\nvalid/loss/bar\r\nvalid/f1\r\n```\r\nthen the sections in the wandb workspace would be organized as:\r\n```\r\ntrain\r\n└── loss\r\n    ├── foo\r\n    └── bar\r\nvalid\r\n├── loss\r\n│   ├── foo\r\n│   └── bar\r\n└── f1\r\n```\r\n\n\n### Alternatives\n\n_No response_\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "anmolmann",
        "body": "Hi @oliverwm1, thanks for writing in and requesting this feature. I've shared this feature request internally to our product team. We'll keep you posted regarding the updates."
      },
      {
        "user": "b-d-e",
        "body": "Hi @anmolmann - this would be super useful for me too. Are there any plans to support it?"
      },
      {
        "user": "anmolmann",
        "body": "Hi @b-d-e , thanks for writing in on this thread. However, our team has decided internally that we won't be able to work on this feature anytime soon due to other high-priority tasks."
      }
    ]
  },
  {
    "issue_number": 7987,
    "title": "[Feature]: Add option to skip warning on unsupported schemes",
    "author": "dmcc",
    "state": "open",
    "created_at": "2024-07-22T20:07:56Z",
    "updated_at": "2025-05-12T21:20:29Z",
    "labels": [
      "c:artifacts"
    ],
    "body": "### Description\n\nThis is pretty minor, but hopefully easy to address (and might end up giving more control over warnings in general, which would also be great).\r\n\r\nThe [way](https://github.com/quant-aq/aeromancy) that we're using W&B involves using custom URL schemes for Artifacts. As a result, we get [this warning message](https://github.com/wandb/wandb/blob/8d8c711084977aaf75c3203a831067fdf0f49924/wandb/sdk/artifacts/storage_handlers/tracking_handler.py#L65) often:\r\n\r\n    wandb: WARNING Artifact references with unsupported schemes cannot be checksummed: [...]\n\n### Suggested Solution\n\nWe would like a method for either disabling the warning or making our custom scheme \"supported\" (still without checksumming).\n\n### Alternatives\n\nWe considered existing supported schemes but they don't work for our cases (e.g., they're not actually valid HTTP URLs, nor are they files, etc.).\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "JoanaMarieL",
        "body": "Hello @dmcc , thank you for writing in and will be happy to create a feature request for this one. Could you share more about it like the impact to your workflow, etc. "
      },
      {
        "user": "dmcc",
        "body": "Thanks @JoanaMarieL! The main impact is log spam and that Aeromancy users will think there's an issue when there isn't. It's not a huge deal but it will likely confuse users.\r\n\r\nTo expand on a possible solution, it might be nice to have a keyword argument to `Run.log_artifact`, perhaps something like `ignore_unsupported_schemas=True` to bypass the warning.\r\n\r\nhttps://github.com/wandb/wandb/blob/2a450fa8643038b89dce9520574eda59692b8498/wandb/sdk/wandb_run.py#L3034"
      },
      {
        "user": "JoanaMarieL",
        "body": "Thank you for these information, I've created a Feature Request and will update you for any progress. "
      }
    ]
  },
  {
    "issue_number": 8304,
    "title": "[Bug]: wandbcallback xgboost integration fails for xgboost.cv ",
    "author": "seanv507",
    "state": "open",
    "created_at": "2024-09-09T09:45:03Z",
    "updated_at": "2025-05-12T21:19:44Z",
    "labels": [
      "ty:bug",
      "c:sdk:integration"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\r\nxgboost = 2.1.1\r\nwandb = 0.17.9\r\n\r\nThe callback works with xgboost.train but when using xgboost.cv I get the following error.\r\n\r\nbst = xgb.cv(\r\n        params,\r\n        dtrain,\r\n        num_round,\r\n        nfold=len(folds),\r\n        folds = folds,\r\n        metrics = {\"reg:squaredlogerror\"},\r\n        callbacks=[WandbCallback(log_model=False)],\r\n        verbose_eval=True\r\n    )\r\n\r\n```\r\n\r\nFile ~/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/training.py:565, in cv(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle, custom_metric)\r\n    [557](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/training.py:557) callbacks_container = CallbackContainer(\r\n    [558](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/training.py:558)     callbacks,\r\n    [559](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/training.py:559)     metric=metric_fn,\r\n    [560](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/training.py:560)     is_cv=True,\r\n    [561](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/training.py:561)     output_margin=callable(obj) or metric_fn is feval,\r\n    [562](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/training.py:562) )\r\n    [564](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/training.py:564) booster = _PackedBooster(cvfolds)\r\n--> [565](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/training.py:565) callbacks_container.before_training(booster)\r\n    [567](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/training.py:567) for i in range(num_boost_round):\r\n    [568](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/training.py:568)     if callbacks_container.before_iteration(booster, i, dtrain, None):\r\n\r\nFile ~/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/callback.py:179, in CallbackContainer.before_training(self, model)\r\n    [177](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/callback.py:177) \"\"\"Function called before training.\"\"\"\r\n    [178](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/callback.py:178) for c in self.callbacks:\r\n--> [179](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/callback.py:179)     model = c.before_training(model=model)\r\n    [180](https://file+.vscode-resource.vscode-cdn.net/Users/seanviolante/projects/kaggle_store_sales/~/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/callback.py:180)     msg = \"before_training should return the model\"\r\n    [181](https://file+.vscode-resource.vscode-cdn.net/Users/seanviolante/projects/kaggle_store_sales/~/miniforge3/envs/ML/lib/python3.11/site-packages/xgboost/callback.py:181)     if self.is_cv:\r\n\r\nFile ~/..../lib/python3.11/site-packages/wandb/integration/xgboost/xgboost.py:117, in WandbCallback.before_training(self, model)\r\n    [115](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/wandb/integration/xgboost/xgboost.py:115) \"\"\"Run before training is finished.\"\"\"\r\n    [116](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/wandb/integration/xgboost/xgboost.py:116) # Update W&B config\r\n--> [117](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/wandb/integration/xgboost/xgboost.py:117) config = model.save_config()\r\n    [118](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/wandb/integration/xgboost/xgboost.py:118) wandb.config.update(json.loads(config))\r\n    [120](https://file+.vscode-resource.vscode-cdn.net/XXX/miniforge3/envs/ML/lib/python3.11/site-packages/wandb/integration/xgboost/xgboost.py:120) return model\r\n\r\nAttributeError: '_PackedBooster' object has no attribute 'save_config'\r\n```\r\n\r\n",
    "comments": [
      {
        "user": "JoanaMarieL",
        "body": "Hi @seanv507 , thank you for writing in and happy to help. Could you please provide the following to have better understanding of the scenario:\r\n\r\n-Specific Use case or experiment  you are doing\r\n-the debug.log and debug-internal.log. These files are under your local folder wandb/run-<date>_<time>-<run-id>/logs in the same directory where you’re running your code.\r\n-Full Stack Trace Error"
      },
      {
        "user": "seanv507",
        "body": "Use case: I want to use xgb.cv function to do crossvalidation instead of a\r\nsingle train-test split\r\n\r\nbelow is a reproducible  example modified from your demos (\r\nhttps://github.com/wandb/examples/blob/master/examples/boosting-algorithms/xgboost-dermatology/train.py\r\n)\r\n\r\nthe problem is that xgboost.cv uses a _PackedBooster class (which has a\r\nmodel for each crossvalidation fold).  So I believe one would have to do\r\nmodel.cvfolds[0].bst.save_config in the callback code.\r\n\r\n\r\n\r\n# modified from\r\nhttps://github.com/dmlc/xgboost/blob/master/demo/multiclass_classification/train.py\r\n#%%\r\nimport wandb\r\nimport numpy as np\r\nimport xgboost as xgb\r\n\r\nwandb.init()\r\n#%%\r\n# label need to be 0 to num_class -1\r\ndata = np.loadtxt(\r\n\"./dermatology.data\",\r\ndelimiter=\",\",\r\nconverters={33: lambda x: int(x == \"?\"), 34: lambda x: int(x) - 1},\r\n)\r\nsz = data.shape\r\n\r\ntrain = data[: int(sz[0] * 0.7), :]\r\ntest = data[int(sz[0] * 0.7) :, :]\r\ntrain_X = train[:, :33]\r\ntrain_Y = train[:, 34]\r\n\r\ntest_X = test[:, :33]\r\ntest_Y = test[:, 34]\r\n\r\nxg_train = xgb.DMatrix(train_X, label=train_Y)\r\nxg_test = xgb.DMatrix(test_X, label=test_Y)\r\n\r\n\r\n# setup parameters for xgboost\r\nparam = {}\r\n# use softmax multi-class classification\r\nparam[\"objective\"] = \"multi:softmax\"\r\n# scale weight of positive examples\r\nparam[\"eta\"] = 0.1\r\nparam[\"max_depth\"] = 6\r\nparam[\"silent\"] = 1\r\nparam[\"nthread\"] = 4\r\nparam[\"num_class\"] = 6\r\nwandb.config.update(param)\r\n\r\nwatchlist = [(xg_train, \"train\"), (xg_test, \"test\")]\r\nnum_round = 5\r\nbst = xgb.train(\r\nparam, xg_train, num_round, watchlist, callbacks=\r\n[wandb.xgboost.WandbCallback()]\r\n)\r\n\r\n#%%\r\n# ADD FOLLOWING LINES\r\n\r\nxg_all = xgb.DMatrix(data[:,:33], label=data[:,34])\r\n\r\nxgb.cv(\r\nparam, xg_all, num_round, nfold = 3, metrics=['mlogloss'],\r\ncallbacks=[wandb.xgboost.WandbCallback()]\r\n)\r\n# ADD ABOVE LINES\r\n\r\n#%%\r\n# get prediction\r\npred = bst.predict(xg_test)\r\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\r\nprint(\"Test error using softmax = {}\".format(error_rate))\r\nwandb.summary[\"Error Rate\"] = error_rate\r\n\r\n# %%\r\n\r\n\r\n\r\n\r\nOn Tue, Sep 10, 2024 at 9:19 AM Joana Marie Llabora <\r\n***@***.***> wrote:\r\n\r\n> Hi @seanv507 <https://github.com/seanv507> , thank you for writing in and\r\n> happy to help. Could you please provide the following to have better\r\n> understanding of the scenario:\r\n>\r\n> -Specific Use case or experiment you are doing\r\n> -the debug.log and debug-internal.log. These files are under your local\r\n> folder wandb/run-_-/logs in the same directory where you’re running your\r\n> code.\r\n> -Full Stack Trace Error\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/wandb/wandb/issues/8304#issuecomment-2339862862>, or\r\n> unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAZHCL2MYDOTHHFUGRAMYC3ZV2MRDAVCNFSM6AAAAABN4EWR7CVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGMZZHA3DEOBWGI>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      },
      {
        "user": "JoanaMarieL",
        "body": "Hello @seanv507 thank you so much for these details. This has been logged as Feature Request and will keep you updated for any progress."
      }
    ]
  },
  {
    "issue_number": 8314,
    "title": "[Feature]: Removing the Cap on Run Comparer",
    "author": "MoH-assan",
    "state": "open",
    "created_at": "2024-09-09T20:52:48Z",
    "updated_at": "2025-05-12T21:19:39Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "### Description\r\n\r\n<!--- Describe your feature here --->\r\nIt seems that Run Comparer is capped at ten runs. \r\nThis is even the case in the official example below. \r\nhttps://wandb.ai/stacey/pytorch_gan_tutorial/reports/Run-Comparison-Table--VmlldzozNTM0Nw\r\n\r\n### Suggested Solution\r\n\r\n<!--- Describe your solution here --->\r\nRemove the Cap on Run Comparer :) \r\n\r\n### Benefits\r\nComparing different runs, especially in large projects. \r\n\r\n",
    "comments": [
      {
        "user": "JoanaMarieL",
        "body": "Hi @MoH-assan , thank you for your request, could you also please share how this feature will impact your workflow so we could add it up when we file the Feature Request."
      },
      {
        "user": "MoH-assan",
        "body": "@JoanaMarieL Added Benefits to the feature request "
      },
      {
        "user": "JoanaMarieL",
        "body": "Thank you for the update @MoH-assan , filed it as a Feature Request and will update  you for any progress."
      }
    ]
  },
  {
    "issue_number": 1775,
    "title": "[Feature] Parallel sync",
    "author": "jmuchovej",
    "state": "closed",
    "created_at": "2021-01-30T03:55:42Z",
    "updated_at": "2025-05-12T20:59:04Z",
    "labels": [
      "ty:feature",
      "c:sdk:sync",
      "a:sdk",
      "s:nexus-fix"
    ],
    "body": "**Is your feature request related to a problem? Please describe.**\r\nI've recently had to do my uploads manually (incidentally left `wandb` in offline mode). In attempting to do `wandb sync --sync-all`, the client uploads each run sequentially. \r\n\r\nI'm curious if supporting parallel uploads (at least to network saturation) would be a useful feature to others.\r\n\r\n**Describe the solution you'd like**\r\nParallel uploads – probably some set of options like `--nprocs` and `--saturate`. Maybe this makes sense even within a run, but I imagine that probably stresses the W&B side quite a bit.\r\n\r\n**Describe alternatives you've considered**\r\nMonkeypatching my install to support spinning up processes to handle uploads.\r\n\r\n**Additional context**\r\nN/A\r\n",
    "comments": [
      {
        "user": "cvphelps",
        "body": "Hi there John, this isn't in the near term roadmap but thank you for the suggestion. We'll track this as a potential addition for our next major release."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open 60 days with no activity."
      },
      {
        "user": "DbCrWk",
        "body": "Would it be possible to bump this? @cvphelps\r\n\r\nI unfortunately have to run things in airgapped contexts (i.e. a computing environment without Internet access) and then manually run `wandb sync` to upload the results. It sometimes takes a long time, but I usually am running this on a beefy machine that could do parallel uploads."
      }
    ]
  },
  {
    "issue_number": 9800,
    "title": "[Q]: How to set the y-axis of a line plot to `regex` programatically?",
    "author": "bikcrum",
    "state": "open",
    "created_at": "2025-05-02T05:01:16Z",
    "updated_at": "2025-05-12T20:34:45Z",
    "labels": [
      "ty:feature",
      "a:sdk",
      "c:sdk:define-metric"
    ],
    "body": "### Ask your question\n\nHi, \n\nI am logging numerous plots with keys such as `reward/a_1`, `reward/a_2`, `reward/a_3`, ... so on. I want to group them in a single plot. Right now, I am doing it manually using the WebUI by changing the `Y-axis` to a regex `reward/a_.*`. However, doing it manually is very time-consuming as I have other sets of such keys. I am wondering if I can do this programmatically. \n\nIs it possible to set the `Y-axis` of a line plot to a regex using the API? Specifically, I want to add a new line plot by setting `X` to `steps` and `Y` to `reward/a_.*` and then remove all individual plots.\n\nLet me know if you have a solution to this problem.\n\nThank you!",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hi @bikcrum! Thank you for writing in. Unfortunately, as of right now, our workspace API does not offer this feature. I would be more than happy to submit a feature request for you to see this in our product in the future. "
      },
      {
        "user": "bikcrum",
        "body": "@ArtsiomWB Thank you for submitting a feature request."
      },
      {
        "user": "ArtsiomWB",
        "body": "Quick question! Are you able to do it via the UI instead? We do offer regex matching when creating graphs in the UI. \n\nhttps://github.com/user-attachments/assets/9f576e6e-5722-43e8-910c-bb34c33fae60"
      }
    ]
  },
  {
    "issue_number": 5550,
    "title": "[CLI]: wandb: Network error (ConnectTimeout), entering retry loop",
    "author": "cs-mshah",
    "state": "closed",
    "created_at": "2023-05-15T05:02:31Z",
    "updated_at": "2025-05-12T06:46:50Z",
    "labels": [
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n<!--- Description of the issue below  -->\r\nI get frequent connection issues about Network timeouts from my wandb cli on my DGX server. I don't have issues elsewhere.\r\n\r\n\r\n<!--- A full traceback of the exception in the quotes below -->\r\n```shell\r\nwandb: Network error (ConnectTimeout), entering retry loop.\r\n```\r\n\n\n### Additional Files\n\n_No response_\n\n### Environment\n\nWandB version:0.15.0\r\n\r\nOS: NVIDIA DGX Server Version 5.3.1 (GNU/Linux 5.4.0-113-generic x86_64)\r\n\r\nPython version: Python 3.9.16\r\n\r\nVersions of relevant libraries:\r\nPyTorch\n\n### Additional Context\n\nI have even tried the following: \r\nhttps://docs.wandb.ai/guides/technical-faq/troubleshooting#how-do-i-deal-with-network-issues\r\n`wandb.init(settings=wandb.Settings(start_method=\"fork\"))`",
    "comments": [
      {
        "user": "rsanandres-wandb",
        "body": "Hello @cs-mshah !\r\n\r\nWhen we see `Network error (ConnectTimeout)` Errors, it is normally due to something blocking the connection specifically to `wandb` such as a VPN, Proxy, or a load balancer. However, I can look into the logs and make sure that the error isn't due to something else.\r\n\r\nCould you get the debug logs? The `debug.log` and `debug-internal.log` files can be found in the wandb folder, which is located in the same directory where the script was executed. Inside the wandb folder, you'll find subfolders with names in the format \"run-DATETIME-ID,\" each associated with a single run. Could you gather the debug logs for one of the failed runs and send it to me? "
      },
      {
        "user": "cs-mshah",
        "body": "here is the debug.log:\r\n```\r\n2023-05-12 10:17:03,593 INFO    MainThread:3208098 [wandb_setup.py:_flush():76] Configure stats pid to 3208098\r\n2023-05-12 10:17:03,593 INFO    MainThread:3208098 [wandb_setup.py:_flush():76] Loading settings from /home/dgxadmin/.config/wandb/settings\r\n2023-05-12 10:17:03,593 INFO    MainThread:3208098 [wandb_setup.py:_flush():76] Loading settings from /home/dgxadmin/Manan/CrossDomainGCD/OpenLDN/wandb/settings\r\n2023-05-12 10:17:03,593 INFO    MainThread:3208098 [wandb_setup.py:_flush():76] Loading settings from environment variables: {'api_key': '***REDACTED***', 'mode': 'offline'}\r\n2023-05-12 10:17:03,593 INFO    MainThread:3208098 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}\r\n2023-05-12 10:17:03,593 INFO    MainThread:3208098 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'train.py', 'program': '/home/dgxadmin/Manan/CrossDomainGCD/OpenLDN/train.py'}\r\n2023-05-12 10:17:03,593 INFO    MainThread:3208098 [wandb_init.py:_log_setup():507] Logging user logs to /home/dgxadmin/Manan/CrossDomainGCD/OpenLDN/wandb/offline-run-20230512_101703-wzzbma81/logs/debug.log\r\n2023-05-12 10:17:03,593 INFO    MainThread:3208098 [wandb_init.py:_log_setup():508] Logging internal logs to /home/dgxadmin/Manan/CrossDomainGCD/OpenLDN/wandb/offline-run-20230512_101703-wzzbma81/logs/debug-internal.log\r\n2023-05-12 10:17:03,593 INFO    MainThread:3208098 [wandb_init.py:init():547] calling init triggers\r\n2023-05-12 10:17:03,593 INFO    MainThread:3208098 [wandb_init.py:init():554] wandb.init called with sweep_config: {}\r\nconfig: {'description': 'dann', 'data_root': 'data/pacs', 'run_started': '12-05-23_1017', 'out': 'outputs/dataset_pacs_arch_resnet50_no_novel_3_dann_12-05-23_1017', 'no_progress': False, 'dataset': 'pacs', 'no_class': 7, 'lbl_percent': 100, 'no_novel': 3, 'create_splits': False, 'train_domain': 'photo', 'test_domain': 'art_painting', 'train_split': 0.0, 'arch': 'resnet50', 'pretrained': 'swav_800ep_pretrain.pth.tar', 'epochs': 20, 'batch_size': 32, 'img_size': 224, 'num_workers': 64, 'resume': '', 'seed': 0, 'iteration': 1000, 'optimizer': 'adam', 'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.001, 'scheduler': 'lambda', 'lr_gamma': 0.001, 'lr_decay': 0.75, 'method': 'dann', 'bottleneck_dim': 256, 'temp': 0.07, 'disk_dataset_path': '/home/dgxadmin/datasets', 'tsne': False, 'tsne_freq': 10, 'no_known': 4, 'exp_name': 'dataset_pacs_arch_resnet50_no_novel_3_dann_12-05-23_1017', 'n_gpu': 1}\r\n2023-05-12 10:17:03,593 INFO    MainThread:3208098 [wandb_init.py:init():595] starting backend\r\n2023-05-12 10:17:03,593 INFO    MainThread:3208098 [wandb_init.py:init():599] setting up manager\r\n2023-05-12 10:17:03,598 INFO    MainThread:3208098 [backend.py:_multiprocessing_setup():106] multiprocessing start_methods=fork,spawn,forkserver, using: fork\r\n2023-05-12 10:17:03,601 INFO    MainThread:3208098 [wandb_init.py:init():605] backend started and connected\r\n2023-05-12 10:17:03,609 INFO    MainThread:3208098 [wandb_init.py:init():695] updated telemetry\r\n2023-05-12 10:17:03,699 INFO    MainThread:3208098 [wandb_init.py:init():782] starting run threads in backend\r\n2023-05-12 10:17:08,555 INFO    MainThread:3208098 [wandb_run.py:_console_start():2157] atexit reg\r\n2023-05-12 10:17:08,555 INFO    MainThread:3208098 [wandb_run.py:_redirect():2012] redirect: SettingsConsole.WRAP_RAW\r\n2023-05-12 10:17:08,555 INFO    MainThread:3208098 [wandb_run.py:_redirect():2077] Wrapping output streams.\r\n2023-05-12 10:17:08,555 INFO    MainThread:3208098 [wandb_run.py:_redirect():2102] Redirects installed.\r\n2023-05-12 10:17:08,556 INFO    MainThread:3208098 [wandb_init.py:init():824] run started, returning control to user process\r\n2023-05-12 12:08:28,875 WARNING MsgRouterThr:3208098 [router.py:message_loop():77] message_loop has been closed\r\n```\r\n\r\nhere is the [`debug-internal.log`](https://pastebin.com/TBX00v3z)"
      },
      {
        "user": "cs-mshah",
        "body": "I had to shift the wandb folder to another machine and sync from there using `wandb sync --sync-all`. There were Network errors there too, but it eventually got uploaded after a long wait. Before it used to happen almost immediately."
      }
    ]
  },
  {
    "issue_number": 5113,
    "title": "SVG exported from W&B is not displayed in Overleaf",
    "author": "AntonioLopardo",
    "state": "open",
    "created_at": "2023-03-08T18:50:18Z",
    "updated_at": "2025-05-08T08:51:21Z",
    "labels": [
      "a:app"
    ],
    "body": "### Current Behavior\n\nAs mention in #1446 the SVG plots exported from W&B are not displayed correctly in Overleaf, unlike any other SVG I've tried. The error by Overleaf mentions a missing .pdf_tex file. \r\n![image](https://user-images.githubusercontent.com/8061862/223807150-c5cf3ecb-64aa-469e-8420-59929f7eb460.png)\r\n\r\n\n\n### Expected Behavior\n\nThe plots exported from W&B should be in a format displayable by Overleaf. \n\n### Steps To Reproduce\n\nAs mentioned in the issue #1446, to reproduce this, please try using the overleaf template from [the answer in this thread](https://tex.stackexchange.com/questions/442077/is-it-possible-to-use-svg-images-with-overleaf) and replace the svg with something exported from wandb: ![image](https://user-images.githubusercontent.com/21180505/152408705-a3b47923-ec64-47b4-b240-af43caf91d1f.svg)\n\n### Screenshots\n\n![image](https://user-images.githubusercontent.com/8061862/223806876-dfc8cef3-158b-4ae2-9f83-da264b079b09.png)\r\n\n\n### Environment\n\nOS: MacOs\r\n\r\nBrowsers: Chrome\r\n\r\nVersion:\r\n\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bill Morrisson commented: \nThank you for providing us this information @AntonioLopardo I will be reproducing it on my end and bring it up with the engineering team if it is consistent. Otherwise I will get back to you.\n"
      },
      {
        "user": "AntonioLopardo",
        "body": "Any updates on this, should I assume the problem is reproducible and you are considering a fix?\r\nThanks for any info!"
      },
      {
        "user": "nate-wandb",
        "body": "Hi @AntonioLopardo, I was able to reproduce the issue in this [Overleaf](https://www.overleaf.com/read/zhctqwvkzrfr). I'll pass this onto our engineering team and follow up once they have a chance to look into it.\r\n\r\nThank you,\r\nNate"
      }
    ]
  },
  {
    "issue_number": 9807,
    "title": "[Bug-App]: WANDB_SILENT does not completely silence output",
    "author": "oleg-kachan",
    "state": "closed",
    "created_at": "2025-05-04T23:34:11Z",
    "updated_at": "2025-05-05T21:20:07Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nSetting `WANDB_SILENT=true` does not completely silences output, three lines are outputted instead of seven\n\nIn the case of setting `WANDB_SILENT=true` in the beginning of the script to be run\n```\nimport os\nos.environ[\"WANDB_SILENT\"] = \"true\"\n```\n\nthis three lines are appearing _after_ run is competed\n```\nwandb: \nwandb: 🚀 View run ... at: ...\nwandb: Find logs at: ...\n```\n\nIn the case of `WANDB_SILENT` not set, this seven lines are appearing _before_ the run\n```\nwandb: Currently logged in as: ... to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.19.10\nwandb: Run data is saved locally in ...\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run ...\nwandb: ⭐️ View project at https://wandb.ai/...\nwandb: 🚀 View run at https://wandb.ai/...\n```\n\nSo `WANDB_SILENT=true` just changes the output message and prints it after the run, instead of before it. Is wandb so talkative that it is considered \"wandb silent\"?\n\nCould the default behavior be changed to verbosing the output if required instead of silencing it?",
    "comments": [
      {
        "user": "oleg-kachan",
        "body": "Seems that the undesired lines are generated by func `printFooter` from wandb/core/internal/stream/stream.go file\n\nhttps://github.com/wandb/wandb/blob/e9335ea42ac0258c68fef050079a87ead797b349/core/internal/stream/stream.go#L400-L422"
      }
    ]
  },
  {
    "issue_number": 9806,
    "title": "[Q]: How should I make the selected runs visible?",
    "author": "isCopyman",
    "state": "open",
    "created_at": "2025-05-03T14:20:46Z",
    "updated_at": "2025-05-05T19:52:17Z",
    "labels": [
      "ty:question",
      "a:app"
    ],
    "body": "### Ask your question\n\n![Image](https://github.com/user-attachments/assets/aa3eeb72-df56-438c-96e2-177610e11298)\n\n![Image](https://github.com/user-attachments/assets/7350e533-6ce0-4f9b-8d47-3738b1e45c2c)\n\nIt seems that only the visible runs can be exported. How should I quickly make these selected runs visible? It seems that there is no corresponding option provided.\nIn addition, I think it is very necessary to introduce the shift + click operation.",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bonnie Shen commented: \nHello @isCopyman! Thanks for writing to us! Hope you are having a lovely day!\nCurrently we only have the options to select/deselect all runs and each individual run. I apologize for the inconvenience. But if you can use filters to narrow down the runs of interests, and then select/deselect individual run, it can help speed up the process.\nI will file a feature request on your behalf for the shift + click option. Thank you for your suggestion!\n"
      }
    ]
  },
  {
    "issue_number": 9768,
    "title": "[Q]: Query Panel can't zoom in or zoom out by the wheel of mouse",
    "author": "isCopyman",
    "state": "open",
    "created_at": "2025-04-25T16:13:40Z",
    "updated_at": "2025-05-05T19:50:37Z",
    "labels": [
      "ty:question",
      "a:app"
    ],
    "body": "### Ask your question\n\n<!--- Ask your question here --->\nOnce I zoom in or out with the left mouse button, I don't know how to restore it to the original view before zooming. There are no corresponding buttons or switches. I suggest adding some button tools at a certain position of the graph, just like Plotly does.\n![Image](https://github.com/user-attachments/assets/eb50e52f-0c0b-4cf5-b764-465669930cca)",
    "comments": [
      {
        "user": "isCopyman",
        "body": "This is really annoying.  Is it because I'm using it the wrong way?"
      },
      {
        "user": "FareedFarag",
        "body": "Double click on the figure to reset to original scale. I agree that this should be more obvious in the UI."
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi @isCopyman! Thank you for writing in, and thanks to @FareedFarag for helping out. \n\nThat is correct, double-clicking on the graph should reset the zoom for you. I will go ahead and make a note to our eng team so they can see if they can make it more obvious that the double-click exists. "
      }
    ]
  },
  {
    "issue_number": 9749,
    "title": "[Q]: Batch deletion of logged images",
    "author": "igudav",
    "state": "open",
    "created_at": "2025-04-21T09:48:37Z",
    "updated_at": "2025-05-05T19:50:24Z",
    "labels": [
      "ty:feature",
      "c:sdk:public-api",
      "a:sdk"
    ],
    "body": "### Ask your question\n\nDuring the training process, I log images extensively. I used all the available storage space, and it is full of images. I don't want to delete the entire runs to clean up the storage; instead, I want to delete 90% of the logged images (such as those logged every 10 epochs). I found the following way to do it:\n```python\nhistory = run.scan_history(page_size=10000)\nfor h in history:\n    if h[\"_step\"] % 1000 == 0 and not h[\"_step\"] % 10000 == 0:\n        for obj in h.values():\n            match obj:\n                case {\"_type\": \"image-file\", \"path\": path}:\n                    file = run.file(path)\n                    file.delete()\n```\nThis method deletes files one at a time, which takes an extremely long time to complete. Is there a more efficient way to achieve this? Perhaps using the internal GQL API?  ",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @igudav, Thanks for reaching out!\n\nAt the moment, deleting files in batches isn’t supported via the SDK. While it’s technically possible to do this through the internal GraphQL API, we don’t recommend or officially support that approach, as it can lead to inconsistencies in your runs.\n\nThat said, I’ve logged this as a feature request with our team, and I’ll be sure to keep you updated on any progress."
      }
    ]
  },
  {
    "issue_number": 9767,
    "title": "[Q]: Query panels scatter point size",
    "author": "isCopyman",
    "state": "closed",
    "created_at": "2025-04-25T09:37:56Z",
    "updated_at": "2025-05-05T09:27:31Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\n<!--- Ask your question here --->\nHow should I draw this type of graph? How can I set the number of parameters as the size of the scatter plot points and add different legends? \nhttps://docs.wandb.ai/guides/app/features/panels/query-panels/\n![Image](https://github.com/user-attachments/assets/f406e7f1-16c5-4ba4-b1aa-f2e3ea2d9377)",
    "comments": [
      {
        "user": "isCopyman",
        "body": "How should I associate the size with the value of a certain key?"
      },
      {
        "user": "luisbergua",
        "body": "Hey @isCopyman, thanks for your question! That can be set in the settings of the panel. For example for a table like this:\n\n![Image](https://github.com/user-attachments/assets/e9b42b4a-dce5-4162-bbf2-9c4007641c21)\n\nYou can set the color and size of the point as follows:\n![Image](https://github.com/user-attachments/assets/716daaac-d10c-4bcb-9228-7e3842d4fdc0)\n![Image](https://github.com/user-attachments/assets/97723b0e-1bee-4e80-a838-e4ec0d81e8f4)\n\nPlease let me know if this is helpful!\n\n\n\n "
      },
      {
        "user": "luisbergua",
        "body": "Hi @isCopyman, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!\n"
      }
    ]
  },
  {
    "issue_number": 4755,
    "title": "[Feature]: Add kwarg `vega_spec_json` to  `wandb.plot_table`",
    "author": "janosh",
    "state": "open",
    "created_at": "2023-01-10T01:23:01Z",
    "updated_at": "2025-05-03T03:43:04Z",
    "labels": [
      "ty:feature",
      "c:sdk:custom-charts",
      "a:sdk"
    ],
    "body": "### Description\r\n\r\n`wandb.plot_table` currently takes a `vega_spec_name` argument which must be the name of a plot specification stored in a wandb project.\r\n\r\nhttps://github.com/wandb/wandb/blob/f87c083091ff1d70303536deb8213540038dcc86/wandb/sdk/wandb_run.py#L1891-L1895\r\n\r\nIt would be great to have the plot specification alongside the code. For 3 reasons:\r\n1. to have it under version control\r\n2. to have the spec move with the plot data (currently when migrating the code to a new project that doesn't define the same Vega spec, the plot will be broken:\r\n\r\n   ![](https://weightsandbiases.zendesk.com/attachments/token/AZussjR8DCA4xsrK2ga42VOoW/?name=Screenshot+2023-01-09+at+17.00.44.png)\r\n\r\n3. to allow easier renaming of variables in the code while still matching the spec. Currently I have to remember to login to wandb web UI and update the Vega spec there to match code changes. This is error prone.\r\n\r\n### Suggested Solution\r\n\r\nWould be great if this worked:\r\n\r\n```py\r\nscatter_plot_vega_spec = \r\n{\r\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.json\",\r\n  \"description\": \"Scatter plot with parity line (x=y)\",\r\n  \"data\": {\r\n    \"name\": \"wandb\"\r\n  },\r\n  \"transform\": [],\r\n  \"title\": {\r\n    \"text\": \"${string:title}\",\r\n    \"fontWeight\": \"bold\",\r\n    \"dy\": 25\r\n  },\r\n  \"layer\": [\r\n    {\r\n      \"mark\": {\r\n        \"type\": \"circle\",\r\n        \"tooltip\": true\r\n      },\r\n      \"selection\": {\r\n        \"grid1\": {\r\n          \"type\": \"interval\",\r\n          \"bind\": \"scales\"\r\n        }\r\n      },\r\n      \"encoding\": {\r\n        \"x\": {\r\n          \"field\": \"${field:x}\",\r\n          \"type\": \"quantitative\",\r\n           \"format\": \".3\",\r\n          \"title\": \"${string:x_label}\"\r\n        },\r\n        \"y\": {\r\n          \"field\": \"${field:y}\",\r\n          \"type\": \"quantitative\",\r\n          \"format\": \".3\"\r\n        },\r\n        \"color\": {\r\n          \"type\": \"nominal\",\r\n          \"field\": \"name\",\r\n          \"scale\": {\r\n            \"range\": {\r\n              \"field\": \"color\"\r\n            }\r\n          },\r\n          \"legend\": false\r\n        }\r\n      }\r\n    # ...\r\n    }\r\n  ]\r\n}\r\n\r\n\r\nwandb.plot_table(\r\n    data_table=table,\r\n    fields=fields,\r\n    string_fields=kwargs,\r\n    vega_spec_json=scatter_plot_vega_spec\r\n)\r\n```\r\n\r\n### Alternatives\r\n\r\nMaybe it would make sense to accept an `\"id\"` key in the `vega_spec_json` dict to uniquely idenify this spec so that previous runs that logged a plot with the same `\"id\"` in the `vega_spec_json` will update their plots to use the latest spec. That way you keep the ability to update all plots in a project using a certain spec at once.\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "nate-wandb",
        "body": "Hi @janosh, thank you for the feature request! The main request seems pretty straightforward but I just wanted to clarify my understanding of the Alternative you proposed.\r\n\r\nIf I understand correctly, you would like to be able to embed an `id` key in the json spec. Then if you later update the json but still use the same `id` all of the charts in the UI that use that `id` would render based on the new json spec rather than the one they were originally uploaded with. "
      },
      {
        "user": "janosh",
        "body": "> If I understand correctly, you would like to be able to embed an id key in the json spec. Then if you later update the json but still use the same id all of the charts in the UI that use that id would render based on the new json spec rather than the one they were originally uploaded with.\r\n\r\nExactly!"
      },
      {
        "user": "nate-wandb",
        "body": "Great! I can submit this to the engineering team and follow up once they have a chance to look into it. \r\n\r\nThanks for the feature request!"
      }
    ]
  },
  {
    "issue_number": 6474,
    "title": "[App]: Downloading csv from the web app generates incoherent column names",
    "author": "pablo2909",
    "state": "open",
    "created_at": "2023-10-19T10:43:00Z",
    "updated_at": "2025-05-02T22:11:36Z",
    "labels": [
      "a:app"
    ],
    "body": "### Current Behavior\r\n\r\nI generated a plot in a report from grouping line plots from multiple runs. I download then the csv associated to this plot to work on it. The plot looks like this: \r\n\r\n<img width=\"1644\" alt=\"image\" src=\"https://github.com/wandb/wandb/assets/40873702/3c484f73-b078-49e1-9eff-85bf1b0487b6\">\r\n\r\nWhen I download the csv and extract the column names I get this:\r\n\r\n```bash\r\nIndex(['Training step', 'loss.beta_on_loss: true - _step',\r\n       'loss.beta_on_loss: true - _step__MIN',\r\n       'loss.beta_on_loss: true - _step__MAX',\r\n       'loss.beta_on_loss: true - Variance Loss CVL over Variance Training Loss',\r\n       'loss.beta_on_loss: true - Variance Loss CVL over Variance Training Loss__MIN',\r\n       'loss.beta_on_loss: true - Variance Loss CVL over Variance Training Loss__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:45:31 - _step',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:45:31 - _step__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:45:31 - _step__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:45:31 - Variance Loss CVL over Variance Training Loss',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:45:31 - Variance Loss CVL over Variance Training Loss__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:45:31 - Variance Loss CVL over Variance Training Loss__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:43:43 - _step',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:43:43 - _step__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:43:43 - _step__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:43:43 - Variance Loss CVL over Variance Training Loss',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:43:43 - Variance Loss CVL over Variance Training Loss__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:43:43 - Variance Loss CVL over Variance Training Loss__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:41:11 - _step',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:41:11 - _step__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:41:11 - _step__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:41:11 - Variance Loss CVL over Variance Training Loss',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:41:11 - Variance Loss CVL over Variance Training Loss__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:41:11 - Variance Loss CVL over Variance Training Loss__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:39:29 - _step',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:39:29 - _step__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:39:29 - _step__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:39:29 - Variance Loss CVL over Variance Training Loss',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:39:29 - Variance Loss CVL over Variance Training Loss__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:39:29 - Variance Loss CVL over Variance Training Loss__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:34:26 - _step',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:34:26 - _step__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:34:26 - _step__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:34:26 - Variance Loss CVL over Variance Training Loss',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:34:26 - Variance Loss CVL over Variance Training Loss__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:34:26 - Variance Loss CVL over Variance Training Loss__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:32:29 - _step',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:32:29 - _step__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:32:29 - _step__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:32:29 - Variance Loss CVL over Variance Training Loss',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:32:29 - Variance Loss CVL over Variance Training Loss__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:32:29 - Variance Loss CVL over Variance Training Loss__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:31:11 - _step',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:31:11 - _step__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:31:11 - _step__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:31:11 - Variance Loss CVL over Variance Training Loss',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:31:11 - Variance Loss CVL over Variance Training Loss__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:31:11 - Variance Loss CVL over Variance Training Loss__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:29:51 - _step',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:29:51 - _step__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:29:51 - _step__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:29:51 - Variance Loss CVL over Variance Training Loss',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:29:51 - Variance Loss CVL over Variance Training Loss__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:29:51 - Variance Loss CVL over Variance Training Loss__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:25:19 - _step',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:25:19 - _step__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:25:19 - _step__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:25:19 - Variance Loss CVL over Variance Training Loss',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:25:19 - Variance Loss CVL over Variance Training Loss__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl_2023-10-19 10:25:19 - Variance Loss CVL over Variance Training Loss__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl - _step',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl - _step__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl - _step__MAX',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl - Variance Loss CVL over Variance Training Loss',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl - Variance Loss CVL over Variance Training Loss__MIN',\r\n       'loss.beta_on_loss: 2gaussians_dsm_cvl - Variance Loss CVL over Variance Training Loss__MAX'],\r\n      dtype='object')\r\n```\r\nThe naming is incoherent. I do have a parameter `loss.beta_on_loss` but I don't understand why some columns have `loss.beta_on_loss: true` and others nothing. Note that I am grouping wrt to two parameters `beta_on_loss` and `sigma`. Also in the column names I do recognise the names I gave to my runs and that are grouped, but then a lot of the runs are missing. In total I used 30 runs to create this plot.\r\n\r\n### Expected Behavior\r\n\r\nI expect the column names to be the one of the legends.\r\n\r\n### Steps To Reproduce\r\n\r\n_No response_\r\n\r\n### Screenshots\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nOS:\r\nMac OSX\r\nBrowsers:\r\nSafari & Firefox \r\nVersion:\r\n0.14.0\r\n\r\n\r\n### Additional Context\r\n\r\nThank you for any help",
    "comments": [
      {
        "user": "JoanaMarieL",
        "body": "Hello @pablo2909 ,\r\n\r\nI am glad to assist your concern, may I please request for the actual link of your project workspace please?"
      },
      {
        "user": "pablo2909",
        "body": "Hi, thank you for your help, please find it [here](https://wandb.ai/pablo2909/SimpleScoreBased?workspace=user-pablo2909)"
      },
      {
        "user": "JoanaMarieL",
        "body": "Thank you @pablo2909 for sharing your project workspace, we'll check this out and get back to you."
      }
    ]
  },
  {
    "issue_number": 9766,
    "title": "[Q]: Comparing run across different projects.",
    "author": "bikcrum",
    "state": "closed",
    "created_at": "2025-04-25T04:05:18Z",
    "updated_at": "2025-05-01T18:14:19Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\n<!--- Ask your question here --->\n\nHi,\n\nI would like to compare runs from two different projects in Wandb Web UI. At the moment, I am achieving this by moving a run from its original project into another project where I already have a reference run. \n\nIdeally, I would like to either be able to compare runs across projects directly or have the option to duplicate a run so that it exists in both the original and target projects without affecting the original.\n\nIs there a way to achieve this currently, or is support for this kind of workflow being considered?\n\nThanks!",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hi @bikcrum! Thank you for writing in!\n\nAt the moment, we do not have the option of cloning runs across projects, but I think that would be an amazing feature request I can submit to our engineering team. \n\nCurrently, the suggested workaround we have for customers is [by using W&B Reports](https://www.youtube.com/watch?v=uD4if_nGrs4). You should be able to add run sets from 2 different projects into a single report and have the charts overlay over one another. "
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved. "
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi, since we have not heard back from you, we are going to close this request. If you would like to reopen the conversation, please let us know! "
      }
    ]
  },
  {
    "issue_number": 2366,
    "title": "[Feature]  Remove columns from the runs table",
    "author": "chris-tkinter",
    "state": "open",
    "created_at": "2021-07-06T18:14:21Z",
    "updated_at": "2025-05-01T07:40:00Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "**Is your feature request related to a problem? Please describe.**\r\nI have been using w&b for tracking my argparse args. However I often found that the one arg flag should be renamed or should be removed that completely. However the old arg flag still persists in the w&b ui.  \r\nFor example, I think I no longer need the `folder_name` flag and would like to remove it from the w&b run tables as well. But currently I don't think there is a method to do that other than just mark this column invisible. \r\n![image](https://user-images.githubusercontent.com/86935669/124647433-d0629580-de4a-11eb-88e0-1fe9b7d55d97.png)\r\nI think these unwanted flags will pile up and make browsing the config args cumbersome in that people would see some flags unused in the code. Also if the programmer mistyped a flag, it would remains in the run tables. Then if we do a filter by that flag we could mistakenly filter by a mistyped flag.\r\nThanks!",
    "comments": [
      {
        "user": "tyomhak",
        "body": "Hey @chris-tkinter , thank you for the feature request :)"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open 60 days with no activity."
      },
      {
        "user": "weiyaw",
        "body": "Is there an update on this request?"
      }
    ]
  },
  {
    "issue_number": 6481,
    "title": "[Bug]: Logging Caption in object 3d + silently dropping points when number of points are large",
    "author": "ayushjain1144",
    "state": "closed",
    "created_at": "2023-10-20T22:25:30Z",
    "updated_at": "2025-04-30T21:50:17Z",
    "labels": [
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Describe the bug\r\n\r\n<!--- Description of the issue below  -->\r\nThere are a couple of bugs: \r\na) Caption logging is not working with Object3D\r\nb) i notice that when the number of points we are trying to log grows a lot, wandb just drops a lot of points silently. I struggled a lot with this and thought my data is buggy but when I subsampled my points, I was able to see everything in wandb. I am unable to share a working example, but I think if you spawn a lot of points in a big grid size (maybe 30X30X30), it will start showing the bug.\r\n\r\nSnippet to reproduce the captioning bug\r\n<!--- A minimal code snippet between the quotes below  -->\r\n```python\r\n wandb.log({\r\n        f'{prefix}unprojection': wandb.Object3D(our_pc, caption=f'Our PC {scene_name} {percent_close}'),\r\n    })\r\n```\r\n\r\n<!--- A full traceback of the exception in the quotes below -->\r\n```shell\r\nNo exception, just fails to log captions. \r\n```\r\n\r\n\r\n### Additional Files\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nWandB version: 0.13.10\r\n\r\nOS: linux\r\n\r\nPython version: 3.8.13\r\n\r\nVersions of relevant libraries:\r\n\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "umakrishnaswamy",
        "body": "hey @ayushjain1144 - looking into this right now. Also, I noticed you're on version 0.13.10 of our SDK - do you experience these same issues upon upgrading to our latest SDK version (0.15.12)?"
      },
      {
        "user": "ayushjain1144",
        "body": "Hi @umakrishnaswamy, sorry for the late reply. I checked with the latest version, and the issue is still there"
      },
      {
        "user": "umakrishnaswamy",
        "body": "@ayushjain1144 - hey ayush, I was able to reproduce this behavior (the first bug) on our latest sdk version (0.15.12). I will escalate this accordingly and let you know of any progress that arises. \r\n\r\nas for the second, if you have any sort of code snippet that I could reference and use for a reproduction, that would be super helpful! "
      }
    ]
  },
  {
    "issue_number": 8634,
    "title": "init times out in 90 seconds, bumping up to 120 seconds does not help either",
    "author": "shashank2000",
    "state": "open",
    "created_at": "2024-10-16T16:36:05Z",
    "updated_at": "2025-04-30T15:27:10Z",
    "labels": [
      "a:sdk",
      "c:sdk:init"
    ],
    "body": "> Hmm so I still see this same issue @kptkin - I have `accelerate` version `1.0.1` and `wandb` version `0.18.3`. Even running `wandb.init()` leads to the same issue (`wandb.errors.errors.CommError: Run initialization has timed out after 90.0 sec`.)\n> \n> Are there any other leads here?  \n\n _Originally posted by @shashank2000 in [#7671](https://github.com/wandb/wandb/issues/7671#issuecomment-2415367198)_",
    "comments": [
      {
        "user": "shashank2000",
        "body": "reverting to version 0.17.8 works"
      },
      {
        "user": "kptkin",
        "body": "@shashank2000 do you think you could provide a small reproduction of how you use `accelerate` with `wandb` so we could further investigate this issue?\n\nalso if you could share the logs under your run folder, that would be helpful as well.\n\nThanks in advance!"
      },
      {
        "user": "overtheskyy",
        "body": "@shashank2000 I'm in a kaggle environment and I had this error, tried reverting to version 0.17.8 and it didn't work for me. Do you have an idea why?\n\nThanks! "
      }
    ]
  },
  {
    "issue_number": 9777,
    "title": "[Bug-App]: Run grouping broken, metric displayed with rounding error, runs under broken groups don't display",
    "author": "tristanengst",
    "state": "open",
    "created_at": "2025-04-28T07:12:29Z",
    "updated_at": "2025-04-30T13:51:32Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\nWhen using the Web UI to group runs, sometimes a numerical value I group by (in my case, learning rate with the `lr` config key) is not displayed (or maybe, under the hood, computed?) properly. In the attached image, it seems like it's being rounded incorrectly.\n\nWhen I click the arrow to display the runs grouped underneath, the arrow's orientation switches from horizontal to vertical, but no runs are displayed.\n\nSo far, I've observed this only with my `lr` config key, and when its value is `0.003`, as in the attached image. The detected number of runs (three) under both of the buggy groupings is correct.\n\n<img width=\"459\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/18be3b45-58f0-44d4-a4aa-d9701af55ced\" />\n\nI've also confirmed that this error doesn't always happen, and I haven't figured out anything that predicts when it does or does not.\n\n---\nBrowser version: Brave 1.77.101\nOS version: macOS 15.3.2\nI'm not using my own WandB server",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @tristanengst, Could you confirm whether you’re grouping by the lr config parameter or the lr metric?\nAlso, if possible, could you share an example workspace where you’re seeing this issue? On my end, I’m only seeing the empty dropdown for groups where the runs have logged lr as null."
      },
      {
        "user": "tristanengst",
        "body": "Hi @aajais ! **(1)** I am grouping by the `lr` config parameter, not the metric. **(2)** Is there a way I can share the workspace privately?\n\nI'm fairly sure it's not that the `lr` config parameter is null—if I change the grouping so the runs display and have `lr` as a column name, then it displays correctly as `0.003`:\n\n![Image](https://github.com/user-attachments/assets/8028ac85-f556-40b2-b091-ade50e9fc09e)"
      },
      {
        "user": "aajais",
        "body": "Sure, could you write in to support@wandb.com and I’ll take a look? Before you do, it’d be super helpful if you could reproduce the issue, then save it as a new view and share the link to that view. You can find the *\"Save as new view\"* button in the top right corner of your workspace (screenshot below for reference). Thanks!\n\n<img width=\"1510\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d45e0727-996e-439c-b385-c32d624b909c\" />"
      }
    ]
  },
  {
    "issue_number": 9449,
    "title": "[Feature]: Add option to translate warning `Tried to log to step %d that is less than the current step` to an error",
    "author": "EricCousineau-TRI",
    "state": "open",
    "created_at": "2025-02-11T19:31:54Z",
    "updated_at": "2025-04-30T09:47:18Z",
    "labels": [
      "ty:feature",
      "a:sdk",
      "c:sdk:ux"
    ],
    "body": "### Description\n\nWe introduced a bug that logged one step into the future, and ended up losing quite a bit of useful logging data, e.g.\n```\nwandb: WARNING Tried to log to step 3 that is less than the current step 4. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n```\nwhich happens here:\nhttps://github.com/wandb/wandb/blob/9153d0ecfa5e82a706618242a567500e7fe68e25/core/internal/stream/handler.go#L1009-L1016\n\n### Suggested Solution\n\nIs there a way to make configure this into a fatal error, such that we fail fast rather than needing to monitor stdout?\nOr is there a way to \"listen\" to the log messages for WandB so we can fail fast ourselves?",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hey @EricCousineau-TRI, thanks for flagging this! This is currently not possible, but I'll ensure to raise a feature request with our Product Team, and will let you know if there's any progress on it "
      },
      {
        "user": "skpig",
        "body": "I'm not sure if this is a reasonable proposal. On some preemptible training clusters, training jobs may be interrupted multiple times and forced to restart from earlier checkpoints. In such cases, there could be some repeated training during the early steps after a restart. This might require `wandb.log()` to log a few data for some previously seen steps again (even though we don't need to).\n"
      }
    ]
  },
  {
    "issue_number": 3424,
    "title": "[Feature]: support true deletion of run / history",
    "author": "adefossez",
    "state": "open",
    "created_at": "2022-03-24T17:11:43Z",
    "updated_at": "2025-04-29T23:11:49Z",
    "labels": [
      "ty:feature",
      "a:sdk",
      "c:sdk:resume"
    ],
    "body": "### Description\r\n\r\nI am the author of Dora (https://github.com/facebookresearch/dora), a framework for experiment and grid search management. A key notion in Dora is that experiments are uniquely identified by a hash of their hyper parameters (more precisely, the hyper parameters that differ from a base config). This means for instance that when scheduling twice the same grid search, Dora can automatically find that a given set of hyper-parameters has already been scheduled, and avoid entirely duplicated runs issues. This hash is also a very convenient way of identifying an experiment, that will be constant across users and time.\r\n\r\nI would like to use the Dora hash signature as an id in wandb, as this makes a number of things more convenient (like knowing easily the URL for a given experiment). However, it is currently not possible because there is no way to truely erase a run or its history from WanDB. In particular, if I run an experiment with a given Dora signature, and then wipe out its checkpoint (for instance there was a bug in the implementation), then I cannot reuse the same id with wandb. Setting `resume = False` won't solve the issue, as this doesn't wipe out the history, only seem to overwrite gradually which is not really nice.\r\n\r\nOf course I could use a different WanDB id each time, but I thought in my case it it much nicer to have the possibility to use the Dora signature hash, which saves me the trouble of having to save anywhere the mapping with a WanDb id. I also tried deleting the run with `run.delete()` but this is not a true delete as it doesn't really free the id for later reuse.\r\n\r\n### Suggested Solution\r\n\r\nGive the freedom for your users to really delete entries. Give the freedom to your users to rewrite history as they see fit. I get it that you want to enforce some safety for the majority of the people, but for some specific use case and integration with other tools, allowing for more control over what is stored would be awesome.\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "ramit-wandb",
        "body": "Hi @adefossez,\r\n\r\nThank you for your suggestion! I have made a request for our engineering team to review this. I'll keep you updated on the status of this feature.\r\n\r\nThanks,\r\nRamit"
      },
      {
        "user": "vanpelt",
        "body": "Hey @adefossez something definitely on our roadmap.  Unfortunately it's very complicated because data is keyed off of this ID in many other datastores like the history of metrics and the files in object stores.  The ultimate solution will likely still have a delay to support \"unarchiving\".  Do you think it would be possible to add a single additional byte to the id to account for this like \"version\" or something equivalent that could be incremented?"
      },
      {
        "user": "JacobARose",
        "body": "> Hey @adefossez something definitely on our roadmap. Unfortunately it's very complicated because data is keyed off of this ID in many other datastores like the history of metrics and the files in object stores. The ultimate solution will likely still have a delay to support \"unarchiving\". Do you think it would be possible to add a single additional byte to the id to account for this like \"version\" or something equivalent that could be incremented?\r\n\r\nI believe if wandb offered an intuitive way to automatically increment an experiment's version if it detects it's been run before, that would go a long way toward reducing the overall bloat/ambiguity that comes from an evolving code base being used to execute complex experiments. \r\n\r\nA likely related but orthogonal problem: I've frequently had to delete entire projects b/c of earlier runs that used slight variations of the name of a logged metric. Such as changing \"train/F1_epoch\" to \"train_F1_epoch\", both will take up space in the project table forever."
      }
    ]
  },
  {
    "issue_number": 1856,
    "title": "[Feature] Wandb sweeper for hydra",
    "author": "ashleve",
    "state": "closed",
    "created_at": "2021-02-17T17:01:48Z",
    "updated_at": "2025-04-29T20:40:38Z",
    "labels": [
      "ty:feature",
      "c:sweeps"
    ],
    "body": "**Is your feature request related to a problem? Please describe.**\r\nCurrently wandb sweeps can be used alongside [Hydra](https://hydra.cc) but it's not very convenient.\r\nMost of the times I would like to overwrite only certain parameters in hydra config, by downloading them from wandb sweep server. This is possible, but requires me to manage the overriding logic by myself.\r\n\r\n**Describe the solution you'd like**\r\nIt would be great if wandb provided a custom sweeper plugin for hydra, similar to the one that's available there for optuna: https://hydra.cc/docs/next/plugins/optuna_sweeper\r\nThis way doing sweeps alongside hydra could be as easy as:\r\n```\r\n# initialize sweep and automatically override some of the config parameters\r\npython train.py --multirun hparams_search=wandb_sweep.yaml \r\n```\r\n\r\n**Additional context**\r\nI think supporting sweeps in hydra has been mentioned here recently: https://github.com/wandb/client/issues/1233\r\n",
    "comments": [
      {
        "user": "ariG23498",
        "body": "Hey @hobogalaxy \r\nThanks for the feature request\r\nWe will look into this!"
      },
      {
        "user": "MohammedAljahdali",
        "body": "hey @hobogalaxy, I am currently trying to use hydra sweeps with wandb, but I am facing an issue where when a new run in the sweep starts and the id of the wandb run changes (I do that using the hydra:job.num) , and a new wandb object gets created with this new id, the version attribute  of wandb get set  to the same id as the first run in the sweep and does not change. Thus making it impossible to track separate runs of the sweep in wandb. Wondering if you have some ideas on this problem."
      },
      {
        "user": "ashleve",
        "body": "Hi @MohammedAljahdali, I don't really see a reason for setting id of the wandb run to hydra job number? Why not just stay with random ids? \r\nIf you want to keep it organized, I usually just come up with different name for each hydra sweep and set it to wandb \"group\" parameter (so it's easier filter it in UI). \r\nSomething like: \r\n`python train.py  +experiment=exp_name --multirun model.lr=0.01,0.005 logger.wandb.group=\"mnist_conv_net_sweep1\"`"
      }
    ]
  },
  {
    "issue_number": 9306,
    "title": "[Bug-App]:  \"Error Importing Optional Module moviepy.editor\" when using Wandb with Ray Tune",
    "author": "Finebouche",
    "state": "closed",
    "created_at": "2025-01-21T14:46:21Z",
    "updated_at": "2025-04-28T22:35:53Z",
    "labels": [
      "ty:bug",
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n**Description:**\nI'm encountering an issue where `wandb` fails to import the optional module `moviepy.editor` when running in a `MultiAgentEnvRunner` with RAY Tune. The error occurs when `wandb.util.get_module` tries to lazily import `moviepy.editor`, but it results in a `ModuleNotFoundError`.\n\n**The following error appears:**\n\n```\nError importing optional module moviepy.editor\nTraceback (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 215, in import_module_lazy\n    return sys.modules[name]\n           ~~~~~~~~~~~^^^^^^\nKeyError: 'moviepy.editor'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 244, in get_module\n    return import_module_lazy(name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 219, in import_module_lazy\n    raise ModuleNotFoundError\nModuleNotFoundError\n```\n\n**Environment:**\n- `wandb` version: 1.9.4\n- Python version: 3.11\n- OS: macOS (Miniforge environment)\n- Installed via: Conda environment\n\n\n\n",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hi @Finebouche, thanks for flagging this! Would you mind sharing a minimal reproduction code, and your `ray --version` so we can reproduce this on our end? "
      },
      {
        "user": "luisbergua",
        "body": "Hi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved.\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Aayush Jaiswal commented: \nHi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved.\n"
      }
    ]
  },
  {
    "issue_number": 7269,
    "title": "[Q] argument `fps` to `wandb.Video` does not change speed of created video",
    "author": "liubaoryol",
    "state": "closed",
    "created_at": "2024-03-30T19:41:49Z",
    "updated_at": "2025-04-28T22:34:28Z",
    "labels": [
      "ty:bug",
      "c:sdk:media",
      "a:sdk",
      "a:app"
    ],
    "body": "I am creating a video from a numpy array, but the speed is too fast and I am not able to properly see it and understand it.\r\n\r\nLet's take the example code from the official documentation to reproduce the error:\r\n\r\n```\r\nimport numpy as np\r\nimport wandb\r\n\r\nwandb.init()\r\n# axes are (time, channel, height, width)\r\nframes = np.random.randint(low=0, high=256, size=(10, 3, 100, 100), dtype=np.uint8)\r\nwandb.log({\"video\": wandb.Video(frames, fps=4)})\r\n```\r\nI want the change between frames to be slower, so I changed `fps=4` to `fps=1` and it is still fast. I also changed `fps=10000` and see no difference at all.\r\nNOTE: the resulting video of wandb that I am referencing to is a gif on my local. I am judging not by the wandb dashboard, as it seems to have internal errors itself so I'm not able to visualize it there yet (hopefully this will be solved soon)",
    "comments": [
      {
        "user": "MBakirWB",
        "body": "Hi @liubaoryol, we appreciate you flagging this. We were able to confirm that this is a bug on our end. We will provide an update once the issue is resolved. "
      },
      {
        "user": "Beanpow",
        "body": "I think this is a bug of `moviepy==1.0.3`. Please refer to https://github.com/Zulko/moviepy/issues/2170."
      },
      {
        "user": "thanos-wandb",
        "body": "Hi @liubaoryol thank you for reporting this issue. Just a quick update this should be resolved with this PR: https://github.com/wandb/wandb/pull/8585 therefore marking this ticket as resolved for now. Please let us know if you are still encountering any issue and we will be more than happy to keep investigating."
      }
    ]
  },
  {
    "issue_number": 6297,
    "title": "[CLI]: wandb.Image breaks plt.imshow",
    "author": "csparker247",
    "state": "closed",
    "created_at": "2023-09-15T14:46:04Z",
    "updated_at": "2025-04-28T22:34:12Z",
    "labels": [
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Describe the bug\r\n\r\n<!--- Description of the issue below  -->\r\nI have a script where I log an image using `wandb.Image` then optionally use matplotlib to plot points on top of the same image. Since point plotting is optional, I import matplotlib inside my point plotting method. In practice, this looks like something like this:\r\n```python\r\ndef plot_points(img, pts, step):\r\n  import matplotlib.pyplot as plt\r\n  fig, ax = plt.subplots()\r\n  plt.imshow(img)\r\n  plt.scatter(pts[..., 0], pts[..., 1], marker='.')\r\n  wandb.log({'Plot': plt}, step=step)\r\n\r\nwimg = wandb.Image(img, caption='Image')\r\nwandb.log({'Image': wimg}, step=step)\r\nif log_pts:\r\n  plot_points(img, pts, step)\r\n```\r\nWhen I run this code on my laptop, this works just fine. However, when I run it on my university's computing cluster, I get `AttributeError: module 'PIL' has no attribute 'Image'` at the `plt.imshow` call. \r\n\r\nI have no idea why this doesn't fail on all of my machines, but I've tracked the issue down to the import order for `matplotlib.pyplot`. I have no issues if I import `pyplot` before using `wandb.Image`, but I get the `AttributeError` if I import it afterwards. The following is a minimal code example which reproduces the issue:\r\n\r\n<!--- A minimal code snippet between the quotes below  -->\r\n```python\r\nimport numpy as np\r\nimport wandb\r\n\r\n# import before calling wandb.Image and all is fine \r\n# import matplotlib.pyplot as plt\r\n\r\nwandb.init()\r\n\r\nstep=0\r\nimg = np.zeros((768, 768))\r\n\r\nwimg = wandb.Image(img)\r\nwandb.log({'Plot': wimg}, step=step)\r\n\r\nimport matplotlib.pyplot as plt\r\nfig, ax = plt.subplots()\r\nplt.imshow(img, cmap='gray')\r\nwandb.log({'Image Plot': plt}, step=step)\r\n```\r\n\r\n<!--- A full traceback of the exception in the quotes below -->\r\n```shell\r\n# paths redacted\r\nTraceback (most recent call last):\r\n  File \"/home/.../minimal.py\", line 18, in <module>\r\n    plt.imshow(img, cmap='gray')\r\n  File \"/usr/local/.../.venv/lib/python3.10/site-packages/matplotlib/pyplot.py\", line 3346, in imshow\r\n    __ret = gca().imshow(\r\n  File \"/usr/local/.../.venv/lib/python3.10/site-packages/matplotlib/__init__.py\", line 1465, in inner\r\n    return func(ax, *map(sanitize_sequence, args), **kwargs)\r\n  File \"/usr/local/.../.venv/lib/python3.10/site-packages/matplotlib/axes/_axes.py\", line 5751, in imshow\r\n    im.set_data(X)\r\n  File \"/usr/local/.../.venv/lib/python3.10/site-packages/matplotlib/image.py\", line 721, in set_data\r\n    if isinstance(A, PIL.Image.Image):\r\nAttributeError: module 'PIL' has no attribute 'Image'\r\n```\r\n\r\nThe obvious workaround is to just import `pyplot` at the start of my script, and I'll probably do that now, but I ultimately want to avoid this since point plotting is optional functionality. \r\n\r\n### Additional Files\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nWandB version: 0.15.10\r\n\r\nOS: Ubuntu 22.04 (actually the Docker image nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04)\r\n\r\nPython version: 3.10.12\r\n\r\nVersions of relevant libraries:\r\n- matplotlib 3.8.0\r\n- numpy 1.25.2\r\n- Pillow 10.0.0\r\n\r\n\r\nI do not have the issue on my laptop with the following configuration:\r\nWandB version: 0.15.10\r\nOS: macOS Ventura 13.5.1 (22G90)\r\nPython version: 3.10.13 (Homebrew)\r\nmatplotlib 3.8.0\r\nPillow 10.0.0\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "csparker247",
        "body": "Looking into this a little bit, I suspect that the issue here might be the lazy loading of `PIL.Image`:\r\n\r\nhttps://github.com/wandb/wandb/blob/00aed12f84d031e59c98691eef8f69d05e1a7e59/wandb/sdk/data_types/image.py#L274-L277"
      },
      {
        "user": "rsanandres-wandb",
        "body": "Hello @csparker247 !\r\n\r\nThank you for providing the detailed bug report! Since this seems to transient in nature, I was also unable to replicate this issue so would you be able to provide the `debug.log` and `debug-internal.log` for the SDK bug report I will be creating?\r\n\r\nThey should be located in the wandb folder in the same directory as where the script was run. The wandb folder has folders formatted as run-DATETIME-ID associated with a single run. Could you retrieve the debug.log and debug-internal.log files from one of these folders specifically from the run that is having issues? "
      },
      {
        "user": "csparker247",
        "body": "Hi @rsanandres-wandb, \r\n\r\nHere you go! These are from a new run I just performed using the minimal example above. Note that I went through and redacted some of the sensitive paths, usernames, etc.\r\n\r\n[debug-internal.log](https://github.com/wandb/wandb/files/12690686/debug-internal.log)\r\n[debug.log](https://github.com/wandb/wandb/files/12690687/debug.log)\r\n"
      }
    ]
  },
  {
    "issue_number": 6107,
    "title": "[App]: Initially logged image segmentation masks disappear after experiment is being resumed",
    "author": "pete-machine",
    "state": "closed",
    "created_at": "2023-08-18T07:40:47Z",
    "updated_at": "2025-04-28T22:33:57Z",
    "labels": [
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Current Behavior\r\n\r\nInitially logged image segmentation masks disappear in the web app, when the same experiment run is being resumed.\r\n\r\n### Expected Behavior\r\n\r\nI expect segmentation masks to remain when the experiment is being resumed. \r\n\r\n### Steps To Reproduce\r\n\r\nRun below code to reproduce the issue. Requires numpy and pillow installed. \r\n\r\n```\r\nimport numpy as np\r\nimport wandb\r\n\r\ndef create_test_wandb_image():\r\n    image = np.zeros((100, 200, 3), dtype=np.uint8)\r\n    mask = np.zeros((100, 200), dtype=np.uint8)\r\n    mask[:20, :20] = 1\r\n    mask[40:60, 40:60] = 3\r\n    mask[20:40, 20:40] = 2\r\n    class_labels = {idx: f\"C{idx}\" for idx in range(mask.max())}\r\n    wandb_image = wandb.Image(image, masks={\"ground_truth\": {\"mask_data\": mask, \"class_labels\": class_labels}})\r\n    return wandb_image\r\n\r\nproject = \"simple-resume-with-id\"\r\nrun_wandb = wandb.init(project=project)\r\nrun_wandb.log({f\"main_script_value0\": 1.0})\r\nrun_wandb.log({f\"image0\": create_test_wandb_image()})\r\nrun_wandb.finish()\r\n\r\n# run_wandb = wandb.init(resume=\"must\", id=run_id)  # This is not working - maybe this could be stated in the docs.\r\nrun_wandb = wandb.init(resume=\"must\", id=run_wandb.id, project=project)\r\nrun_wandb.log({f\"main_script_value2\": 1.0})\r\nrun_wandb.log({f\"image2\": create_test_wandb_image()})\r\nrun_wandb.finish()\r\n```\r\n\r\n### Screenshots\r\n\r\nVisualization of image0 and image1 in wandb app. \r\nI log the same image as `image0` and `image1`, but the segmentation masks of `image0` are not visualized. \r\n![Screenshot from 2023-08-18 09-26-49](https://github.com/wandb/wandb/assets/9336834/74c6dabc-bf9b-409b-a6b1-bcfb830f810b)\r\n\r\n\r\n### Environment\r\n\r\nOS: Linux\r\n\r\nBrowsers: Chrome\r\n\r\nVersion: wandb=0.15.8\r\n\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "umakrishnaswamy",
        "body": "Hey @pete-machine, I was able to reproduce this behavior on my end (thank you for providing the reproducible code!); this looks like a bug and I will escalate appropriately and update you when any progress arises on this. Thank you!"
      },
      {
        "user": "pete-machine",
        "body": "Thank you! I should also mention that the same issue is true for bounding boxes."
      },
      {
        "user": "jackbodine",
        "body": "I've just encountered this bug as well. Also on Linux (Ubuntu) but with wandb version 0.19.6. Just wanted to chime in and say it appears to still be there. \n\nAlso if anyone stumbles upon this thread and has managed to find a workaround that would be nice <3"
      }
    ]
  },
  {
    "issue_number": 3982,
    "title": "[CLI]: Logging bounding boxes only works with float dimensions, not int",
    "author": "johko",
    "state": "closed",
    "created_at": "2022-07-25T19:26:17Z",
    "updated_at": "2025-04-28T22:32:49Z",
    "labels": [
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Describe the bug\r\n\r\n<!--- Description of the issue below  -->\r\nWhen logging bounding boxes for an image and using `int` values as width and height, the library throws an Error:\r\n`TypeError: Object of type int is not JSON serializable`\r\nWhen casting the values to `float` it works without a problem.\r\n<!--- A minimal code snippet between the quotes below  -->\r\n``` python\r\nframe = cv2.imread(\"haar_cascades/data/9_Press_Conference_Press_Conference_9_86.jpg\")\r\nframe = imutils.resize(frame, width=500)\r\ngray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n\r\nfaceRects = face_detector.detectMultiScale(\r\n    gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30),\r\n    flags=cv2.CASCADE_SCALE_IMAGE)\r\n\r\nbox_data = []\r\n\r\nclass_labels = {\r\n    0: \"face\"\r\n}\r\n\r\nfor (x,y,w,h) in faceRects:\r\n\r\n    midX = int(x+w/2)\r\n    midY = int(y+h/2) \r\n    box = {\r\n                \"position\": {\r\n                    \"middle\": [midX, midY],\r\n                    \"width\": w,\r\n                    \"height\": h\r\n                },\r\n                \"domain\" : \"pixel\",\r\n                \"class_id\" : 0\r\n            }\r\n    box_data.append(box)\r\n\r\npredictions = {\"predictions\": {\r\n        \"box_data\": box_data,\r\n        \"class_labels\": class_labels\r\n    }\r\n    }\r\n\r\nimg = wandb.Image(frame, boxes=predictions)\r\n```\r\nHere is the example in a colab: https://colab.research.google.com/drive/1dyrgGANhitXN8RND8nSDItDvX6OE4-FF?usp=sharing\r\n\r\n<!--- A full traceback of the exception in the quotes below -->\r\n```shell\r\nTypeError                                 Traceback (most recent call last)\r\n/home/johannes/Projects/blog/haar/haar_cascade.ipynb Cell 5' in <cell line: 38>()\r\n     [31](vscode-notebook-cell:/home/johannes/Projects/blog/haar/haar_cascade.ipynb#ch0000004?line=30) predictions = {\"predictions\": {\r\n     [32](vscode-notebook-cell:/home/johannes/Projects/blog/haar/haar_cascade.ipynb#ch0000004?line=31)         \"box_data\": box_data,\r\n     [33](vscode-notebook-cell:/home/johannes/Projects/blog/haar/haar_cascade.ipynb#ch0000004?line=32)         \"class_labels\": class_labels\r\n     [34](vscode-notebook-cell:/home/johannes/Projects/blog/haar/haar_cascade.ipynb#ch0000004?line=33)     }\r\n     [35](vscode-notebook-cell:/home/johannes/Projects/blog/haar/haar_cascade.ipynb#ch0000004?line=34)     }\r\n     [37](vscode-notebook-cell:/home/johannes/Projects/blog/haar/haar_cascade.ipynb#ch0000004?line=36) re_im =cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n---> [38](vscode-notebook-cell:/home/johannes/Projects/blog/haar/haar_cascade.ipynb#ch0000004?line=37) img = wandb.Image(frame, boxes=predictions)\r\n     [40](vscode-notebook-cell:/home/johannes/Projects/blog/haar/haar_cascade.ipynb#ch0000004?line=39) wandb.log({\"obama_pc\": img})\r\n\r\nFile ~/.local/share/virtualenvs/haar-tXjf4l0j/lib/python3.9/site-packages/wandb/sdk/data_types/image.py:147, in Image.__init__(self, data_or_path, mode, caption, grouping, classes, boxes, masks)\r\n    144 else:\r\n    145     self._initialize_from_data(data_or_path, mode)\r\n--> 147 self._set_initialization_meta(grouping, caption, classes, boxes, masks)\r\n\r\nFile ~/.local/share/virtualenvs/haar-tXjf4l0j/lib/python3.9/site-packages/wandb/sdk/data_types/image.py:175, in Image._set_initialization_meta(self, grouping, caption, classes, boxes, masks)\r\n    172         boxes_final[key] = box_item\r\n    173     elif isinstance(box_item, dict):\r\n    174         # TODO: Consider injecting top-level classes if user-provided is empty\r\n--> 175         boxes_final[key] = BoundingBoxes2D(box_item, key)\r\n    176     total_classes.update(boxes_final[key]._class_labels)\r\n    177 self._boxes = boxes_final\r\n...\r\n    178     \"\"\"\r\n--> 179     raise TypeError(f'Object of type {o.__class__.__name__} '\r\n    180                     f'is not JSON serializable')\r\n\r\nTypeError: Object of type int is not JSON serializable\r\n```\r\n\r\n\r\n### Additional Files\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nWandB version: 0.12.18\r\n\r\nOS: Ubuntu 18.04\r\n\r\nPython version: 3.9\r\n\r\nVersions of relevant libraries: opencv-python==4.6.0.66\r\n\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "lesliewandb",
        "body": "Hi @johko! Thank you for bringing this to our attention :) This is a known bug that we have and we have a ticket out currently to fix this. I'll increase the priority we have on it and let you know once this has been fixed"
      },
      {
        "user": "johko",
        "body": "Thank you @lesliewandb :) I looked for a similar issue but couldn't find it. Good to know that you are on it."
      },
      {
        "user": "saydemr",
        "body": "Any updates on this one?"
      }
    ]
  },
  {
    "issue_number": 3656,
    "title": "[CLI]: Logging media with leading / as key leads to permission denied",
    "author": "sebimarkgraf",
    "state": "closed",
    "created_at": "2022-05-12T13:50:11Z",
    "updated_at": "2025-04-28T22:32:09Z",
    "labels": [
      "ty:bug",
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n<!--- Description of the issue below  -->\r\n\r\n<!--- A minimal code snippet between the quotes below  -->\r\n```python \r\nimport wandb\r\nwandb.init(...)\r\n\r\nwandb.log({\"/test_image\": wandb.Image(...)})\r\n```\r\n\r\n<!--- A full traceback of the exception in the quotes below -->\r\n```shell\r\nPermission Error: [Errno 13] Permission Denied: '/test_image_0_1823490.png'\r\n```\r\n\r\nThe issue here is the joining of the key together with the path in `wandb/sdk/data_types/base_types/media.py`.\r\nJoining with a leading slash leads to the file having a root directory as destination, which will be blocked on most devices.\r\n\r\nA decent solution could be cleaning the key to not have leading slashes or throwing an error that makes this easier to debug.\r\n\n\n### Additional Files\n\n_No response_\n\n### Environment\n\nWandB version: 0.12.13 and 0.12.16\r\n\r\nOS: Linux\r\n\r\nPython version: 3.8\r\n\r\nVersions of relevant libraries:\r\n\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "ash0ts",
        "body": "Hello @sebimarkgraf,\r\n\r\nThe error you are facing does not have to do with the leading slash in the key you are logging to `wandb`. If you take a look at [this colab](https://colab.research.google.com/drive/1PjWHz3vvoMZlA25ffA56AUw_3vSwaSGx?usp=sharing) you can see that the image I simulated logs properly to `wandb`, albeit with a specific nesting.\r\n\r\n`Permission Error: [Errno 13] Permission Denied:`\r\noccurs when a file you are trying to open is opened by a user who does not have the proper permissions, or if you are trying to open a file that is actually a folder. [Here is a SO post that explains some workaround to ensure you don't face this.](https://stackoverflow.com/questions/36434764/permissionerror-errno-13-permission-denied).\r\n\r\nAre you passing in a filepath to your call to `wandb.Image`? If so, I would recommend assigning the `wandb.Image` to an intermediate variable before passing it to `run.log` to determine if the creation of the `wandb.Image` is the actual problem"
      },
      {
        "user": "sebimarkgraf",
        "body": "Hi @ash0ts,\r\n\r\nThanks for your swift reply and the reproduction of the issue.\r\n\r\nYour Colab suffers from the same problem. Just add a cell at the end with `!ls /`.\r\nIt just does not produce any errors because Colabs have a writable root folder by default."
      },
      {
        "user": "ash0ts",
        "body": "Hello @sebimarkgraf,\r\n\r\nI misunderstood your issue earlier. I see the problem that you have mentioned. Thank you for bringing this to our attention! We really appreciate it. I will raise this to our engineers to fix.\r\n\r\nFor more context, can you let us know why you have keys that have a leading slash to it?"
      }
    ]
  },
  {
    "issue_number": 8717,
    "title": "[Q]: Is there more detailed introduction or example of Query panels combined plot?",
    "author": "Neronjust2017",
    "state": "open",
    "created_at": "2024-10-26T14:43:38Z",
    "updated_at": "2025-04-25T08:33:39Z",
    "labels": [
      "ty:question",
      "c:docs",
      "a:app"
    ],
    "body": "### Ask your question\n\nHi, this issue comes from https://github.com/wandb/examples/issues/577, maybe you only need to answer once.\n\nI combine my two tables together through a key with inner or outer join. The new table is like this, joined on the first column, and other columns have two values (from the original two tables).\n![Image](https://github.com/user-attachments/assets/cc6de924-43dd-4460-a2e2-c0c78467c08e)\n\n1. I want to now convert it into a combined scatter plot, with different colors for different original tables (for example, yellow and pink). How can I set the \"Color\" below?\n![Image](https://github.com/user-attachments/assets/3ed3134d-df9b-49e3-b601-c0f9dabb6dc9)\n\n2. Is there syntax instruction for weave expression? And how should I customize the shape or size of each point based on different column values？\n![Image](https://github.com/user-attachments/assets/0c935c91-4f5d-4e9c-b307-0dd7ed78b476)\n\n3. I think the following query panel in the doc https://docs.wandb.ai/guides/app/features/panels/query-panel/ is what I can refer to. Can you share this example？\n![Image](https://github.com/user-attachments/assets/92f1a2f4-6e71-4aa4-84f0-d6d5b6268738)\n\n\n",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hey @Neronjust2017, thanks for writing in! I would recommend checking [this](https://wandb.ai/luis_team_test/weave_example_queries/reports/Weave-queries---Vmlldzo1NzIxOTY2?accessToken=bvzq5hwooare9zy790yfl3oitutbvno2i6c2s81gk91750m53m2hdclj0jvryhcr) report, it has examples and explanations to create query expressions. Additionally, could you please share a link to your project? I can take a look and help with the query"
      },
      {
        "user": "luisbergua",
        "body": "Hey @Neronjust2017, just wanted to see if the information I provided was helpful? Feel free to share a link to your project, so I can take a look and help with the query "
      },
      {
        "user": "luisbergua",
        "body": "Hi @Neronjust2017, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!\n"
      }
    ]
  },
  {
    "issue_number": 9730,
    "title": "[Bug]: ``wandb.finish()`` with ``log_model=“all”`` is very slow when online",
    "author": "OverLordGoldDragon",
    "state": "open",
    "created_at": "2025-04-16T12:10:33Z",
    "updated_at": "2025-04-24T21:30:15Z",
    "labels": [
      "ty:bug",
      "c:artifacts"
    ],
    "body": "### Describe the bug\n\n21 secs to upload 8 files, 32kB.\n\nMRE below. Scales with `N_EPOCHS`.\n\nNot reproduced with `LOG_MODEL != \"all\"` or `OFFLINE = 1`.\n\nConfirmed it’s model uploading, since:\n\n```\nwandb: ⣟ uploading artifact model-vjl4ilqt (13s)\nwandb: ⣟ uploading artifact model-vjl4ilqt (12s)\n```\n\npeaks out at the observed total slowdown time.\n\n<details>\n<summary><b>MRE</b></summary>\n\n```python\n# -*- coding: utf-8 -*-\nimport os\nimport wandb\nimport torch\nimport lightning as L\nimport torch.nn as nn\nfrom lightning.pytorch.loggers import WandbLogger\nfrom torch.utils.data import DataLoader, Dataset\nfrom time import time\n\n# KEY CONFIGS ---------\nLOG_MODEL = (False, True, \"all\")[2]  # \"all\"= slow\nOFFLINE = 0  # (0 = slow)\n\n# --- Configuration ---\n# W&B\nWANDB_PROJECT =\nWANDB_ENTITY =\nos.environ['WANDB_API_KEY'] =\n\n# Data & Training\nDATA_LEN = 8000\nN_SAMPLES = 100\nN_EPOCHS = 10\n\n# Other\nif OFFLINE:\n    os.environ[\"WANDB_MODE\"] = \"offline\"\n\n# Helpers --------------------------------------------------------------------\n# Dataset helpers\nclass DummyAudioDataset(Dataset):\n    def __getitem__(self, idx):\n        x = torch.randn(1, DATA_LEN)  # add channel dim\n        y = torch.randint(0, 10, (1,)).squeeze()\n        return x, y\n\n    def __len__(self):\n        return N_SAMPLES\n\n# Model helpers\ndef build_cnn(input_channels=1):\n    return nn.Sequential(\n        nn.Conv1d(input_channels, 16, kernel_size=3, padding=1),\n        nn.AdaptiveAvgPool1d(1),\n        nn.Flatten(),\n        nn.Linear(16, 10)\n    )\n\nclass AudioClassifierPL(L.LightningModule):\n    def __init__(self, data_len=8000):\n        super().__init__()\n        self.data_len = data_len\n        self.model = build_cnn()\n        self.criterion = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        return self.model(x)  # x: (batch, 1, data_len)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        pred = self(x)\n        loss = self.criterion(pred, y)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\n\n# Main logic -----------------------------------------------------------------\ndef main():\n    # W&B\n    wandb.login(key=os.environ['WANDB_API_KEY'])\n    wandb_logger = WandbLogger(\n        name=\"cnn_mre\",\n        settings=wandb.Settings(save_code=False),\n        project=WANDB_PROJECT,\n        entity=WANDB_ENTITY,\n        log_model=LOG_MODEL\n    )\n    # Trigger logger init and get run details\n    _ = wandb_logger.experiment\n\n    # Data\n    dummy_dataset = DummyAudioDataset()\n    dummy_dataloader = DataLoader(\n        dummy_dataset, batch_size=16, shuffle=True, num_workers=0)\n\n    # PL module\n    pl_model = AudioClassifierPL(data_len=DATA_LEN)\n\n    # Trainer\n    trainer = L.Trainer(max_epochs=N_EPOCHS, logger=[wandb_logger],\n                        callbacks=[], enable_progress_bar=True)\n\n    # Train\n    trainer.fit(pl_model, train_dataloaders=dummy_dataloader)\n    print(\"Training complete.\")\n\n    # Slowdown occurs here\n    t1 = time()\n    if wandb.run:\n        wandb.finish()\n    print('\\n\\nTime after \"Training complete.\": %.3g' % (time() - t1))\n\n\nif __name__ == \"__main__\":\n    t0 = time()\n    main()\n    print(\"Total time: %.3g\" % (time() - t0))\n```\n\n</details>\n\n**Env**: wandb 0.19.9 (`-c conda-forge`), Python 3.12.9, Win 11",
    "comments": [
      {
        "user": "paulosabile-wb",
        "body": "Hi @OverLordGoldDragon Good day and thank you for reporting this to us! Let me review this for a while and we'll get back to you for an update. Thanks!"
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi @OverLordGoldDragon, thank you for writing in. \n\nI am seeing a similar behavior on our end. Thank you for sending over the reproduction example. I'll report this to our engineering team and will let you know if I get any updates from them. "
      }
    ]
  },
  {
    "issue_number": 3751,
    "title": "wandb mobile/desktop app?",
    "author": "guglielmocamporese",
    "state": "open",
    "created_at": "2022-06-07T02:37:39Z",
    "updated_at": "2025-04-24T04:06:32Z",
    "labels": [
      "a:app"
    ],
    "body": "Are you planning to develop a mobile/desktop app for wandb? The iOS app for iPhone/iPad/Mac would be awesome!",
    "comments": [
      {
        "user": "armanhar",
        "body": "Hey @guglielmocamporese, while we are not working on it at the moment, we have an internal ticket tracking this request. I'll bump up its priority. I will notify you about updates on this."
      },
      {
        "user": "drigoni",
        "body": "Up"
      },
      {
        "user": "moelllerniklas",
        "body": "Up"
      }
    ]
  },
  {
    "issue_number": 4247,
    "title": "[Feature]: Support for multi-threaded / parallel experiments",
    "author": "Zahlii",
    "state": "closed",
    "created_at": "2022-09-08T07:51:08Z",
    "updated_at": "2025-04-23T22:15:58Z",
    "labels": [
      "ty:feature",
      "c:sdk:internal-process",
      "a:sdk"
    ],
    "body": "### Description\n\nTo my understanding, it's currently only possible to log experiments done sequentially, or experiments done in using several sub-processes. However, a big part of out work consists in running multiple experiments using a ThreadPool (as this is much cheaper than using processes, and still provides speed-up for many libraries releasing the GIL for work). \r\n\r\nAs such, I wonder if there is any way of tracking these kind of experiments right now.\r\nAs an example, think about evaluating 10 different hyperparameter choices on one and the same data set, e.g. using 3-fold cross-validation, and running each of the 10 models in parallel. How would we go ahead and track (average) metrics of all 10 models? How would we go about tracking each set of 3*10 results?\r\n\r\nI can see https://github.com/wandb/examples/blob/master/examples/wandb-sweeps/sweeps-cross-validation/train-cross-validation.py, but this uses multiprocessing and parallelizes over splits, not across models (or across both at same time)\n\n### Suggested Solution\n\nI think just being able to use the following approach, independent on whether its sequential, parallel or thread-parallel, would be very helpful:\r\n\r\n    def run_model_on_split(model_id, hyperparams, x_train, x_test, y_train, y_test):\r\n      with wandb.init(group_name=model_id, ...) as run:\r\n          ...\r\n          run.log(acc)\r\n          return acc\r\n\r\n    with ThreadPoolExecutor(10) as tpe: // may as well be processpool\r\n        for n_estimators in range(10):\r\n           for split_data in cv.split(x, y):\r\n              future = tpe.submit(run_model_on_split, n_estimators, {\"n_estimators\": n_estimators}, *split_data)\r\n            \r\n\r\n \n\n### Alternatives\n\n_No response_\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "ramit-wandb",
        "body": "Hi @Zahlii!\r\n\r\nThank you for your feature request! You are right, while we are working to move in this direction, this style of experiment tracking is not fully supported yet. I'll create an internal ticket to track this request and provide you with updates as movement is made here.\r\n\r\nThanks,\r\nRamit"
      },
      {
        "user": "timoffex",
        "body": "The 0.19.10 release includes a new setting that allows having multiple active runs in the same process:\n\n```python\nwith wandb.init(reinit=\"create_new\") as run1:\n    with wandb.init(reinit=\"create_new\") as run2:\n        # Both run1 and run2 are active here. This was not allowed before.\n```\n\nHowever, `wandb.init()` is not thread-safe, so if you're using a thread pool, you'll need to use a lock around invoking `wandb.init()`. The safest way that accounts for all exceptions would be something like this:\n\n```python\nwith lock:\n    run = wandb.init(reinit=\"create_new\")\n\n# Using a context manager ensures the run will be marked finished\n# if there is an exception.\nwith run:\n    ...\n```\n\nwhere `lock` is a`threading.Lock` that's shared by all threads that invoke `wandb.init()`.\n\n**IMPORTANT CAVEAT**: This is a very new feature, and many W&B integrations will not work with this setting because they were designed around a single, global run. Runs created with `\"create_new\"` will be ignored by such integrations. There may also be some edge cases around artifact operations that we will still need to fix. Please file issues if you encounter problems!\n\n**NOTE**: `reinit` is a setting, so you can pass it to `wandb.Settings()` as well, or use the `WANDB_REINIT` environment variable to have it apply to all `wandb.init()` calls."
      }
    ]
  },
  {
    "issue_number": 3463,
    "title": "[App]: Experiment name shadowed by the \"green circle\"",
    "author": "eldarkurtic",
    "state": "closed",
    "created_at": "2022-04-02T11:28:39Z",
    "updated_at": "2025-04-23T19:22:10Z",
    "labels": [
      "a:app"
    ],
    "body": "### Current Behavior\n\nThe last two characters of an experiment name are being shadowed by the little \"green circle\" indicating that the experiment is online and running. When the name is too long to fit in the default view, even expanding to the right to see the full name doesn't move the green circle from the last two characters.\n\n### Expected Behavior\n\nTo see the full name of the experiment.\n\n### Steps To Reproduce\n\n_No response_\n\n### Screenshots\n\nNon-expanded view:\r\n<img width=\"323\" alt=\"Screenshot 2022-04-02 at 13 25 25\" src=\"https://user-images.githubusercontent.com/8884008/161380994-38970eaf-ece6-4cb7-bf93-04546ff53d69.png\">\r\n\r\nExpanded view:\r\n<img width=\"496\" alt=\"Screenshot 2022-04-02 at 13 25 49\" src=\"https://user-images.githubusercontent.com/8884008/161380997-90b00d34-5269-4cc8-afcd-27f9d919f971.png\">\r\n\n\n### Environment\n\nChrome Version 99.0.4844.84 (Official Build) (x86_64)\r\n\r\n\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "armanhar",
        "body": "Hey Eldar, thanks for flagging this. I'll file a ticket.\r\n"
      }
    ]
  },
  {
    "issue_number": 2014,
    "title": "[App] Plotly animation isn't playable",
    "author": "nicolas-dufour",
    "state": "closed",
    "created_at": "2021-03-30T14:21:00Z",
    "updated_at": "2025-04-23T14:34:04Z",
    "labels": [
      "a:app"
    ],
    "body": "**Describe the bug**\r\nWhen logging plotly animated chart, the animation is freezed to the first frame\r\n\r\n**Expected behavior**\r\nThe plot should not be freezed to the first frame of the animation\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Operating System**\r\n - OS: windows\r\n - Browser: Chrome\r\n - Version: 22\r\n",
    "comments": [
      {
        "user": "vanpelt",
        "body": "Hey @nicolas-dufour can you provide some example code, a colab, or a screenshot of the issue?"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open 60 days with no activity."
      },
      {
        "user": "sydholl",
        "body": "Because this ticket has gone stale, we are closing it. Please comment to reopen. "
      }
    ]
  },
  {
    "issue_number": 9631,
    "title": "[Q]: Cannot download artifacts in offline mode.",
    "author": "JuanFMontesinos",
    "state": "closed",
    "created_at": "2025-03-25T15:01:03Z",
    "updated_at": "2025-04-23T14:00:45Z",
    "labels": [
      "ty:question",
      "c:artifacts"
    ],
    "body": "### Ask your question\n\nDear devs.,\nI've seen through time how offline mode is more and more restricted regarding artifact's usage.\nInitially, artifacts could be download normally from an offline run. \nThen, artifacts could no longer be downloaded via an offline run but could still be downloaded via api.\nThe following snippet was working\n```\n from_api = uri.startswith(\"wandb+api://\") or wb.run is None:\n  if from_api:\n      api = wb.Api()\n      artifact = api.artifact(f\"{aiw.WANDB_ENTITY}/{aiw.WANDB_PROJECT}/{uri}\")\n  else:\n      artifact = wb.run.use_artifact(f\"{aiw.WANDB_ENTITY}/{aiw.WANDB_PROJECT}/{uri}\")\n  artifact_dir = artifact.download()\n```\nNow, apparently artifacts can neither be downloaded via API.\nWhy becoming so restrictive? Is there any workaround? I need access to artifacts for debugging and don't want to be polluting my runs table with dozens of debug runs. \n\nAlso noticed the api has changed and an offline run is no longer None but a run object and therefore we need to check so by\n`wb.run.offline` rather than `wb.run is None`\n\n\n",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hi @JuanFMontesinos, thanks for flagging this! Let me please raise the problem with our Artifacts Team, and will keep you posted "
      },
      {
        "user": "luisbergua",
        "body": "Hey @JuanFMontesinos! Just wanted to let you know that in our latest `wandb==0.19.10` you can download artifacts with the API again when running in offline mode, see the [PR](https://github.com/wandb/wandb/pull/9695) addressing this"
      }
    ]
  },
  {
    "issue_number": 9731,
    "title": "[Bug-App]: Cannot update Netron on web UI",
    "author": "bikcrum",
    "state": "open",
    "created_at": "2025-04-16T23:37:16Z",
    "updated_at": "2025-04-23T09:15:31Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\nWhen I click on the model in the files tab in the Web UI, the prompt to update the netron shows up, but when I click on that, it gives me a connection refused error. It used to show up the model graph couple of weeks back. \n\nPlease check the issue in the video below:-\n\nhttps://github.com/user-attachments/assets/3c90f3b6-2475-4162-8da4-4ce8dbc4b6aa\n\nwandb, version 0.19.8",
    "comments": [
      {
        "user": "paulosabile-wb",
        "body": "Hi @bikcrum Good day and thank you for reaching out to us. Happy to help you on this!\n\nThe issue with the netron update should now be fixed. Could you please check and let us know if there's no more blocker now from your end? Thanks!"
      },
      {
        "user": "paulosabile-wb",
        "body": "Hi @bikcrum  since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!"
      }
    ]
  },
  {
    "issue_number": 2701,
    "title": "[CLI] With WANDB_MODE=offline python client still try to sync results",
    "author": "cssb",
    "state": "open",
    "created_at": "2021-09-26T11:21:41Z",
    "updated_at": "2025-04-22T23:35:37Z",
    "labels": [
      "ty:bug",
      "a:sdk",
      "c:sdk:code-quality"
    ],
    "body": "**Description**\r\nI use W&B on a server without an Internet connection.\r\nFor working with W&B, I set WANDB_MODE=offline then sync my runs from another machine with an Internet connection.\r\nAll work fine until I update to wandb version 0.12.2.\r\nWith version 0.12.2 when the program ends, I get the message:\r\n\"... wandb: Network error (ConnectTimeout), entering retry loop.\"\r\nand the program cannot terminate.\r\n\r\nThe code to reproduce,  run on a machine without internet connection:\r\n```python\r\n# test_wandb_offline.py\r\nimport os\r\nos.environ[\"WANDB_MODE\"]=\"offline\"\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport wandb\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    print(wandb.__version__)\r\n    wandb.init(project=\"my_wandb_test_project\")\r\n    wandb.config[\"run_1\"] = 1\r\n    wandb.log({\"val_1\":1})\r\n    wandb.log({\"val_1\":2})\r\n    wandb.log({\"val_1\":3})\r\n    wandb.summary.update({\"val_2\":123})\r\n    print(\"Done!\")\r\n```\r\n\r\nWith wandb version 0.12.2 the program cannot terminate and output looks like:\r\n```\r\n0.12.2\r\nwandb: W&B syncing is set to `offline` in this directory.  Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\r\n~/anaconda/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nDone!\r\n\r\nwandb: Waiting for W&B process to finish, PID 24846\r\nwandb: Program ended successfully.\r\nwandb: Network error (ConnectTimeout), entering retry loop.\r\n```\r\n\r\nWith wandb version 0.12.1 and early, the program terminates normally.\r\n\r\n\r\n**Environment**\r\n- OS: CentOS Linux release 7.9.2009 (Core)\r\n- Environment: n/a\r\n- Python Version: 3.7.0\r\n",
    "comments": [
      {
        "user": "donlaiq",
        "body": "Hello cssb,\r\n\r\nMaybe, I am not a good reference at all (I've installed wandb just a few minutes ago), but I've run exactly the same code you are mentioning in the post, and it runs flawlessly.\r\nJust for your reference, I've created a brand new virtual environment with Python 3.7.3, where I'm installing just numpy, pandas and wandb (just the required libraries), and my OS is Debian 10.\r\n\r\n`wandb: Waiting for W&B process to finish, PID 13232`\r\n`wandb: Program ended successfully.`\r\n`wandb: Find user logs for this run at...`\r\n"
      },
      {
        "user": "cssb",
        "body": "Hello donlaiq, are you running this code on the offline machine (without connection to the Internet)?"
      },
      {
        "user": "donlaiq",
        "body": "Oh, boy! No I wasn't. In that case, you are right, I'm running in the same issue."
      }
    ]
  },
  {
    "issue_number": 9729,
    "title": "[Bug-App]: Wrong sorting by name when using groups",
    "author": "nick-janssen",
    "state": "open",
    "created_at": "2025-04-16T09:51:02Z",
    "updated_at": "2025-04-22T18:10:52Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\nWhen sorting my grouped runs by name the sorting is wrong. When sorting by a number the sorting is correct.\n\n![Image](https://github.com/user-attachments/assets/bfb46d05-314f-4786-9036-492e086b14a1)",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bonnie Shen commented: \nHello @nick-janssen. Thank you for reporting this to us! Would you please share us a project link to where you are seeing this issue?\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bonnie Shen commented: \nHello @nick-janseen! We filed a bug report on this behavior you encountered. Thank you for the report!\n"
      }
    ]
  },
  {
    "issue_number": 3272,
    "title": "[Q] wandb.image caption is not shown in web?",
    "author": "yougerli",
    "state": "closed",
    "created_at": "2022-02-22T09:55:24Z",
    "updated_at": "2025-04-22T16:44:19Z",
    "labels": [
      "ty:bug",
      "c:sdk:media",
      "a:sdk",
      "c:table"
    ],
    "body": "Weights and Biases version: 0.12.10\r\nPython version: 3.6.13\r\nOperating System: Linux\r\n\r\n\r\nWhen I use wandb to show image in web, captions is not shown in web\r\n```\r\nimport wandb\r\nclass Eval_WandBWriter:\r\n    def __init__(self, log_dir: str, project: str, entity: str):\r\n        wandb.init(project=project, entity=entity, dir=log_dir)\r\n        self.wandb = wandb\r\n\r\n    def write(self, img_path_list, img_label_list):\r\n        N = len(img_path_list)\r\n        table = self.wandb.Table(columns=[\"name\"] + [f\"recall_{i}\" for i in range(1, N)])\r\n        img = [self.wandb.Image(x, caption=\"please show me caption!!\") for i, x in enumerate(img_path_list)]\r\n        table.add_data(*img)\r\n        self.wandb.log({\"image_table\": table})\r\n\r\n    def close(self):\r\n        self.wandb.finish()\r\n\r\nif __name__ == \"__main__\":\r\n    writer = Eval_WandBWriter(\".\", \"eval\", \"yougerli\")\r\n    path=[ \"images/1.jpg\", \"images/2.jpg\",\"images/3.jpg\"]\r\n    labels = [1,2,3]\r\n    writer.write(path, labels)\r\n    writer.close()\r\n```\r\n\r\n\r\nWeb is shown below:\r\n<img width=\"1743\" alt=\"image\" src=\"https://user-images.githubusercontent.com/99822303/155107866-72e79d9d-e22b-4e32-9571-fd78d0639efe.png\">\r\n",
    "comments": [
      {
        "user": "nicofirst1",
        "body": "I'm trying to do the same, but the images are not shown at all. \r\nI only get image's paths"
      },
      {
        "user": "anmolmann",
        "body": "@yougerli, currently, the captions can't be shownfor wandb Images in tables. There's a feature request filed for this and I've bumped up the priority for you. We'll follow up with you as soon as there's an update.\r\nMeanwhile, you might want to log captions as a separate column in the table? \r\n\r\n@nicofirst1, could you please validate once if the path you're passing to `wandb.Image` is correct and if there're images stored at that path?"
      },
      {
        "user": "anmolmann",
        "body": "Hey @yougerli , quick update: we tested on our end and the caption logging for tables is working now. We'll close this thread as the ask has been resolved now. "
      }
    ]
  },
  {
    "issue_number": 9725,
    "title": "[Bug-App]: 500 Internal Server when creating new user",
    "author": "sanchez-alex",
    "state": "closed",
    "created_at": "2025-04-15T20:23:48Z",
    "updated_at": "2025-04-22T09:06:38Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\nWhen creating a new user, the a 500 Internal Server Error is returned with this as the text: {\"schemas\":[\"urn:ietf:params:scim:api:messages:2.0:Error\"],\"detail\":\"Post \\\"http:///api/sign\\\": http: no Host in request URL\",\"status\":\"500\"} \n\nMeeting with the wandb team, we have established this is a bug for azure dedicated cloud and is impacting the email invite sent when creating users. Bug has been filed to the engineering team for wandb. Creating this public bug for tracking purposes",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Thanks for flagging here as well @sanchez-alex! We'll keep you posted with the status of the bug as soon as our engineers provide any updates "
      },
      {
        "user": "luisbergua",
        "body": "Hey @sanchez-alex! This has been fixed, and will be included in our next server release, so closing this for now "
      }
    ]
  },
  {
    "issue_number": 9648,
    "title": "HTML tries to open file",
    "author": "imh",
    "state": "closed",
    "created_at": "2025-03-27T22:49:40Z",
    "updated_at": "2025-04-21T20:32:48Z",
    "labels": [
      "c:sdk:media",
      "a:sdk"
    ],
    "body": "The docs ([here](https://docs.wandb.ai/guides/track/log/media/#html) and [here](https://docs.wandb.ai/ref/python/data-types/html/)) say that HTML can be used with strings. And it works for a lot of strings. However, if you happen to have a file matching the string you passed in, it uses the contents of that instead.\n\nhttps://github.com/wandb/wandb/blob/ed5b294787e1a7181812f3334bb193e26875b536/wandb/sdk/data_types/html.py#L29-L36\n\nThe result is *very* confusing behavior, where we scratch our heads and say \"huh why did the model predict this random string here,\" or the somewhat better, \"why did my my training run fail here?\"",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @imh! Thank you for writing in. \n\nWould love to take a look into this one. Could you please send me a toy code example of you running into this behavior?"
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved. "
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!\n"
      }
    ]
  },
  {
    "issue_number": 3070,
    "title": "[CLI] Wandb Table is slow ",
    "author": "koulanurag",
    "state": "open",
    "created_at": "2021-12-20T23:11:06Z",
    "updated_at": "2025-04-19T00:40:58Z",
    "labels": [
      "c:table"
    ],
    "body": "Creating wandb Table from data-frame is very slow. I observed it with a data-frame having milliion rows.\r\n\r\nUpon investigation, I found it to be because of iterative addition of data in the wandb table from the dataframe. I hope this could be optimized.\r\n\r\nCode Reference: https://github.com/wandb/client/blob/master/wandb/data_types.py#L294",
    "comments": [
      {
        "user": "vanpelt",
        "body": "Hey @koulanurag we're working on an implementation of tables that can scale to millions of rows and expect to release next year.  Currently tables are limited to 200k rows.  We should likely add logic to warn or fail when attempting to load larger data frames and will keep this ticket open for that feature."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open 60 days with no activity."
      },
      {
        "user": "janosh",
        "body": "Not stale."
      }
    ]
  },
  {
    "issue_number": 3237,
    "title": "[CLI] Creating a table from a dataframe with enum values as column headers fails",
    "author": "dlangerm",
    "state": "open",
    "created_at": "2022-02-10T14:45:11Z",
    "updated_at": "2025-04-19T00:33:39Z",
    "labels": [
      "ty:feature",
      "c:table"
    ],
    "body": "**Description**\r\nAs the title says. Creating a Table from a dataframe with enum column names causes the following exception\r\n\r\n`columns argument expects list of strings or ints`\r\n\r\n**Wandb features**\r\nTable()\r\n\r\n**How to reproduce**\r\n1. See above\r\n\r\n**Environment**\r\n- Python Version: 3.9\r\n\r\nThere are two fixes for this in my mind:\r\n\r\n1. The _assert_valid_columns call is too restrictive and should at least attempt to cast the columns to strings \r\n2. The error above should be more clear. I had not given the `columns` argument to the Table constructor, and yet that's what it was suggesting I did wrong.\r\n",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Leslie commented: \nHello, can you send me your wandb.log code snippet so I can see how you are logging your data? I find it interesting that you're getting an error regarding columns when you're not indicating it. Also, can you tell me what version of wandb you are using?\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Leslie commented: \nHi, I'm just checking in to see if you still need help with this issue?\n"
      },
      {
        "user": "dlangerm",
        "body": "```python\r\nfrom wandb import Table\r\nfrom enum import Enum\r\n\r\nclass en(str, Enum):\r\n    test = 'test'\r\n\r\ndf = pd.DataFrame()\r\n\r\ndf[en.test] = [5] * 20\r\n\r\nresult_table = Table(dataframe=df)\r\n```\r\n\r\nResults in \r\n\r\n![image](https://user-images.githubusercontent.com/13711976/153926666-2850598c-de2a-41ef-8dff-3b44a948725a.png)\r\n"
      }
    ]
  },
  {
    "issue_number": 2400,
    "title": "[Feature] Loading wandb.Table into a Dataframe",
    "author": "igorgad",
    "state": "open",
    "created_at": "2021-07-16T03:53:21Z",
    "updated_at": "2025-04-19T00:06:38Z",
    "labels": [
      "ty:feature",
      "s:workaround",
      "c:table"
    ],
    "body": "**Is your feature request related to a problem? Please describe.**\r\nNope\r\n\r\n**Describe the solution you'd like**\r\n\r\nWould like a `wand.Table.to_dataframe()` method to create a pandas dataframe from a wandb.Table object.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nThe following code excerpt works as expected.\r\n```\r\nartifact = run.use_artifact('dataset:v9')\r\nartifact_dir = artifact.download()\r\ntjs = json.load(open(artifact_dir + '/dataset.table.json'))\r\ntable = wandb.Table.from_json(tjs, artifact)\r\npd.DataFrame(table.data, columns=table.columns)\r\n```\r\n\r\nThanks,.\r\n",
    "comments": [
      {
        "user": "alvesfelipe",
        "body": "I just used this solution, and it worked for me! \r\n\r\nThank you @igorgad "
      },
      {
        "user": "tyomhak",
        "body": "Hey @igorgad , thank you for the feature request :)"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open 60 days with no activity."
      }
    ]
  },
  {
    "issue_number": 9691,
    "title": "[Bug]: store tags as tuple instead of list",
    "author": "AnikiFan",
    "state": "open",
    "created_at": "2025-04-08T12:07:05Z",
    "updated_at": "2025-04-18T17:53:15Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\nAccording to the docs of [`wandb.init()`](https://docs.wandb.ai/ref/python/init/),\n\n> To add tags to a resumed run without overwriting the current tags, use run.tags += [\"new_tag\"] after calling run = wandb.init().\n\nHowever, running below scripts sequencially\n\n```python\nimport wandb \nrun = wandb.init(\n    project=\"test\",\n    name=\"test\",\n    tags=[\"old_tag\"],\n    id=\"test\",\n    resume=\"allow\"\n)\nwandb.finish()\n```\n\n```python\nimport wandb \nrun = wandb.init(\n    project=\"test\",\n    name=\"test\",\n    tags=[\"old_tag\"],\n    id=\"test\",\n    resume=\"allow\"\n)\nrun.tags += [\"new_tag\"]\nwandb.finish()\n```\n\nwill get following error:\n\n```\nTraceback (most recent call last):\n  File \"<path_to_script>\", line 9, in <module>\n    run.tags += [\"new_tag\"]\nTypeError: can only concatenate tuple (not \"list\") to tuple\n```\n\nwhich I believed is due to the initializiation of member `run_tags`.\n\nhttps://github.com/wandb/wandb/blob/0126ae722376ad3cfd60231e0c6860db545eb01f/wandb/sdk/wandb_init.py#L1439\nwandb              0.19.9\nPython 3.13.2\nLinux server11 5.4.0-176-generic #196-Ubuntu SMP Fri Mar 22 16:46:39 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hey @AnikiFan, thanks for flagging this! I can see the problem, will check with our engineers and update the description. Since this is a tuple, you should add new tags using:\n````\nimport wandb \nrun = wandb.init(\n    project=\"\",\n    tags=[\"old_tag\"],\n)\n\nrun.tags += (\"new_tag\",)\nprint(run.tags)\nrun.finish()\n ````\n"
      }
    ]
  },
  {
    "issue_number": 9338,
    "title": "[Bug]: file.delete() Fails with CommError Due to Internal Server Error",
    "author": "nkosaku",
    "state": "open",
    "created_at": "2025-01-26T13:41:08Z",
    "updated_at": "2025-04-18T14:38:24Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\nAn error occurs when attempting to delete files from a wandb run using the API. Specifically, the API fails with a CommError due to an internal server issue, as shown below.\n\n### Code\n```Python\nimport wandb\n\napi = wandb.Api()\n\nrun = api.run(\"user/project/run_id\")\nfor file in run.files():\n    if file.name.endswith(\".png\"):\n        file.delete()\n        break\n```\n\n### Error Output\n```\nwandb: Network error (HTTPError), entering retry loop.\n...\nCommError: could not unmarshal \"<Base64EncodedString>\" (string) into int32: incompatible type (Error 500: Internal Server Error)\n```\n\n### Environment Details\n- **wandb Version:** 0.19.4\n- **Python Version:** 3.10.9\n- **Operating System:** macOS 15.3",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bonnie Shen commented: \nHello @nkosaku! Thank you for reaching out to us! Could you please share me the following in order for me to investigate further?\n\n\n1. the full stack trace of the CommError you reported.\n2. the python script of how you uploaded the \".png\" files.\n\nBest, W&B\n"
      },
      {
        "user": "nkosaku",
        "body": "Hello Bonnie. Thank you for your response!\nHere is the information you requested for further investigation:\n\n1. **Full Stack Trace:**\n```\nwandb: Network error (HTTPError), entering retry loop.\nwandb: ERROR could not unmarshal \"UHJvamVjdEludGVybmFsSWQ6NDA4NTM1NzU=\" (string) into int32: incompatible type (Error 500: Internal Server Error)\nTraceback (most recent call last):\n  File \"~/miniconda3/envs/temp-wandb/lib/python3.10/site-packages/wandb/apis/normalize.py\", line 25, in wrapper\n    return func(*args, **kwargs)\n  File \"~/miniconda3/envs/temp-wandb/lib/python3.10/site-packages/wandb/apis/public/files.py\", line 220, in delete\n    self.client.execute(\n  File \"~/miniconda3/envs/temp-wandb/lib/python3.10/site-packages/wandb/sdk/lib/retry.py\", line 212, in wrapped_fn\n    return retrier(*args, **kargs)\n  File \"~/miniconda3/envs/temp-wandb/lib/python3.10/site-packages/wandb/sdk/lib/retry.py\", line 131, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File \"~/miniconda3/envs/temp-wandb/lib/python3.10/site-packages/wandb/apis/public/api.py\", line 72, in execute\n    return self._client.execute(*args, **kwargs)\n  File \"~/miniconda3/envs/temp-wandb/lib/python3.10/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n  File \"~/miniconda3/envs/temp-wandb/lib/python3.10/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n  File \"~/miniconda3/envs/temp-wandb/lib/python3.10/site-packages/wandb/sdk/lib/gql_request.py\", line 59, in execute\n    request.raise_for_status()\n  File \"~/miniconda3/envs/temp-wandb/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://api.wandb.ai/graphql\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"~/projects/wandb-download/wandb_report_issue.py\", line 18, in <module>\n    file.delete()\n  File \"~/miniconda3/envs/temp-wandb/lib/python3.10/site-packages/wandb/apis/normalize.py\", line 35, in wrapper\n    raise CommError(message, error)\nwandb.errors.errors.CommError: could not unmarshal \"UHJvamVjdEludGVybmFsSWQ6NDA4NTM1NzU=\" (string) into int32: incompatible type (Error 500: Internal Server Error)\n```\n\n2. **Python Script:**\n```python\nimport wandb\nimport numpy as np\n\n# Create a run and upload image\nrun = wandb.init(project=\"temp\")\nrun_id = run.id\n\nimage = wandb.Image(np.random.rand(100, 100, 3))\nrun.log({\"image\": image})\nrun.finish()\n\n# Download image with wandb.Api\napi = wandb.Api()\nrun = api.run(f\"temp/{run_id}\")\nfor file in run.files():\n    if file.name.endswith(\".png\"):\n        file.delete()\n```\n\nPlease let me know if any additional details are needed."
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bonnie Shen commented: \nThank you for sending these over @nkosaku! I recreated this on my end as well. And this is not only limited to the png files. All files returned by api can not be deleted and error out the same CommError message. I actually suspect this is intended behavior as these files uploaded by wandb are used for other functionalities of W&B. And deleting them can lead to errors elsewhere. I'm confirming this internally with my team. If this is the case, I'll request to have a better error message for this. Will keep you posted.\n"
      }
    ]
  },
  {
    "issue_number": 9708,
    "title": "[Q]: Wandb.log() a Table with a user-defined artifact name",
    "author": "skpig",
    "state": "closed",
    "created_at": "2025-04-10T08:28:59Z",
    "updated_at": "2025-04-18T02:17:25Z",
    "labels": [
      "ty:question",
      "c:table"
    ],
    "body": "### Ask your question\n\nhttps://docs.wandb.ai/guides/track/log/log-tables/#access-tables-programmatically\n\nAs shown above, all tables will be stored as artifacts with fixed name \"run-<run-id>-<table-name>:<tag>\". Is it possible to set this name manually?",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hi @skpig! Thank you for writing in. \n\nWhen you log the table, you can download its table artifact as such:\n`run-<run-id>-<table-name>:<tag>`\n\nWe let you rename your tables in the UI by going over to the Artifacts page, clicking on the desired table artifact\n\n<img width=\"1415\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/869aa128-1898-4cab-bd61-0c254ad94506\" />\n\nand clicking `rename`. \n\nLet us know if you have any follow-up questions. "
      },
      {
        "user": "skpig",
        "body": "Hi, @ArtsiomWB. Thanks for your prompt reply! Is it possible to rename the artifact within Python code (e.g. rename to the run-name instead of run-id)?"
      },
      {
        "user": "ArtsiomWB",
        "body": "Thank you for the quick follow up!\n\nunfortunately, it is not possible to rename artifacts outside of runs using the SDK"
      }
    ]
  },
  {
    "issue_number": 9588,
    "title": "[Bug]: file.delete() fails with \"object has no attribute '_project_internal_id'\"",
    "author": "AaronSpieler",
    "state": "closed",
    "created_at": "2025-03-17T09:08:02Z",
    "updated_at": "2025-04-18T02:02:09Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nHello;\n\nrunning the following script:\n\n```\nimport wandb\n\napi = wandb.Api()\nruns = api.runs(\"user/project\")\nfile_name = \"model.pt\"\n\nfor run in runs:\n    files = run.files()\n    for file in files:\n        if file.name == file_name:\n            file.delete()\n```\n\nI get the following error:\n\n```\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nFile [/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/normalize.py:25](http://localhost:8888/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/normalize.py#line=24), in normalize_exceptions.<locals>.wrapper(*args, **kwargs)\n     24 try:\n---> 25     return func(*args, **kwargs)\n     27 except requests.HTTPError as error:\n\nFile [/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/public/files.py:204](http://localhost:8888/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/public/files.py#line=203), in File.delete(self)\n    203 if self._server_accepts_project_id_for_delete_file():\n--> 204     variable_values[\"projectId\"] = self.run._project_internal_id\n    205     project_id_variable_fragment = \", $projectId: Int\"\n\nFile [/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/attrs.py:51](http://localhost:8888/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/attrs.py#line=50), in Attrs.__getattr__(self, name)\n     50 else:\n---> 51     raise AttributeError(f\"{repr(self)!r} object has no attribute {name!r}\")\n\nAttributeError: '<Run user[/project/93250c0x](http://localhost:8888/project/93250c0x) (finished)>' object has no attribute '_project_internal_id'\n\nDuring handling of the above exception, another exception occurred:\n\nCommError                                 Traceback (most recent call last)\nCell In[3], line 9\n      7 if file.name == \"artifacts[/best_model_state_dict.eqx](http://localhost:8888/best_model_state_dict.eqx)\":\n      8     print(abl_id, file.name)\n----> 9     file.delete()\n\nFile [/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/normalize.py:79](http://localhost:8888/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/normalize.py#line=78), in normalize_exceptions.<locals>.wrapper(*args, **kwargs)\n     77     raise\n     78 else:\n---> 79     raise CommError(message, err).with_traceback(sys.exc_info()[2])\n\nFile [/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/normalize.py:25](http://localhost:8888/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/normalize.py#line=24), in normalize_exceptions.<locals>.wrapper(*args, **kwargs)\n     23 message = \"Whoa, you found a bug.\"\n     24 try:\n---> 25     return func(*args, **kwargs)\n     27 except requests.HTTPError as error:\n     28     errors = parse_backend_error_messages(error.response)\n\nFile [/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/public/files.py:204](http://localhost:8888/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/public/files.py#line=203), in File.delete(self)\n    201 # Add projectId to mutation and variables if the server supports it.\n    202 # Otherwise, do not include projectId in mutation for older server versions which do not support it.\n    203 if self._server_accepts_project_id_for_delete_file():\n--> 204     variable_values[\"projectId\"] = self.run._project_internal_id\n    205     project_id_variable_fragment = \", $projectId: Int\"\n    206     project_id_mutation_fragment = \"projectId: $projectId\"\n\nFile [/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/attrs.py:51](http://localhost:8888/opt/miniconda3/envs/wandb-env/lib/python3.13/site-packages/wandb/apis/attrs.py#line=50), in Attrs.__getattr__(self, name)\n     49     return self._attrs[name]\n     50 else:\n---> 51     raise AttributeError(f\"{repr(self)!r} object has no attribute {name!r}\")\n\nCommError: '<Run user[/project/93250c0x](http://localhost:8888/project/93250c0x) (finished)>' object has no attribute '_project_internal_id'\n```\n\nThis is on osx with wandb 0.19.8.",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @AaronSpieler, Thanks for reaching out and bringing this to our attention! I was able to reproduce the issue on my end and will go ahead and file a bug report for it.\n\nIn the meantime, as a workaround, could you try downgrading your SDK version to `0.18.7`? I was able to delete files successfully using this version. Let me know if this works for you or if you run into any issues!"
      },
      {
        "user": "AaronSpieler",
        "body": "Yes this worked for me too, thx!"
      }
    ]
  },
  {
    "issue_number": 7324,
    "title": "[CLI]: wandb doesn't report logs from all nodes for distributed training using HuggingFace trainer",
    "author": "tnnandi",
    "state": "closed",
    "created_at": "2024-04-06T20:19:24Z",
    "updated_at": "2025-04-17T23:41:14Z",
    "labels": [
      "c:sdk:integration",
      "c:sdk:system-metrics",
      "c:sdk:distributed-training"
    ],
    "body": "### Describe the bug\r\n\r\n<!--- Description of the issue below  -->\r\n\r\nBy default, simply adding \"report_to\": \"wandb\" as an argument for training_args (for HF Trainer) only creates plots (say, for GPU usage) for only the master node on the wandb GUI. By overriding the default wandb.init() as shown below, I can create one eantry for each GPU on the wandb GUI, but only that from rank 0 are plotted. FYI: I'm using DeepSpeed for distributed training. \r\n\r\nFor a training instance with 2 nodes (each with 4 GPUs), wandb shows only 4 plots for GPU system metrics. Weights and Biases GUI should show system metrics from all GPUs involved in the training, and not from only those on the master node. FYI: individually logging into the different nodes, I've confirmed that those are indeed being used. \r\n\r\n<!--- A minimal code snippet between the quotes below  -->\r\n```\r\ntraining_args = {\r\n    \"learning_rate\": max_lr,\r\n    \"do_train\": True,\r\n    \"do_eval\": False,\r\n    \"group_by_length\": True,\r\n    \"length_column_name\": \"length\",\r\n    \"disable_tqdm\": False,\r\n    # \"lr_scheduler_type\": lr_schedule_fn,\r\n    # \"warmup_steps\": warmup_steps,\r\n    \"weight_decay\": weight_decay,\r\n    \"per_device_train_batch_size\": geneformer_batch_size,\r\n    \"num_train_epochs\": epochs,\r\n    \"save_strategy\": \"steps\",\r\n    \"save_steps\": np.floor(num_examples / geneformer_batch_size / 8),  # 8 saves per epoch\r\n    \"logging_steps\": 1000,\r\n    \"output_dir\": training_output_dir,\r\n    \"logging_dir\": logging_dir,\r\n    \"log_on_each_node\": True,\r\n    \"report_to\": \"wandb\",\r\n}\r\n\r\ntraining_args = TrainingArguments(**training_args)\r\n\r\nprint(\"Starting training.\")\r\n\r\nwandb.init(\r\n    project=\"geneformer_multinode_project\",\r\n    name=\"geneformer_multinode\",\r\n    tags=[\"2_node\"],\r\n    group=\"geneformer_group\",\r\n)\r\n\r\n\r\n# define the trainer\r\ntrainer = GeneformerPretrainer(\r\n    model=model, .....\r\n\r\n\r\n```\r\n\r\n<!--- A full traceback of the exception in the quotes below -->\r\n```shell\r\n\r\n```\r\n\r\n\r\n### Additional Files\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nWandB version: 0.16.6\r\n\r\nOS: SUSE linux\r\n\r\nPython version: 3.9.0\r\n\r\nVersions of relevant libraries: torch 2.1.2+cu118, transformers  4.39.3, accelerate 0.29.1, deepspeed 0.14.0\r\n\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nHello tnnandi,\n\nThank you for reaching out, and I hope your week is going well!\n\nRegarding your query about logging metrics across multiple nodes with wandb, here's how it generally works:\n\n\n1. **System Metrics on Multiple Nodes**: We collect system metrics on nodes that are actively logging to wandb. If you choose to log only from the rank 0 process, it will capture metrics specifically from that node. Conversely, if you log from each machine separately, you'll gather system metrics from all those nodes.\n\n\n1. **Logging Individual Processes**: To effectively track each process, you would initialize and log from each process separately using `wandb.init()` and `wandb.log()`. It's crucial to call `wandb.finish()` at the end of training to ensure all processes close correctly.\n\n\n1. **Managing Multiple Runs**: This approach will create multiple wandb runs, which can be viewed in the wandb UI. To manage and organize these runs effectively, especially across multiple experiments, you can utilize the `group` parameter during initialization. This helps you track which wandb run belongs to which experiment.\n\nHere’s a simple Python code snippet to demonstrate setting the group parameter:\n\n```python\nif **name** == \"**main**\":\n# Get args\nargs = parse_args()\n# Initialize wandb run\nrun = wandb.init(\nentity=args.entity,\nproject=args.project,\ngroup=\"DDP\" # Grouping all runs for the experiment\n)\n# Start model training with DDP\ntrain(args, run)\n```\n\nThis setup not only tracks metrics from individual nodes but also helps in debugging data distribution and monitoring individual batch metrics outside the main node.\n\nExplore the W&B App UI to view an example dashboard of metrics tracked from multiple processes. Note that there are two W&B Runs grouped together in the left sidebar. Click on a group to view the dedicated group page for the experiment. The dedicated group page displays metrics from each process separately.\n\nPlease review the documentation linked in your query for detailed guidance. Please share more about your current setup, and I can provide more targeted advice to help address any specific issues you are facing.\n\nThank you for your inquiry, and I'm here to help with any further questions!\n\nBest regards,\nJason\n"
      },
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Jason Davenport commented: \nHi Internal, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!\n"
      },
      {
        "user": "noaleetz",
        "body": "Hi @tnnandi - I have shared an update in [this](https://github.com/wandb/wandb/issues/7470#issuecomment-2814205540) github issue which I think will be helpful for 2 things you mentioned above:\n- show system metrics from all process involved in experiment\n- show console logs from all processes "
      }
    ]
  },
  {
    "issue_number": 9053,
    "title": "[Q]: Is it possible to find or reproduce wandb hyperparameters?",
    "author": "RobaNoRyooshu",
    "state": "open",
    "created_at": "2024-12-10T14:01:25Z",
    "updated_at": "2025-04-17T23:14:07Z",
    "labels": [
      "ty:question",
      "c:sweeps"
    ],
    "body": "### Ask your question\n\nHello,\n\nwe are trying to reproduce the results of an influential paper \"Why do tree-based models still outperform deep learning on tabular data?\" by Grinsztajn L., Oyallon E., and Varoquaux G.\nHowever, we have found ourselves in a situation where wandb is not available on the system we are using. \n\nIs there a script or some piece of code that can give us N random hyperparameter settings from a wandb sweep .yaml file?\n\nIs it possible to find exactly which hyperparameters were randomly used in the paper we are investigating, since hopefully the server-side sampling is reproducible given the wandb library version and .yaml files?",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hi @RobaNoRyooshu, thanks for reaching our. We cannot share any data that is not public, so I would recommend asking thr authors of the paper directly. To confirm, is [this](https://arxiv.org/abs/2207.08815) the paper you're referring to?"
      },
      {
        "user": "RobaNoRyooshu",
        "body": "Yes, that is the paper."
      },
      {
        "user": "luisbergua",
        "body": "Thanks for confirming! I would recommend checking with the authors and see if they're open to sharing the underlying data. Closing this for now, please do not hesitate to reach out if you need any other help!"
      }
    ]
  },
  {
    "issue_number": 9270,
    "title": "[Bug]: wandb.login() hangs indefinitely under certain conditions",
    "author": "profPlum",
    "state": "closed",
    "created_at": "2025-01-15T21:31:19Z",
    "updated_at": "2025-04-17T22:25:03Z",
    "labels": [
      "ty:bug",
      "a:sdk",
      "c:sdk:login"
    ],
    "body": "### Describe the bug\n\nProblem:\nIt seems that wandb.login() sometimes hangs forever. I've observed it happen under these conditions (not comprehensive):\n* Multiple independent processes are simultaneously trying to use wandb.login() with the same key & project.\n* Or when I'm training hundreds of models sequentially, it is likely to hang on one of them & obstruct the whole process.\n\nExpected Behavior:\nHandle login requests successfully, and if that is not possible due to limitations of the plan (idk if this is true) then simply raise an error.\n\nMy Setup:\n* wandb version = 0.19.3\n* OS = Linux\n* Plan: Free\n\nP.S. [wandb.login(timeout) arg](https://docs.wandb.ai/ref/python/login/) says: \"timeout | (int, optional) Number of seconds to wait for user input.\"\nI don't know what this means, but I'd like to know if this timeout also applies to waiting for the server to respond, so that a workaround could be `wandb.login(key=key, timeout=X)`?\n\n\n\n",
    "comments": [
      {
        "user": "pierrot-lc",
        "body": "I think I'm having the same issue when launching a sweep agent. The sweep starts but hangs on the first run.\n\nI'm using:\n- `wandb` = 0.19.3\n- `python` = 3.13.1\n- OS = Linux\n\nHere's the logs:\n\n```\n2025-01-16 09:13:34,146 - wandb.wandb_agent - INFO - Running runs: []\n2025-01-16 09:13:34,568 - wandb.wandb_agent - INFO - Agent received command: run\n2025-01-16 09:13:34,568 - wandb.wandb_agent - INFO - Agent starting run with config:\n\tdataset: grp\n\tmodel.norm: nodes\n2025-01-16 09:13:34,569 - wandb.wandb_agent - INFO - About to run command: python3 main.py group=our-norms mode=online dataset=grp model.norm=nodes\n2025-01-16 09:13:39,577 - wandb.wandb_agent - INFO - Running runs: ['pjh9vcx4']\n```\n\nNote that the command works if I launch it manually (`python3 main.py group=our-norms mode=online dataset=grp model.norm=nodes`)."
      },
      {
        "user": "fmamberti-wandb",
        "body": "Hi @profPlum , thank you for reaching out with your enquiry.\n\nWould you mind sharing if there is a specific reason why you are trying to run the `wandb.login()` at the begin of each run? \n\nIf you are logging to the same entity/project with the same key, you shouldn't need to re-login before every run, you should be able to run `wandb.init()` to initiate the run, and that would use your stored api key to log data.\n\nWould you mind testing running your script with `wandb.init()` only, without running `wandb.login()` first and let me know if you still see the runs hanging?\n\nIt may also be worth checking how many runs in parallels you are trying to start, as you may also be reaching the rate limits, which could cause the process to hang."
      },
      {
        "user": "profPlum",
        "body": "> Would you mind sharing if there is a specific reason why you are trying to run the `wandb.login()` at the begin of each run?\n\nI’m on a slurm cluster each individual run is independent of the others (e.g. different temporary machines, no guarantees of being run at similar times etc).\n\nSo unless you are trying to imply that once I login one time then this api key is stored in some file in my home directory **after which I will never need to login again**. Then doing so each time is required."
      }
    ]
  },
  {
    "issue_number": 753,
    "title": "Allow renaming groups of completed runs",
    "author": "exalate-issue-sync[bot]",
    "state": "open",
    "created_at": "2020-01-10T18:13:13Z",
    "updated_at": "2025-04-17T20:21:58Z",
    "labels": [
      "a:app"
    ],
    "body": "Is there any way to move a 'run' to another 'group' or changing the name of 'group'? I mistakenly set wrong group name for a run but couldn't find how to fix it!",
    "comments": [
      {
        "user": "issue-label-bot[bot]",
        "body": "Issue-Label Bot is automatically applying the label `enhancement` to this issue, with a confidence of 0.60. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/wandb/client) and [code](https://github.com/hamelsmu/MLapp) for this bot."
      },
      {
        "user": "Hyper5phere",
        "body": "I think this would be a very useful thing to have. It would be absurd to request the user the rerun his/her multiple days long training session just to get the grouping right. Some people want to keep things well organised, which is likely the very reason they started using wandb in the first place."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open 60 days with no activity."
      }
    ]
  },
  {
    "issue_number": 3760,
    "title": "[Q] Programmatically set color of run",
    "author": "tileb1",
    "state": "open",
    "created_at": "2022-06-08T18:56:32Z",
    "updated_at": "2025-04-17T15:34:21Z",
    "labels": [
      "c:sdk:public-api",
      "a:sdk"
    ],
    "body": "Hello,\r\n\r\nFollowing the doc, I was not able to find whether or not it is possible to set the color of a run programmatically: https://docs.wandb.ai/ref/app/features/panels/run-colors. From what I understand, the colors are locally scoped so I guess this is not an option?\r\n\r\nCan you confirm this? If so, would you be willing to add this to the api by specifying the pair (scope, run_id)?\r\n\r\nThanks in advance!",
    "comments": [
      {
        "user": "armanhar",
        "body": "Hi! This is not possible at the moment. I'll file a ticket for your request"
      },
      {
        "user": "tileb1",
        "body": "Hey @armanhar, thanks for the reply! Can you let me know once you have a timeline for this feature? Thanks"
      },
      {
        "user": "armanhar",
        "body": "@tileb1 sure, I'll notify you when there are updates on this."
      }
    ]
  },
  {
    "issue_number": 3764,
    "title": "[Q] Private project, public run",
    "author": "EmilienDupont",
    "state": "open",
    "created_at": "2022-06-09T14:32:21Z",
    "updated_at": "2025-04-17T14:40:03Z",
    "labels": [
      "a:app"
    ],
    "body": "Hi, \r\n\r\nWe used wandb for a research project and as part of publishing the paper we would like to make the runs we used in the paper public. However, from searching it seems like if we want to make a run public, we need to make the entire project public (including runs which are not relevant to the paper). Is this correct? Is there any way to make a single run within a private project public?\r\n\r\nThank you!",
    "comments": [
      {
        "user": "ramit-wandb",
        "body": "Hey Emilien,\r\n\r\nNo, you can not make a single run public while keeping the project private. You can, however create a [Report](https://docs.wandb.ai/guides/reports/reports-walkthrough), add information to it and then share that report with users who do not have access to the project.\r\n\r\nThanks,\r\nRamit"
      },
      {
        "user": "EmilienDupont",
        "body": "Hey Ramit,\r\n\r\nThank you for your reply! Is there any plan to add this feature? \r\n\r\nIn my experience, this workflow is super common in ML research (at least at universities): \r\n\r\n1. You work on a private wandb project for your research\r\n2. When you are ready to publish you want to share all the trained models from the paper, without having to share all the failed experiments with bugs or potential follow up experiments that are not yet public\r\n\r\nSo I think adding this feature would be very useful to a large number of PhD students :)\r\n\r\nIn the meantime, what is the best option for us? Can we copy runs to a new project (including trained model weights and artifacts) and then make this public? Or is the only option to make the private project public and delete all the unwanted runs?\r\n\r\nThank you again for your help!"
      },
      {
        "user": "ramit-wandb",
        "body": "Hi Emilien,\r\n\r\nThank you for your response! I'm curious - did the report method not work for you? You should be able to select specific runs without exposing your whole private project. I would like to understand what did not work for you so that I can make a feature request that better resolves this problem.\r\n\r\nThanks,\r\nRamit"
      }
    ]
  },
  {
    "issue_number": 3366,
    "title": "[Feature]: Stop Training from wandb API",
    "author": "dreamflasher",
    "state": "open",
    "created_at": "2022-03-11T18:15:34Z",
    "updated_at": "2025-04-16T17:45:29Z",
    "labels": [
      "ty:feature",
      "c:sdk:internal-process",
      "c:sdk:public-api",
      "a:sdk"
    ],
    "body": "### Description\r\n\r\nWe'd like to be able to stop a training run from wandb from the API. \r\n\r\n### Suggested Solution\r\n\r\nAdd a button to the web UI \"stop run\" and add a flag to each run \"stop requested\", expose via API to set this flag. Every time a wandb function is called for that run (ie. training logging) it checks if \"stop requested\" is true, and if yes terminates the current process (or raise an exception that can be handled by the training code, ideally make it an option if it raises an exception or just terminates that code).\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "nate-wandb",
        "body": "Hi @dreamflasher,\r\nThank you for the feature request! Stopping the run via the UI can be done by going to the run overview tab in the UI and clicking the stop icon next to the run state:\r\n![Screen Shot 2022-03-14 at 10 10 35 AM](https://user-images.githubusercontent.com/97066933/158215583-0dbddce1-517d-40b6-969b-69a264181f6e.png)\r\nLet me know if this differs from what you were looking for.\r\n\r\nThe rest of your feature request does not currently exist so I will pass this onto our engineering team for you and let you know if we make any progress towards this.\r\n\r\nThank you,\r\nNate"
      },
      {
        "user": "dreamflasher",
        "body": "That's exactly what I have been looking for – in the API. Do you expose this feature in the API?"
      },
      {
        "user": "nate-wandb",
        "body": "No, unfortunately stopping a run via the API isn't currently possible so I'll put in a feature request for this. The way you described the stopping process is essentially how the stop button on the UI works so I imagine that if we added support to do this through the API it would work the same way. \r\n\r\nI'll also add that you would like a way to specify how the stop is handled and if it raises an error or terminates the training script. "
      }
    ]
  },
  {
    "issue_number": 9721,
    "title": "[Bug]: x_update_finish_state not taking effect?",
    "author": "john-2pir",
    "state": "closed",
    "created_at": "2025-04-12T20:53:35Z",
    "updated_at": "2025-04-16T16:18:55Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nHello,\n\nI have a script that computes some benchmark metrics for my model, which runs in a background thread in a separate process while training continues (since it's a CPU-intensive process). I'd like to export the results to Wandb for the current training run. Training will meanwhile continue in the original process, so I want to take care not to mess up any logging that continues to happen.\n\nI have mostly figured out a way to do this:\n\n```\n# Example of inputs\nstructured_exp_scores_dict = {\"bench/my_metric\": 10}\nGLOBAL_STEP = 1000\n\nwandb.login()\n\nsettings = Settings(\n    x_disable_stats=True,\n    x_disable_meta=True,\n    x_disable_machine_info=True,\n    console=\"off\",\n    # Make sure we don't finish the run since parent training process will continue to log.\n    x_update_finish_state=False,\n)\nwandb.init(project=wandb_project_id, id=wandb_run_id, resume=\"must\", settings=settings)\n\nwandb.define_metric(\"bench/bench_step\", overwrite=False)\nwandb.define_metric(\"bench/*\", step_metric=\"bench/bench_step\", overwrite=False)\nstructured_exp_scores_dict.update({\"bench/bench_step\": GLOBAL_STEP})\nwandb.log(structured_exp_scores_dict, commit=True)\n```\n\nThe problem I'm having is that the run status seems to get updated to \"Finished\" despite setting `x_update_finish_state=False`.\n\nI understand `x_update_finish_state` is not part of the public API, but is there any reason this shouldn't work right now? Or is there a better way to accomplish this without the complexity of asynchronously reading the results of the benchmark from the parents process (which originally called `wandb.init()`) and logging them there?\n\nThanks so much!",
    "comments": [
      {
        "user": "john-2pir",
        "body": "I investigated the SDK code a bit more and came to the conclusion that writing from two separate processes for the same wandb session/run is not really supported.\n\nIt seems the only pattern that comes close is follow the docs [here](https://docs.wandb.ai/guides/track/log/distributed-training/) for setting things up as if they're distributed training. In my case, that would mean logging any of the benchmark metrics as their own run but then grouping them with the training metrics using the `group` param.\n\nSuch a solution is even more complicated than the alternative I was imagining of simply logging the benchmark results asynchronously to a file, watching for filesystem updates in the main training thread, and uploading those metrics in the existing wandb session in the main training thread, so I ultimately went with that approach instead. I prefer that a lot because then it means I don't need to ever worry about UI grouping.\n\nIf there is a better way to accomplish what I was intended, it would be great to know for reference. Thanks so much :)\n\nI will leave this bug open because I'm not sure if `x_update_finish_state` is actually working as intended or not. What I observed is that if I tried my approach with a run that previously had a state of \"Killed\" or \"Failed\", the status would be updated to \"Finished\" (successfully) in the UI, even when I set this option. In fact, the SDK didn't seem to even reach the place `x_update_finish_state` is checked, so it's possible I misunderstood how this flag is intended to work (maybe it only applies to sigint or sigkill interruptions?)."
      },
      {
        "user": "noaleetz",
        "body": "Hi @john-2pir - thanks for writing in. I'm the PM for our SDK team. We have a new setting that is available that should let you do exactly that: multiple processes writing to the same run. \n\nWe're rolling this out to public preview, and that will include an update to the docs you linked above with the new (preview/experimental) option for how to do distributed training with a better logging pattern, but in the meantime I can shared a code snippet below:\n\n### How It Works:\n- **`mode=\"shared\"`**  \n  Tells W&B that multiple processes are intentionally logging to the same run.\n- **`x_primary=True`** (Primary Node) - defined [here](https://github.com/wandb/wandb/blob/cfc0ab112bd9f19e26eebd3699166d44cdf5705c/wandb/sdk/wandb_settings.py#L660C5-L660C14)\n  This node is responsible for finalizing the run state, uploading shared config, etc.  \n  Typically, exactly **one** process (often rank=0 in distributed training) sets `x_primary=True`.\n- **`x_primary=False`** (Worker Nodes) - defined [here](https://github.com/wandb/wandb/blob/cfc0ab112bd9f19e26eebd3699166d44cdf5705c/wandb/sdk/wandb_settings.py#L660C5-L660C14)\n  Worker processes still log metrics to the run but do **not** perform finalizing actions.\n- **`x_update_finish_state=False`**  - defined [here](https://github.com/wandb/wandb/blob/cfc0ab112bd9f19e26eebd3699166d44cdf5705c/wandb/sdk/wandb_settings.py#L766)\n  Ensures worker nodes do not mark the run as finished when they call `.finish()`.  \n  Only the primary node can finalize the run in W&B.\n\n### Process 1: Primary Node\n\n```python\nimport wandb\n\nprimary_run = wandb.init(\n    project=\"my-project\",\n    id=SHARED_RUN_ID,  # if generated, be sure it is passed down to worker processes\n    settings=wandb.Settings(\n        mode=\"shared\",    # Multiple processes write to the same run\n        x_primary=True    # This node can finalize the run\n    )\n)\n\n# Log some training metrics...\nwandb.log({\"primary_metric\": 0.123})\n\n# When training completes, only the primary node actually finalizes the run:\nprimary_run.finish()\n```\n\n\n### Process 2: Other Nodes\n\n```python\nimport wandb\n\nworker_run = wandb.init(\n    project=\"my-project\",\n    id=\"shared_run_id\",  # Must match the primary node's run ID\n    settings=wandb.Settings(\n        mode=\"shared\",\n        x_primary=False,             \n        x_update_finish_state=False  # Avoid setting the run to \"finished\"\n    )\n)\n\n# Log some metrics...\nwandb.log({\"worker_metric\": 0.456})\n\n# The worker node can still call finish, but it won't finalize the run:\nworker_run.finish()\n```\n\nTLDR: Use shared mode and designate only one process as “primary,” so it’s the only one allowed to finalize the run."
      },
      {
        "user": "john-2pir",
        "body": "Hey, that's awesome! It's exactly what I needed. Thank you for sharing the code. I will try it out when I have a chance.\n\nI am going to close the issue for now since this resolves my problem, but if I do get a chance to give this a try and have feedback, I will reach out to let you know. Again, I really appreciate it!"
      }
    ]
  },
  {
    "issue_number": 5857,
    "title": "[Q] Is it possible to disable everything except system metrics?",
    "author": "markovalexander",
    "state": "open",
    "created_at": "2023-07-07T15:07:00Z",
    "updated_at": "2025-04-14T23:04:40Z",
    "labels": [
      "a:sdk",
      "c:sdk:system-metrics"
    ],
    "body": "Trying to log multinode experiments calling `wandb.init` only in main process results into logging system metrics only from the master node. \r\n\r\nI want to call `wandb.init` on all nodes (with the same `group`), but disable everything (including console capturing, since output.log is not synced properly with multinode experiment) but system metrics (like GPU usage).\r\n\r\nI tried to find the correct options in [settings](https://github.com/wandb/wandb/blob/1ad7e2fbe4964381cd41b954801d2039fbc28326/wandb/proto/wandb_settings.proto#L26), but I don't understand what should I use (there is no documentation on settings at all). Can you please help me? ",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hi @markovalexander to double-check, you are interested in seeing system metrics off of every single node instead of just the main one right?"
      },
      {
        "user": "markovalexander",
        "body": "Hi @ArtsiomWB ! Yes, exactly.  I want all metrics (except system) and console output to be logged from first node first process only, and system metrics from all the nodes "
      },
      {
        "user": "ArtsiomWB",
        "body": "Hi @markovalexander! Apologies it took a while to get back to you, I've been checking internally with our engineering team, and unfortunately as of right now, there is no way to customize what gets logged and what doesn't. If you would like to I would love to submit a feature request for you, because this does sound like a useful feature to have.  "
      }
    ]
  },
  {
    "issue_number": 4645,
    "title": "[Feature]: Move single or selected multiple run to new or existing group",
    "author": "omerferhatt",
    "state": "open",
    "created_at": "2022-12-15T15:20:48Z",
    "updated_at": "2025-04-14T15:45:13Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "### Description\n\nA run that is not in any group cannot be moved to a new or existing group. \n\n### Suggested Solution\n\nI should be able to put or remove a run from a group I want. If necessary, I should be able to lock the group and prevent it from changing. For example, only the user who did the run should have the authority to move or break the new group. Since this is the only categorization feature under the project, this needs to be done quickly through the interface. Writing code for this is pure nonsense.\n\n### Alternatives\n\n_No response_\n\n### Additional Context\n\n_No response_",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hi @omerferhatt, thanks for suggesting this! So I can see two different feature request here:\r\n1.  **Put/remove a run from a group**. Since the group is done based in the values of a column from the Run Tables, then the request would be to be able to modify this value from the UI, is it?\r\n2. **Lock the group**. When something is modified in the UI, this is only visible in the concrete user's Workspace (the user can be changed in the url `https://wandb.ai/team/project/overview?workspace=user-username` by changing the username) so if runs are grouped with `wandb.init(group=group_name)` changes will only affect a concrete user. The extra feature that you'd like to have would be, after grouping some runs (both from `wandb.init()` or the UI), lock this and avoid everybody except from the user to modify this?\r\n\r\nPlease let me know if I'm capturing your feedback correctly or if I'm missing something!"
      },
      {
        "user": "luisbergua",
        "body": "Hi @omerferhatt, I just wanted to follow up here! Would you mind explaining me if the two features that I explained in my previous message captured your feedback properly or am I misunderstanding something? Thanks! "
      },
      {
        "user": "omerferhatt",
        "body": "Hello @luisbergua,\r\n\r\nYes actually it would be better to have two correct feature requests. But the second one was a feature I thought of as optional, what we really need and need are editable groups."
      }
    ]
  },
  {
    "issue_number": 5654,
    "title": "[Feature]: Allow Renaming of Groups in the browser interface",
    "author": "PhilippDahlinger",
    "state": "open",
    "created_at": "2023-06-02T07:55:05Z",
    "updated_at": "2025-04-14T15:44:53Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "### Description\n\nI would like to rename a group name in the browser interface, in the same way that I can rename a run name.\n\n### Suggested Solution\n\nAdd a small menu option left to the group name when hovering over it, then add the option \"rename group\". \n\n### Alternatives\n\nThere are as far as I can tell options to use the wandb.Api, however this is not practical und cumbersome.\n\n### Additional Context\n\nThere exists already an issue (https://github.com/wandb/wandb/issues/753) which asks for the exact same feature. However, it is closed. Since there are still a lot of people asking for this feature, I reopen a new issue to draw attention to this problem.",
    "comments": [
      {
        "user": "umakrishnaswamy",
        "body": "Hello @PhilippDahlinger,\r\n\r\nThank you for bringing this up! Our eng team is aware of this request, and I've bumped it again for more visibility. Will let you know when progress is made!\r\n"
      },
      {
        "user": "jalkestrup",
        "body": "+1"
      },
      {
        "user": "Rowing0914",
        "body": "+1"
      }
    ]
  },
  {
    "issue_number": 9712,
    "title": "[Feature]: Improve error message \"It appears that you do not have permission to access the requested resource\" to include cases where resource doesn't exist",
    "author": "zlenyk",
    "state": "open",
    "created_at": "2025-04-11T09:58:15Z",
    "updated_at": "2025-04-14T10:58:40Z",
    "labels": [
      "ty:feature"
    ],
    "body": "### Description\n\nWhen I'm trying to download an artifact that doesn't exist (for example I provided a wrong path) I'm still getting a message: \"It appears that you do not have permission to access the requested resource\".\n\nUsually it makes me double-check my project permissions, API key and after ~30s I realize that I just input the wrong path.\n\n\n### Suggested Solution\n\nCould we write something like:\n\"It appears that you do not have permission to access the requested resource or the resource doesn't exist\"\n",
    "comments": [
      {
        "user": "fmamberti-wandb",
        "body": "Hi @zlenyk , thanks for reaching out. I've raised a feature request with our product team."
      }
    ]
  },
  {
    "issue_number": 2143,
    "title": "[App] cannot view logged video in run stats",
    "author": "pragyakale",
    "state": "closed",
    "created_at": "2021-04-30T12:13:33Z",
    "updated_at": "2025-04-13T11:17:10Z",
    "labels": [
      "a:app"
    ],
    "body": "**Describe the bug**\r\nVideo file type is not recognized correctly\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Log a video using \r\n`wandb.log(\r\n  {\"video\": wandb.Video(numpy_array_or_path_to_video, fps=4, format=\"gif\")})`\r\n2. Go to run>files>Media on the browser\r\n\r\n**Expected behavior**\r\nExpected to see the .mp4 video\r\n\r\n**Screenshots**\r\n\r\n![Screenshot from 2021-04-30 14-10-47](https://user-images.githubusercontent.com/14981373/116693336-110ee000-a9be-11eb-857c-fa285ab1ac06.png)\r\n\r\n![Screenshot from 2021-04-30 14-11-07](https://user-images.githubusercontent.com/14981373/116693354-179d5780-a9be-11eb-9f01-592f1156c870.png)\r\n\r\n\r\n**Operating System**\r\n - OS: Ubuntu\r\n - Browser: Chrome\r\n\r\n**Additional context**\r\nWhen I download and play the logged file it plays correctly.\r\n",
    "comments": [
      {
        "user": "cvphelps",
        "body": "@pragyakale It looks like we don't have the right codec hooked up for MP4s yet, thanks for reporting this!"
      },
      {
        "user": "NotAnyMike",
        "body": "Hi @cvphelps, any idea when will this be fixed?"
      },
      {
        "user": "vanpelt",
        "body": "@NotAnyMike, it looks like you're logging an \"mp4\" video but telling us it's a \"gif\" video.  Can you try logging with `format=\"mp4\"` instead of `format=\"gif\"`?"
      }
    ]
  },
  {
    "issue_number": 9719,
    "title": "[Bug-App]: Bad error message",
    "author": "NeilGirdhar",
    "state": "closed",
    "created_at": "2025-04-11T21:23:43Z",
    "updated_at": "2025-04-11T22:01:30Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\nWhen using `tune.run` on a closure (a local function containing values that are _closed over_), Ray gives a cryptic pickling error.  The objective function needs to be global, which means closures aren't supported.",
    "comments": []
  },
  {
    "issue_number": 9699,
    "title": "[Bug-App]: Deleting a run sometimes causes another run to disappear temporarily on the web UI",
    "author": "HovChen",
    "state": "closed",
    "created_at": "2025-04-09T02:35:21Z",
    "updated_at": "2025-04-11T14:30:23Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\nWhen using the W&B web interface, deleting a run sometimes causes a different run to temporarily disappear from the run list. However, after refreshing the page, the missing run reappears.\n\nEnvironment\n•\tBrowser:Chrome 133.0.6943.142\n•\tOS: macOS 15.0.1",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hi @HovChen, thanks for flagging this! Would you mind sharing how exactly you're deleting your run? Is it from the Runs Table, or in the main workspace? Also, is this affecting all projects regardless the number of runs they have, or just a specific one? If you could share a video showing this behavior that would be really helpful to try and reproduce this on our side "
      },
      {
        "user": "HovChen",
        "body": "Hi @luisbergua , thanks for your quickly reply. I encountered the problem when I was in this page:\n<img width=\"1468\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b9c3b339-68d1-4132-b0c9-cfce0ce02a71\" />\nI discovered the problem when I deleted my run from the run table. However, when I have just tried to reproduce this several times, the problem has not happened again, so I guess it may have happened randomly. It may also be useful to note that the run I deleted was faulty (did not complete properly), hopefully this will help you. If I encounter the same problem in the future and find the rule, I will feedback the problem as much as possible, thank you very much for the very useful tool!"
      },
      {
        "user": "luisbergua",
        "body": "Hey @HovChen, thanks for your answer! I also did some tests on my end but was unable to reproduce this, so it might have been a transient problem. If you see this in the future, sharing the browser Network and Console logs would be really helpful for us to see what's happening behind the scenes. Will close this for now!"
      }
    ]
  },
  {
    "issue_number": 9615,
    "title": "[Bug]: Early Terminate does not seem to work",
    "author": "Towdo",
    "state": "closed",
    "created_at": "2025-03-24T09:19:21Z",
    "updated_at": "2025-04-11T14:01:10Z",
    "labels": [
      "ty:bug",
      "c:sweeps"
    ],
    "body": "### No stopped runs with early_terminate set\n\nDear maintainers,\n\nit seems like the `early_terminate` parameter of the sweep config doesn't work at all.\nRunning [sweep_check.py](https://github.com/wandb/wandb/blob/main/landfill/standalone_tests/sweep_check.py) with `python3 ./sweep_check.py --test grid_hyper --grid_hyper_delay 30` results in 9 full runs.\n\n<img width=\"364\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/05a9115f-ffff-4224-a459-561e23324c08\" />\n\nI'm using:\nwandb 0.19.6\nPython 3.10.16\nAlmaLinux 9.2",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @Towdo, Thanks for reaching out. I have been working on reproducing this issue on my end. I will reach back once I have more updates. "
      },
      {
        "user": "Towdo",
        "body": "Hey @aajais, thank you for having a look at my problem!\n\nI could reproduce this on a different machine in a fresh conda environment running:\nwandb 0.19.8\nPython 3.13.2\nUbuntu 24.04\n\nBut I forgot to add one possibly crucial detail to the original issue.\nFor `sweep_check.py` to run, I had to add `entity = MYENTITY` in line 193, because I would otherwise get the following error:\n\n```\nTesting: grid_hyper\nwandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n403 response executing GraphQL.\n{\"errors\":[{\"message\":\"permission denied\",\"path\":[\"upsertSweep\"],\"extensions\":{\"code\":\"PERMISSION_ERROR\"}}],\"data\":{\"upsertSweep\":null}}\nwandb: ERROR Error while calling W&B API: permission denied (<Response [403]>)\nwandb: ERROR permission denied\nTraceback (most recent call last):\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/sdk/lib/retry.py\", line 134, in __call__\n    result = self._call_fn(*args, **kwargs)\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/sdk/internal/internal_api.py\", line 398, in execute\n    return self.client.execute(*args, **kwargs)  # type: ignore\n           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 52, in execute\n    result = self._get_result(document, *args, **kwargs)\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 60, in _get_result\n    return self.transport.execute(document, *args, **kwargs)\n           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/sdk/lib/gql_request.py\", line 59, in execute\n    request.raise_for_status()\n    ~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://api.wandb.ai/graphql\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/towdo/Code/wandb/./sweep_check.py\", line 258, in <module>\n    main()\n    ~~~~^^\n  File \"/home/towdo/Code/wandb/./sweep_check.py\", line 254, in main\n    f(args)\n    ~^^^^^^\n  File \"/home/towdo/Code/wandb/./sweep_check.py\", line 193, in sweep_grid_hyperband\n    sweep_id = wandb.sweep(config, project=PROJECT)\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/sdk/wandb_sweep.py\", line 86, in sweep\n    sweep_id, warnings = api.upsert_sweep(sweep, prior_runs=prior_runs)\n                         ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/apis/internal.py\", line 137, in upsert_sweep\n    return self.api.upsert_sweep(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/apis/normalize.py\", line 65, in wrapper\n    raise err\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/apis/normalize.py\", line 25, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/sdk/internal/internal_api.py\", line 3351, in upsert_sweep\n    raise e\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/sdk/internal/internal_api.py\", line 3345, in upsert_sweep\n    response = self.gql(\n        mutation,\n        variable_values=variables,\n        check_retry_fn=util.no_retry_4xx,\n    )\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/sdk/internal/internal_api.py\", line 370, in gql\n    ret = self._retry_gql(\n        *args,\n        retry_cancel_event=self.context.cancel_event,\n        **kwargs,\n    )\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/sdk/lib/retry.py\", line 150, in __call__\n    retry_timedelta_triggered = check_retry_fn(e)\n  File \"/home/towdo/miniforge3/envs/wandb/lib/python3.13/site-packages/wandb/util.py\", line 855, in no_retry_4xx\n    raise UsageError(body[\"errors\"][0][\"message\"])\nwandb.errors.errors.UsageError: permission denied\n```\n\nIf I can provide you with anything to help me fix this, please don't hesitate to ask.\nBeing able to early terminate runs would help my workflow a lot!"
      },
      {
        "user": "aajais",
        "body": "@Towdo, thanks so much for the detailed info! I was able to reproduce the error on my end as well and have logged it as a bug. I’ll keep you posted as we make progress on a fix. Really appreciate your patience in the meantime!"
      }
    ]
  },
  {
    "issue_number": 9701,
    "title": "[Q]: Unable to dynamically set run name using `wandb.config` during sweep – access before `wandb.init()` causes error",
    "author": "pvti",
    "state": "closed",
    "created_at": "2025-04-09T08:39:22Z",
    "updated_at": "2025-04-09T19:52:51Z",
    "labels": [
      "ty:question"
    ],
    "body": "<!--- Ask your question here --->\nHi W&B team 👋\n\nI'm running a sweep using `wandb.agent`, and I want to assign a custom name to each run based on its sweep parameters (e.g., batch size and learning rate). Here's what I'm trying to do in my script:\n\n`def main():\n    wandb.init(name=f\"{args.approach} bs={wandb.config.bs} lr={wandb.config.lr}\")\n`\nHowever, this throws an error because `wandb.config` is not accessible before `wandb.init()`. I understand why that happens, but I can’t see an alternative way to dynamically name my runs based on the sweep parameters.\n`wandb.errors.errors.Error: You must call wandb.init() before wandb.config.bs\n`\nFull Traceback:\n```\nRun lx69hwpe errored:\n  File \"/home/van-tien.pham/anaconda3/lib/python3.12/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/home/van-tien.pham/flex/scratch_sweep.py\", line 193, in main\n    wandb.init(name=f\"{args.approach} bs={wandb.config.bs} lr={wandb.config.lr}\")\n                                          ^^^^^^^^^^^^^^^\n  File \"/home/van-tien.pham/anaconda3/lib/python3.12/site-packages/wandb/sdk/lib/preinit.py\", line 27, in __getattr__\n    raise wandb.Error(f\"You must call wandb.init() before {self._name}.{key}\")\nwandb.errors.errors.Error: You must call wandb.init() before wandb.config.bs\n\n```\n\nSimplified Explanation of What I'm Doing\n```\nsweep_configuration = {\n    \"method\": \"grid\",\n    \"name\": \"sweep\",\n    \"metric\": {\"goal\": \"maximize\", \"name\": \"top1\"},\n    \"parameters\": {\n        \"bs\": {\"values\": [16, 32, 64]},\n        \"lr\": {\"values\": [1e-6, 1e-5, 1e-4]},\n    },\n}\n\nsweep_id = wandb.sweep(sweep_configuration, project=\"Flex Sweep\")\nwandb.agent(sweep_id, function=main)\n\n```\nAnd in `main()`, I want to customize the run name based on the sweep parameters, like this:\n`wandb.init(name=f\"{args.approach} bs={wandb.config.bs} lr={wandb.config.lr}\")\n`\n\nBut this fails because `wandb.config` isn’t available until after `wandb.init()` is called.\n\nFeature Request / Question\n\nIs there a clean way to name runs after `wandb.init()` is called using values from `wandb.config`?\n\nSomething like this would be ideal:\n```\nrun = wandb.init()\nrun.name = f\"{args.approach} bs={run.config.bs} lr={run.config.lr}\"\nrun.save()\n\n```\n\nIf this isn’t possible currently, could W&B support setting the run name after `wandb.init()`? It would be very helpful for organizing and visualizing sweep results.\n\nThanks so much for your time and for making such a great tool 🙌",
    "comments": [
      {
        "user": "pvti",
        "body": "With ChatGPT,  I resolved the issue by separating the two steps:\n```\nwandb.init()  # Must come first\n\n# Then safely access wandb.config and set the run name\nwandb.run.name = f\"{args.approach} bs={wandb.config.bs} lr={wandb.config.lr}\"\n```\n\nThis works as expected and correctly assigns names for each sweep run using the sweep-config values.\n"
      },
      {
        "user": "ArtsiomWB",
        "body": "Thank you for the follow up!\n\nIf you're all set for now, I will close this ticket for tracking purposes. If you have any follow-up questions, feel free to add them here, and it will reopen the thread "
      }
    ]
  },
  {
    "issue_number": 9589,
    "title": "[Q]: What can I do to prevent YOLO from saving my model?",
    "author": "Cestovatels",
    "state": "closed",
    "created_at": "2025-03-17T12:47:27Z",
    "updated_at": "2025-04-09T16:09:05Z",
    "labels": [
      "ty:question",
      "c:sdk:integration"
    ],
    "body": "### Ask your question\n\nI activated wandb in my YOLO code. I don't want it to send best.pt and last.pt models to wandby. I don't even want to send some things in the media folder. I couldn't figure out how to do this. \nMy code construction is as follows;\n\n```py\nimport wandb\nfrom ultralytics import YOLO, settings\n\nsettings.update({'wandb': True})\nwandb.login(key=\"*****************...\")\n\nmodel = YOLO(\"Yolov8x.pt\")\nmodel.train(data=\"data.yaml\", epochs=epochs, imgsz=imgsz, project=\"training_name\", name=\"training_name\", batch=batch, patience=patience, device=devices, overlap_mask=overlap_mask, fliplr=fliplr)\n```\nThanks for answer.\n",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @Cestovatels, Thanks for reaching out! This seems like a question best directed to the Ultralytics team, as it looks like they hard code this behavior in their [code](https://github.com/ultralytics/ultralytics/blob/477412cce9b7125a0e14b3ea84998c71217de0a0/ultralytics/utils/callbacks/wb.py#L137). That said, could you share a link to an example run where you’re encountering this issue? I’d be happy to take a closer look and see if there’s anything we can do on our end."
      },
      {
        "user": "aajais",
        "body": "@Cestovatels, just wanted to follow up on this Thanks!"
      },
      {
        "user": "aajais",
        "body": "@Cestovatels, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved."
      }
    ]
  },
  {
    "issue_number": 9620,
    "title": "[Q]: How do I load existing wandb runs on my hard disk into wandb local?",
    "author": "richardrl",
    "state": "closed",
    "created_at": "2025-03-24T22:09:48Z",
    "updated_at": "2025-04-09T16:01:27Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\nHow can I load a local wandb folder, with the following data:\n\n![Image](https://github.com/user-attachments/assets/5c697b67-2698-4c02-a4be-c580d370e1c7)\n\ninto the web interface from: `wandb local`?\n\nthere must be a place to point the offline file into the web interface",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @richardrl, Thanks for reaching out! Could you share a bit more about what you’re trying to do? Are you looking to sync an offline run to the W&B web interface? And just to clarify, are you using a local server where you’d like to push that offline run?\n\nIf so, you can use the [wandb sync](https://docs.wandb.ai/ref/cli/wandb-sync/) CLI command to upload the run. Feel free to include any flags that best match your setup. Happy to help further once I know a bit more!"
      },
      {
        "user": "richardrl",
        "body": "@aajais When we do wandb local, it starts up a local server. But it doesnt load any runs. I want to load existing runs into it"
      },
      {
        "user": "aajais",
        "body": "@richardrl Got it. Could you let me know which version of the SDK you’re currently using? \n\nEdit: Also, just to make sure everything’s set up correctly, the recommended workflow is to first start your server and get it fully configured. Then, log in to that server using: `wandb login --relogin --host=your_host_url` This ensures you’re authenticated with the right instance. Once you’re logged in, you can start logging runs to this server. If you’ve logged any runs in offline mode, you can sync them later using the [wandb sync](https://docs.wandb.ai/ref/cli/wandb-sync/) CLI command"
      }
    ]
  },
  {
    "issue_number": 8971,
    "title": "[Bug]: One bug caused by the recent upgrade of Moviepy to version 2.0.",
    "author": "jypjypjypjyp",
    "state": "closed",
    "created_at": "2024-11-29T11:28:44Z",
    "updated_at": "2025-04-09T03:25:22Z",
    "labels": [
      "ty:bug",
      "a:sdk"
    ],
    "body": "### Describe the bug\n\nI encountered this error when using wandb.video. I executed the installation commands according to the error message, but the problem still hasn't been solved. I presume the reason is that Moviepy has been recently updated to version 2.0, and the module name has been changed from moviepy.editor to moviepy, so there is no need to add \"editor\" anymore.\n\n```\nmpy = util.get_module(\n            \"moviepy.editor\",\n            required='wandb.Video requires moviepy when passing raw data.  Install with \"pip install wandb[media]\"',\n        )\n```\n\n\nThe version of the wandb package you are using.\n0.18.7\nThe version of Python you are using.\n3.9.19\nThe version of the operating system you are using. \nUbuntu 18",
    "comments": [
      {
        "user": "thanos-wandb",
        "body": "Hi @jypjypjypjyp thank you for reporting this issue, this is indeed a compatibility issue that occurs with newer versions of MoviePy (2.0+), due to the change in their import structure. We've fixed the issue (relevant PR [here](https://github.com/wandb/wandb/pull/8931)) and will be available in our next SDK release.\n\nIn the meantime, you could pin MoviePy to version 1.0.3 using: `pip install -U moviepy==1.0.3`\n\nApologies for any disruption this has caused."
      },
      {
        "user": "basilevh",
        "body": "This bug still exists, when will it be fixed? Thanks!"
      }
    ]
  },
  {
    "issue_number": 9676,
    "title": "[Bug]:",
    "author": "Paul-Hubert",
    "state": "closed",
    "created_at": "2025-04-03T11:58:40Z",
    "updated_at": "2025-04-09T00:43:47Z",
    "labels": [
      "ty:bug",
      "a:sdk",
      "c:sdk:settings"
    ],
    "body": "### Describe the bug\n\nHi, I'm getting an error in wandb version 0.19.9.\n\n```\nTraceback (most recent call last):\n\n------------MY CUSTOM CODE---------------------\n  File \"c:\\users\\paul boursin\\desktop\\dynamical\\ml-agents\\ml-agents\\mlagents\\plugins\\stats_writer.py\", line 64, in register_stats_writer_plugins\n    plugin_stats_writers = plugin_func(run_options)\n  File \"c:\\users\\paul boursin\\desktop\\dynamical\\ml-agents\\ml-agents\\mlagents\\plugins\\stats_writer.py\", line 39, in get_default_stats_writers\n    WandbWriter(run_options)\n  File \"c:\\users\\paul boursin\\desktop\\dynamical\\ml-agents\\ml-agents\\mlagents\\trainers\\stats.py\", line 302, in __init__\n    wandb.setup(wandb.Settings(program=__name__, program_relpath=__name__))\n\n------------WANDB PYTHON CODE-----------------\n  File \"C:\\Users\\Paul Boursin\\anaconda3\\envs\\env\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py\", line 383, in setup\n    return _setup(settings=settings)\n  File \"C:\\Users\\Paul Boursin\\anaconda3\\envs\\env\\lib\\contextlib.py\", line 75, in inner\n    return func(*args, **kwds)\n  File \"C:\\Users\\Paul Boursin\\anaconda3\\envs\\env\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py\", line 319, in _setup\n    _singleton = _WandbSetup(settings=settings, pid=pid)\n  File \"C:\\Users\\Paul Boursin\\anaconda3\\envs\\env\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py\", line 96, in __init__\n    self._settings = self._settings_setup(settings)\n  File \"C:\\Users\\Paul Boursin\\anaconda3\\envs\\env\\lib\\site-packages\\wandb\\sdk\\wandb_setup.py\", line 130, in _settings_setup\n    s.update_from_system_environment()\n  File \"C:\\Users\\Paul Boursin\\anaconda3\\envs\\env\\lib\\site-packages\\wandb\\sdk\\wandb_settings.py\", line 1532, in update_from_system_environment\n    self.program_relpath = self.program_relpath or self._get_program_relpath(\n  File \"C:\\Users\\Paul Boursin\\anaconda3\\envs\\env\\lib\\site-packages\\wandb\\sdk\\wandb_settings.py\", line 1676, in _get_program_relpath\n    relative_path = os.path.relpath(full_path_to_program, start=root)\n  File \"C:\\Users\\Paul Boursin\\anaconda3\\envs\\env\\lib\\ntpath.py\", line 703, in relpath\n    raise ValueError(\"path is on mount %r, start on mount %r\" % (\nValueError: path is on mount '\\\\\\\\?\\\\C:', start on mount 'C:'\n```\n\nLooking at the code, the problem seems to be in wandb_settings.py:1669\nI get :\nfull_path_to_program=`\\\\?\\C:\\Users\\Paul Boursin\\anaconda3\\envs\\env\\Scripts\\mlagents-learn-script.py`\n\nI downgraded to 0.18.5 which is what I was previously using, and it works nicely.\n\nI don't think I need to upgrade right now, so this bug report is not urgent for me.\n\nI can provide more info if needed.\n\nCheers",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey Paul!\n\nThank you for writing in. Are you able to share the toy code example with us to so I can try and reproduce this on my side?\n\nLooks like you are running this on windows? Could you please provide the debug.log and debug-internal.log files associated with the run where you are running into this issue? These files should be located in the wandb folder relative to your working directory. \n"
      },
      {
        "user": "Paul-Hubert",
        "body": "Thanks.\n\nIt crashes on this call (the first wandb call, so I don't think the rest of the code is necessary here) :\n`wandb.setup(wandb.Settings(program=__name__, program_relpath=__name__))`\nwith\n`__name__=mlagents.trainers.stats`\nMight be a problem with how I'm passing the program name.\n\nI made a custom wandb plugin for mlagents, if you're interested in the full code.\n\n[debug.log](https://github.com/user-attachments/files/19589567/debug.log)\n[debug-internal.log](https://github.com/user-attachments/files/19589566/debug-internal.log)"
      },
      {
        "user": "jacobromero",
        "body": "Hi @Paul-Hubert thanks for reporting this issue!\nThis is indeed an issue on our side, I'm looking into how we want to handle this case better.\n\nThe issue is occurring because it appears you are running the program from a different drive than where the program is located on. In your case it looks like the program file is located on a network drive possibly (due to the `\\\\` path prefix on windows).\n\nThe root cause on our side is related to a change in how we are applying settings across various locations (e.g. environment variables, user provided settings with `wandb.Settings(...)`, etc.). \nWhich we would attempt to infer the running program's relative path to the current working directory the program was ran from first before using the user provided `program_relpath` setting.\n\nThe specific change was made in v0.19.0, and should be limited to only when running on windows from a different drive than where the program is located. In the event that you do need to upgrade sooner you can possibly mitigate this issue but running the program from the same drive/root as where the program is located, if possible.\n"
      }
    ]
  },
  {
    "issue_number": 9425,
    "title": "[Bug]: Logging does not update progress bar",
    "author": "francois-rozet",
    "state": "closed",
    "created_at": "2025-02-06T14:35:20Z",
    "updated_at": "2025-04-08T23:13:48Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\nThe logs interface does not update `tqdm` progress bars.\n\n### Reproducing\n\nWith the following code,\n\n```python\nimport wandb\nimport tqdm\nimport time\n\nrun = wandb.init(project=\"tqdm-test\")\n\nfor i in tqdm.tqdm(range(1000), ncols=88, ascii=True):\n    run.log({\"acc\": i / 1000})\n    time.sleep(10)\n\nrun.finish()\n```\n\nthe progress bar looks like\n\n```\n  1%|6                                              | 13/1000 [02:10<2:44:31, 10.00s/it]\n```\n\nBut the logs tab in the web interface always shows\n\n```\n1   0%|                                                          | 0/1000 [00:00<?, ?it/s] \n```\n\n> See https://wandb.ai/francois-rozet/tqdm-test/runs/rlqhqf9v/logs",
    "comments": [
      {
        "user": "tcapelle",
        "body": "This is weird, I remember I faced some of this back then. Have you tried printing something every iteration just to check?"
      },
      {
        "user": "francois-rozet",
        "body": "I just added a `print(i)` at each iteration. The progress bar still does not update (even though it does in my code).\n\n```\n1   0%|                                                          | 0/1000 [00:00<?, ?it/s]\n2 0\n3 1\n4 2\n5 3 \n```\n\n> See https://wandb.ai/francois-rozet/tqdm-test/runs/4xekt4ma/logs"
      },
      {
        "user": "ArtsiomWB",
        "body": "Hey guys! Thank you for writing in. \n\nI can repro this on my end as well. \n\nI can check with our engineering team if this behavior is intentional or not. Some time ago, we had users writing in, asking to not display `tqdm` logs in the `logs` tab, because it created a lot of noise. I'll get back to you once I have an answer. "
      }
    ]
  },
  {
    "issue_number": 9684,
    "title": "[Bug]: Summary metrics (min, max, last, etc.) ignore logged values after resuming a job",
    "author": "ricoj",
    "state": "open",
    "created_at": "2025-04-04T11:44:36Z",
    "updated_at": "2025-04-08T11:12:34Z",
    "labels": [
      "ty:bug",
      "a:sdk",
      "c:sdk:define-metric"
    ],
    "body": "Summary metrics (using `define_metric(..., summary=...)`) fail to update the summary in resumed jobs. When resuming a job and then logging more values, the `SummarySubDict` for the value `\"x\"` is correct but after the job is finished the summary values `\"x.min\"`, `\"x.max\"`, and `\"x.last\"` are not updated in the summary (potentially because they already exist in the summary at that time). This can be seen in\n* the command line output of the summary (see below)\n* the summary on the wandb website\n* after resuming the job again: the `SummarySubDict` is gone and the aggregations `\"x.min\"`, `\"x.max\"`, and `\"x.last\"` only include data from before the first resume.\n\nHere is a minimal example with the print outputs as comments (wandb version=0.19.9, python version=3.10.12, operating system=Ubuntu 22.04.4). The final summary should have summary[\"x.max\"]==summary[\"x.last\"]==19, but it is 9 instead.\n\n```\nimport wandb\n\nproject = \"summary_with_restart\"\nid = \"001\"\n\nwandb.init(project=project, id=id)\n\nwandb.define_metric(\"x\", summary=\"last,min,max\")\nfor x in range(10):\n    wandb.log({\"x\": x})\n\nprint(dict(wandb.summary))\n# {'_timestamp': 1743766838.3081837, '_runtime': 0.741306473, '_step': 9, 'x': <wandb.sdk.wandb_summary.SummarySubDict object at 0x77b97488f820>}\nprint(dict(wandb.summary[\"x\"]))\n# {'last': 9, 'max': 9, 'min': 0}\n\nwandb.finish()\nwandb.init(project=project, id=id, resume=\"must\")\n\nwandb.define_metric(\"x\", summary=\"last,min,max\")\nfor x in range(10, 20):\n    wandb.log({\"x\": x})\n\nprint(dict(wandb.summary))\n# {'_runtime': 2.012157371, '_step': 19, '_timestamp': 1743766841.729719, '_wandb.runtime': 0, 'x.last': 9, 'x.min': 0, 'x': <wandb.sdk.wandb_summary.SummarySubDict object at 0x77b9749edc60>, 'x.max': 9}\nprint(dict(wandb.summary[\"x\"]))\n# {'min': 10, 'last': 19, 'max': 19}\n\nwandb.finish()\n\n# Run history:\n#\n# x\t▁▂▃▃▄▅▆▆▇█\n#\n# Run summary:\n#\n# x.last    9\n# x.max     9\n# x.min     0\n\nwandb.init(project=project, id=id, resume=\"must\")\n\nprint(dict(wandb.summary))\n# {'_wandb.runtime': 0, 'x': 19, 'x.last': 9, 'x.max': 9, 'x.min': 0, '_runtime': 2.012157371, '_step': 19, '_timestamp': 1743766841.729719}\n```",
    "comments": [
      {
        "user": "kptkin",
        "body": "Hello @ricoj, \nThanks a lot for reporting the issue, you are right this is indeed a bug and we are going to fix it for the next release.\n\nOne side note as best practice we suggest using the run's instance methods instead of the global ones. For example:\n\ninstead of this:\n```\nwandb.init(project=project, id=id)\n\nwandb.define_metric(\"x\", summary=\"last,min,max\")\nfor x in range(10):\n    wandb.log({\"x\": x})\n\nwandb.finish()\n```\n\nuse this:\n```\nrun = wandb.init(project=project, id=id)\n\nrun.define_metric(\"x\", summary=\"last,min,max\")\nfor x in range(10):\n    run.log({\"x\": x})\n\nrun.finish()\n```\n\nAlso when possible we are also encouraging using the context manager. Hence:\n\ninstead of this:\n```\n run = wandb.init(...)\n...\nrun.finish()\n```\n\nuse this:\n```\nwith wandb.init(...) as run:\n    ...\n```` "
      },
      {
        "user": "ricoj",
        "body": "Hi @kptkin,\n\nThank you for the quick response and for going to fix this in the next version. I also appreciate you sharing these best practices, very helpful.\n\nCheers,\nRico"
      }
    ]
  },
  {
    "issue_number": 5337,
    "title": "Select runs that logged image with the key Val_dice_Ground truth_Epoch: 3_ to visualize data here.",
    "author": "aleemsidra",
    "state": "closed",
    "created_at": "2023-04-13T14:39:54Z",
    "updated_at": "2025-04-08T07:00:06Z",
    "labels": [],
    "body": "I am logging images to wandb as follows:\r\n```\r\nif wandb_mode == \"online\":\r\n                print(\"logging val_dice_images\")\r\n                mask = torch.zeros(size=val_segmented_volume[125:129].shape) \r\n                mask[val_segmented_volume[125:129] > 0.5] = 1\r\n                log_images(input_samples[125:129], mask.unsqueeze(1), gt_samples[125:129], epoch , \"Val_dice\")\r\n\r\n\r\n\r\ndef log_images(input, preds, gt, epoch,  stage, img_id = \" \"):\r\n\r\n    grid_img = vutils.make_grid(input, \r\n                                normalize=False, scale_each=False)\r\n    \r\n    wandb.log({f\"{stage}_Input images_Epoch: {epoch}_{img_id}\": wandb.Image(grid_img)}, step=wandb.run.step)\r\n\r\n    grid_img = vutils.make_grid(preds,\r\n                                normalize=False,\r\n                                scale_each=False)\r\n    \r\n\r\n    wandb.log({f\"{stage}_predictions_Epoch: {epoch}_{img_id}\": wandb.Image(grid_img)}, step=wandb.run.step)\r\n\r\n    grid_img = vutils.make_grid(gt,\r\n                                normalize=False,\r\n                                scale_each=False)\r\n    \r\n    wandb.log({f\"{stage}_Ground truth_Epoch: {epoch}_{img_id}\": wandb.Image(grid_img)}, step=wandb.run.step)\r\n\r\n\r\n```\r\n\r\nHowever, on wandb when I try to visualize them, I get `Select runs that logged image with the key Val_dice_Ground truth_Epoch: 3_ to visualize data here.` Though I have selected the corresponding run. I am seeing the other plots normally, but images are not appearing. How to resolve it?",
    "comments": [
      {
        "user": "thegenerativegeneration",
        "body": "I have the same problem. Sometimes it works, sometimes not. I've had this problem across multiple projects this past week.\r\n\r\nImages are logged to the local folder but don't turn up remotely. \r\n\r\nSeems to be related: https://github.com/wandb/wandb/issues/5332"
      },
      {
        "user": "nate-wandb",
        "body": "Hi @thegenerativegeneration @aleemsidra , we are currently having a site wide issue processing files. The files should sync eventually but I would recommend holding onto the local run directory as a backup. The issue is listed here on our [status page](https://status.wandb.com/)  and this should be resolved shortly."
      },
      {
        "user": "nate-wandb",
        "body": "Hi All, I just wanted to follow up and let you know this should be resolved now. Let us know if you are still seeing any issues. "
      }
    ]
  },
  {
    "issue_number": 6286,
    "title": "[Feature]: Step slider for `wandb.Table` output",
    "author": "johnPertoft",
    "state": "open",
    "created_at": "2023-09-13T14:31:37Z",
    "updated_at": "2025-04-08T02:20:12Z",
    "labels": [
      "ty:feature",
      "a:app",
      "c:table"
    ],
    "body": "### Description\r\n\r\nI'm logging a `wandb.Table` after every training+evaluation epoch. The rows of this table are sampled rows from an evaluation set that I'd like to visualize predictions for at different epochs. I would like to have a slider to see the output at each epoch step.\r\n\r\nAll my tables are available under the artifacts tab but it would be nice to have a slider to go between them in the same page as the rest of the logged metrics.\r\n\r\nIf there already exists a way to get *something like this* I'd be interested in this as well.\r\n\r\n### Suggested Solution\r\n\r\nA slider to quickly go between different iterations of a `wandb.Table` under the same key.\r\n\r\n### Alternatives\r\n\r\n_No response_\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "johnPertoft",
        "body": "Or is the recommended way to add rows from subsequent epochs to the same table and then do a groupby on input for example?"
      },
      {
        "user": "umakrishnaswamy",
        "body": "Hey @johnPertoft, currently we do not support step sliders for wandb Tables, and this is an active feature request. I'll be sure to update you if progress is made on it.\r\n\r\nI really like your idea of adding rows from subsequent epochs and grouping by values, and this is what I would recommend as well. Another idea could be logging these values as separate tables/artifacts and viewing them in reports (would be a bit more cumbersome to go through, so I'd still recommend the former more so). Please let me know if you have other questions!"
      },
      {
        "user": "kxhit",
        "body": "Hi, @umakrishnaswamy  I also want the feature of supporting step slider for table of images. As the list of images are limited by 108 images, I change my wandb code to use table of images, but found out there is no step slider so I can see the changes of my images across training steps. I really want to have this feature and please let me know. Thanks!"
      }
    ]
  },
  {
    "issue_number": 9681,
    "title": "[Q]:  unable to verified academic email",
    "author": "Tlhey",
    "state": "open",
    "created_at": "2025-04-04T01:41:13Z",
    "updated_at": "2025-04-07T17:01:34Z",
    "labels": [
      "ty:question",
      "a:app"
    ],
    "body": "### Ask your question\n\n<!--- Ask your question here --->\nI have signed up a year ago, and now the account is 'Your free trial has expired.' I wanted to Apply for a free W&B academic account. \nHowever, I cannot choose my edu email to veirfy, is there anything I can do to verify it? My email is ys396@duke.edu\n\n<img width=\"813\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d693e5ab-20d7-4809-b12f-a7db80c7406a\" />\n\nThank you!",
    "comments": [
      {
        "user": "paulosabile-wb",
        "body": "Hi @Tlhey Good day and thank you for reaching out to us! Happy to assist you here.\n\nAre you able to go to the user settings and resend the verification email? You need to verify the email first and set it as your primary email to proceed."
      },
      {
        "user": "Tlhey",
        "body": "Hi, @paulosabile-wb Thank you for your reply!\n\nI was logged out from my ys396@duke.edu account, and I used my gmail ys396duke.edu@gmail.com to sign up for a new one, and linked my shcool email to it. Now I am able to use the api key normally."
      },
      {
        "user": "paulosabile-wb",
        "body": "Hi @Tlhey I am happy to hear that you were able to apply for an academic plan using your email. I will now mark this request as Solved. Please feel free to reach out again if you need assistance!\n\nHave a great weekend!\n\n\nBest Regards,\nPaulo"
      }
    ]
  },
  {
    "issue_number": 9682,
    "title": "[Q]: Offline mode",
    "author": "kyleok",
    "state": "closed",
    "created_at": "2025-04-04T02:33:31Z",
    "updated_at": "2025-04-07T03:36:35Z",
    "labels": [
      "ty:question",
      "c:sweeps"
    ],
    "body": "### Ask your question\n\nHi,\nServer I need to use do not provide network bridge in docker due to network policy, but Host machine does have regular network available.\nin this environment, is there way to utilize WandB as well?\nI know that there is offline mode, but as far as I understand, I won't be able to use sweep for multiple machines",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @kyleok! Thank you for writing in. \n\nIn general, you do need a connection to your wandb app in order to run sweeps. Unfortunately, sweeps in SDK's offline mode are not available. You can, though, have a local deployment of wandb app that is not connected to the internet, but you still need the wandb SDK to be able to connect to it. "
      },
      {
        "user": "kyleok",
        "body": "Thanks for answering!"
      }
    ]
  },
  {
    "issue_number": 3455,
    "title": "[Q] Log image per epoch and have slider per epoch",
    "author": "azinonos",
    "state": "closed",
    "created_at": "2022-04-01T14:59:11Z",
    "updated_at": "2025-04-04T16:56:13Z",
    "labels": [
      "c:sdk:media",
      "a:app"
    ],
    "body": "Hello,\r\n\r\nIn my training loop I am logging an image generated from the data at every epoch. I am using PyTorch Lightning and Weights & Biases. The command I'm using is like the below:\r\n\r\n```\r\nfig = # compute fig\r\nself.logger.experiment.log({\"heatmap\": [wandb.Image(fig)]})\r\n```\r\n\r\nHowever, when I log it to wandb, I get a weird slider which is not epochs but random steps. For example the first step is step 14, the second is step 19, the third is step 24 etc. So even though I can slide the slider to see the progress, I never really know which epoch each image corresponds to.\r\n\r\n![image](https://user-images.githubusercontent.com/17152554/161290031-02be693c-c972-4593-8360-24e939b04b8d.png)\r\n\r\nAlternatively, I tried to log multiple images (which is an uglier way) by putting the current epoch number as part of the title:\r\n\r\n```\r\nfig = # compute fig\r\nself.logger.experiment.log({f\"heatmap_e{self.current_epoch}\": [wandb.Image(fig)]})\r\n```\r\n\r\nThis logs multiple images with the epoch number, but does so in a seemingly arbitrary order. If I use the sorting functionality, I still don't get a correct sorting of the images, but a different arbitrary combination.\r\n\r\n![image](https://user-images.githubusercontent.com/17152554/161290078-5e2f9311-e696-4552-8589-2ddc23384ef1.png)\r\n\r\n\r\nSo to summarise:\r\nQuestion/Issue 1) How to log images on every epoch and have a slider epoch instead of random steps?\r\nQuestion/Issue 2) How to sort multiple images in a correct order instead of arbitrarily?",
    "comments": [
      {
        "user": "ramit-wandb",
        "body": "Hi @azinonos,\r\n\r\nImages (or for that matter any data logged through `wandb.log`) are ordered on the basis of `step`, an internal variable that allows us to order the data present in a meaningful manner. `step` is incremented on each call to `wandb.log`. \r\n\r\nTo answer your question, there are 2 ways to manage the step number:\r\n\r\n1) Collect all your metrics and send them over in a single `log` call:\r\n```python3\r\nwandb.log({\r\n  'metric_1' : ...,\r\n  'metric_2' : ...\r\n})\r\n```\r\nvs\r\n```python3\r\nwandb.log({'metric_1' : ...})\r\nwandb.log({'metric_2' : ...})\r\n```\r\n\r\n2) Manually manage your `step` value using `wandb.log({...}, step=STEP)`. Please note that the value of step must increase monotonically, otherwise calls to `log` may be ignored.\r\n\r\nAs for your second question, the images are always ordered in the order in which they are logged to W&B. There is no way to rearrange this order currently.\r\n\r\nThanks,\r\nRamit"
      },
      {
        "user": "azinonos",
        "body": "Hi Ramit,\r\n\r\nThanks for the response. For the second solution, when you set a manual step with step=STEP, does this affect all steps on a global scope? Or just for this specific metric?\r\n\r\nRegarding the logging order, I do log them in a specific order monotonically, but they end up getting logged randomly - that's the problem."
      },
      {
        "user": "ramit-wandb",
        "body": "Hey @azinonos,\r\n\r\nWhen you set a manual step, it only defines the step value for that particular call to `wandb.log`. Its effect does not extend beyond that.\r\n\r\nWould it be possible for you to share a minimal script or colab link reproducing the issue where images are not in order? It will help me understand why these images are not ordered correctly for you.\r\n\r\nThanks,\r\nRamit"
      }
    ]
  },
  {
    "issue_number": 1093,
    "title": "Media Panel: custom x-axis options",
    "author": "tshrjn",
    "state": "closed",
    "created_at": "2020-06-10T07:49:14Z",
    "updated_at": "2025-04-04T16:53:45Z",
    "labels": [
      "ty:feature",
      "c:sdk:custom-charts",
      "a:sdk",
      "a:app"
    ],
    "body": "### Description\r\nThe step counter for [custom plots](https://docs.wandb.com/library/log#custom-plots) doesn’t have other `x-axis` option except for `step`.\r\nAnd the Legend doesn’t work properly with custom plots. [The color doesn’t show up here.]\r\nAlso, the title set up in matplotlib cuts off.\r\n\r\n<img width=\"1565\" alt=\"Screen Shot 2020-06-10 at 3 40 36 AM\" src=\"https://user-images.githubusercontent.com/8372098/84241395-68b60b80-aacd-11ea-8c23-f40262566d18.png\">\r\n\r\n\r\n### Code Description\r\nReference code being used in pytorch-lighning's `LightningModule`.\r\n```\r\n        fig, ax = plt.subplots()\r\n        colors = {\r\n            'train': 'blue',\r\n            'valid': 'green',\r\n            'test':  'red'}\r\n\r\n        for split, yearwise_skills in skill_dict.items():\r\n            years, skills = list(zip(*yearwise_skills))\r\n            ax.scatter(years, skills, c=colors[split], label=split)\r\n\r\n        ax.set_title(f'Epoch {self.current_epoch}')\r\n        ax.set_ylabel(f'Skills at Epoch {self.current_epoch}')\r\n        ax.set_xlabel('Years')\r\n        ax.legend()\r\n        ax.grid(True)\r\n\r\n        self.logger.experiment.log({\"Skill_by_years\": plt})\r\n```\r\n\r\n\r\n### Version info\r\n\r\n```bash\r\n* CUDA:\r\n        - GPU:\r\n                - GeForce RTX 2080 Ti\r\n        - available:         True\r\n        - version:           10.1\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.5.0\r\n        - pytorch-lightning: 0.7.5\r\n        - tensorboard:       2.2.0\r\n        - tqdm:              4.45.0\r\n        - wandb:            0.8.35\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                -\r\n        - processor:         x86_64\r\n        - python:            3.6.10\r\n        - version:           #75-Ubuntu SMP Tue Oct 1 05:24:09 UTC 2019\r\n```",
    "comments": [
      {
        "user": "issue-label-bot[bot]",
        "body": "Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/wandb/client) and [code](https://github.com/hamelsmu/MLapp) for this bot."
      },
      {
        "user": "vanpelt",
        "body": "@tshrjn behind the scenes we're using Plotly to convert the matplotlib object into an interactive chart.  If you wrap your matplotlib object in `wandb.Image` we'll instead render out a png which should be exactly what you see in a notebook context.  You can also generate Plotly charts directly which should give your more control over the final rendering.\r\n\r\nRegarding the step slider, currently we only support the internal step counter to move across time.  Can you share a little more about your usecase with custom x-axis values?"
      },
      {
        "user": "tshrjn",
        "body": "Is there a way to use the step slider with Plotly charts? \r\n\r\nAlso, a use case for step-slider is similar to that of the x-axis available in the workspace for instance, epoch instead of a step.\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 9366,
    "title": "[Bug-App]: Cannot rename runs in the workspace on Firefox",
    "author": "vassil-atn",
    "state": "closed",
    "created_at": "2025-01-30T11:59:48Z",
    "updated_at": "2025-04-04T16:47:22Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\nSince a few weeks ago I have been unable to rename runs from the Workspace page when using Firefox as the browser. When clicking the rename button nothing happens. The function works just fine on Chrome, and also you can rename specific runs from their Overview page on Firefox. Is this a problem on Firefox's end or on Wandb's?\n\nBrowser: Firefox version 126.0.1 (64-bit)\nOS: Ubuntu 20.04",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @vassil-atn! Thank you for writing in. \n\nI think I see the behavior you are talking about. I am able to rename my runs without any issues on Chrome, but when pressing `rename run` on FireFox, the button does not do anything. I'll go ahead and report this to our engineering team. Thank you for reporting this. "
      },
      {
        "user": "ArtsiomWB",
        "body": "Hey @vassil-atn! Are you still seeing this behavior on your side?"
      },
      {
        "user": "vassil-atn",
        "body": "Thanks, it's all good now, I'll close the issue!"
      }
    ]
  },
  {
    "issue_number": 3211,
    "title": "[Feature] Allow legend reordering ",
    "author": "andreimargeloiu",
    "state": "open",
    "created_at": "2022-02-04T10:55:48Z",
    "updated_at": "2025-04-04T16:44:41Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "**Context**\r\nI'm creating a line plot with multiple similar metrics inside a run. The metrics I'm plotting differ only by some hyperparameters. It's a bit annoying that I cannot change the order of the Y lines after being added.\r\n\r\nIt would be great to allow changing the legend order by drag-and-drop. In my case, I would like to order them after some hyper-parameters, so the legend is easy to read.\r\n\r\n![CleanShot 2022-02-04 at 10 27 07](https://user-images.githubusercontent.com/18227298/152513496-854f9f0e-85db-42fb-ad90-be63682b594c.png)\r\n\r\n![CleanShot 2022-02-04 at 10 55 28](https://user-images.githubusercontent.com/18227298/152517188-68278cf7-a480-470d-b79a-3fa394cff289.png)\r\n",
    "comments": [
      {
        "user": "armanhar",
        "body": "Hey Andrei, thanks for the suggestion. Filing a ticket."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open 60 days with no activity."
      },
      {
        "user": "hanglearning",
        "body": "This is a useful feature"
      }
    ]
  },
  {
    "issue_number": 9667,
    "title": "[Feature]: Allow downloading output.log for crashed/running runs",
    "author": "abhinavg4",
    "state": "open",
    "created_at": "2025-04-01T06:51:03Z",
    "updated_at": "2025-04-04T00:02:07Z",
    "labels": [
      "ty:feature",
      "a:sdk",
      "c:sdk:console"
    ],
    "body": "### Description\n\nCurrently, it seems like wandb only allows downloading output.log files for finished runs. I would love to be able to download the output.log file for crashed/running runs too. \n\nI'm currently using the wandb files API, and I cannot see output.log. Since this file is already available in wandb web UI, I think we should be able to access it via API too. \n\n\n### Suggested Solution\n\n<!--- Describe your solution here --->\n",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @abhinavg4, Thanks for reaching out. I’d be happy to put in a feature request for making output.log downloadable for crashed or running runs. In the meantime, I wanted to share a quick tip: if you’re looking for the output.log file, you can usually find it in the `./wandb/run-{date_time}-{run_id}/` directory on the machine where the run was executed.\n\nFor completed runs, you can also download the log file using the W&B API. Here’s a sample script that does just that:\n```python\nimport wandb\n\n# Initialize API\napi = wandb.Api(api_key=\"\")\n\n# Replace with your entity (username or team), project, and run ID\nentity = \"\"          \nproject = \"\"   \nrun_id = \"\"          \n\n# Get the run object\nrun = api.run(f\"{entity}/{project}/{run_id}\")\n\n# List all files and download the output.log\nfor file in run.files():\n    if file.name.endswith(\"output.log\"):\n        print(f\"Downloading: {file.name}\")\n        file.download(replace=True)\n```"
      },
      {
        "user": "abhinavg4",
        "body": "Hi @aajais, thanks for the quick reply. We extensively use wandb, and we work with preemptible compute. Hence, using `./wandb/run-{date_time}-{run_id}/` is unfortunately not an option for us (Also the reason why most of our runs are marked crashed). I would appreciate it if you could put in a feature request or point me to any piece of code that I can use to download output.log for crashed/running runs. "
      },
      {
        "user": "aajais",
        "body": "@abhinavg4 currently, downloading log files from crashed runs isn’t supported because W&B only uploads files at the end of a run, during the `wandb.finish()` call. If the run is interrupted, this final upload step often doesn’t happen, which means the logs and other files aren’t persisted. \n\nAs a workaround, you can try explicitly calling `wandb.finish()` in a cleanup or exit handler (like try/finally or using atexit) to increase the chances that all buffered files get uploaded before the script exits. That said, this isn’t foolproof for abrupt shutdowns like preemptions.\n\nI’ve gone ahead and logged a feature request for more resilient handling of these cases particularly around partial uploads for interrupted runs and I’ll keep you posted as we make progress on that."
      }
    ]
  },
  {
    "issue_number": 9602,
    "title": "How to sync (upload) run in winows without administrator priviliges?",
    "author": "pfcouto",
    "state": "closed",
    "created_at": "2025-03-19T18:28:37Z",
    "updated_at": "2025-04-03T15:19:40Z",
    "labels": [
      "ty:question",
      "c:sdk:sync",
      "a:sdk"
    ],
    "body": "### Ask your question\n\nI just ran a few tests on a machine without internet access. I transferred the folder of the runs to a different machine and was trying to upload them. Even though I can install wandb in PowerShell (I am in Windows 11) since I don't have administrator privileges I can't run any `wandb` command in PowerShell, therefore I cannot do `wandb sync` and there is no option in the Python API to sync.\n\nWhat can I do now?",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hi @pfcouto, thanks for writing in! Have you tried running the CLI command from a Python file?\n````\nimport subprocess\nimport sys\n\nrun_folder = \"\"  # adjust this to the actual path\n\nsubprocess.run([sys.executable, \"-m\", \"wandb\", \"sync\", run_folder])\n````\nIf this doesn't work, I would recommend moving the folder to a machine where you have admin privileges and running from there. Please let me know if this helps!\n"
      },
      {
        "user": "luisbergua",
        "body": "Hi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved.\n"
      },
      {
        "user": "pfcouto",
        "body": "Hello @luisbergua \n\nThat did not work for me and I can't transfer the files to another PC. Is there a local option, using Docker, Podman or openshift that I can use?\n\nThanks!"
      }
    ]
  },
  {
    "issue_number": 9672,
    "title": "[Q]: academic email verification",
    "author": "wannanfeng",
    "state": "closed",
    "created_at": "2025-04-02T16:56:41Z",
    "updated_at": "2025-04-03T11:50:31Z",
    "labels": [
      "ty:question"
    ],
    "body": "### Ask your question\n\n<!--- Ask your question here --->\nI try to add a verified academic email，but I cannot receive email.\nI tried several times but it ended up showing **rate limit exceeded An application error occurred.**\n\nWhat else can I do to verify my academic email?  my email is **2963057784@cau.edu.cn**",
    "comments": [
      {
        "user": "paulosabile-wb",
        "body": "Hi @wannanfeng Good day and thank you for reaching out to us! Happy to help you on this.\n\nCould you please share your username and your current email address in wandb so I can check this from our end? Thanks!"
      },
      {
        "user": "wannanfeng",
        "body": "@paulosabile-wb  Thank you very much. I have successfully verified it, but due to the school's gateway settings, I received the email 10 hours later. Successfully obtained email authentication by modifying the whitelist"
      },
      {
        "user": "paulosabile-wb",
        "body": "Awesome! Thanks for confirming this!  I am happy to hear that it is working fine from your end now. I will now mark this request as Solved. Please feel free to reach out again if you need assistance!\n\nHave a great weekend!\n\n\nBest Regards,\nPaulo"
      }
    ]
  },
  {
    "issue_number": 3233,
    "title": "[Q] Is there a clean way to suppress unwatch warnings?",
    "author": "JakeColor",
    "state": "closed",
    "created_at": "2022-02-08T18:24:53Z",
    "updated_at": "2025-03-31T18:23:09Z",
    "labels": [],
    "body": "`wandb` logs a warning every time we `.unwatch()` a model, spamming up our logs.\r\n\r\nIs there a clean way to suppress this warning? \r\n\r\n```\r\n**wandb**: WARNING [...] model has not been watched\r\n```\r\n\r\n## Our code\r\n``` python\r\nself.logger.experiment[0].unwatch(self.model)\r\n```\r\n\r\n## What we've tried:\r\n\r\n``` python\r\nwith warnings.catch_warnings():\r\n    warnings.filterwarnings(\"ignore\", message=\"model has not been watched\")\r\n    self.logger.experiment[0].unwatch(self.model)\r\n```\r\n\r\nThanks!",
    "comments": [
      {
        "user": "ramit-wandb",
        "body": "Hey @JakeColor,\r\n\r\nLooking at the warning message, It seems like you are calling `unwatch(self.model)` before actually calling `watch(self.model)`. I think the best way to go about this would be to create a flag to make sure you call `watch` before.\r\n\r\nIf you still want a method to get rid of the warnings, I have somewhat of a hack that will work:\r\n```python3\r\nstderr = sys.stderr\r\nsys.stderr = open(os.devnull, 'w')\r\n\r\nself.logger.experiment[0].unwatch(self.model)\r\n\r\nsys.stderr = stderr\r\n```\r\n\r\nI'm going to put a feature request to allow for better management of our warning/error messages.\r\n\r\nThanks,\r\nRamit"
      },
      {
        "user": "JakeColor",
        "body": "Thanks. I'll give this a try in the near future and re-open this issue if i still have issues.\r\n\r\nPlease reference this issue in future releases so I can replace our hack w/ a better solution!"
      }
    ]
  },
  {
    "issue_number": 6974,
    "title": "[CLI]:  Updates to run config and wandb offline runs",
    "author": "klieret",
    "state": "open",
    "created_at": "2024-02-08T17:26:20Z",
    "updated_at": "2025-03-31T16:58:25Z",
    "labels": [
      "ty:feature",
      "c:sdk:sync",
      "a:sdk",
      "c:sdk:config"
    ],
    "body": "### Describe the bug\r\n\r\n**Expectation**: If I change the run config (`run.config.update({...}, allow_val_change=True) mid-run and then run `wandb sync /path/to/wandb_dir` externally, the config should be updated in the cloud\r\n\r\n**What actually happens:** Changes to the run config are not synchronized until the run is finished. This issue was first [raised](https://github.com/klieret/wandb-offline-sync-hook/issues/81) by @MoH-assan and I've reproduced it.\r\n\r\n**Context:** Academic HPC clusters typically run ML experiments on batch nodes without internet. The only way to use wandb is with offline runs that are periodically synced from a head node with internet access, making use of a shared file system (more information in [a python package that I wrote to simplify this workflow](https://github.com/klieret/wandb-offline-sync-hook)). This way, you still get almost-live updates from your runs.\r\n\r\nI used the following script to reproduce the issue:\r\n\r\n```python\r\n#!/usr/bin/env python3\r\n\r\nfrom pathlib import Path\r\nimport os\r\nimport wandb\r\nimport time\r\nimport random\r\nimport subprocess\r\nimport coolname\r\n\r\nos.environ[\"WANDB_MODE\"] = \"offline\"\r\n\r\nname = coolname.generate_slug(2)\r\nrun = wandb.init(\r\n    config={\"a\": 1},\r\n    project=\"test\",\r\n    allow_val_change=True,\r\n    name=name,\r\n    id=name,\r\n    entity=\"klieret\",\r\n)\r\n\r\nwandb_dir = Path(run.dir)\r\nconfig_file_path = wandb_dir / \"config.yaml\"\r\nprint(wandb_dir)\r\n\r\n\r\ndef sync_offline_run():\r\n    cmd = [\"wandb\", \"sync\", str(Path(wandb_dir).parent)]\r\n    print(cmd)\r\n    subprocess.run(cmd)\r\n\r\n\r\ndef print_online_config():\r\n    api = wandb.Api()\r\n    run = api.run(f\"klieret/test/{name}\")\r\n    print(run.config)\r\n\r\n\r\ndef simulate_training():\r\n    for i in range(100):\r\n        run.log({\"metric\": random.random()})\r\n        time.sleep(0.01)\r\n\r\n\r\nprint(\"----------------\")\r\n\r\nsimulate_training()\r\nsync_offline_run()\r\nprint_online_config()\r\n\r\nprint(\"----------------\")\r\n\r\nrun.config.update({\"a\": 2}, allow_val_change=True)\r\n\r\nsimulate_training()\r\nsync_offline_run()\r\nprint_online_config()\r\n\r\nprint(\"----------------\")\r\n\r\nrun.finish()\r\nsync_offline_run()\r\nprint_online_config()\r\n```\r\n\r\n\r\n### Additional Files\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\nWandB version: 0.16.3\r\n\r\nOS: OSX\r\n\r\nPython version: 3.11.3\r\n\r\nVersions of relevant libraries:\r\n\r\n\r\n### Additional Context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hi @klieret! To confirm, you start an offline run, change its config, then finish the run and sync it to the cloud. Once it's synced it does not have the updated config you are interested in?"
      },
      {
        "user": "klieret",
        "body": "Hi @ArtsiomWB, thanks for your reply! \r\n\r\nAfter the run is finished and we manually sync again, the config is updated correctly.\r\n\r\nHowever, any config change while the run is still training will not be synced (even though all `.log` calls are).  \r\n\r\nI realize that this is probably a rare use case. However, I wonder if a problem with the pytorch lightning integration could be related: There, I observe that all config items are only synced to the cloud after the run is finished for this same same \"offline\" synchronization strategy outlined above (I'm planning to open an issue for this soon). "
      },
      {
        "user": "ArtsiomWB",
        "body": "Thank you for reporting this, @klieret!\r\n\r\nI have submitted this concern to our engineering team"
      }
    ]
  },
  {
    "issue_number": 9606,
    "title": "[Bug-App]: Spiking in batch logging when using WandbMetricsLogger",
    "author": "akum04",
    "state": "closed",
    "created_at": "2025-03-20T14:31:35Z",
    "updated_at": "2025-03-31T16:32:43Z",
    "labels": [
      "ty:bug",
      "a:app"
    ],
    "body": "### Describe the bug\n\nI'm using  `wandb.integration.keras import WandbMetricsLogger` to log the loss values ,with `log_freq` set to `'batch'`. However, I encounter regular spikes at the end of epoch. \nTo verify the values, I also logged using the `tf.keras.callbacks.TensorBoard` during the same training process, I didn't encounter such large spikes as in wandb. What is the difference and which one is actually calculating the loss values correctly across the batch.\nNo smoothing in the plots. no outliers excluded in both cases during plotting.\n\n![loss_from_wandb](https://github.com/user-attachments/assets/9350783b-665d-419d-b31a-f8307c13fd7e)\n\n![loss_from_tensorboard](https://github.com/user-attachments/assets/2d7b0cde-ac87-4b4b-9bda-dd38aadc25cd)\n",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @akum04! Thank you for writing in!\n\nWould love to look into this. What wandb version are you currently on, as well as could you please send us the example toy code snippet, so I could try reproducing this behavior on my end?"
      },
      {
        "user": "akum04",
        "body": "Hi @ArtsiomWB,\n\nI'm using `wandb==0.18.0`. I'll try to create a seperate standalone code. \nBut for now, I'm using the following callbacks in the `model.fit`.\n\nFor tensorboard callback:\n`tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=logdir, update_freq='batch')`\n\nFor wandb callback:\n`wml = WandbMetricsLogger(log_freq='batch')`\n\nModel.fit function \n`model.fit(train_dataset,\n              epochs=cfg.training.num_epochs,\n              batch_size=cfg.training.batch_size,\n              steps_per_epoch=steps_per_epoch,\n              callbacks=[checkpoint_callback, wml, tensorboard_callback],\n              validation_data=val_dataset,\n              validation_steps=validation_steps)`"
      },
      {
        "user": "ArtsiomWB",
        "body": "Hey @akum04! Any luck putting together the full script?\n\nUnfortunately, I am not having much luck of reproducing this behavior on my end. "
      }
    ]
  },
  {
    "issue_number": 2201,
    "title": "[CLI] Updating Dict Values in Config not reflected in Web App",
    "author": "hv10",
    "state": "closed",
    "created_at": "2021-05-16T13:28:15Z",
    "updated_at": "2025-03-31T16:10:35Z",
    "labels": [
      "a:sdk"
    ],
    "body": "**Description**\r\nWhen updating a dictionary based value of the wandb.config of a run, the change is reflected in the script of the run but not in the web app.\r\n\r\n**Wandb features**\r\nwandb.init()\r\nwandb.config\r\nwandb.config.update()\r\n\r\n**How to reproduce**\r\n```python\r\nimport wandb\r\n\r\nwandb.init(project=\"test_nested_update\")\r\nconfig = wandb.config\r\nconfig.d = {\"a\": 1}\r\nprint(config.d) # shows {\"a\":1}\r\nconfig.d[\"b\"] = 2\r\nprint(config.d) # shows {\"a\":1, \"b\":2}\r\nconfig.update({\"d\": {**config.d, \"b\": 3}}) # fails with error (as expected)\r\n```\r\nConsole Output:\r\n```bash\r\n{'a': 1}\r\n{'a': 1, 'b': 2}\r\nwandb: ERROR Attempted to change value of key \"d\" from {'a': 1, 'b': 2} to {'a': 1, 'b': 3}\r\nwandb: ERROR If you really want to do this, pass allow_val_change=True to config.update()\r\nTraceback (most recent call last):\r\n  File \"/home/nodanz/RapLyricGeneration/experiments/tests/wandb_nest_issue.py\", line 9, in <module>\r\n    config.update({\"d\": {**config.d, \"b\": 3}})\r\n  File \"/home/nodanz/anaconda3/envs/py38/lib/python3.8/site-packages/wandb/sdk/wandb_config.py\", line 169, in update\r\n    sanitized = self._update(d, allow_val_change)\r\n  File \"/home/nodanz/anaconda3/envs/py38/lib/python3.8/site-packages/wandb/sdk/wandb_config.py\", line 162, in _update\r\n    sanitized = self._sanitize_dict(\r\n  File \"/home/nodanz/anaconda3/envs/py38/lib/python3.8/site-packages/wandb/sdk/wandb_config.py\", line 218, in _sanitize_dict\r\n    k, v = self._sanitize(k, v, allow_val_change)\r\n  File \"/home/nodanz/anaconda3/envs/py38/lib/python3.8/site-packages/wandb/sdk/wandb_config.py\", line 231, in _sanitize\r\n    raise config_util.ConfigError(\r\nwandb.sdk.lib.config_util.ConfigError: Attempted to change value of key \"d\" from {'a': 1, 'b': 2} to {'a': 1, 'b': 3}\r\nIf you really want to do this, pass allow_val_change=True to config.update()\r\n```\r\nScreenshot showing that the cfg value was not updated in Web App:\r\n\r\n![2021-05-16-152345_841x234_scrot](https://user-images.githubusercontent.com/12249918/118398977-3d338d80-b65b-11eb-80f3-c3c4cb1558ec.png)\r\n\r\n**Environment**\r\n- OS: linux (arch)\r\n- Environment: pyenv\r\n- Python Version: 3.8.8\r\n\r\n",
    "comments": [
      {
        "user": "ariG23498",
        "body": "Hey @hv10 \r\nI tried reproducing the issue and had the following solution for it:\r\n```python\r\nrun = wandb.init(project=\"GH-2201\", entity=\"repro\")\r\nwith run:\r\n    cfg = wandb.config\r\n    print(cfg)\r\n    cfg.d = {\"a\": 1}\r\n    print(cfg.d)\r\n    cfg.d[\"b\"] = 2\r\n    print(cfg.d)\r\n    run.config.update({\"d\": {**cfg.d, \"b\": 3}})\r\n```\r\n\r\nThis does not fail and also updates the config of the run.\r\n\r\n![image](https://user-images.githubusercontent.com/36856589/118506244-c7a6ea80-b74a-11eb-829a-b2cc1ad7c0c4.png)\r\n"
      },
      {
        "user": "hv10",
        "body": "Sadly the proposed solution did not work for me. Nonetheless adding the `allow_val_change=True` keyword parameter to `run.config.update` did work, which is also fine in my case as I do not change but only add a new value."
      },
      {
        "user": "smartinezai",
        "body": "I am also having the same issue"
      }
    ]
  },
  {
    "issue_number": 2848,
    "title": "[Feature] Run grid sweep in random order",
    "author": "ianbenlolo",
    "state": "open",
    "created_at": "2021-10-28T21:52:49Z",
    "updated_at": "2025-03-28T20:10:04Z",
    "labels": [
      "ty:feature",
      "c:sweeps"
    ],
    "body": "Running a sweeps in grid search is great but when single runs take long, id like it to run in a random order so that i can get a clearer image of the final results earlier on during the runs.   \r\nie. if my grid search has \r\n```\r\nmethod: grid\r\n[...]\r\nparameters:\r\n    model: \r\n        values:\r\n            - a\r\n            - b\r\n            - c\r\n    lr: \r\n        values:\r\n            - 0.00001\r\n            - 0.0001\r\n            - 0.005\r\n    drop:\r\n        values:\r\n            - 0.08\r\n            - 0.01\r\n```\r\nI'd like to get a sense of how each model performs early on, rather than having a full look of the first model, then second, ...\r\n\r\n\r\nI'd like the grid method to have an option of passing the parameters in a more random order.  \r\n\r\nNot many alternatives exist other than not running with wandb. \r\n\r\nPlease let me know if this is clear.\r\nThanks!",
    "comments": [
      {
        "user": "aidanjdonohue",
        "body": "Hi @ianbenlolo, I think you have identified the advantage of starting off with a random sweep and then doing a more exhaustive search around promising parameters. However, it seems reasonable to allow the option for a randomized grid search. I'll keep you updated on this. "
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open 60 days with no activity."
      },
      {
        "user": "Mjvolk3",
        "body": "Random over the grid would be a very nice feature. Any updates on this?\r\n\r\n[grid_search_next_runs](https://github.com/wandb/sweeps/blob/99bccfba31834805974c59f21b6ac82b0bdb8f01/src/sweeps/grid_search.py#L22C1-L28C31)\r\n\r\n```python\r\ndef grid_search_next_runs(\r\n    runs: List[SweepRun],\r\n    sweep_config: Union[dict, SweepConfig],\r\n    validate: bool = False,\r\n    n: int = 1,\r\n    randomize_order: bool = False,\r\n) -> List[Optional[SweepRun]]:\r\n```\r\nHow to pass `random_order = True` when running sweeps?"
      }
    ]
  },
  {
    "issue_number": 9616,
    "title": "[Bug]: Wandb sweeps freeze when importing PyTorch",
    "author": "emorris7",
    "state": "closed",
    "created_at": "2025-03-24T12:42:15Z",
    "updated_at": "2025-03-27T19:11:50Z",
    "labels": [
      "ty:bug",
      "c:sweeps"
    ],
    "body": "### Describe the bug\n\n<!--- Describe your issue here --->\nHi,\n\nI have recently encountered an issue when trying to run weights and biases sweeps with PyTorch and Hydra. When running `python3 test_wb.py` my script runs fine and logs correctly to W&B. However, when I generate a sweep with `wandb sweep configs/test_sweeps.yaml` and run an agent locally on my Mac, the resulting run fails to progress past the torch import statement (the agent eventually crashes with no errors that I can find). The terminal logs are as follows:\n```\nwandb: Starting wandb agent 🕵️\n2025-03-24 12:21:58,606 - wandb.wandb_agent - INFO - Running runs: []\n2025-03-24 12:21:58,933 - wandb.wandb_agent - INFO - Agent received command: run\n2025-03-24 12:21:58,933 - wandb.wandb_agent - INFO - Agent starting run with config:\n        lr: 0.1\n2025-03-24 12:21:58,938 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python3 test_wb.py lr=0.1\nPrint 1\nPrint 2\n2025-03-24 12:22:03,950 - wandb.wandb_agent - INFO - Running runs: ['8qrq2yxy']\n```\n\nWhen I remove the torch import statement everything works correctly. I've included my script, Hydra config, sweep config and conda environment details below for recreating the results:\n\nHydra config `configs/test_wb.yaml`\n```\nwandb:\n  project_name: debugging_sweep\n  mode: online\n\nlr: 0.1\n```\nSweep config `configs/test_sweeps.yaml`\n```\nprogram: test_wb.py\nmethod: grid\nparameters:\n  lr:\n    values: [0.1, 0.2]\n\n\ncommand:\n  - ${env}\n  - python3\n  - ${program}\n  - ${args_no_hyphens}\n ```\n\nscript `test_wb.py`\n```\nimport logging\nimport wandb\nimport hydra\nimport omegaconf\nprint(\"Print 1\")\nimport numpy as np\nprint(\"Print 2\")\nimport torch.nn as nn\nprint(\"Print 3\")\n\n\n@hydra.main(version_base=None, config_path=\"./configs\", config_name=\"test_wb\")\ndef main(cfg):\n    run = wandb.init(\n        project=cfg.wandb.project_name,\n        mode=cfg.wandb.mode,\n        config=omegaconf.OmegaConf.to_container(\n            cfg, resolve=True, throw_on_missing=True\n        ),\n    )\n\n    logging.info(\"Learning rate %f\", cfg.lr)\n    wandb.log({\"lr\":cfg.lr})\n\n    wandb.finish()\n\n\nif __name__ =='''__main__''':\n    main()\n```\n(\"Print 3\" is never printed)\n\nI am using a conda environment with the following packages:\n```\nname: wandb-example\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - annotated-types=0.6.0=py313hca03da5_0\n  - antlr-python-runtime=4.9.3=pyhd8ed1ab_1\n  - appdirs=1.4.4=pyhd3eb1b0_0\n  - blas=1.0=openblas\n  - brotli-python=1.0.9=py313h313beb8_9\n  - bzip2=1.0.8=h80987f9_6\n  - ca-certificates=2025.2.25=hca03da5_0\n  - certifi=2025.1.31=pyhd8ed1ab_0\n  - charset-normalizer=3.3.2=pyhd3eb1b0_0\n  - click=8.1.7=py313hca03da5_0\n  - cpython=3.13.2=py313hd8ed1ab_101\n  - docker-pycreds=0.4.0=pyhd3eb1b0_0\n  - eval_type_backport=0.2.2=pyha770c72_0\n  - expat=2.6.4=h313beb8_0\n  - filelock=3.13.1=py313hca03da5_0\n  - fsspec=2024.12.0=py313hca03da5_0\n  - gitdb=4.0.7=pyhd3eb1b0_0\n  - gitpython=3.1.43=py313hca03da5_0\n  - gmp=6.3.0=h313beb8_0\n  - gmpy2=2.2.1=py313h5c1b81f_0\n  - hydra-core=1.3.2=pyhd8ed1ab_1\n  - idna=3.7=py313hca03da5_0\n  - jinja2=3.1.6=py313hca03da5_0\n  - libabseil=20240722.0=cxx17_h07bc746_4\n  - libblas=3.9.0=1_h2ec9a88_netlib\n  - libcblas=3.9.0=1_h9886b1c_netlib\n  - libcxx=20.1.1=ha82da77_0\n  - libexpat=2.6.4=h286801f_0\n  - libffi=3.4.4=hca03da5_1\n  - libgfortran=5.0.0=11_3_0_hca03da5_28\n  - libgfortran5=11.3.0=h009349e_28\n  - liblapack=3.9.0=1_h9886b1c_netlib\n  - liblzma=5.6.4=h39f12f2_0\n  - libmpdec=4.0.0=h80987f9_0\n  - libopenblas=0.3.21=h269037a_0\n  - libprotobuf=5.28.3=h3bd63a1_1\n  - libsqlite=3.49.1=h3f77e49_2\n  - libtorch=2.5.1=cpu_generic_h297bf48_15\n  - libuv=1.50.0=h5505292_0\n  - libzlib=1.3.1=h8359307_2\n  - llvm-openmp=20.1.1=hdb05f8b_1\n  - markupsafe=3.0.2=py313h80987f9_0\n  - mpc=1.3.1=h80987f9_0\n  - mpfr=4.2.1=h80987f9_0\n  - mpmath=1.3.0=py313hca03da5_0\n  - ncurses=6.5=h5e97a16_3\n  - networkx=3.4.2=py313hca03da5_0\n  - nomkl=3.0=0\n  - numpy=2.2.2=py313h7c57ca2_0\n  - numpy-base=2.2.2=py313hb98e858_0\n  - omegaconf=2.3.0=pyhd8ed1ab_0\n  - openssl=3.4.1=h81ee809_0\n  - optree=0.14.1=py313h48ca7d4_0\n  - packaging=24.2=py313hca03da5_0\n  - pip=25.0=py313hca03da5_0\n  - platformdirs=3.10.0=py313hca03da5_0\n  - protobuf=5.28.3=py313h928ef07_0\n  - psutil=5.9.0=py313h80987f9_1\n  - pybind11=2.13.6=py313h48ca7d4_1\n  - pybind11-global=2.13.6=py313h48ca7d4_1\n  - pydantic=2.10.3=py313hca03da5_0\n  - pydantic-core=2.27.1=py313h2aea54e_0\n  - pysocks=1.7.1=py313hca03da5_0\n  - python=3.13.2=h81fe080_101_cp313\n  - python_abi=3.13=0_cp313\n  - pytorch=2.5.1=cpu_generic_py313_hfbf95ac_15\n  - pyyaml=6.0.2=py313h80987f9_0\n  - readline=8.2=h1a28f6b_0\n  - requests=2.32.3=py313hca03da5_1\n  - sentry-sdk=2.18.0=py313hca03da5_0\n  - setproctitle=1.2.2=py313h80987f9_2\n  - setuptools=72.1.0=py313hca03da5_0\n  - six=1.16.0=pyhd3eb1b0_1\n  - sleef=3.8=h8391f65_0\n  - smmap=4.0.0=pyhd3eb1b0_0\n  - sqlite=3.49.1=hd7222ec_2\n  - sympy=1.13.3=pyh2585a3b_105\n  - tk=8.6.13=h5083fa2_1\n  - typing=3.10.0.0=py313hca03da5_0\n  - typing-extensions=4.12.2=py313hca03da5_0\n  - typing_extensions=4.12.2=py313hca03da5_0\n  - tzdata=2025a=h04d1e81_0\n  - urllib3=2.3.0=py313hca03da5_0\n  - wandb=0.19.8=py313hdde674f_0\n  - wheel=0.45.1=py313hca03da5_0\n  - xz=5.6.4=h80987f9_1\n  - yaml=0.2.5=h1a28f6b_0\n  - zlib=1.3.1=h8359307_2\n  ```\nI've tried using a pip environment but encountered the same issues. I can run the command generated for the sweep (e.g. `/usr/bin/env python3 test_wb.py lr=0.2`) without any problems.",
    "comments": [
      {
        "user": "aajais",
        "body": "Hi @emorris7, Thanks for reaching out and for providing all the detailed context. Could you try adding `settings=wandb.Settings(start_method=\"thread\")` to your `wandb.init()` call, as suggested in [this doc](https://docs.wandb.ai/guides/integrations/hydra/#troubleshoot-multiprocessing)? We’ve seen this help in similar situations, and it’d be great to know if it works for you too."
      },
      {
        "user": "emorris7",
        "body": "Hi @aajais, thanks for your reply and suggestion. Unfortunately changing the setting doesn't seem to help. The agent still hangs and eventually crashes."
      },
      {
        "user": "aajais",
        "body": "@emorris7, thanks again for sharing your code! I tested it out on my end using the same setup (Mac + Conda), and I was able to reproduce the issue when I created a new environment using this specific PyTorch build: `pytorch=2.5.1=cpu_generic_py313_hfbf95ac_15`. So it looks like the issue might be tied to that particular PyTorch installation.\n\nCould you try recreating your Conda environment like this?\n```bash\nconda create -n sweep python=3.11\npip install torch==2.5.1\n```\n\nThat setup worked well in my testing. Let me know if it works for you too!"
      }
    ]
  },
  {
    "issue_number": 9497,
    "title": "[Bug]: wandb.agent shows memory usage as 0 in nvidia-smi",
    "author": "12tqian",
    "state": "open",
    "created_at": "2025-02-20T18:17:45Z",
    "updated_at": "2025-03-26T15:42:42Z",
    "labels": [
      "ty:bug",
      "c:sweeps"
    ],
    "body": "### Describe the bug\n\nWhen starting a sweep using `wandb.agent`, the GPU usage will show as `0` in `nvidia-smi`. This is because `threading` is used and the cuda context isn't set properly. Here is reproducing code:\n\n```\nimport torch\nimport wandb\n\n\ndef objective():\n    torch.ones(1000, 1000).to(\"cuda\")\n    breakpoint()\n\n\ndef initialize_sweep():\n    sweep_configuration = {\n        \"method\": \"bayes\",\n        \"name\": \"test\",\n        \"metric\": {\"goal\": \"minimize\", \"name\": \"score\"},\n        \"parameters\": {\n            \"lr\": {\"min\": 1e-6, \"max\": 1e-4},\n        },\n    }\n\n    sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"debug\", entity=\"12tqian\")\n\n    return sweep_id\n\n\ndef main():\n    # # Below is fix\n    # # Initialize cuda context\n    # torch.set_default_device(\"cuda\")\n    # torch.cuda.synchronize()\n\n    sweep_id = initialize_sweep()\n\n    # Call gpustat/nvidia-smi during breakpoint\n    wandb.agent(sweep_id, objective, count=1, project=\"debug\", entity=\"12tqian\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\nBelow is code reproducing using just `threading`.\n\n```\nfrom threading import Thread\nimport torch\n\n\ndef allocate_memory():\n    torch.ones(1000, 1000).to(\"cuda\")\n    breakpoint()\n\n\ndef main():\n    # # Below is fix because it sets cuda contexts\n    # torch.set_default_device(\"cuda\")\n    # torch.cuda.synchronize()\n\n    t = Thread(target=allocate_memory)\n    t.start()\n\n    # Run gpustat/nvidia-smi in another terminal during breakpoint\n    # Memory will show up as ? in gpustat and 0 in nvidia-smi\n\n    t.join()\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n\n\nPython 3.12.2\nWandb: 0.19.1\nUbuntu 22.04.5 LTS",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @12tqian! Thank you for reporting this!\n\nWorking on a repro!"
      },
      {
        "user": "ArtsiomWB",
        "body": "Hey @12tqian! I think I was able to repro the behavior on my side, could you please send me a screenshot of your `nvidia-smi` call to confirm and then I will report this to our eng team. "
      },
      {
        "user": "12tqian",
        "body": "Here is screenshot of my `nvidia-smi`.\n\n<img width=\"656\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9ad84d69-ac48-4fef-9af8-04b26ba4b185\" />"
      }
    ]
  },
  {
    "issue_number": 9498,
    "title": "[Feature]: set dir to remote file system such as s3 or gcs",
    "author": "likesum",
    "state": "open",
    "created_at": "2025-02-20T20:17:49Z",
    "updated_at": "2025-03-26T15:42:24Z",
    "labels": [
      "ty:feature",
      "a:sdk",
      "c:sdk:settings"
    ],
    "body": "### Description\n\nRight now the `dir` argument of `wand.init()` can only a local file path. It would great if it can support path like `s3://` or `gs://`.\n\n\n### Suggested Solution\n\n<!--- Describe your solution here --->\n",
    "comments": [
      {
        "user": "ArtsiomWB",
        "body": "Hey @likesum! Thank you for writing in!\nI'll go ahead and submit this feature request to our engineering team. Thank you for bringing this up!\n"
      }
    ]
  },
  {
    "issue_number": 9505,
    "title": "[Feature]: New sweep configuration method `grid_shuffled`",
    "author": "RylanSchaeffer",
    "state": "open",
    "created_at": "2025-02-22T21:42:25Z",
    "updated_at": "2025-03-26T15:42:01Z",
    "labels": [
      "ty:feature",
      "c:sweeps"
    ],
    "body": "### Description\n\nW&B's sweep enables specifying a sweep `method`. I personally use `grid` the most. However, one downside of `grid` sweeping is that results are produced sequentially as determined by the order of `parameters` in the sweep. I would instead strongly prefer for the grid to be swept in _random_ order for me to get a sense of the results before the sweep has completed.\n\nBy analogy, I don't want a picture to fill in top-down or bottom-up; I'd rather the pixels appear in random order so I can get a sense of the overall structure of my results as the runs trickle in.\n\n### Suggested Solution\n\nCreate a `grid_shuffled` method that does a grid sweep albeit randomly permuting the order of the runs.",
    "comments": [
      {
        "user": "exalate-issue-sync[bot]",
        "body": "Bonnie Shen commented: \nHello @RylanSchaeffer! Thanks for writing to us! Hope you are having a lovely day!\nI also think this is a good feature to have. And I'm happy to file a feature request on your behalf.\n"
      },
      {
        "user": "dmitryshribak",
        "body": "Also very interested in this feature!"
      }
    ]
  },
  {
    "issue_number": 9519,
    "title": "[Feature]: One-click Automatic Color Contrast Optimization for Line Charts",
    "author": "songyuc",
    "state": "open",
    "created_at": "2025-02-26T03:37:45Z",
    "updated_at": "2025-03-26T15:41:49Z",
    "labels": [
      "ty:feature",
      "a:app"
    ],
    "body": "### Description\n\n**Summary:**  \nIt would be extremely useful to have a one-click feature in the chart settings that automatically adjusts the color palette of line charts to maximize the visual contrast between different series/items. This would help users easily distinguish between multiple lines without manually tweaking each color.\n\n**Motivation:**  \n- **Efficiency:** Manually adjusting colors can be time-consuming, especially when dealing with many data series.  \n- **Clarity:** An automatic optimization would ensure that each line is clearly distinguishable, enhancing the readability of the chart.  \n- **Accessibility:** Optimized high-contrast colors could also benefit users with color vision deficiencies, making visualizations more inclusive.\n\n**Proposed Feature:**  \n- **One-click Optimization:** A button or toggle in the chart settings to automatically apply a high-contrast color palette to all line charts.  \n- **Predefined Palettes:** Offer a selection of high-contrast color palettes (for example, options similar to ColorBrewer’s qualitative palettes like Set1, Dark2, etc.).  \n- **Manual Override:** Allow users to revert to manual color selection if they prefer custom settings.\n\n**Benefits:**  \n- **Streamlined Workflow:** Saves time for users who frequently work with visualizations by eliminating the need for manual color adjustments.  \n- **Enhanced Visual Quality:** Improves the overall presentation and interpretability of charts, which is crucial for data analysis and reporting.  \n- **Wider Accessibility:** Helps ensure that visualizations are accessible to a broader audience, including those with visual impairments.\n\n**Additional Context:**  \nI believe this feature would greatly benefit users who frequently work with multi-line charts, such as researchers and data scientists. It would enhance the usability of W&B’s visualization tools and contribute to a more user-friendly experience.\n\nThank you for considering this suggestion!\n\n### Suggested Solution\n\n_No response_",
    "comments": [
      {
        "user": "luisbergua",
        "body": "Hey @songyuc, thank you so much for sharing this great and detailed feedback! I'll ensure to share it with our Product Team"
      }
    ]
  }
]