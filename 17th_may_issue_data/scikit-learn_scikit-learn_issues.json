[
  {
    "issue_number": 31554,
    "title": "Allow batch based metrics calculation of sklearn.metrics",
    "author": "tahamukhtar20",
    "state": "open",
    "created_at": "2025-06-16T10:14:05Z",
    "updated_at": "2025-06-17T14:33:47Z",
    "labels": [
      "Performance",
      "module:metrics"
    ],
    "body": "### Describe the workflow you want to enable\n\nI have a lot of data and need to calculate metrics such as accuracy_score, jaccard_score, f1_score, recall, precision etc.\n\n### Describe your proposed solution\n\n When I try to calculate these it can literally take days, so i created a small solution which can batch and avg in the end, or for the weighted metrics it can do a weighted avg of each, this accelerated the calculation to just a couple of minutes, because I have a 32 core CPU. I'm willing to contribute with the proper guidance as I'm unfamiliar with the codebase, but I think many people can benefit from this. I'm unsure if there is already a work around of this present in the codebase, but if there is one do let me know, thanks a lot.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "Could you please explain more the situation? Where's the bottleneck? Calculating `.predict`? Or after that? Or that the data doesn't fit in your memory.\n\nWe'd need to have more details to be able to move forward. I'd be happy to give you pointers on where you could contribute then."
      },
      {
        "user": "tahamukhtar20",
        "body": "No, memory isn't an issue. It's also possible that I'm calculating it incorrectly, or there is already an existing workaround for this. But here is what's happening,\nThis is the current code:\n```\ndef evaluate(model: DeepLabV3Plus, test_loader):\n    model.eval()\n    all_preds, all_targets = [], []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n            inputs, targets = batch[\"image\"].to(DEVICE), batch[\"mask\"].to(DEVICE)\n            inputs = inputs.float().to(DEVICE)\n            outputs = model(inputs)\n            targets = targets.unsqueeze(1)\n            preds = torch.sigmoid(outputs).cpu().numpy()\n            all_preds.append(preds)\n            all_targets.append(targets.cpu().numpy())\n    print(\"Evaluation completed. Calculating metrics...\")\n\n    all_preds = np.concatenate(all_preds, axis=0)\n    all_targets = np.concatenate(all_targets, axis=0)\n\n    preds_binary = (all_preds > 0.5).astype(int)\n\n    accuracy = accuracy_score(all_targets.flatten(), preds_binary.flatten())\n    jaccard = jaccard_score(all_targets.flatten(), preds_binary.flatten(), average='binary')\n    f1 = f1_score(all_targets.flatten(), preds_binary.flatten(), average='binary')\n    recall = recall_score(all_targets.flatten(), preds_binary.flatten(), average='binary')\n    precision = precision_score(all_targets.flatten(), preds_binary.flatten(), average='binary')\n\n    print(f\"Accuracy: {accuracy:.4f}, Jaccard: {jaccard:.4f}, F1 Score: {f1:.4f}\")\n    print(f\"Recall: {recall:.4f}, Precision: {precision:.4f}\")\n```\nWhen I try to calculate it in this way, all the predictions and targets are calculated pretty quickly. When I flatten and give it to all the metrics(jaccard, f1, recall, precision, etc). It takes a long time, most probably because this is a lot of data in my case, roughly 17000 images, 512x512, flattened. The bottlenecks are the metric functions. I want a way to allow these functions to support some sort of parallelism by doing batch processing and averaging those, or something along those lines. Let me know what you think. I'm sure this can be implemented by applying batching or threading outside, but it would be nice to have if it were built in.\nIf there's already a recommended way to handle this, please let me know. If not, Iâ€™d be happy to help contribute a solution with guidance. Thanks!"
      },
      {
        "user": "adrinjalali",
        "body": "> When I try to calculate it in this way, all the predictions and targets are calculated pretty quickly. When I flatten and give it to all the metrics(jaccard, f1, recall, precision, etc). It takes a long time, most probably because this is a lot of data in my case, roughly 17000 images, 512x512, flattened.\n\nCan you paste the alternative code which is much slower?"
      }
    ]
  },
  {
    "issue_number": 31548,
    "title": "DOC About Us page: multi-column list for emeritus contributors",
    "author": "reshamas",
    "state": "open",
    "created_at": "2025-06-15T15:51:33Z",
    "updated_at": "2025-06-17T14:15:28Z",
    "labels": [
      "Documentation",
      "help wanted"
    ],
    "body": "References #31519 \nReferences #30826 \n\n---\n\nIt would be good to keep the file. The new proposed layout looks like this, and it save 28 lines of whitespace. So users can get to the important section faster, how to support scikit-learn.\n\n### Before\n<img width=\"1145\" alt=\"Screenshot 2025-06-12 at 6 53 17â€¯AM\" src=\"https://github.com/user-attachments/assets/59db6862-580d-41d2-ac0e-b5fd6629ee79\" />\n\n### After\n<img width=\"970\" alt=\"Screenshot 2025-06-12 at 6 52 40â€¯AM\" src=\"https://github.com/user-attachments/assets/371d8d38-1f47-4251-8753-445f363071c3\" />\n\n_Originally posted by @reshamas in https://github.com/scikit-learn/scikit-learn/pull/31519#discussion_r2142352666_\n            ",
    "comments": []
  },
  {
    "issue_number": 31542,
    "title": "Huber Loss for HistGradientBoostingRegressor",
    "author": "AndrewYRoyal",
    "state": "open",
    "created_at": "2025-06-13T13:24:16Z",
    "updated_at": "2025-06-17T13:38:19Z",
    "labels": [
      "New Feature",
      "help wanted",
      "Hard"
    ],
    "body": "### Describe the workflow you want to enable\n\nHuber loss is available as an option for `GradientBoostingRegressor` and works great when training on data with frequent outliers (thank you!). `HistGradientBoostingRegressor` however does not support Huber loss, which may be required when scaling to larger datasets. \n\n### Describe your proposed solution\n\nAdd HuberLoss as an option for the `HistGradientBoostingRegressor` class. \n\n### Describe alternatives you've considered, if relevant\n\nPossibly allow custom loss functions for the `HistGradientBoostingRegressor`\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "Adding the loss seems quite reasonable. Adding a custom loss might be a bit more complicated, and hard to do with good performance. If you think you can add the loss with a PR, we'd welcome that.\n\nI wonder if @lorentzenchr has an opinion on this one."
      },
      {
        "user": "Nithishkaranam2002",
        "body": "I'd be interested in contributing Huber loss support to HistGradientBoostingRegressor. Let me know if this is something the maintainers would support.\n"
      },
      {
        "user": "lorentzenchr",
        "body": "I would prefer to not implement Huber for HGBT. Mainly because it is not that clear, what a model minimizing Huber loss actually estimates/predicts, something between mean and median. Maybe, @AndrewYRoyal's use case can be solved differently? It would help if you could outline our use case briefly."
      }
    ]
  },
  {
    "issue_number": 22893,
    "title": "SLEP006 - Metadata Routing task list",
    "author": "adrinjalali",
    "state": "open",
    "created_at": "2022-03-18T16:27:22Z",
    "updated_at": "2025-06-17T13:35:19Z",
    "labels": [
      "Hard",
      "Meta-issue",
      "Metadata Routing"
    ],
    "body": "This issue is to track the work we need to do before we can merge `sample-props` branch into `main`:\r\n\r\n- [x] Based on this prototype: https://github.com/scikit-learn/scikit-learn/pull/16079\r\n- [x] Merge https://github.com/scikit-learn/scikit-learn/pull/22083 into `sample-props`. This PR only touches `BaseEstimator` and hence _consumers_. It does NOT touch meta-estimators, scorers or cv splitters.\r\n- [x] Work on splitters and merge that into `sample-props`: https://github.com/scikit-learn/scikit-learn/pull/22765\r\n- [x] feature flag: https://github.com/scikit-learn/scikit-learn/issues/26045: https://github.com/scikit-learn/scikit-learn/pull/26103\r\n- [x] Add new terms to the glossary: router, consumer, metadata: https://github.com/scikit-learn/scikit-learn/pull/26685\r\n- [x] Work on scorers and merge that into `sample-props` ~(note that this involves an ongoing discussion on whether we'd like to mutate a scorer or not)~: https://github.com/scikit-learn/scikit-learn/pull/22757\r\n- [x] Get backward compatibility mechanisms in place for meta-estimators which already route given metadata: https://github.com/scikit-learn/scikit-learn/pull/22986\r\n- [ ] Work on meta-estimators: this involves writing `get_metadata_routing` in easy cases, and a whole lot more in cases where we'd like to keep backward compatibility in parsing input args suchs as `estimator__param` in `Pipeline`\r\n- [x] Once all the above is merged into `sample-props`, do a few tests with third party estimators to see if they'd work out of the box. Note that _consumer_ estimators should work out of the box as long as they inherit from `BaseEstimator` and their `fit` accepts metadata as explicit arguments rather than `**kwargs`.\r\n  - Merged w/o completion of meta-estimators, they got done progressively.\r\n- [x] Check whether a library such as cuML could vendor `_metadata_requests.py` and work with scikit-learn meta-estimators w/o depending on the library.\r\n  - dropped the idea of supporting vendoring, they'd need to inherit from `BaseEstimator`\r\n- [x] Refactor tests: https://github.com/scikit-learn/scikit-learn/issues/23918\r\n- [x] Merge `sample-props` into `main`: https://github.com/scikit-learn/scikit-learn/pull/24027\r\n\r\n## Enhancements:\r\n- [x] https://github.com/scikit-learn/scikit-learn/pull/24023\r\n- [ ] https://github.com/scikit-learn/scikit-learn/issues/18936\r\n- [ ] https://github.com/scikit-learn/scikit-learn/issues/28467\r\n\r\n## Cleanup\r\n- [x] https://github.com/scikit-learn/scikit-learn/issues/26505\r\n\t- [x] https://github.com/scikit-learn/scikit-learn/pull/26503\r\n\r\n## Open issues:\r\n- [x] https://github.com/scikit-learn/scikit-learn/issues/22987\r\n\t- [x] https://github.com/scikit-learn/scikit-learn/pull/26506\r\n- [x] https://github.com/scikit-learn/scikit-learn/issues/22988: https://github.com/scikit-learn/scikit-learn/pull/23342\r\n- [ ] https://github.com/scikit-learn/scikit-learn/issues/23920\r\n- [ ] https://github.com/scikit-learn/scikit-learn/issues/23928\r\n\t- [ ] global flags: https://github.com/scikit-learn/scikit-learn/issues/26050\r\n\t- [ ] auto-routing: https://github.com/scikit-learn/scikit-learn/issues/26179\r\n- [x] https://github.com/scikit-learn/scikit-learn/issues/23933\r\n\t- [x] https://github.com/scikit-learn/scikit-learn/pull/26911\r\n- [x] https://github.com/scikit-learn/scikit-learn/issues/25776: https://github.com/scikit-learn/scikit-learn/pull/25852 - resolution is to use the feature flag instead of breaking change.\r\n- [ ] https://github.com/scikit-learn/scikit-learn/issues/27977\r\n\r\nOur plan is to hopefully have this feature in 1.1, which we should be releasing in late April/early May.\r\n\r\ncc @jnothman @thomasjpfan @lorentzenchr \r\n\r\n## Misc:\r\n\r\n- https://github.com/scikit-learn/scikit-learn/pull/25851\r\n- https://github.com/scikit-learn/scikit-learn/pull/27389\r\n\r\n-------------------------------------------------------------\r\nFrom #24027 \r\n\r\nOnce we're finished with merging PRs to `sample-props` branch, we should merge this.\r\n\r\nI've opened this for us to have an idea of what the whole project is touching.\r\n\r\nWe probably should rebase and merge this instead of squash and merge.\r\n\r\nThis closes and fixes the following issues and PRs:\r\n\r\n- Routing issues:\r\nEnables #6322, \r\n\r\n- AdaBoost:\r\nFixes #2630, \r\nFixes #21706\r\n\r\n- ClassifierChain:\r\nFixes  #11429, \r\n\r\n- FeatureUnion:\r\nFixes #7136\r\n\r\n- LogisticRegressionCV:\r\nFixes  #8950,\r\n\r\n- Multioutput*\r\nFixes #15953\r\n\r\n- OneVsRestClassifier\r\nFixes #10882\r\n\r\n- RFE/RFECV:\r\nFixes #7308\r\n\r\n- *SearchCV:\r\nFixes #8127, \r\nFixes #8158,\r\n\r\n- cross_val*:\r\nFixes #13432, \r\nFixes #4632, \r\nFixes #7646, \r\nFixes #20349\r\n\r\n- Stacking*\r\nFixes #18028,\r\n\r\n- Voting*\r\nFixes #20167\r\n",
    "comments": [
      {
        "user": "jnothman",
        "body": "Do you see this as a collaborative effort? To what extent can we still use #20350? To what extent can we share test components? Which of these metaestimators have existing fit param routing that needs to be deprecated?"
      },
      {
        "user": "jnothman",
        "body": "Your list appears to be missing some `*CV` estimators (E.g. ElasticNetCV) that will have to route to splitters if not scorers."
      },
      {
        "user": "jnothman",
        "body": "Also missing are functions in `sklearn.model_selection._validation`"
      }
    ]
  },
  {
    "issue_number": 26024,
    "title": "Make more of the \"tools\" of scikit-learn Array API compatible",
    "author": "betatim",
    "state": "open",
    "created_at": "2023-03-30T12:01:18Z",
    "updated_at": "2025-06-17T13:34:20Z",
    "labels": [
      "API",
      "Meta-issue",
      "Array API"
    ],
    "body": "ðŸš¨ ðŸš§ This issue requires a bit of patience and experience to contribute to ðŸš§ ðŸš¨ \r\n\r\n- Original issue introducing array API in scikit-learn: #22352\r\n- array API official doc/spec: https://data-apis.org/array-api/\r\n- scikit-learn doc: https://scikit-learn.org/dev/modules/array_api.html\r\n\r\nPlease mention this issue when you create a PR, but please don't write \"closes #26024\" or \"fixes #26024\".\r\n\r\nscikit-learn contains lots of useful tools, in addition to the many estimators it has. For example [metrics](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics), [pipelines](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.pipeline), [pre-processing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) and [mode selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection). These are useful to and used by people who do not necessarily use an estimator from scikit-learn. This is great.\r\n\r\nThe fact that many users install scikit-learn \"just\" to use `train_test_split` is a testament to how useful it is to provide easy to use tools that do the right(!) thing. Instead of everyone implementing them from scratch because it is \"easy\" and making mistakes along the way.\r\n\r\nIn this issue I'd like to collect and track work related to making it easier to use all these \"tools\" from scikit-learn even if you are not using Numpy arrays for your data. In particular thanks to the Array API standard it should be \"not too much work\" to make things usable with data that is in an array that conforms to the Array API standard.\r\n\r\nThere is work in #25956 and #22554 which adds the basic infrastructure needed to use \"array API arrays\".\r\n\r\nThe goal of this issue is to make code like the following work:\r\n```python\r\n>>> from sklearn.preprocessing import MinMaxScaler\r\n>>> from sklearn import config_context\r\n>>> from sklearn.datasets import make_classification\r\n>>> import torch\r\n>>> X_np, y_np = make_classification(random_state=0)\r\n>>> X_torch = torch.asarray(X_np, device=\"cuda\", dtype=torch.float32)\r\n>>> y_torch = torch.asarray(y_np, device=\"cuda\", dtype=torch.float32)\r\n\r\n>>> with config_context(array_api_dispatch=True):\r\n...     # For example using MinMaxScaler on PyTorch tensors\r\n...     scale = MinMaxScaler()\r\n...     X_trans = scale.fit_transform(X_torch, y_torch)\r\n...     assert type(X_trans) == type(X_torch)\r\n...     assert X_trans.device == X_torch.device\r\n```\r\n\r\nThe first step (or maybe part of the first) is to check which of them already \"just work\". After that is done we can start the work (one PR per class/function) making changes.\r\n\r\n\r\n## Guidelines for testing\r\n\r\nGeneral comment: most of the time when we add array API support to a function in scikit-learn, we do not touch the existing (numpy-only) tests to make sure that the PR does not change the default behavior of scikit-learn on traditional inputs when array API is not enabled.\r\n\r\nIn the case of an estimator, it can be enough to add the `array_api_support=True` estimator tag in a method named `__sklearn_tags__`. For metric functions, just register it in the `array_api_metric_checkers` in `sklearn/metrics/tests/test_common.py` to include it in the common test.\r\n\r\nFor other kinds of functions not covered by existing common tests, or when the array API support depends heavily on non-default values, it might be required to add one or more new test functions to the related module-level test file. The general testing scheme is the following:\r\n\r\n- generate some random test data with numpy or `sklearn.datasets.make_*`;\r\n- call the function once on the numpy inputs without enabling array API dispatch;\r\n- convert the inputs to a namespace / device combo passed as parameter to the test;\r\n- call the function with array API dispatching enabled (under a `with sklearn.config_context(array_api_dispatch=True)` block\r\n- check that the results are on the same namespace and device as the input\r\n- convert back the output to a numpy array using `_convert_to_numpy`\r\n- compare the original / reference numpy results and the `xp` computation results converted back to numpy using `assert_allclose` or similar.\r\n\r\nThose tests should have `array_api` somewhere in their name to makes sure that we can run all the array API compliance tests with a keyword search in the pytest command line, e.g.:\r\n\r\n```\r\npytest -k array_api sklearn/some/subpackage\r\n```\r\n\r\nIn particular, for cost reasons, our CUDA GPU CI only runs `pytest -k array_api sklearn`. So it's very important to respect this naming conventions, otherwise we will not tests all what we are supposed to test on CUDA.\r\n\r\nMore generally, look at merged array API pull requests to see how testing is typically handled.",
    "comments": [
      {
        "user": "betatim",
        "body": "Mark an estimator or function as done if it not only \"doesn't raise an exception\" but also outputs a sensible value. The latter is something that will require a human at the start, but maybe later we can write a test for it.\n\nBelow a list of preprocessors and metrics. The lists are pretty long already, so I won't add more stuff until we make progress (or decide that a different area is a better starting point).\n\nThe next thing to work on is to work out if there is some generic advice around \"fixing\" these.\n\nNOTE: it's possible to test the changes in your pull request on a CUDA GPU host for free with the help of this notebook on Google Colab: https://gist.github.com/EdAbati/ff3bdc06bafeb92452b3740686cc8d7c\n\n---\n\nTransformers from `sklearn.preprocessing`:\n* [x] Binarizer https://github.com/scikit-learn/scikit-learn/pull/31190\n* [ ] FunctionTransformer - no exception with pytorch `X`\n* [ ] KBinsDiscretizer\n* [x] KernelCenterer https://github.com/scikit-learn/scikit-learn/pull/27556\n* [ ] ~~LabelBinarizer https://github.com/scikit-learn/scikit-learn/pull/28626~~ (_Not suitable for the array API_)\n* [x] LabelEncoder https://github.com/scikit-learn/scikit-learn/pull/27381\n* [x] MaxAbsScaler https://github.com/scikit-learn/scikit-learn/pull/27110\n* [x] MinMaxScaler https://github.com/scikit-learn/scikit-learn/pull/26243 and #29751\n* [ ] ~~MultiLabelBinarizer~~ (_Not suitable for the array API_)\n* [x] Normalizer https://github.com/scikit-learn/scikit-learn/pull/27558\n* [ ] OneHotEncoder (_Mainly used to convert categorical data, thus not suitable for the array API_)\n* [ ] OrdinalEncoder (_Mainly used to convert categorical data, thus not suitable for the array API_)\n* [ ] PolynomialFeatures\n* [ ] PowerTransformer (depends on https://github.com/scikit-learn/scikit-learn/pull/27113)\n* [ ] QuantileTransformer (requires https://github.com/data-apis/array-api/issues/795)\n* [ ] RobustScaler (requires https://github.com/data-apis/array-api/issues/795)\n* [ ] SplineTransformer (requires https://github.com/data-apis/array-api/issues/795)\n* [ ] StandardScaler https://github.com/scikit-learn/scikit-learn/pull/27113\n* [ ] TargetEncoder\n\n<details>\n<summary>Details</summary>\nCode used to create the list:\n\n```python\nfor name, Trn in discovery.all_estimators(type_filter=\"transformer\"):\n    if Trn.__module__.startswith(\"sklearn.preprocessing.\"):\n        with config_context(array_api_dispatch=True):\n            tr = Trn()\n            try:\n                tr.fit_transform(X_torch, y_torch)\n                print(f\"* [ ] {name} - no exception with pytorch X\")\n            except:\n                print(f\"* [ ] {name}\")\n```\n</details>\n\nMetrics from `sklearn.metrics`:\n* [x] accuracy_score https://github.com/scikit-learn/scikit-learn/pull/27137\n* [x] additive_chi2_kernel https://github.com/scikit-learn/scikit-learn/pull/29144\n* [ ] adjusted_mutual_info_score\n* [ ] adjusted_rand_score\n* [ ] auc https://github.com/scikit-learn/scikit-learn/pull/29755\n* [ ] average_precision_score\n* [ ] balanced_accuracy_score\n* [ ] brier_score_loss #31191\n* [ ] calinski_harabasz_score\n* [ ] check_paired_arrays\n* [ ] check_pairwise_arrays\n* [ ] check_scoring\n* [x] chi2_kernel https://github.com/scikit-learn/scikit-learn/pull/29267\n* [ ] class_likelihood_ratios\n* [ ] classification_report\n* [ ] cohen_kappa_score\n* [ ] completeness_score\n* [ ] confusion_matrix #30440\n* [ ] consensus_score\n* [ ] contingency_matrix #29251\n* [x] cosine_distances https://github.com/scikit-learn/scikit-learn/pull/29265\n* [x] cosine_similarity https://github.com/scikit-learn/scikit-learn/pull/29014\n* [ ] coverage_error\n* [ ] d2_absolute_error_score\n* [ ] d2_pinball_score\n* [x] d2_tweedie_score https://github.com/scikit-learn/scikit-learn/pull/29207\n* [ ] davies_bouldin_score\n* [ ] dcg_score https://github.com/scikit-learn/scikit-learn/pull/29339, #31152\n* [ ] det_curve\n* [ ] distance_metrics\n* [x] entropy https://github.com/scikit-learn/scikit-learn/pull/29141\n* [x] euclidean_distances https://github.com/scikit-learn/scikit-learn/pull/29433\n* [x] explained_variance_score https://github.com/scikit-learn/scikit-learn/pull/29978\n* [x] f1_score https://github.com/scikit-learn/scikit-learn/pull/27369\n* [x] fbeta_score https://github.com/scikit-learn/scikit-learn/pull/30395\n* [ ] fowlkes_mallows_score\n* [ ] get_scorer\n* [ ] get_scorer_names\n* [x] hamming_loss https://github.com/scikit-learn/scikit-learn/pull/30838\n* [ ] haversine_distances\n* [ ] hinge_loss (_Depends on LabelBinarizer_: https://github.com/scikit-learn/scikit-learn/pull/28626)\n* [ ] homogeneity_completeness_v_measure\n* [ ] homogeneity_score\n* [ ] jaccard_score\n* [ ] kernel_metrics\n* [ ] label_ranking_average_precision_score\n* [ ] label_ranking_loss\n* [ ] laplacian_kernel https://github.com/scikit-learn/scikit-learn/pull/29881\n* [x] linear_kernel https://github.com/scikit-learn/scikit-learn/pull/29475\n* [ ] log_loss https://github.com/scikit-learn/scikit-learn/pull/30439 (_Depends on LabelBinarizer_)\n* [ ] make_scorer\n* [ ] manhattan_distances https://github.com/scikit-learn/scikit-learn/pull/29881\n* [ ] matthews_corrcoef\n* [x] max_error  #29212\n* [x] mean_absolute_error https://github.com/scikit-learn/scikit-learn/pull/27736\n* [x] mean_absolute_percentage_error https://github.com/scikit-learn/scikit-learn/pull/29300\n* [x] mean_gamma_deviance https://github.com/scikit-learn/scikit-learn/pull/29239\n* [x] mean_pinball_loss https://github.com/scikit-learn/scikit-learn/pull/29978\n* [x] mean_poisson_deviance https://github.com/scikit-learn/scikit-learn/pull/29227\n* [x] mean_squared_error #29142\n* [x] mean_squared_log_error https://github.com/scikit-learn/scikit-learn/pull/29709\n* [x] mean_tweedie_deviance #28106\n* [x] median_absolute_error #31406\n* [x] multilabel_confusion_matrix #27369\n* [ ] mutual_info_score\n* [ ] nan_euclidean_distances\n* [ ] ndcg_score #31152\n* [ ] normalized_mutual_info_score\n* [ ] pair_confusion_matrix\n* [x] paired_cosine_distances https://github.com/scikit-learn/scikit-learn/pull/29112\n* [ ] paired_distances\n* [ ] paired_euclidean_distances\n* [ ] paired_manhattan_distances\n* [ ] pairwise_distances\n* [ ] pairwise_distances_argmin\n* [ ] pairwise_distances_argmin_min\n* [ ] pairwise_distances_chunked - no exception with pytorch y\n* [ ] pairwise_kernels #29822\n* [x] polynomial_kernel https://github.com/scikit-learn/scikit-learn/pull/29475\n* [ ] precision_recall_curve\n* [x] precision_recall_fscore_support https://github.com/scikit-learn/scikit-learn/pull/27369\n* [x] precision_score https://github.com/scikit-learn/scikit-learn/pull/30395\n* [x] r2_score #27904\n* [ ] rand_score\n* [x] rbf_kernel https://github.com/scikit-learn/scikit-learn/pull/29433\n* [x] recall_score https://github.com/scikit-learn/scikit-learn/pull/30395\n* [ ] roc_auc_score\n* [ ] roc_curve #30878\n* [x] sigmoid_kernel https://github.com/scikit-learn/scikit-learn/pull/29475\n* [ ] silhouette_samples\n* [ ] silhouette_score\n* [ ] top_k_accuracy_score\n* [ ] v_measure_score\n* [x] zero_one_loss https://github.com/scikit-learn/scikit-learn/pull/27137\n\n<details>\n<summary>Details</summary>\n\n```python\nfor name, func in discovery.all_functions():\n    if func.__module__.startswith(\"sklearn.metrics.\"):\n        with config_context(array_api_dispatch=True):\n            try:\n                func(y_torch, y_torch)\n                print(f\"* [ ] {name} - no exception with pytorch y\")\n            except:\n                print(f\"* [ ] {name}\")\n```\n\n</details>"
      },
      {
        "user": "betatim",
        "body": "It turns out, it is more tricky than you think. For example in `MinMaxScaler` we compute the min/max of the features, ignoring NaNs. However, there doesn't seem to be a nice way to do this with the Array API (yet). Filed https://github.com/data-apis/array-api/issues/621 to discuss it."
      },
      {
        "user": "betatim",
        "body": "A more comprehensible (less sentence fragments) version of the below text is in https://github.com/scikit-learn/scikit-learn/pull/25956/files#r1172450244\r\n\r\n---\r\n\r\nSome thoughts: should we add a \"compat layer\" in scikit-learn to add things like `nanmin` to the namespace? Should we contribute it to the array-api-compat project? should we lobby for it to be part of the array api itself?\r\n\r\ncc @thomasjpfan maybe you have thoughts/opinion and/or time to chat about this."
      }
    ]
  },
  {
    "issue_number": 30621,
    "title": "Add links to examples from the docstrings and user guide",
    "author": "StefanieSenger",
    "state": "open",
    "created_at": "2025-01-10T12:29:04Z",
    "updated_at": "2025-06-17T13:01:39Z",
    "labels": [
      "Documentation",
      "Sprint",
      "good first issue",
      "Meta-issue"
    ],
    "body": "_TLDR: Meta-issue for new contributors to add links to the examples in helpful places of the rest of the docs._\n\n## Description\nThis meta-issue is a good place to start with your first contributions to scikit-learn.\n\nThis issue builds on top of #26927 and is introduced for easier maintainability. The goal is exactly the same as in the old issue.\n\nHere, we improve the documentation by making the [Examples](https://scikit-learn.org/stable/auto_examples/index.html) more discoverable by **adding links to examples in relevant sections of the documentation in the _API documentation_ and in the _User Guide_**:\n- the [API documentation](https://scikit-learn.org/stable/api/index.html)  is made from the docstrings of public classes and functions which can be found in the `sklearn` folder of the project\n- the [User Guide](https://scikit-learn.org/stable/user_guide.html) can be found in the `doc/modules` folder of the project\n\nTogether with the [examples](https://scikit-learn.org/stable/auto_examples/index.html) (which are in the `examples` folder of the project), these files get rendered into html when the documentation is build and then are displayed on the [scikit-learn website](https://scikit-learn.org).\n\n**Important: We estimate that only 70% of the examples in this list will ultimately be referenced. This means part of the task is deciding which examples deserve being referenced and we are aware that this is not a trivial decision, especially for new contributors. We encourage you to share your reasoning, and a team member will make the final call. We hope this isnâ€™t too frustrating, but please know that evaluating an example is not just an exercise for new contributors; itâ€™s a meaningful and valuable contribution to the project, even (and especially) if the example you worked on doesnâ€™t end up being linked.**\n\n\n## Workflow\nWe recommend this workflow for you:\n\n0. have `pre-commit` installed in your environment as in point 10 of _How to contribute_ in the [development guide](https://scikit-learn.org/dev/developers/contributing.html#contributing-code) (this will re-format your contribution to the standards used in scikit-learn and will spare you a lot of confusion when you are a beginner)\n\n1. pick an example to work on\n    - Make sure your example of interest had not recently been claimed by someone else by looking through the discussion of this issue (you will have to load hidden items in this discussion). Hint: If somebody has claimed an example several weeks ago and then never started it, you can take it. You can also take over tasks marked as _stalled_.\n    - search the repo for other links to your example and check if the example is already linked in relevant parts of the docs\n        - how to search the repo: a) find the file name of your example in the examples folder (it starts with `plot_...`); b) use full text search of your IDE to look for where that name appears\n        - you can totally ignore the \"Gallery examples\" on the website, as it is auto-generated; do only look for real links in the repo\n    - comment on the issue to claim an example (you don't need to wait for a team member's approval before starting to work)\n\n2. find suitable spots in either the _API documentation_  or the _User Guide_ (or both) where users would be happy to find your example linked\n    - read through your example and understand where it is making its most useful statements\n    - how to find a good spot (careful: we are extremely picky here)\n        - if the example demonstrates a certain real world use case: find where in the _User Guide_ the same use case is treated or could be treated\n        - if the example shows how to use a certain param: the param description in the _API documentation_ might be a good spot to put the link\n        - if the example compares different techniques: this highly calls for mentioning it in the more theoretical parts of the _User Guide_\n        - not all the examples listed here need to be referenced: a link to an example on simply how to use some estimator, doesn't add enough value\n            - if you find an example that doesn't add enough value to be linked: please leave a comment here; this kind of contribution is highly appreciated\n    - not a good spot: the `See Also` section, which is (theoretically) reserved for links to other API functionalities, not examples\n\n3. add links\n    - An example with the path examples/developing_estimators/sklearn_is_fitted.py whould be referenced like this: \n    ```     \n      :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    ```\n     - see this example PR, that shows how to add a link to the User Guide: #26926\n     - we aim **not** to use the `.. rubric:: Examples` section to put the example if possible, but to integrate it into the text; be aware that if you add a link like this \\:ref:\\`title \\<link\\>\\`, you can change its title so that the example's title gets substituted by your picked title and the link can be fitted more nicely to the sentences\n    - please avoid adding your link to a list of other examples, since we strive to add the links in the most relevant places\n    - please avoid adding a new `.. rubric:: Examples` section\n\n4. test build the documentation before opening your PR\n    - have a look into the [Documentation part of the Development Guide](https://scikit-learn.org/dev/developers/contributing.html#building-the-documentation) to learn how to locally build the documentation.\n    - Check if your changes are displayed as desired by opening the test build in your browser.\n\n5. open PR\n    - use a PR title like `DOC add links to <name of example>` (starting with DOC)\n    - do not refer to this issue on the title of the PR, instead: \n    - do refer to this issue using in the *Reference Issues/PRs* section of your PR, do refer to this issue using \"Towards `#30621`\" (do **not** use \"Closes #...\" or \"Fixes #...\")\n\n \n6. check the CI\n    - After the CI tests have finished (~90 minutes) you can find one that says \"Check the rendered docs here!\". In there, you can look into how the CI has built the documentation for the changed files to check if everything looks alright. You will see something like `auto_examples/path_to_example, [dev], [stable]`, where the first link is your branche's version, the second is the main dev branch and the third link is the last released scikit-learn version that is used for the stable documentation on the website.\n    - if the CI shows any failure, you should to take action by investigating and proposing solutions; as a rule of thump, you can find the most useful information from the CIs, if you click the upper links first; in any case you need to click through several layers until you see actual test results with more information (and until it looks similar to running pytest, ruff or doctest locally)\n    - if the CI shows linting issues, check if you have installed and activated `pre-commit` properly, and fix the issue by the action the CI proposes (for instance adding or deleting an empty line)\n    - if you are lost and don't know what to do with a CI failure, look through other PRs from this issue; most things have already happened to others\n    - sometimes, http request errors such as 404 or 405 show up in the CI, in which case you should push an empty commit (`git commit --allow-empty -m \"empty commit to re-trigger CI\"`)\n\n7. wait for reviews and be ready to adjust your contribution later on\n\n## Expectation management for new contributors\n\n How long will your first PR take you up until the point you open a PR?\n- 8-16 hours if you have never contributed to any project and have only basic or no understanding of the workflow yet\n- 2-8 hours if you know the workflow and are just new to scikit-learn (more to the shorter end if you know about linting and sphinx and are able to address CI outputs)\n- 1-2 hours for your 2nd, 3rd, ... PR on the same issue for everyone\n\nHow long will it take us to merge your PR?\n- we strive for a scikit-learn member to look at your PR within a few days and suggest changes depending on technical quality of the PR and an assessment of added value to the user (if the link is useful to have in the spot you suggested)\n- we strive for a maintainer to evaluate your PR within a few weeks; they might also suggest changes before approving and merging\n- the whole process on average takes several weeks and can take up months, depending of availability of maintainers and on how many review cycles are necessary\n\n## ToDo\nHere's a list of all the remaining examples:\n\n- examples/applications:\n  - [x] plot_model_complexity_influence.py #no references need to be added: #30814\n  - [ ] plot_out_of_core_classification.py #30462 (stalled)\n  - [x] plot_prediction_latency.py #30462 (stalled) #31477\n  - [ ] plot_topics_extraction_with_nmf_lda.py\n- examples/bicluster:\n  - [ ] plot_bicluster_newsgroups.py #31393\n  - [ ] plot_spectral_coclustering.py #29606 (stalled) #31422\n- examples/calibration:\n  - [ ] plot_compare_calibration.py\n- examples/classification:\n  - [ ] plot_classifier_comparison.py\n  - [x] plot_digits_classification.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/31258#pullrequestreview-2802839406)\n- examples/cluster:\n  - [x] plot_agglomerative_clustering_metrics.py #30867\n  - [x] plot_cluster_comparison.py #30127\n  - [x] plot_coin_ward_segmentation.py #30916\n  - [x] plot_dict_face_patches.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/issues/30621#issuecomment-2716167959)\n  - [ ] plot_digits_agglomeration.py #30979\n  - [ ] plot_digits_linkage.py\n  - [ ] plot_face_compress.py\n  - [x] plot_inductive_clustering.py #30182\n  - [x] plot_segmentation_toy.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/30978#issuecomment-2851550593) #30978\n  - [ ] plot_ward_structured_vs_unstructured.py #30861\n- examples/covariance:\n  - [ ] plot_mahalanobis_distances.py #https://github.com/scikit-learn/scikit-learn/pull/31485\n  - [ ] plot_robust_vs_empirical_covariance.py #31511\n  - [x] plot_sparse_cov.py #31278\n- examples/decomposition:\n  - [x] plot_ica_blind_source_separation.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/issues/30621#issuecomment-2649370018): https://github.com/scikit-learn/scikit-learn/pull/30786\n  - [x] plot_ica_vs_pca.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/issues/30621#issuecomment-2649370018):  https://github.com/scikit-learn/scikit-learn/pull/30786\n  - [ ] plot_image_denoising.py #30864\n  - [ ] plot_sparse_coding.py #31472\n  - [ ] plot_varimax_fa.py\n- examples/ensemble:\n  - [ ] plot_bias_variance.py #30845\n  - [ ] plot_ensemble_oob.py #31514\n  - [ ] plot_feature_transformation.py\n  - [ ] plot_forest_hist_grad_boosting_comparison.py\n  - [ ] plot_forest_importances_faces.py\n  - [x] plot_forest_importances.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/issues/30621#issuecomment-2731163071)\n  - [x] plot_forest_iris.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/issues/30621#issuecomment-2676356956)\n  - [x] plot_gradient_boosting_categorical.py #30749\n  - [x] plot_gradient_boosting_oob.py #30749\n  - [x] plot_gradient_boosting_regularization.py #30749\n  - [ ] plot_monotonic_constraints.py #31471\n  - [ ] plot_random_forest_regression_multioutput.py\n  - [x] plot_stack_predictors.py #30747\n  - [x] plot_voting_decision_regions.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/30847#discussion_r1963601795) #30847\n  - [x] plot_voting_probas.py #30847\n- examples/feature_selection:\n  - [x] plot_feature_selection.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/31000#issuecomment-2728836616) #31000\n  - [x] plot_f_test_vs_mi.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/issues/30621#issuecomment-2734809734)\n  - [ ] plot_rfe_with_cross_validation.py\n  - [ ] plot_select_from_model_diabetes.py\n- examples/gaussian_process:\n  - [ ] plot_gpc_iris.py #30605 (stalled)\n  - [ ] plot_gpc_isoprobability.py #30605 (stalled)\n  - [ ] plot_gpc.py #30605 (stalled)\n  - [ ] plot_gpc_xor.py #30605 (stalled)\n  - [ ] plot_gpr_co2.py\n  - [ ] plot_gpr_noisy.py\n  - [x] plot_gpr_noisy_targets.py #30850\n  - [x] plot_gpr_on_structured_data.py #31150\n  - [ ] plot_gpr_prior_posterior.py\n- examples/inspection:\n  - [x] plot_causal_interpretation.py #30752\n  - [ ] plot_linear_model_coefficient_interpretation.py\n  - [ ] plot_permutation_importance_multicollinear.py\n  - [ ] plot_permutation_importance.py\n- examples/linear_model:\n  - [x] plot_ard.py #31425\n  - [ ] plot_huber_vs_ridge.py\n  - [ ] plot_iris_logistic.py\n  - [x] plot_lasso_and_elasticnet.py #30587 (stalled)  #31425\n  - [ ] plot_lasso_coordinate_descent_path.py\n  - [ ] plot_lasso_dense_vs_sparse_data.py\n  - [ ] plot_lasso_lars_ic.py\n  - [ ] plot_lasso_lars.py\n  - [x] plot_lasso_model_selection.py [no reference needs to be added](https://github.com/scikit-learn/scikit-learn/pull/31522#pullrequestreview-2932240624) #31522\n  - [ ] plot_logistic_l1_l2_sparsity.py\n  - [ ] plot_logistic_multinomial.py\n  - [ ] plot_logistic_path.py\n  - [ ] plot_logistic.py #30942 [example should be removed](https://github.com/scikit-learn/scikit-learn/pull/30942#issuecomment-2834605886) (stalled)\n  - [ ] plot_multi_task_lasso_support.py\n  - [x] plot_nnls.py #31280\n  - [ ] plot_ols_3d.py\n  - [x] plot_ols.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/issues/30621#issuecomment-2600872584)\n  - [x] plot_ols_ridge_variance.py #30683\n  - [ ] plot_omp.py\n  - [ ] plot_poisson_regression_non_normal_loss.py\n  - [ ] plot_polynomial_interpolation.py\n  - [ ] plot_quantile_regression.py\n  - [ ] plot_ridge_coeffs.py\n  - [ ] plot_ridge_path.py\n  - [ ] plot_robust_fit.py\n  - [ ] plot_sgd_comparison.py\n  - [ ] plot_sgd_iris.py\n  - [ ] plot_sgd_separating_hyperplane.py\n  - [ ] plot_sgd_weighted_samples.py\n  - [ ] plot_sparse_logistic_regression_20newsgroups.py\n  - [ ] plot_sparse_logistic_regression_mnist.py\n  - [ ] plot_theilsen.py\n  - [ ] plot_tweedie_regression_insurance_claims.py\n- examples/manifold:\n  - [ ] plot_lle_digits.py\n  - [x] plot_manifold_sphere.py #30959\n  - [x] plot_swissroll.py #31378\n  - [ ] plot_t_sne_perplexity.py\n- examples/miscellaneous:\n  - [ ] plot_anomaly_comparison.py\n  - [ ] plot_display_object_visualization.py\n  - [ ] plot_estimator_representation.py\n  - [ ] plot_johnson_lindenstrauss_bound.py\n  - [ ] plot_kernel_approximation.py\n  - [ ] plot_metadata_routing.py\n  - [ ] plot_multilabel.py\n  - [x] plot_multioutput_face_completion.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/issues/30621#issuecomment-2676356956)\n  - [ ] plot_outlier_detection_bench.py\n  - [ ] plot_partial_dependence_visualization_api.py\n  - [ ] plot_pipeline_display.py\n  - [ ] plot_roc_curve_visualization_api.py\n  - [ ] plot_set_output.py\n- examples/mixture:\n  - [ ] plot_concentration_prior.py\n  - [x] plot_gmm_covariances.py #31249\n  - [ ] plot_gmm_init.py\n  - [ ] plot_gmm_pdf.py #31230 (stalled)\n  - [x] plot_gmm.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/30841#issue-2855807102): #30841\n  - [x] plot_gmm_selection.py #30841\n  - [x] plot_gmm_sin.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/30841#issue-2855807102): #30841\n- examples/model_selection:\n  - [x] plot_confusion_matrix.py #30949\n  - [ ] plot_cv_predict.py #31504\n  - [x] plot_det.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/30977#pullrequestreview-2684987302)\n  - [ ] plot_grid_search_digits.py\n  - [ ] plot_grid_search_refit_callable.py\n  - [ ] plot_grid_search_stats.py #30965\n  - [ ] plot_grid_search_text_feature_extraction.py #30974\n  - [ ] plot_likelihood_ratios.py\n  - [ ] plot_multi_metric_evaluation.py\n  - [ ] plot_permutation_tests_for_classification.py\n  - [x] plot_precision_recall.py [#no reference needs to be added](https://github.com/scikit-learn/scikit-learn/issues/30621#issuecomment-2669520889)\n  - [ ] plot_randomized_search.py\n  - [ ] plot_roc_crossval.py\n  - [ ] plot_roc.py\n  - [ ] plot_successive_halving_heatmap.py\n  - [ ] plot_successive_halving_iterations.py\n  - [ ] plot_train_error_vs_test_error.py\n  - [x] plot_underfitting_overfitting.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/issues/30621#issuecomment-2681734179)\n  - [x] <strike>plot_validation_curve.py</strike> #had been merged with another example in #29936\n- examples/neighbors:\n  - [ ] plot_digits_kde_sampling.py\n  - [ ] plot_kde_1d.py\n  - [ ] plot_lof_novelty_detection.py #31405\n  - [ ] plot_lof_outlier_detection.py\n  - [x] plot_nca_classification.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/30849#issuecomment-2665171341) #30849\n  - [x] plot_nca_dim_reduction.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/30849#issuecomment-2665171341) #30849\n  - [x] plot_nca_illustration.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/30849#issuecomment-2665171341) #30849\n  - [ ] plot_species_kde.py\n- examples/semi_supervised:\n  - [x] plot_label_propagation_digits_active_learning.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/30553#issuecomment-2582852356) #30553\n  - [x] plot_label_propagation_digits.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/30553#issuecomment-2582852356) #30553\n  - [x] plot_label_propagation_structure.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/30553#issuecomment-2582852356) #30553\n  - [ ] plot_self_training_varying_threshold.py\n  - [ ] plot_semi_supervised_newsgroups.py #30882 #31104\n  - [x] plot_semi_supervised_versus_svm_iris.py [no reference needs to be added](https://github.com/scikit-learn/scikit-learn/pull/31499#issuecomment-2976206315) #31499\n- examples/svm:\n  - [ ] plot_custom_kernel.py\n  - [ ] plot_iris_svc.py\n  - [ ] plot_linearsvc_support_vectors.py\n  - [ ] plot_oneclass.py\n  - [ ] plot_rbf_parameters.py\n  - [ ] plot_separating_hyperplane.py #31045\n  - [ ] plot_separating_hyperplane_unbalanced.py\n  - [ ] plot_svm_anova.py\n  - [ ] plot_svm_margin.py #26969 (stalled) #30975 ([maybe remove the example](https://github.com/scikit-learn/scikit-learn/pull/30975#pullrequestreview-2684941292)) #31045 is on merging this example with plot_separating_hyperplane.py\n  - [ ] plot_weighted_samples.py  #30676\n- examples/tree:\n  - [x] plot_iris_dtc.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/pull/30650#issuecomment-2653822241) #30650\n  - [x] <strike>plot_tree_regression_multioutput.py </strike> # was merged with another example in #26962\n  - [x] plot_unveil_tree_structure.py [#no references need to be added](https://github.com/scikit-learn/scikit-learn/issues/30621#issuecomment-2626465696)\n\n## What comes next?\n- after working a bit here, you might want to further explore contributing to scikit learn\n- we have #22827 and #25024 that are both also suitable for beginners, but might move forwards a little slower than here\n- we are looking for people who are willing to do some intense work to improve or merge some examples; these will be PRs that will be intensely discussed and thoroughly reviewed and will probably take several months; if this sounds good to you, please open an issue with a suggestion and maintainers will evaluate your idea\n    - this could look like #29963 and #29962\n    - we also have an open issue to discuss examples that can be removed: #27151\n- if you are more senior professionally, you can look through the issues with the [`help wanted`](https://github.com/scikit-learn/scikit-learn/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22help%20wanted%22) label or with the [`moderate`](https://github.com/scikit-learn/scikit-learn/labels/Moderate) label or you can take over [stalled PRs](https://github.com/scikit-learn/scikit-learn/issues?q=is%3Apr%20state%3Aopen%20label%3AStalled); these kind of contributions need to be discussed with maintainers and I would recommend seeking their approval first and not invest too much work before you get a go",
    "comments": [
      {
        "user": "marenwestermann",
        "body": "Leaving a comment so I get updates about incoming PRs :)\r\n"
      },
      {
        "user": "virchan",
        "body": "Commenting to stay updated!"
      },
      {
        "user": "sarang-26",
        "body": "Hi Sklearn Team,\r\n\r\nexcited to work on this on this issue. I will be following the workflow and start working on this.\r\n\r\nIts my first time contributing to sklearn, hence hoping I learn as much as possible and also contribute to the open source community!\r\n\r\nI will be working on \r\n\r\nexamples/classification:\r\n plot_classifier_comparison.py\r\n plot_digits_classification.py\r\n \r\n Updated:\r\n \r\n Hi @StefanieSenger, I have found few places, where I can place examples from SVM in the User Guide.\r\n Could I also select few more examples to work on:\r\n plot_custom_kernel.py\r\n plot_iris_svc.py\r\n plot_linearsvc_support_vectors.py\r\n plot_oneclass.py\r\n plot_rbf_parameters.py\r\n\r\n\r\nIs it okay, to select a group of this size and work on this in a single PR? (from review perspective)\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 30389,
    "title": "Make `_check_n_features` and `_check_feature_names` public",
    "author": "glemaitre",
    "state": "open",
    "created_at": "2024-12-02T10:44:49Z",
    "updated_at": "2025-06-17T12:16:22Z",
    "labels": [
      "Easy",
      "Documentation",
      "help wanted"
    ],
    "body": "Since we are moving, `_check_n_features` and `_check_feature_names` into a new module, I'm wondering if we should make them public as well.\r\n\r\nI can imagine some people that don't want to use `validate_data` but still want to set `self.n_features_in_` or `self.feature_names_in_`.",
    "comments": [
      {
        "user": "glemaitre",
        "body": "ping @adrinjalali @ogrisel @jeremiedbb @lesteve "
      },
      {
        "user": "adrinjalali",
        "body": "`validate_data` has a `do not check_array` param, that should be basically running these instead."
      },
      {
        "user": "glemaitre",
        "body": "You mean `skip_check_array` :). Indeed, new in 1.6.\r\n\r\nI would therefore only advocate for improving the documentation of those two functions such that they mention to use `validate_data` and point to this parameter.\r\n\r\nWe probably want to update our own code base as well because we don't follow this pattern for the `FunctionTransformer` as far I remember:\r\n\r\nhttps://github.com/scikit-learn/scikit-learn/blob/fba028b07ed2b4e52dd3719dad0d990837bde28c/sklearn/preprocessing/_function_transformer.py#L181-L190\r\n\r\nWe could make a small search in the project to check if the call to the private functions is legitimate or not."
      }
    ]
  },
  {
    "issue_number": 30223,
    "title": "Add Accumulated local effects (ALE) to inspection",
    "author": "mayer79",
    "state": "open",
    "created_at": "2024-11-05T10:43:20Z",
    "updated_at": "2025-06-17T12:02:50Z",
    "labels": [
      "New Feature",
      "Needs Decision"
    ],
    "body": "### Describe the workflow you want to enable\n\nI'd love to push the inspection module further by adding Accumulated local effects (ALE) from Apley 2020. A great description can be found in Christoph's online book https://christophm.github.io/interpretable-ml-book/ale.html\n\nALE fix the problem of partial dependence that they force the model to be evaluated on impossible or rare feature combinations. ALE are defined for numeric features that can be binned. From both bin edges, the slope of the partial dependence is calculated locally, i.e., only using observations in the bin. The slopes from all bins are cumsummed and vertically centered to the average response or prediction.\n\n### Reference\n\nApley, Daniel W., and Jingyu Zhu. 2020. Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models. Journal of the Royal Statistical Society Series B: Statistical Methodology, 82 (4): 1059â€“1086. doi:10.1111/rssb.12377.\n\n### Describe your proposed solution\n\nPseudo code to calculate ALE for one feature:\n\n``` py\npd_slopes = np.zeros_like(bins)\n\nfor i, bin in enumerate(bins):\n  X_bin = pick n_per_bin = 200 rows from data in bin\n  if X_bin is empty:\n    next\n  pd = partial_dependence_brute(model, X_bin, grid = [lower bin edge, upper bin edge], sample_weights)\n  pd_slopes[i] = pd[1] - pd[0]\n\nreturn np.cumsum(pd_slopes)\n```\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "@glemaitre or @thomasjpfan maybe have an opinion on the inclusion criteria here."
      },
      {
        "user": "glemaitre",
        "body": "I think that it could be a good addition to the module. The reasoning of what it solves make sense. Regarding the pseudo code, it seems that it would be not a huge amount of code to maintain.\r\n\r\nI have to read more in details the literature to be sure thought.\r\n\r\n@meyer89 Something that I'm thinking looking at your description: does the recursion method of the partial dependence would already lead to not using impossible combination of feature, isn't it?"
      },
      {
        "user": "mayer79",
        "body": "> Something that I'm thinking looking at your description: does the recursion method of the partial dependence would already lead to not using impossible combination of feature, isn't it?\r\n\r\nGood question! I don't think it is better (if the gender node is switched off, then also men will hit the post-menopausal node etc.). But I'd say that the problem is generally smaller for tree-based models than for other types of models."
      }
    ]
  },
  {
    "issue_number": 31566,
    "title": "âš ï¸ CI failed on Wheel builder (last failure: Jun 17, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-06-17T10:25:50Z",
    "updated_at": "2025-06-17T12:02:44Z",
    "labels": [
      "Needs Triage"
    ],
    "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/15697733135)** (Jun 17, 2025)\n",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "From the logs, looks like a timeout, closing to see if it happens again."
      }
    ]
  },
  {
    "issue_number": 30958,
    "title": "Request: base class with HTML repr but without being an 'Estimator'",
    "author": "david-cortes",
    "state": "open",
    "created_at": "2025-03-07T20:02:51Z",
    "updated_at": "2025-06-17T09:39:36Z",
    "labels": [
      "New Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nCreating third-party packages that offer objects that are meant to be passed to estimators, but which aren't estimators themselves.\n\n### Describe your proposed solution\n\nWould be nice if there could be some class similar to `BaseEstimator` that would offer pretty printing, HTML representations, and so on; but without needing to be an estimator (e.g. without having metadata routing and similar).\n\nThis could be used for example as a base class for objects that are meant to be passed as constructor arguments to actual estimators, and which are thus desirable to show with a pretty-printed form when visualizing estimators. For example, something like parameterizable probability distributions offered as objects in third-party packages that are meant to be passed to estimators from said third-party packages.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "glemaitre",
        "body": "Right now, one could almost have some of to utils with the following import:\n\n```python\nfrom .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n```\n\nThey are kind of private but they could be part of the dev API. They could be potentially consolidated into a mixin but having the function is already helpful."
      },
      {
        "user": "glemaitre",
        "body": "I think that we started to support this use case with the following mixin:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/d4d4af8c471c60d183d0cb67e14e6434b0ebb9fb/sklearn/utils/_repr_html/base.py#L118-L146\n\n@david-cortes Do you think that it answers your need?"
      },
      {
        "user": "david-cortes",
        "body": "> [@david-cortes](https://github.com/david-cortes) Do you think that it answers your need?\n\nNot quite - that class requires implementing a function to generate the HTML representation if I'm understanding it correctly.\n\nWhat I had in mind is some class that would automatically create the same kind of HTML representation and pretty-printing as `BaseEstimator`, but without inheriting the other methods that are expected from estimators nor expecting 'fit'/'transform'/etc."
      }
    ]
  },
  {
    "issue_number": 29802,
    "title": "âš ï¸ CI failed on Linux_Docker.debian_32bit (last failure: Apr 19, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "open",
    "created_at": "2024-09-08T03:35:56Z",
    "updated_at": "2025-06-17T03:13:07Z",
    "labels": [],
    "body": "**CI is still failing on [Linux_Docker.debian_32bit](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=75850&view=logs&j=86340c1f-3d76-5202-0821-7817a0f52092)** (Apr 19, 2025)\n- test_format_agnosticism[30-csr_matrix-RadiusNeighbors-float32]\n- test_format_agnosticism[30-csr_array-RadiusNeighbors-float32]",
    "comments": [
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=77358&view=logs&j=86340c1f-3d76-5202-0821-7817a0f52092) on Jun 17, 2025"
      },
      {
        "user": "jeremiedbb",
        "body": "Probably a test too sensitive to the random seed."
      },
      {
        "user": "lesteve",
        "body": "The March 28 failure [build log](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=75171&view=logs&j=86340c1f-3d76-5202-0821-7817a0f52092) is genuine. All the PRs with a recent push seem broken e.g. a `polars` related one https://github.com/scikit-learn/scikit-learn/pull/31095 or a pytest-related one https://github.com/scikit-learn/scikit-learn/pull/31074. A separated issue was created in https://github.com/scikit-learn/scikit-learn/issues/31098.\n\nNo idea why this would happen off the top of my head :scream: maybe a Debian Docker image upgrade somehow? \n\nFailing tests:\n```\nFAILED tests/test_common.py::test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data] - AssertionError: \nFAILED utils/tests/test_estimator_checks.py::test_check_estimator_clones - AssertionError: \n```\n\nThe error message is something like:\n```\nComparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\n```    \n\n<details>\n<summary>Stacktraces for the two failing tests</summary>\n\n```\n=================================== FAILURES ===================================\n_ test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data] _\n\nestimator = LinearRegression(positive=True)\ncheck = functools.partial(<function check_sample_weight_equivalence_on_dense_data at 0xd91bde88>, 'LinearRegression')\nrequest = <FixtureRequest for <Function test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data]>>\n\n    @parametrize_with_checks(\n        list(_tested_estimators()), expected_failed_checks=_get_expected_failed_checks\n    )\n    def test_estimators(estimator, check, request):\n        # Common tests for estimator instances\n        with ignore_warnings(\n            category=(FutureWarning, ConvergenceWarning, UserWarning, LinAlgWarning)\n        ):\n>           check(estimator)\n\ncheck      = functools.partial(<function check_sample_weight_equivalence_on_dense_data at 0xd91bde88>, 'LinearRegression')\nestimator  = LinearRegression(positive=True)\nrequest    = <FixtureRequest for <Function test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data]>>\n\n/io/sklearn/tests/test_common.py:122: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/io/sklearn/utils/estimator_checks.py:1570: in check_sample_weight_equivalence_on_dense_data\n    _check_sample_weight_equivalence(name, estimator_orig, sparse_container=None)\n        estimator_orig = LinearRegression(positive=True)\n        name       = 'LinearRegression'\n/io/sklearn/utils/_testing.py:145: in wrapper\n    return fn(*args, **kwargs)\n        args       = ('LinearRegression', LinearRegression(positive=True))\n        fn         = <function _check_sample_weight_equivalence at 0xd91bdde8>\n        kwargs     = {'sparse_container': None}\n        self       = _IgnoreWarnings(record=True)\n/io/sklearn/utils/estimator_checks.py:1566: in _check_sample_weight_equivalence\n    assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)\n        X          = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n        0.15599452, 0.05808361, 0.86617615, 0.6011..., 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n        0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072]])\n        X_pred1    = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n        4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n       -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n        X_pred2    = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n       1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n       1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n        X_repeated = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n        0.15599452, 0.05808361, 0.86617615, 0.6011..., 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n        0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072]])\n        X_weighted = array([[0.60754485, 0.17052412, 0.06505159, 0.94888554, 0.96563203,\n        0.80839735, 0.30461377, 0.09767211, 0.6842..., 0.69673717, 0.62894285, 0.87747201, 0.73507104,\n        0.80348093, 0.28203457, 0.17743954, 0.75061475, 0.80683474]])\n        err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n        estimator_orig = LinearRegression(positive=True)\n        estimator_repeated = LinearRegression(positive=True)\n        estimator_weighted = LinearRegression(positive=True)\n        method     = 'predict'\n        n_samples  = 15\n        name       = 'LinearRegression'\n        rng        = RandomState(MT19937) at 0xCCEBACE8\n        sparse_container = None\n        sw         = array([3, 4, 0, 3, 1, 0, 4, 4, 0, 3, 0, 0, 3, 2, 0])\n        y          = array([0, 1, 2, 2, 1, 1, 2, 2, 1, 2, 0, 0, 1, 1, 1])\n        y_repeated = array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       1, 1, 1, 1, 1])\n        y_weighted = array([1, 2, 1, 2, 1, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1])\n/io/sklearn/utils/_testing.py:283: in assert_allclose_dense_sparse\n    assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)\n        atol       = 1e-09\n        err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n        rtol       = 1e-07\n        x          = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n        4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n       -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n        y          = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n       1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n       1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nactual = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n        4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n       -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\ndesired = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n       1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n       1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\nrtol = 1e-07, atol = 1e-09, equal_nan = True\nerr_msg = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\nverbose = True\n\n    def assert_allclose(\n        actual, desired, rtol=None, atol=0.0, equal_nan=True, err_msg=\"\", verbose=True\n    ):\n        \"\"\"dtype-aware variant of numpy.testing.assert_allclose\n    \n        This variant introspects the least precise floating point dtype\n        in the input argument and automatically sets the relative tolerance\n        parameter to 1e-4 float32 and use 1e-7 otherwise (typically float64\n        in scikit-learn).\n    \n        `atol` is always left to 0. by default. It should be adjusted manually\n        to an assertion-specific value in case there are null values expected\n        in `desired`.\n    \n        The aggregate tolerance is `atol + rtol * abs(desired)`.\n    \n        Parameters\n        ----------\n        actual : array_like\n            Array obtained.\n        desired : array_like\n            Array desired.\n        rtol : float, optional, default=None\n            Relative tolerance.\n            If None, it is set based on the provided arrays' dtypes.\n        atol : float, optional, default=0.\n            Absolute tolerance.\n        equal_nan : bool, optional, default=True\n            If True, NaNs will compare equal.\n        err_msg : str, optional, default=''\n            The error message to be printed in case of failure.\n        verbose : bool, optional, default=True\n            If True, the conflicting values are appended to the error message.\n    \n        Raises\n        ------\n        AssertionError\n            If actual and desired are not equal up to specified precision.\n    \n        See Also\n        --------\n        numpy.testing.assert_allclose\n    \n        Examples\n        --------\n        >>> import numpy as np\n        >>> from sklearn.utils._testing import assert_allclose\n        >>> x = [1e-5, 1e-3, 1e-1]\n        >>> y = np.arccos(np.cos(x))\n        >>> assert_allclose(x, y, rtol=1e-5, atol=0)\n        >>> a = np.full(shape=10, fill_value=1e-5, dtype=np.float32)\n        >>> assert_allclose(a, 1e-5)\n        \"\"\"\n        dtypes = []\n    \n        actual, desired = np.asanyarray(actual), np.asanyarray(desired)\n        dtypes = [actual.dtype, desired.dtype]\n    \n        if rtol is None:\n            rtols = [1e-4 if dtype == np.float32 else 1e-7 for dtype in dtypes]\n            rtol = max(rtols)\n    \n>       np_assert_allclose(\n            actual,\n            desired,\n            rtol=rtol,\n            atol=atol,\n            equal_nan=equal_nan,\n            err_msg=err_msg,\n            verbose=verbose,\n        )\nE       AssertionError: \nE       Not equal to tolerance rtol=1e-07, atol=1e-09\nE       Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\nE       Mismatched elements: 6 / 15 (40%)\nE       Max absolute difference among violations: 2.51014256\nE       Max relative difference among violations: 2.17024526\nE        ACTUAL: array([ 8.881784e-16,  1.000000e+00,  2.000000e+00,  1.185498e+00,\nE               4.062418e+00,  1.000000e+00,  2.000000e+00,  2.000000e+00,\nE               4.105658e+00,  2.000000e+00, -2.799363e-02, -8.906428e-01,\nE              -8.008100e-01,  1.000000e+00,  1.000000e+00])\nE        DESIRED: array([0.      , 1.      , 2.      , 0.941865, 1.726709, 1.      ,\nE              2.      , 2.      , 1.872389, 2.      , 1.508778, 0.761074,\nE              1.709333, 1.      , 1.      ])\n\nactual     = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n        4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n       -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\natol       = 1e-09\ndesired    = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n       1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n       1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\ndtypes     = [dtype('float64'), dtype('float64')]\nequal_nan  = True\nerr_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\nrtol       = 1e-07\nverbose    = True\n\n/io/sklearn/utils/_testing.py:237: AssertionError\n_________________________ test_check_estimator_clones __________________________\n\n    def test_check_estimator_clones():\n        # check that check_estimator doesn't modify the estimator it receives\n    \n        iris = load_iris()\n    \n        for Estimator in [\n            GaussianMixture,\n            LinearRegression,\n            SGDClassifier,\n            PCA,\n            MiniBatchKMeans,\n        ]:\n            # without fitting\n            with ignore_warnings(category=ConvergenceWarning):\n                est = Estimator()\n                set_random_state(est)\n                old_hash = joblib.hash(est)\n>               check_estimator(\n                    est, expected_failed_checks=_get_expected_failed_checks(est)\n                )\n\nEstimator  = <class 'sklearn.linear_model._base.LinearRegression'>\nest        = LinearRegression()\niris       = {'data': array([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5,... width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\nold_hash   = 'fdcbee8ed611695d1e19a9bdabd615ac'\n\n/io/sklearn/utils/tests/test_estimator_checks.py:919: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/io/sklearn/utils/_param_validation.py:218: in wrapper\n    return func(*args, **kwargs)\n        args       = (LinearRegression(),)\n        func       = <function check_estimator at 0xd91bd668>\n        func_sig   = <Signature (estimator=None, generate_only=False, *, legacy: 'bool' = True, expected_failed_checks: 'dict[str, str] | N...al['warn'] | None\" = 'warn', on_fail: \"Literal['raise', 'warn'] | None\" = 'raise', callback: 'Callable | None' = None)>\n        global_skip_validation = False\n        kwargs     = {'expected_failed_checks': {}}\n        parameter_constraints = {'callback': [<built-in function callable>, None], 'expected_failed_checks': [<class 'dict'>, None], 'generate_only': ['boolean'], 'legacy': ['boolean'], ...}\n        params     = {'callback': None, 'estimator': LinearRegression(), 'expected_failed_checks': {}, 'generate_only': False, ...}\n        prefer_skip_nested_validation = False\n        to_ignore  = ['self', 'cls']\n/io/sklearn/utils/estimator_checks.py:856: in check_estimator\n    check(estimator)\n        callback   = None\n        check      = functools.partial(<function check_sample_weight_equivalence_on_dense_data at 0xd91bde88>, 'LinearRegression')\n        check_result = {'check_name': 'check_sample_weight_equivalence_on_dense_data', 'estimator': LinearRegression(), 'exception': None, 'expected_to_fail': False, ...}\n        estimator  = LinearRegression(positive=True)\n        expected_failed_checks = {}\n        generate_only = False\n        legacy     = True\n        name       = 'LinearRegression'\n        on_fail    = 'raise'\n        on_skip    = 'warn'\n        reason     = 'Check is not expected to fail'\n        test_can_fail = False\n        test_results = [{'check_name': 'check_estimator_cloneable', 'estimator': LinearRegression(), 'exception': None, 'expected_to_fail': F...k_no_attributes_set_in_init', 'estimator': LinearRegression(), 'exception': None, 'expected_to_fail': False, ...}, ...]\n/io/sklearn/utils/estimator_checks.py:1570: in check_sample_weight_equivalence_on_dense_data\n    _check_sample_weight_equivalence(name, estimator_orig, sparse_container=None)\n        estimator_orig = LinearRegression(positive=True)\n        name       = 'LinearRegression'\n/io/sklearn/utils/_testing.py:145: in wrapper\n    return fn(*args, **kwargs)\n        args       = ('LinearRegression', LinearRegression(positive=True))\n        fn         = <function _check_sample_weight_equivalence at 0xd91bdde8>\n        kwargs     = {'sparse_container': None}\n        self       = _IgnoreWarnings(record=True)\n/io/sklearn/utils/estimator_checks.py:1566: in _check_sample_weight_equivalence\n    assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)\n        X          = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n        0.15599452, 0.05808361, 0.86617615, 0.6011..., 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n        0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072]])\n        X_pred1    = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n        4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n       -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n        X_pred2    = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n       1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n       1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n        X_repeated = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n        0.15599452, 0.05808361, 0.86617615, 0.6011..., 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n        0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072]])\n        X_weighted = array([[0.60754485, 0.17052412, 0.06505159, 0.94888554, 0.96563203,\n        0.80839735, 0.30461377, 0.09767211, 0.6842..., 0.69673717, 0.62894285, 0.87747201, 0.73507104,\n        0.80348093, 0.28203457, 0.17743954, 0.75061475, 0.80683474]])\n        err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n        estimator_orig = LinearRegression(positive=True)\n        estimator_repeated = LinearRegression(positive=True)\n        estimator_weighted = LinearRegression(positive=True)\n        method     = 'predict'\n        n_samples  = 15\n        name       = 'LinearRegression'\n        rng        = RandomState(MT19937) at 0xC9302628\n        sparse_container = None\n        sw         = array([3, 4, 0, 3, 1, 0, 4, 4, 0, 3, 0, 0, 3, 2, 0])\n        y          = array([0, 1, 2, 2, 1, 1, 2, 2, 1, 2, 0, 0, 1, 1, 1])\n        y_repeated = array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       1, 1, 1, 1, 1])\n        y_weighted = array([1, 2, 1, 2, 1, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1])\n/io/sklearn/utils/_testing.py:283: in assert_allclose_dense_sparse\n    assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)\n        atol       = 1e-09\n        err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n        rtol       = 1e-07\n        x          = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n        4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n       -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n        y          = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n       1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n       1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nactual = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n        4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n       -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\ndesired = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n       1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n       1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\nrtol = 1e-07, atol = 1e-09, equal_nan = True\nerr_msg = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\nverbose = True\n\n    def assert_allclose(\n        actual, desired, rtol=None, atol=0.0, equal_nan=True, err_msg=\"\", verbose=True\n    ):\n        \"\"\"dtype-aware variant of numpy.testing.assert_allclose\n    \n        This variant introspects the least precise floating point dtype\n        in the input argument and automatically sets the relative tolerance\n        parameter to 1e-4 float32 and use 1e-7 otherwise (typically float64\n        in scikit-learn).\n    \n        `atol` is always left to 0. by default. It should be adjusted manually\n        to an assertion-specific value in case there are null values expected\n        in `desired`.\n    \n        The aggregate tolerance is `atol + rtol * abs(desired)`.\n    \n        Parameters\n        ----------\n        actual : array_like\n            Array obtained.\n        desired : array_like\n            Array desired.\n        rtol : float, optional, default=None\n            Relative tolerance.\n            If None, it is set based on the provided arrays' dtypes.\n        atol : float, optional, default=0.\n            Absolute tolerance.\n        equal_nan : bool, optional, default=True\n            If True, NaNs will compare equal.\n        err_msg : str, optional, default=''\n            The error message to be printed in case of failure.\n        verbose : bool, optional, default=True\n            If True, the conflicting values are appended to the error message.\n    \n        Raises\n        ------\n        AssertionError\n            If actual and desired are not equal up to specified precision.\n    \n        See Also\n        --------\n        numpy.testing.assert_allclose\n    \n        Examples\n        --------\n        >>> import numpy as np\n        >>> from sklearn.utils._testing import assert_allclose\n        >>> x = [1e-5, 1e-3, 1e-1]\n        >>> y = np.arccos(np.cos(x))\n        >>> assert_allclose(x, y, rtol=1e-5, atol=0)\n        >>> a = np.full(shape=10, fill_value=1e-5, dtype=np.float32)\n        >>> assert_allclose(a, 1e-5)\n        \"\"\"\n        dtypes = []\n    \n        actual, desired = np.asanyarray(actual), np.asanyarray(desired)\n        dtypes = [actual.dtype, desired.dtype]\n    \n        if rtol is None:\n            rtols = [1e-4 if dtype == np.float32 else 1e-7 for dtype in dtypes]\n            rtol = max(rtols)\n    \n>       np_assert_allclose(\n            actual,\n            desired,\n            rtol=rtol,\n            atol=atol,\n            equal_nan=equal_nan,\n            err_msg=err_msg,\n            verbose=verbose,\n        )\nE       AssertionError: \nE       Not equal to tolerance rtol=1e-07, atol=1e-09\nE       Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\nE       Mismatched elements: 6 / 15 (40%)\nE       Max absolute difference among violations: 2.51014256\nE       Max relative difference among violations: 2.17024526\nE        ACTUAL: array([ 8.881784e-16,  1.000000e+00,  2.000000e+00,  1.185498e+00,\nE               4.062418e+00,  1.000000e+00,  2.000000e+00,  2.000000e+00,\nE               4.105658e+00,  2.000000e+00, -2.799363e-02, -8.906428e-01,\nE              -8.008100e-01,  1.000000e+00,  1.000000e+00])\nE        DESIRED: array([0.      , 1.      , 2.      , 0.941865, 1.726709, 1.      ,\nE              2.      , 2.      , 1.872389, 2.      , 1.508778, 0.761074,\nE              1.709333, 1.      , 1.      ])\n\nactual     = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n        4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n       -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\natol       = 1e-09\ndesired    = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n       1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n       1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\ndtypes     = [dtype('float64'), dtype('float64')]\nequal_nan  = True\nerr_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\nrtol       = 1e-07\nverbose    = True\n\n/io/sklearn/utils/_testing.py:237: AssertionError\n```\n\n</details>"
      }
    ]
  },
  {
    "issue_number": 29909,
    "title": "âš ï¸ CI failed on Linux.pymin_conda_forge_openblas_min_dependencies (last failure: Jan 19, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "open",
    "created_at": "2024-09-23T03:06:33Z",
    "updated_at": "2025-06-17T03:03:39Z",
    "labels": [],
    "body": "**CI is still failing on [Linux.pymin_conda_forge_openblas_min_dependencies](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=73632&view=logs&j=85dc54f1-b746-54c6-aaa8-29d3ad6201c9)** (Jan 19, 2025)\n- test_linear_regression_sample_weights[95-True-csr_matrix]",
    "comments": [
      {
        "user": "lesteve",
        "body": "Probably a global random seed thing\r\n```\r\nAssertionError: score_native_tree=0.5179181827260624 + 0.07 should be strictly greater than 0.6384293492599133\r\n```\r\n\r\nMore info from [build log](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=70436&view=logs&j=85dc54f1-b746-54c6-aaa8-29d3ad6201c9&t=29b4082b-c348-5d75-9100-2eb67283d8d6):\r\n```\r\n_ test_missing_values_is_resilience[16-ones-make_friedman1-ExtraTreeRegressor-0.07] _\r\n[gw0] linux -- Python 3.9.19 /usr/share/miniconda/envs/testvenv/bin/python\r\n\r\nmake_data = <function make_friedman1 at 0x7f6a7b5d7f70>\r\nTree = <class 'sklearn.tree._classes.ExtraTreeRegressor'>\r\nsample_weight_train = 'ones', global_random_seed = 16, tolerance = 0.07\r\n\r\n    @pytest.mark.parametrize(\r\n        \"make_data, Tree, tolerance\",\r\n        [\r\n            # Due to the sine link between X and y, we expect the native handling of\r\n            # missing values to always be better than the naive mean imputation in the\r\n            # regression case.\r\n            #\r\n            # Due to randomness in ExtraTree, we expect the native handling of missing\r\n            # values to be sometimes better than the naive mean imputation, but not always\r\n            (datasets.make_friedman1, DecisionTreeRegressor, 0),\r\n            (datasets.make_friedman1, ExtraTreeRegressor, 0.07),\r\n            (make_friedman1_classification, DecisionTreeClassifier, 0.03),\r\n            (make_friedman1_classification, ExtraTreeClassifier, 0.12),\r\n        ],\r\n    )\r\n    @pytest.mark.parametrize(\"sample_weight_train\", [None, \"ones\"])\r\n    def test_missing_values_is_resilience(\r\n        make_data, Tree, sample_weight_train, global_random_seed, tolerance\r\n    ):\r\n        \"\"\"Check that trees can deal with missing values have decent performance.\"\"\"\r\n        n_samples, n_features = 5_000, 10\r\n        X, y = make_data(\r\n            n_samples=n_samples,\r\n            n_features=n_features,\r\n            noise=1.0,\r\n            random_state=global_random_seed,\r\n        )\r\n    \r\n        X_missing = X.copy()\r\n        rng = np.random.RandomState(global_random_seed)\r\n        X_missing[rng.choice([False, True], size=X.shape, p=[0.9, 0.1])] = np.nan\r\n        X_missing_train, X_missing_test, y_train, y_test = train_test_split(\r\n            X_missing, y, random_state=global_random_seed\r\n        )\r\n        if sample_weight_train == \"ones\":\r\n            sample_weight = np.ones(X_missing_train.shape[0])\r\n        else:\r\n            sample_weight = None\r\n    \r\n        # max_depth is used to avoid overfitting and also improve the runtime\r\n        # of the test.\r\n        max_depth = 10\r\n        native_tree = Tree(max_depth=max_depth, random_state=global_random_seed)\r\n        native_tree.fit(X_missing_train, y_train, sample_weight=sample_weight)\r\n        score_native_tree = native_tree.score(X_missing_test, y_test)\r\n    \r\n        tree_with_imputer = make_pipeline(\r\n            SimpleImputer(), Tree(max_depth=max_depth, random_state=global_random_seed)\r\n        )\r\n        tree_with_imputer.fit(X_missing_train, y_train)\r\n        score_tree_with_imputer = tree_with_imputer.score(X_missing_test, y_test)\r\n    \r\n>       assert score_native_tree + tolerance > score_tree_with_imputer, (\r\n            f\"{score_native_tree=} + {tolerance} should be strictly greater than\"\r\n            f\" {score_tree_with_imputer}\"\r\n        )\r\nE       AssertionError: score_native_tree=0.5179181827260624 + 0.07 should be strictly greater than 0.6384293492599133\r\n\r\nTree       = <class 'sklearn.tree._classes.ExtraTreeRegressor'>\r\nX          = array([[2.23291079e-01, 5.23163341e-01, 5.50701457e-01, ...,\r\n        1.63731425e-01, 7.03248668e-02, 9.41010860e-01],\r\n...\r\n       [5.92203881e-01, 8.26556141e-01, 2.54711031e-01, ...,\r\n        5.16697864e-01, 5.30599042e-01, 9.16955179e-01]])\r\nX_missing  = array([[2.23291079e-01, 5.23163341e-01, 5.50701457e-01, ...,\r\n        1.63731425e-01, 7.03248668e-02,            nan],\r\n...\r\n       [5.92203881e-01, 8.26556141e-01, 2.54711031e-01, ...,\r\n        5.16697864e-01, 5.30599042e-01,            nan]])\r\nX_missing_test = array([[0.01930501, 0.17344262, 0.40972254, ..., 0.49048711, 0.5089482 ,\r\n        0.77180737],\r\n       [       nan, 0.04...32,\r\n        0.73406437],\r\n       [0.20753419, 0.79531327, 0.08311383, ..., 0.29711538, 0.17633919,\r\n        0.03536329]])\r\nX_missing_train = array([[0.77276163, 0.06244841, 0.36194878, ..., 0.42608152, 0.10719802,\r\n        0.77663952],\r\n       [0.10841232, 0.43...an,\r\n        0.52733418],\r\n       [0.16938625, 0.12877933, 0.25585444, ..., 0.2152097 , 0.79415208,\r\n        0.24693722]])\r\nglobal_random_seed = 16\r\nmake_data  = <function make_friedman1 at 0x7f6a7b5d7f70>\r\nmax_depth  = 10\r\nn_features = 10\r\nn_samples  = 5000\r\nnative_tree = ExtraTreeRegressor(max_depth=10, random_state=16)\r\nrng        = RandomState(MT19937) at 0x7F6A89A01C40\r\nsample_weight = array([1., 1., 1., ..., 1., 1., 1.])\r\nsample_weight_train = 'ones'\r\nscore_native_tree = 0.5179181827260624\r\nscore_tree_with_imputer = 0.6384293492599133\r\ntolerance  = 0.07\r\ntree_with_imputer = Pipeline(steps=[('simpleimputer', SimpleImputer()),\r\n                ('extratreeregressor',\r\n                 ExtraTreeRegressor(max_depth=10, random_state=16))])\r\ny          = array([ 4.78976726,  5.25470178,  9.07131   , ..., 11.97427023,\r\n       11.91456126, 17.47855052])\r\ny_test     = array([ 7.82170559, 12.54926247, 14.26820336, ..., 17.03654542,\r\n       19.82950097, 15.68593767])\r\ny_train    = array([ 4.5818887 ,  6.98481701, 12.30711026, ..., 16.08614182,\r\n       11.74892358, 10.26765938])\r\n\r\n../1/s/sklearn/tree/tests/test_tree.py:2614: AssertionError\r\n\r\n```"
      },
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=77358&view=logs&j=85dc54f1-b746-54c6-aaa8-29d3ad6201c9) on Jun 17, 2025"
      },
      {
        "user": "jeremiedbb",
        "body": "Looks like a specific random seed failure, but the actual value is quite far from the expected value so it may not be just a matter of tolerance. Maybe the test has too optimistic expectations or the random data generation is not constrained enough. We need to investigate this test imo"
      }
    ]
  },
  {
    "issue_number": 27846,
    "title": "âš ï¸ CI failed on Ubuntu_Atlas.ubuntu_atlas (last failure: Feb 19, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "open",
    "created_at": "2023-11-26T02:59:56Z",
    "updated_at": "2025-06-17T03:02:50Z",
    "labels": [
      "Build / CI"
    ],
    "body": "**CI is still failing on [Ubuntu_Atlas.ubuntu_atlas](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=74235&view=logs&j=689a1c8f-ff4e-5689-1a1a-6fa551ae9eba)** (Feb 19, 2025)\n- test_float_precision[33-MiniBatchKMeans-dense]",
    "comments": [
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=77358&view=logs&j=689a1c8f-ff4e-5689-1a1a-6fa551ae9eba) on Jun 17, 2025"
      },
      {
        "user": "ogrisel",
        "body": "For the record:\r\n\r\n```python\r\n../1/s/sklearn/cluster/tests/test_k_means.py:851\r\n[...]\r\nE       AssertionError: \r\nE       Not equal to tolerance rtol=0.0001, atol=0\r\nE       \r\nE       Mismatched elements: 1 / 1 (100%)\r\nE       Max absolute difference: 0.04080632\r\nE       Max relative difference: 0.00011865\r\nE        x: array(343.971558)\r\nE        y: array(343.930751)\r\n\r\nactual     = array(343.97155762)\r\natol       = 0.0\r\ndesired    = array(343.9307513)\r\ndtypes     = [dtype('float64'), dtype('float64')]\r\nequal_nan  = True\r\nerr_msg    = ''\r\nrtol       = 0.0001\r\nverbose    = True\r\n```\r\n\r\nI assume we can just increase the `rtol` value for this test but this should be re-run with all random seeds on an atlas environment to find a suitable `rtol` value that is strict enough but not too much."
      }
    ]
  },
  {
    "issue_number": 28780,
    "title": "`FunctionTransformer` need `feature_names_out` even if `func` returns DataFrame",
    "author": "fedorkobak",
    "state": "open",
    "created_at": "2024-04-06T10:17:51Z",
    "updated_at": "2025-06-16T22:56:36Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\r\n\r\nTrying to call `transform` for `FunctionTransformer` for which `feature_names_out` is configured raises error that advises to use `set_output(transform='pandas')`. But this doesn't change anything.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```py\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.preprocessing import FunctionTransformer\r\n\r\nmy_transformer = FunctionTransformer(\r\n    lambda X : pd.concat(\r\n        [\r\n            X[col].rename(f\"{col} {str(power)}\")**power\r\n            for col in X\r\n            for power in range(2,4)\r\n        ],\r\n        axis=1\r\n    ),\r\n    feature_names_out = (\r\n        lambda transformer, input_features: [\r\n            f\"{feature} {power_str}\"\r\n            for feature in input_features\r\n            for power_str in [\"square\", \"cubic\"]\r\n        ]\r\n    )\r\n)\r\n# I specified transform=pandas\r\nmy_transformer.set_output(transform='pandas')\r\nsample_size = 10\r\nX = pd.DataFrame({\r\n    \"feature 1\" : [1,2,3,4,5],\r\n    \"feature 2\" : [3,4,5,6,7]\r\n})\r\nmy_transformer.fit(X)\r\nmy_transformer.transform(X)\r\n```\r\n\r\n### Expected Results\r\n\r\n`pandas.DataFrame` like following\r\n\r\n|    |   feature 1 square |   feature 1 cubic |   feature 2 square |   feature 2 cubic |\r\n|---:|-------------------:|------------------:|-------------------:|------------------:|\r\n|  0 |                  1 |                 1 |                  9 |                27 |\r\n|  1 |                  4 |                 8 |                 16 |                64 |\r\n|  2 |                  9 |                27 |                 25 |               125 |\r\n|  3 |                 16 |                84 |                 36 |               216 |\r\n|  4 |                 25 |               125 |                 49 |               343 |\r\n\r\n### Actual Results\r\n\r\n```\r\nValueError: The output generated by `func` have different column names than the ones provided by `get_feature_names_out`. Got output with columns names: ['feature 1 2', 'feature 1 3', 'feature 2 2', 'feature 2 3'] and `get_feature_names_out` returned: ['feature 1 square', 'feature 1 cubic', 'feature 2 square', 'feature 2 cubic']. The column names can be overridden by setting `set_output(transform='pandas')` or `set_output(transform='polars')` such that the column names are set to the names provided by `get_feature_names_out`.\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\r\nexecutable: /usr/bin/python3\r\n   machine: Linux-6.5.0-14-generic-x86_64-with-glibc2.35\r\n\r\nPython dependencies:\r\n      sklearn: 1.4.1.post1\r\n          pip: 24.0\r\n   setuptools: 68.2.2\r\n        numpy: 1.24.2\r\n        scipy: 1.11.1\r\n       Cython: None\r\n       pandas: 2.2.1\r\n   matplotlib: 3.7.1\r\n       joblib: 1.3.1\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/fedor/.local/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so\r\n        version: 0.3.21\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/fedor/.local/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/fedor/.local/lib/python3.10/site-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so\r\n        version: 0.3.21.dev\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n```\r\n",
    "comments": [
      {
        "user": "lesteve",
        "body": "There are at least two things:\r\n1. you want to fix your code, not setting `feature_names_out` would work. If you want to tweak the column names, I would suggest you do it in your `FunctionTransformer` `func` argument (i.e. first positional argument).\r\n2. you are saying that the error message is not super helpful in your case. It does say that the column names don't match but I would agree that the part about \"The column names can be overridden\" seems to imply you can change the column names without worrying that they don't match\r\n"
      },
      {
        "user": "fedorkobak",
        "body": "@lesteve you're right. If I change my code snippet like following:\r\n```py\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.preprocessing import FunctionTransformer\r\n\r\nmy_transformer = FunctionTransformer(\r\n    lambda X : pd.DataFrame(\r\n        {\r\n            f\"{str(col)}^{power}\" : X[col]**power\r\n            for col in X\r\n            for power in range(2,4)\r\n        }\r\n    ),\r\n    feature_names_out = (\r\n        lambda transformer, input_features: [\r\n            f\"{str(feature)}^{power}\"\r\n            for feature in input_features\r\n            for power in range(2,4)\r\n        ]\r\n    )\r\n)\r\n\r\nmy_transformer.set_output(transform='pandas')\r\nsample_size = 10\r\nX = pd.DataFrame({\r\n    \"feature 1\" : [1,2,3,4,5],\r\n    \"feature 2\" : [3,4,5,6,7]\r\n})\r\nmy_transformer.fit(X)\r\nmy_transformer.transform(X)\r\nmy_transformer.get_feature_names_out()\r\n```\r\nSo output columns of `func` the same as result of `feature_names_out` - everything goes fine. Thank you.\r\n\r\nBut in my opinion it would be more intuitive if `FunctionTransformer` would just use the result of `features_names_out` - because you'll need to define it anyway if you want to build a pipeline that can provide information about feature names in the later steps. Also, in older versions of sklearn it was like this - just try my first snippet in version 1.3.0 - everything works fine.\r\n\r\nIf that's the way it's intended, you can close this issue."
      },
      {
        "user": "lesteve",
        "body": "This seems related to https://github.com/scikit-learn/scikit-learn/pull/28241 and https://github.com/scikit-learn/scikit-learn/pull/27801. cc @glemaitre since he has this in his brain cache more than me.\r\n\r\nMy naive (and apparently wrong) expectation would have been that if your `func` returns a `DataFrame`, you don't need to use the `feature_names_out` argument and the `get_feature_names_out` returns the columns of the DataFrame that `func` returns.\r\n\r\n@fedorkobak small tip: you can use syntax highlighting in markdown to make code snippets more readable, see https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks#syntax-highlighting for more details. I have edited your comment accordingly."
      }
    ]
  },
  {
    "issue_number": 25896,
    "title": "Support other dataframes like polars and pyarrow not just pandas",
    "author": "lorentzenchr",
    "state": "closed",
    "created_at": "2023-03-17T18:01:16Z",
    "updated_at": "2025-06-16T19:13:08Z",
    "labels": [
      "New Feature",
      "RFC"
    ],
    "body": "### Describe the workflow you want to enable\r\n\r\nCurrently, scikit-learn nowhere claims to support [pyarrow](https://arrow.apache.org/docs/python/) or [polars](https://www.pola.rs/). And indeed,\r\n```python\r\nimport numpy as np\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.preprocessing import StandardScaler, KBinsDiscretizer\r\nfrom sklearn.compose import ColumnTransformer\r\n\r\nX, y = load_iris(as_frame=True, return_X_y=True)\r\nsepal_cols = [\"sepal length (cm)\", \"sepal width (cm)\"]\r\npetal_cols = [\"petal length (cm)\", \"petal width (cm)\"]\r\n\r\npreprocessor = ColumnTransformer(\r\n    [\r\n        (\"scaler\", StandardScaler(), sepal_cols),\r\n        (\"kbin\", KBinsDiscretizer(encode=\"ordinal\"), petal_cols),\r\n    ],\r\n    verbose_feature_names_out=False,\r\n)\r\n\r\nimport polars as pl  # or import pyarrow as pa\r\nX_pl = pl.from_pandas(X)  # or X_pa = pa.table(X)\r\n\r\npreprocessor.fit_transform(X_pl)\r\n# preprocessor.set_output(transform=\"pandas\").fit_transform(X_pl)\r\n```\r\n\r\nerrors with\r\n```\r\nAttributeError: 'numpy.ndarray' object has no attribute 'columns'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError: Specifying the columns using strings is only supported for pandas DataFrames\r\n```\r\n\r\n### Describe your proposed solution\r\n\r\nscikit-learn should support those dataframes, maybe via the [python dataframe interchange protocol](https://data-apis.org/dataframe-protocol/latest/index.html).\r\n\r\nIn that regard, a new option like `set_output(transform=\"dataframe\")` would be nice.\r\n\r\n### Describe alternatives you've considered, if relevant\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\nSome related discussion came up in #25813.",
    "comments": [
      {
        "user": "Vishal-sys-code",
        "body": "While scikit-learn does not currently support Polars or Pyarrow dataframes out-of-the-box, there are some possible workarounds to use these dataframes with scikit-learn.\r\n\r\nOne possible solution would be to convert the Polars or Pyarrow dataframe to a Pandas dataframe before passing it to scikit-learn's ```ColumnTransformer```. This can be done using the ```to_pandas()``` method in Polars or ```pa.Table.to_pandas()``` method in Pyarrow. \r\n\r\n```py\r\nimport polars as pl\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.compose import ColumnTransformer\r\n\r\n# Load data into a Polars dataframe\r\nX_pl = pl.DataFrame({...})\r\n\r\n# Convert Polars dataframe to Pandas dataframe\r\nX_pd = X_pl.to_pandas()\r\n\r\n# Create ColumnTransformer\r\npreprocessor = ColumnTransformer(\r\n    [\r\n        (\"scaler\", StandardScaler(), [\"sepal length (cm)\", \"sepal width (cm)\"]),\r\n    ]\r\n)\r\n\r\n# Fit and transform using ColumnTransformer\r\nX_transformed = preprocessor.fit_transform(X_pd)\r\n```\r\n\r\nAnother possible solution would be to write a custom transformer that can directly handle Polars or Pyarrow dataframes. This transformer would need to implement the ```fit_transform()``` method and should be compatible with scikit-learn's ```ColumnTransformer```.\r\n\r\n```py\r\nimport polars as pl\r\nfrom sklearn.base import BaseEstimator, TransformerMixin\r\n\r\nclass PolarsTransformer(BaseEstimator, TransformerMixin):\r\n    def __init__(self, pl_transformer):\r\n        self.pl_transformer = pl_transformer\r\n    \r\n    def fit(self, X, y=None):\r\n        return self\r\n    \r\n    def transform(self, X, y=None):\r\n        X_pl = pl.from_pandas(X)\r\n        X_transformed_pl = self.pl_transformer.fit_transform(X_pl)\r\n        X_transformed_pd = X_transformed_pl.to_pandas()\r\n        return X_transformed_pd\r\n```\r\n\r\nWith this custom transformer, you can pass it directly to scikit-learn's ```ColumnTransformer```:\r\n\r\n```py\r\nimport polars as pl\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.compose import ColumnTransformer\r\n\r\n# Load data into a Polars dataframe\r\nX_pl = pl.DataFrame({...})\r\n\r\n# Create PolarsTransformer\r\npreprocessor = ColumnTransformer(\r\n    [\r\n        (\"scaler\", PolarsTransformer(StandardScaler()), [\"sepal length (cm)\", \"sepal width (cm)\"]),\r\n    ]\r\n)\r\n\r\n# Fit and transform using ColumnTransformer\r\nX_transformed = preprocessor.fit_transform(X_pl)\r\n```"
      },
      {
        "user": "adrinjalali",
        "body": "We definitely should fix this, I'm not sure if @thomasjpfan already has plans for it."
      },
      {
        "user": "betatim",
        "body": "I think it would make a lot of sense to support other popular data frames, especially if they support the data frame protocol.\r\n\r\n> I'm not sure if @thomasjpfan already has plans for it.\r\n\r\nIf people have plans to work on things like this, it would be great to share them before they start working on it. Seems like a good opportunity to get collaboration going."
      }
    ]
  },
  {
    "issue_number": 28341,
    "title": "RFC switch to Polars as the default dataframe lib in our examples",
    "author": "adrinjalali",
    "state": "closed",
    "created_at": "2024-02-01T11:58:02Z",
    "updated_at": "2025-06-16T19:10:26Z",
    "labels": [
      "RFC"
    ],
    "body": "We now support polars as an output for our transformers and `ColumnTransformer`, but our examples use `pd.DataFrame`.\r\n\r\nOur `datasets` module also returnes either a numpy array or a pandas DataFrame.\r\n\r\nI'm suggesting that we enable datasets to return a polars dataframe, and to switch our examples from pandas to polars.\r\n\r\nThis has a few benefits:\r\n- Polars on users' systems takes advantage of multi-core CPUs which is the case for pretty much all users these days. So it's quite faster in most cases.\r\n- Pandas is dealing with issues related to Arrow, and even if they don't require Arrow as a required dependency, there will be behavior changes whether Arrow is installed or not at least on String Dtype.\r\n\r\nAnother thing is the (in)stability issues related to Pandas API where we need to deal with deprecation warnings very often. Although I'm not sure how stable the API is on the places where we touch the API on the polars side (maybe cc @MarcoGorelli )\r\n\r\nWDYT @scikit-learn/core-devs @scikit-learn/documentation-team @scikit-learn/contributor-experience-team \r\n\r\nThis is not really a core part of our library, but what we put in our examples and our default choices affect people since many learn from our examples.",
    "comments": [
      {
        "user": "GaelVaroquaux",
        "body": "> I'm suggesting that we enable datasets to return a polars dataframe, and to switch our examples from pandas to polars.\n\nWhile I love polars, I think that most users still use pandas. I thus would argue to have both in the examples. By both, I do not mean double the work, but switch a few, keep a few in pandas, and each time mention in a comment that scikit-learn works with both.\n\n"
      },
      {
        "user": "glemaitre",
        "body": "I agree with @GaelVaroquaux. I would be considering having a mix of example, even more when we don't take profit from polars features.\r\n\r\nCurrently our support for polars is closer to rewrapping a NumPy arrays into a Polars DataFrame without leveraging the lazyness.\r\n\r\nIn an ideal world, if we come to have estimator to leverage those features (e.g. via the dataframe API) then we should certainly demonstrate it in the documentation."
      },
      {
        "user": "marenwestermann",
        "body": "I also agree with Gael. I think that switching some examples with\r\ndocumentation that both pandas and polars are supported is a good idea.\r\n\r\nOn Thu, 1 Feb 2024 at 14:09, Guillaume Lemaitre ***@***.***>\r\nwrote:\r\n\r\n> I agree with @GaelVaroquaux <https://github.com/GaelVaroquaux>. I would\r\n> be considering having a mix of example, even more when we don't take profit\r\n> from polars features.\r\n>\r\n> Currently our support for polars is closer to rewrapping a NumPy arrays\r\n> into a Polars DataFrame without leveraging the lazyness.\r\n>\r\n> In an ideal world, if we come to have estimator to leverage those features\r\n> (e.g. via the dataframe API) then we should certainly demonstrate it in the\r\n> documentation.\r\n>\r\n> â€”\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/scikit-learn/scikit-learn/issues/28341#issuecomment-1921291070>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AEB3BIX3BT42R52HJ2GBOJTYROHZ3AVCNFSM6AAAAABCUYK3X6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSMRRGI4TCMBXGA>\r\n> .\r\n> You are receiving this because you are on a team that was mentioned.Message\r\n> ID: ***@***.***>\r\n>\r\n"
      }
    ]
  },
  {
    "issue_number": 26530,
    "title": "TransformedTargetRegressor forces 1d y shape to regressor",
    "author": "Daniel3009",
    "state": "open",
    "created_at": "2023-06-07T13:58:14Z",
    "updated_at": "2025-06-16T18:54:27Z",
    "labels": [
      "Bug",
      "good first issue"
    ],
    "body": "### Describe the bug\r\n\r\nI experience the following error when using TransformedTargetRegressor with my skorch model:\r\nValueError: The target data shouldn't be 1-dimensional but instead have 2 dimensions, with the second dimension having the same size as the number of regression targets (usually 1). Please reshape your target data to be 2-dimensional (e.g. y = y.reshape(-1, 1).\r\n\r\n#### After checking the Source Code this lead me the the following unexpected behaivor which makes little sense:\r\n\r\nIf TransformedTargetRegressor is fitted with with a 2d dimensional y, it will still be transformed to a 1d dimensional output\r\n\r\ny should have the same input and output shapes with a TransformedTargetRegressor or there should be an init argument to disable the change of the input shape\r\n(Yes, internally it gets casted to 2d, but Iâ€™m talking about the In and Outputs) \r\n\r\nhttps://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/compose/_target.py#L20\r\nTransformedTargetRegressor-->fit\r\n\r\n```python\r\n        if y.ndim == 1:\r\n            y_2d = y.reshape(-1, 1)\r\n        else:\r\n            y_2d = y\r\n        self._fit_transformer(y_2d)\r\n\r\n[...]\r\n\r\n        if y_trans.ndim == 2 and y_trans.shape[1] == 1:\r\n            y_trans = y_trans.squeeze(axis=1)\r\n```\r\nBut in the end we squeeze it back into a 1d which causes issues for models which expect a 2d input of y\r\ny was 2d in the beginning for a reason\r\n\r\n### **The following code would solve this:**\r\n```\r\n        if y_trans.ndim == 2 and y_trans.shape[1] == 1 and y.ndim==1:  #only squeeze back to 1d if y is 1d\r\n            y_trans = y_trans.squeeze(axis=1)\r\n```\r\n\r\nThis could only create an issue where the y input was for some reason 2d but should be 1d for the regressor. \r\nIn this case an attribute would be nice\r\n```\r\n        if y_trans.ndim == 2 and y_trans.shape[1] == 1 and self.output_dim == 1:\r\n            y_trans = y_trans.squeeze(axis=1)\r\n```\r\n\r\nAlso in TransformedTargetRegressor-->predict the results dont get squeezed after the prediction of the estimator - only if the original input shape was 1, in that case it is squeezed\r\n\r\nSo the result looks as expected, but only if the regressor takes a 1d y\r\nIf the estimator expects a 2d y the code fails\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nregressor = TransformedTargetRegressor(\r\n    transformer=MinMaxScaler()\r\n)\r\nX, y = np.random.rand(10, 10), np.expand_dims(np.random.rand(10), 1)\r\nregressor.fit(X, y)\r\n```\r\n\r\n### Expected Results\r\n\r\nThe shape of y stays the same as the input OR there is a attribute which allows the choice of  (1d or original)  or (1d or 2d)\r\n\r\ninput | internal | output\r\n2d â€”> 2d â€”> 2d\r\n1d â€”> 2d â€”> 1d\r\n\r\n### Actual Results\r\n\r\nthe regressor gets just a 1d array even through y was specifically set to 2d\r\n(I don't know how to extract these results without an debugger)\r\n\r\nIt works for this example because the default regressor is used, but when using it with other models they might need the 2nd dimention of y, because it was specifically reshaped  (-1,1)\r\n\r\ninput | internal | output\r\n2d â€”> 2d â€”> 1d    THIS creates issues for the regressive which is passed to the Transformer if it expects a 2d array because a 2d y was given \r\n1d â€”> 2d â€”> 1d\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]\r\nexecutable: /anaconda/envs/azureml_py310_sdkv2/bin/python\r\n   machine: Linux-5.15.0-1017-azure-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.1.3\r\n          pip: 22.1.2\r\n   setuptools: 61.2.0\r\n        numpy: 1.23.2\r\n        scipy: 1.9.0\r\n       Cython: None\r\n       pandas: 1.4.3\r\n   matplotlib: 3.6.2\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n```\r\n",
    "comments": [
      {
        "user": "glemaitre",
        "body": "The reason for the reshaping is explained in the [documentation note](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html#sklearn.compose.TransformedTargetRegressor.fit):\r\n\r\n> Internally, the target y is always converted into a 2-dimensional array to be used by scikit-learn transformers. At the time of prediction, the output will be reshaped to have the same number of dimensions as y.\r\n\r\nUsually, we always consider `y` being of shape `(n_samples,)` for single target or `(n_samples, n_outputs)` otherwise but apparently it is not something that we enforce or that we are consistent with:\r\n\r\n```python\r\nIn [17]: LinearRegression().fit(X, y).predict(X)\r\nOut[17]: \r\narray([[0.24711583],\r\n       [0.74242216],\r\n       [0.55222396],\r\n       [0.84829166],\r\n       [0.17998768],\r\n       [0.0245152 ],\r\n       [0.22000849],\r\n       [0.61546997],\r\n       [0.1222009 ],\r\n       [0.48522966]])\r\n\r\nIn [18]: from sklearn.ensemble import RandomForestRegressor\r\n\r\nIn [19]: RandomForestRegressor().fit(X, y).predict(X)\r\n<ipython-input-19-52a4ba3e81e7>:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\r\n  RandomForestRegressor().fit(X, y).predict(X)\r\nOut[19]: \r\narray([0.33050337, 0.57584412, 0.52647597, 0.61801084, 0.29033987,\r\n       0.16826488, 0.32458667, 0.4966041 , 0.17528285, 0.45642003])\r\n```\r\n\r\nHere, the linear model would work as expected but the tree will make the conversion and raise a warning.\r\n\r\nSo in a meta-estimator, it makes it even more complex. I assume that a possibility is to convert to a 1D-vector and warn because it will work with all scikit-learn models. We could also raise an error and not warn at all.\r\n\r\n"
      },
      {
        "user": "Daniel3009",
        "body": "Yes, internally y is converted to 2d always.\r\n\r\nThe issue is the following behaviour I added in the post above: \r\n\r\nExpected\r\ninput | internal | output\r\n2d â€”> 2d â€”> 2d\r\n1d â€”> 2d â€”> 1d\r\nY doesnâ€™t change shape between Input and Output \r\n\r\nActual Currently\r\ninput | internal | output\r\n2d â€”> 2d â€”> 1d XXXX\r\n1d â€”> 2d â€”> 1d\r\nY output is always 1d even though the dimension gets saved "
      },
      {
        "user": "glemaitre",
        "body": "We should be fixing the `TransformedTargetRegressor` to tolerate this case.\r\nWe can see later regarding the consistency across regressors later on."
      }
    ]
  },
  {
    "issue_number": 30766,
    "title": "Update project metadata to avoid using the deprecated way to declare the license.",
    "author": "ogrisel",
    "state": "open",
    "created_at": "2025-02-03T16:17:53Z",
    "updated_at": "2025-06-16T18:29:23Z",
    "labels": [
      "good first issue"
    ],
    "body": "Once https://github.com/scikit-learn/scikit-learn/pull/30746#pullrequestreview-2590397434 is merged, it should be possible to use the new standardized way to declare the licensing information in our `pyproject.toml` file. See:\n\nhttps://peps.python.org/pep-0639/#deprecate-license-classifiers",
    "comments": [
      {
        "user": "rrricharrrd",
        "body": "/take"
      }
    ]
  },
  {
    "issue_number": 30689,
    "title": "FeatureHasher and HashingVectorizer does not expose requires_fit=False tag",
    "author": "glemaitre",
    "state": "open",
    "created_at": "2025-01-21T10:28:21Z",
    "updated_at": "2025-06-16T18:09:44Z",
    "labels": [
      "Enhancement",
      "good first issue"
    ],
    "body": "While `FeatureHasher` and `HashingVectorizer` are stateless estimator (at least in their docstrings), they do not expose the `requires_fit` tag to `False` as other stateless estimator.\n\n@adrinjalali Do you recall when changing the tags if there was a particular reason for those estimator to not behave the same way than others?",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "My PR didn't change a tag in that regard: \n\nhttps://github.com/scikit-learn/scikit-learn/pull/29677/files#diff-2ce614636a32fc7ddace8d5f36e320e0b348012dff967ad17c7fed8ea40b5723R196\n\nNote that `requires_fit` is tricky if the estimator does input / feature name / feature count validation."
      },
      {
        "user": "rrricharrrd",
        "body": "/take"
      }
    ]
  },
  {
    "issue_number": 27342,
    "title": "ENH Add `pos_label` parameter to `TargetEncoder`",
    "author": "lucyleeow",
    "state": "open",
    "created_at": "2023-09-12T02:06:36Z",
    "updated_at": "2025-06-16T18:06:06Z",
    "labels": [
      "Enhancement",
      "good first issue"
    ],
    "body": "### Describe the workflow you want to enable\n\nAdd a `pos_label` parameter to `TargetEncoder` to enable the user to specify which label should be the positive class when the target is binary.\n\n### Describe your proposed solution\n\nAdd a `pos_label` parameter that is passed to the `LabelBinarizer` `pos_label` parameter.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\nRef: https://github.com/scikit-learn/scikit-learn/pull/26674#discussion_r1317446826\r\n\r\ncc @ogrisel ",
    "comments": [
      {
        "user": "glemaitre",
        "body": "Indeed, it makes sense. I am also thinking that the current validation of `LabelBinarizer` is not support UX friendly since it accepts only `Integral` values. I assume that we should be able to pass any type of classes.\r\n\r\nThen we could use the `_check_pos_label_consistency` and improve it as well by passing `classes` if already computed (to avoid calling `np.unique`) and also add an extra check that `pos_label` is in `classes`."
      },
      {
        "user": "ivantishchenko",
        "body": "/take"
      }
    ]
  },
  {
    "issue_number": 30309,
    "title": "'Section Navigation' bar missing from stable documentation website on several pages",
    "author": "K-Hern",
    "state": "open",
    "created_at": "2024-11-20T03:28:24Z",
    "updated_at": "2025-06-16T17:29:47Z",
    "labels": [
      "Documentation",
      "good first issue"
    ],
    "body": "### Describe the issue linked to the documentation\r\n\r\nWhen on the stable version of the documentation website the 'Section Navigation' header on the left side of the page remains present, but the navigation bar contents disappear. While on the dev page the feature functions as expected.\r\nIt should be noted this issue is inconsistent. Some stable pages list the section navigation and work perfectly fine ([like this one here](https://scikit-learn.org/stable/modules/tree.html)) while others do not.\r\n\r\nThis presents an issue as some links take the user to the stable version and others the dev version.\r\n\r\nFor example: [Present Here](https://scikit-learn.org/dev/developers/contributing.html#submitting-a-bug-report-or-a-feature-request), [Absent Here](https://scikit-learn.org/stable/developers/contributing.html#submitting-a-bug-report-or-a-feature-request)\r\n\r\n<img width=\"2046\" alt=\"Screenshot 2024-11-19 at 19 22 09\" src=\"https://github.com/user-attachments/assets/fe9e7e2a-4c04-4245-92e9-08dd697882ec\">\r\n<img width=\"2048\" alt=\"Screenshot 2024-11-19 at 19 22 39\" src=\"https://github.com/user-attachments/assets/378dc707-0dde-4abf-bc4a-4f38a0ff9513\">\r\n\r\nDiscovered running on Chrome Browser Version 130.0.6723.117",
    "comments": [
      {
        "user": "glemaitre",
        "body": "Basically this is a bug from `pydata-sphinx-theme`: https://github.com/pydata/pydata-sphinx-theme/issues/1662\r\n\r\nHere, we could temporary do a hack where we disable the navigation manually until it is the issue get fix."
      },
      {
        "user": "staehlmich",
        "body": "/take"
      }
    ]
  },
  {
    "issue_number": 26711,
    "title": "AttributeError: This 'LabelEncoder' has no attribute 'set_output'",
    "author": "Clarit7",
    "state": "open",
    "created_at": "2023-06-27T07:08:20Z",
    "updated_at": "2025-06-16T17:25:46Z",
    "labels": [
      "Bug",
      "Documentation",
      "good first issue"
    ],
    "body": "### Describe the bug\n\nI tried to call **'set_output'** from LabelEncoder object and got the AttributeError.\r\n\r\n[The document](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) says sklearn.preprocessing.LabelEncoder has **'set_output'** method, but it was not working.\r\n\r\nSoon I found most of other **'set_output'** available estimators inherits both of sklearn.base.OneToOneFeatureMixin and sklearn.base.TransformerMinxin\r\n\r\nHowerver, [LabelEncoder](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/preprocessing/_label.py) only inherits the TransformerMinxin.\r\n\r\n```python\r\nclass LabelEncoder(TransformerMixin, BaseEstimator):\r\n```\r\n\r\n</br>\r\n\r\nFunction **'set_output'** seems available when **'_auto_wrap_is_configured'** is True. [(utils._set_output.py)](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/_set_output.py)\r\n\r\n```python\r\n    @available_if(_auto_wrap_is_configured)\r\n    def set_output(self, *, transform=None):\r\n        \"\"\"Set output container.\r\n\r\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\r\n        for an example on how to use the API.\r\n\r\n        Parameters\r\n        ----------\r\n        transform : {\"default\", \"pandas\"}, default=None\r\n            Configure output of `transform` and `fit_transform`.\r\n\r\n            - `\"default\"`: Default output format of a transformer\r\n            - `\"pandas\"`: DataFrame output\r\n            - `None`: Transform configuration is unchanged\r\n\r\n        Returns\r\n        -------\r\n        self : estimator instance\r\n            Estimator instance.\r\n        \"\"\"\r\n        if transform is None:\r\n            return self\r\n\r\n        if not hasattr(self, \"_sklearn_output_config\"):\r\n            self._sklearn_output_config = {}\r\n\r\n        self._sklearn_output_config[\"transform\"] = transform\r\n        return self\r\n```\r\n\r\n</br>\r\n\r\nThen estimator should have **'get_feature_names_out'** to make **'_auto_wrap_is_configured'** returns True. [(utils._set_output.py)](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/_set_output.py)\r\n\r\n```python\r\ndef _auto_wrap_is_configured(estimator):\r\n    \"\"\"Return True if estimator is configured for auto-wrapping the transform method.\r\n\r\n    `_SetOutputMixin` sets `_sklearn_auto_wrap_output_keys` to `set()` if auto wrapping\r\n    is manually disabled.\r\n    \"\"\"\r\n    auto_wrap_output_keys = getattr(estimator, \"_sklearn_auto_wrap_output_keys\", set())\r\n    return (\r\n        hasattr(estimator, \"get_feature_names_out\")\r\n        and \"transform\" in auto_wrap_output_keys\r\n    )\r\n```\r\n\r\n</br>\r\n\r\n To have **'get_feature_names_out'** attr, estimator should inherit OneToOneFeatureMixin as I think. [(base.py)](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/base.py)\r\n\r\n```python\r\nclass OneToOneFeatureMixin:\r\n    \"\"\"Provides `get_feature_names_out` for simple transformers.\r\n\r\n    This mixin assumes there's a 1-to-1 correspondence between input features\r\n    and output features, such as :class:`~preprocessing.StandardScaler`.\r\n    \"\"\"\r\n\r\n    def get_feature_names_out(self, input_features=None):\r\n        \"\"\"Get output feature names for transformation.\r\n\r\n        Parameters\r\n        ----------\r\n        input_features : array-like of str or None, default=None\r\n            Input features.\r\n\r\n            - If `input_features` is `None`, then `feature_names_in_` is\r\n              used as feature names in. If `feature_names_in_` is not defined,\r\n              then the following input feature names are generated:\r\n              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\r\n            - If `input_features` is an array-like, then `input_features` must\r\n              match `feature_names_in_` if `feature_names_in_` is defined.\r\n\r\n        Returns\r\n        -------\r\n        feature_names_out : ndarray of str objects\r\n            Same as input features.\r\n        \"\"\"\r\n        check_is_fitted(self, \"n_features_in_\")\r\n        return _check_feature_names_in(self, input_features)\r\n```\r\n\r\n</br>\r\n\r\nI want to know that it is kind of a bug, or the document says wrong information.\n\n### Steps/Code to Reproduce\n\n```python\r\nfrom sklearn.preprocessing import LabelEncoder\r\n\r\nLabelEncoder().set_output()\r\n```\n\n### Expected Results\n\nNo error is thrown\n\n### Actual Results\n\n```python\r\n--------------------------------------------------------------------------------\r\nAttributeError                                 Traceback (most recent call last)                        \r\n/tmp/ipykernel_16765/3091610596.py in <module>\r\n---> 1 LabelEncoder().set_output()\r\n\r\n~/.local/lib/python3.8/site-packages/sklearn/utils/_available_if.py in __get__(self, obj, owner)\r\n      31        # this is to allow access to the docstrings.\r\n      32        if not self.check(obj):\r\n--->  33            raise attr_err\r\n      34        out = MethodType(self.fn, obj)\r\n      35\r\nAttributeError: This 'LabelEncoder' has no attribute 'set_output'\r\n```\r\n            \r\n             \r\n            \n\n### Versions\n\n```shell\n1.2.2\n```\n",
    "comments": [
      {
        "user": "glemaitre",
        "body": "Not entirely sure what we want here since the `LabelEncoder` and `LabelBinarizer` are dedicated to transforming the target `y` and not the input `X`. For consistency reasons, it might make sense to output a Series or a DataFrame, however, the part of the feature names does not make any sense.\r\n\r\n@thomasjpfan any thoughts on this one?"
      },
      {
        "user": "Clarit7",
        "body": "@glemaitre Thanks for replying. Then, should 'set_output' be removed from the document?"
      },
      {
        "user": "glemaitre",
        "body": "Not for the moment, I would like to have thought of some @scikit-learn/core-devs on the issue to know what we consider the way forward here."
      }
    ]
  },
  {
    "issue_number": 27462,
    "title": "Adding more functionality to DecisionBoundaryDisplay",
    "author": "glemaitre",
    "state": "open",
    "created_at": "2023-09-25T10:11:54Z",
    "updated_at": "2025-06-16T17:18:11Z",
    "labels": [
      "good first issue"
    ],
    "body": "This issue is to track the improvements that should be made to the `DecisionBoundaryDisplay`:\n\n- [x] handle multiclass classification\n    - [x] target a specific class: https://github.com/scikit-learn/scikit-learn/pull/27291\n    - [x] represent the max of probability\n- [ ] allow to plot the decision boundary using a keyword without the need to call `DecisionBoundaryDisplay` twice\n- [ ] Allow to pass `xlim/ylim` to overwrite the limit of the data seen in `from_estimator`. ",
    "comments": [
      {
        "user": "StefanieSenger",
        "body": "I have used `DecisionBoundaryDisplay` to plot `from_estimator` of a `DecisionTreeClassifier` as [in this example](https://glemaitre.github.io/traces-sklearn/content/notebooks/06_tree_model.html).\n\nBased on that I would suggest further improvements:\n1. add a title to the plot that clearly states which `response_method` from which estimator was used in the plot\n2. add a legend that shows the values (or value regions) of the response method used (percentages for `predict_proba`, classes for `proba`, raw values for `decision_function`)\n   - 2b. redefine the representation of the legend that defines the classes, since they look like a scale rather than marking membership\n\nDo you think this would be useful, @glemaitre?"
      },
      {
        "user": "glemaitre",
        "body": "I agree that we can improve the plot. Maybe some of those improvement would be opt-in via parameter. I need to think more about each of them but having proof of concept will help anyway to know if we want it a certain way."
      },
      {
        "user": "ansamz",
        "body": "/take"
      }
    ]
  },
  {
    "issue_number": 29734,
    "title": "Default argument pos_label=1 is not ignored in f1_score metric for multiclass classification",
    "author": "slimebob1975",
    "state": "open",
    "created_at": "2024-08-29T07:45:11Z",
    "updated_at": "2025-06-16T17:14:58Z",
    "labels": [
      "Bug",
      "good first issue"
    ],
    "body": "### Describe the bug\r\n\r\nI get a `ValueError` for `pos_label=1` default argument value to `f1_score` metric with argument `average='micro'` for the iris flower classification problem:\r\n\r\n```pytb\r\nValueError: pos_label=1 is not a valid label: It should be one of ['setosa' 'versicolor' 'virginica']\r\n```\r\n\r\nAccording to the documentation, the `pos_label` argument should be ignored for the multiclass problem:\r\n\r\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#f1-score\r\n\r\n_The class to report if `average='binary'` and the data is binary, otherwise this parameter is ignored._\r\n\r\nSetting `pos_label` explicitly to None solves the problem and produces the expected output, see below.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\n# Import necessary libraries\r\nimport numpy as np\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.model_selection import cross_val_score\r\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\nfrom sklearn.metrics import make_scorer, f1_score\r\n\r\n# Load the Iris dataset\r\ndata = load_iris()\r\nX = data.data  # Features\r\ny = data.target  # Labels\r\n\r\n# Convert labels to string type\r\ny = np.array([data.target_names[label] for label in data.target])\r\n\r\n# Initialize the Linear Discriminant Analysis classifier\r\nclassifier = LinearDiscriminantAnalysis()\r\n\r\n# Define a custom scorer using F1 score with average='micro'\r\nf1_scorer = make_scorer(f1_score, average='micro', pos_label=1)\r\n\r\n# Perform cross-validation with cross_val_score\r\ntry:\r\n    scores = cross_val_score(classifier, X, y, cv=5, scoring=f1_scorer)\r\n    print(f\"Cross-validated F1 Scores (micro average): {scores}\")\r\n    print(f\"Mean F1 Score: {np.mean(scores)}\")\r\nexcept ValueError as e:\r\n    print(f\"Error: {e}\")\r\n```\r\n\r\n### Expected Results\r\n\r\n```\r\nCross-validated F1 Scores (micro average): [1.         1.         0.96666667 0.93333333 1.        ]\r\nMean F1 Score: 0.9800000000000001\r\n```\r\n\r\n### Actual Results\r\n\r\n```pytb\r\nCross-validated F1 Scores (micro average): [nan nan nan nan nan]\r\nMean F1 Score: nan\r\n[C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1000](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/model_selection/_validation.py#line=999): UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \r\nTraceback (most recent call last):\r\n  File \"[C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 139](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/metrics/_scorer.py#line=138), in __call__\r\n    score = scorer._score(\r\n            ^^^^^^^^^^^^^^\r\n  File \"[C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 371](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/metrics/_scorer.py#line=370), in _score\r\n    y_pred = method_caller(\r\n             ^^^^^^^^^^^^^^\r\n  File \"[C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 89](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/metrics/_scorer.py#line=88), in _cached_call\r\n    result, _ = _get_response_values(\r\n                ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"[C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_response.py\", line 204](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/utils/_response.py#line=203), in _get_response_values\r\n    raise ValueError(\r\nValueError: pos_label=1 is not a valid label: It should be one of ['setosa' 'versicolor' 'virginica']\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\python.exe\r\n   machine: Windows-10-10.0.19045-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.0\r\n          pip: 23.2.1\r\n   setuptools: 68.0.0\r\n        numpy: 1.26.2\r\n        scipy: 1.11.4\r\n       Cython: None\r\n       pandas: 2.1.1\r\n   matplotlib: 3.8.0\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: mkl\r\n    num_threads: 8\r\n         prefix: mkl_rt\r\n       filepath: C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Library\\bin\\mkl_rt.2.dll\r\n        version: 2023.1-Product\r\nthreading_layer: intel\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: vcomp\r\n       filepath: C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n```\r\n",
    "comments": [
      {
        "user": "glemaitre",
        "body": "Indeed this is a bug. We have a discrepancy between the `f1_score` function and the scorer.\r\n\r\n```\r\ny_pred = classifier.predict(X)\r\nf1_score(y, y_pred, average=\"micro\", pos_label=1)\r\n```\r\n\r\nwill work because it ignores the `pos_label`. However, I think that the scorer will raise an error before to actually call the `f1_score` function because it is less lenient.\r\n\r\nI don't really know what is the right fix here. We could have either to modify `f1_score` but it could be tricky. We could otherwise implement a workaround in the scorer but this is weird to special case this function. So we should carefully assess the problem and check what are all the possible resolution."
      },
      {
        "user": "Higgs32584",
        "body": "it also appears that both average='micro' and pos_label=None have to be present for the code to run successfully. If either is removed, we get an error as well.\r\n\r\n"
      },
      {
        "user": "Higgs32584",
        "body": "I tried various combinations of params and this is what I got for each\r\nfor Micro, macro, and weighted, when pos_label=None, it worked. For samples and binary, it did not, which makes sense for binary, idk how samples works. when pos_label was 1, _none_ of the averages worked. \r\n\r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 16298,
    "title": "List of estimators with known incorrect handling of `sample_weight`",
    "author": "rth",
    "state": "open",
    "created_at": "2020-01-29T23:09:10Z",
    "updated_at": "2025-06-16T16:42:26Z",
    "labels": [
      "Meta-issue"
    ],
    "body": "An issue with an associated common check originally discussed in https://github.com/scikit-learn/scikit-learn/pull/15015 \n\n> This is a pretty simple sample_weight test that says that a weight of 0 is equivalent to not having the samples.\n> I think every failure here should be considered a bug. This is:\n\n- [ ] AdaBoostClassifier/Regressor\n- [x] BayesianRidge #30644\n- [x] CalibratedClassifierCV #29796\n- [ ] CategoricalNB #31556\n- [ ] GradientBoostingClassifier/Regressor\n  - not sure if `fit` is deterministic or not with the config used in `check_sample_weight_equivalence`: to be investigated.\n    - rounding errors make it non-deterministic and break equivalence because of systematic bias to handle near-tied splits: https://github.com/scikit-learn/scikit-learn/issues/23728#issuecomment-2737437203\n- [ ] HistGradientBoostingClassifier/Regressor\n  - [ ] `check_sample_weight_equivalence` fails event when subsampling for binning is not enabled.\n  - [ ] subsampling for binning needs to be `sample_weight` aware but can then be only properly tested with a statistical test instead of `check_sample_weight_equivalence`\n  - [ ] subsampling also happens (without respecting weights) to compute non-default scoring on a subsampled training set when early stopping is enabled without a validation set\n  - [ ] #29641\n- [ ] HuberRegressor\n- [x] LinearRegression\n  - [x] #30040\n  - [x] #30131\n- [ ] LinearSVC \n  - [x] #30057\n  - [ ] apparently related to the use of `liblinear`\n- [ ] LinearSVR\n  - [ ] apparently related to the use of `liblinear`\n- [ ] LogisticRegression\n  - [ ] `lbfgs` causes `check_sample_weight_equivalence` to fail (slightly)\n  - [ ] `liblinear` with `C=0.01` causes `check_sample_weight_equivalence` to fail (slightly)\n- [x] LogisticRegressionCV\n  - #29419\n- [x] ElasticNetCV / LassoCV #29442 and #29796 (both are needed)\n- [x] Ridge\n  - `check_sample_weight_equivalence` now passes for this estimator after lowering the `tol` value for `lsqr` and `sparse-cg` in the per-check params.\n- [x] RidgeClassifier\n  - `check_sample_weight_equivalence` now passes for this estimator after lowering the `tol` value for `lsqr` and `sparse-cg` in the per-check params.\n- [x] RidgeClassifierCV. #29796\n- [ ] DecisionTreeRegressor\n   - [ ] biased handling of near-tied splits: https://github.com/scikit-learn/scikit-learn/issues/23728#issuecomment-2737437203\n- [x] RidgeCV and RidgeClassifierCV can delegate to GridSearchCV when using a non-default `cv` (which is the case in `check_sample_weight_equivalence`) or `scoring` params.\n    - [x] fixed by #30743\n- [ ] OneClassSVM\n- [ ] NuSVC (same as SVC)\n- [ ] NuSVR\n- [ ] SVC\n  - `check_sample_weight_equivalence` fails with `probability=False`: to be investigated\n  - this expected with `probability=True` as the weights are not propagated to the internal CV implemented in libsvm\n- [ ] SVR\n- [ ] GridSearchCV / RandomizedSearchCV / ...\n   - [x] did not forward `sample_weight` to their scorer by default #30743.\n   - [ ] do this even when metadata routing is disabled\n   - [ ] implement a default routing policy for `sample_weight` in general: #26179\n\nThe following estimators have a stochastic fit, so testing for correct handling of sample weights cannot be tested with `check_sample_weight_equivalence` but instead [requires a statistical test](https://github.com/scikit-learn/scikit-learn/issues/16298#issuecomment-2331044662):\n\n- [x] BaggingClassifier/Regressor\n  - #31414\n- [x] BisectingKMeans\n- [x] KBinsDiscretizer\n  - #29907\n  - note that with a small `n_samples` and uniform or quantile strategies, the fit is deterministic.\n- [ ] SGDClassifier/Regressor\n- [ ] Perceptron (likely same bug as SGD)\n- [ ] RANSACRegressor\n  - [ ] https://github.com/scikit-learn/scikit-learn/issues/15836\n- [x] KMeans\n- [ ] MiniBatchKMeans\n- [ ] IsolationForest\n- [ ] RandomForestClassifier/Regressor\n- [ ] ExtraTreeRegressor\n- [ ] ExtraTreesRegressor\n- [ ] RandomTreesEmbedding\n\n(some might have been fixed since, need to check).\n \nThe required sample weight invariance properties (including the behavior of sw=0) were also discussed in https://github.com/scikit-learn/scikit-learn/issues/15657\n\nEDIT: expected `sample_weight` semantics have since been more generally in the [refactoring of `check_sample_weight_invariance` into `check_sample_weight_equivalence`](https://github.com/scikit-learn/scikit-learn/pull/29818) to check fitting with integer sample weights is equivalent to fitting with repeated data points with a number of repetitions that matches the original weights.",
    "comments": [
      {
        "user": "jnothman",
        "body": "I think it's unclear what needs to happen in this issue, and I doubt they\nall can be fixed straightforwardly.\n"
      },
      {
        "user": "rth",
        "body": "> I think it's unclear what needs to happen in this issue, and I doubt they\r\nall can be fixed straightforwardly.\r\n\r\nThe goal of this issue, is to move the list of failing estimators from https://github.com/scikit-learn/scikit-learn/pull/15015 PR to here, merge that PR with an XFAIL flag once it is supported with https://github.com/scikit-learn/scikit-learn/pull/16306. \r\n\r\nFinally, keep this issue open for discussion and allow for the failing estimators to be fixed in separate PRs. I agree that the fixing will likely not be straightforwardly."
      },
      {
        "user": "ogrisel",
        "body": "@snath-xoc started to investigate more into the current state of `sample_weight` handling in scikit-learn estimators with the following gist.\r\n\r\nhttps://gist.github.com/snath-xoc/fb28feab39403a1e66b00b5b28f1dcbf\r\n\r\nThis gist is focused at estimators with a stochastic fit method (that might involve resampling, e.g. `BaggingClassifier` or `RANSACRegressor`). For those case we cannot expect the `assert_allclose` assertions of our current `check_sample_weights_invariance` to hold exactly but only in expectation. This is tested by fitting with many different values of `random_state` passed as a constructor argument. Also notes that sometimes, the problem only happens for non-default values of the other constructor parameters."
      }
    ]
  },
  {
    "issue_number": 23179,
    "title": "Wrong normalization for pseudolikelihood in Restricted Boltzmann Machine.",
    "author": "KVasya",
    "state": "open",
    "created_at": "2022-04-21T15:30:40Z",
    "updated_at": "2025-06-16T16:24:23Z",
    "labels": [
      "Bug",
      "module:neural_network",
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nPseudolikelihood of a sample under Restricted Boltzmann machine includes  the number of features  as a multiplier. \r\nThus pseudolikelihood reflects the dimensionality of visible units, apart from the sample probability. \r\nI believe this multiplier should be removed. \n\n### Steps/Code to Reproduce\n\nfrom sklearn.neural_network import BernoulliRBM\r\nimport numpy as np\r\n \r\nRBM_= BernoulliRBM(verbose=1) \r\nX_10= np.ones([10,10])\r\nX_1000= np.ones([10,1000])\r\n                  \r\n\r\nprint('Learning RBM with 10 visible units:')\r\nRBM_.fit(X_small)\r\nprint(100*'_')\r\nprint('Learning RBM with 1000 visible units')\r\nRBM_.fit(X_large)\r\n                  \n\n### Expected Results\n\nThe pseudolikelihoods for 10 and 1000 visible units should be of the same order at corresponding learning stages.\n\n### Actual Results\n\nLearning RBM with 10 visible units:\r\n[BernoulliRBM] Iteration 1, pseudo-likelihood = -0.19, time = 0.00s\r\n[BernoulliRBM] Iteration 2, pseudo-likelihood = -0.28, time = 0.00s\r\n[BernoulliRBM] Iteration 3, pseudo-likelihood = -0.22, time = 0.00s\r\n[BernoulliRBM] Iteration 4, pseudo-likelihood = -0.12, time = 0.00s\r\n[BernoulliRBM] Iteration 5, pseudo-likelihood = -0.10, time = 0.00s\r\n[BernoulliRBM] Iteration 6, pseudo-likelihood = -0.07, time = 0.00s\r\n[BernoulliRBM] Iteration 7, pseudo-likelihood = -0.07, time = 0.00s\r\n[BernoulliRBM] Iteration 8, pseudo-likelihood = -0.08, time = 0.00s\r\n[BernoulliRBM] Iteration 9, pseudo-likelihood = -0.10, time = 0.01s\r\n[BernoulliRBM] Iteration 10, pseudo-likelihood = -0.06, time = 0.00s\r\n____________________________________________________________________________________________________\r\nLearning RBM with 1000 visible units\r\n[BernoulliRBM] Iteration 1, pseudo-likelihood = -3.80, time = 0.01s\r\n[BernoulliRBM] Iteration 2, pseudo-likelihood = -0.76, time = 0.02s\r\n[BernoulliRBM] Iteration 3, pseudo-likelihood = -0.94, time = 0.01s\r\n[BernoulliRBM] Iteration 4, pseudo-likelihood = -1.97, time = 0.01s\r\n[BernoulliRBM] Iteration 5, pseudo-likelihood = -2.01, time = 0.01s\r\n[BernoulliRBM] Iteration 6, pseudo-likelihood = -0.61, time = 0.01s\r\n[BernoulliRBM] Iteration 7, pseudo-likelihood = -1.06, time = 0.01s\r\n[BernoulliRBM] Iteration 8, pseudo-likelihood = -1.09, time = 0.01s\r\n[BernoulliRBM] Iteration 9, pseudo-likelihood = -0.28, time = 0.01s\r\n[BernoulliRBM] Iteration 10, pseudo-likelihood = -1.01, time = 0.01s\r\n\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]\r\nexecutable: /home/x/PycharmProjects/RBM/env/bin/python\r\n   machine: Linux-4.15.0-142-generic-x86_64-with-debian-stretch-sid\r\n\r\nPython dependencies:\r\n          pip: 22.0.4\r\n   setuptools: 40.8.0\r\n      sklearn: 1.0.2\r\n        numpy: 1.21.5\r\n        scipy: 1.7.3\r\n       Cython: None\r\n       pandas: None\r\n   matplotlib: 3.5.1\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\n```\n",
    "comments": [
      {
        "user": "henriquessss",
        "body": "Hi!\nI would like to know if I can work on this issue! \nI reproduced the code using the latest version and the bug is still present."
      },
      {
        "user": "antoinebaker",
        "body": "> The pseudolikelihoods for 10 and 1000 visible units should be of the same order at corresponding learning stages.\n\nOn the provided example, they seem to have same order of magnitude ?"
      }
    ]
  },
  {
    "issue_number": 30748,
    "title": "Unexpected behavior for subclassing `Pipeline`",
    "author": "schroedk",
    "state": "closed",
    "created_at": "2025-02-02T13:30:21Z",
    "updated_at": "2025-06-16T13:25:02Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the issue linked to the documentation\n\nHey, I don't know if I should call this a bug, but for me at least it was unexpected behavior. I tried to subclass from `Pipeline`\nto implement a customization, so having a simplified configuration, which is used to build a sequence of transformations.  \n\nIt generates an `AttributeError`, due to not having an instance attribute with the same name as a positional argument (same is true for a kwarg) of the subclasses's init. Find a minimal example below.\n\nIs this expected behavior? It does not harm to set the instance attributes with the same name, but it is surprising it is demanded and is very implicit. Also, it does not pop up, when you instantiate the object, but only when you try to call a method on it.\n\nIn case it is absolutely necessary, it may need some documentation. \n\nIn addition, I tried to globally skip parameter validation and it did not help in this situation, which might be a real bug?\n \nThanks for your help, and your good work:)\n\nA simple example: \n```python\nimport sklearn\nsklearn.set_config(\n    skip_parameter_validation=True,  # disable validation\n)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport pandas as pd\n\n\nclass TakeColumn(BaseEstimator, TransformerMixin):\n    def __init__(self, column: str):\n        self.column = column\n\n    def __str__(self):\n        return self.__class__.__name__ + f\"[{self.column}]\"\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        return X[[self.column]]\n\n\nclass CategoricalFeature(Pipeline):\n    def __init__(self, column: str, encode=True):\n\n        take_column = TakeColumn(column)\n        steps = [(str(take_column), take_column)]\n\n        if encode:\n            encoder = OneHotEncoder()\n            steps.append((str(encoder), encoder))\n\n        # setting instance attributes having the same name, removes the exception\n        #self.column = column\n        #self.encode = encode\n\n        super().__init__(steps)\n\n\ndf = pd.DataFrame([[\"a\"], [\"b\"], [\"c\"]], columns=[\"column\"])\n\ncolumn_feature = CategoricalFeature(\"column\")\nsome_other_feature = CategoricalFeature(\"other_column\", encode=False)\n\n# this fails, if instance attributes are not set with the same name as the\n# corresponding parameter of __init__\nresult = column_feature.fit_transform(df)\n\n```\n\nOutput:\n```\nTraceback (most recent call last):\n  File \"<input>\", line 1, in <module>\n  File \"/Users/kristof/Library/Application Support/JetBrains/IntelliJIdea2024.3/plugins/python-ce/helpers/pydev/_pydev_bundle/pydev_umd.py\", line 197, in runfile\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kristof/Library/Application Support/JetBrains/IntelliJIdea2024.3/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\n  File \"/Users/kristof/Projects/pipeline-issue/default_console.py\", line 45, in <module>\n    result = column_feature.fit_transform(df)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kristof/Projects/pipeline-issue/.venv/lib/python3.12/site-packages/sklearn/base.py\", line 1382, in wrapper\n    estimator._validate_params()\n  File \"/Users/kristof/Projects/pipeline-issue/.venv/lib/python3.12/site-packages/sklearn/base.py\", line 438, in _validate_params\n    self.get_params(deep=False),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kristof/Projects/pipeline-issue/.venv/lib/python3.12/site-packages/sklearn/pipeline.py\", line 299, in get_params\n    return self._get_params(\"steps\", deep=deep)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kristof/Projects/pipeline-issue/.venv/lib/python3.12/site-packages/sklearn/utils/metaestimators.py\", line 30, in _get_params\n    out = super().get_params(deep=deep)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kristof/Projects/pipeline-issue/.venv/lib/python3.12/site-packages/sklearn/base.py\", line 248, in get_params\n    value = getattr(self, key)\n            ^^^^^^^^^^^^^^^^^^\nAttributeError: 'CategoricalFeature' object has no attribute 'column'\n```\n\n\n```\nSystem:\n    python: 3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ]\nexecutable: /Users/kristof/Projects/pipeline-issue/.venv/bin/python\n   machine: macOS-15.3-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: None\n   setuptools: None\n        numpy: 2.2.2\n        scipy: 1.15.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: None\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 12\n         prefix: libomp\n       filepath: /Users/kristof/Projects/pipeline-issue/.venv/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib\n        version: None\n```\n\n### Suggest a potential alternative/fix\n\n_No response_",
    "comments": [
      {
        "user": "betatim",
        "body": "You are running into what is documented in the \"instantiation\" section of the guide to develop your own estimator https://scikit-learn.org/dev/developers/develop.html#instantiation\n\nTL;DR: in an estimator's `__init__` you should not perform any work, just store all the (keyword) arguments as instance attributes. All other work, including validation, should happen in `fit`."
      },
      {
        "user": "schroedk",
        "body": "Thanks a lot for the quick answer and the hint to the documentation. So, do you would say the best advice is to not inherit from `Pipeline` at all?"
      },
      {
        "user": "glemaitre",
        "body": "> inherit from Pipeline at all\n\nYep, you should only use `BaseEstimator` if you want to be compatible. I see that @betatim already provided the link to the documentation that explain the expected API constraints.\n\nI'm therefore closing this issue."
      }
    ]
  },
  {
    "issue_number": 31555,
    "title": "is_classifier returns False for custom classifier wrappers in scikit-learn 1.6.1, even with ClassifierMixin and _estimator_type",
    "author": "greg500",
    "state": "closed",
    "created_at": "2025-06-16T12:26:15Z",
    "updated_at": "2025-06-16T12:42:51Z",
    "labels": [
      "Bug",
      "Needs Triage"
    ],
    "body": "### Describe the bug\n\n#### Describe the bug\n\nSince upgrading to scikit-learn 1.6.1, the utility function `is_classifier` always returns `False` for custom classifier wrappers, even if they inherit from `ClassifierMixin` and explicitly define `_estimator_type = \"classifier\"`.\n\nThis was not the case in previous versions (<=1.5.x), and breaks many downstream code patterns relying on `is_classifier`, as well as certain custom scorer usages and checks.\n\n\n### Steps/Code to Reproduce\n\n```python\nimport sklearn\nprint(\"scikit-learn version:\", sklearn.__version__)\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin, is_classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclass BinCls(BaseEstimator, ClassifierMixin):\n    _estimator_type = \"classifier\"\n    def __init__(self, model=None):\n        self.model = model\n\n    def fit(self, X, y):\n        self.model.fit(X, y)\n        self.classes_ = self.model.classes_\n        return self\n\n    def predict(self, X):\n        return self.model.predict(X)\n\n    def predict_proba(self, X):\n        return self.model.predict_proba(X)\n\nrng = RandomForestClassifier()\nclf = BinCls(rng)\nprint(\"is_classifier(clf) =\", is_classifier(clf))  # Expect True, but gets False\n\n\n### Expected Results\n\nprint(\"is_classifier(clf) =\", is_classifier(clf))  # Expect True, but gets False\n\n### Actual Results\n\nprint(\"is_classifier(clf) =\", is_classifier(clf)) # Expect True, but gets False\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]\nexecutable: C:\\Users\\Greg\\anaconda3\\envs\\ml_trade\\python.exe\n   machine: Windows-10-10.0.19045-SP0\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0\n   setuptools: 72.1.0\n        numpy: 2.1.3\n        scipy: 1.15.2\n       Cython: 3.1.1\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: mkl\n    num_threads: 4\n         prefix: mkl_rt\n       filepath: C:\\Users\\Greg\\anaconda3\\envs\\ml_trade\\Library\\bin\\mkl_rt.2.dll\n        version: 2023.1-Product\nthreading_layer: intel\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: vcomp\n       filepath: C:\\Users\\Greg\\anaconda3\\envs\\ml_trade\\vcomp140.dll\n        version: None\n```",
    "comments": [
      {
        "user": "glemaitre",
        "body": "We fixed a couple of things related to tags in the last version.\n\nHere, what you observe is a bug in the code that you provide:\n\n```python\nclass BinCls(BaseEstimator, ClassifierMixin):\n```\n\nThe mixin should always go to the left of the base class for the inheritance to work:\n\n\n```python\nclass BinCls(ClassifierMixin, BaseEstimator):\n```\n\n`is_classifier` in the past was relaying on `_estimator_type` and it was therefore working. Now, we rely on the mixin that set the tags and it is therefore important that the inheritance is done properly."
      }
    ]
  },
  {
    "issue_number": 27159,
    "title": "RandomForest{Classifier,Regressor} split criterion documentation",
    "author": "howarth",
    "state": "open",
    "created_at": "2023-08-24T20:10:55Z",
    "updated_at": "2025-06-16T12:23:19Z",
    "labels": [
      "Documentation",
      "good first issue"
    ],
    "body": "### Describe the issue linked to the documentation\n\nThere's no where in the documentation that explains what method is used to identify which values to consider as candidate splits. For example, for regression, an exhaustive method would be to sort each feature and then use the halfway point between each feature value. In this case there would be O(N*F) candidate splits where N=# data and F= # features. Another way would be to randomly choose K values between the min and max of each feature.\r\n\r\nDoes anyone know what method is used? Could someone point me to where in the code this occurs?\n\n### Suggest a potential alternative/fix\n\n_No response_",
    "comments": [
      {
        "user": "Snerdify",
        "body": "I'm working on this"
      },
      {
        "user": "Snerdify",
        "body": "@howarth just to be clear , you were unable to find the methods for evaluating candidate splits , correct?\r\n"
      },
      {
        "user": "Snerdify",
        "body": "/take"
      }
    ]
  },
  {
    "issue_number": 30352,
    "title": "Revisit the \"chance level\" for the different displays",
    "author": "glemaitre",
    "state": "open",
    "created_at": "2024-11-26T17:06:23Z",
    "updated_at": "2025-06-16T12:22:38Z",
    "labels": [
      "Documentation",
      "API",
      "good first issue"
    ],
    "body": "@e-pet commented on different PRs & issues some interesting fact. I take the opportunity to consolidate some of those comments here.\r\n\r\nFirst, we use the term \"chance\" that is ambiguous depending of the displays. The term \"baseline\" would probably be better. In addition, I checked and I think we should make an extra effort on the definition of the baseline for each of the type of plot: for ROC curve, the baseline is \"a random classifier assigning the positive class with probability p and the negative class with probability 1 âˆ’ p\" [1] while for the PR curve, the baseline is derived from the \"always-positive classifier\" where any recall or precision under Ï€ should be discarded [1].\r\n\r\nIt leads to a second where in the PR curve, we plot the horizontal line derived from the always-positive classifier but we don't discard when recall < Ï€. In this case, as mentioned by @e-pet, it might make sense to show the hyperbolic line of the always-positive classifier instead (cf. Fig. 2 in [1]).\r\n\r\n@e-pet feel free to add any other points that you wanted to discuss. Here, I wanted to focus on the one that looks critical and could be addressed.\r\n\r\n[1] [Flach, P., & Kull, M. (2015). Precision-recall-gain curves: PR analysis done right. Advances in neural information processing systems, 28.](https://papers.nips.cc/paper_files/paper/2015/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf)",
    "comments": [
      {
        "user": "e-pet",
        "body": "Hi @glemaitre, thank you for collecting this here!\r\n\r\nI did some further reading in the meantime (and fixed at least one misunderstanding of mine), so let me try to summarize the issue(s).\r\n- My dismissal of the horizontal line derived from the always-positive classifier was wrong. One can construct it this way: have a \"model\" return completely random scores, and then tune the decision threshold between 0 and 1. This yields exactly that horizontal line. So: the horizontal line is valid (=associated with actual baseline models one can easily construct at every point along the line), and it *is* actually based on a 'chance' baseline. (Might be worth pointing this out somewhere in the documentation, if this is not already the case?)\r\n- Separately from this, Flach and Kull argue that in terms of the area under the PR curve, the baseline to beat is a curve that has $F_1$ score equaling that of the always-positive classifier at every point along the curve. The interpretation is simple: if your model is below this line, you perform worse than a trivial always-positive classifier in terms of $F_1$. This is where the hyperbolic baseline comes from. I am actually not sure what the equivalent curve in the ROC case would be, or whether it differs from the standard diagonal? Note that at least to my current understanding, this baseline is 'virtual' in the sense that it is not trivially possible to actually construct models with the precision / recall values specified by every point along the curve.\r\n- To complicate matters further, there is actually a *third* 'baseline-like' line that one *could* show: this is the unreachable region in PR space, described [here](https://pmc.ncbi.nlm.nih.gov/articles/PMC3858955/) (and also very easy to calculate). Points below this line cannot be reached by any classifier (with important consequences for the validity of naive AUCPR comparisons, as spelled out both in this paper and the Flach/Kull paper)."
      },
      {
        "user": "ogrisel",
        "body": "For the ROC curve, what we currently call \"chance level\" (the diagonal ROC curve) is any non-informative baseline/predictor: a predictor whose predictions do not depend on X. They can be any constant predict_proba predictions: 0, 1, the fraction of the majority class in the training set or any arbitrary constant prediction. Or even a classifier that outputs a random predicted probability at each test point would also have a ROC Curve lying on the diagonal in the limit of a large prediction set.\n\nPersonally, I don't like the \"chance-level\" naming because it is a bit fuzzy, and it's not intuitive how a classifier that constantly predicts 1 or 0 can be related to \"chance\". I would rather name this \"non-informative predictor\" or \"non-informative baseline\" (or event \"constant predictor\")."
      }
    ]
  },
  {
    "issue_number": 30717,
    "title": "MNT Make binary display method parameters' order consistent",
    "author": "lucyleeow",
    "state": "open",
    "created_at": "2025-01-25T11:39:26Z",
    "updated_at": "2025-06-16T12:21:00Z",
    "labels": [
      "good first issue",
      "module:model_selection"
    ],
    "body": "This came up while working on #30399 . These are all classes inheriting the `_BinaryClassifierCurveDisplayMixin`.\n\n* `RocCurveDisplay` and `PrecisionRecallDisplay` are pretty consistent, we would just need to change where `pos_label` is. No strong preference to where it should be.\n* `DetCurveDisplay` does not have the chance level line, `drop_intermediate` and `depsine`. Chance line is added in #29151 (we should ensure order is consistent in that PR). Note there is discussion of adding `drop_intermediate` in that PR as well\n* `CalibrationDisplay` - is a bit different from the rest, e.g., there is a reference line (perfect calibration) and not a chance line. We could move `ax` up though, to be consistent with the other displays.\n\n\n<details open>\n<summary>Table of parameters</summary>\n\n|                  | CalibrationDisplay                                                                         | DetCurveDisplay                                                                              | RocCurveDisplay                                                                                                                                                      | PrecisionRecallDisplay                                                                                                                                               |\n|------------------|--------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| plot             | ax<br>name<br>ref_line<br>kwargs                                                           | ax<br>name<br>kwargs                                                                         | ax<br>name<br>plot_chance_level<br>chance_level_kw<br>despine<br>kwargs                                                                                              | ax<br>name<br>plot_chance_level<br>chance_level_kw<br>despine<br>kwargs                                                                                              |\n| from_estimator   | estimator<br>X<br>y<br>n_bins<br>strategy<br>pos_label<br>name<br>ref_line<br>ax<br>kwargs | estimator<br>X<br>y<br>sample_weight<br>response_method<br>pos_label<br>name<br>ax<br>kwargs | estimator<br>X<br>y<br>sample_weight<br>drop_intermediate<br>response_method<br>pos_label<br>name<br>ax<br>plot_chance_level<br>chance_level_kw<br>despine<br>kwargs | estimator<br>X<br>y<br>sample_weight<br>pos_label<br>drop_intermediate<br>response_method<br>name<br>ax<br>plot_chance_level<br>chance_level_kw<br>despine<br>kwargs |\n| from_predictions | y_true<br>y_prob<br>n_bins<br>strategy<br>pos_label<br>name<br>ref_line<br>ax<br>kwargs    | y_true<br>y_pred<br>sample_weight<br>pos_label<br>name<br>ax<br>kwargs                       | y_true<br>y_pred<br>sample_weight<br>drop_intermediate<br>pos_label<br>name<br>ax<br>plot_chance_level<br>chance_level_kw<br>despine<br>kwargs                       | y_true<br>y_pred<br>sample_weight<br>pos_label<br>drop_intermediate<br>name<br>ax<br>plot_chance_level<br>chance_level_kw<br>despine<br>kwargs                       |\n\n</details>\n\nDiscussed with @DeaMariaLeon @glemaitre ",
    "comments": [
      {
        "user": "DeaMariaLeon",
        "body": "/take"
      }
    ]
  },
  {
    "issue_number": 31318,
    "title": "`ValueError` raised by `FeatureUnion._set_output` with `FunctionTransform` that outputs a pandas `Series` in scikit-learn version 1.6",
    "author": "Bilalbrahimi",
    "state": "open",
    "created_at": "2025-05-06T11:44:35Z",
    "updated_at": "2025-06-16T12:16:34Z",
    "labels": [
      "Bug",
      "good first issue"
    ],
    "body": "### Describe the bug\n\nHello,\n\nI'm currently working with scikit-learn version 1.6, and I encountered a regression that wasn't present in version 1.4.\n\nThe following minimal code computes two features â€” the cumulative mean of age and weight grouped by id. Each transformation function returns a pandas.Series:\n\n\n\nWhen I run this code with scikit-learn 1.6, I get the following error:\n\nAfter investigation, I found that the issue occurs because each transformer returns a Series, not a DataFrame. If I update the functions to return DataFrame objects instead, the error disappears.\n\nInterestingly, in scikit-learn 1.4, the same code works correctly even when the functions return Series.\n\n\nDo you have any explanation for why this changed between version 1.4 and 1.6 ?\n\nThanks in advance for your help!\n\n### Steps/Code to Reproduce\n\n\n```python\nimport pandas as pd\nfrom sklearn.pipeline import FunctionTransformer, FeatureUnion\nimport numpy as np\n\ndef compute_cumulative_mean_age(df: pd.DataFrame) -> pd.Series:\n    return (\n        df[\"age\"]\n        .astype(float)\n        .groupby(df[\"id\"])\n        .expanding()\n        .mean()\n        .droplevel(level=\"id\")\n        .reindex(df.index)\n        .rename(\"cumulative_mean_age\")\n    )\n\ndef compute_cumulative_mean_weight(df: pd.DataFrame) -> pd.Series:\n    return (\n        df[\"poids\"]\n        .astype(float)\n        .groupby(df[\"id\"])\n        .expanding()\n        .mean()\n        .droplevel(level=\"id\")\n        .reindex(df.index)\n        .rename(\"cumulative_mean_weight\")\n    )\n\ndef compute_features(df: pd.DataFrame) -> pd.DataFrame:\n    feature_union = FeatureUnion(\n        [\n            (\"cumulative_mean_age\", FunctionTransformer(compute_cumulative_mean_age)),\n            (\"cumulative_mean_weight\", FunctionTransformer(compute_cumulative_mean_weight))\n        ]\n    ).set_output(transform=\"pandas\")\n\n    return feature_union.fit_transform(X=df).astype(float)\n\ndef transform(df: pd.DataFrame) -> pd.DataFrame:\n    return compute_features(df)\n\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    df = pd.DataFrame({\n        'id': [1, 2, 3, 1, 4, 5, 6, 6, 7, 8],\n        'age': np.random.randint(18, 70, size=10),\n        'poids': np.random.randint(50, 100, size=10)\n    })\n\n    print(transform(df))\n```\n\n### Expected Results\n\n```pytb\nTraceback (most recent call last):\n  File \"C:\\Users\\XXX\\PycharmProjects\\fraude_detection_pec_audio\\tmp.py\", line 73, in <module>\n    print(transform(df=df))\n          ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\XXX\\PycharmProjects\\fraude_detection_pec_audio\\tmp.py\", line 55, in transform\n    return compute_features(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\XXX\\PycharmProjects\\fraude_detection_pec_audio\\tmp.py\", line 45, in compute_features\n    .fit_transform(\n     ^^^^^^^^^^^^^^\n  File \"C:\\Users\\XXX\\PycharmProjects\\fraude_detection_pec_audio\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 332, in wrapped\n    return _wrap_data_with_container(method, data_to_wrap, X, self)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\XXX\\PycharmProjects\\fraude_detection_pec_audio\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 307, in _wrap_data_with_container\n    return adapter.create_container(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\XXX\\PycharmProjects\\fraude_detection_pec_audio\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 135, in create_container\n    X_output = _create_pandas_dataframe_from_non_pandas_container(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\XXX\\PycharmProjects\\fraude_detection_pec_audio\\.venv\\Lib\\site-packages\\sklearn\\utils\\fixes.py\", line 428, in _create_pandas_dataframe_from_non_pandas_container\n    return pd.DataFrame(X, index=index, copy=copy)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\XXX\\PycharmProjects\\fraude_detection_pec_audio\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 722, in __init__\n    mgr = ndarray_to_mgr(\n          ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\XXX\\PycharmProjects\\fraude_detection_pec_audio\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 349, in ndarray_to_mgr\n    _check_values_indices_shape_match(values, index, columns)\n  File \"C:\\Users\\XXX\\PycharmProjects\\fraude_detection_pec_audio\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 420, in _check_values_indices_shape_match\n    raise ValueError(f\"Shape of passed values is {passed}, indices imply {implied}\")\nValueError: Shape of passed values is (20, 1), indices imply (10, 1)\n```\n\n### Actual Results\n\n```pytb\n    raise ValueError(f\"Shape of passed values is {passed}, indices imply {implied}\")\nValueError: Shape of passed values is (20, 1), indices imply (10, 1)\n```\n\n### Versions\n\n```shell\nsklearn: 1.6.0\nnumpy: 1.26.4\npandas: 1.5.3\n```",
    "comments": [
      {
        "user": "glemaitre",
        "body": "I would argue that the previous beahviour worked by chance but it was a bug. Scikit-learn transformers API expect to return an array of shape `(n_samples, n_features)`. Having a single dimension is really ambiguous because we cannot guess if it is a single sample or a single feature. For instance, if you use `set_output(transform=\"default\")`, then you will get an array of shape `(20,)` where I would argue that the `FeatureUnion` failed.\n\nSo here, I think that the fact that we raise an error is actually fine. The error message is a bit tricky thought. The fact it does not raise with `set_output(transform=\"default\")` is actually annoying I think.\n\nSo one way would be to validate the output of `FunctionTransformer` being sure it should be 2-D. However, we sometime want this transformer to be lenient so I wonder if the behaviour should be driven by a parameter, similarly to the `validate` parameter but for the output."
      },
      {
        "user": "glemaitre",
        "body": "@jeremiedbb @ogrisel do you have any thoughts regarding if `FunctionTransformer` should be permissive regarding the shape of the output or if we should strictly follow the API of other transformers.\n\nI'm thinking that we usually want this transformer to be permissive on the input side."
      },
      {
        "user": "ogrisel",
        "body": "Note that the mentioned code already raises 2 informative warnings before raising the final exception:\n\n```python\n/Users/ogrisel/code/scikit-learn/sklearn/preprocessing/_function_transformer.py:311: UserWarning: When `set_output` is configured to be 'pandas', `func` should return a pandas DataFrame to follow the `set_output` API  or `feature_names_out` should be defined.\n  warnings.warn(warn_msg.format(\"pandas\"))\n/Users/ogrisel/code/scikit-learn/sklearn/preprocessing/_function_transformer.py:311: UserWarning: When `set_output` is configured to be 'pandas', `func` should return a pandas DataFrame to follow the `set_output` API  or `feature_names_out` should be defined.\n  warnings.warn(warn_msg.format(\"pandas\"))\n```\n\nThe function transformer should not return a pandas series but a pandas dataframe.\n\nMaybe we could change this to accept series and automatically wrap them as dataframes because there is no ambiguity in this particular case.\n\nIf not, I agree that we should at least improve the error message.\n\nLet me try to simplify the reproducer to make it minimal."
      }
    ]
  },
  {
    "issue_number": 30339,
    "title": "DOC: clarify the documentation for the loss functions used in GBRT, and Absolute Error in particular.",
    "author": "AhmedThahir",
    "state": "closed",
    "created_at": "2024-11-23T19:46:07Z",
    "updated_at": "2025-06-16T10:08:59Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the bug\r\n\r\nFrom my understanding, currently there is no way to minimize the MAE (Mean Absolute Error). Quantile regression with quantile=0.5 will optimize for the Median Absolute Error. This would be different from optimizing the MAE when the conditional distribution of the response variable is not symmetrically-distributed.\r\n\r\nhttps://github.com/scikit-learn/scikit-learn/blob/46a7c9a5e4fe88dfdfd371bf36477f03498a3390/sklearn/_loss/loss.py#L574-L577\r\n\r\n**What I expect**\r\n- Using `HistGradientBoostingRegressor(loss=\"absolute_error\")` should optimize for the mean of absolute errors.\r\n- Using `HistGradientBoostingRegressor(loss=\"quantile\", quantile=0.5)` should optimize for the median of absolute errors.\r\n\r\n```python\r\n        if sample_weight is None:\r\n            return np.mean(y_true, axis=0)\r\n        else:\r\n            return _weighted_mean(y_true, sample_weight)\r\n```\r\n\r\n**What happens**\r\nBoth give the same results\r\n- Using `HistGradientBoostingRegressor(loss=\"absolute_error\")` optimizes for the median of absolute errors\r\n- Using `HistGradientBoostingRegressor(loss=\"quantile\", quantile=0.5)` optimizes for the median of absolute errors\r\n\r\n**Suggested Actions**\r\n\r\nIf this is intended behavior:\r\n- Feel free to close this issue marked as resolved.\r\n- Kindly add a note in the documentation that \"Absolute Error optimizes for Median Absolute Error, not Mean Absolute Error\" as \"absolute_error\" is not very clear.\r\n- I would appreciate if there was more explanation regarding on using custom loss functions #21614. This way, we could optimize for Mean Absolute Error, Median Absolute Error, Log Cosh, etc. as per the requirement.\r\n\r\n**Note**\r\nI have tried my best to go through the documentation prior to creating this issue. I am a fresh graduate in Computer Science, and if you believe this issue is not well-framed due to a misunderstanding of my concepts, kindly advise me and I'll work on it.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\n# Imports\r\nfrom sklearn.ensemble import HistGradientBoostingRegressor\r\nimport numpy as np\r\n\r\n# Dataset Generation\r\nx = np.linspace(start=0, stop=10, num=100)\r\n\r\nn_repeat = 100 # no of x for each x\r\nX = np.repeat(x, n_repeat)[:, np.newaxis]\r\ny_true_mean = 1 * np.repeat(x, n_repeat)\r\nnoise = np.random.RandomState(0).lognormal(mean=0, sigma=1, size=y_true_mean.shape[0])\r\ny_noisy = y_true_mean + noise\r\n\r\n# Model Creation\r\nmae = HistGradientBoostingRegressor(loss=\"absolute_error\") # should be mean of absolute errors\r\nquantile = HistGradientBoostingRegressor(loss=\"quantile\", quantile=0.5) # should be median of absolute errors\r\n\r\n# Fit & Prediction\r\ny_pred_mae = mae.fit(X, y_noisy).predict(X)\r\ny_pred_quantile = quantile.fit(X, y_noisy).predict(X)\r\n\r\n# Prediction Comparison\r\nprint((y_pred_mae - y_pred_quantile).sum()) # both give same results\r\n```\r\n\r\n### Expected Results\r\n\r\nMedian and mean of absolute errors should give different results for a log-normally distributed response. Hence, the predictions should be different from each other, and the difference of their predictions, should total as a non-zero value.\r\n\r\n### Actual Results\r\n\r\nPredictions by both models are the same, which can be seen in the difference of their predictions, totaling as 0.\r\n\r\n```shell\r\n0.\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\r\nexecutable: /usr/bin/python3\r\n   machine: Linux-6.1.85+-x86_64-with-glibc2.35\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.2\r\n          pip: 24.1.2\r\n   setuptools: 75.1.0\r\n        numpy: 1.26.4\r\n        scipy: 1.13.1\r\n       Cython: 3.0.11\r\n       pandas: 2.2.2\r\n   matplotlib: 3.8.0\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 2\r\n         prefix: libopenblas\r\n       filepath: /usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so\r\n        version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 2\r\n         prefix: libopenblas\r\n       filepath: /usr/local/lib/python3.10/dist-packages/scipy.libs/libopenblasp-r0-01191904.3.27.so\r\n        version: 0.3.27\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 2\r\n         prefix: libgomp\r\n       filepath: /usr/local/lib/python3.10/dist-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n```",
    "comments": [
      {
        "user": "glemaitre",
        "body": "The part of the code that you are pointing out is the computation of the intercept. The estimator of a model minimizing the mean absolute error is the median. So the computation of the intercept looks logical to me."
      },
      {
        "user": "AhmedThahir",
        "body": "From my understanding, optimizing Mean Absolute Error and Median Absolute Error should give different results, but I believe that either I have not been able to communicate my question clearly, or have I not provided sufficient backing for my question, or I have confused the code for the intercept above.\r\n\r\nAny resources would be much appreciated.\r\n> I would appreciate if there was more explanation regarding on using custom loss functions Custom Loss function? #21614. This way, we could optimize for Mean Absolute Error, Median Absolute Error, Log Cosh, etc. as per the requirement.\r\n\r\nAside from that, I will try my best to research further and formulate a better question in the future. You may close this issue. Thank you!"
      },
      {
        "user": "ogrisel",
        "body": "The median is the minimizer of the mean absolute error. Let's sample same data distributed in such a way that the mean and the median are different:\r\n\r\n```python\r\nimport numpy as np\r\n\r\ndata = np.random.lognormal(mean=0, sigma=1, size=1000)\r\nnp.median(data).round(4), np.mean(data).round(4)\r\n```\r\n```\r\n(np.float64(0.9989), np.float64(1.6216))\r\n```\r\n\r\nLet's find the minimizer of MAE using `scipy.optimize`: \r\n\r\n```python\r\nfrom scipy.optimize import fmin\r\n\r\n\r\ndef mae(x, data):\r\n    return np.mean(np.abs(data - x))\r\n\r\nx = np.zeros(shape=1, dtype=np.float64)\r\nfmin(mae, x, args=(data,), disp=False)\r\n```\r\n```\r\narray([0.99875])\r\n```"
      }
    ]
  },
  {
    "issue_number": 30453,
    "title": "Issue: Unable to Apply Feature Engineering After Feature Transformation in a Pipeline",
    "author": "olitei",
    "state": "closed",
    "created_at": "2024-12-10T12:00:08Z",
    "updated_at": "2025-06-16T10:06:03Z",
    "labels": [],
    "body": "I need to:\r\n\r\nGenerate predictions (meta-features) from base learners.\r\nApply feature engineering (e.g., scaling, PCA) only on the meta-features.\r\nTrain a final model using the engineered meta-features.\r\nHowever, in a standard Pipeline, all transformations are applied sequentially to the input features. Thereâ€™s no built-in way to apply feature engineering only after the meta-feature transformation step.\r\n\r\n\r\nI should be able to:\r\n\r\nGenerate meta-features using base learners.\r\nApply feature engineering only to these meta-features (not the original input features).\r\nTrain the final model using the engineered meta-features.\r\nSuggested Solution\r\nAllow pipelines to apply transformations specifically to intermediate outputs (e.g., meta-features).\r\nAdd an option for transformers to operate only on outputs from previous steps in the pipeline.\r\n\r\nThis is a common use case for stacking and ensemble workflows. Simplifying this functionality would improve flexibility and usability.",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Our pipeline is not directly suitable to implement stacking as a whole because one typically needs to do [cross-fitting](https://scikit-learn.org/stable/glossary.html#term-cross-fitting): computing predictions for data-points that were not used to train the first stage estimators by iteratively partitioning the data into folds similar to what we do for cross-validation. This is one of the reasons `StackingClassifier` and `StackingRegressor` were implemented as meta-estimators: https://scikit-learn.org/stable/modules/ensemble.html#stacking\r\n\r\nThat being said, we could maybe also introduce a `StackingTransformer` class that only implements the cross-fitting logic without fitting the second stage classifier / regressor: instead it would return the predictions on the held-out set in `fit_transform`.\r\n\r\nThis way, it would be possible to train downstream models on a mix of features and \"meta-features\" way you want using `Pipeline`, `ColumnTransformer` and `FeatureUnion`.\r\n\r\nWe already implement a similar cross-fitting transformer pattern in our `TargetEncoder` (for supervised categorical variable encoding):\r\n\r\nhttps://scikit-learn.org/stable/modules/preprocessing.html#target-encoder\r\n\r\nHowever, there is a big caveat emptor: users must be aware that `fit_transform(X_train)` will intentionally not return the same results as `.fit(X_train).transform(X_train)` (as explained in the [docstring of TargetEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#targetencoder)) and only the former should be used to generate the training set of the downstream model in the pipeline. The alternative would lead to a catastrophic distribution shift that would severely degrade the model predictive performance, as illustrated in this example:\r\n\r\nhttps://scikit-learn.org/stable/auto_examples/preprocessing/plot_target_encoder_cross_val.html"
      },
      {
        "user": "olitei",
        "body": "Imagine this case regarding Regression Problems...\nCan I create a pipeline like this in order to apply a PCA for base_learners predictions?\n\n```python\n#   Generate a sample dataset\nX, y = make_regression(n_samples=1000, n_features=20, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nbase_learners = [\n    ('lr', LinearRegression()),\n    ('rf', RandomForestRegressor(n_estimators=20, random_state=42)),\n]\n\ntransformer_pipeline = Pipeline([\n    ('pca', PCA(n_components=2)),  # Feature engineering on predictions\n])\n#   Define the meta-model\nmeta_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\nstacking = StackingRegressor(\n    estimators=base_learners,\n    final_estimator=Pipeline([\n        ('transformer', transformer_pipeline),  # Apply transformation\n        ('meta', meta_model)  # Train meta-model\n    ]),\n    passthrough=False  # Include original features\n)\nstacking.fit(X_train, y_train)\nstacking.fit_transform(X_train, y_train)\n```"
      },
      {
        "user": "glemaitre",
        "body": "You could make the the `base_learners` are pipelines with a `PCA`. It is nice in the way that you can then apply a different transformation on each base learner (some learners will require different type of encoding, scaling, etc.). However, you don't share the transformers between each base learners."
      }
    ]
  },
  {
    "issue_number": 30546,
    "title": "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 33810) + inhomogeneous part.",
    "author": "mtoan65",
    "state": "closed",
    "created_at": "2024-12-27T13:47:54Z",
    "updated_at": "2025-06-16T10:00:07Z",
    "labels": [
      "Bug",
      "Needs Info"
    ],
    "body": "Hello Scikit-learn team,\r\n\r\nI am encountering an issue while running inference VotingClassifier model with `voting=\"hard\"` argument, I found that this issue may related to [NEP 34](https://numpy.org/neps/nep-0034-infer-dtype-is-object.html) restriction of `dtype=object` in numpy and the solution is downgrading to numpy `1.23.1`. However, it doesn't work in my case due to dependency conflicts with pandas and other packages. I'd appreciate if you could analyze this issue and provide an update when possible.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mtoan65/Documents/Sentiment_Analysis/training.py\", line 135, in <module>\r\n    ensemble_model, trained_models, model_results, ensemble_results = main(sparse=False)\r\n                                                                      ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mtoan65/Documents/Sentiment_Analysis/training.py\", line 127, in main\r\n    trained_ensemble, ensemble_results = train_ensemble_model(\r\n                                         ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mtoan65/Documents/Sentiment_Analysis/training.py\", line 89, in train_ensemble_model\r\n    ensemble_results, trained_ensemble = train_and_evaluate_ensemble(voting_clf, X_train, X_test, y_train, y_test)\r\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mtoan65/Documents/Sentiment_Analysis/training/ensemble_trainer.py\", line 33, in train_and_evaluate_ensemble\r\n    y_pred_ensemble = voting_clf.predict(X_test)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mtoan65/Documents/Sentiment_Analysis/.venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 443, in predict\r\n    predictions = self._predict(X)\r\n                  ^^^^^^^^^^^^^^^^\r\n  File \"/home/mtoan65/Documents/Sentiment_Analysis/.venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 80, in _predict\r\n    return np.asarray([est.predict(X) for est in self.estimators_]).T\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 33810) + inhomogeneous part.\r\n```\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```\r\ntry:\r\n  main_logger.info(\"Training ensemble\")\r\n  voting_clf.fit(X_train, y_train)\r\n  \r\n  main_logger.info(\"Evaluating ensemble\")\r\n  y_pred_ensemble = voting_clf.predict(X_test)\r\n  results = classification_report(y_test, y_pred_ensemble, output_dict=True)\r\n  main_logger.info(f\"Ensemble Results:\\n{classification_report(y_test, y_pred_ensemble)}\")\r\n  \r\n  return results, voting_clf\r\n\r\nexcept Exception as e:\r\n    main_logger.error(f\"Error in ensemble training: {str(e)}\")\r\n    raise\r\n```\r\n\r\n### Expected Results\r\n\r\n```Finish training```\r\n\r\n### Actual Results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mtoan65/Documents/Sentiment_Analysis/training.py\", line 135, in <module>\r\n    ensemble_model, trained_models, model_results, ensemble_results = main(sparse=False)\r\n                                                                      ^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mtoan65/Documents/Sentiment_Analysis/training.py\", line 127, in main\r\n    trained_ensemble, ensemble_results = train_ensemble_model(\r\n                                         ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mtoan65/Documents/Sentiment_Analysis/training.py\", line 89, in train_ensemble_model\r\n    ensemble_results, trained_ensemble = train_and_evaluate_ensemble(voting_clf, X_train, X_test, y_train, y_test)\r\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mtoan65/Documents/Sentiment_Analysis/training/ensemble_trainer.py\", line 33, in train_and_evaluate_ensemble\r\n    y_pred_ensemble = voting_clf.predict(X_test)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mtoan65/Documents/Sentiment_Analysis/.venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 443, in predict\r\n    predictions = self._predict(X)\r\n                  ^^^^^^^^^^^^^^^^\r\n  File \"/home/mtoan65/Documents/Sentiment_Analysis/.venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 80, in _predict\r\n    return np.asarray([est.predict(X) for est in self.estimators_]).T\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 33810) + inhomogeneous part.\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\n1.5.2\r\n```\r\n",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Hi @mtoan65. Sorry, but we cannot help you with such a user-specific report. To help you, we need a minimal reproducer.\r\n\r\nPlease follow these guidelines until the end: https://scikit-learn.org/stable/developers/minimal_reproducer.html\r\n\r\nBy the way, the solution to a problem is never downgrading to an older numpy release but instead to adapt the code to follow the best practice documented in the latest version. Otherwise your code will no longer work once the old numpy release is no longer supported."
      },
      {
        "user": "glemaitre",
        "body": "Closing since we lack the info to reproduce the problem. Feel free to reopen an issue with the requested information."
      }
    ]
  },
  {
    "issue_number": 31302,
    "title": "Reference `ValidationCurveDisplay` from `validation_curve`'s docstring in a \"See also section\"",
    "author": "ogrisel",
    "state": "closed",
    "created_at": "2025-05-05T12:50:38Z",
    "updated_at": "2025-06-16T06:56:58Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the issue linked to the documentation\n\nThe docstring of `validation_curve` should point to the `ValidationCurveDisplay.from_estimator` factory method as a complementary tool that both computes the curves points and display them using matplotlib.\n\nIf you want to open a PR for this, please review some \"See also\" sections in other sections using `git grep` or the search feature of your IDE or github code search:\n\nhttps://github.com/search?q=repo%3Ascikit-learn%2Fscikit-learn+%22See+also%22+language%3APython+path%3A%2F%5Esklearn%5C%2F%2F&type=code\n",
    "comments": [
      {
        "user": "Meddhif13",
        "body": "Hey @ArturoAmorQ I would like to work on the issue,"
      },
      {
        "user": "glemaitre",
        "body": "Addressed in https://github.com/scikit-learn/scikit-learn/pull/31314"
      }
    ]
  },
  {
    "issue_number": 31546,
    "title": "Regression in `DecisionBoundaryDisplay.from_estimator` with `colors` and `plot_method='contour'` after upgrading to v1.7.0",
    "author": "jshn9515",
    "state": "open",
    "created_at": "2025-06-14T16:22:44Z",
    "updated_at": "2025-06-16T05:59:30Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nHello. Recently, after upgrading to scikit-learn v1.7.0, I encountered an issue when using `DecisionBoundaryDisplay.from_estimator` with the `colors` keyword argument. Specifically, the following error is raised:\n```python\n  File \"D:\\Project\\Python Project\\venv\\Lib\\site-packages\\sklearn\\inspection\\_plot\\decision_boundary.py\", line 276, in plot\n    plot_func(self.xx0, self.xx1, response, cmap=cmap, **safe_kwargs)\n    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Project\\Python Project\\venv\\Lib\\site-packages\\matplotlib\\contour.py\", line 689, in __init__\n    raise ValueError('Either colors or cmap must be None')\nValueError: Either colors or cmap must be None\n```\nHowever, in v1.6.0, everything works fine.\n\nAfter further investigation, it seems this issue was introduced by PR #29797, where both `cmap` and `colors` are passed to `plot_func` unconditionally, without explicit conflict handling:\nhttps://github.com/scikit-learn/scikit-learn/blob/d4d4af8c471c60d183d0cb67e14e6434b0ebb9fb/sklearn/inspection/_plot/decision_boundary.py#L276\nAdditionally, when setting `plot_method='contour'` in multiclass classification scenarios, the decision boundary is no longer shown as it was in v1.6.0. It appears that this regression is due to the switch in v1.7.0 to always using a cmap to plot the entire decision surface in multiclass scenarios.\n\nHere are the visual differences:\n- v1.6.0 with `plot_method='contour'`:\n![Image](https://github.com/user-attachments/assets/858d2540-47d5-4637-b992-89dc9b196b08)\n- v1.7.0 with the same code:\n![Image](https://github.com/user-attachments/assets/6d7a5c0e-2df9-47c0-bc9c-3a4e0e5dbac4)\n## Suggestion\nTo preserve backward compatibility and expected behavior:\n- Check for mutual exclusivity of `colors` and `cmap` and raise a clear warning/error;\n- Retain the old behavior when `plot_method='contour'`.\n\nI'd be happy to open a PR to help address this regression if the core team is supportive.\n\n### Steps/Code to Reproduce\n\n```python\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.linear_model import LogisticRegression\n\niris = load_iris()\nX = iris.data[:, :2]\nclassifier = LogisticRegression().fit(X, iris.target)\ndisp = DecisionBoundaryDisplay.from_estimator(\n    classifier,\n    X,\n    # plot_method='contour',\n    xlabel=iris.feature_names[0],\n    ylabel=iris.feature_names[1],\n    colors='black',\n    alpha=0.5,\n)\ndisp.ax_.scatter(X[:, 0], X[:, 1], c=iris.target, edgecolor='k')\nplt.show()\n```\n\n### Expected Results\n\nNo error is raised when using `plot_method='contour'` together with `colors`, and the decision boundary is displayed correctly as expected.\n\n### Actual Results\n\n```python\nTraceback (most recent call last):\n  File \"d:\\Project\\Python Project\\cc.py\", line 9, in <module>\n    disp = DecisionBoundaryDisplay.from_estimator(\n        classifier,\n    ...<4 lines>...\n        alpha=0.5,\n    )\n  File \"D:\\Project\\Python Project\\venv\\Lib\\site-packages\\sklearn\\inspection\\_plot\\decision_boundary.py\", line 558, in from_estimator\n    return display.plot(ax=ax, plot_method=plot_method, **kwargs)\n           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Project\\Python Project\\venv\\Lib\\site-packages\\sklearn\\inspection\\_plot\\decision_boundary.py\", line 276, in plot\n    plot_func(self.xx0, self.xx1, response, cmap=cmap, **safe_kwargs)\n    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Project\\Python Project\\venv\\Lib\\site-packages\\matplotlib\\__init__.py\", line 1521, in inner\n    return func(\n        ax,\n        *map(cbook.sanitize_sequence, args),\n        **{k: cbook.sanitize_sequence(v) for k, v in kwargs.items()})\n  File \"D:\\Project\\Python Project\\venv\\Lib\\site-packages\\matplotlib\\axes\\_axes.py\", line 6794, in contourf\n    contours = mcontour.QuadContourSet(self, *args, **kwargs)\n  File \"D:\\Project\\Python Project\\venv\\Lib\\site-packages\\matplotlib\\contour.py\", line 689, in __init__\n    raise ValueError('Either colors or cmap must be None')\nValueError: Either colors or cmap must be None\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.13.4 (tags/v3.13.4:8a526ec, Jun  3 2025, 17:46:04) [MSC v.1943 64 bit (AMD64)]\nexecutable: D:\\Project\\Python Project\\venv\\Scripts\\python.exe\n   machine: Windows-11-10.0.26100-SP0\n\nPython dependencies:\n      sklearn: 1.7.0\n          pip: None\n   setuptools: 80.9.0\n        numpy: 2.3.0\n        scipy: 1.15.3\n       Cython: 3.1.2\n       pandas: 2.3.0\n   matplotlib: 3.10.3\n       joblib: 1.5.1\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 16\n         prefix: vcomp\n       filepath: D:\\Project\\Python Project\\venv\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\n        version: None\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: D:\\Project\\Python Project\\venv\\Lib\\site-packages\\numpy.libs\\libscipy_openblas64_-13e2df515630b4a41f92893938845698.dll\n        version: 0.3.29\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: D:\\Project\\Python Project\\venv\\Lib\\site-packages\\scipy.libs\\libscipy_openblas-f07f5a5d207a3a47104dca54d6d0c86a.dll\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n```",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "Thanks for the report. A pull request would be very much appreciated. Feel free to ping me for a review."
      },
      {
        "user": "Nithishkaranam2002",
        "body": "Hi, Iâ€™d be happy to work on a PR to fix this issue."
      },
      {
        "user": "jshn9515",
        "body": "Hi @adrinjalali, Iâ€™ve opened PR #31553 to fix this issue. Feel free to take a look when you have time."
      }
    ]
  },
  {
    "issue_number": 30924,
    "title": "KBinsDiscretizer uniform strategy bin assignment wrong due to floating point multiplication",
    "author": "bnomis",
    "state": "open",
    "created_at": "2025-03-02T17:35:39Z",
    "updated_at": "2025-06-16T05:31:11Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nKBinsDiscretizer uniform strategy uses numpy.linspace to make bin edges. \n\nnumpy.linspace works out a delta like: delta = (max - min)/num_bins \n\nThen the bin edges are computed: delta * n\n\nThe issue is the floating point multiplication introduces noise in the low bits. \n\nFor example, consider the case of floating point sample values from zero to one and five bins. Then:\n\ndelta = 1/5 = 0.2\n\nThe right edge of bin 2 (zero indexed) should be 0.6 = 0.2 * 3 but (in my tests) it's 0.6000000000000001 \n\nExample python calculation:\n\n```python\n>>> 1/5 * 3\n0.6000000000000001\n```\n\nThis means a sample values of 0.6 get assigned to bin 2 but it should be in bin 3\n\nOne work around is to use the fractions module or better still the decimal module. The code below demonstrates the issue\n\n```python\n#!/usr/bin/env python\nimport decimal\nimport fractions\nimport sys\nfrom typing import NoReturn\n\n\ndef test_float_fractions():\n    # check floating point multiplication\n    step = 1 / 5\n    f_step = fractions.Fraction(1, 5)\n    d_step = decimal.Decimal(1) / decimal.Decimal(5)\n\n    print('float vs fractions')\n    for n in range(101):\n        float_value = step * n\n        fraction_value = f_step * n\n        fraction_float = float(fraction_value)\n        if float_value != fraction_float:\n            fraction_str = str(fraction_value)\n            print(f'{n:2} float {float_value:20.16f} fraction {fraction_float:20.16f} {fraction_str:>5}')\n\n    print('')\n    print('float vs decimals')\n    for n in range(101):\n        float_value = step * n\n        decimal_value = d_step * n\n        if float_value != decimal_value:\n            print(f'{n:2} float {float_value:23.20f}  decimal {decimal_value:23.20f}')\n\n\ndef main(argv) -> NoReturn:\n    m = 0\n    try:\n        test_float_fractions()\n    except Exception as e:\n        print(f'Exception: {e}')\n    sys.exit(m)\n\n\nif __name__ == '__main__':\n    main(sys.argv[1:])\n```\n\nRunning the above yields the output below:\n\n\n```shell\nfloat vs fractions\n 3 float   0.6000000000000001 fraction   0.6000000000000000   3/5\n 6 float   1.2000000000000002 fraction   1.2000000000000000   6/5\n 7 float   1.4000000000000001 fraction   1.3999999999999999   7/5\n12 float   2.4000000000000004 fraction   2.3999999999999999  12/5\n14 float   2.8000000000000003 fraction   2.7999999999999998  14/5\n17 float   3.4000000000000004 fraction   3.3999999999999999  17/5\n19 float   3.8000000000000003 fraction   3.7999999999999998  19/5\n23 float   4.6000000000000005 fraction   4.5999999999999996  23/5\n24 float   4.8000000000000007 fraction   4.7999999999999998  24/5\n28 float   5.6000000000000005 fraction   5.5999999999999996  28/5\n29 float   5.8000000000000007 fraction   5.7999999999999998  29/5\n33 float   6.6000000000000005 fraction   6.5999999999999996  33/5\n34 float   6.8000000000000007 fraction   6.7999999999999998  34/5\n38 float   7.6000000000000005 fraction   7.5999999999999996  38/5\n39 float   7.8000000000000007 fraction   7.7999999999999998  39/5\n41 float   8.2000000000000011 fraction   8.1999999999999993  41/5\n46 float   9.2000000000000011 fraction   9.1999999999999993  46/5\n48 float   9.6000000000000014 fraction   9.5999999999999996  48/5\n51 float  10.2000000000000011 fraction  10.1999999999999993  51/5\n53 float  10.6000000000000014 fraction  10.5999999999999996  53/5\n56 float  11.2000000000000011 fraction  11.1999999999999993  56/5\n58 float  11.6000000000000014 fraction  11.5999999999999996  58/5\n61 float  12.2000000000000011 fraction  12.1999999999999993  61/5\n63 float  12.6000000000000014 fraction  12.5999999999999996  63/5\n66 float  13.2000000000000011 fraction  13.1999999999999993  66/5\n68 float  13.6000000000000014 fraction  13.5999999999999996  68/5\n71 float  14.2000000000000011 fraction  14.1999999999999993  71/5\n73 float  14.6000000000000014 fraction  14.5999999999999996  73/5\n76 float  15.2000000000000011 fraction  15.1999999999999993  76/5\n78 float  15.6000000000000014 fraction  15.5999999999999996  78/5\n82 float  16.4000000000000021 fraction  16.3999999999999986  82/5\n87 float  17.4000000000000021 fraction  17.3999999999999986  87/5\n92 float  18.4000000000000021 fraction  18.3999999999999986  92/5\n96 float  19.2000000000000028 fraction  19.1999999999999993  96/5\n97 float  19.4000000000000021 fraction  19.3999999999999986  97/5\n\nfloat vs decimals\n 1 float  0.20000000000000001110  decimal  0.20000000000000000000\n 2 float  0.40000000000000002220  decimal  0.40000000000000000000\n 3 float  0.60000000000000008882  decimal  0.60000000000000000000\n 4 float  0.80000000000000004441  decimal  0.80000000000000000000\n 6 float  1.20000000000000017764  decimal  1.20000000000000000000\n 7 float  1.40000000000000013323  decimal  1.40000000000000000000\n 8 float  1.60000000000000008882  decimal  1.60000000000000000000\n 9 float  1.80000000000000004441  decimal  1.80000000000000000000\n11 float  2.20000000000000017764  decimal  2.20000000000000000000\n12 float  2.40000000000000035527  decimal  2.40000000000000000000\n13 float  2.60000000000000008882  decimal  2.60000000000000000000\n14 float  2.80000000000000026645  decimal  2.80000000000000000000\n16 float  3.20000000000000017764  decimal  3.20000000000000000000\n17 float  3.40000000000000035527  decimal  3.40000000000000000000\n18 float  3.60000000000000008882  decimal  3.60000000000000000000\n19 float  3.80000000000000026645  decimal  3.80000000000000000000\n21 float  4.20000000000000017764  decimal  4.20000000000000000000\n22 float  4.40000000000000035527  decimal  4.40000000000000000000\n23 float  4.60000000000000053291  decimal  4.60000000000000000000\n24 float  4.80000000000000071054  decimal  4.80000000000000000000\n26 float  5.20000000000000017764  decimal  5.20000000000000000000\n27 float  5.40000000000000035527  decimal  5.40000000000000000000\n28 float  5.60000000000000053291  decimal  5.60000000000000000000\n29 float  5.80000000000000071054  decimal  5.80000000000000000000\n31 float  6.20000000000000017764  decimal  6.20000000000000000000\n32 float  6.40000000000000035527  decimal  6.40000000000000000000\n33 float  6.60000000000000053291  decimal  6.60000000000000000000\n34 float  6.80000000000000071054  decimal  6.80000000000000000000\n36 float  7.20000000000000017764  decimal  7.20000000000000000000\n37 float  7.40000000000000035527  decimal  7.40000000000000000000\n38 float  7.60000000000000053291  decimal  7.60000000000000000000\n39 float  7.80000000000000071054  decimal  7.80000000000000000000\n41 float  8.20000000000000106581  decimal  8.20000000000000000000\n42 float  8.40000000000000035527  decimal  8.40000000000000000000\n43 float  8.59999999999999964473  decimal  8.60000000000000000000\n44 float  8.80000000000000071054  decimal  8.80000000000000000000\n46 float  9.20000000000000106581  decimal  9.20000000000000000000\n47 float  9.40000000000000035527  decimal  9.40000000000000000000\n48 float  9.60000000000000142109  decimal  9.60000000000000000000\n49 float  9.80000000000000071054  decimal  9.80000000000000000000\n51 float 10.20000000000000106581  decimal 10.20000000000000000000\n52 float 10.40000000000000035527  decimal 10.40000000000000000000\n53 float 10.60000000000000142109  decimal 10.60000000000000000000\n54 float 10.80000000000000071054  decimal 10.80000000000000000000\n56 float 11.20000000000000106581  decimal 11.20000000000000000000\n57 float 11.40000000000000035527  decimal 11.40000000000000000000\n58 float 11.60000000000000142109  decimal 11.60000000000000000000\n59 float 11.80000000000000071054  decimal 11.80000000000000000000\n61 float 12.20000000000000106581  decimal 12.20000000000000000000\n62 float 12.40000000000000035527  decimal 12.40000000000000000000\n63 float 12.60000000000000142109  decimal 12.60000000000000000000\n64 float 12.80000000000000071054  decimal 12.80000000000000000000\n66 float 13.20000000000000106581  decimal 13.20000000000000000000\n67 float 13.40000000000000035527  decimal 13.40000000000000000000\n68 float 13.60000000000000142109  decimal 13.60000000000000000000\n69 float 13.80000000000000071054  decimal 13.80000000000000000000\n71 float 14.20000000000000106581  decimal 14.20000000000000000000\n72 float 14.40000000000000035527  decimal 14.40000000000000000000\n73 float 14.60000000000000142109  decimal 14.60000000000000000000\n74 float 14.80000000000000071054  decimal 14.80000000000000000000\n76 float 15.20000000000000106581  decimal 15.20000000000000000000\n77 float 15.40000000000000035527  decimal 15.40000000000000000000\n78 float 15.60000000000000142109  decimal 15.60000000000000000000\n79 float 15.80000000000000071054  decimal 15.80000000000000000000\n81 float 16.19999999999999928946  decimal 16.20000000000000000000\n82 float 16.40000000000000213163  decimal 16.40000000000000000000\n83 float 16.60000000000000142109  decimal 16.60000000000000000000\n84 float 16.80000000000000071054  decimal 16.80000000000000000000\n86 float 17.19999999999999928946  decimal 17.20000000000000000000\n87 float 17.40000000000000213163  decimal 17.40000000000000000000\n88 float 17.60000000000000142109  decimal 17.60000000000000000000\n89 float 17.80000000000000071054  decimal 17.80000000000000000000\n91 float 18.19999999999999928946  decimal 18.20000000000000000000\n92 float 18.40000000000000213163  decimal 18.40000000000000000000\n93 float 18.60000000000000142109  decimal 18.60000000000000000000\n94 float 18.80000000000000071054  decimal 18.80000000000000000000\n96 float 19.20000000000000284217  decimal 19.20000000000000000000\n97 float 19.40000000000000213163  decimal 19.40000000000000000000\n98 float 19.60000000000000142109  decimal 19.60000000000000000000\n99 float 19.80000000000000071054  decimal 19.80000000000000000000\n```\n\nMy suggestion is to use the decimal module for uniform discretisation. \n\n\n### Steps/Code to Reproduce\n\n```python\n#!/usr/bin/env python\nimport decimal\nimport fractions\nimport sys\nfrom typing import NoReturn\n\nimport numpy\nimport sklearn.preprocessing\n\n\ndef test_sklearn_uniform_bug():\n    # sample values\n    values = numpy.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n    # expected quantised bin\n    # right side of bin   <.2  <.2  <.4  <.4  <.6  <.6  <.8  <.8  <1   <1   <1\n    expected =           [0,   0,   1,   1,   2,   2,   3,   3,   4,   4,   4]\n\n    # reshape to list of lists\n    reshaped = numpy.reshape(values, shape=(-1, 1))\n    qnt = sklearn.preprocessing.KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n    qnt.fit(reshaped)\n    fitted = qnt.transform(reshaped)\n    # reshape to list of bins\n    quantised = numpy.reshape(fitted, shape=(-1))\n\n    # check bin assignment\n    for i in range(len(expected)):\n        if quantised[i] != expected[i]:\n            print(f'bin for {values[i]} {quantised[i]} != {expected[i]}')\n\n    # check bin edges\n    expected_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n    bin_edges = qnt.bin_edges_[0]\n    for i in range(len(expected_bins)):\n        if expected_bins[i] != bin_edges[i]:\n            print(f'bin edge {expected_bins[i]} != {bin_edges[i]}')\n\n    # check floating point multiplication\n    step = 1/5\n    bin_3 = step * 3\n    if bin_3 != 0.6:\n        print(f'floating point multiplication {bin_3} != 0.6')\n    else:\n        print('floating point multiplication ok')\n\n    # check fractions multiplication\n    f_step = fractions.Fraction(1, 5)\n    f_bin_3 = 3 * f_step\n    if float(f_bin_3) != 0.6:\n        print(f'fractions multiplication {f_bin_3} != 0.6')\n    else:\n        print('fractions multiplication ok')\n\n    # check decimal multiplication\n    d_step = decimal.Decimal(1) / decimal.Decimal(5)\n    d_bin_3 = 3 * d_step\n    if float(d_bin_3) != 0.6:\n        print(f'decimal multiplication {d_bin_3} != 0.6')\n    else:\n        print('decimal multiplication ok')\n\n\ndef main(argv) -> NoReturn:\n    m = 0\n    try:\n        test_sklearn_uniform_bug()\n    except Exception as e:\n        print(f'Exception: {e}')\n    sys.exit(m)\n\n\nif __name__ == '__main__':\n    main(sys.argv[1:])\n```\n\n### Expected Results\n\n```shell\nfloating point multiplication ok\nfractions multiplication ok\ndecimal multiplication ok\n```\n\n### Actual Results\n\n```shell\nbin for 0.6 2.0 != 3\nbin edge 0.6 != 0.6000000000000001\nfloating point multiplication 0.6000000000000001 != 0.6\nfractions multiplication ok\ndecimal multiplication ok\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.8 (main, Dec 30 2024, 15:10:22) [Clang 16.0.0 (clang-1600.0.26.6)]\nexecutable: /Users/simonb/.pyenv/versions/nmc/bin/python\n   machine: macOS-15.3.1-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0\n   setuptools: 75.8.0\n        numpy: 2.2.2\n        scipy: 1.15.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 11\n         prefix: libomp\n       filepath: /Users/simonb/.pyenv/versions/3.12.8/envs/nmc/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib\n        version: None\n```",
    "comments": [
      {
        "user": "Rishab260",
        "body": "Hi, I tried the following. \n```py\n\nimport decimal\nimport fractions\nfrom typing import NoReturn\n\nimport numpy as np\nimport sklearn.preprocessing\n\nclass FixedKBinsDiscretizer(KBinsDiscretizer):\n    def __init__(self, n_bins=5, *, encode='onehot', strategy='uniform', precision=10):\n        super().__init__(n_bins=n_bins, encode=encode, strategy=strategy)\n        self.precision = precision  # it is precision for rounding bins edges\n\n    def fit(self, X, y=None):\n        super().fit(X, y)\n        self._round_bin_edges()\n        return self\n\n    def _round_bin_edges(self):\n        \"\"\"Rounds bin edges and ensures uniqueness to avoid precision errors.\"\"\"\n        self.bin_edges_ = [np.unique(np.round(edges, decimals=self.precision)) for edges in self.bin_edges_]\n\ndef test_sklearn_uniform_bug():\n    # sample values\n    values = numpy.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n    # expected quantised bin\n    # right side of bin   <.2  <.2  <.4  <.4  <.6  <.6  <.8  <.8  <1   <1   <1\n    expected =           [0,   0,   1,   1,   2,   2,   3,   3,   4,   4,   4]\n\n    # reshape to list of lists\n    reshaped = numpy.reshape(values, (-1, 1))\n    qnt = FixedKBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n    qnt.fit(reshaped)\n    fitted = qnt.transform(reshaped)\n    # reshape to list of bins\n    quantised = numpy.reshape(fitted, (-1))\n\n    # check bin assignment\n    for i in range(len(expected)):\n        if quantised[i] != expected[i]:\n            print(f'bin for {values[i]} {quantised[i]} != {expected[i]}')\n\n    # check bin edges\n    expected_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n    bin_edges = qnt.bin_edges_[0]\n    for i in range(len(expected_bins)):\n        if expected_bins[i] != bin_edges[i]:\n            print(f'bin edge {expected_bins[i]} != {bin_edges[i]}')\n\n    # check floating point multiplication\n    step = 1/5\n    bin_3 = step * 3\n    if not np.isclose(bin_3, 0.6, atol=1e-10):\n        print(f'floating point multiplication {bin_3} != 0.6')\n    else:\n        print('floating point multiplication ok')\n\n    # check fractions multiplication\n    f_step = fractions.Fraction(1, 5)\n    f_bin_3 = 3 * f_step\n    if float(f_bin_3) != 0.6:\n        print(f'fractions multiplication {f_bin_3} != 0.6')\n    else:\n        print('fractions multiplication ok')\n\n    # check decimal multiplication\n    d_step = decimal.Decimal(1) / decimal.Decimal(5)\n    d_bin_3 = 3 * d_step\n    if float(d_bin_3) != 0.6:\n        print(f'decimal multiplication {d_bin_3} != 0.6')\n    else:\n        print('decimal multiplication ok')\n\n\ndef main(argv) -> NoReturn:\n    try:\n        test_sklearn_uniform_bug()\n    except Exception as e:\n        print(f'Exception: {e}')\n\n\nif __name__ == '__main__':\n    main(sys.argv[1:])\n```\n\n### Output:\n```\nfloating point multiplication ok\nfractions multiplication ok\ndecimal multiplication ok\n```\n### Note:\n- I have replaced `bin_3 != 0.6` with `not np.isclose(bin_3, 0.6, atol=1e-10)`\n\n\n\n"
      },
      {
        "user": "ogrisel",
        "body": "Thanks for the report. Would any of you be interested in opening a PR with both a non-regression test and the proposed fix?"
      },
      {
        "user": "Rishab260",
        "body": "Hi @ogrisel, I am interested in working on this."
      }
    ]
  },
  {
    "issue_number": 31540,
    "title": "Make `sklearn.metrics._scorer._MultimetricScorer` part of the public API",
    "author": "MarcBresson",
    "state": "open",
    "created_at": "2025-06-13T09:18:28Z",
    "updated_at": "2025-06-15T16:22:01Z",
    "labels": [
      "Enhancement",
      "API",
      "Needs Decision"
    ],
    "body": "### Describe the workflow you want to enable\n\nThis tool is great to run multiple scorers on a single estimator thanks to the caching mechanism. It is a bummer that it is not part of the public API.\n\n### Describe your proposed solution\n\nMake it part of the public API\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "What exactly is your usecase @MarcBresson ? We don't necessarily have to make the class public. We can allow `make_scorer` to accept a dictionary / list of scorers and return the object instead, for a nicer API maybe. It can probably even simplify a few places inside scikit-learn itself."
      }
    ]
  },
  {
    "issue_number": 31344,
    "title": "Add MultiHorizonTimeSeriesSplit for Multi-Horizon Time Series Cross-Validation",
    "author": "andrelrodriguess",
    "state": "open",
    "created_at": "2025-05-09T14:36:35Z",
    "updated_at": "2025-06-15T15:57:33Z",
    "labels": [
      "New Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nThe current `TimeSeriesSplit` in scikit-learn supports cross-validation for time series data with a single prediction horizon per split, which limits its use for scenarios requiring forecasts over multiple future steps (e.g., predicting 1, 3, and 5 days ahead). I propose adding a new class, `MultiHorizonTimeSeriesSplit`, to enable cross-validation with multiple prediction horizons in a single split.\n\nThis would allow users to:\n- Specify a list of horizons (e.g., `[1, 3, 5]`) to generate train-test splits where the test set includes indices for multiple future steps.\n- Evaluate time series models for short, medium, and long-term forecasts simultaneously.\n- Simplify workflows for applications like demand forecasting, financial modeling, or weather prediction, avoiding manual splitting.\n\nExample usage with daily temperatures:\n```\nfrom sklearn.model_selection import MultiHorizonTimeSeriesSplit\nimport numpy as np\n\n# Daily temperatures for 10 days (in Â°C)\nX = np.array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\ncv = MultiHorizonTimeSeriesSplit(n_splits=2, horizons=[1, 2])\nfor train_idx, test_idx in cv.split(X):\n    print(f\"Train indices: {train_idx}, Test indices: {test_idx}\")\n```\nExpected output:\n```\nTrain indices: [0 1 2 3 4], Test indices: [5 6]\nTrain indices: [0 1 2 3 4 5 6], Test indices: [7 8]\n```\n\n### Describe your proposed solution\n\nI propose implementing a new class, `MultiHorizonTimeSeriesSplit`, inheriting from `TimeSeriesSplit`. The class will:\n- Add a `horizons` parameter (list of integers) to specify prediction steps.\n- Modify the `split` method to generate test indices for each horizon while preserving temporal order.\n- Include input validation to ensure valid horizons and splits.\n\nTo ensure the correctness of MultiHorizonTimeSeriesSplit, we will develop unit tests covering various configurations and edge cases. For benchmarking, we will assess the computational efficiency and correctness of the new class compared to manual splitting. We will use synthetic time series to evaluate scalability and measure split generation time and memory usage, running tests on a personal laptop.\n\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "jones-miguel",
        "body": "/take"
      },
      {
        "user": "betatim",
        "body": "@ogrisel @glemaitre you seem to know things about time series, what do you think about this?"
      },
      {
        "user": "glemaitre",
        "body": "With @ogrisel we are going to explore shortly a bit of time-series forecasting. I think that it will be a good opportunity to assess whether such CV splitter should be in the scope of scikit-learn or if it is preferable to contribute it to a specialized time-series third-party library.\n\nBasically, if we are already able to do many forecast case solely with scikit-learn, I think that having the tool to evaluate then is really useful."
      }
    ]
  },
  {
    "issue_number": 31373,
    "title": "SimpleImputer converts `int32[pyarrow]` extension array to `float64`, subsequently crashing with numpy `int32` values",
    "author": "I-Al-Istannen",
    "state": "open",
    "created_at": "2025-05-16T16:30:30Z",
    "updated_at": "2025-06-13T22:29:20Z",
    "labels": [
      "Bug",
      "module:impute"
    ],
    "body": "### Describe the bug\n\nWhen using the `SimpleImputer` with a pyarrow-backed pandas DataFrame, any float/integer data is converted to `None`/`float64` instead.\nThis causes the imputer to be fitted to `float64`, crashing on a dtype assertion when passing it a numpy-backed `int32` DataFrame after fitting.\n\nThe flow is the following:\n1. The imputer calls `_validate_input`:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/impute/_base.py#L319\n2. This calls `validate_data`:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/impute/_base.py#L344-L353\n3. This calls `check_array`:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L2951-L2952\n4. Our input is a pandas dataframe:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L909\n5. This now checks if the dtypes need to be converted:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L925-L927\n6. Our input is backed by an extension array _and_ `int32[pyarrow]` is an integer datatype, so we return `True` here:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L714-L724\n7. Finally we pass the \"needs conversion\" check and convert the dataframe to `dtype` (which is `None` here, which apparently means `float64`):\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L966-L971\n\n### Steps/Code to Reproduce\n\n```py\nimport polars as pl\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\nprint(\n    SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n      .fit(pl.DataFrame({\"a\": [10]}, schema={\"a\": pl.Int32}).to_pandas(use_pyarrow_extension_array=False))\n      ._fit_dtype\n)\n# prints dtype('int32'), as expected\n\nprint(\n    SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n      .fit(pl.DataFrame({\"a\": [10]}, schema={\"a\": pl.Int32}).to_pandas(use_pyarrow_extension_array=True))\n      ._fit_dtype\n)\n# prints dtype('float64') (!!)\n```\n\n\n### Expected Results\n\nBoth imputers should be fitted with `int32` values.\n\n### Actual Results\n\nThe imputer using the pyarrow extension array is fitted with `float64`.\n\nThis causes crashes when using the Imputer with normal `int32` columns backed by numpy, as they won't be converted and therefore the dtypes differ.\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.9 (main, Mar 11 2025, 17:26:57) [Clang 20.1.0 ]\nexecutable: /tmp/scikit/.venv/bin/python3\n   machine: Linux-6.14.4-arch1-2-x86_64-with-glibc2.41\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: None\n   setuptools: None\n        numpy: 2.2.5\n        scipy: 1.15.3\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: None\n       joblib: 1.5.0\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: /tmp/scikit/.venv/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-6bb31eeb.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: /tmp/scikit/.venv/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-68440149.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 16\n         prefix: libgomp\n       filepath: /tmp/scikit/.venv/lib/python3.12/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n        version: None\n```",
    "comments": [
      {
        "user": "gdacciaro",
        "body": "The core issue stems from the fact that `pandas.api.types.is_integer_dtype` does not recognize polars (or pyarrow-backed) integer dtypes as integer types. For example:\n\n```\nfrom pandas.api.types import is_integer_dtype\nimport polars as pl\nimport pandas as pd\n\nprint(is_integer_dtype(int))           # True\nprint(is_integer_dtype(pl.Int32))      # False <--- problematic\nprint(is_integer_dtype(pd.Int32Dtype())) # True\n\n```\nThis leads to unexpected conversion of pyarrow-backed integer columns to float64 inside check_array in scikit-learn, causing inconsistent dtype handling and eventual crashes when mixing pandas and polars dataframes or numpy arrays.\n\n**Proposal**\n\nI would like to work on making the `check_array` function dtype-checking logic more agnostic and robust, so that it correctly recognizes integer types from both pandas and polars (including pyarrow extension arrays). This would avoid unwanted dtype conversions and improve compatibility.\n\nHappy to discuss the best approach and how to write tests for this.\n\n/take\n"
      },
      {
        "user": "I-Al-Istannen",
        "body": "I am not passing in a polars type, I am passing in a pandas dataframe backed by pyarrow. The `is_integer_type` check actually _passes_, as outlined in my issue â€” if it were to return `False`, it wouldn't try to convert it :)\n\n```py\n>>> typ = pl.DataFrame({\"a\": [10]}, schema={\"a\": pl.Int32}).to_pandas(use_pyarrow_extension_array=True).dtypes[\"a\"]\n>>> is_integer_dtype(typ)\nTrue\n```\n"
      },
      {
        "user": "Astroficboy",
        "body": "The issue could be the first check in `def _validate_input(self, X, in_fit):`   #\n\n![Image](https://github.com/user-attachments/assets/155ad464-8cd4-48f3-adf6-295888c56cfa)\n\nIt is checking for data types and assigning a default from `FLOAT_DTYPES = (np.float64, np.float32, np.float16)`\n\n@gdacciaro will adding support for pyarrow dtypes in the if-else block work?\n\n```\nfrom pandas.api.types import is_extension_array_dtype, is_string_dtype, is_numeric_dtype\n\nif self.strategy in (\"most_frequent\", \"constant\"):\n        if isinstance(X, list) and any(\n            isinstance(elem, str) for row in X for elem in row\n        ):\n            dtype = object\n        else:\n            dtype = None\n    else:\n        # Allow Arrow-backed numeric dtypes\n        if hasattr(X, \"dtypes\") and all(\n            is_numeric_dtype(dt) or is_extension_array_dtype(dt)\n            for dt in X.dtypes\n        ):\n            dtype = None  # Let sklearn decide; don't enforce float64\n        else:\n            dtype = FLOAT_DTYPES\n\n    return dtype\n```"
      }
    ]
  },
  {
    "issue_number": 31536,
    "title": "Improve sample_weight handling in sag(a)",
    "author": "snath-xoc",
    "state": "open",
    "created_at": "2025-06-12T16:04:19Z",
    "updated_at": "2025-06-13T15:30:22Z",
    "labels": [
      "Enhancement"
    ],
    "body": "### Describe the bug\n\nThis may be more of a discussion, but overall I am not sure what treatment of weighting would preserve the convergence guarantees for the SAG(A) solver. So far as I see it, at each update step we uniformly select some index $i_j$ such that the update steps can be generalised as:\n\n$x^{k+1} = x^{k} - \\sum_{j=1}^{k} \\alpha_{j} S(j, i_{1:k}) f'_{i_j}(x^j)$\n\nWhere $S(j, i_{1:k}) = 1/n$ if $j$ is the maximum iteration at which $i_j$ is selected. \n\nFor frequency based weighting, one could sample $i_j$ using weights as a probability, and under non-uniform sampling the SAG(A) convergence guarantees still seem to hold, (see [here]([https://inria.hal.science/hal-00860051/document])).\n\n Alternatively as currently done, the weights could be multiplied through with the gradient update and that could also work, however I am not sure which method is best (we also here need to additionally consider the division by the cardinality of the set of \"seen\" elements within each update step).\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom scipy.stats import kstest\nfrom sklearn.linear_model.tests.test_sag import sag, squared_dloss\nfrom sklearn.datasets import make_regression\nfrom sklearn.utils._testing import assert_allclose_dense_sparse\n\nstep_size=0.01\nalpha=1\n\nn_features = 1\n\nrng = np.random.RandomState(0)\nX, y = make_regression(n_samples=10000,random_state=77,n_features=n_features)\nweights = rng.randint(0,5,size=X.shape[0])\n\nX_repeated = np.repeat(X,weights,axis=0)\ny_repeated = np.repeat(y,weights,axis=0)\n\nweights_w_all = np.zeros([n_features,100])\nweights_r_all = np.zeros([n_features,100])\n\nfor random_state in np.arange(100):\n\n    weights_w, int_w = sag(X,y,step_size=step_size,alpha=alpha,sample_weight=weights,dloss=squared_dloss,random_state=random_state)\n    weights_w_all[:,random_state] = weights_w\n    weights_r, int_r = sag(X_repeated,y_repeated,step_size=step_size,alpha=alpha,dloss=squared_dloss,random_state=random_state)\n    weights_r_all[:,random_state] = weights_r\n\nprint(kstest(weights_r_all[0],weights_w_all[0]))\n```\n\nnote that I modified ```sag``` in test_sag.py to accept random_state:\n\n```python\ndef sag(\n    X,\n    y,\n    step_size,\n    alpha,\n    n_iter=1,\n    dloss=None,\n    sparse=False,\n    sample_weight=None,\n    fit_intercept=True,\n    saga=False,\n    random_state=77,\n):\n    n_samples, n_features = X.shape[0], X.shape[1]\n\n    weights = np.zeros(X.shape[1])\n    sum_gradient = np.zeros(X.shape[1])\n    gradient_memory = np.zeros((n_samples, n_features))\n\n    intercept = 0.0\n    intercept_sum_gradient = 0.0\n    intercept_gradient_memory = np.zeros(n_samples)\n\n    rng = np.random.RandomState(random_state)\n    decay = 1.0\n    seen = set()\n\n    # sparse data has a fixed decay of .01\n    if sparse:\n        decay = 0.01\n\n    for epoch in range(n_iter):\n        for k in range(n_samples):\n            #if sample_weight is not None:\n            #    idx = rng.choice(np.arange(n_samples),p=sample_weight/np.sum(sample_weight))\n            #else:\n            idx = int(rng.rand()*n_samples)\n            # idx = k\n            entry = X[idx]\n            seen.add(idx)\n            p = np.dot(entry, weights) + intercept\n            gradient = dloss(p, y[idx])\n            if sample_weight is not None:\n                gradient *= sample_weight[idx]\n            update = entry * gradient + alpha * weights\n            gradient_correction = update - gradient_memory[idx]\n            sum_gradient += gradient_correction\n            gradient_memory[idx] = update\n            if saga:\n                weights -= gradient_correction * step_size * (1 - 1.0 / len(seen))\n\n            if fit_intercept:\n                gradient_correction = gradient - intercept_gradient_memory[idx]\n                intercept_gradient_memory[idx] = gradient\n                intercept_sum_gradient += gradient_correction\n                gradient_correction *= step_size * (1.0 - 1.0 / len(seen))\n                if saga:\n                    intercept -= (\n                        step_size * intercept_sum_gradient / len(seen) * decay\n                    ) + gradient_correction\n                else:\n                    intercept -= step_size * intercept_sum_gradient / len(seen) * decay\n\n            weights -= step_size * sum_gradient / len(seen)\n\n    return weights, intercept\n```\n\n### Expected Results\n\nkstest should have p-value larger than 0.025\n\n### Actual Results\n\n```\nKstestResult(statistic=np.float64(0.44), pvalue=np.float64(4.414205948474835e-09), statistic_location=np.float64(24.644506472064027), statistic_sign=np.int8(-1))\n```\n\nWith an example histogram of:\n\n![Image](https://github.com/user-attachments/assets/a3081d21-4ac9-491a-a437-e4f916b862b9)\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.4 | packaged by conda-forge | (main, Jun 17 2024, 10:13:44) [Clang 16.0.6 ]\nexecutable: /Users/shrutinath/micromamba/envs/scikit-learn/bin/python\n   machine: macOS-14.3-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.8.dev0\n          pip: 24.0\n   setuptools: 75.8.0\n        numpy: 2.0.0\n        scipy: 1.14.0\n       Cython: 3.0.10\n       pandas: 2.2.2\n   matplotlib: 3.9.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n...\n    num_threads: 8\n         prefix: libomp\n       filepath: /Users/shrutinath/micromamba/envs/scikit-learn/lib/libomp.dylib\n        version: None\nOutput is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n```",
    "comments": [
      {
        "user": "snath-xoc",
        "body": "@ogrisel @antoinebaker @ngazagna @josephsalmon "
      }
    ]
  },
  {
    "issue_number": 31503,
    "title": "HDBSCAN performance issues compared to original hdbscan implementation (likely because Boruvka algorithm is not implemented)",
    "author": "lkwinta",
    "state": "open",
    "created_at": "2025-06-08T14:53:52Z",
    "updated_at": "2025-06-13T12:37:39Z",
    "labels": [
      "New Feature",
      "help wanted",
      "Hard"
    ],
    "body": "### Describe the bug\n\nWhen switching from Sklearn HDBSCAN implementation to original one from `hdbscan` library, I've notice that Sklearn's implementation has much worse implementation. I've tried investigating different parameters but it doesn't seem to have an effect on the performance.\n\nI've created synthetic benchmark using `make_blobs` function.  And those are my results:\n\nCPU: Ryzen 5 1600, 12 Threads@3.6Ghz*\nRAM: 32GB DDR4\n\n```python\n# dataset\nX, y = make_blobs(n_samples=10000, centers=5, cluster_std=0.60, random_state=0, n_features=10)\n\n# hdbscan params \nog_hdbscan = OGHDBSCAN(core_dist_n_jobs=-1)\nsk_hdbscan = SKHDBSCAN(n_jobs=-1)\n```\n\n![Image](https://github.com/user-attachments/assets/42bc818c-8547-4297-9020-e87a02b7bd90)\n\n* Tested out on Google Collab with similar results\n\n### Steps/Code to Reproduce\n\nI am starting both algorithms with `n_jobs=-1` to rule out the difference that may occure because of default setting of `core_dist_n_jobs=4` in `hdbscan`\n\n```python\nfrom hdbscan import HDBSCAN as OGHDBSCAN\nfrom sklearn.cluster import HDBSCAN as SKHDBSCAN\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=10000, centers=5, cluster_std=0.60, random_state=0, n_features=10)\n\nog_hdbscan = OGHDBSCAN(core_dist_n_jobs=-1)\nsk_hdbscan = SKHDBSCAN(n_jobs=-1)\n\nRUNS = 10\n\ndef time_hdbscan(hdbscan, X, runs):\n    times = []\n    for _ in range(runs):\n        start = time.time()\n        hdbscan.fit(X)\n        end = time.time()\n        times.append(end - start)\n    return times\n\ntimes_og = time_hdbscan(og_hdbscan, X, RUNS)\ntimes_sk = time_hdbscan(sk_hdbscan, X, RUNS)\n\nprint(\"Mean time OGHDBSCAN: \", np.mean(times_og))\nprint(\"Mean time SKHDBSCAN: \", np.mean(times_sk))\n\nplt.plot(range(RUNS), times_og, label='OGHDBSCAN', marker='o')\nplt.plot(range(RUNS), times_sk, label='SKHDBSCAN', marker='x')\nplt.xlabel('Run')\nplt.ylabel('Time (seconds)')\nplt.title('HDBSCAN Timing Comparison')\nplt.legend()\nplt.show()\n```\n\n### Expected Results\n\nSimilar performance between algorithms from Sklearn and `hdbscan` library\n\n### Actual Results\n\nSklearn implementation of `HDBSCAN` gets much worse performance than original library. For example when testing much bigger dataset, i.e. \n```python\nX, y = make_blobs(n_samples=100000, centers=5, cluster_std=0.60, random_state=0, n_features=20)\n```\n\n`hdbscan` library performs `fit` in 25s on my hardware, while Sklearn needs 5 minutes to perform clustering.\n\n### Versions\n\n```shell\nSystem:\n    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\nexecutable: d:\\Documents\\Projects\\Machine-Learning-Basics\\.venv\\Scripts\\python.exe\n   machine: Windows-10-10.0.19045-SP0\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: None\n   setuptools: 80.9.0\n        numpy: 1.26.4\n        scipy: 1.15.3\n       Cython: None\n       pandas: 2.3.0\n   matplotlib: 3.10.3\n       joblib: 1.5.1\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 12\n         prefix: libopenblas\n       filepath: D:\\Documents\\Projects\\Machine-Learning-Basics\\.venv\\Lib\\site-packages\\numpy.libs\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll\n        version: 0.3.23.dev\nthreading_layer: pthreads\n   architecture: Zen\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 12\n         prefix: vcomp\n       filepath: D:\\Documents\\Projects\\Machine-Learning-Basics\\.venv\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\n        version: None\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 12\n         prefix: libscipy_openblas\n       filepath: D:\\Documents\\Projects\\Machine-Learning-Basics\\.venv\\Lib\\site-packages\\scipy.libs\\libscipy_openblas-f07f5a5d207a3a47104dca54d6d0c86a.dll\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n```",
    "comments": [
      {
        "user": "lesteve",
        "body": "I can reproduce a similar behaviour on my machine.\n\nOne of the reason is that `hdbscan.HDBSCAN` uses Boruvka algorithm by default which is not implemented in `sklearn.HDBSCAN`. There was some work some time ago to add Boruvka algorithm in https://github.com/scikit-learn/scikit-learn/pull/27572. See also https://github.com/scikit-learn/scikit-learn/issues/26801 for more context.\n\nThis would definitely be a significant effort to revive the Boruvka PR and push it forward."
      },
      {
        "user": "snath-xoc",
        "body": "I could help here if you like, would be curious to investigate the Brouvka algorithm implementation"
      }
    ]
  },
  {
    "issue_number": 31450,
    "title": "Spherical K-means support (unit norm centroids and input)",
    "author": "Radu1999",
    "state": "open",
    "created_at": "2025-05-28T20:47:24Z",
    "updated_at": "2025-06-13T11:59:45Z",
    "labels": [
      "New Feature",
      "Needs Decision - Include Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nHi,\nI was wondering if there isâ€”or has beenâ€”any initiative to support cosine similarity in the KMeans implementation (i.e., spherical KMeans). I find the algorithm quite useful and would be happy to propose an implementation. The addition should be relatively straightforward.\n\n### Describe your proposed solution\n\nEnable the use of cosine similarity with KMeans or implement a separate SphericalKMeans class.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "Namit24",
        "body": "Hey I'd like to take up this @Radu1999 "
      },
      {
        "user": "Radu1999",
        "body": "> Hey I'd like to take up this [@Radu1999](https://github.com/Radu1999)\n\nNo, it's ok, I was planning to implement it once I confirm there is interest for it."
      },
      {
        "user": "Namit24",
        "body": "Fairs go for it then"
      }
    ]
  },
  {
    "issue_number": 31350,
    "title": "SimpleImputer casts `category` into `object` when using \"most_frequent\" strategy",
    "author": "jschubnell",
    "state": "open",
    "created_at": "2025-05-11T03:55:11Z",
    "updated_at": "2025-06-13T11:13:12Z",
    "labels": [
      "Bug",
      "API",
      "Needs Decision",
      "module:impute"
    ],
    "body": "### Describe the bug\n\nThe column `dtype` changes from `category` to `object` when I transform it using `SimpleImputer`.\n\nHere is a list of related Issues and PRs that I found while trying to solve this problem:\n#29381 \n#18860\n#17625 \n#17526\n#17525\n\nIf this is truly a bug, I would like to work on a fix.\n\n### Steps/Code to Reproduce\n\n```python\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\n\ndf = pd.DataFrame(data=['A', 'B', 'C', 'A', pd.NA], columns=['column_1'], dtype='category')\n\ndf.info()\n\nimputer = SimpleImputer(missing_values=pd.NA, strategy=\"most_frequent\").set_output(transform='pandas')\n\noutput = imputer.fit_transform(df)\n\noutput.info()\n```\n\n### Expected Results\n\nThis is the output I expected to see on the terminal\n```\n> > > df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5 entries, 0 to 4\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   column_1  4 non-null      category\ndtypes: category(1)\nmemory usage: 269.0 bytes\n\n>>> output.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5 entries, 0 to 4\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   column_1  5 non-null      category\ndtypes: object(1)\nmemory usage: 172.0+ bytes\n```\n\nI expected `output` to keep the same `dtype` as the original `pd.DataFrame`.\n\n### Actual Results\n\nThe actual results for when `output.info()` is called is:\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5 entries, 0 to 4\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   column_1  5 non-null      object\ndtypes: object(1)\nmemory usage: 172.0+ bytes\n```\nObserve that the `Dtype` for `column_1` is now object instead of category.\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:46:43) [GCC 11.2.0]\nexecutable: /home/user/miniconda3/envs/prod/bin/python\n   machine: Linux-6.8.0-59-lowlatency-x86_64-with-glibc2.39\n\nPython dependencies:\n      sklearn: 1.5.2\n          pip: 25.0\n   setuptools: 75.8.0\n        numpy: 2.1.1\n        scipy: 1.14.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.9.2\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: /home/user/miniconda3/envs/prod/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-ff651d7f.so\n        version: 0.3.27\nthreading_layer: pthreads\n   architecture: SkylakeX\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: /home/user/miniconda3/envs/prod/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-c128ec02.so\n        version: 0.3.27.dev\nthreading_layer: pthreads\n   architecture: SkylakeX\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 16\n         prefix: libgomp\n       filepath: /home/user/miniconda3/envs/prod/lib/python3.12/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n        version: None\n```",
    "comments": [
      {
        "user": "rohnsha0",
        "body": "/take"
      },
      {
        "user": "jschubnell",
        "body": "@rohnsha0 can I work on this?"
      },
      {
        "user": "rohnsha0",
        "body": "Sorry @jschubnell I've implemented it already locally, will push soon! "
      }
    ]
  },
  {
    "issue_number": 31538,
    "title": "å½“selector = VarianceThreshold(threshold=0.1)å’Œselector = VarianceThreshold()è¾“å‡ºçš„ç»“æžœä¸ä¸€æ ·",
    "author": "xuazsa",
    "state": "closed",
    "created_at": "2025-06-13T00:58:02Z",
    "updated_at": "2025-06-13T10:28:25Z",
    "labels": [
      "Bug",
      "Needs Triage"
    ],
    "body": "### Describe the bug\n\nimport numpy as np\nX = np.arange(30,dtype=float).reshape((10, 3))\nX[:,1] = 1\nfrom sklearn.feature_selection import VarianceThreshold\nvt = VarianceThreshold(threshold=0.01)\nxt = vt.fit_transform(X)\n# æœªè®¾ç½®é˜ˆå€¼æ—¶ï¼Œå¯èƒ½æœªå®žé™…è®¡ç®—æ–¹å·®\nvt1 = VarianceThreshold()\nvt1.fit(X)                # å…ˆè°ƒç”¨fitæ–¹æ³•\nprint(vt1.variances_)     # çŽ°åœ¨å¯ä»¥å®‰å…¨è®¿é—®\n\n# è®¾ç½®é˜ˆå€¼åŽå¼ºåˆ¶è®¡ç®—\nvt2 = VarianceThreshold(threshold=0.01)\nvt2.fit(X)  # å®žé™…æ‰§è¡Œè®¡ç®—\nprint(vt2.variances_)     # è¾“å‡ºæ­£ç¡®å€¼\nvt = VarianceThreshold(threshold=0.01)\nvt.fit(X)  # ç¡®ä¿å®žé™…è®¡ç®—\nprint(vt.variances_)\n# æ£€æŸ¥æ–¹å·®è®¡ç®—ä¸€è‡´æ€§\nmanual_var = np.var(X, axis=0, ddof=0)\nsklearn_var = vt.variances_\nif not np.allclose(manual_var, sklearn_var):\n    print(f\"è­¦å‘Šï¼šæ–¹å·®è®¡ç®—ä¸ä¸€è‡´ï¼æ‰‹åŠ¨:{manual_var}ï¼Œsklearn:{sklearn_var}\")\n    # ç¡®ä¿ä½¿ç”¨æœ€æ–°ç¨³å®šç‰ˆ\nimport sklearn\nprint(\"scikit-learnç‰ˆæœ¬:\", sklearn.__version__)  # åº” â‰¥ 1.0\n\n[27.  0. 27.]\n[74.25  0.   74.25]\n[74.25  0.   74.25]\nscikit-learnç‰ˆæœ¬: 1.7.0\n\n### Steps/Code to Reproduce\n\n[27.  0. 27.]\n[74.25  0.   74.25]\n[74.25  0.   74.25]\nscikit-learnç‰ˆæœ¬: 1.7.0\n\n### Expected Results\n\n[27.  0. 27.]\n[74.25  0.   74.25]\n[74.25  0.   74.25]\nscikit-learnç‰ˆæœ¬: 1.7.0\n\n### Actual Results\n\n[27.  0. 27.]\n[74.25  0.   74.25]\n[74.25  0.   74.25]\nscikit-learnç‰ˆæœ¬: 1.7.0\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.1 (tags/v3.12.1:2305ca5, Dec  7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)]\nexecutable: c:\\Users\\wp\\Desktop\\python312\\python.exe\n   machine: Windows-10-10.0.19045-SP0\n\nPython dependencies:\n      sklearn: 1.7.0\n          pip: 25.1.1\n   setuptools: 78.1.0\n        numpy: 1.26.0\n        scipy: 1.13.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.1\n       joblib: 1.4.2\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 4\n         prefix: libopenblas\n...\n       filepath: C:\\Users\\wp\\Desktop\\python312\\Lib\\site-packages\\scipy.libs\\libopenblas_v0.3.27--3aa239bc726cfb0bd8e5330d8d4c15c6.dll\n        version: 0.3.27\nthreading_layer: pthreads\n   architecture: Haswell\n```",
    "comments": [
      {
        "user": "lesteve",
        "body": "Our working language is English. I am going to close this one, but feel free to reopen when you have translated your comment into English."
      },
      {
        "user": "xuazsa",
        "body": ">>>import numpy as np\n>>>import sklearn\n>>>printï¼ˆâ€œscikit-learn versionï¼šâ€ï¼Œ sklearn.versionï¼‰ # \n\n>>>X = np.arangeï¼ˆ30ï¼Œdtype=floatï¼‰.reshapeï¼ˆï¼ˆ10ï¼Œ 3ï¼‰ï¼‰\n>>>X[ï¼šï¼Œ1] = 1\n>>>from sklearn.feature_selection import VarianceThreshold\n>>>vt = VarianceThresholdï¼ˆthreshold=0.01ï¼‰\n\n>>>xt = vt.fit_transformï¼ˆXï¼‰\n>>>vt1 = VarianceThresholdï¼ˆï¼‰\n>>>vt1.fitï¼ˆXï¼‰\n>>>printï¼ˆvt1.variances_ï¼‰ \n\n>>>vt2 = VarianceThresholdï¼ˆthreshold=0.01ï¼‰\n>>>vt2.fitï¼ˆXï¼‰\n>>>printï¼ˆvt2.variances_ï¼‰\n>>>vt = VarianceThresholdï¼ˆthreshold=0.01ï¼‰\n>>>vt.fitï¼ˆXï¼‰\n>>>printï¼ˆvt.variances_ï¼‰\n\n\nscikit-learn versionï¼š 1.7.0\n[27. 0.27.]\n[74.25 0. 74.25]\n[74.25 0. 74.25]\n\nwhy  vt1 = VarianceThresholdï¼ˆï¼‰and  vt2 = VarianceThresholdï¼ˆthreshold=0.01ï¼‰output  different"
      },
      {
        "user": "glemaitre",
        "body": "> why vt1 = VarianceThresholdï¼ˆï¼‰and vt2 = VarianceThresholdï¼ˆthreshold=0.01ï¼‰output different\n\nBecause by default `threshold=0.0` so the behaviour is expected to be different."
      }
    ]
  },
  {
    "issue_number": 28726,
    "title": "Is there any way to see alphas/coefs/intercept associated with *all* scenarios tested within ElasticNetCV",
    "author": "cppt",
    "state": "open",
    "created_at": "2024-03-29T19:20:19Z",
    "updated_at": "2025-06-13T10:15:46Z",
    "labels": [
      "New Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nI like that ElasticNetCV outputs the MSE path for CV folds/alphas but is there any way to similarly track associated model params (ie, coef/intercept) for each scenario and include them as part of output.\r\n\r\nI get that it's easier to just output 'best' estimators/params but would be useful to add granularity to allow identifying a 'sweet spot', either via MSE curve or something else, which would make outputting all params additive.   \r\n\r\n\n\n### Describe your proposed solution\n\nAs described, run existing scenarios as is but instead of holding only through evaluation of 'best' model, save all model params/outputs and return in an additional data object/structure.   \n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "glemaitre",
        "body": "Maybe you want to have a look at the path function: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.enet_path.html#sklearn.linear_model.enet_path"
      },
      {
        "user": "hammad7",
        "body": "@cppt , here is the code:\r\n\r\n```\r\nimport numpy as np\r\nfrom sklearn.linear_model import enet_path\r\n\r\n# Create some sample data\r\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\r\ny = np.dot(X, np.array([1, 2])) + 3\r\n\r\n# Compute the regularization path\r\nalphas, coefs, intercepts = enet_path(X, y, verbose = 5)\r\n# Print the alphas, coefficients, and intercepts for all scenarios tested\r\nfor alpha, coef, intercept in zip(alphas, coefs, intercepts):\r\n    print(\"Alpha:\", alpha)\r\n    print(\"Coefficients:\", coef)\r\n    print(\"Intercept:\", intercept)\r\n    print()\r\n```\r\n\r\nOutput:\r\n(array([0., 0.]), 0.0, 0.0302, 1)\r\n(array([0.        , 0.05715559]), 0.0, 0.0302, 2)\r\n(array([0.        , 0.11667845]), 5.684341886080802e-14, 0.0302, 2)\r\n(array([0.        , 0.17856487]), 0.0, 0.0302, 2)\r\n(array([0.        , 0.24279889]), -5.684341886080802e-14, 0.0302, 2)\r\n(array([0.        , 0.30935145]), 0.0, 0.0302, 2)\r\n(array([0.01901234, 0.37443649]), 1.3194308507991082e-06, 0.0302, 4)\r\n(array([0.06464451, 0.43584286]), 5.815761113581175e-06, 0.0302, 4)\r\n(array([0.11104886, 0.49826743]), 1.3875526946094396e-05, 0.0302, 4)\r\n(array([0.15812687, 0.56158046]), 1.6494158217028598e-06, 0.0302, 5)\r\n(array([0.2057831 , 0.62564193]), 3.247510619530658e-06, 0.0302, 5)\r\n(array([0.25390901, 0.69030648]), 5.998067450718736e-06, 0.0302, 5)\r\n(array([0.30239422, 0.75542238]), 1.0576523891359102e-05, 0.0302, 5)\r\n(array([0.35112521, 0.82083339]), 1.797996975483329e-05, 0.0302, 5)\r\n(array([0.39998632, 0.88638024]), 2.9643879514651417e-05, 0.0302, 5)\r\n(array([0.44886095, 0.95190213]), 4.75841175102687e-05, 0.0302, 5)\r\n(array([0.4976327 , 1.01723829]), 7.456191761434638e-05, 0.0302, 5)\r\n(array([0.54618652, 1.08222949]), 0.00011426530710423322, 0.0302, 5)\r\n(array([0.59440985, 1.14671955]), 0.00017149506439295692, 0.0302, 5)\r\n(array([0.64219371, 1.21055678]), 0.00025233719833295254, 0.0302, 5)\r\n(array([0.68943375, 1.27359533]), 0.0003642978385300921, 0.0302, 5)\r\n(array([0.73600503, 1.33570614]), 9.317236222727843e-05, 0.0302, 6)\r\n(array([0.78185772, 1.39674312]), 0.0001400085866976042, 0.0302, 6)\r\n(array([0.82688711, 1.45659175]), 0.00020630181253977753, 0.0302, 6)\r\n(array([0.87101494, 1.51514063]), 0.0002981673163162668, 0.0302, 6)\r\n(array([0.91417102, 1.5722888 ]), 0.00042285210944470464, 0.0302, 6)\r\n(array([0.95629359, 1.6279462 ]), 0.0005886332188183019, 0.0302, 6)\r\n(array([0.99732949, 1.68203387]), 0.0008046054945296532, 0.0302, 6)\r\n(array([1.03718305, 1.73450754]), 0.0003165159688549579, 0.0302, 7)\r\n(array([1.07590454, 1.78527184]), 0.0004431231468089436, 0.0302, 7)\r\n(array([1.11342785, 1.83429845]), 0.0006085892242211344, 0.0302, 7)\r\n(array([1.14973329, 1.88155252]), 0.0008200316172946032, 0.0302, 7)\r\n(array([1.18480902, 1.92700936]), 0.0010844981319380054, 0.0302, 7)\r\n(array([1.21865063, 1.97065383]), 0.001408358238293772, 0.0302, 7)\r\n(array([1.25117521, 2.01252455]), 0.0007281750637666562, 0.0302, 8)\r\n(array([1.28254042, 2.05254682]), 0.0009553156995636414, 0.0302, 8)\r\n(array([1.31269417, 2.09076491]), 0.0012308004081234003, 0.0302, 8)\r\n(array([1.34165629, 2.12719574]), 0.001556845843310839, 0.0302, 8)\r\n(array([1.36945136, 2.16186239]), 0.0019345101170102907, 0.0302, 8)\r\n(array([1.39599369, 2.19485863]), 0.0011828625463508047, 0.0302, 9)\r\n(array([1.42152022, 2.22610224]), 0.0014705246886403955, 0.0302, 9)\r\n(array([1.44597336, 2.25568279]), 0.0017984996816977628, 0.0302, 9)\r\n(array([1.46939059, 2.28364133]), 0.002161982013863195, 0.0302, 9)\r\n(array([1.49181143, 2.31002169]), 0.0025561466318571036, 0.0302, 9)\r\n(array([1.51313001, 2.33495931]), 0.001755255011964607, 0.0302, 10)\r\n(array([1.53365873, 2.35833888]), 0.002060662494283605, 0.0302, 10)\r\n(array([1.55331617, 2.38028421]), 0.0023873322183831647, 0.0302, 10)\r\n(array([1.57214462, 2.40084582]), 0.0027231035870052267, 0.0302, 10)\r\n(array([1.59018678, 2.42007443]), 0.0030601661872324826, 0.0302, 10)\r\n(array([1.60731468, 2.43812958]), 0.002267589904306533, 0.0302, 11)\r\n(array([1.62389131, 2.45485819]), 0.0025147333974242514, 0.0302, 11)\r\n(array([1.6398085 , 2.47040489]), 0.0027633870586996068, 0.0302, 11)\r\n(array([1.65510415, 2.48482075]), 0.0029952686152476815, 0.0302, 11)\r\n(array([1.66981648, 2.49815509]), 0.0032039541598951615, 0.0302, 11)\r\n(array([1.68398224, 2.51045643]), 0.003383846876634067, 0.0302, 11)\r\n(array([1.69763653, 2.52177235]), 0.003530214226813655, 0.0302, 11)\r\n(array([1.7108127 , 2.53214943]), 0.003639320592169426, 0.0302, 11)\r\n(array([1.72354233, 2.54163315]), 0.003708501889304827, 0.0302, 11)\r\n(array([1.73585512, 2.55026788]), 0.0037361839696927746, 0.0302, 11)\r\n(array([1.74777888, 2.5580968 ]), 0.003721852482371446, 0.0302, 11)\r\n(array([1.75933949, 2.56516191]), 0.0036659837869166267, 0.0302, 11)\r\n(array([1.77056088, 2.57150404]), 0.00356994707446745, 0.0302, 11)\r\n(array([1.78146505, 2.57716285]), 0.0034358874668125594, 0.0302, 11)\r\n(array([1.79227591, 2.58203659]), 0.003982404951348606, 0.0302, 10)\r\n(array([1.80262024, 2.58643133]), 0.003807589819487589, 0.0302, 10)\r\n(array([1.81289059, 2.59012413]), 0.004213254973232772, 0.0302, 9)\r\n(array([1.82271506, 2.59341721]), 0.003930192435560365, 0.0302, 9)\r\n(array([1.83247549, 2.59608888]), 0.004142907300813903, 0.0302, 8)\r\n(array([1.84181918, 2.59842501]), 0.003749310136090145, 0.0302, 8)\r\n(array([1.85130873, 2.60007179]), 0.004356432275689048, 0.0302, 6)\r\n(array([1.86025481, 2.60154708]), 0.0039903360093003215, 0.0302, 6)\r\n(array([1.86928474, 2.60244646]), 0.0043643070791796745, 0.0302, 4)\r\n(array([1.87801167, 2.60307353]), 0.004425443659130224, 0.0302, 3)\r\n(array([1.88645693, 2.60344448]), 0.004249947511216767, 0.0302, 2)\r\n(array([1.89441145, 2.60373623]), 0.0033649108293953844, 0.0302, 2)\r\n(array([1.90196687, 2.60391233]), 0.0020436509041052986, 0.0302, 2)\r\n(array([1.90918874, 2.60395269]), 0.0004711652838800262, 0.0302, 2)\r\n(array([1.91612453, 2.6038482 ]), 0.00167596941914816, 0.0302, 2)\r\n(array([1.92280901, 2.6035971 ]), 0.004071920576677712, 0.0302, 2)\r\n(array([1.92950366, 2.60303499]), 0.005798944142170193, 0.0302, 3)\r\n(array([1.9365236 , 2.60195723]), 0.005998496484806548, 0.0302, 5)\r\n(array([1.94326137, 2.60081402]), 0.006433019368176307, 0.0302, 5)\r\n(array([1.95001382, 2.59941164]), 0.006279225706987468, 0.0302, 6)\r\n(array([1.95646083, 2.59799461]), 0.006416734108870781, 0.0302, 6)\r\n(array([1.96290002, 2.59636619]), 0.006047004658215727, 0.0302, 7)\r\n(array([1.96901443, 2.59476707]), 0.006006448644048312, 0.0302, 7)\r\n(array([1.97487238, 2.59316222]), 0.006104199110143593, 0.0302, 7)\r\n(array([1.98050722, 2.59154059]), 0.006250992622957696, 0.0302, 7)\r\n(array([1.98593567, 2.58990216]), 0.006404871518275179, 0.0302, 7)\r\n(array([1.99116647, 2.58825181]), 0.006546459726492593, 0.0302, 7)\r\n(array([1.9962046 , 2.58659636]), 0.006667379095167458, 0.0302, 7)\r\n(array([2.00105333, 2.58494317]), 0.006764688637442617, 0.0302, 7)\r\n(array([2.00571525, 2.58329945]), 0.006838155449508498, 0.0302, 7)\r\n(array([2.01019279, 2.58167192]), 0.006888886693848573, 0.0302, 7)\r\n(array([2.01448848, 2.58006668]), 0.006918629284542455, 0.0302, 7)\r\n(array([2.01860507, 2.5784891 ]), 0.006929403744237117, 0.0302, 7)\r\n(array([2.02254557, 2.57694391]), 0.006923308817433416, 0.0302, 7)\r\n(array([2.0260774 , 2.57560473]), 0.007546682213824152, 0.0302, 6)\r\n(array([2.02977259, 2.57406636]), 0.007255271415334885, 0.0302, 7)\r\n(array([2.03303197, 2.57276514]), 0.007721195772496525, 0.0302, 6)\r\n[1, 2, 2, 2, 2, 2, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 9, 9, 8, 8, 6, 6, 4, 3, 2, 2, 2, 2, 2, 2, 3, 5, 5, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6]\r\nAlpha: 36.5\r\nCoefficients: [0.         0.         0.         0.         0.         0.\r\n 0.01901234 0.06464451 0.11104886 0.15812687 0.2057831  0.25390901\r\n 0.30239422 0.35112521 0.39998632 0.44886095 0.4976327  0.54618652\r\n 0.59440985 0.64219371 0.68943375 0.73600503 0.78185772 0.82688711\r\n 0.87101494 0.91417102 0.95629359 0.99732949 1.03718305 1.07590454\r\n 1.11342785 1.14973329 1.18480902 1.21865063 1.25117521 1.28254042\r\n 1.31269417 1.34165629 1.36945136 1.39599369 1.42152022 1.44597336\r\n 1.46939059 1.49181143 1.51313001 1.53365873 1.55331617 1.57214462\r\n 1.59018678 1.60731468 1.62389131 1.6398085  1.65510415 1.66981648\r\n 1.68398224 1.69763653 1.7108127  1.72354233 1.73585512 1.74777888\r\n 1.75933949 1.77056088 1.78146505 1.79227591 1.80262024 1.81289059\r\n 1.82271506 1.83247549 1.84181918 1.85130873 1.86025481 1.86928474\r\n 1.87801167 1.88645693 1.89441145 1.90196687 1.90918874 1.91612453\r\n 1.92280901 1.92950366 1.9365236  1.94326137 1.95001382 1.95646083\r\n 1.96290002 1.96901443 1.97487238 1.98050722 1.98593567 1.99116647\r\n 1.9962046  2.00105333 2.00571525 2.01019279 2.01448848 2.01860507\r\n 2.02254557 2.0260774  2.02977259 2.03303197]\r\nIntercept: 0.0\r\n\r\nAlpha: 34.04002216123752\r\nCoefficients: [0.         0.05715559 0.11667845 0.17856487 0.24279889 0.30935145\r\n 0.37443649 0.43584286 0.49826743 0.56158046 0.62564193 0.69030648\r\n 0.75542238 0.82083339 0.88638024 0.95190213 1.01723829 1.08222949\r\n 1.14671955 1.21055678 1.27359533 1.33570614 1.39674312 1.45659175\r\n 1.51514063 1.5722888  1.6279462  1.68203387 1.73450754 1.78527184\r\n 1.83429845 1.88155252 1.92700936 1.97065383 2.01252455 2.05254682\r\n 2.09076491 2.12719574 2.16186239 2.19485863 2.22610224 2.25568279\r\n 2.28364133 2.31002169 2.33495931 2.35833888 2.38028421 2.40084582\r\n 2.42007443 2.43812958 2.45485819 2.47040489 2.48482075 2.49815509\r\n 2.51045643 2.52177235 2.53214943 2.54163315 2.55026788 2.5580968\r\n 2.56516191 2.57150404 2.57716285 2.58203659 2.58643133 2.59012413\r\n 2.59341721 2.59608888 2.59842501 2.60007179 2.60154708 2.60244646\r\n 2.60307353 2.60344448 2.60373623 2.60391233 2.60395269 2.6038482\r\n 2.6035971  2.60303499 2.60195723 2.60081402 2.59941164 2.59799461\r\n 2.59636619 2.59476707 2.59316222 2.59154059 2.58990216 2.58825181\r\n 2.58659636 2.58494317 2.58329945 2.58167192 2.58006668 2.5784891\r\n 2.57694391 2.57560473 2.57406636 2.57276514]\r\nIntercept: 0.0\r\n"
      },
      {
        "user": "guilhermecsnpeixoto",
        "body": "Just so everyone is aware, a colleague and I are currently looking into this feature and plan to work on an implementation. We'll keep the thread updated with any progress or questions as they arise."
      }
    ]
  },
  {
    "issue_number": 28668,
    "title": "Automatically move `y` (and `sample_weight`) to the same device and namespace as `X` ",
    "author": "betatim",
    "state": "open",
    "created_at": "2024-03-20T13:02:03Z",
    "updated_at": "2025-06-13T08:38:39Z",
    "labels": [
      "Array API"
    ],
    "body": "(From https://github.com/scikit-learn/scikit-learn/pull/27800#issuecomment-1878709518 by @ogrisel)\r\n\r\nThe proposal/idea is to allow `y` to not be on the same device (and namespace?) as `X` when using Array API inputs. Currently we require/assume that they are on the same device and namespace, it is a requirement. However pipelines can't modify `y` which means it is not possible to move from CPU to GPU as one of the steps of the pipeline, the whole pipeline has to stay on one device. The below details some example and code to motivate allowing `X` ad `y` being on different devices (and namespaces).\r\n\r\n---\r\n\r\nSuppose we have:\r\n\r\n```python\r\n>>> import torch\r\n>>> from sklearn import set_config\r\n>>> from sklearn.datasets import make_regression\r\n>>> from sklearn.linear_model import Ridge\r\n>>> set_config(array_api_dispatch=True)\r\n>>> X, y = make_regression(n_samples=int(1e5), n_features=int(1e3), random_state=0)\r\n>>> X_torch_cuda = torch.tensor(X).to(\"cuda\")\r\n>>> y_torch_cuda = torch.tensor(y).to(\"cuda\")\r\n```\r\n\r\nI did a quick benchmark with timeit on a host with a 32 cores CPU and an A100 GPU: we get a bit more than 10x speed-up (which is in the range of what I would have expected):\r\n\r\n```python\r\n>>> %time Ridge(solver=\"svd\").fit(X, y)\r\nCPU times: user 1min 29s, sys: 1min 4s, total: 2min 34s\r\nWall time: 6.18 s\r\nRidge(solver='svd')\r\n>>> %time Ridge(solver=\"svd\").fit(X_torch_cuda, y_torch_cuda)\r\nCPU times: user 398 ms, sys: 2.74 ms, total: 401 ms\r\nWall time: 402 ms\r\nRidge(solver='svd')\r\n```\r\n\r\nI also tried the following:\r\n\r\n```python\r\n>>> Ridge(solver=\"svd\").fit(X_torch_cuda, y)\r\nTraceback (most recent call last):\r\n  Cell In[36], line 1\r\n    Ridge(solver=\"svd\").fit(X_torch_cuda, y)\r\n  File ~/code/scikit-learn/sklearn/base.py:1194 in wrapper\r\n    return fit_method(estimator, *args, **kwargs)\r\n  File ~/code/scikit-learn/sklearn/linear_model/_ridge.py:1197 in fit\r\n    device_ = device(*input_arrays)\r\n  File ~/code/scikit-learn/sklearn/utils/_array_api.py:104 in device\r\n    raise ValueError(\"Input arrays use different devices.\")\r\nValueError: Input arrays use different devices.\r\n```\r\n\r\nI think it might be reasonable to expect this pattern to fail in general. explicitly ask the the user to provide inputs with consistently allocated data buffers.\r\n\r\nHowever, we might want to be more lenient for the particular case of `y` (and `sample_weight`) and change `Ridge.fit` to automatically move `y` to the same namespace and device as `X`:\r\n\r\n```python\r\ny = xp.asarray(y, device=device(X))\r\n```\r\n\r\nThe reason would be to improve the usability of the Array API for the following pipelines:\r\n\r\n\r\n```python\r\nX_pandas_df, y_pandas_series = fetch_some_pandas_data()\r\n\r\npipeline = make_pipeline(\r\n    some_column_transformer(),  # works on CPU on the input dataframe\r\n    FunctionTransformer(func=lambda X: torch.tensor(X).to(\"float32\").to(\"cuda\")),\r\n    Ridge(solver=\"svd\"),\r\n)\r\npipeline.fit(X_pandas_df, y_pandas_series)\r\n```\r\n\r\nThe pipeline steps can only transform `X` and not `y` (or `sample_weight`). So it means that user would have to instead call:\r\n\r\n```python\r\npipeline.fit(X_pandas_df, torch.tensor(y_pandas_series).to(\"cuda\"))\r\n```\r\n\r\nwhich feels a bit weird/cumbersome to me.\r\n\r\nThis might not be a big deal though and I don't want to delay this PR to get a consensus on this particular point: I think it's fine the way it is for now but we might want come back to the UX of pipelines with Array API steps later. I will open a dedicated issue.\r\n\r\n---\r\n\r\nIf we take action there are a few estimators that need double checking. For example `LinearDiscriminantAnalysis` currently raises low level PyTorch exceptions when `X` and `y` are on different devices.",
    "comments": [
      {
        "user": "betatim",
        "body": "After thinking about this for a bit I think I like the idea. It solves a problem for which we haven't found a good solution in a while now (how to move from one device to another). I think allowing `y` to \"follow\" `X` when needed is a good solution and somehow sensible.\r\n\r\nSo right now I'm +1"
      },
      {
        "user": "ogrisel",
        "body": "+1 in general, and even more so to make it possible to train classifier pipelines on a GPU with str object-typed class labels passed to the pipeline as we usually allow."
      },
      {
        "user": "ogrisel",
        "body": "Note, on the implementation side, to move NumPy arrays and Python lists to a specific array namespace/device combo, `xp.asarray(source, device=device)` should work (as we currently do in `ensure_commone_namespace_device`).\r\n\r\nBut if the things to move are already already on another namespace/device combo, that will most likely fail in general, for instance with torch:\r\n\r\n```python\r\n>>> import array_api_strict as xp_strict\r\n>>> import array_api_compat.torch as xp_torch\r\n>>> reference = xp_strict.ones(3)\r\n>>> reference.device\r\nCPU_DEVICE\r\n>>> xp_strict.asarray(xp_torch.ones(3, device=\"mps\"), device=reference.device)\r\nTraceback (most recent call last):\r\n  Cell In[49], line 1\r\n    xp_strict.asarray(xp_torch.ones(3, device=\"mps\"), device=reference.device)\r\n  File ~/miniforge3/envs/dev/lib/python3.11/site-packages/array_api_strict/_creation_functions.py:91 in asarray\r\n    res = np.array(obj, dtype=_np_dtype, copy=copy)\r\n  File ~/miniforge3/envs/dev/lib/python3.11/site-packages/torch/_tensor.py:1030 in __array__\r\n    return self.numpy()\r\nTypeError: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\r\n```\r\n\r\nIn this case, we should instead use the new improvements made to the dlpack and Array API specs:\r\n\r\n> Starting Python array API standard v2023, a copy can be explicitly requested (or disabled) through the new copy argument of from_dlpack(). When a copy is made, the producer must set the DLPACK_FLAG_BITMASK_IS_COPIED bit flag. It is also possible to request cross-device copies through the new device argument, though the v2023 standard only mandates the support of kDLCPU.\r\n\r\nhttps://github.com/dmlc/dlpack/blob/main/docs/source/python_spec.rst\r\n\r\nBut note that there is no guarantee that cross-device transfers work in general, so we might always have to move through a CPU device if needed.\r\n\r\nFurthermore, this is not yet implemented in `array-api-compat`/`array-api-strict`:\r\n\r\nhttps://github.com/data-apis/array-api-compat/issues/86#issuecomment-2009767287\r\n\r\nIn the mean time we will have to rely on our `_convert_to_numpy` hack before calling back to `xp.asarray` if we need to do cross-non-numpy-namespaces transfers.\r\n\r\nFor now we can decide to implement the \"auto-follow\" thing only when X is a namespace and `y` and `sample_weight` are numpy or other non-namespace containers."
      }
    ]
  },
  {
    "issue_number": 31533,
    "title": "RFC: stop using scikit-learn `stable_cumsum` and instead use `np/xp.cumsum` directly",
    "author": "ogrisel",
    "state": "open",
    "created_at": "2025-06-12T12:11:30Z",
    "updated_at": "2025-06-13T08:24:32Z",
    "labels": [
      "RFC",
      "Array API"
    ],
    "body": "As discussed in https://github.com/scikit-learn/scikit-learn/pull/30878/files#r2142562746, our current `stable_cumsum` function brings very little value to the user: it does extra computation to check that `np.allclose(np.sum(x), np.cumsum(x)[-1])` and raises a warning otherwise. However, in most cases, users can do nothing about the warning.\n\nFurthermore, as seen in the CI of #30878, the array API compatible libraries we test against do not have the same numerical stability behavior for `sum` and `cumsum`, so it makes it challenging to write a test for the occurrence of this warning that is consistent across libraries.\n\nSo I would rather not waste the overhead of computing `np.sum(x)` and just always directly call `np.cumsum` or `xp.cumsum` and deprecate `sklearn.utils.extmath.stable_cumsum`.\n\n",
    "comments": [
      {
        "user": "ogrisel",
        "body": "If I remember correctly, I think this one of the reasons why our kmeans++ implementation is slower than alternative implementations in other libraries such as Intel oneDAL."
      },
      {
        "user": "OmarManzoor",
        "body": "I agree with this proposal. Thanks @ogrisel "
      },
      {
        "user": "lesteve",
        "body": "`stable_cumsum` also makes sure that the cumsum computation is done with float64, i.e. `np.cumsum(..., dtype=np.float64)`. What would the plan be on this aspect?\n\nDoing a bit of archeology the original motivation for `stable_cumsum` was that `roc_auc_score` could give \"wrong\" results with float32 weights, see https://github.com/scikit-learn/scikit-learn/issues/6842.\n\nI tried quickly removing `stable_cumsum` in `sklearn.metrics._ranking._binary_clf_curve` (see diff below) and I can reproduce the problematic behaviour locally:\n\n```py\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\n\nrng = np.random.RandomState(42)\nn_samples = 4 * 10 ** 7\ny = rng.randint(2, size=n_samples)\nprediction = rng.normal(size=n_samples) + y * 0.01\ntrivial_weight = np.ones(n_samples)\nprint('without weights', roc_auc_score(y, prediction))\nprint('float32 weights', roc_auc_score(y, prediction, sample_weight=trivial_weight.astype('float32')))\nprint('float64 weights', roc_auc_score(y, prediction, sample_weight=trivial_weight.astype('float64')))\n```\n\n```\nwithout weights 0.5027392452566966\nfloat32 weights 0.5835567712783813\nfloat64 weights 0.5027392452566966\n```\n\n```diff\ndiff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 2d0e5211c2..802279a812 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -898,11 +898,11 @@ def _binary_clf_curve(y_true, y_score, pos_label=None, sample_weight=None):\n     threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n \n     # accumulate the true positives with decreasing threshold\n-    tps = stable_cumsum(y_true * weight)[threshold_idxs]\n+    tps = np.cumsum(y_true * weight)[threshold_idxs]\n     if sample_weight is not None:\n         # express fps as a cumsum to ensure fps is increasing even in\n         # the presence of floating point errors\n-        fps = stable_cumsum((1 - y_true) * weight)[threshold_idxs]\n+        fps = np.cumsum((1 - y_true) * weight)[threshold_idxs]\n     else:\n         fps = 1 + threshold_idxs - tps\n     return fps, tps, y_score[threshold_idxs]\n```\n\n"
      }
    ]
  },
  {
    "issue_number": 31290,
    "title": "`_safe_indexing` triggers `SettingWithCopyWarning` when used with `slice`",
    "author": "MarcoGorelli",
    "state": "open",
    "created_at": "2025-05-01T14:17:02Z",
    "updated_at": "2025-06-13T01:01:17Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nHere's something I noticed while looking into https://github.com/scikit-learn/scikit-learn/pull/31127\n\nThe test\n```\npytest sklearn/utils/tests/test_indexing.py::test_safe_indexing_pandas_no_settingwithcopy_warning\n```\nchecks that a copy is produced, and that no `SettingWithCopyWarning` is produced\n\nIndeed, no copy is raised, but why is using `_safe_indexing` with a slice allowed to not make a copy? Is this intentional?\n\nBased on responses, I can suggest what to do instead in https://github.com/scikit-learn/scikit-learn/pull/31127\n\n(I am a little surprised that this always makes copies, given that a lot of the discussion in https://github.com/scikit-learn/scikit-learn/issues/28341 centered around wanting to avoid copies)\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\n\nfrom sklearn.utils import _safe_indexing\nimport pandas as pd\n\nX = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [3, 4, 5]})\nsubset = _safe_indexing(X, slice(0, 2), axis=0)\nsubset.iloc[0, 0] = 10\n```\n\n### Expected Results\n\nNo `SettingWithCopyWarning`\n\n### Actual Results\n\n```\n/home/marcogorelli/scikit-learn-dev/t.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  subset.iloc[0, 0] = 10\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nexecutable: /home/marcogorelli/scikit-learn-dev/.venv/bin/python\n   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\n\nPython dependencies:\n      sklearn: 1.7.dev0\n          pip: 24.2\n   setuptools: None\n        numpy: 2.1.0\n        scipy: 1.14.0\n       Cython: 3.0.11\n       pandas: 2.2.2\n   matplotlib: None\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: /home/marcogorelli/scikit-learn-dev/.venv/lib/python3.11/site-packages/numpy.libs/libscipy_openblas64_-ff651d7f.so\n        version: 0.3.27\nthreading_layer: pthreads\n   architecture: SkylakeX\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: /home/marcogorelli/scikit-learn-dev/.venv/lib/python3.11/site-packages/scipy.libs/libscipy_openblas-c128ec02.so\n        version: 0.3.27.dev\nthreading_layer: pthreads\n   architecture: SkylakeX\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 16\n         prefix: libgomp\n       filepath: /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0\n        version: None\n```",
    "comments": [
      {
        "user": "MarcoGorelli",
        "body": "based on git logs for related functionality, I'm going to tag @glemaitre @ogrisel @lorentzenchr  @jeremiedbb to ask what the indended behaviour is"
      },
      {
        "user": "glemaitre",
        "body": "Let me jump in with a couple of thoughts and historical views.\n\nLet's put aside the `slice` part to better grasp the expectation behind `_safe_indexing`. From the top of the head, the 99% of use-cases is along the line: get some rows to work with them without altering them. So a kind of read-only subsampling and thus no need for copying would be the best option. However, I don't think that we never implemented a defensive read-only mechanism (via a flag or something similar) and we most probably expect a defensive copy.\n\nThe 1% that I'm aware in scikit-learn that would need to modify `X` is the permutation importance algorithm where we want to make a copy and permute the rows of a column:\n\nhttps://github.com/scikit-learn/scikit-learn/issues/31284\n\nNow about the `slice`: 100% of the internal use cases only rely on array-like indexing and do not use slices. I think that slice in `_safe_indexing` was supported because we wanted to have the same indexing capabilities than NumPy arrays. When it comes to the public API, `slice` would only be accepted (as far I can recall) in `ColumnTransformer` where columns can be specified with a `slice` to dispatch them into a transformer.\n\nSo to come back on the original test that could fail, it was written when dealing with sampling during cross-validation and `slice` are not accepted in this case. Therefore, it never has been tested in terms of unit tests.\n\nSo for the time being, I would maybe suggest to:\n\n- trigger a copy for `slice` to have the same behaviour than other indexing method\n- question whether or not we should deprecate this option. I would guess that using slice with integer to select dataset should not be the most used feature and if it used with string, then I would almost consider it as a bad practice because it is extremely implicit regarding the column selected."
      },
      {
        "user": "Muwinuddin",
        "body": "hi @MarcoGorelli .I'd love to contribute to resolving the bug where _safe_indexing triggers a SettingWithCopyWarning when used with slices, which can cause confusion and potentially unintended behavior.\n\nAfter reviewing the codebase, it seems this warning arises because slicing pandas objects like DataFrames or Series may return a view instead of a copy. One possible solution is to ensure _safe_indexing explicitly returns a copy in such cases (e.g., using .copy(deep=True)), especially when indexing with slices.\n\nHereâ€™s my proposed plan:\n\nInvestigate all usage patterns of _safe_indexing where slicing and pandas objects intersect.\n\nAdd logic to safely return a copy where necessary.\n\nEnsure any fix is backward-compatible and doesnâ€™t significantly impact performance.\n\nAdd relevant unit tests to cover the edge cases and confirm the warning is resolved.\n\nPlease let me know if this approach aligns with the maintainers' expectations or if there's anything you'd suggest before I start working on a PR. Looking forward to contributing!"
      }
    ]
  },
  {
    "issue_number": 31527,
    "title": "âš ï¸ CI failed on Wheel builder (last failure: Jun 12, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-06-12T04:36:25Z",
    "updated_at": "2025-06-12T15:23:10Z",
    "labels": [
      "Needs Triage"
    ],
    "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/15601223966)** (Jun 12, 2025)\n",
    "comments": [
      {
        "user": "lesteve",
        "body": "The free-threaded failures are likely due to cibuildwheel 3.0.0 release, from [changelog](https://cibuildwheel.pypa.io/en/stable/changelog/#v300):\n\n> Removed the CIBW_PRERELEASE_PYTHONS and CIBW_FREE_THREADED_SUPPORT options - these have been folded into the [enable](https://cibuildwheel.pypa.io/en/stable/options/#enable) option instead. (#2095)\n\nSee https://github.com/pypa/cibuildwheel/pull/2095 for more details."
      }
    ]
  },
  {
    "issue_number": 31520,
    "title": "32-Bit Raspberry Pi OS Installation Issues with UV",
    "author": "nebhead",
    "state": "closed",
    "created_at": "2025-06-11T01:05:52Z",
    "updated_at": "2025-06-12T15:04:52Z",
    "labels": [
      "Bug",
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nWhen attempting to install scikit-learn==1.4.2 - 1.6.1 on Raspberry Pi OS Lite 32-Bit (Bookworm) or Raspberry Pi OS Lit 32-Bit (Bullseye) with UV, the following error is given:\n```\n  Ã— Failed to download and build `scikit-learn==1.4.2`\n  â”œâ”€â–¶ Failed to resolve requirements from `build-system.requires`\n  â”œâ”€â–¶ No solution found when resolving: `setuptools`, `wheel`, `cython>=3.0.8`, `numpy==2.0.0rc1`, `scipy>=1.6.0`\n  â•°â”€â–¶ Because there is no version of numpy==2.0.0rc1 and you require numpy==2.0.0rc1, we can conclude that your\n      requirements are unsatisfiable.\n```\n\nIf I had to guess, it's that the numpy==2.0.0rc1 is the issue, but I'm not sure.  \n\nBullseye is also on Python 3.9 so the last version we can install is v1.6.1.  \n\n\n\n### Steps/Code to Reproduce\n\n```bash\n# 1. Install UV\n# 2. Create Virtual Environment\nuv venv --system-site-packages test \n# 3. Start venv\nsource test/bin/activate\n# 4. Install scikit-learn\nuv pip install scikit-learn==1.6.1\n```\n\n### Expected Results\n\nExpect that it should install correctly without errors. \n\n### Actual Results\n\n```\n  Ã— Failed to download and build `scikit-learn==1.4.2`\n  â”œâ”€â–¶ Failed to resolve requirements from `build-system.requires`\n  â”œâ”€â–¶ No solution found when resolving: `setuptools`, `wheel`, `cython>=3.0.8`, `numpy==2.0.0rc1`, `scipy>=1.6.0`\n  â•°â”€â–¶ Because there is no version of numpy==2.0.0rc1 and you require numpy==2.0.0rc1, we can conclude that your\n      requirements are unsatisfiable.\n```\n\n### Versions\n\n```shell\n1.4.2\n1.6.0\n1.6.1\n```",
    "comments": [
      {
        "user": "lesteve",
        "body": "Some suggestions:\n- for scikit-learn 1.4.2 and the `numpy==2.0.0rc1` it seems like `uv` has a prerelease handling that you may want to look at https://docs.astral.sh/uv/reference/cli/#uv-run--prerelease\n- I am guessing the error with scikit-learn 1.6.1 is different, could you post it as well for further reference?\n- could you try with `pip` to see whether the same problem happens?\n\nI am going to close this one as I feel this is quite unlikely to be an issue in scikit-learn. In case further investigation shows that I am wrong, feel free to reopen!\n\n\n"
      },
      {
        "user": "lesteve",
        "body": "Actually looks like there is no `numpy==2.0.0rc1` on PyPI. I guess it was removed somehow, this is quite unexpected ðŸ¤” ... probably this numpy mailing list [thread](https://mail.python.org/archives/list/numpy-discussion@python.org/thread/MOIRY73LIHSQTEWDBRYGNU6767NZC2HG/#H7BQQGWC3QY5SYQJRZW74KSGF4KSIXRJ) is related ...\n\nI guess to have something that works I would suggest trying with scikit-learn 1.6.1."
      },
      {
        "user": "nebhead",
        "body": "You're correct!  The 1.6.1 version has a different issue.  It appears with further investigation that UV will try to build numpy in this version, but after several hours, will run out of space and error out.  \n\nRunning on Raspberry Pi OS 12 Bookworm 32-Bit with Python 3.11.  \n\nSo perhaps it's just that building numpy is too much for the Raspberry Pi 3b+.  \n\n"
      }
    ]
  },
  {
    "issue_number": 31407,
    "title": "Cannot recover DBSCAN from memory-overuse",
    "author": "hubernikus",
    "state": "open",
    "created_at": "2025-05-21T11:38:43Z",
    "updated_at": "2025-06-12T13:13:19Z",
    "labels": [
      "Bug",
      "help wanted",
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nI also just ran into this issue that the program gets killed when running DBSCAN, similar to:\nhttps://github.com/scikit-learn/scikit-learn/issues/22531\n\nThe documentation update already helps and I think it's ok for the algorithm to fail. But currently there is no way for me to recover, and a more informative error message would be useful. Since now DBSCAN just reports `killed` and it requires a bit of search to see what fails:\n```\n>>> DBSCAN(eps=1, min_samples=2).fit(np.random.rand(10_000_000, 3))\nKilled\n```\n\ne.g., something like how `numpy` does it:\n```\n>>> n = int(1e6)\n>>> np.random.rand(n, n)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"numpy/random/mtrand.pyx\", line 1219, in numpy.random.mtrand.RandomState.rand\n  File \"numpy/random/mtrand.pyx\", line 437, in numpy.random.mtrand.RandomState.random_sample\n  File \"_common.pyx\", line 307, in numpy.random._common.double_fill\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 7.28 TiB for an array with shape (1000000, 1000000) and data type float64\n```\n\nAdditionally, I noted that the memory accumulated with consecutive calling of DBSCAN. Which can lead to a killed program even though there is enough memory when running a single fit.\nI was able to resolve this by explicitly calling `import gc; gc.collect()` after each run. Maybe this could be invoked at the end of each DBSCAN fit?\n\n### Steps/Code to Reproduce\n\n```python\ntry:\n    DBSCAN(eps=1, min_samples=2).fit(np.random.rand(10_000_000, 3))\nexcept:\n    print(\"Caught exception\")\n```\n\n\n### Expected Results\n\n```python\nCaught exception\n```\n\n### Actual Results\n\n```python\nKilled\n```\n\n### Versions\n\n```shell\n>>> import sklearn; sklearn.show_versions()\n\nSystem:\n    python: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nexecutable: /usr/bin/python3\n   machine: Linux-6.14.6-arch1-1-x86_64-with-glibc2.35\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: None\n   setuptools: 80.7.1\n        numpy: 1.26.4\n        scipy: 1.15.3\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.3\n       joblib: 1.5.0\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 20\n         prefix: libopenblas\n       filepath: /usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so\n        version: 0.3.23.dev\nthreading_layer: pthreads\n   architecture: Prescott\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 20\n         prefix: libscipy_openblas\n       filepath: /usr/local/lib/python3.10/dist-packages/scipy.libs/libscipy_openblas-68440149.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 20\n         prefix: libgomp\n       filepath: /usr/local/lib/python3.10/dist-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n        version: None\n```",
    "comments": [
      {
        "user": "lesteve",
        "body": "I would suggest to try to use `sklearn.cluster.HDBSCAN` instead of `DBSCAN` and report if you see any improvements.\n\nAccording to https://github.com/scikit-learn/scikit-learn/issues/26726#issuecomment-1675057064, it may use a lot less memory."
      },
      {
        "user": "hubernikus",
        "body": "For this specific use case, I can also down sample the dataset. But I'd like to make this decision automatically.\n\nAnd I feel for general use case, it would be great to be able to recover from this memory error, or even predict the error, such that the user can adapt the algorithm."
      },
      {
        "user": "lesteve",
        "body": "So the problem is likely a low-level one. Somewhere in our Cython code our memory usage grows, and at one point the OS OOM killer kills the Python process.\n\nI am not sure there is a straightforward way to surface the error in a user-friendly manner but maybe I am wrong and if someone finds a way to improve the situation, this would be more than welcome!\n\n"
      }
    ]
  },
  {
    "issue_number": 31521,
    "title": "TarFile.extractall() got an unexpected keyword argument 'filter'",
    "author": "TE-YongweiSun",
    "state": "open",
    "created_at": "2025-06-11T05:12:28Z",
    "updated_at": "2025-06-12T12:52:50Z",
    "labels": [
      "Bug",
      "Regression"
    ],
    "body": "### Describe the bug\n\nFor the latest version `1.7.0`, it can be installed with Python 3.10, but the parameter `filter` is available starting from Python 3.12 (See: https://docs.python.org/3/library/tarfile.html#tarfile.TarFile.extractall ). \nhttps://github.com/scikit-learn/scikit-learn/blob/5194440b5d41e73ff436c45e35aa1476223f753c/sklearn/datasets/_twenty_newsgroups.py#L87\n\nAs a result, when I attempted to download the `20newsgroups` dataset, an error occurred:\n\n```\n  File \"\\xxx\\sklearn\\utils\\_param_validation.py\", line 218, in wrapper\n    return func(*args, **kwargs)\n  File \"\\xxx\\sklearn\\datasets\\_twenty_newsgroups.py\", line 322, in fetch_20newsgroups\n    cache = _download_20newsgroups(\n  File \"\\xxx\\sklearn\\datasets\\_twenty_newsgroups.py\", line 87, in _download_20newsgroups\n    fp.extractall(path=target_dir, filter=\"data\")\nTypeError: TarFile.extractall() got an unexpected keyword argument 'filter'\n```\n\n### Steps/Code to Reproduce\n\n```\nfrom sklearn.datasets import fetch_20newsgroups\ncats = ['alt.atheism', 'sci.space']\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n```\n\n### Expected Results\n\n```\nlist(newsgroups_train.target_names)\nnewsgroups_train.filenames.shape\nnewsgroups_train.target.shape\nnewsgroups_train.target[:10]>>> cats = ['alt.atheism', 'sci.space']\n```\n\n### Actual Results\n\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"\\xxx\\sklearn\\utils\\_param_validation.py\", line 218, in wrapper\n    return func(*args, **kwargs)\n  File \"\\xxx\\sklearn\\datasets\\_twenty_newsgroups.py\", line 322, in fetch_20newsgroups\n    cache = _download_20newsgroups(\n  File \"\\xxx\\sklearn\\datasets\\_twenty_newsgroups.py\", line 87, in _download_20newsgroups\n    fp.extractall(path=target_dir, filter=\"data\")\nTypeError: TarFile.extractall() got an unexpected keyword argument 'filter'\n```\n\n### Versions\n\n```shell\n`1.7.0`\n```",
    "comments": [
      {
        "user": "lesteve",
        "body": "Thanks for the issue! We should indeed fix this:\n- revert the relevant part of https://github.com/scikit-learn/scikit-learn/pull/31022\n- mention that we can drop `sklearn.utils.fixes.tarfile_extractall` when our minimum supported version is Python 3.12 is the first version that has the `filter` from the start i.e. 3.12.0.\n\nFor completeness, the doc is slightly misleading: if you are using Python 3.10.x with `x>=12`, Python 3.11.x with `x>=4` or Python>=3.12 you are fine. This is based on the [3.10 doc](https://docs.python.org/3.10/library/tarfile.html#tarfile.TarFile.extractall) that says\n> Changed in version 3.10.12: Added the filter parameter\n\nand the [3.11 doc](https://docs.python.org/3.11/library/tarfile.html#tarfile.TarFile.extractall) says\n> Changed in version 3.11.4: Added the filter parameter.\n\n\n"
      }
    ]
  },
  {
    "issue_number": 31200,
    "title": "DOC Examples (imputation): add scaling when using k-neighbours imputation",
    "author": "5nizza",
    "state": "closed",
    "created_at": "2025-04-14T12:17:24Z",
    "updated_at": "2025-06-12T09:11:13Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the issue linked to the documentation\n\nTwo examples for missing-values imputation use k-neighbors imputation without scaling data first.\nAs a result, the approaches under-perform.\nThe examples are:\n\n1. https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py\n2. https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html\n\nIn the first example, the effect is quite small, adding scaling before calling k-neighbours imputer changes MSE for the california dataset for k-NN from 0.2987 Â± 0.1469 to 0.2912 Â± 0.1410 and for the diabetes dataset from 3314 Â± 114  to 3323 Â± 90.\n\nIn the second example (comparing iterative imputations), the change is more significant: before the change, iterative imputation with k-neighbors performed worse than imputation with mean, after the scaling -- it performs better than mean imputation.\n\nIn both cases, it is a better practice to scale data before using a k-neighbors approach which is based on distances between points.\n\n![Image](https://github.com/user-attachments/assets/167560c9-3011-425f-a29f-74548fc9e8bc)\n\n### Suggest a potential alternative/fix\n\nI will submit a patch to fix an issue.",
    "comments": [
      {
        "user": "ogrisel",
        "body": "I agree that k-NN requires feature scaling in general."
      }
    ]
  },
  {
    "issue_number": 21138,
    "title": "Example with ranking metrices",
    "author": "lorentzenchr",
    "state": "open",
    "created_at": "2021-09-24T11:25:42Z",
    "updated_at": "2025-06-12T07:27:05Z",
    "labels": [
      "Documentation",
      "Moderate",
      "help wanted",
      "module:metrics"
    ],
    "body": "### Describe the issue linked to the documentation\n\nSome of the metrices in #2805 were implemented in #7739.\n\n### Suggest a potential alternative/fix\n\nIt would be nice to add (to) an example the usage of those ranking metrices with the addition of, e.g., kendall's tau and spearman's rho:\r\n```python\r\nfrom scipy.stats import kendalltau, spearmanr\r\nfrom sklearn.metrics import make_scorer\r\n\r\nkenall_tau_score = make_scorer(kendalltau)\r\nspearman_rho_score = make_scorer(kendalltau)\r\n```",
    "comments": [
      {
        "user": "glemaitre",
        "body": "If I am not mistaken, I don't think that we have any example tackling the problem of recommendation. It would be nice to have a full example with a predictive model and the way to evaluate it?"
      },
      {
        "user": "acse-srm3018",
        "body": "take"
      },
      {
        "user": "adrinjalali",
        "body": "@acse-srm3018 are you still working on this? If you have a draft, feel free to open a draft PR, mark it WIP, and ask for feedback if you need any help. "
      }
    ]
  },
  {
    "issue_number": 29542,
    "title": "FEA Add missing-value support to sparse splitter in RandomForest and ExtraTrees",
    "author": "adam2392",
    "state": "open",
    "created_at": "2024-07-22T12:28:29Z",
    "updated_at": "2025-06-11T22:40:23Z",
    "labels": [
      "help wanted",
      "module:tree",
      "cython"
    ],
    "body": "### Summary\r\nWhile missing-value support for decision trees have been added recently, they only work when encoded in a dense array. Since `RandomForest*` and `ExtraTrees*` both support sparse `X`, if a user encodes `np.nan` inside sparse `X`, it should still work.\r\n\r\n### Solution\r\nAdd missing-value logic in `SparsePartitioner` in `_parititoner.pyx`, `BestSparseSplitter` and `RandomSparseSplitter` in `_splitter.pyx`.\r\n\r\nThe logic is the same as in the dense case, but just has to handle the fact that `X` is now sparse CSC array format.\r\n\r\n### Misc.\r\n              FYI https://github.com/scikit-learn/scikit-learn/pull/27966 will introduce native support for missing values in the `ExtraTree*` models (i.e. random splitter). \r\n\r\nOne thing I noticed though as I went through the PR is that the current codebase still does not support missing values in the sparse splitter. I think this might be pretty easy to add, but should we re-open this issue technically?\r\n\r\nXref: https://github.com/scikit-learn/scikit-learn/issues/5870#issuecomment-1166581736\r\n\r\n_Originally posted by @adam2392 in https://github.com/scikit-learn/scikit-learn/issues/5870#issuecomment-2212688552_\r\n            ",
    "comments": [
      {
        "user": "Higgs32584",
        "body": "This _might_ be related to #27993 , although it is more so related to multi-task random forests"
      },
      {
        "user": "vitorpohlenz",
        "body": "Hi @adam2392 ,\n\nIf you could help me, I have some questions about this Issue.\n\n1. Is this Issue still open, and is it worth working on it? I followed the history associated, and it seems that the first conversations about it started 10 years ago.\n\n2. Do you think this is \"easy enough\" to be a good \"second issue?\" Because my first and only contribution was a few weeks ago. @glemaitre cc here just because your [comment](https://github.com/scikit-learn/scikit-learn/issues/5870#issuecomment-2242817769) on a related Issue. Also, I would like a \"second opinion\" on this question.\n"
      },
      {
        "user": "adam2392",
        "body": "Hi @vitorpohlenz !\n\nThank you for your interest in contributing! \n\n1. Yes it is still open and it is worth working on it. \n2. Is it easy? That depends on your experience with Cython and/or C++ and also understanding of decision trees. It also depends on how deep you want to dive into the code. The main thing to add is the same tree logic handling for missing values for sparse array inputs. \n\nLmk what you think and if you have a proposed idea of how to proceed. "
      }
    ]
  },
  {
    "issue_number": 31525,
    "title": "Issue with the `RidgeCV` diagram representation with non-default alphas",
    "author": "glemaitre",
    "state": "open",
    "created_at": "2025-06-11T20:41:12Z",
    "updated_at": "2025-06-11T20:55:36Z",
    "labels": [
      "Bug"
    ],
    "body": "It seems that we introduced a regression in the HTML representation. The following code is failing:\n\n```python\nimport numpy as np\nfrom sklearn.linear_model import RidgeCV\n\nRidgeCV(np.logspace(-3, 3, num=10)\n```\n\nleads to the following error:\n\n```pytb\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile ~/Documents/teaching/demo_data_science_agent/.pixi/envs/default/lib/python3.13/site-packages/IPython/core/formatters.py:406, in BaseFormatter.__call__(self, obj)\n    404     method = get_real_method(obj, self.print_method)\n    405     if method is not None:\n--> 406         return method()\n    407     return None\n    408 else:\n\nFile ~/Documents/teaching/demo_data_science_agent/.pixi/envs/default/lib/python3.13/site-packages/sklearn/utils/_repr_html/base.py:145, in ReprHTMLMixin._repr_html_inner(self)\n    140 def _repr_html_inner(self):\n    141     \"\"\"This function is returned by the @property `_repr_html_` to make\n    142     `hasattr(estimator, \"_repr_html_\") return `True` or `False` depending\n    143     on `get_config()[\"display\"]`.\n    144     \"\"\"\n--> 145     return self._html_repr()\n\nFile ~/Documents/teaching/demo_data_science_agent/.pixi/envs/default/lib/python3.13/site-packages/sklearn/utils/_repr_html/estimator.py:480, in estimator_html_repr(estimator)\n    469 html_template = (\n    470     f\"<style>{style_with_id}</style>\"\n    471     f\"<body>\"\n   (...)    476     '<div class=\"sk-container\" hidden>'\n    477 )\n    479 out.write(html_template)\n--> 480 _write_estimator_html(\n    481     out,\n    482     estimator,\n    483     estimator.__class__.__name__,\n    484     estimator_str,\n    485     first_call=True,\n    486     is_fitted_css_class=is_fitted_css_class,\n    487     is_fitted_icon=is_fitted_icon,\n    488 )\n    489 with open(str(Path(__file__).parent / \"estimator.js\"), \"r\") as f:\n    490     script = f.read()\n\nFile ~/Documents/teaching/demo_data_science_agent/.pixi/envs/default/lib/python3.13/site-packages/sklearn/utils/_repr_html/estimator.py:386, in _write_estimator_html(out, estimator, estimator_label, estimator_label_details, is_fitted_css_class, is_fitted_icon, first_call, param_prefix)\n    384 elif est_block.kind == \"single\":\n    385     if hasattr(estimator, \"_get_params_html\"):\n--> 386         params = estimator._get_params_html()._repr_html_inner()\n    387     else:\n    388         params = \"\"\n\nFile ~/Documents/teaching/demo_data_science_agent/.pixi/envs/default/lib/python3.13/site-packages/sklearn/base.py:310, in BaseEstimator._get_params_html(self, deep)\n    306 ordered_out = {name: out[name] for name in init_default_params if name in out}\n    307 ordered_out.update({name: out[name] for name in remaining_params})\n    309 non_default_ls = tuple(\n--> 310     [name for name, value in ordered_out.items() if is_non_default(name, value)]\n    311 )\n    313 return ParamsDict(ordered_out, non_default=non_default_ls)\n\nFile ~/Documents/teaching/demo_data_science_agent/.pixi/envs/default/lib/python3.13/site-packages/sklearn/base.py:296, in BaseEstimator._get_params_html.<locals>.is_non_default(param_name, param_value)\n    291 if isinstance(param_value, BaseEstimator) and type(param_value) is not type(\n    292     init_default_params[param_name]\n    293 ):\n    294     return True\n--> 296 if param_value != init_default_params[param_name] and not (\n    297     is_scalar_nan(init_default_params[param_name])\n    298     and is_scalar_nan(param_value)\n    299 ):\n    300     return True\n    301 return False\n\nValueError: operands could not be broadcast together with shapes (10,) (3,) \n```\n\n@DeaMariaLeon Would you mind to look at it. If I get the time, I can look at it a bit more in details.",
    "comments": [
      {
        "user": "glemaitre",
        "body": "My guess here is that `param_value != init_default_params[param_name]` compare 2 numpy arrays:\n\n```python\nnp.array([1, 2, 3]) != np.array([1, 2, 3, 4])\n```\n\n```pytb\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[10], line 1\n----> 1 np.array([1, 2, 3]) != np.array([1, 2, 3, 4])\n\nValueError: operands could not be broadcast together with shapes (3,) (4,) \n```\n\nIt happens because the two arrays do not have the same size. I'm thinking that we could safely have a `try`/`except` around the comparison because if it fails, it means that the parameter are not the same:\n\n```python\ntry:\n    if param_value != init_default_params[param_name] and not (\n           is_scalar_nan(init_default_params[param_name])\n           and is_scalar_nan(param_value)\n    ):\n        return True\nexcept Exception:\n    # any exception raised means that we don't have the same\n    # parameters\n    return True\nreturn False\n```\n\nSomething in this line of spirit."
      }
    ]
  },
  {
    "issue_number": 23727,
    "title": "[RFC] WASM / pyodide as a (somewhat) officially supported platform for scikit-learn",
    "author": "ogrisel",
    "state": "closed",
    "created_at": "2022-06-22T13:06:41Z",
    "updated_at": "2025-06-11T09:38:07Z",
    "labels": [
      "RFC",
      "Needs Investigation"
    ],
    "body": "We started having bug reports (at least one indirect, in real life report at a conference: #23707) from users of scikit-learn in WASM environment (e.g. pyodide / jupyterlite, pyscript...).\r\n\r\nShall we invest effort in setting CI tooling to properly test and maybe even handle packaging of scikit-learn to target that platform?\r\n\r\nIt's very likely that not all of scikit-learn will work out of the box, but with proper tooling in place we could maintain a public list of modules that have all their tests that pass and maybe a list of modules that required so patches to handle graceful degradation to target this platform (e.g. number of parallel worker threads with n_jobs).\r\n\r\n@rth put some interesting info in the following comment on how to run the tests:\r\n\r\n- https://github.com/scikit-learn/scikit-learn/issues/23707#issuecomment-1163053666\r\n\r\nPros:\r\n- WASM is likely to be a very popular target platform, at least for education (can directly teach Python programming and ML concepts without having to teach how to install packages from the command line first).\r\n\r\nCons:\r\n- test execution is probably much slower that on our regular CI targets;\r\n- need to maintain a list of known issues / limitations;\r\n- more packaging, release process will be even more complicated;\r\n- SciPy is quite heavily patched because there is no working Fortran compiler on that platform  (that might change soon with lfotran) so it relies on a semi-hackish Fortran to C transpilation step that introduces additional complexity.",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Maybe we should start by listing the fraction of modules with failing test and scanning through them to estimate how many stem from a common known upstream limitation and how likely it is going to be lifted in the short to medium term or if there is a somewhat maintainable work-around we might want to include in the scikit-learn code base or as an external packaging patch."
      },
      {
        "user": "rth",
        "body": "Yes, I think running it in CI is probably indeed a bit early (and also it would be really slow). The first more investigative steps could be a good start,\r\n 1. manually run the test suite module by module, see what fails (and report upstream). This would already be very helpful. Last time I did it was in 2018 in https://github.com/pyodide/pyodide/issues/139#issuecomment-436430260 and the situation should be much better now.\r\n 2. figure out how to best run the test suite programmatically for a large package such as scikit-learn. Pyodide has a [pytest plugin](https://github.com/pyodide/pyodide/tree/main/pyodide-test-runner/pyodide_test_runner) which will be better packaged once https://github.com/pyodide/pyodide/pull/2742 is merged. Once installed [it exposes  pytest fixtures](https://github.com/pyodide/pyodide/blob/main/packages/scikit-learn/test_scikit-learn.py#L11)  that would allow running some Python code in Pyodide inside a browser (Chrome or Firefox) with selenium or Node.js. Though for now, this package has no users outside of Pyodide, so more work is likely necessary to make it standalone and re-usable by external packages.\r\n\r\nThen once we have some way to run Python code inside the browser from a Python script (or pytest) on the host, the question remains how to best run the full scikit-learn test suite. The problem is that when running `pytest.main` over the full package directly it takes a while and no feedback is reported to the user until the run completes. Furthermore, if there is a fatal error in scipy somewhere (similar to a segfault in terms of outcome) then the whole session would crash. So it's probably better to run pytest inside WASM on smaller chunks, serialize back the results and concatenate them on the host. A  bit similar things about which I was wondering in https://github.com/pytest-dev/pytest-xdist/issues/336 as in the end the problem is very similar to running pytest on the remote node (except that communication is not happening over the network).\r\n\r\nIn any case, if anyone is interested in investigating this, I'd be happy to talk more about it."
      },
      {
        "user": "ogrisel",
        "body": "Thanks for the summary, I agree with your plan."
      }
    ]
  },
  {
    "issue_number": 31475,
    "title": "MultiOutputRegressor can't process estimators with synchronization primitives",
    "author": "Closius",
    "state": "open",
    "created_at": "2025-06-03T10:30:37Z",
    "updated_at": "2025-06-10T13:07:23Z",
    "labels": [
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\n[MultiOutputRegressor ](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html) can't process estimators with threading/multiprocessing synchronization primitives\n\nI want to propagate stop_event to the callback of regressor. I think the issue is because MultiOutputRegressor is trying to pickle each of estimator to move it to another thread/process. And if the estimator contains any synchronization primitives - they can't be pickled, so it fails. Maybe the solution might be to allow to provide pre-created estimators (for each of output) and provide them to the init of MultiOutputRegressor.\n\nI need to use MultiOutputRegressor because I need to export XGBoost model into onnx with a help of [skl2onnx](https://onnx.ai/sklearn-onnx/). If I don't use MultiOutputRegressor  - skl2onnx doesn't allow me to export, despite XGBoost has an [experimental way of multiple outputs](https://xgboost.readthedocs.io/en/stable/tutorials/multioutput.html).\n\nOr maybe I missed something. Please help.\n\n\nPackages:\n\n```\nxgboost                   3.0.0\n```\n\n### Steps/Code to Reproduce\n\n```python\nfrom threading import Event\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\nfrom sklearn.multioutput import MultiOutputRegressor\n\nfrom xgboost import XGBRegressor, Booster\nfrom xgboost import callback as xgb_callbacks\n\n\nclass Callback(xgb_callbacks.TrainingCallback):\n    def __init__(self, stop_event: Event):\n        super().__init__()\n        self.stop_event = stop_event\n\n    def after_iteration(self, model: Booster, epoch: int, evals_log: dict[str, dict]) -> bool:\n        print(f\"xgboost training: epoch {epoch}, evals_log {evals_log}\")\n        return False\n\n\ndef train_xgboost(X_train, y_train):\n    stop_event = Event()\n\n    base_model = XGBRegressor(n_estimators=45, callbacks=[Callback(stop_event)])\n    model = MultiOutputRegressor(base_model)\n    # base_model.callbacks = [Callback(stop_event)]\n\n    eval_set = []\n    for i in range(y_train.shape[1]):\n        y = y_train[:, i]\n        eval_set.append((X_train, y))\n\n    model.fit(X=X_train, y=y_train, eval_set=eval_set, verbose=True)\n\n    return model\n\n\ndef get_data():\n    X, y = make_regression(n_samples=100,\n                           n_features=5,\n                           n_informative=2,\n                           n_targets=3,\n                           noise=10.0,\n                           random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n    return X_train, y_train, X_test, y_test\n\ndef main():\n    X_train, y_train, X_test, y_test = get_data()\n    ml_model = train_xgboost(X_train, y_train)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Expected Results\n\nNo errors\n\n### Actual Results\n\n```\nTraceback (most recent call last):\n  File \"D:\\my_test\\issue_pickl.py\", line 54, in <module>\n    main()\n  File \"D:\\my_test\\issue_pickl.py\", line 50, in main\n    ml_model = train_xgboost(X_train, y_train)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\issue_pickl.py\", line 33, in train_xgboost\n    model.fit(X=X_train, y=y_train, eval_set=eval_set, verbose=True)\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\multioutput.py\", line 274, in fit\n    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 77, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\joblib\\parallel.py\", line 1918, in __call__\n    return output if self.return_generator else list(output)\n                                                ^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\joblib\\parallel.py\", line 1847, in _get_sequential_output\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\multioutput.py\", line 59, in _fit_estimator\n    estimator = clone(estimator)\n                ^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\base.py\", line 94, in clone\n    return estimator.__sklearn_clone__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\base.py\", line 300, in __sklearn_clone__\n    return _clone_parametrized(self)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\base.py\", line 127, in _clone_parametrized\n    new_object_params[name] = clone(param, safe=False)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\base.py\", line 95, in clone\n    return _clone_parametrized(estimator, safe=safe)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\base.py\", line 105, in _clone_parametrized\n    return estimator_type([clone(e, safe=safe) for e in estimator])\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\base.py\", line 105, in <listcomp>\n    return estimator_type([clone(e, safe=safe) for e in estimator])\n                           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\base.py\", line 95, in clone\n    return _clone_parametrized(estimator, safe=safe)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\base.py\", line 108, in _clone_parametrized\n    return copy.deepcopy(estimator)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 172, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 271, in _reconstruct\n    state = deepcopy(state, memo)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 146, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 231, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 172, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 271, in _reconstruct\n    state = deepcopy(state, memo)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 146, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 231, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 172, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 271, in _reconstruct\n    state = deepcopy(state, memo)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 146, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 231, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\my_test\\sys\\win64\\python\\Lib\\copy.py\", line 161, in deepcopy\n    rv = reductor(4)\n         ^^^^^^^^^^^\nTypeError: cannot pickle '_thread.lock' object\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\nexecutable: D:\\my_test\\sys\\win64\\python\\python.exe\n   machine: Windows-10-10.0.22631-SP0\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0.1\n   setuptools: 65.5.0\n        numpy: 1.26.4\n        scipy: 1.15.2\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.1\n       joblib: 1.4.2\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 32\n       filepath: D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\n        version: None\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 24\n         prefix: libopenblas\n       filepath: D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\numpy.libs\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll\n        version: 0.3.23.dev\nthreading_layer: pthreads\n   architecture: Prescott\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 24\n         prefix: libscipy_openblas\n       filepath: D:\\my_test\\sys\\win64\\python\\Lib\\site-packages\\scipy.libs\\libscipy_openblas-f07f5a5d207a3a47104dca54d6d0c86a.dll\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n```",
    "comments": [
      {
        "user": "betatim",
        "body": "Thanks for opening this. One assumption about \"scikit-learn compatible estimators\" is that they are `clone()`able. This eventually ends up calling `copy.deepcopy()` which uses pickle to try and make a copy of a `Event`. And it all falls apart.\n\nI'm not sure what we could (easily) do for this. While you could maybe work around it in this specific case, the more fundamental problem that the estimator you are using isn't `clone`able would remain. You'd possible run into the same problem somewhere else/scikit-learn would grow a bunch of locations where this works and others where it doesn't."
      },
      {
        "user": "Closius",
        "body": "I think it shouldn't be an issue for the clonable estimator. Because the usage of callbacks are quite common. Maybe there there might be the option to reuse the synchronization primitives in all cloned estimators during the clone (deepcopy) operation. \n\nI overcome this by training estimators and then monkeypatch MultiOutputRegressor vars that are populated in fit() - then the export to onnx is working perfectly. So, the improvements might be to manage estimators manually, outside MultiOutputRegressor and provide them to MultiOutputRegressor in init or via the additional constructor \n\nBtw, predict() is also not working with not clonable estimator. \n\nnot related to this issue: And I would add the capability to save/loade the model to MultiOutputRegressor. For example the save() method could save all estimators in a separate folder and zip into archive. That would be quite handy"
      },
      {
        "user": "lesteve",
        "body": "Not sure we can do something about it, as Tim said, \"scikit-learn compatible estimators\" needs to be clonable.\n\nMaybe you can try to explain your use case a bit more? In particular can you give more details about your motivation behind having a `threading.Event` in your xgboost callback?\n\nGeneric advice for a possible work-arounds, I would try to see whether custom pickling could help here, e.g. implementing `__getstate__`/`__setstate__` or `__reduce__`. Maybe implementing custom cloning through `__sklearn_clone__` could help as well."
      }
    ]
  },
  {
    "issue_number": 30826,
    "title": "DOC Donating to the project",
    "author": "reshamas",
    "state": "open",
    "created_at": "2025-02-13T17:13:24Z",
    "updated_at": "2025-06-10T11:47:31Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the issue linked to the documentation\n\nFor discussion.\nUpdating this page: https://scikit-learn.org/stable/about.html#donating-to-the-project\n\nInclude option(s) for various donation links (in addition to directly via NF), such as GitHub Sponsors and Benevity, OC.\n\n### Suggest a potential alternative/fix\n\n_No response_",
    "comments": [
      {
        "user": "dhruuvd-1704",
        "body": "Hi @reshamas,\nI saw this issue about updating the donation page to include alternative methods like GitHub Sponsors, Benevity, and Open Collective. I'd love to contribute to this as my first issue!\n\nWould it be possible to assign this to me? I can also explore alternative solutions if needed and suggest the best approach. Since this is my first contribution, I might need some guidance, but Iâ€™ll make sure to follow the contributing guidelines.\n\nLooking forward to your thoughts!\nThanks!\n\n"
      },
      {
        "user": "NANTEGEBABRAH",
        "body": "Iâ€™d love to contribute by improving the donation documentation to make it clearer and more accessible. A well-structured guide can help increase financial support for the project.\n\nI have experience in writing clear documentation and would focus on; Explaining why donations matter, Providing step-by-step instructions and Adding FAQs for common concerns.\nCould you assign me   @reshamas  Iâ€™d be happy to help!"
      },
      {
        "user": "reshamas",
        "body": "I will work on remaining text to address this issue. "
      }
    ]
  },
  {
    "issue_number": 22178,
    "title": "power_t: does it make sense for this parameter to have negative values",
    "author": "reshamas",
    "state": "closed",
    "created_at": "2022-01-10T19:07:21Z",
    "updated_at": "2025-06-10T10:55:31Z",
    "labels": [
      "Bug",
      "help wanted",
      "module:linear_model"
    ],
    "body": "sklearn/linear_model/_stochastic_gradient.py\r\n\r\nRef #22115\r\n\r\n>I do not anticipate negative `power_t` to be mathematically meaningful but apparently our code accepts it without crashing... So ok with documenting it.\r\n\r\n_Originally posted by @ogrisel in https://github.com/scikit-learn/scikit-learn/pull/22115#r780109928_\r\n\r\n@thomasjpfan \r\n>I feel like this a case where documenting `-inf` will lead to more people trying out. If this is not mathematically meaningful, then we could be promoting a bad practice?\r\n\r\ncc:  @glemaitre",
    "comments": [
      {
        "user": "urbanophile",
        "body": "My $0.02 is that this should be restricted to (0, inf). I have only heard of people wanting to increase the learning rate when doing something like cosine annealing, a linear warmup and decay or a cyclic learning rate for much more complicated deep neural networks. In the context of a linear classifier, I think you want the whole method to be pretty well-behaved, and the algorithm will not converge if the learning rate increases. \r\n\r\nSo, I'm happy to change this and submit a pull request if other people are agreeable. "
      },
      {
        "user": "lorentzenchr",
        "body": "Yes, we should limit `power_t` to [0, infinity)."
      },
      {
        "user": "ritvi-alagusankar",
        "body": "@lorentzenchr I've opened #31474 to address this issue. This is my first contribution, so please let me know if any changes are needed.\n"
      }
    ]
  },
  {
    "issue_number": 31512,
    "title": "Add free-threading wheel for Linux arm64 (aarch64)",
    "author": "MatthewSZhang",
    "state": "closed",
    "created_at": "2025-06-10T01:59:41Z",
    "updated_at": "2025-06-10T10:02:32Z",
    "labels": [
      "New Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nI am a maintainer for the third-party package [fastcan](https://github.com/scikit-learn-contrib/fastcan). I tested the package on the free-threading Python (cp313t), and found scikit-learn missing a wheel for Linux arm64 (aarch64) on PyPI.\n\nI would like to have the official release wheel rather than building it from source.\n\n### Describe your proposed solution\n\nI tested scikit-learn on my own fork, and the free-threading wheel for Linux arm64 (scikit_learn-1.8.dev0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl) can be successfully built. So I suppose that wheel is just mistakenly missed.\n\nJust add that wheel in wheel.yml should be fine.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "lesteve",
        "body": "numpy and scipy have a Linux arm64 free-threaded wheel so I guess it would make sense to have one for scikit-learn as well."
      }
    ]
  },
  {
    "issue_number": 31462,
    "title": "Feat: DummyClassifier strategy that produces randomized probabilities",
    "author": "tmcclintock",
    "state": "open",
    "created_at": "2025-06-01T17:27:18Z",
    "updated_at": "2025-06-08T15:06:16Z",
    "labels": [
      "New Feature",
      "Needs Decision - Include Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\n# Motivation\n\nThe `dummy` module is fantastic for testing pipelines all the way up through enterprise scales. The [strategies](https://github.com/scikit-learn/scikit-learn/blob/98ed9dc73/sklearn/dummy.py#L374) offered in the `DummyClassifier` are excellent for testing corner cases. However, the strategies offered fall short when testing pipelines that include downstream tasks that depend on moments of the predicted probabilities (e.g. gains charts).\n\nThis is because the existing strategies do not include sampling _random probabilities_.\n\n## Proposed API:\n\nConsider adding a new strategy with a name like `uniform-proba` or `score-random` or something similar that results in this behavior for binary classification:\n\n```python\nprint(DummyClassifier(strategy=\"uniform-proba\").fit(X, y).predict_proba(X))\n\"\"\"\n[[0.5651713  0.4348287 ]\n [0.36557341 0.63442659]\n [0.42386353 0.57613647]\n ...\n [0.30348692 0.69651308]\n [0.59589879 0.40410121]\n [0.32664176 0.67335824]]\n\"\"\"\n```\n\n### Describe your proposed solution\n\n## Proposed implementation\n\nI had something like this in mind:\n```python\nclass DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):\n    ...\n\n    def predict_proba(self, X):\n        ...\n        for k in range(self.n_outputs_):\n            if self._strategy == \"uniform-proba\":\n                out = rs.dirichlet([1] * n_classes_[k], size=n_samples)\n                out = out.astype(np.float64)\n            ...\n```\n\nSimilar to the `\"stratified\"` strategy, this simple implementation relies on `numpy.random`, in this case the [`dirichlet`](https://numpy.org/doc/2.0/reference/random/generated/numpy.random.RandomState.dirichlet.html) distribution. By setting all the `alpha`s to 1, we are specifying that the probabilities of each class are equally distributed -- in contrast, the `\"stratified\"` strategy effectively samples from a dirichlet distribution with one alpha equal to 1 and the rest equal to 0.\n\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\nI am happy to make the PR. The biggest question is what the strategy string should be.\n\nThank you for reading ðŸ™.",
    "comments": [
      {
        "user": "betatim",
        "body": "I think this could be useful. What to call the strategy and which strategy to use."
      },
      {
        "user": "tmcclintock",
        "body": "Thanks, @betatim. Do you recommend I create a PR or wait for more discussion?"
      },
      {
        "user": "betatim",
        "body": "To be honest, I don't know. If you are ok investing a bit of time to make a PR that would be good, though it could be wasted if people don't like the idea.\n\n@ogrisel do you have an opinion on this or know who we could ask?"
      }
    ]
  },
  {
    "issue_number": 19710,
    "title": "Introduce honest sampling into sklearn RandomForestClassifier",
    "author": "EYezerets",
    "state": "open",
    "created_at": "2021-03-18T16:40:22Z",
    "updated_at": "2025-06-08T08:45:30Z",
    "labels": [
      "New Feature",
      "module:ensemble",
      "module:tree"
    ],
    "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nHonest sampling in forests was first outlined by Breiman et al. 1984 in Classification and Regression Forests, in which it was suggested that random forests improve performance when the feature space is partitioned on one subset of samples, and the posteriors are estimated on a disjoint subset of the samples. This idea was revived by Denil et al. as structure vs. estimation points, and clarified and implemented by Wager and Athey. They identified several benefits of honest sampling: reduced bias, centered confidence intervals, reduced mean squared error, and the possibility of building causal forests.\r\n\r\nFrom Wager and Athey 2018 (section 2.4: Honest Trees and Forests):\r\n\r\n> In our discussion so far, we have emphasized the flexible nature of our results: for a wide variety of causal forests that can be tailored to the application area, we achieve both consistency and centered asymptotic normality, provided the subsample size s scales at an appropriate rate. Our results do, however, require the individual trees to satisfy a fairly strong condition, which we call honesty: a tree is honest if, for each training example i, it only uses the response Yi to estimate the within-leaf treatment effect Ï„ using (5) or to decide where to place the splits, but not both. \r\n\r\nIn addition, Wager and Athey note that subsampling does not \"waste\" training data:\r\n\r\n> However, in our case, the forest subsampling mechanism enables us to achieve honesty without wasting any data in this sense, because we rerandomize the I/J-data splits over each subsample. Thus, although no data point can be used for split selection and leaf estimation in a single tree, each data point will participate in both I and J samples of some trees, and so will be used for both specifying the structure and treatment effect estimates of the forest. Although our original motivation for considering double-sample trees was to eliminate bias and thus enable centered confidence intervals, we find that in practice, double-sample trees can improve upon standard random forests in terms of mean-squared error as well.\r\n\r\n**Describe the solution youâ€™d like**\r\n\r\n\r\n[EconML](https://github.com/microsoft/EconML/) has forked scikit-learn to create honest trees and generalized random forests for causal questions. We intend, instead, to merge back into scikit-learn based on the insights from EconML in regression trees, while also building classification trees, with the ability to accept both dense and sparse data.\r\n\r\n\r\n\r\n_Key references:_\r\n\r\nBreiman L, Friedman J, Stone C, Olshen R. Classification and Regression Forests. 1984.\r\n\r\nDenil M, Matheson D, De Freitas N. Narrowing the Gap: Random Forests In Theory and In Practice. Proc 31st Int Conf Mach Learn. 2014; 665â€“673. Available: http://jmlr.org/proceedings/papers/v32/denil14.html\r\n\r\nWager S, Athey S. [Estimation and Inference of Heterogeneous Treatment Effects using Random Forests](https://arxiv.org/abs/1510.04342). J Am Stat Assoc. 2018;113: 1228â€“1242. doi:10.1080/01621459.2017.1319839\r\n\r\nGuo R, Mehta R, Arroyo J, Helm H, Shen C, Vogelstein JT. Estimating Information-Theoretic Quantities with Uncertainty Forests. 2019; 1â€“19. Available: http://arxiv.org/abs/1907.00325\r\n",
    "comments": [
      {
        "user": "EYezerets",
        "body": "Another idea I would like to incorporate is not just a parameter to turn honesty on and off, but also one to dictate how the data is split (not necessarily 50/50). "
      },
      {
        "user": "EYezerets",
        "body": "Could I please ask for some guidance on the optimal implementation of this for sklearn? I could either create a new type of base tree, or add quite a few new parameters to the existing one (not just honest = T/F but also keeping track of how the samples are split). Would either of those be preferred? A new honest tree would maybe clutter up the files a bit, but the changes to the existing architecture would be less invasive, perhaps. @cmarmo Could you or your colleagues please kindly advise me? Thank you in advance for your time!"
      },
      {
        "user": "lorentzenchr",
        "body": "@EYezerets Thanks you for proposing this interesting sampling idea for trees/forests (if it stems from Breiman, it's even quite old :smirk:) As a non-expert for trees and forests, I, personally, would like to see more comparisons with honest sampling on several real datasets in order to be convinced. My understanding of a forest is to have many overfitting decision trees.\r\nMaybe, the functionality for honest trees is already there if you first fit them on one \"split dataset\" (as usual) and then re-estimate the node values on a second \"value dataset\" (this is the tricky part)? \r\n\r\n> EconML has forked scikit-learn to create honest trees and generalized random forests for causal questions. We intend, instead, to merge back into scikit-learn based on the insights from EconML in regression trees\r\n\r\nWhat is your motivation and what are the benefits to *merge back* into scikit-learn?\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 31368,
    "title": "`_weighted_percentile` NaN handling with array API",
    "author": "lucyleeow",
    "state": "open",
    "created_at": "2025-05-15T12:10:19Z",
    "updated_at": "2025-06-08T06:09:23Z",
    "labels": [
      "Array API"
    ],
    "body": "There isn't *necessarily* anything to fix here, but I thought it would be useful to open this for documentation, at least.\n\n---\n\n`_weighted_percentile` added support for NaN in #29034 and support for array APIs in #29431.\n\nOur implementation relys on `sort` putting NaN values at the end:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/8cfc72b81f7f19a03b5316440efc7d6bebd3c27c/sklearn/utils/stats.py#L70-L74\n\nAFAICT (confirmed by @ev-br) array API specs do not specify how `sort` should handle NaN, which means it is left to individual packages to determine.\n\n* torch seems to follow numpy and sort NaN to the end (tested manually with `float('nan')` and `torch.nan`) but this is not mentioned in the [docs](https://docs.pytorch.org/docs/stable/generated/torch.sort.html). There is some discussion of ordering NaN as the largest value here: https://github.com/pytorch/pytorch/issues/46544#issuecomment-883356705 and a related issue about negative NaN here: https://github.com/pytorch/pytorch/issues/116567\n* CuPy seems to follow numpy behaviour as well (relevant issues: https://github.com/cupy/cupy/issues/3324, and they seem to have [tests](https://github.com/cupy/cupy/blob/66820586ee1c41013868a8de4977c84f29180bc8/tests/cupy_tests/sorting_tests/test_sort.py#L161) to check that their results are the same as numpy with nan sorting )\n\nAs everything works, I don't think we need to do anything here (especially as we ultimately want to drop maintaining our own quantile function), but just thought it would be useful to document.\n\ncc @StefanieSenger @ogrisel ",
    "comments": [
      {
        "user": "StefanieSenger",
        "body": "I think it would be nice to define a handling of NaNs for `xp.sort` (and other functions?) in the spec, in order to define the status quo as a standard (also for future array libraries that want to adopt the array api standard). Maybe we open an issue in the `data-apis` repo?"
      },
      {
        "user": "lucyleeow",
        "body": "Agreed, it's on my to-do list. The outcome may just be that the spec explicitly states that nans handling method is left to the package though."
      },
      {
        "user": "lucyleeow",
        "body": "So there is a note that states:\n\n> the sort order of NaNs and signed zeros is unspecified and thus implementation-dependent.\n\nsee: https://data-apis.org/array-api/latest/API_specification/sorting_functions.html\n\nSo while it is not guaranteed to work, I think our implementation is acceptable. Maybe it's just like our assumption that arrays are stored 'C' order."
      }
    ]
  },
  {
    "issue_number": 31498,
    "title": "Doc website incorrectly flags stable as unstable",
    "author": "GaelVaroquaux",
    "state": "closed",
    "created_at": "2025-06-06T09:06:38Z",
    "updated_at": "2025-06-06T09:20:18Z",
    "labels": [
      "Bug",
      "Needs Triage"
    ],
    "body": "### Describe the bug\n\nCurrent website gives:\n![Image](https://github.com/user-attachments/assets/78ec363e-92cf-4a3f-afc5-68639078d9b3)\n\nI tried having a look on how to fix this, but went in a rabbit hole that the version switcher is generated by \"list_versions.py\" in the circle-ci scripts and this exceeded the time that I have. IMHO, such automation is over-engineered and does not make things more reliable, as we are seeing currently\n\n### Steps/Code to Reproduce\n\nGo to https://scikit-learn.org/stable/\n\n### Expected Results\n\nNot having the banner on top\n\n### Actual Results\n\nThe banner of the top of the website displays\n\n### Versions\n\n```shell\nstable\n```",
    "comments": [
      {
        "user": "glemaitre",
        "body": "It looks like bad timing on our side sending the mail for the release: there is an extra CD on the repository of the documentation that needed a couple of more minutes to update the website.\n\nThe process is finished and the banner disappeared for 1.7 so everything should be fine."
      }
    ]
  },
  {
    "issue_number": 30917,
    "title": "DecisionTreeClassifier having unexpected behaviour with 'min_weight_fraction_leaf=0.5'",
    "author": "snath-xoc",
    "state": "closed",
    "created_at": "2025-02-28T11:07:09Z",
    "updated_at": "2025-06-04T15:11:25Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nWhen fitting DecisionTreeClassifier on a duplicated sample set (i.e. each sample repeated by two), the result is not the same as when fitting on the original sample set. This only happens for 'min_weight_fraction_leaf' specified as <0.5. This also effects ExtraTreesClassifier and ExtraTreeClassifier.\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy.stats import kstest\nimport numpy as np\n\nrng = np.random.RandomState(0)\n    \nn_samples = 20\nX = rng.rand(n_samples, n_samples * 2)\ny = rng.randint(0, 3, size=n_samples)\n\nX_repeated = np.repeat(X,2,axis=0)\ny_repeated = np.repeat(y,2)\n\npredictions = []\npredictions_dup = []\n\n## Fit estimator\nfor seed in range(100):\n    est = DecisionTreeClassifier(random_state=seed, max_features=0.5, min_weight_fraction_leaf=0.5).fit(X,y)\n    est_dup = DecisionTreeClassifier(random_state=seed, max_features=0.5, min_weight_fraction_leaf=0.5).fit(X_repeated,y_repeated)\n\n    ##Get predictions\n    predictions.append(est.predict_proba(X)[:,:-1])\n    predictions_dup.append(est_dup.predict_proba(X)[:,:-1])\n\npredictions = np.vstack(predictions)\npredictions_dup = np.vstack(predictions_dup)\n\nfor pred, pred_dup in (predictions.T,predictions_dup.T):\n    print(kstest(pred,pred_dup).pvalue)\n\n```\n\n### Expected Results\n\np-values are more than Ëœ0.05\n\n### Actual Results\n\n```\np-values = 2.0064970441275627e-69\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.4 | packaged by conda-forge | (main, Jun 17 2024, 10:13:44) [Clang 16.0.6 ]\nexecutable: /Users/shrutinath/micromamba/envs/scikit-learn/bin/python\n   machine: macOS-14.3-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.7.dev0\n          pip: 24.0\n   setuptools: 75.8.0\n        numpy: 2.0.0\n        scipy: 1.14.0\n       Cython: 3.0.10\n       pandas: 2.2.2\n   matplotlib: 3.9.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n...\n    num_threads: 8\n         prefix: libomp\n       filepath: /Users/shrutinath/micromamba/envs/scikit-learn/lib/libomp.dylib\n        version: None\nOutput is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n```",
    "comments": [
      {
        "user": "snath-xoc",
        "body": "This occurs for seed in range(300) as well with p-values of:\n```\n1.336814951659136e-178\n```"
      },
      {
        "user": "jeremiedbb",
        "body": "This is related to this RFC about sample weight invariance properties (https://github.com/scikit-learn/scikit-learn/issues/15657), and this case in particular as already been identified as problematic, see https://github.com/scikit-learn/scikit-learn/issues/15657#issuecomment-555615961.\n\nThis is another example of an hyper-parameter which depend on n_samples but makes things weird or not very user friendly if modified to depend on weight sum instead."
      },
      {
        "user": "pedroL0pes",
        "body": "/take"
      }
    ]
  },
  {
    "issue_number": 28928,
    "title": "Allow to use prefitted SelectFromModel in ColumnTransformer",
    "author": "NTSER",
    "state": "closed",
    "created_at": "2024-05-01T13:18:14Z",
    "updated_at": "2025-06-04T14:31:34Z",
    "labels": [
      "Enhancement"
    ],
    "body": "```python\r\nimport pandas as pd\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.feature_selection import SelectFromModel\r\n\r\niris = load_iris()\r\nX = pd.DataFrame(data=iris.data, columns=iris.feature_names)\r\ny = iris.target\r\n\r\nfeature_selection_cols = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)']\r\nclf = LogisticRegression(max_iter=1000)\r\nclf.fit(X[feature_selection_cols], y)\r\nct = ColumnTransformer(\r\n    [(\r\n        'SelectFromModel',\r\n        SelectFromModel(clf, prefit=True, max_features=2),\r\n        feature_selection_cols,\r\n    )],\r\n    remainder='passthrough',\r\n)\r\nct.fit(X, y)\r\n```\r\n\r\nyields:\r\n\r\n```python-traceback\r\nTraceback (most recent call last)\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py:349, in SelectFromModel.fit(self, X, y, **fit_params)\r\n    348 try:\r\n--> 349     check_is_fitted(self.estimator)\r\n    350 except NotFittedError as exc:\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1461, in check_is_fitted(estimator, attributes, msg, all_or_any)\r\n   1460 if not _is_fitted(estimator, attributes, all_or_any):\r\n-> 1461     raise NotFittedError(msg % {\"name\": type(estimator).__name__})\r\n\r\nNotFittedError: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nNotFittedError                            Traceback (most recent call last)\r\nCell In[1], line 22\r\n     13 clf.fit(X[feature_selection_cols], y)\r\n     14 ct = ColumnTransformer(\r\n     15     [(\r\n     16         'SelectFromModel',\r\n   (...)\r\n     20     remainder='passthrough',\r\n     21 )\r\n---> 22 ct.fit(X, y)\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:717, in ColumnTransformer.fit(self, X, y)\r\n    699 \"\"\"Fit all transformers using X.\r\n    700 \r\n    701 Parameters\r\n   (...)\r\n    713     This estimator.\r\n    714 \"\"\"\r\n    715 # we use fit_transform to make sure to set sparse_output_ (for which we\r\n    716 # need the transformed data) to have consistent output type in predict\r\n--> 717 self.fit_transform(X, y=y)\r\n    718 return self\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)\r\n    155 @wraps(f)\r\n    156 def wrapped(self, X, *args, **kwargs):\r\n--> 157     data_to_wrap = f(self, X, *args, **kwargs)\r\n    158     if isinstance(data_to_wrap, tuple):\r\n    159         # only wrap the first output for cross decomposition\r\n    160         return_tuple = (\r\n    161             _wrap_data_with_container(method, data_to_wrap[0], X, self),\r\n    162             *data_to_wrap[1:],\r\n    163         )\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\r\n   1145     estimator._validate_params()\r\n   1147 with config_context(\r\n   1148     skip_parameter_validation=(\r\n   1149         prefer_skip_nested_validation or global_skip_validation\r\n   1150     )\r\n   1151 ):\r\n-> 1152     return fit_method(estimator, *args, **kwargs)\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:754, in ColumnTransformer.fit_transform(self, X, y)\r\n    751 self._validate_column_callables(X)\r\n    752 self._validate_remainder(X)\r\n--> 754 result = self._fit_transform(X, y, _fit_transform_one)\r\n    756 if not result:\r\n    757     self._update_fitted_transformers([])\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:681, in ColumnTransformer._fit_transform(self, X, y, func, fitted, column_as_strings)\r\n    675 transformers = list(\r\n    676     self._iter(\r\n    677         fitted=fitted, replace_strings=True, column_as_strings=column_as_strings\r\n    678     )\r\n    679 )\r\n    680 try:\r\n--> 681     return Parallel(n_jobs=self.n_jobs)(\r\n    682         delayed(func)(\r\n    683             transformer=clone(trans) if not fitted else trans,\r\n    684             X=_safe_indexing(X, column, axis=1),\r\n    685             y=y,\r\n    686             weight=weight,\r\n    687             message_clsname=\"ColumnTransformer\",\r\n    688             message=self._log_message(name, idx, len(transformers)),\r\n    689         )\r\n    690         for idx, (name, trans, column, weight) in enumerate(transformers, 1)\r\n    691     )\r\n    692 except ValueError as e:\r\n    693     if \"Expected 2D array, got 1D array instead\" in str(e):\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:65, in Parallel.__call__(self, iterable)\r\n     60 config = get_config()\r\n     61 iterable_with_config = (\r\n     62     (_with_config(delayed_func, config), args, kwargs)\r\n     63     for delayed_func, args, kwargs in iterable\r\n     64 )\r\n---> 65 return super().__call__(iterable_with_config)\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1863, in Parallel.__call__(self, iterable)\r\n   1861     output = self._get_sequential_output(iterable)\r\n   1862     next(output)\r\n-> 1863     return output if self.return_generator else list(output)\r\n   1865 # Let's create an ID that uniquely identifies the current call. If the\r\n   1866 # call is interrupted early and that the same instance is immediately\r\n   1867 # re-used, this id will be used to prevent workers that were\r\n   1868 # concurrently finalizing a task from the previous call to run the\r\n   1869 # callback.\r\n   1870 with self._lock:\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1792, in Parallel._get_sequential_output(self, iterable)\r\n   1790 self.n_dispatched_batches += 1\r\n   1791 self.n_dispatched_tasks += 1\r\n-> 1792 res = func(*args, **kwargs)\r\n   1793 self.n_completed_tasks += 1\r\n   1794 self.print_progress()\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:127, in _FuncWrapper.__call__(self, *args, **kwargs)\r\n    125     config = {}\r\n    126 with config_context(**config):\r\n--> 127     return self.function(*args, **kwargs)\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:957, in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)\r\n    955 with _print_elapsed_time(message_clsname, message):\r\n    956     if hasattr(transformer, \"fit_transform\"):\r\n--> 957         res = transformer.fit_transform(X, y, **fit_params)\r\n    958     else:\r\n    959         res = transformer.fit(X, y, **fit_params).transform(X)\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)\r\n    155 @wraps(f)\r\n    156 def wrapped(self, X, *args, **kwargs):\r\n--> 157     data_to_wrap = f(self, X, *args, **kwargs)\r\n    158     if isinstance(data_to_wrap, tuple):\r\n    159         # only wrap the first output for cross decomposition\r\n    160         return_tuple = (\r\n    161             _wrap_data_with_container(method, data_to_wrap[0], X, self),\r\n    162             *data_to_wrap[1:],\r\n    163         )\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:919, in TransformerMixin.fit_transform(self, X, y, **fit_params)\r\n    916     return self.fit(X, **fit_params).transform(X)\r\n    917 else:\r\n    918     # fit method of arity 2 (supervised transformation)\r\n--> 919     return self.fit(X, y, **fit_params).transform(X)\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1152, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\r\n   1145     estimator._validate_params()\r\n   1147 with config_context(\r\n   1148     skip_parameter_validation=(\r\n   1149         prefer_skip_nested_validation or global_skip_validation\r\n   1150     )\r\n   1151 ):\r\n-> 1152     return fit_method(estimator, *args, **kwargs)\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py:351, in SelectFromModel.fit(self, X, y, **fit_params)\r\n    349         check_is_fitted(self.estimator)\r\n    350     except NotFittedError as exc:\r\n--> 351         raise NotFittedError(\r\n    352             \"When `prefit=True`, `estimator` is expected to be a fitted \"\r\n    353             \"estimator.\"\r\n    354         ) from exc\r\n    355     self.estimator_ = deepcopy(self.estimator)\r\n    356 else:\r\n\r\nNotFittedError: When `prefit=True`, `estimator` is expected to be a fitted estimator.\r\n```",
    "comments": [
      {
        "user": "glemaitre",
        "body": "`ColumnTransformer` clones the selector and thus the fitted model.\r\nSo this kind of duplicate from https://github.com/scikit-learn/scikit-learn/issues/8370"
      },
      {
        "user": "ogrisel",
        "body": "I checked an indeed this is a discrepancy with the behavior of pipelines that does not attempt to clone the passed estimators prior to fitting:\r\n\r\n```python\r\n>>> from sklearn.datasets import load_iris\r\n... from sklearn.linear_model import LogisticRegression\r\n... from sklearn.pipeline import make_pipeline\r\n... from sklearn.feature_selection import SelectFromModel\r\n... \r\n... iris = load_iris()\r\n... X = pd.DataFrame(data=iris.data, columns=iris.feature_names)\r\n... y = iris.target\r\n... \r\n... clf = LogisticRegression(max_iter=1000)\r\n... clf.fit(X, y)\r\n... \r\n... pipe = make_pipeline(SelectFromModel(clf, prefit=True, max_features=2)).fit(X, y)\r\n```"
      },
      {
        "user": "ogrisel",
        "body": "One option that would not imply modifying the cloning behavior of `ColumnTransformer` and other meta-estimators would be to define a custom `SelectFromModel.__sklearn_clone__` clone method that would skip the cloning of the underlying base estimator when `prefit=True` is passed to `SelectFromModel`.\r\n\r\nWe could probably do a similar thing for any meta-estimator that can be configured to operate on fitted base estimators."
      }
    ]
  },
  {
    "issue_number": 31443,
    "title": "Folder/Directory descriptions not present",
    "author": "hardik1408",
    "state": "closed",
    "created_at": "2025-05-28T07:46:08Z",
    "updated_at": "2025-06-04T14:04:09Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the issue linked to the documentation\n\nI was navigating through the codebase, trying to find source code for some algorithms. I noticed that there are no descriptions of files present within a folder, which would actually make it easier to navigate through the codebase. We can have a small readme file within folders which would describe what is present in that folder. \n\n### Suggest a potential alternative/fix\n\n_No response_",
    "comments": [
      {
        "user": "allmight05",
        "body": "Is it open? Can I take this PR?"
      },
      {
        "user": "HussainAther",
        "body": "This is a great point.  Iâ€™ve also found that navigating large codebases like scikit-learn can be challenging without brief descriptions or READMEs in subdirectories. Even a single sentence explaining the intent of each folder (e.g., algorithm families, utilities, tests) could go a long way for new contributors or those exploring the internals for the first time.\n\nIt might be helpful to prioritize documentation for the most commonly accessed directories first (like sklearn/linear_model/, sklearn/ensemble/, etc.), and even include guidance for contributors who want to help write these descriptions.\n\nDefinitely a solid suggestion that aligns well with improving dev onboarding and overall accessibility."
      },
      {
        "user": "hardik1408",
        "body": "Yeah exactly, that was the whole point. I am ready to work on this issue, if someone assigns me this"
      }
    ]
  },
  {
    "issue_number": 13339,
    "title": "Bad error messages in ClassifierChain on multioutput multiclass",
    "author": "amueller",
    "state": "open",
    "created_at": "2019-02-28T16:37:16Z",
    "updated_at": "2025-06-04T13:54:08Z",
    "labels": [
      "good first issue",
      "module:multioutput"
    ],
    "body": "trying to run classifier chain on a multiclass problem with a reshaped y (which is interpreted as multioutput multiclass) doesn't work (see #9245), which is fine. But the error messages are really bad making it hard to understand what's going on.\r\n\r\nWe should detect that we're trying to do multioutput multiclass instead of multilabel and give an informative error.",
    "comments": [
      {
        "user": "EdinCitaku",
        "body": "Hey Im pretty new, can I take a look at this and try to fix it?"
      },
      {
        "user": "jnothman",
        "body": "This should reproduce the issue:\r\n\r\n```py\r\nfrom sklearn.multioutput import ClassifierChain\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.datasets import make_classification\r\n\r\nXs, ys = zip(*(make_classification(random_state=i, n_classes=3, n_informative=3) for i in range(3)))\r\nX = np.hstack(Xs)\r\nY = np.transpose(ys)\r\nClassifierChain(LogisticRegression()).fit(X, Y).predict(X)\r\n```\r\n\r\nbut actually this is running without complaint for me!!\r\n\r\n@amueller, please provide a reproducible snippet??"
      },
      {
        "user": "amueller",
        "body": "ah, it's only ``decision_function`` that's broken.\r\nIf ``predict`` works, maybe fixing ``decision_function`` is reasonable?\r\nI didn't actually read #9245 in detail: it looks like the ``predict`` part is already done in master?\r\n\r\n\r\n```python\r\nfrom sklearn.multioutput import ClassifierChain\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.datasets import make_classification\r\nimport numpy as np\r\n\r\nXs, ys = zip(*(make_classification(random_state=i, n_classes=3, n_informative=3) for i in range(3)))\r\nX = np.hstack(Xs)\r\nY = np.transpose(ys)\r\nClassifierChain(LogisticRegression()).fit(X, Y).decision_function(X)\r\n```"
      }
    ]
  },
  {
    "issue_number": 30921,
    "title": "Persistent UserWarning about KMeans Memory Leak on Windows Despite Applying Suggested Fixes",
    "author": "rahimHub",
    "state": "closed",
    "created_at": "2025-03-01T19:34:29Z",
    "updated_at": "2025-06-04T13:47:34Z",
    "labels": [
      "Bug",
      "Needs Info"
    ],
    "body": "### Describe the bug\n\nIssue Description\nWhen running code involving GaussianMixture (or KMeans), a UserWarning about a known memory leak on Windows with MKL is raised, even after implementing the suggested workaround (OMP_NUM_THREADS=1 or 2). The warning persists across multiple environments and configurations, indicating the issue may require further investigation.\n\nWarning Message:\n\n```\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\nwarnings.warn(\n```\n\nSteps to Reproduce\n\n1-Code Example:\n\n```python\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\" # Also tested with \"2\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.mixture import GaussianMixture\n\n# Generate synthetic 3D data\nX, _ = make_blobs(n_samples=300, n_features=3, centers=3, random_state=42)\n\n# Train GMM model\ngmm = GaussianMixture(n_components=3, random_state=42)\ngmm.fit(X) # Warning triggered here\n```\n\n## Environment:\n\nOS: Windows 11\nPython: 3.10.12\nscikit-learn: 1.3.2\nnumpy: 1.26.0 (linked to MKL via Anaconda)\nInstallation Method: Anaconda (conda install scikit-learn).\n\n## Expected vs. Actual Behavior\nExpected: Setting OMP_NUM_THREADS should suppress the warning and resolve the memory leak.\n\nActual: The warning persists despite environment variable configurations, reinstalls, and thread-limiting methods.\n\n## Attempted Fixes\n\nSet OMP_NUM_THREADS=1 or 2 in code and system environment variables.\nLimited threads via threadpoolctl:\ncode:\n```python\nfrom threadpoolctl import threadpool_limits\nwith threadpool_limits(limits=1, user_api='blas'):\ngmm.fit(X)\n```\n\nReinstalled numpy and scipy with OpenBLAS instead of MKL.\nTested in fresh conda environments.\nUpdated all packages to latest versions.\nNone of these resolved the warning.\n\nAdditional Context:\nThe warning appears even when using GaussianMixture, which indirectly relies on KMeans-related code.\n\nThe issue is specific to Windows + MKL. No warnings on Linux/Mac.\n\nFull error log: [Attach log if available].\n\nQuestions for Maintainers:\nIs there a deeper configuration or bug causing this warning to persist?\nAre there alternative workarounds for Windows users?\nIs this issue being tracked in ongoing development?\nThank you for your time and support!\nLet me know if further details are needed.\n\n### Steps/Code to Reproduce\n\n```python\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Also tested with \"2\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.mixture import GaussianMixture\n\n# Generate synthetic 3D data\nX, _ = make_blobs(n_samples=300, n_features=3, centers=3, random_state=42)\n\n# Train GMM model\ngmm = GaussianMixture(n_components=3, random_state=42)\ngmm.fit(X)  # Warning triggered here\n```\n\n### Expected Results\n\n```\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n  warnings.warn(\n```\n\n### Actual Results\n\n```\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n  warnings.warn(\n```\n\n\n### Versions\n\n```shell\nscikit-learn: 1.3.2\n\nnumpy: 1.26.0 (linked to MKL via Anaconda)\n```",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Thanks for the report.\n\n> Reinstalled numpy and scipy with OpenBLAS instead of MKL.\n\nHave you checked the output of `python -c \"import sklearn; sklearn.show_version()\"` to check that there is no MKL linked into the Python process anymore?\n\nPlease include the full output of `sklearn.show_version()` in each environment in your report.\n\n\n> Are there alternative workarounds for Windows users?\n\nUsing OpenBLAS should fix the problem. See the precise conditions that lead to this warning to be raised in the source code:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/d0ee195cdc1e321ec1d094283aaa30fe061d9572/sklearn/cluster/_kmeans.py#L910-L928\n\nBTW, next time, please use markdown formatting to make the issue more readable (I edited it myself this time)."
      },
      {
        "user": "adrinjalali",
        "body": "Closing as lack of response from the OP."
      }
    ]
  },
  {
    "issue_number": 31274,
    "title": "Automatically move `y_true` to the same device and namespace as `y_pred` for metrics",
    "author": "lucyleeow",
    "state": "open",
    "created_at": "2025-04-30T05:41:14Z",
    "updated_at": "2025-06-04T13:40:06Z",
    "labels": [
      "API",
      "Array API"
    ],
    "body": "This is closely linked to #28668 but separate enough to warrant it's own issue (https://github.com/scikit-learn/scikit-learn/issues/28668#issuecomment-2814771519). This is mostly a summary of discussions so far. If we are happy with a decision, we can move to updating the documentation.\n\n---\n\nFor classification metrics to support array API, there is a problem in the case where `y_pred` is not in the same namespace/device as `y_true`.\n\n`y_pred` is likely to be the output of `predict_proba` or `decision_function` and would be in the same namespace/device as `X` (if we decide in #28668 that \"everything should follow X\").\n`y_true` could be an integer array or a numpy array or pandas series (this is pertinent as `y_true` may be string labels)\n\nMotivating use case:\n\nUsing e.g., `GridSearchCV` or `cross_validate` with a pipeline that moves `X` to GPU.\nConsider a pipeline like below (copied from https://github.com/scikit-learn/scikit-learn/issues/28668#issuecomment-2154958666): \n\n```python\npipeline = make_pipeline(\n   SomeDataFrameAwareFeatureExtractor(),\n   MoveFeaturesToPyTorch(device=\"cuda\"),\n   SomeArrayAPICapableClassifier(),\n)\n```\n\nPipelines do not ever touch `y` so we are not able to alter `y` within the pipeline.\nWe would need to pass a metric to `GridSearchCV` or `cross_validate`, which would be passed `y_true` and `y_pred` on different namespace / devices.\n\nThus the motivation to automatically move `y_true` to the same namespace / device as `y_pred`, in metrics functions.\n\n(Note another example is discussed in https://github.com/scikit-learn/scikit-learn/pull/30439#issuecomment-2531072292)\n\nAs it is more likely that `y_pred` is on GPU, `y_true` follow `y_pred` was slightly preferred over `y_pred` follows `y_true`. Computation wise, CPU vs GPU is probably similar for metrics like log-loss, but for metrics that require sorting (e.g., ROC AUC) GPU may be faster? (see https://github.com/scikit-learn/scikit-learn/pull/30439#issuecomment-2532238196 for more discussion on this point)\n\n\nQuestion for my own clarification, the main motivation is for usability, so the user does not have to manually convert `y_true` ? Would a helper function to help the user convert `y_true` to the correct namespace/device be interesting?\n\n\ncc @ogrisel @betatim ",
    "comments": [
      {
        "user": "ogrisel",
        "body": "+1 for this proposal and +1 for a PR to make that policy explicit in `array_api.rst`."
      },
      {
        "user": "ogrisel",
        "body": "> Question for my own clarification, the main motivation is for usability, so the user does not have to manually convert y_true ?\n> Would a helper function to help the user convert y_true to the correct namespace/device be interesting?\n\nNot just this. Some pipelines would not be implementable otherwise. For instance, let's consider the following:\n\n```python\ndf = pd.read_parquet(\"data_file.parquet\")\nX = df[categorical_feature_names]  # categorical or str/object dtype\ny = df[target_column_name]  # continuous value to regress.\n\npipeline = make_pipeline(\n   TargetEncoder(),  # works on categorical X but also needs y, hence CPU for both\n   FunctionTransformer(partial(torch.asarray, device=\"cuda:0\")),\n   Ridge(),  # only numerical inputs: faster to do it all on GPU, y follows X => y_pred on GPU.\n)\n\ncv_results = cross_validate(pipeline, X, y, scoring=\"r2\")\n\n# Or similarly to grid search hyper parameters of `TargetEncoder` and `Ridge` jointly.\n```\n\n> Would a helper function to help the user convert y_true to the correct namespace/device be interesting?\n\nI don't think it's possible to write a helper to deal with the above use case: `y` needs to be on CPU for `TargetEncoder` hence when passed to `cross_validate`. Note that `TargetEncoder` is a supervised transformer: it consumes `y` but does not transform it (our pipelines do not allow transforming `y`).\n\n`Ridge` works much faster when fed numerical `X` on GPU and `y` follows `X` means that `y_pred` would naturally be on the GPU. Then the scoring function internally called by `cross_validate` or `GridSearchCV` needs to accept mixed inputs and implement `y_true` follows `y_pred`."
      },
      {
        "user": "lucyleeow",
        "body": "Thanks @ogrisel I had not considered that.\n\nI didn't attend the meeting but are we mostly in consensus about this? Happy to move forward with doc + implementation if so."
      }
    ]
  },
  {
    "issue_number": 31391,
    "title": "Avoid bundling tests in wheels",
    "author": "stefan6419846",
    "state": "closed",
    "created_at": "2025-05-19T13:55:41Z",
    "updated_at": "2025-06-04T13:35:14Z",
    "labels": [
      "Needs Decision"
    ],
    "body": "### Describe the bug\n\nThe wheels currently include tests and test data. These usually are of no additional value outside of the source distributions and thus just bloat the distribution and complicate reviews. For this reasons, I recommend excluding them from future wheels.\n\nThis matches the official recommendation for Python packaging as well (see https://packaging.python.org/en/latest/discussions/package-formats/#what-is-a-wheel):\n\n> Wheels are meant to contain exactly what is to be installed, and nothing more. In particular, wheels should never include tests and documentation, while sdists commonly do.\n\n### Steps/Code to Reproduce\n\nDownload the current wheels and look for `sklearn/datasets/tests`\n\n### Expected Results\n\nThe directory is absent.\n\n### Actual Results\n\nThe directory exists.\n\n### Versions\n\n```shell\n1.6.1\n```",
    "comments": [
      {
        "user": "lesteve",
        "body": "Could you add a bit more details why this is causing you any issue? Do you have an environment where additional size really matters? When you say it \"complicate reviews\", I guess you mean some kind of compliance review (basing this guess on a few issues you recently opened)?\n\nIt looks like you care about this and have [opened issues in many projects](https://github.com/search?q=author%3Astefan6419846+bundling&type=issues) so I would be curious to know more.\n\nAs a maintainer, I know I have found it convenient in the past to be able to run the tests against an installed wheel. For example upload to test.pypi.org install from test.pypi.org and double-check everything was fine by running `pytest --pyargs <my-package>`. Not sure how you would run the tests against an installed wheel if we were to remove the tests from the wheel. \n\nI quickly double-checked and it seems like numpy and scipy also still have tests in their wheels, although there has been some work for making it easier to produce wheels without tests: https://github.com/numpy/numpy/issues/25737.\n\nI am marking this as \"Needs Decision\" for now.\n"
      },
      {
        "user": "stefan6419846",
        "body": "> Could you add a bit more details why this is causing you any issue? Do you have an environment where additional size really matters? When you say it \"complicate reviews\", I guess you mean some kind of compliance review (basing this guess on a few issues you recently opened)?\n\nI do indeed care about this, yes. In this specific case, I am working on license compliance reviews for possibly integrating this into our (commercial) products. This involves a semi-automated process of basically reviewing every single file.\n\nFor packages with plain Python code and good test coverage, tests tend to be more than 50% of the actual source code. From time to time, especially test data seems to have dubious origins or other issues which complicate clean distribution. Avoiding bundling the tests inside the wheels helps cutting down the amount of work and the storage required for this. At the same time, this aligns with the PyPA recommendations on wheel builds. Usually, I as the final user do not want to run the library tests as this is part of the upstream process, although there are no warranties of course - I usually have my own integration and unit tests ensuring that updates do not break my code.\n\nPlease note that the aforementioned statistics do not hold true for *scikit-learn* directly. Some rough statistics show 459 test files (6 MB) and 517 regular files (37 MB), although you should take this with some caution due to additional files being created by unpacking archives for example. *scipy* and *numpy* are further candidates where I did not yet open a corresponding issue - their (unpacked) numbers are 1181 test files out of 1948 total files and 514 test files out of 1022 files in the current sample."
      },
      {
        "user": "betatim",
        "body": "We (e.g. the [cuml](https://github.com/rapids/cuml) project) rely on the bundled tests and find it useful that all you have to do is to install the version of scikit-learn you want to test against. We could of course checkout the right version from the git repository, etc but it introduces several points of failure where you could end up with tests that are from a different version than the version you have installed."
      }
    ]
  },
  {
    "issue_number": 31423,
    "title": "The libomp.dylib shipped with the macOS x86_64 package does not have an SDK version set",
    "author": "gernophil",
    "state": "closed",
    "created_at": "2025-05-24T23:02:36Z",
    "updated_at": "2025-06-04T13:31:29Z",
    "labels": [
      "Bug",
      "Needs Triage"
    ],
    "body": "### Describe the bug\n\nI want to build an macOS app that uses scikit-learn as a dependency. Using the arm64 package of scikit-learn for this works flawlessly. However, if I want to do the same using the macOS x86_64 packages Apple's notarizing step always breaks the app. This is likely due to the shipped libomp.dylib in the x86_64 package (installed using pip) does not have an SDK version set:\n```\notool -l ./venv/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib | grep -A 3 LC_VERSION_MIN_MACOSX\n      cmd LC_VERSION_MIN_MACOSX\n  cmdsize 16\n  version 10.9\n      sdk n/a\n```\nThe arm64 version has this set:\n```\notool -l ./venv/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib | grep -A 4 LC_BUILD_VERSION\n      cmd LC_BUILD_VERSION\n  cmdsize 32\n platform 1\n    minos 11.0\n      sdk 11.0\n```\nIt would be great, if you could set this (to at least 10.9; would probably need a rebuild of the dylib from source). I already tried some workarounds, but so far none have been successful. Is there any chance you would consider that :)?\n\n### Steps/Code to Reproduce\n\n```\n% otool -l ./venv/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib | grep -A 3 LC_VERSION_MIN_MACOSX\n      cmd LC_VERSION_MIN_MACOSX\n```\n\n### Expected Results\n\n```\n  cmdsize 16\n  version 10.9\n      sdk 10.9\n```\n\n### Actual Results\n\n```\n  cmdsize 16\n  version 10.9\n      sdk n/a\n```\n\n### Versions\n\n```shell\nscikit-learn==1.6.1 (from pip freeze)\n```",
    "comments": [
      {
        "user": "SumitkCodes",
        "body": "Iâ€™d like to work on this issue and attempt to fix the missing SDK version in the x86_64 libomp.dylib. Iâ€™ll investigate how to ensure the library is built with the correct SDK version so that notarization passes.\n\nPlease let me know if anyone is already working on this, or if there are any specific guidelines I should follow while contributing.\n\nThanks!"
      },
      {
        "user": "gernophil",
        "body": "I had some discussions with the devs from PyInstaller about this. Sounded like they have some experience what's necessary and what's the culprits of not having it in there. Maybe get in touch with them :). "
      },
      {
        "user": "betatim",
        "body": "I'm not sure if that file is built from scratch as part of the scikit-learn build. I suspect not though. This means asking \"more upstream\" is probably the right thing to do.\n\nPlease re-open if my assumption is wrong and it is indeed built from source as part of scikit-learn's build process"
      }
    ]
  },
  {
    "issue_number": 31415,
    "title": "Discrepancy between output of classifier feature_importances_ with different sklearn installations",
    "author": "adamwitmer",
    "state": "closed",
    "created_at": "2025-05-22T23:58:23Z",
    "updated_at": "2025-06-04T13:23:10Z",
    "labels": [
      "Bug",
      "Needs Triage"
    ],
    "body": "### Describe the bug\n\nI am currently using `scikit-learn` classifier `feature_importances_` attribute on a project to rank important features from my model, and my `CI` pipeline runs the project test-suite using instances of `scikit-learn==1.3.2` and `scikit-learn==1.5.2` on a remote linux host. I am experiencing some discrepancies in the output of the relevant test (for which I have provided a minimal viable reproducer below) on different machines/installations/sklearn versions. \n\nThere are a few specific problems I am experiencing:\n\n1. Locally, the test will pass using a binary installation of `scikit-learn==1.3.2` and fail using `scikit-learn==1.5.2`. With the help of my team, we have traced this error back and found the earliest failing version to be `1.4.1.post1`.  We suspect that the error originates from a change made in https://github.com/scikit-learn/scikit-learn/pull/27639 that has to do with the switch from absolute counts to store proportions in `tree_.values` but have not determined a root cause for the discrepancy.\n2. As mentioned in (1) when running the test-suite locally on my `Mac-ARM64` machine, the test will fail as described, however, when running the test on a remote linux machine, the test will pass with both sklearn versions\n3. The test will fail when I build the code from source vs. from the binary distribution of `scikit-learn==1.3.2`\n\nMy main question is, what could be the cause of these observed discrepancies between sklearn version, installation type and environment and which output is most \"correct\"?\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\nfrom pandas.testing import assert_frame_equal\nimport pdb\n\n\n# this test serves as a minimal viable reproducer for the\n# difference observed in output of tree values between\n# sklearn versions 1.3.2 and 1.4.2. this test should pass\n# when using sklearn==1.3.2 and fail when using sklearn==1.4.2\n\n# first create a minimal dataset of values for training\n# a Random Forest Classifier\nrandom_state = 123\nrng = np.random.default_rng(random_state)\nX = rng.integers(0, 2, size=(1000, 12))\ny = np.asarray([1, 0] * 500)\ny[-1] = 1\n\nX[:, 0] = y\nX[:, 1] = 1 - y\nclfr = RandomForestClassifier(n_estimators=100, random_state=random_state)\nclfr.fit(X, y)\n\n# find the importances of the estimator and check that ranking of the importances\nimportances_out = clfr.feature_importances_\n# these are the importance values that are expected from ``sklearn==1.3.2``\nimportances_exp = np.array(\n    [\n        0.52090464,\n        0.46263368,\n        0.00115268,\n        0.00179985,\n        0.00177495,\n        0.00169134,\n        0.00157653,\n        0.00135364,\n        0.00175814,\n        0.00169148,\n        0.00162767,\n        0.00203539,\n    ]\n)\nimportances_out = pd.DataFrame(importances_out).sort_values(by=0)\nimportances_exp = pd.DataFrame(importances_exp).sort_values(by=0)\n\nassert_frame_equal(importances_out, importances_exp)\n```\n\n\n### Expected Results\n\nThe expected values above, i.e. `importances_exp` represent the ranked feature importances of a \"cooked\" dataset where the input to the model is an array of random values, except two rows, which are perfectly (inversely) correlated to the target values `y`.  As we expect, the two highly correlated values show the highest importance and the random values show the lowest importance. The test checks that the ranking of the input values is correct by comparing the `DataFrames` storing the sorted output values from `clfr.feature_importances_`.\n\n### Actual Results\n\nThe expected output above, which comes from `feature_importances_` when using `scikit-learn==1.3.2` differs by some floating point values from the output  when using `scikit-learn>=1.4.2`, i.e.:\n\n```python\narray([[0.52087185],\n       [0.46270245],\n       [0.00203539],\n       [0.00179985],\n       [0.00178587],\n       [0.00177495],\n       [0.00169148],\n       [0.00169134],\n       [0.00159994],\n       [0.00155658],\n       [0.00135364],\n       [0.00113665]])\n```\n\nwhere the ranking of the values is changed by the discrepancy between floating point values of the lower ranked features:\n\n```bash\nDataFrame.index values are different (16.66667 %)\n[left]:  Index([2, 7, 6, 10, 5, 9, 4, 8, 3, 11, 1, 0], dtype='int64')\n[right]: Index([2, 7, 6, 10, 5, 9, 8, 4, 3, 11, 1, 0], dtype='int64')\nAt positional index 6, first diff: 4 != 8\n```\n\n### Versions\n\n```shell\nOn `Mac-arm64` with `scikit-learn==1.3.2`:\n\nSystem:\n    python: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\nexecutable: /Users/awitmer/venv_7/bin/python\n   machine: macOS-14.7.5-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.3.2\n          pip: 23.3.1\n   setuptools: 69.0.2\n        numpy: 1.25.0\n        scipy: 1.14.0\n       Cython: None\n       pandas: 2.0.2\n   matplotlib: 3.7.1\n       joblib: 1.2.0\nthreadpoolctl: 3.1.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n         prefix: libomp\n       filepath: /Users/awitmer/venv_7/lib/python3.11/site-packages/sklearn/.dylibs/libomp.dylib\n        version: None\n    num_threads: 12\n\n       user_api: blas\n   internal_api: openblas\n         prefix: libopenblas\n       filepath: /Users/awitmer/venv_7/lib/python3.11/site-packages/numpy/.dylibs/libopenblas64_.0.dylib\n        version: 0.3.23\nthreading_layer: pthreads\n   architecture: armv8\n    num_threads: 12\n\nOn `Mac-arm64` with `scikit-learn==1.5.2`:\n\nSystem:\n    python: 3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\nexecutable: /Users/awitmer/venv_7/bin/python\n   machine: macOS-14.7.5-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.5.2\n          pip: 23.3.1\n   setuptools: 69.0.2\n        numpy: 1.25.0\n        scipy: 1.14.0\n       Cython: None\n       pandas: 2.0.2\n   matplotlib: 3.7.1\n       joblib: 1.2.0\nthreadpoolctl: 3.1.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n         prefix: libopenblas\n       filepath: /Users/awitmer/venv_7/lib/python3.11/site-packages/numpy/.dylibs/libopenblas64_.0.dylib\n        version: 0.3.23\nthreading_layer: pthreads\n   architecture: armv8\n    num_threads: 12\n\n       user_api: openmp\n   internal_api: openmp\n         prefix: libomp\n       filepath: /Users/awitmer/venv_7/lib/python3.11/site-packages/sklearn/.dylibs/libomp.dylib\n        version: None\n    num_threads: 12\n\nOn linux based server with `scikit-learn==1.3.2`:\n\nSystem:\n    python: 3.11.11 (main, Dec  9 2024, 15:32:27) [GCC 8.5.0 20210514 (Red Hat 8.5.0-22)]\nexecutable: /usr/bin/python3.11\n   machine: Linux-4.18.0-553.47.1.el8_10.x86_64-x86_64-with-glibc2.28\n\nPython dependencies:\n      sklearn: 1.3.2\n          pip: 25.0.1\n   setuptools: 65.5.1\n        numpy: 1.26.4\n        scipy: 1.15.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.9.0\n       joblib: 1.2.0\nthreadpoolctl: 3.1.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n         prefix: libgomp\n       filepath: /vast/home/awitmer/.local/lib/python3.11/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n        version: None\n    num_threads: 40\n\n       user_api: blas\n   internal_api: openblas\n         prefix: libopenblas\n       filepath: /vast/home/awitmer/.local/lib/python3.11/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so\n        version: 0.3.23.dev\nthreading_layer: pthreads\n   architecture: Haswell\n    num_threads: 40\n```",
    "comments": [
      {
        "user": "tylerjereddy",
        "body": "I don't know if `sklearn` can actually guarantee stability here--if I change the `random_state` of the estimator only (i.e., use the same input data, just change the estimator seed), the number of index mismatches for the importances is often quite high (almost all of the random data), whether I use Mac or Linux on the provided reproducer.\n\nThat somewhat matches the intuition that if the randomly-permuted feature data is all roughly equally unimportant for deciding the response, the tree algorithm doesn't really have a sane way to distinguish their relative importances consistently.\n\nIt may be nice to have a way to bisect this as we discussed--that is currently being hindered by the source builds differing from the binaries.\n\nMy immediate intuition is that this could mean that the in-house test may be trying to enforce something that isn't formally guaranteed to be stable in practice (the feature rankings of very noisy/random features).\n\nMaybe @thomasjpfan or @virchan might have an opinion.\n\n"
      },
      {
        "user": "virchan",
        "body": "I ran the Code-to-Reproduce using `scikit-learn 1.3.2` and `1.5.2`, with the following library versions fixed:\n\n```console\nPython: 3.11.7\nnumpy: 1.25.0\nscipy: 1.14.0\npandas: 2.0.2\n```\n\nThe final line\n\n```python\nassert_frame_equal(importances_out, importances_exp)\n```\n\npassed successfully in both cases. So, from what I can tell, the difference in scikit-learn versions doesn't seem to be the cause of the issue.\n\nLet me know if you get a different result in this setup, @adamwitmer.\n\nI haven't yet tested the following combination:\n\n```console\nPython: 3.11.11\nnumpy: 1.26.4\nscipy: 1.15.1\npandas: 2.2.3\n```"
      },
      {
        "user": "adrinjalali",
        "body": "We certainly don't support comparing such values across versions.\n\nAnd if you look at it, you can consider the disparity coming from floating point disparities. The actual difference in the feature importance values are minute.\n\nAs for which one is \"more correct\", we hope the latest release of course. That's why things change, people report issues, or we find them, and we fix them. That very often results in getting different results if you compare versions."
      }
    ]
  },
  {
    "issue_number": 31408,
    "title": "estimator_checks_generator does not return (estimator, check) when hitting an expected failed check",
    "author": "Remi-Gau",
    "state": "closed",
    "created_at": "2025-05-21T14:15:00Z",
    "updated_at": "2025-06-04T12:17:22Z",
    "labels": [
      "Bug",
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nCurrently running sklearn_check_generator with mark=\"skip\" on our estimators.\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_checks.estimator_checks_generator.html\n\nI would like to start running those checks with \"xfail\".\n\nBut when I do so any test 'marked' via the `expected_failed_checks` dictionary gives a \n`ValueError: too many values to unpack (expected 2)`\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.utils.estimator_checks import estimator_checks_generator\nfrom sklearn.base import BaseEstimator\n\nexpected_failed_checks = {\"check_complex_data\": \"some reason\"}\n\nestimator = BaseEstimator()\n\n# fine\nfor e, check in estimator_checks_generator(\n                estimator=estimator,\n                expected_failed_checks=expected_failed_checks, \n                mark=\"skip\"\n            ):\n    print(check)\n\n# error\nfor e, check in estimator_checks_generator(\n                estimator=estimator,\n                expected_failed_checks=expected_failed_checks, \n                mark=\"xfail\"\n            ):\n    print(check)\n```\n\n### Expected Results\n\nestimator_checks_generator to yield (estimator, check) tuples whether xfail or skip was requested\n\n### Actual Results\n\n```\nfunctools.partial(<function check_estimator_cloneable at 0x7ec1883e5120>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_cloneable at 0x7ec1883e5120>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_tags_renamed at 0x7ec1883e62a0>, 'BaseEstimator')\nfunctools.partial(<function check_valid_tag_types at 0x7ec1883e6200>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_repr at 0x7ec1883e51c0>, 'BaseEstimator')\nfunctools.partial(<function check_no_attributes_set_in_init at 0x7ec1883e4b80>, 'BaseEstimator')\nfunctools.partial(<function check_fit_score_takes_y at 0x7ec1883da0c0>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_overwrite_params at 0x7ec1883e4a40>, 'BaseEstimator')\nfunctools.partial(<function check_dont_overwrite_parameters at 0x7ec1883d8e00>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_fit_returns_self at 0x7ec1883dbb00>, 'BaseEstimator')\nfunctools.partial(<function check_readonly_memmap_input at 0x7ec1883dbc40>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_unfitted at 0x7ec1883dbd80>, 'BaseEstimator')\nfunctools.partial(<function check_do_not_raise_errors_in_init_or_set_params at 0x7ec1883e6d40>, 'BaseEstimator')\nfunctools.partial(<function check_n_features_in_after_fitting at 0x7ec1883e6160>, 'BaseEstimator')\nfunctools.partial(<function check_mixin_order at 0x7ec1883d9f80>, 'BaseEstimator')\nfunctools.partial(<function check_positive_only_tag_during_fit at 0x7ec1883e54e0>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_dtypes at 0x7ec1883da200>, 'BaseEstimator')\n<function _maybe_mark.<locals>.wrapped at 0x7ec20dbda2a0>\nfunctools.partial(<function check_dtype_object at 0x7ec1883d8a40>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_empty_data_messages at 0x7ec1883da3e0>, 'BaseEstimator')\nfunctools.partial(<function check_pipeline_consistency at 0x7ec1883d9e40>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_nan_inf at 0x7ec1883da520>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_sparse_tag at 0x7ec1883bbe20>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_sparse_array at 0x7ec1883d8040>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_sparse_matrix at 0x7ec1883bbf60>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_pickle at 0x7ec1883da7a0>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_pickle at 0x7ec1883da7a0>, 'BaseEstimator', readonly_memmap=True)\nfunctools.partial(<function check_f_contiguous_array_estimator at 0x7ec1883d80e0>, 'BaseEstimator')\nfunctools.partial(<function check_parameters_default_constructible at 0x7ec1883e5260>, 'BaseEstimator')\nfunctools.partial(<function check_methods_sample_order_invariance at 0x7ec1883d9260>, 'BaseEstimator')\nfunctools.partial(<function check_methods_subset_invariance at 0x7ec1883d9120>, 'BaseEstimator')\nfunctools.partial(<function check_fit2d_1sample at 0x7ec1883d9440>, 'BaseEstimator')\nfunctools.partial(<function check_fit2d_1feature at 0x7ec1883d9620>, 'BaseEstimator')\nfunctools.partial(<function check_get_params_invariance at 0x7ec1883e58a0>, 'BaseEstimator')\nfunctools.partial(<function check_set_params at 0x7ec1883e59e0>, 'BaseEstimator')\nfunctools.partial(<function check_dict_unchanged at 0x7ec1883d8c20>, 'BaseEstimator')\nfunctools.partial(<function check_fit_idempotent at 0x7ec1883e5e40>, 'BaseEstimator')\nfunctools.partial(<function check_fit_check_is_fitted at 0x7ec1883e5ee0>, 'BaseEstimator')\nfunctools.partial(<function check_n_features_in at 0x7ec1883e5f80>, 'BaseEstimator')\nfunctools.partial(<function check_fit1d at 0x7ec1883d9760>, 'BaseEstimator')\nfunctools.partial(<function check_fit2d_predict1d at 0x7ec1883d8f40>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_cloneable at 0x7ec1883e5120>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_cloneable at 0x7ec1883e5120>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_tags_renamed at 0x7ec1883e62a0>, 'BaseEstimator')\nfunctools.partial(<function check_valid_tag_types at 0x7ec1883e6200>, 'BaseEstimator')\nfunctools.partial(<function check_estimator_repr at 0x7ec1883e51c0>, 'BaseEstimator')\nfunctools.partial(<function check_no_attributes_set_in_init at 0x7ec1883e4b80>, 'BaseEstimator')\nfunctools.partial(<function check_fit_score_takes_y at 0x7ec1883da0c0>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_overwrite_params at 0x7ec1883e4a40>, 'BaseEstimator')\nfunctools.partial(<function check_dont_overwrite_parameters at 0x7ec1883d8e00>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_fit_returns_self at 0x7ec1883dbb00>, 'BaseEstimator')\nfunctools.partial(<function check_readonly_memmap_input at 0x7ec1883dbc40>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_unfitted at 0x7ec1883dbd80>, 'BaseEstimator')\nfunctools.partial(<function check_do_not_raise_errors_in_init_or_set_params at 0x7ec1883e6d40>, 'BaseEstimator')\nfunctools.partial(<function check_n_features_in_after_fitting at 0x7ec1883e6160>, 'BaseEstimator')\nfunctools.partial(<function check_mixin_order at 0x7ec1883d9f80>, 'BaseEstimator')\nfunctools.partial(<function check_positive_only_tag_during_fit at 0x7ec1883e54e0>, 'BaseEstimator')\nfunctools.partial(<function check_estimators_dtypes at 0x7ec1883da200>, 'BaseEstimator')\nTraceback (most recent call last):\n  File \"/home/remi-gau/github/nilearn/nilearn/tmp/sk.py\", line 17, in <module>\n    for e, check in estimator_checks_generator(\n        ^^^^^^^^\nValueError: too many values to unpack (expected 2)\n```\n\n### Versions\n\n```shell\nchecked from 1.6.0 to 1.7.0rc1\n```",
    "comments": [
      {
        "user": "betatim",
        "body": "I can reproduce this. Taking a look"
      },
      {
        "user": "betatim",
        "body": "Could you explain a bit what you are trying to do? Or asked differently: how did you end up using `estimator_checks_generator`?\n\nAfter thinking about this for a bit I think `estimator_checks_generator` should really only be used to construct things like [`parametrize_with_checks`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_checks.parametrize_with_checks.html) or `check_estimator()` but not by itself. This means we should update the docstring to point people to those two as well as fix the return value.\n\nThe reason your example fails is because `estimator_checks_generator` will return a `pytest.param` when `mark=\"xfail\"` in order to have the test marked as XFAIL within pytest. This means it doesn't (always) return `(estimator, check)` tuples. I investigated if we could use a different approach but couldn't find a simple way that would replicate the pytest xfail behaviour. In particular when you have a test that is marked as \"expected to fail\" but that passes. Which makes me think that is best used for building things like `parametrize_with_checks` instead of explicit `for` loops."
      },
      {
        "user": "Remi-Gau",
        "body": "Currently this is how I use it:\n\nhttps://github.com/nilearn/nilearn/blob/c95672634b52f789fec1003142d29fab100c10c6/nilearn/_utils/estimator_checks.py#L258-L317\n\n```python\n        if SKLEARN_GT_1_5:\n            tags = est.__sklearn_tags__()\n\n\n            niimg_input = getattr(tags.input_tags, \"niimg_like\", False)\n            surf_img = getattr(tags.input_tags, \"surf_img\", False)\n\n\n            if niimg_input or surf_img:\n                if expected_failed_checks is None:\n                    expected_failed_checks = CHECKS_TO_SKIP_IF_IMG_INPUT\n                else:\n                    expected_failed_checks |= CHECKS_TO_SKIP_IF_IMG_INPUT\n\n\n            for e, check in sklearn_check_generator(\n                estimator=est,\n                expected_failed_checks=expected_failed_checks,\n                # TODO use  mark=\"xfail\"\n                # once using only expected_failed_checks and no valid_checks\n                mark=\"skip\",\n            ):\n                # DANGER\n                # must rely on sklearn private function _check_name\n                # to get name of the check:\n                # things may break with no deprecation warning\n                name = _check_name(check)\n\n\n                if valid and name in valid_checks:\n                    yield e, check, name\n                if not valid and name not in valid_checks:\n                    yield e, check, name\n\n\n        else:\n            for e, check in sklearn_check_estimator(\n                estimator=est, generate_only=True\n            ):\n                tags = est._more_tags()\n\n\n                niimg_input = \"niimg_like\" in tags[\"X_types\"]\n                surf_img = \"surf_img\" in tags[\"X_types\"]\n\n\n                if niimg_input or surf_img:\n                    if expected_failed_checks is None:\n                        expected_failed_checks = CHECKS_TO_SKIP_IF_IMG_INPUT\n                    else:\n                        expected_failed_checks |= CHECKS_TO_SKIP_IF_IMG_INPUT\n\n\n                if (\n                    isinstance(expected_failed_checks, dict)\n                    and check.func.__name__ in expected_failed_checks\n                ):\n                    continue\n\n\n                if valid and check.func.__name__ in valid_checks:\n                    yield e, check, check.func.__name__\n                if not valid and check.func.__name__ not in valid_checks:\n                    yield e, check, check.func.__name__\n\n\n    if valid:\n        for est in estimator:\n            for e, check in nilearn_check_estimator(estimator=est):\n                yield e, check, check.__name__\n```\n\nPart of the issue is that we are running test with the latest sklearn that supports expected_failed_checks, but also with older versions (>=1.4.0) that do not support this, originally I hacked something to handle this.\n\nI am not using sklearn check_estimator directly or its decorator in my tests, but am just generating the checks and running them \"myself\".\n\nFor example:\n\nhttps://github.com/nilearn/nilearn/blob/c95672634b52f789fec1003142d29fab100c10c6/nilearn/maskers/tests/test_nifti_masker.py#L26-L48\n\n```python\n@pytest.mark.parametrize(\n    \"estimator, check, name\",\n    check_estimator(\n        estimator=[NiftiMasker()],\n        expected_failed_checks=expected_failed_checks_0pt13pt2(),\n    ),\n)\ndef test_check_estimator(estimator, check, name):  # noqa: ARG001\n    \"\"\"Check compliance with sklearn estimators.\"\"\"\n    check(estimator)\n\n\n\n\n@pytest.mark.xfail(reason=\"invalid checks should fail\")\n@pytest.mark.parametrize(\n    \"estimator, check, name\",\n    check_estimator(\n        estimator=[NiftiMasker()],\n        valid=False,\n    ),\n)\ndef test_check_estimator_invalid(estimator, check, name):  # noqa: ARG001\n    \"\"\"Check compliance with sklearn estimators.\"\"\"\n    check(estimator)\n```\n\nThis setup allowed me to have a single place where I checked the sklearn version rather having to do that for every estimator that I test.\n\nI will try find a workaround, though any suggestion is welcome."
      }
    ]
  },
  {
    "issue_number": 30832,
    "title": "Numpy Array Error when Training MultioutputClassifer with LogisticRegressionCV with classes underrepresented",
    "author": "lionelkusch",
    "state": "open",
    "created_at": "2025-02-14T10:34:16Z",
    "updated_at": "2025-06-04T11:55:51Z",
    "labels": [
      "Bug",
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nWhen I train the MultioutputClassifer with LogisticRegressionCV with classes underrepresented, I get the following numpy error.\nI think this is connected to the issue #28178 and #26401.\n\n### Steps/Code to Reproduce\n\n```python\nimport sklearn\nprint(sklearn.__version__)\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.multioutput import MultiOutputClassifier\nimport numpy as np\n\n\nn, m = 20, 5\nmodel = MultiOutputClassifier(LogisticRegressionCV())\nX = np.random.randn(n, m)\ny = np.concatenate([[np.random.randint(0, 2, n),\n                     np.random.randint(0, 5, n)]], axis=0).T\ny[-3:, 0] = [3, 4, 5]\nmodel.fit(X, y)\n```\n\n### Expected Results\n\n1.6.1\n\n### Actual Results\n\n1.6.1\n\n```pytb\n.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"error_skitlearn.py\", line 14, in <module>\n    model.fit(X, y)\n  File \".venv/lib/python3.12/site-packages/sklearn/multioutput.py\", line 543, in fit\n    super().fit(X, Y, sample_weight=sample_weight, **fit_params)\n  File \".venv/lib/python3.12/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/sklearn/multioutput.py\", line 274, in fit\n    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/joblib/parallel.py\", line 1918, in __call__\n    return output if self.return_generator else list(output)\n                                                ^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n    estimator.fit(X, y, **fit_params)\n  File \".venv/lib/python3.12/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 2038, in fit\n    coefs_paths = np.reshape(\n                  ^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py\", line 324, in reshape\n    return _wrapfunc(a, 'reshape', shape, order=order)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py\", line 54, in _wrapfunc\n    return _wrapit(obj, method, *args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py\", line 42, in _wrapit\n    conv = _array_converter(obj)\n           ^^^^^^^^^^^^^^^^^^^^^\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0]\nexecutable: .venv/bin/python\n   machine: Linux-6.8.0-52-generic-x86_64-with-glibc2.39\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0.1\n   setuptools: 75.8.0\n        numpy: 2.2.3\n        scipy: 1.15.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: .venv/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-6bb31eeb.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: .venv/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-68440149.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libgomp\n       filepath: .venv/lib/python3.12/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n        version: None\n```",
    "comments": [
      {
        "user": "ogrisel",
        "body": "The warning is informative:\n\n```\n.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n```\n\nThe coefficients are not aligned because some cross-validation estimators do not see all the classes at training time as others.\n\nWe can probably try to make `LogisticRegressionCV` robust to this case. If not, we should at least detect this problem earlier in fit and raise a more meaningful exception.\n\nNote that the problem is not related to `MultioutputClassifier` and can directly be reproduced with:\n\n```python\nfrom sklearn.linear_model import LogisticRegressionCV\nimport numpy as np\n\nn, m = 20, 5\nX = np.random.randn(n, m)\ny = np.random.randint(0, 2, n)\ny[-3:] = [3, 4, 5]\nLogisticRegressionCV().fit(X, y)\n```\n\n```\n/Users/ogrisel/code/scikit-learn/sklearn/model_selection/_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n  warnings.warn(\n/Users/ogrisel/code/scikit-learn/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/ogrisel/code/scikit-learn/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\nTraceback (most recent call last):\n  Cell In[4], line 8\n    LogisticRegressionCV().fit(X, y)\n  File ~/code/scikit-learn/sklearn/base.py:1389 in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File ~/code/scikit-learn/sklearn/linear_model/_logistic.py:2049 in fit\n    coefs_paths = np.reshape(\n  File ~/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:328 in reshape\n    return _wrapfunc(a, 'reshape', shape, order=order)\n  File ~/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:54 in _wrapfunc\n    return _wrapit(obj, method, *args, **kwds)\n  File ~/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:42 in _wrapit\n    conv = _array_converter(obj)\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.\n```\n"
      },
      {
        "user": "nunofbiomartins",
        "body": "/take"
      },
      {
        "user": "nunofbiomartins",
        "body": "Hello @ogrisel and everyone. I'm working on this issue, and I'm trying to understand it the best way so I can fix it.\n\nDo you think that increasing the dataset by repeating the data in each class until the class with the minimum number of samples reaches the number of splits is a bad idea?\n\nExample:\n\nold_dataset\n[[ 0.12, -0.43],\n [ 1.23,  0.56],\n [-0.34,  0.78],\n [ 0.90, -1.56],\n [-0.56,  0.34]]\n\nold_classes\n[0, 1, 0, 1, 3]        --> Class 3 only has 1 sample ( Let's say we set the **n_splits** to **2** )\n\n_* Some code to repeat the the dataset and classes *_\n\nnew_dataset\n[[ 0.12, -0.43],\n [ 1.23,  0.56],\n [-0.34,  0.78],\n [ 0.90, -1.56],\n [-0.56,  0.34],\n [ 0.12, -0.43],\n [ 1.23,  0.56],\n [-0.34,  0.78],\n [ 0.90, -1.56],\n [-0.56,  0.34]]\n\nnew_classes\n[0, 1, 0, 1, 3, 0, 1, 0, 1, 3]        --> Now, the number of samples in every class is **greater than or equal to n_splits**.\n\nEven though it might lead to some overfitting, it is still better than getting a ValueError, right?\nAre there any other tips someone can give me to help me fix this bug?\n\nAny thoughts or tips will be helpful. \nI will keep working on this.\nThank you a lot!\n\n\n\n"
      }
    ]
  },
  {
    "issue_number": 31133,
    "title": "Add sankey style confusion matrix visualization",
    "author": "qmarcou",
    "state": "open",
    "created_at": "2025-04-02T14:02:28Z",
    "updated_at": "2025-06-04T00:49:21Z",
    "labels": [
      "New Feature",
      "Needs Decision - Include Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nConfusion matrices can be displayed as a colored matrix using the [ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay) class.\n\n![confusion_matrix](https://scikit-learn.org/stable/_images/sphx_glr_plot_label_propagation_digits_001.png)\n\nHowever color scaling is hard to interpret, and spatial cues are easier to help interpret quantities than color variations. \nThe number represented in each box represent absolute ones, while one may be interested in either row based, column based, or complete matrix sum based normalization. \nPlus it gets even harder to read for multiclass classification.  \n\n### Describe your proposed solution\n\nI would propose introducing sankey like plots to visualize the confusion matrix data, using a `ConfusionMatrixSankeyDisplay` class.\nThe qualitative information is displayed by different colors (easy to interpret), while actual amounts in each cell are represented by the size of the flows (easier to interpret quantitatively than colorscale variations).\nOn the left size one can see the number of occurrence of each label in the ground truth data (confusion matrix row marginals). On the right side the number of occurrence of each label in the predictions (confusion matrix column marginals). Each flow represents both row-normalized (left side) and column-normalized (right side) at the same time, and could be labeled with the actual absolute number of examples in each confusion matrix cell.\nInterpretation is straightforward even in the multiclass case.\n\nThere exist a matplotlib based implementation doing almost exactly this in the small [pySankeyBeta](https://github.com/Pierre-Sassoulas/pySankey) package. Here is a sample from its readme:\n![sankey](https://github.com/Pierre-Sassoulas/pySankey/raw/main/.github/img/fruits.png)\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Thanks for opening this feature request. Here is some personal feedback:\n\n- I agree that a Sankey diagram is helps to visualize confusion between observed and predicted class membership, possibly easier to read than a confusion matrix, especially if classes are not balanced.\n- [pySankeyBeta](https://github.com/Pierre-Sassoulas/pySankey) is GPLv3 which is incompatible with the scikit-learn license. So if we decide to include such a feature in scikit-learn with cannot directly reuse this code without an explicit authorization of the copyright holder of `pySankeyBeta`.\n\nLet's see what other maintainers think about this proposal."
      },
      {
        "user": "qmarcou",
        "body": "Indeed I did not think about the license issue, good point! This might indeed be an issue since pySankeyBeta is a fork of a unmaintained package, and the original author's last activity on GH was in 2020...\nThere might be a way to rewrite something simpler using [matplotlib's Sankey class](https://matplotlib.org/stable/api/sankey_api.html), but I find the documentation a bit obscure."
      },
      {
        "user": "wndu123",
        "body": "Hello! I'd like to work on this. I'll propose a BSD-compatible implementation approach for ConfusionMatrixSankeyDisplay."
      }
    ]
  },
  {
    "issue_number": 29962,
    "title": "DOC merging the examples related to OPTICS, DBSCAN, and HDBSCAN",
    "author": "glemaitre",
    "state": "open",
    "created_at": "2024-09-29T17:41:41Z",
    "updated_at": "2025-06-03T21:14:43Z",
    "labels": [
      "Documentation"
    ],
    "body": "As stated in https://github.com/scikit-learn/scikit-learn/issues/27151, it would be great to reduce the number of examples in the gallery.\r\n\r\nRight now, we have three examples for:\r\n\r\n- OPTICS: https://scikit-learn.org/dev/auto_examples/cluster/plot_optics.html#sphx-glr-auto-examples-cluster-plot-optics-py\r\n- DBSCAN: https://scikit-learn.org/dev/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py\r\n- HDBSCAN: https://scikit-learn.org/dev/auto_examples/cluster/plot_hdbscan.html#sphx-glr-auto-examples-cluster-plot-hdbscan-py\r\n\r\nThose clustering methods are really close to each others; some being an improvement from another one. Therefore, we could rework a single example that is not only a demo but rather show the pros & cons from each approach.",
    "comments": [
      {
        "user": "saldanhad",
        "body": "/take"
      },
      {
        "user": "mbedmutha",
        "body": "@saldanhad are you working on this? I'd be happy to help!"
      },
      {
        "user": "daustria",
        "body": "may i take this one? would love to try to streamline the examples here."
      }
    ]
  },
  {
    "issue_number": 29963,
    "title": "DOC rework the example presenting the regularization path of Lasso, Lasso-LARS, and Elastic Net",
    "author": "glemaitre",
    "state": "open",
    "created_at": "2024-09-29T17:45:13Z",
    "updated_at": "2025-06-03T17:10:37Z",
    "labels": [
      "Documentation"
    ],
    "body": "We recently merge two examples and the resulting example is shown here: https://scikit-learn.org/dev/auto_examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.html\r\n\r\nThis example should be revisited where we should have more narrative in a tutorial-like style. Indeed, this example could explain in more details what is a regularization path and discuss the difference between Lasso and Lasso-LARS, and between Lasso and ElasticNet.\r\n\r\nSome of the experiment are really closed to the one presented in this paper: https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf",
    "comments": [
      {
        "user": "virchan",
        "body": "I'd like to work on this issue, if it's available."
      },
      {
        "user": "Rachit23110261",
        "body": "@glemaitre I would like to ask for clarification regarding tutorial-like style. Do we have to explain the examples, like, explain how lasso -LARS works differently from lasso and elastic net regression through the example?"
      },
      {
        "user": "glemaitre",
        "body": "> Do we have to explain the examples, like, explain how lasso -LARS works differently from lasso and elastic net regression through the example?\r\n\r\nKind of. Basically, since we want to show differences through those plots, I think that we need to:\r\n\r\n- first, introduce what is a regularization path\r\n- then, explain the difference between Lasso and ElasticNet regarding the regularization and look at the path because we will have something along the same line\r\n- finally explain the difference between Lasso and Lasso-LARS, that is related to the solver, and how it impacts the path also."
      }
    ]
  },
  {
    "issue_number": 31473,
    "title": "Add option to return final cross-validation score in SequentialFeatureSelector",
    "author": "flaviorv",
    "state": "open",
    "created_at": "2025-06-02T23:20:53Z",
    "updated_at": "2025-06-03T09:08:55Z",
    "labels": [
      "New Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nCurrently, when using `SequentialFeatureSelector`, it internally performs cross-validation to decide which features to select, based on the scoring function. However, the final cross-validation score (e.g., recall) is not returned by the SFS object.\n\n\n\n### Describe your proposed solution\n\nAdd an attribute (e.g., `final_cv_score_`) that stores the mean cross-validation score of the final model with the selected features. This would avoid having to run another cross-validation externally to get the final performance score.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\nThis feature would be especially useful when the scoring metric is expensive to compute, as it would avoid redundant cross-validation runs.",
    "comments": [
      {
        "user": "manitejagaddam",
        "body": "Hey i gotch you, You need to calculate the final_cv_score_ in the SequentialFeatureSelector, so do you want the cv score for each epoch of the model or you need an entire cv score\n\nCase 1) If you want and final_cv_score__ for each epoch you need to modify the scklearn code and add the accuracy field in it.\n\nCase 2) If you want overall fianl_cv_score_ for entire model then use the accuracy_Score form the sklearn.metrics \nYou can get the predicted metrics from the method **get_support** the you can calulate the fianl_cv_score_\n\n\nIf you want modified SequentialFeatureSelector code with the field final_cv_score_ please check my github.\n"
      },
      {
        "user": "betatim",
        "body": "I think this sounds like a reasonable feature to add.\n\nI tried to look around existing estimators to see what name is used for the fitted parameter. In `TunedThresholdClassifier` and `GridSearchCV` it is called `best_score_`. On the one hand it would be nice to have something consistent, on the other hand maybe here it isn't really a \"best score\"? Maybe something to address in a discussion in a PR."
      }
    ]
  },
  {
    "issue_number": 31441,
    "title": "Regression error characteristic curve",
    "author": "alexshtf",
    "state": "closed",
    "created_at": "2025-05-28T05:40:22Z",
    "updated_at": "2025-06-02T21:28:09Z",
    "labels": [
      "New Feature",
      "Needs Triage"
    ],
    "body": "### Describe the workflow you want to enable\n\nAdd more fine-grained diagnostic, similar to ROC or Precision-Recall curves, to regression problems. It appears that this library has a lot of excellent tools for classification, and I believe it would benefit from some additional tools for regression.\n\n### Describe your proposed solution\n\nCompute Regression Error Characteristic (REC) [1] curve - for each error threshold the percentage of samples whose error is below that threshold. This is essentially the CDF of the regression errors. Its function is similar to that of ROC curves - allows comparing performance profiles of regressors beyond just one summary statistic, such as RMSE or MAE.\n\nI already implement a pull-request:\nhttps://github.com/scikit-learn/scikit-learn/pull/31380\n\nScreenshot from the merge request:\n\n![Image](https://github.com/user-attachments/assets/1974e8e7-03da-47c7-adb5-5c75eb24d61e)\n\nIf you believe this feature is useful, please help me with reviewing and merging it.\n\n### Describe alternatives you've considered, if relevant\n\nRegression Receiver Operating Characteristic (RROC) curves, proposed [2], which plot over-prediction vs under-prediction, are a different form of diagnostic curves for regression. They may also be useful, but I think we should begin from somewhere, and I belive it's better to begin from REC, both because the paper has more citations, and because it turned out to be very useful for me at work, and I believe it can be similarly useful to other scientists.\n\n### Additional context\n\n**References**\n---\n\n[1]: Bi, J. and Bennett, K.P., 2003. Regression error characteristic curves. In Proceedings of the 20th international conference on machine learning (ICML-03) (pp. 43-50).\n[2]: HernÃ¡ndez-Orallo, J., 2013. ROC curves for regression. Pattern Recognition, 46(12), pp.3395-3411.",
    "comments": [
      {
        "user": "HussainAther",
        "body": "This is a fantastic idea. thank you for bringing REC curves into the conversation!\n\nHaving more nuanced diagnostics for regression models has been a long-standing gap in the standard ML workflow. The analogy to ROC/PR curves is spot-on: one summary metric (e.g., MAE/RMSE) canâ€™t always capture performance tradeoffs, especially across varying error tolerances. The cumulative nature of the REC curve gives a much clearer picture of model robustness in practical contexts.\n\nAlso appreciate the inclusion of reference. Bi & Bennett (2003) is a solid foundation. And your submitted PR (#31380) looks very promising from a quick glance. Iâ€™d love to see this feature included in future versions of scikit-learn. It could really benefit both academic research and applied ML workflows.\n\nLooking forward to seeing how this progresses! Great work."
      },
      {
        "user": "lorentzenchr",
        "body": "@alexshtf Thanks for opening this issue.\n\nI am -1 on this feature for 2 reasons:\n- The proposed functionality is very easy to implement:\n  ```python\n   import matplotlib.pyplot as plt\n   from scipy.stats import ecdf\n   errors = my_error_function(y_obs, y_pred)\n   cdf_errors = ecdf(errors)\n   ax = plt.subplot()\n   cdf_errors.cdf.plot(ax)\n   plt.show()\n   ```\n- I am not convinced about the insight this generates. A much better diagnostic tool for regression in the CAP curve, see #10003.\n\nTherefore, I am closing this issue. But still, feel free to continue discussion."
      },
      {
        "user": "alexshtf",
        "body": "@lorentzenchr I believe ease of implementation is not that important - otherwise, why does scikit-learn have `PredictionErrorDisplay` visualization?\n\nSo I believe the only remaining issue are the insights. Regressors are used for many downstream tasks, not just \"let's predict a value and show it to the user\", i.e,. computing bids for ad auctions. In general, more accurate bids generate better revenue, but simple summary metrics don't always help you understand why a certain predictors behaves as it does. So I thought it might be useful and valuable for other people as well.\n\nIt appears you disagree on the diagnostic value, but it appears I should first dig deeper into CAP curves and see if they could provide similar value for the same use cases."
      }
    ]
  },
  {
    "issue_number": 31219,
    "title": "Add Categorical Feature Support to `IterativeImputer`",
    "author": "MohamedMostafa259",
    "state": "open",
    "created_at": "2025-04-17T12:24:59Z",
    "updated_at": "2025-06-02T15:41:17Z",
    "labels": [
      "New Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nI want to impute missing values in categorical columns using a similar approach to `IterativeImputer`, which currently works only for continuous data. Specifically, I want to enable the following workflow:\n\n- Identify and handle categorical columns in the dataset\n- Use classifier models (e.g., RandomForestClassifier) to impute missing values in categorical columns based on other features\n- Integrate with existing pipelines seamlessly, without needing to separate and impute categorical columns manually\n\n### Describe your proposed solution\n\nExtend the current `IterativeImputer` class (or create a new class, such as `IterativeCategoricalImputer`) to handle categorical data:\n\n- Detect categorical columns automatically (e.g., using `dtype='object'` or `category`) or accept them via a `categorical_features` parameter\n- Encode the categorical variables using an internal encoder (e.g., `LabelEncoder`)\n- Use a classifier model (e.g., `RandomForestClassifier`) instead of a regression model for those columns\n- Predict only the missing values, then inverse transform the predictions back to the original categories\n\nThis would enable more robust and automatic preprocessing for datasets that have numeric and categorical features.\n\n### Describe alternatives you've considered, if relevant\n\n- Manually encoding categorical variables, using a classifier-based imputation strategy outside of `IterativeImputer`\n- Using other libraries like `autoimpute` or `fancyimpute`, which support mixed-type imputation but lack full integration with Scikit-learn pipelines\n- Creating separate imputation steps for categorical and numeric features and merging them later, which adds complexity and can introduce data leakage risks\n\nNone of these are as clean or pipeline-friendly as a built-in solution.\n\n\n### Additional context\n\nThis feature would make `IterativeImputer` more powerful and suitable for real-world datasets that include both numeric and categorical variables with missing values.\n\nI'm currently working with datasets like this in my own data science projects and would be interested in contributing a prototype or proof-of-concept if this idea is accepted by the maintainers.",
    "comments": [
      {
        "user": "ogrisel",
        "body": "I agree that what you propose would be a great usability improvement and make this model much more practical and useful.\n\n> Encode the categorical variables using an internal encoder (e.g., LabelEncoder)\n> Use a classifier model (e.g., RandomForestClassifier) instead of a regression model for those columns\n> Predict only the missing values, then inverse transform the predictions back to the original categories\n\nI am not sure why you would need `LabelEncoder`-based preprocessing. `RandomForestClassifier` and all scikit-learn classifiers do that automatically internally if you fit with `str` object class labels for instance.\n\nHowever, I agree that `RandomForestClassifier` and the majority of scikit-learn estimator do not currently accept categorical inputs (yet) so we need to take care of this (see below).\n\nFor an implementation point of view, we might want to deprecate the `estimator` hyperparameter and related public fitted attributes in favor of a pair of `regressor` and `classifier` hyperparameter. We could use `RandomForestRegressor` and `RandomForestClassifier` by default for the sake of consistency.\n\nBut since they do not accept categorical inputs, a shared `preprocessor` argument.\n\nBy default, all of them could be left to `None` in which case, internally, we would have:\n\n```python\nclassifier = RandomForestClassifier(n_estimators=10, max_depth=5)\nregressor = RandomForestRegressor(n_estimators=10, max_depth=5)\npreprocessor = ColumnTransformer(\n    [\n        (\n            \"categorical\",\n            OrdinalEncoder(\n                handle_unknown=\"use_encoded_value\", unknown_value=-1\n            ),\n            categorical_features,\n         )\n    ],\n    remainder=\"passthrough\",\n)\n```\n\nOr alternatively, we could use the `TargetEncoder` instead of `OrdinalEncoder`, or the `FeatureUnion` of both together.\n\nThen the data would be `fit_transformed` by the shared `preprocessor` before being iteratively fed to the instances of `regressor` and `classifier` for each feature to impute.\n\nMaybe the default preprocessor could be in charge of including the `SimpleImputer` with strategy `\"most_frequent\"` for categorical variable and `\"mean\"` or `\"median\"` for continuous variables. We could then drop the `initial_strategy` parameter of the `IterativeImputer` imputer class since the users would have full flexibility to configure there own default imputation via the `preprocessor` argument.\n\n\nWhat is problematic:\n\n- the `sample_posterior` is not easy to adapt to `RandomForestRegressor` because this is a point-estimator of the conditional expectation `E[y|X]` rather than a probabilistic regressor. Since it's disabled by default, maybe this is fine: we can leave it to the user to explicitly pass probabilistic regressor if they way to use this option. I think this option is the reason why we use `BayesianRidge` as the default base regressor, though.\n- `n_nearest_features` does not support categorical features. Maybe we could raise `NotImplementedError` if enabled in the presence of non-continuous features.\n"
      },
      {
        "user": "ogrisel",
        "body": "Maybe @GaelVaroquaux has some opinions to share on this."
      },
      {
        "user": "MohamedMostafa259",
        "body": "Thanks so much for your response @ogrisel. I appreciate your comments and clarifications!\n\nYou were correct in mentioning that the LabelEncoder was not needed for target variables. I was thinking about being consistent across the whole pipeline and assumed it would be added. However, since classifiers encode string labels correctly as part of their work, I can remove that part entirely.\n\nI also really liked your suggestion of taking in separate classifier, regressor, and preprocessor parameters, which adds a lot of \nflexibility and reduces the complexity of the object coming in. \n\nYour comment about sample_posterior was right on point since it is off by default, I will put it in the documentation. I will also specify that if users need a probabilistic regressor then they should pass one. I will also add the logic to raise NotImplementedError when n_nearest_features is used when the data type is not continuous.\n\nWould it make sense to develop a prototype implementation of a new class called IterativeMixedDataImputer (mainly to not break the behavior of IterativeImputer) and if all goes well we can merge them together or refactor with that or possibly other classes?\n\nThanks again, and I would like to hear thoughts from @GaelVaroquaux."
      }
    ]
  },
  {
    "issue_number": 31360,
    "title": "Describe `set_{method}_request()` API, expose `_MetadataRequester`, or expose `_BaseScorer`",
    "author": "Jacob-Stevens-Haas",
    "state": "open",
    "created_at": "2025-05-13T10:14:39Z",
    "updated_at": "2025-06-01T10:36:29Z",
    "labels": [
      "Documentation",
      "Needs Investigation",
      "Metadata Routing"
    ],
    "body": "### Describe the issue linked to the documentation\n\nTL;DR: Metadata routing for scoring could either use a base class or documentation of how to write `set_score_request()`.\n\nCurrently the [Metadata Estimator Dev Guide](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_metadata_routing.html#metadata-routing) has examples of a metadata-consuming estimator and a metadata-routing estimator.  However, the metadata routing is also designed for scorers and CV splitters which may or may not be estimators.   Fortunately, `sklearn.model_selection` exposes `BaseCrossValidator`, which like `BaseEstimator`, subclasses `_MetadataRequester`.  Unfortunately, ~there's no base class for scorers.~ the base class for scorers, `_BaseScorer`, is not public.\n\nI don't understand how to string together the relevant methods that should be a part of `set_score_params`, The current workaround is to simply subclass `BaseEstimator`, even if I'm not making an estimator, or to subclass `_MetadataRequester`, even though its not part of the public API.  ~Or use `make_scorer` to pin the kwargs when instantiating the meta-estimator, rather than in `fit()`~\n\nMy use case is for scoring a time-series model where the data generating mechanism is known to the experiment, but not the model, and I need to compare the fit model to the true data generating mechanism.  I understand how to use a custom scorer in `RandomizedSearchCV`, and the [metadata API](https://scikit-learn.org/stable/metadata_routing.html#api-interface) explains how meta estimators like `RandomizedSearchCV` can pass additional arguments to my custom scorer.\n\n### Suggest a potential alternative/fix\n\n* publicly exposing `_MetadataRequester`\n* publicly exposing `_BaseScorer`\n* Document how to write the `set_{method}_request()` methods.  It looks like `_MetadataRequester` uses a descriptor `RequestMethod`, which relies on an instance having a `_get_metadata_request` method and a `_metadata_request` attribute (which it doesn't really describe).\n\nEDIT: Found out about `_BaseScorer`.  Also, removed `make_scorer` workaround, as it doesn't result in passing the fitted estimator to the `_score_func`",
    "comments": [
      {
        "user": "MagicDake",
        "body": "/take"
      },
      {
        "user": "betatim",
        "body": "@MagicDake thanks for your enthusiasm! However I am not sure this issue is yet ready for someone to go implement a solution. The suggested actions read (to me) like a list of alternatives - so the first step here is to have a discussion about the options and form a consensus. One way to do that is for you to describe how you understand the issue and its solution(s)."
      },
      {
        "user": "MagicDake",
        "body": "\nOkay, based on our deep dive into the source code, here's a more informed and detailed English reply you can post for `betatim`:\n\n---\n\nHi @betatim ,\n\nThanks for the guidance and for encouraging a thorough discussion before implementation. I completely agree that reaching a consensus on the approach is the best way forward.\n\nHaving looked into the relevant parts of the codebase (notably `sklearn/metrics/_scorer.py` and `sklearn/utils/_metadata_requests.py`), my understanding of the issue and potential solutions has been refined:\n\n**Understanding of the Issue:**\n\nThe core challenge is to provide a clear, public, and user-friendly way for custom scorers to participate in scikit-learn's metadata routing mechanism. Users, like Jacob-Stevens-Haas, find it difficult to make their scorers request and consume metadata (e.g., `sample_weight`, or other custom parameters) effectively.\n\nMy key takeaways from the source code are:\n1.  The internal `_BaseScorer` class (in `sklearn/metrics/_scorer.py`) **already inherits from `_MetadataRequester`**. This is a crucial piece of existing infrastructure, meaning `_BaseScorer` instances (and classes inheriting from it) inherently possess the `set_{method}_request()` methods, including `set_score_request()`.\n2.  Scorers created by `make_scorer` are instances of `_Scorer`, which itself inherits from `_BaseScorer`. Therefore, these scorer objects *also* have the `set_score_request()` capability.\n3.  The main gaps seem to be:\n    *   **Visibility/API:** `_BaseScorer` is internal.\n    *   **Documentation/Discoverability:** The fact that `make_scorer` objects can have their metadata requests configured via `set_score_request()` is not well-documented or obvious.\n    *   **Ease of Use for Complex Scorers:** While `make_scorer` is convenient for simple functions, creating more complex, stateful scorers that properly handle metadata routing by inheriting from an internal class is not a viable public pattern.\n\n**Proposed Solutions (Informed by Source Code):**\n\nGiven that `_BaseScorer` already has the foundational link to `_MetadataRequester`, the path forward might be less about building from scratch and more about exposing and refining existing mechanisms:\n\n1.  **Short-Term - Documentation & Clarification (Leveraging Existing Functionality):**\n    *   **Action:** Immediately improve the documentation to clearly state that scorer objects returned by `make_scorer` possess `set_score_request()` (and other `set_{method}_request()` methods) due to their inheritance chain (`_Scorer` -> `_BaseScorer` -> `_MetadataRequester`).\n    *   **Benefit:** This would provide an immediate, albeit perhaps not perfectly idiomatic, way for users to make their `make_scorer`-generated scorers metadata-aware. This directly addresses part of \"Describe set\\_{method}\\_request() API\".\n    *   **Example to add to docs:**\n        ```python\n        from sklearn.metrics import make_scorer, f1_score\n        # Assume my_complex_score_func uses sample_weight and custom_meta\n        scorer = make_scorer(my_complex_score_func)\n        scorer.set_score_request(sample_weight=True, custom_meta=True)\n        # ... then use scorer in GridSearchCV etc.\n        ```\n\n2.  **Medium-Term - Enhance `make_scorer` (Improved Convenience):**\n    *   **Action:** Add an optional parameter to `make_scorer`, perhaps `score_requests: dict = None`. If provided, `make_scorer` would internally call `set_score_request(**score_requests)` on the created scorer object.\n    *   **Benefit:** This makes the common case more straightforward without requiring a separate call after `make_scorer`.\n\n3.  **Longer-Term - Public `BaseScorer` (Robust Solution for Custom Scorers):**\n    *   **Action:** Refactor and publicize `sklearn.metrics._scorer._BaseScorer` into something like `sklearn.metrics.BaseScorer`.\n        *   It would continue to inherit from `sklearn.utils.metadata_routing.MetadataRequester` (which is publicly available as `sklearn.utils.metadata_routing.MetadataRequester`).\n        *   Review and refine its `__init__` method and the way it handles the `score_func` and `_score` method for easier subclassing. The goal is to allow users to primarily override an internal scoring method (like `_score`) and use the inherited `set_score_request()` for metadata.\n    *   **Documentation:** Provide clear examples of subclassing this public `BaseScorer`, implementing the scoring logic, and declaring metadata requirements.\n    *   **Benefit:** This directly addresses the \"make a base class for Scorers\" part of the issue title and provides a solid, object-oriented way for users to build complex, metadata-aware scorers. It also naturally explains how `set_score_request()` works in this context.\n    *   *Regarding \"expose `_MetadataRequester`\"*: `_MetadataRequester`'s functionality is best exposed *through* classes that inherit it (like `BaseEstimator` and this proposed public `BaseScorer`), rather than exposing it directly for all users, as it's more of a mixin/building block.\n\n**Path Forward:**\n\nI believe a combination of these, perhaps starting with improved documentation (1) and then working towards a public `BaseScorer` (3), would be a robust way to resolve this issue. Enhancing `make_scorer` (2) could be a valuable addition for convenience along the way.\n\nThe key is that much of the underlying machinery (thanks to `_BaseScorer` inheriting `_MetadataRequester`) seems to be in place, and the effort might be more focused on API design, public exposure, and documentation.\n\nI'm keen to discuss these points and any other perspectives the team has. My aim is to contribute to a solution that is both powerful and user-friendly.\n\nBest regards,\nMagicDake\n\n"
      }
    ]
  },
  {
    "issue_number": 10165,
    "title": "Scikit-learn doc could give more hints of it playing well with pandas",
    "author": "chrish42",
    "state": "closed",
    "created_at": "2017-11-18T01:34:38Z",
    "updated_at": "2025-05-31T13:25:37Z",
    "labels": [
      "Documentation"
    ],
    "body": "Hi. I've been using scikit-learn for a little while, and it was gone from not really playing well with pandas to working well (0.16.1) to very well (0.18) together. However, that's easy to miss from scikit-learn's otherwise excellent documentation. For me at least, pandas is an essential part of the python machine learning stack as soon as one is dealing with more than strictly numerical datasets. So it'd be nice if them playing together well was mentioned a bit more in the documentation.\r\n\r\nI'm willing to create a pull request with doc changes to help with that. However, since the pull request will likely touch more than a couple of areas in the doc, I thought it'd be a good idea to discuss things here first.\r\n\r\nSo, any interest in changes to the doc to showcase more the fact that scikit-learn now works well with pandas?",
    "comments": [
      {
        "user": "jnothman",
        "body": "Thank you for your appreciation of our work!\r\n\r\nI think the silence is because we mostly considered Pandas interoperability issues bug-fixes rather than roadmapped enhancements. #9151 and #9012 are seen as more specific Pandas-related features, and there are open questions about the possibility of transformers that output dataframes in the future, etc.\r\n\r\nWe rarely reject offers of improvements to the docs. But I'm not sure where you'd describe this or how you'd state it. So you're welcome to open a PR.\r\n\r\nStill, I don't think this is really an \"Issue\" as such, and so if you're not going to contribute a PR, I'd appreciate you closing this issue. Thanks."
      },
      {
        "user": "qinhanmin2014",
        "body": "Something in #9517 for your reference @chrish42 . Personally I might think that's enough but feel free to open a PR if you think there's still something to improve.\r\n```\r\narray-like\r\n\r\n        The most common data format for *input* to Scikit-learn estimators and\r\n        functions, array-like is any type object for which\r\n        :func:`numpy.asarray` will produce an array of appropriate shape\r\n        (usually 1 or 2-dimensional) of appropriate dtype (usually numeric).\r\n\r\n        This includes:\r\n\r\n        * a numpy array\r\n        * a list of numbers\r\n        * a list of length-k lists of numbers for some fixed length k\r\n        * a :class:`pandas.DataFrame` with all columns numeric\r\n        * a numeric :class:`pandas.Series`\r\n```"
      },
      {
        "user": "chrish42",
        "body": "@jnothman \"we mostly considered Pandas interoperability issues bug-fixes rather than roadmapped enhancements\" is nice to hear, but the downside is that, for people who are not close to the project, it can be easy to miss that it went from \"not really working with pandas\" to \"working well\". Here's that I would have in mind, for doc changes:\r\n\r\n- [ ] Tweak doc for Pipeline to make it clearer that, for X and y, whatever works with the underlying transforms in the pipeline will also work fine for the pipeline class\r\n- [ ] Mention at the top-level of the docs for model_selection classes and functions that they also \"work with pandas DataFrames\"\r\n- [ ] Add a couple examples of the more advanced usage (i.e. model selection, pipelines, with complex dataset types) of scikit-learn with pandas\r\n- [ ] Maybe even add a \", works well with Pandas\" at the end of the bullet \"Built on NumPy, SciPy, and matplotlib\" on the home page of the website?\r\n\r\nHow would that sound? I'm happy to contribute, I just want to get a feeling that I'm working in a direction that has a chance of being accepted (before I do all the work). :-) Thanks!\r\n\r\n\r\nThese are all things that tripped me (and other people I know) when trying to figure out just how much scikit-learn works with pandas.\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 31440,
    "title": "âš ï¸ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: May 28, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-05-28T02:34:18Z",
    "updated_at": "2025-05-30T12:59:14Z",
    "labels": [],
    "body": "**CI failed on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76832&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a)** (May 28, 2025)\n- Test Collection Failure",
    "comments": [
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76907&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a) on May 30, 2025"
      }
    ]
  },
  {
    "issue_number": 31444,
    "title": "âš ï¸ CI failed on Wheel builder (last failure: May 28, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-05-28T09:53:22Z",
    "updated_at": "2025-05-29T04:40:36Z",
    "labels": [
      "Needs Triage"
    ],
    "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/15291085639)** (May 28, 2025)\n",
    "comments": [
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://github.com/scikit-learn/scikit-learn/actions/runs/15315855640) on May 29, 2025"
      }
    ]
  },
  {
    "issue_number": 31366,
    "title": "Gaussian Process Log Likelihood Gradient Incorrect",
    "author": "conradstevens",
    "state": "open",
    "created_at": "2025-05-15T07:48:10Z",
    "updated_at": "2025-05-28T13:56:56Z",
    "labels": [
      "Bug",
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nThe gradient function of in the GaussianProcessRegressor Class is incorrect. This leads to inefficiencies fitting kernel (hyper) parameters.\nThe root of the issue is in that the gradient of the kernel function is made with respect to the log of the kernel parameter.\n\nSee the plot on the right showcasing the incorrect gradient currently being used in the optimization step:\n\n![Image](https://github.com/user-attachments/assets/ec7f5582-b928-4738-9371-02ad1391c7c4)\n\n\nTo fix this apply the chain rule giving:\n\n$\\frac{\\partial k(x)}{\\partial \\ln(x)} \\frac{\\partial \\ln(x)}{\\partial x} = \\frac{\\partial k(x)}{\\partial x}$\n$\\frac{\\partial k(x)}{\\partial \\ln(x)} \\frac{1}{x} = \\frac{\\partial k(x)}{\\partial x}$\n\nWhen implementing this the correct gradient is given (middle plot).\n\n\nTO REPRODUCE THIS BUG - see [my branch](https://github.com/conradstevens/scikit-learn/tree/gpr-log-likelihood-check) \nspecifically: [sklearn/gaussian_process/tests/test_gpr_loglikelihood_check.py](https://github.com/conradstevens/scikit-learn/blob/gpr-log-likelihood-check/sklearn/gaussian_process/tests/test_gpr_loglikelihood_check.py)\n\nI am happy to fix this bug by modifying the `_gpr` class's `log_marginal_likelihood` function. Keeping in mind things may get trickier when kernel functions have multiple hyper parameters (likely resulting in multiple iterations of the chain rule). \n\n### Steps/Code to Reproduce\n\nTO REPRODUCE THIS BUG - see [my branch](https://github.com/conradstevens/scikit-learn/tree/gpr-log-likelihood-check) \nspecifically: [sklearn/gaussian_process/tests/test_gpr_loglikelihood_check.py](https://github.com/conradstevens/scikit-learn/blob/gpr-log-likelihood-check/sklearn/gaussian_process/tests/test_gpr_loglikelihood_check.py)\n\n### Expected Results\n\nCRRECTION AND GROUND TRUTH TANGENTS CAN BE FOUND IN [my branch](https://github.com/conradstevens/scikit-learn/tree/gpr-log-likelihood-check) \nspecifically: [sklearn/gaussian_process/tests/test_gpr_loglikelihood_check.py](https://github.com/conradstevens/scikit-learn/blob/gpr-log-likelihood-check/sklearn/gaussian_process/tests/test_gpr_loglikelihood_check.py)\n\n### Actual Results\n\nLogic error giving incorrect results.\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:31:54) [Clang 16.0.6 ]\nexecutable: /opt/homebrew/Caskroom/miniconda/base/envs/sklearn-dev-5/bin/python\n   machine: macOS-14.4.1-x86_64-i386-64bit\n\nPython dependencies:\n      sklearn: 1.8.dev0\n          pip: 24.2\n   setuptools: 72.2.0\n        numpy: 2.1.0\n        scipy: 1.14.1\n       Cython: 3.0.11\n       pandas: 2.2.2\n   matplotlib: 3.9.2\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 10\n         prefix: libopenblas\n       filepath: /opt/homebrew/Caskroom/miniconda/base/envs/sklearn-dev-5/lib/libopenblasp-r0.3.27.dylib\n        version: 0.3.27\nthreading_layer: openmp\n   architecture: Nehalem\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 10\n         prefix: libomp\n       filepath: /opt/homebrew/Caskroom/miniconda/base/envs/sklearn-dev-5/lib/libomp.dylib\n        version: None\n```",
    "comments": [
      {
        "user": "conradstevens",
        "body": "@snath-xoc @antoinebaker @ogrisel "
      },
      {
        "user": "snath-xoc",
        "body": "Thank you @conradstevens for that and interesting results. The gradient of the kernel is always calculated with respect to the log of the hyperparameter space under ```eval_gradient``` (see L289-292 in gaussian_process/kernels.py), so to translate it to the non-log hyperparameter space we should indeed use the chain rule to eventually get to equation 5.9 of RW[2006]. We should probably implement this fix as well as add to the documentation so that it is well-explained. Would you like to open a PR @conradstevens?"
      },
      {
        "user": "conradstevens",
        "body": "@snath-xoc I'll have a look at a fix. Surely this can't be the only place in scikit learn that requires converting the kernel log-gradient to the regular gradient. Would anyone know of another class that does this, we should probably reuse that functionality. "
      }
    ]
  },
  {
    "issue_number": 31222,
    "title": "SVC Sigmoid sometimes ROC AUC from predict_proba & decision_function are each other's inverse",
    "author": "arhall0",
    "state": "open",
    "created_at": "2025-04-17T20:58:26Z",
    "updated_at": "2025-05-28T11:00:11Z",
    "labels": [
      "Bug",
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nUncertain if this is a bug or counter-intuitive expected behavior.\n\nUnder certain circumstances the ROC AUC calculated for `SVC` with the `sigmoid` kernel will not agree depending on if you use `predict_proba` or `decision_function`. In fact, they will be nearly `1-other_method_auc`.\n\nThis was noticed when comparing ROC AUC calculated using `roc_auc_score` with predictions from `predict_proba(X)[:, 1]` to using the scorer from `get_scorer('roc_auc')` which appears to be calling `roc_auc_score` with scores from `decision_function`. \n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, get_scorer\nfrom sklearn.model_selection import train_test_split\n\nn_samples = 100\nn_features = 100\nrandom_state = 123\nrng = np.random.default_rng(random_state)\n\nX = rng.normal(loc=0.0, scale=1.0, size=(n_samples, n_features))\ny = rng.integers(0, 2, size=n_samples)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state)\n\nsvc_params = {\n    \"kernel\": \"sigmoid\",\n    \"probability\": True,\n    \"random_state\":random_state,\n}   \npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('svc', SVC(**svc_params))\n])  \npipeline.fit(X_train, y_train)\ny_proba = pipeline.predict_proba(X_test)[:, 1]\ny_dec = pipeline.decision_function(X_test)\nroc_auc_proba = roc_auc_score(y_test, y_proba)\nroc_auc_dec = roc_auc_score(y_test, y_dec)\nauc_scorer = get_scorer('roc_auc')\nscorer_auc = auc_scorer(pipeline, X_test, y_test)\n\nprint(f\"AUC (roc_auc_score from predict_proba) = {roc_auc_proba:.4f}\")\nprint(f\"AUC (roc_auc_score from decision_function) = {roc_auc_dec:.4f}\")\nprint(f\"AUC (get_scorer) = {scorer_auc:.4f}\")\n```\n\n### Expected Results\n\nThe measures of ROC AUC agree\n\n### Actual Results\n\n```shell\nAUC (roc_auc_score from predict_proba) = 0.5833\nAUC (roc_auc_score from decision_function) = 0.4295\nAUC (get_scorer) = 0.4295\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.11.5\n\nPython dependencies:\n      sklearn: 1.7.dev0\n          pip: 25.0.1\n   setuptools: 65.5.0\n        numpy: 1.26.4\n        scipy: 1.15.2\n       Cython: 3.0.12\n       pandas: 2.2.3\n   matplotlib: 3.10.1\n       joblib: 1.2.0\nthreadpoolctl: 3.1.0\n\nBuilt with OpenMP: True\n```",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Thanks for the report and the reproducer. I agree that this is a surprising behavior (bug?) of the built-in implementation of Platt-scaling from our vendored code of libsvm.\n\nNote that you can instead use `CalibratedClassifierCV` instead, and this alternative does not suffer for this bug:\n\n```python\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, get_scorer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.calibration import CalibratedClassifierCV\n\nn_samples = 100\nn_features = 100\nrandom_state = 123\nrng = np.random.default_rng(random_state)\n\nX = rng.normal(loc=0.0, scale=1.0, size=(n_samples, n_features))\ny = rng.integers(0, 2, size=n_samples)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state)\n\nsvc_params = {\n    \"kernel\": \"sigmoid\",\n    \"random_state\": random_state,\n}\npipeline = Pipeline(\n    [\n        (\"scaler\", StandardScaler()),\n        (\"svc\", SVC(**svc_params)),\n    ]\n)\npipeline_cal = CalibratedClassifierCV(pipeline, method=\"sigmoid\", ensemble=False)\npipeline.fit(X_train, y_train)\npipeline_cal.fit(X_train, y_train)\n\ny_proba = pipeline_cal.predict_proba(X_test)[:, 1]\ny_dec = pipeline.decision_function(X_test)\nroc_auc_proba = roc_auc_score(y_test, y_proba)\nroc_auc_dec = roc_auc_score(y_test, y_dec)\nauc_scorer = get_scorer(\"roc_auc\")\nscorer_auc = auc_scorer(pipeline, X_test, y_test)\n\nprint(f\"AUC (roc_auc_score from predict_proba) = {roc_auc_proba:.4f}\")\nprint(f\"AUC (roc_auc_score from decision_function) = {roc_auc_dec:.4f}\")\nprint(f\"AUC (get_scorer) = {scorer_auc:.4f}\")\n```\n```\nAUC (roc_auc_score from predict_proba) = 0.4295\nAUC (roc_auc_score from decision_function) = 0.4295\nAUC (get_scorer) = 0.4295\n```\n\nWe also noticed that the Platt-scaling implementation of libsvm does not support `sample_weight` properly as documented in #16298.\n\nSince we have a `CalibrationClassifierCV` that works as expected on both accounts, I would be in favor of deprecating the `probability=True` option of `SVC` and point our users to `CalibratedClassifierCV` instead."
      },
      {
        "user": "ogrisel",
        "body": "Note that if you set `ensemble=True` in the above code, I get:\n\n```\nAUC (roc_auc_score from predict_proba) = 0.5000\nAUC (roc_auc_score from decision_function) = 0.4295\nAUC (get_scorer) = 0.4295\n```\n\nThis is expected because the ensembling effect does not preserve the prediction ranking vs fitting a single `SVC` model. Maybe `SVC` does something similar internally, in which case this is not a bug and the expected behavior.\n\nHowever, `SVC` gives no control on whether to ensemble or change the cross-validation strategy to control the size of the ensemble while `CalibrationClassifierCV` does offer such control (in addition to correctly handling `sample_weight`).\n\nSo I would still be in favor of deprecating the `probability=True` option."
      },
      {
        "user": "ogrisel",
        "body": "cc @snath-xoc @antoinebaker "
      }
    ]
  },
  {
    "issue_number": 31267,
    "title": "Change the default data directory",
    "author": "balthild",
    "state": "open",
    "created_at": "2025-04-28T21:22:54Z",
    "updated_at": "2025-05-27T21:20:46Z",
    "labels": [
      "New Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nIt's not a good practice to put files directly into the home directory.\n\n### Describe your proposed solution\n\nA more common way is to put them into the standard cache directories recommended by operating systems:\n\n| OS | Path |\n| -- | ---- |\n| Linux | `$XDG_CACHE_HOME` (if the env var presents) or `~/.cache` |\n| macOS | `~/Library/Caches` |\n| Windows | `%LOCALAPPDATA%` (`~/AppData/Local`) |\n\n### Describe alternatives you've considered, if relevant\n\nPut into `~/.cache/scikit-learn` for all operating systems. Though not being standard, it's still better than the home dir.\n\n### Additional context\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.datasets.get_data_home.html",
    "comments": [
      {
        "user": "ogrisel",
        "body": "I agree this is a good idea. Following system conventions help tools and people looking for suggestions for cleanups when a disk is nearly full.\n\n+1 for a PR on my side."
      },
      {
        "user": "norgera",
        "body": "I made a pr for this.\n\nThe cache directory is named scikit_learn instead of scikit_learn_data as it better aligns with OS cache naming conventions."
      },
      {
        "user": "Namit24",
        "body": "Hey @balthild sent a pr"
      }
    ]
  },
  {
    "issue_number": 31223,
    "title": "Support orthogonal polynomial features (via QR decomposition) in `PolynomialFeatures`",
    "author": "cottnich",
    "state": "closed",
    "created_at": "2025-04-18T04:56:26Z",
    "updated_at": "2025-05-27T19:43:54Z",
    "labels": [
      "New Feature",
      "Needs Decision - Include Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nI want to introduce support for orthogonal polynomial features via QR decomposition in `PolynomialFeatures`, closely mirroring the behavior of R's `poly()` function.\n\nIn regression modeling, using orthogonal polynomials can often lead to improved numerical stability and reduced multi-collinearity among polynomial terms\n\nAs an example of what the difference looks like in R,\n<pre>\n#fits raw polynomial data without an orthogonal basis\nmodel_raw <- lm(y ~ I(x) + I(x^2) + I(x^3), data = data)\n#model_raw <- lm(y ~poly(x,3,raw=TRUE), data = data)\n\n#fits the same degree-3 polynomial using an orthogonal basis\nmodel_poly <- lm(y ~ poly(x, 3), data = data)\n</pre>\n\nThis behavior cannot currently be replicated with `scikit-learn`'s `PolynomialFeatures`, which only produces the raw monomial terms. As a result transitioning from R to Python often leads to discrepancies in model behavior and performance.\n\n\n### Describe your proposed solution\n\nI propose extending `PolynomialFeatures` with a new parameter:\n<pre>\nPolynomialFeatures(..., method=\"raw\")\n</pre>\nAccepted values:\n- `\"raw\"` (default): retains existing behavior, returning standard raw terms\n- `\"qr\"`: applies QR decomposition to each feature to generate orthogonal polynomial features.\n\nBecause R's `poly()` only operates on 1D input vectors, my thought was to apply QR decomposition feature by feature when the input is multi-dimensional. Each column is processed independently, mirroring R's approach.\n\nThis feature would interact with other parameters as follows:\n\n- `include_bias`: When `method=\"qr\"`, The orthogonal polynomial basis inherently includes a transformed first column. However, this column is not a plain column of ones. Therefore, the concept of `include_bias=True` (which appends a column of ones) becomes redundant or misleading in this context. One option is to always set  `include_bias=False` if `method=qr` and always return orthogonal columns only, or raise a warning.\n\n- `interaction_only`: This would be incompatible with `method=\"qr\"` since the QR-based transformation does not naturally support selective inclusion of interaction terms.\n\n### Describe alternatives you've considered, if relevant\n\nCurrently, users must implement QR decomposition manually when orthogonal polynomials are needed. This is a common pattern in statistical workflows but lacks \"off the shelf\" support in any major python library. This feature would eliminate the need to do this decomposition manually and would improve workflows for researchers who are used to R's statistical tools.\n\n### Additional context\n\nThis idea stemmed from a broader effort to convert statistical modeling pipelines from R to python, where discrepencies in regression results were traced to the lack of orthogonal polynomial support in `PolynomialFeatures`.\n\nI have drafted and tested a 1D implementation of this feature but wanted feedback on whether this idea aligns with `scikit-learn`'s scope before moving on. In particular, I'd appreciate input on\n\n- Acceptability of feature-wise orthogonalization for multi-feature input.\n- Preferred parameter naming (e.g., `method=\"qr\"` vs. `orthogonal=True`).\n- Compatibility decisions around parameters like `include_bias` and `interaction_only`.",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Thanks for your proposal.\n\n> Because R's poly() only operates on 1D input vectors, my thought was to apply QR decomposition feature by feature when the input is multi-dimensional. Each column is processed independently, mirroring R's approach.\n\nThat sounds quite different from the current behavior of `PolynomialFeatures` which always considers interactions between input features. I wonder if trying to implement the two approaches into the same class makes sense. Maybe we could implement that as a new transformer but before doing so, I would rather make sure that people actually need this feature before implementing it in scikit-learn and adding the maintenance of a new estimator and cognitive overhead choosing from too many estimators and options.  \n\nCould you please publish prototype code to a gist.github.com or other small repo or notebook? It would be great if you could highlight a simple regression tasks for which this kind of feature engineering is a game changer (either in terms of predictive performance, model size, fitting speed, numerical stability of the fit) compared to what is already available in scikit-learn.\n\nIt would be great to compare to:\n\n- `SplineTransformer()`\n- `PolynomialFeatures()`\n- `make_pipeline(SplineTransformer(), PolynomialFeatures(interaction_only=True)`\n\nand all of the above followed by a `PCA` step.\n\nBTW: maybe @lorentzenchr would like to share his views on such a new feature in scikit-learn."
      },
      {
        "user": "ogrisel",
        "body": "I also noticed that [`formulaic`](https://matthewwardrop.github.io/formulaic/) provides an implementation of the poly function that seems to follow the R-style orthogonalization convention by default:\n\n- https://matthewwardrop.github.io/formulaic/latest/guides/splines/#poly\n\nNote that it is possible to wrap `formulaic` as a scikit-learn transformer but it requires copy-pasting some boilerplate code as explained here:\n\n- https://matthewwardrop.github.io/formulaic/latest/guides/integration/#scikit-learn\n\nRather than replicating such features in scikit-learn, it might be more fruitful to see if `formulaic` authors be open to the idea of adding first-class integration with scikit-learn API/pipelines in directly into formulaic or via a new extension package. If so, we could extend on of scikit-learn examples on polynomial / spline features to show how to use formulaic with scikit-learn a go beyond the options implemented by default in scikit-learn."
      },
      {
        "user": "lorentzenchr",
        "body": "@cottnich What is your motivation?\n\n> As a result transitioning from R to Python often leads to discrepancies in model behavior and performance.\n\nWithout penalty, the predicted values are the same for all design matrices related to orthogonal transformations.\nB-Splines are almost always preferred to pure polynomials of features.\nIf you want to be as close to R's `lm` and `glm`, I recommend to use https://www.statsmodels.org ."
      }
    ]
  },
  {
    "issue_number": 31286,
    "title": "Clarification of output array type when metrics accept multiclass/multioutput",
    "author": "lucyleeow",
    "state": "open",
    "created_at": "2025-05-01T05:28:13Z",
    "updated_at": "2025-05-26T07:11:03Z",
    "labels": [
      "Needs Decision",
      "Array API"
    ],
    "body": "Clarification of how we should handle array output type when a metric outputs several values (i.e. accepts multiclass or multioutput input).\n\nThe issue was summarised succinctly in https://github.com/scikit-learn/scikit-learn/pull/30439#issuecomment-2532238196:\n\n> Not sure what should be the output namespace / device in case we output an array, e.g. roc_auc_score with average=None on multiclass problems...\n\nCurrently all regression/classification metrics that support array API and multiclass or multioutput, all output an array in the same namespace and device as the input (checked code and manually). Summary of these metrics :\n\n### Regression metrics\n\nReturns array in same namespace/device:\n* [explained_variance_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score)\n* [r2_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) \n* [mean_absolute_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error)\n* [mean_absolute_percentage_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html#sklearn.metrics.mean_absolute_percentage_error)\n* [mean_pinball_loss](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_pinball_loss.html#sklearn.metrics.mean_pinball_loss)\n* [mean_squared_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)\n* [mean_squared_log_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error)\n* [root_mean_squared_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.root_mean_squared_error.html#sklearn.metrics.root_mean_squared_error)\n* [root_mean_squared_log_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.root_mean_squared_log_error.html#sklearn.metrics.root_mean_squared_log_error)\n\n### Classification metrics\nReturns array in same namespace/device:\n\n* [precision_recall_fscore_support](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support) and family ([f1_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score), [fbeta_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score), [precision_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score), [recall_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score))\n\nLooking at the metrics code, if we wanted to support a list of scalars, we'd generally have to do extra processing to convert an array (often output of an xp.** function) to a list of scalars.\n\nOnce we arrive at a consensus we should update the array API documentation and update the `check_array_api_metric` in tests such that when the output is array/list - we check that the output type etc is correct.\n\ncc @ogrisel @betatim ",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "@ogrisel once mentioned this was intentional since Array API is experimental and we didn't want to confuse people by complicated docstrings. But I think not having the information is very confusing. I'd be in favor of properly documenting them.\n\nI think @StefanieSenger had the same view at some point."
      },
      {
        "user": "lucyleeow",
        "body": "I wasn't suggesting we should include this in docstrings.\n\nThis issue is mainly because of the concern expressed in the comment quoted, deciding on what we want and adding a note in the array API doc page.\n"
      },
      {
        "user": "adrinjalali",
        "body": "Oh I see.\n\nSince we decided for our metrics to return python scalars, shouldn't the arrays here also be very simple types?"
      }
    ]
  },
  {
    "issue_number": 30767,
    "title": "DOC Add `from_predictions` example to `visualizations.rst`",
    "author": "lucyleeow",
    "state": "closed",
    "created_at": "2025-02-04T05:06:25Z",
    "updated_at": "2025-05-26T06:26:52Z",
    "labels": [
      "Documentation"
    ],
    "body": "Noticed that the `visualizations.rst` page (https://scikit-learn.org/dev/visualizations.html) could be improved while working on #30399\n\n* We should clarify that both `from_estimator` and `from_predictions` return the display object\n* Describe the purpose of the display object more generally (i.e., stores data for the plot)\n* Add an example section using `from_predictions` (currently we just describe in the text that we can get the same plot via `from_predictions`\n* Explicitly detail that we can add to existing plot via `plot` by passing the `ax` parameter\n\nI may have missed some points\n\ncc @DeaMariaLeon  @glemaitre ",
    "comments": [
      {
        "user": "DeaMariaLeon",
        "body": "/take"
      },
      {
        "user": "ayan6943",
        "body": "Hi can you please assign me this issue \n"
      },
      {
        "user": "lucyleeow",
        "body": "@ayan6943 it's been taken, please choose another issue."
      }
    ]
  },
  {
    "issue_number": 29443,
    "title": "KernelDensity(bandwidth='silverman') doesn't throw proper error for 1d X",
    "author": "amueller",
    "state": "open",
    "created_at": "2024-07-10T01:32:02Z",
    "updated_at": "2025-05-25T21:36:08Z",
    "labels": [
      "Bug"
    ],
    "body": "Essentially the bandwidth estimation codepath is not covered in the common tests, but it should be :)",
    "comments": [
      {
        "user": "glemaitre",
        "body": "I assume that we have similar issue with every estimators that do not use default parameters. We should at least add this test in the specialized tests in the meanwhile."
      },
      {
        "user": "joeyspagnoli",
        "body": "/take"
      }
    ]
  },
  {
    "issue_number": 30970,
    "title": "Allow for multiclass cost matrix in FixedThresholdClassifier and TunedThresholdClassifierCV",
    "author": "lorentzenchr",
    "state": "open",
    "created_at": "2025-03-10T15:44:30Z",
    "updated_at": "2025-05-25T15:43:30Z",
    "labels": [
      "New Feature",
      "module:model_selection",
      "module:multiclass"
    ],
    "body": "### Describe the workflow you want to enable\n\nWith #26120, we got `FixedThresholdClassifier` and `TunedThresholdClassifierCV` but only for binary classification. The next logical step would be to extend it to the multiclass setup.\n\n### Describe your proposed solution\n\nFor `FixedThresholdClassifier`, one could allow for a cost matrix instead of a single threshold.\n\n`TunedThresholdClassifierCV` seems straight forward (or I'm missing something).\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "Krishnadubey1008",
        "body": "Hi! Iâ€™d like to take this issue my plan for extending FixedThresholdClassifier to multiclass is\n(1) Add a cost_matrix parameter (n_classes x n_classes) to handle multiclass scenarios, where the predicted class minimizes the expected cost.\n(2) For binary, keep the current threshold parameter for backward compatibility.\n(3) In predict, if cost_matrix is set, compute expected costs via np.dot(proba, cost_matrix.T) and pick the class with the lowest cost.\n(4) Validate that cost_matrix shape matches n_classes.\n\nIs this the right direction? Any feedback or suggestions before I proceed further?\n\nThanks!"
      }
    ]
  },
  {
    "issue_number": 15647,
    "title": "Gaussian Mixture - Implementing weighted sample",
    "author": "dupuisIRT",
    "state": "open",
    "created_at": "2019-11-18T09:35:57Z",
    "updated_at": "2025-05-24T15:47:32Z",
    "labels": [
      "Enhancement",
      "module:mixture"
    ],
    "body": "I implemented a version of Gaussian Mixture for weighted data (fixed values).\r\nThe mathematical framework can be found here (part III):\r\nhttps://arxiv.org/abs/1509.01509\r\nSo if a sample has a weight _w_, it means the sample is observed _w_ times. \r\n\r\nmixture/base.py and mixture/gaussian_mixture are modified by:\r\n- adding the sample weights to the EM algorithm (Eq 6, 8, 9, 10 of the paper)\r\n- adding the sample weights to the log likelihood\r\n- adding a sample_weight argument to fit, predict, score... methods. \r\n\r\nI followed the same logic than the KMeans class for the user interface (with sample_weight as an optional keyword) and use _check_sample_weight.\r\n\r\nIs it worth to open a PR or this feature does not have much interest for the official release?",
    "comments": [
      {
        "user": "jnothman",
        "body": "A pr is welcome IMO\n"
      },
      {
        "user": "keunhong",
        "body": "I was just currently looking for this, so there is interest :)"
      },
      {
        "user": "Reksbril",
        "body": "@dupuisIRT are you going to open a PR, or I can work on this?"
      }
    ]
  },
  {
    "issue_number": 14257,
    "title": "Feature request: Group aware Time-based cross validation",
    "author": "ogrisel",
    "state": "open",
    "created_at": "2019-07-04T13:33:44Z",
    "updated_at": "2025-05-23T06:25:31Z",
    "labels": [
      "New Feature",
      "Enhancement",
      "Moderate",
      "module:model_selection"
    ],
    "body": "Basically combining `TimeSeriesSplit` with the `Group` awareness of other CV strategies such as `GroupKFold`.\r\n\r\nI think it's a good first issue for first time contributors that are already familiar with the existing cross validation tools in scikit-learn:\r\n\r\nhttps://scikit-learn.org/stable/modules/cross_validation.html\r\n\r\nSource code is here:\r\n\r\nhttps://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_split.py",
    "comments": [
      {
        "user": "souravsingh",
        "body": "@ogrisel I am interested in working on this"
      },
      {
        "user": "ogrisel",
        "body": "Feel free to give it a try:\r\n\r\n- start by reading the documentation and the code of the existing CV split strategies,\r\n- then start by writing the tests for the new strategy and issue and early `[WIP]` PR to get feedback on the tests before writing the implementation itself.\r\n\r\nGood luck!"
      },
      {
        "user": "aditya1702",
        "body": "@souravsingh Are you working on this?\r\n"
      }
    ]
  },
  {
    "issue_number": 22849,
    "title": "Verbosity option is not working in GridSearchCV (Jupyter notebook)",
    "author": "ahmadelsayed2009",
    "state": "open",
    "created_at": "2022-03-15T14:48:31Z",
    "updated_at": "2025-05-22T18:33:16Z",
    "labels": [
      "Bug",
      "module:model_selection",
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nSo this issue has been addressed before [here ](https://github.com/scikit-learn/scikit-learn/issues/22291) by [darrencl](https://github.com/darrencl), but the user didn't follow up with [lesteve](https://github.com/lesteve) response.\r\n\r\nThe problem is that GridSearchCV doesn't show the elapsed time periodically, or any log, I am set`n_jobs = -1`, and `verbose = 1`. I tried setting `n_jobs` to other values, the same with `verbose`, but nothing happened.\r\nNote that this didn't happen until I updated scikit-learn from version 0.22.1 to 1.0.2.\r\n\r\n[lesteve](https://github.com/lesteve) in his response assumed that this problem is due to `ipykernel <6`, which is not the case with me.\n\n### Steps/Code to Reproduce\n\n```\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn import datasets\r\n\r\niris = datasets.load_iris()\r\nparams = {'n_estimators':[10,20,30,40,50,60],\r\n          'max_depth':[20,50,60,70,80]}\r\ngrid_obj = GridSearchCV(estimator=RandomForestClassifier(), param_grid=params, n_jobs=-1, verbose=1, cv=5)\r\ngrid_obj.fit(iris.data, iris.target)\r\n```\n\n### Expected Results\n\nThis is the output when using version 0.22.1\r\n![image](https://user-images.githubusercontent.com/17684093/158401635-31755363-2108-41e7-8f7f-08985abd03b8.png)\r\n\n\n### Actual Results\n\n```Fitting 5 folds for each of 30 candidates, totalling 150 fits```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: D:\\Programs\\ApplicationsSetup\\anaconda3\\python.exe\r\n   machine: Windows-10-10.0.19041-SP0\r\n\r\nPython dependencies:\r\n          pip: 21.2.2\r\n   setuptools: 58.0.4\r\n      sklearn: 1.0.2\r\n        numpy: 1.19.2\r\n        scipy: 1.6.2\r\n       Cython: 0.29.25\r\n       pandas: 1.4.1\r\n   matplotlib: 3.5.1\r\n       joblib: 1.1.0\r\nthreadpoolctl: 2.2.0\r\n\r\n\r\n!jupyter --version\r\nSelected Jupyter core packages...\r\nIPython          : 7.31.1\r\nipykernel        : 6.4.1\r\nipywidgets       : 7.6.5\r\njupyter_client   : 6.1.12\r\njupyter_core     : 4.9.1\r\njupyter_server   : 1.13.5\r\njupyterlab       : 3.2.9\r\nnbclient         : 0.5.11\r\nnbconvert        : 6.1.0\r\nnbformat         : 5.1.3\r\nnotebook         : 6.4.9\r\nqtconsole        : 5.2.2\r\ntraitlets        : 5.1.1\n```\n",
    "comments": [
      {
        "user": "glemaitre",
        "body": "If I recall properly, this is not a bug on our side but from Jupyter.\r\nI can even see the output while killing the kernel:\r\n\r\n```shell\r\n[I 17:45:32.952 NotebookApp] Kernel shutdown: 0cc5ae70-c404-4f10-92fb-f39263d60d41\r\n[CV 2/5; 1/30] START max_depth=20, n_estimators=10..............................\r\n[CV 2/5; 1/30] END max_depth=20, n_estimators=10;, score=0.967 total time=   0.0s\r\n[CV 5/5; 2/30] START max_depth=20, n_estimators=20..............................\r\n[CV 5/5; 2/30] END max_depth=20, n_estimators=20;, score=1.000 total time=   0.0s\r\n[CV 1/5; 5/30] START max_depth=20, n_estimators=50..............................\r\n[CV 1/5; 5/30] END max_depth=20, n_estimators=50;, score=0.967 total time=   0.0s\r\n[CV 2/5; 5/30] START max_depth=20, n_estimators=50..............................\r\n[CV 2/5; 5/30] END max_depth=20, n_estimators=50;, score=0.967 total time=   0.0s\r\n[CV 1/5; 9/30] START max_depth=50, n_estimators=30..............................\r\n[CV 1/5; 9/30] END max_depth=50, n_estimators=30;, score=0.967 total time=   0.0s\r\n[CV 2/5; 9/30] START max_depth=50, n_estimators=30..............................\r\n[CV 2/5; 9/30] END max_depth=50, n_estimators=30;, score=0.967 total time=   0.0s\r\n[CV 1/5; 13/30] START max_depth=60, n_estimators=10.............................\r\n[CV 1/5; 13/30] END max_depth=60, n_estimators=10;, score=0.967 total time=   0.0s\r\n[CV 2/5; 13/30] START max_depth=60, n_estimators=10.............................\r\n[CV 2/5; 13/30] END max_depth=60, n_estimators=10;, score=0.967 total time=   0.0s\r\n[CV 3/5; 13/30] START max_depth=60, n_estimators=10.............................\r\n[CV 3/5; 13/30] END max_depth=60, n_estimators=10;, score=0.900 total time=   0.0s\r\n[CV 4/5; 13/30] START max_depth=60, n_estimators=10.............................\r\n[CV 4/5; 13/30] END max_depth=60, n_estimators=10;, score=0.967 total time=   0.0s\r\n[CV 2/5; 16/30] START max_depth=60, n_estimators=40.............................\r\n[CV 2/5; 16/30] END max_depth=60, n_estimators=40;, score=0.967 total time=   0.0s\r\n[CV 3/5; 16/30] START max_depth=60, n_estimators=40.............................\r\n[CV 3/5; 16/30] END max_depth=60, n_estimators=40;, score=0.933 total time=   0.0s\r\n[CV 4/5; 16/30] START max_depth=60, n_estimators=40.............................\r\n[CV 4/5; 16/30] END max_depth=60, n_estimators=40;, score=0.900 total time=   0.0s\r\n[CV 5/5; 16/30] START max_depth=60, n_estimators=40.............................\r\n[CV 5/5; 16/30] END max_depth=60, n_estimators=40;, score=1.000 total time=   0.0s\r\n[CV 4/5; 22/30] START max_depth=70, n_estimators=40.............................\r\n[CV 4/5; 22/30] END max_depth=70, n_estimators=40;, score=0.867 total time=   0.0s\r\n[CV 5/5; 22/30] START max_depth=70, n_estimators=40.............................\r\n[CV 5/5; 22/30] END max_depth=70, n_estimators=40;, score=1.000 total time=   0.0s\r\n[CV 1/5; 23/30] START max_depth=70, n_estimators=50.............................\r\n[CV 1/5; 23/30] END max_depth=70, n_estimators=50;, score=0.967 total time=   0.0s\r\n[CV 2/5; 23/30] START max_depth=70, n_estimators=50.............................\r\n[CV 2/5; 23/30] END max_depth=70, n_estimators=50;, score=0.967 total time=   0.0s\r\n[CV 1/5; 29/30] START max_depth=80, n_estimators=50.............................\r\n[CV 1/5; 29/30] END max_depth=80, n_estimators=50;, score=0.967 total time=   0.0s\r\n[CV 1/5; 1/30] START max_depth=20, n_estimators=10..............................\r\n[CV 1/5; 1/30] END max_depth=20, n_estimators=10;, score=0.967 total time=   0.0s\r\n[CV 3/5; 2/30] START max_depth=20, n_estimators=20..............................\r\n[CV 3/5; 2/30] END max_depth=20, n_estimators=20;, score=0.933 total time=   0.0s\r\n[CV 1/5; 4/30] START max_depth=20, n_estimators=40..............................\r\n[CV 1/5; 4/30] END max_depth=20, n_estimators=40;, score=0.967 total time=   0.0s\r\n[CV 4/5; 6/30] START max_depth=20, n_estimators=60..............................\r\n[CV 4/5; 6/30] END max_depth=20, n_estimators=60;, score=0.933 total time=   0.0s\r\n[CV 5/5; 6/30] START max_depth=20, n_estimators=60..............................\r\n[CV 5/5; 6/30] END max_depth=20, n_estimators=60;, score=1.000 total time=   0.0s\r\n```"
      },
      {
        "user": "glemaitre",
        "body": "@lesteve I recall that you found the root of the problem when working on the `GridSearchCV` verbosity, isn't it?"
      },
      {
        "user": "lesteve",
        "body": "Well my answer was in the issue linked in the top post: https://github.com/scikit-learn/scikit-learn/issues/22291#issuecomment-1022048818. You need to use `ipykernel >= 6` for outputs in subprocesses to be captured inside the notebook.\r\n\r\nIt looks like there may be a new bug in ipykernel though. As you say I don't get the output captured in the notebook but once I kill the kernel, I do get the output in the terminal where I start jupyter ...\r\n\r\n\r\n\r\n\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 31359,
    "title": "Documentation improvement for macOS Homebrew libomp installation",
    "author": "ParthS007",
    "state": "open",
    "created_at": "2025-05-13T09:59:24Z",
    "updated_at": "2025-05-22T14:07:43Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the issue linked to the documentation\n\nThe current documentation in `doc/developers/advanced_installation.rst` under the \"macOS compilers from Homebrew\" section provides environment variable examples using the path `/usr/local/opt/libomp/`. While this is correct for Intel-based Macs, Homebrew on Apple Silicon (arm64) Macs installs packages, including `libomp`, to `/opt/homebrew/opt/libomp/`.\n\nThis can lead to confusion and build issues for users on Apple Silicon hardware who follow the documentation to install from source.\n\nThe documentation will improve from mentioning that `libomp` is often installed as \"keg-only\" by Homebrew, which is why explicitly setting these paths is necessary. Homebrew's own output (`brew info libomp`) often provides guidance on the necessary `CPPFLAGS` and `LDFLAGS`.\n\n\n### Suggest a potential alternative/fix\n\nThe documentation could be updated to:\n1.  Mention the different Homebrew base paths for Intel (`/usr/local`) and Apple Silicon (`/opt/homebrew`).\n2.  Update the example environment variable settings to reflect the `/opt/homebrew/opt/libomp` path as a common case for Apple Silicon, or provide instructions for users to identify and use the correct path for their system.\n3.  Optionally, We could briefly explain the \"keg-only\" nature of `libomp` from Homebrew and how it relates to needing these environment variables.\n\n\n\n\n",
    "comments": [
      {
        "user": "ParthS007",
        "body": "/take\n"
      },
      {
        "user": "lesteve",
        "body": "I guess this section can likely be simplified indeed.\n\nAs mentioned in https://github.com/scikit-learn/scikit-learn/pull/31361#issuecomment-2876202394 I there is likely no need to set environment variables now that we use Meson but this needs to be double-checked.\n"
      },
      {
        "user": "lesteve",
        "body": "Actually I did not remember but I created an issue saying that we can probably remove the environment variables a while ago https://github.com/scikit-learn/scikit-learn/issues/29603."
      }
    ]
  },
  {
    "issue_number": 27806,
    "title": "BUG: pytest error when loading conftest (seemingly platform-specific)",
    "author": "Charlie-XIAO",
    "state": "closed",
    "created_at": "2023-11-19T06:28:18Z",
    "updated_at": "2025-05-22T04:56:12Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\r\n\r\nI'm seeing errors on my Windows machine when running `pytest` (does not work with only `pytest`, and does not work for directories that has `conftest.py`). This seems to be a platform-specific problem, since CI is not complaining. I investigated a bit and found https://github.com/pytest-dev/pytest/issues/9765, but it doesn't look like `pytest` is planning to fix it, at least for now. I tried downgrading to `pytest==7.0.1` and everything worked smoothly, but in https://github.com/scikit-learn/scikit-learn/pull/26373 the minimum version of `pytest` has already been `7.1.2` for scikit-learn (due to some CI errors for `pytest==5.x.x`), so scikit-learn is raising error:\r\n\r\nhttps://github.com/scikit-learn/scikit-learn/blob/5c4288dba42cb67d954cb56c2cebfbf25c05ef89/sklearn/conftest.py#L30-L34\r\n\r\nI'm wondering if it is possible to pin `pytest==7.0.1` or at least relax the minimum requirement a bit to `PYTEST_MIN_VERSION = \"7.0.1\"`? Or are there any other suggestions how I may resolve this issue? @glemaitre who bumped the minimum version of `pytest` to 7.1.2. Truly sorry for the inconvenience caused by my annoying Windows machine.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```bash\r\npytest\r\n```\r\n\r\nor\r\n\r\n```bash\r\npytest sklearn/utils/tests\r\n```\r\n\r\nRunning `pytest` on a single file works correctly.\r\n\r\n### Expected Results\r\n\r\nNo error is thrown.\r\n\r\n### Actual Results\r\n\r\nFor the first example,\r\n\r\n```pytb\r\nâ¯ pytest\r\nTraceback (most recent call last):\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\Scripts\\pytest-script.py\", line 9, in <module>\r\n    sys.exit(console_main())\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 192, in console_main\r\n    code = main()\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 150, in main\r\n    config = _prepareconfig(args, plugins)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 331, in _prepareconfig\r\n    config = pluginmanager.hook.pytest_cmdline_parse(\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\pluggy\\_hooks.py\", line 493, in __call__\r\n    return self._hookexec(self.name, self._hookimpls, kwargs, firstresult)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\pluggy\\_manager.py\", line 115, in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\pluggy\\_callers.py\", line 130, in _multicall\r\n    teardown[0].send(outcome)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\helpconfig.py\", line 104, in pytest_cmdline_parse\r\n    config: Config = outcome.get_result()\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\pluggy\\_result.py\", line 114, in get_result\r\n    raise exc.with_traceback(exc.__traceback__)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\pluggy\\_callers.py\", line 77, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 1075, in pytest_cmdline_parse\r\n    self.parse(args)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 1425, in parse\r\n    self._preparse(args, addopts=addopts)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 1327, in _preparse\r\n    self.hook.pytest_load_initial_conftests(\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\pluggy\\_hooks.py\", line 493, in __call__\r\n    return self._hookexec(self.name, self._hookimpls, kwargs, firstresult)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\pluggy\\_manager.py\", line 115, in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\pluggy\\_callers.py\", line 152, in _multicall\r\n    return outcome.get_result()\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\pluggy\\_result.py\", line 114, in get_result\r\n    raise exc.with_traceback(exc.__traceback__)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\pluggy\\_callers.py\", line 77, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 1153, in pytest_load_initial_conftests\r\n    self.pluginmanager._set_initial_conftests(\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 563, in _set_initial_conftests\r\n    self._try_load_conftest(anchor, importmode, rootpath)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 585, in _try_load_conftest\r\n    self._getconftestmodules(x, importmode, rootpath)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 609, in _getconftestmodules\r\n    mod = self._importconftest(conftestpath, importmode, rootpath)\r\n  File \"D:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py\", line 654, in _importconftest\r\n    assert mod not in mods\r\nAssertionError\r\n```\r\n\r\nFor the second example,\r\n\r\n```pytb\r\nâ¯ pytest sklearn/utils/tests\r\n================================================================== test session starts ===================================================================\r\nplatform win32 -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0\r\nrootdir: D:\\ossd\\scikit-learn-yxiao\r\nconfigfile: setup.cfg\r\nplugins: cov-4.1.0\r\ncollected 1957 items / 1 error\r\n\r\n========================================================================= ERRORS =========================================================================\r\n____________________________________________________ ERROR collecting sklearn/utils/tests/conftest.py ____________________________________________________\r\nD:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\runner.py:341: in from_call\r\n    result: Optional[TResult] = func()\r\nD:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\runner.py:372: in <lambda>\r\n    call = CallInfo.from_call(lambda: list(collector.collect()), \"collect\")\r\nD:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\doctest.py:560: in collect\r\n    module = self.config.pluginmanager._importconftest(\r\nD:\\Downloads\\mambaforge\\envs\\sklearn-env\\lib\\site-packages\\_pytest\\config\\__init__.py:654: in _importconftest\r\n    assert mod not in mods\r\nE   AssertionError\r\n================================================================ short test summary info =================================================================\r\nERROR sklearn/utils/tests/conftest.py - AssertionError\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n============================================================== 1 warning, 1 error in 1.43s ===============================================================\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nâ¯ python -c \"import sklearn; sklearn.show_versions()\"\r\n\r\nSystem:\r\n    python: 3.9.18 | packaged by conda-forge | (main, Aug 30 2023, 03:40:31) [MSC v.1929 64 bit (AMD64)]\r\nexecutable: D:\\Downloads\\mambaforge\\envs\\sklearn-env\\python.exe\r\n   machine: Windows-10-10.0.19045-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.4.dev0\r\n          pip: 23.2.1\r\n   setuptools: 68.2.2\r\n        numpy: 1.26.0\r\n        scipy: 1.11.2\r\n       Cython: 3.0.2\r\n       pandas: 2.1.1\r\n   matplotlib: 3.8.0\r\n       joblib: 1.3.2\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: mkl\r\n    num_threads: 6\r\n         prefix: libblas\r\n       filepath: D:\\Downloads\\mambaforge\\envs\\sklearn-env\\Library\\bin\\libblas.dll\r\n        version: 2022.1-Product\r\nthreading_layer: intel\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 12\r\n         prefix: vcomp\r\n       filepath: D:\\Downloads\\mambaforge\\envs\\sklearn-env\\vcomp140.dll\r\n        version: None\r\n```\r\n",
    "comments": [
      {
        "user": "glemaitre",
        "body": "> I'm wondering if it is possible to pin pytest==7.0.1 or at least relax the minimum requirement a bit to PYTEST_MIN_VERSION = \"7.0.1\"\r\n\r\nWe could try to downgrade but we should make sure that we don't have the INTERNALERROR raised by pytest.\r\n\r\nWhat is weird is that we have windows CI so we should catch such error or maybe this is a Windows 10 error."
      },
      {
        "user": "Charlie-XIAO",
        "body": "Yes perhaps, my laptop broke down not long ago, and since then I switched from Windows 11 back to Windows 10. With Windows 11 I have never seen such error, though I'm not sure what version of `pytest` I had by then. But in https://github.com/pytest-dev/pytest/issues/9765 seems that some Linux users were also having this error, so really weird.\r\n\r\n> We could try to downgrade but we should make sure that we don't have the INTERNALERROR raised by pytest.\r\n\r\n@glemaitre Does this mean that I need to do some extra tests (and in particular how)?"
      },
      {
        "user": "glemaitre",
        "body": "@glemaitre Does this mean that I need to do some extra tests (and in particular how)?\r\n\r\nI need to go back to https://github.com/scikit-learn/scikit-learn/pull/26371, and check that downgrading does not raise the INTERNALERROR."
      }
    ]
  },
  {
    "issue_number": 31412,
    "title": "SimpleImputer converts `int32[pyarrow]` extension array to `float64`, subsequently crashing with numpy `int32` values",
    "author": "intrigus-lgtm",
    "state": "closed",
    "created_at": "2025-05-21T23:34:26Z",
    "updated_at": "2025-05-21T23:34:45Z",
    "labels": [
      "Needs Triage"
    ],
    "body": "### Describe the bug\n\nWhen using the `SimpleImputer` with a pyarrow-backed pandas DataFrame, any float/integer data is converted to `None`/`float64` instead.\nThis causes the imputer to be fitted to `float64`, crashing on a dtype assertion when passing it a numpy-backed `int32` DataFrame after fitting.\n\nThe flow is the following:\n1. The imputer calls `_validate_input`:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/impute/_base.py#L319\n2. This calls `validate_data`:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/impute/_base.py#L344-L353\n3. This calls `check_array`:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L2951-L2952\n4. Our input is a pandas dataframe:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L909\n5. This now checks if the dtypes need to be converted:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L925-L927\n6. Our input is backed by an extension array _and_ `int32[pyarrow]` is an integer datatype, so we return `True` here:\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L714-L724\n7. Finally we pass the \\\"needs conversion\\\" check and convert the dataframe to `dtype` (which is `None` here, which apparently means `float64`):\n  https://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/utils/validation.py#L966-L971\n\n### Steps/Code to Reproduce\n\n```py\nimport polars as pl\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\nprint(\n    SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n      .fit(pl.DataFrame({\\\"a\\\": [10]}, schema={\\\"a\\\": pl.Int32}).to_pandas(use_pyarrow_extension_array=False))\n      ._fit_dtype\n)\n# prints dtype('int32'), as expected\n\nprint(\n    SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n      .fit(pl.DataFrame({\\\"a\\\": [10]}, schema={\\\"a\\\": pl.Int32}).to_pandas(use_pyarrow_extension_array=True))\n      ._fit_dtype\n)\n# prints dtype('float64') (!!)\n```\n\n\n### Expected Results\n\nBoth imputers should be fitted with `int32` values.\n\n### Actual Results\n\nThe imputer using the pyarrow extension array is fitted with `float64`.\n\nThis causes crashes when using the Imputer with normal `int32` columns backed by numpy, as they won't be converted and therefore the dtypes differ.\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.9 (main, Mar 11 2025, 17:26:57) [Clang 20.1.0 ]\nexecutable: /tmp/scikit/.venv/bin/python3\n   machine: Linux-6.14.4-arch1-2-x86_64-with-glibc2.41\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: None\n   setuptools: None\n        numpy: 2.2.5\n        scipy: 1.15.3\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: None\n       joblib: 1.5.0\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: /tmp/scikit/.venv/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-6bb31eeb.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: /tmp/scikit/.venv/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-68440149.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 16\n         prefix: libgomp\n       filepath: /tmp/scikit/.venv/lib/python3.12/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n        version: None\n```",
    "comments": [
      {
        "user": "intrigus-lgtm",
        "body": "That was the wrong window.\nApologies."
      }
    ]
  },
  {
    "issue_number": 31390,
    "title": "Contains code not allowed for commercial use",
    "author": "stefan6419846",
    "state": "open",
    "created_at": "2025-05-19T13:26:08Z",
    "updated_at": "2025-05-21T15:46:40Z",
    "labels": [
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nhttps://github.com/scikit-learn/scikit-learn/blob/ff6bf36f06ca80bf505f37a8c5c42047129952ec/sklearn/datasets/_samples_generator.py#L1900 refers to code at https://homepages.ecs.vuw.ac.nz/~marslast/Code/Ch6/lle.py, which contains the following notice (emphasis mine):\n\n> You are free to use, change, or redistribute the code in any way you wish for **non-commercial purposes**, but please maintain the name of the original author. This code comes with no warranty of any kind.\n\nThis might be problematic for anyone using *scikit-learn* in a commercial context.\n\n### Steps/Code to Reproduce\n\nNot required.\n\n### Expected Results\n\nThe code does not restrict commercial usage hidden deeply inside the code or external references.\n\n### Actual Results\n\nThe code restricts commercial usage hidden deeply inside an external reference.\n\n### Versions\n\n```shell\n1.6.1 and main\n```",
    "comments": [
      {
        "user": "stefan6419846",
        "body": "https://github.com/scikit-learn/scikit-learn/tree/main/sklearn/datasets/tests/data/openml/id_42074 uses a similar incompatible license (CC-BY-NC-SA-4.0 according to https://www.openml.org/search?type=data&sort=runs&id=42074&status=active), but this specific case might be solved by #31391 as well."
      },
      {
        "user": "lesteve",
        "body": "`make_swiss_roll` generates a toy dataset, the code is based on a mathematical formula, that can probably fit on a single line. It doesn't look like the code was copied and pasted but inspired from it, looking at the [original code](https://homepages.ecs.vuw.ac.nz/~marslast/Code/Ch6/lle.py) and the first version of the [scikit-learn code](https://github.com/scikit-learn/scikit-learn/commit/032f76dec46e34f83b19c26191c348f1904dc578).\n\nThe test data one, well I am not sure. I guess that if you are installing scikit-learn in a commercial setting, technically you are making a copy of the CC-BY-NC-SA data and that's not allowed by the license? Maybe a quicker fix would be to use another test data instead ...\n\nEven if we remove tests from wheels, they will still be included in the sdist, so I guess #31191 will not completely fix it.\n\nBy the way, I am curious, I am guessing you have some automated tools help you find this kind of issues?"
      },
      {
        "user": "stefan6419846",
        "body": "> The test data one, well I am not sure. I guess that if you are installing scikit-learn in a commercial setting, technically you are making a copy of the CC-BY-NC-SA data and that's not allowed by the license?\n\nThis depends on the general interpretation (IANAL), but distributing NC-licensed stuff in a commercial application (even when not using it) might be considered critical.\n\nAs for source distributions, this might be okay, as it should be less usual to use these in production environments. But to simplify it and avoiding surprises in the future, removing/replacing it sounds like the cleaner solution.\n\n> By the way, I am curious, I am guessing you have some automated tools help you find this kind of issues?\n\nAt least to some extent, yes. The automated part (especially retrieving possible URLs and licenses) is relying on [ScanCode toolkit](https://github.com/aboutcode-org/scancode-toolkit)/[a custom wrapper](https://github.com/stefan6419846/license_tools), feeding everything into an internal tool where further triage/analysis happens."
      }
    ]
  },
  {
    "issue_number": 31403,
    "title": "[PCA] ValueError: too many values to unpack (expected 3)",
    "author": "muammar",
    "state": "closed",
    "created_at": "2025-05-20T17:54:44Z",
    "updated_at": "2025-05-21T13:11:39Z",
    "labels": [
      "Bug",
      "Needs Triage"
    ],
    "body": "### Describe the bug\n\nI am getting the following error when running PCA with version 1.6.1:\n\n<img width=\"956\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4a576ca3-2268-45c0-8fa8-cccea16fce6d\" />\n\n\n\n### Steps/Code to Reproduce\n\nYou can reproduce it with this snippet: \n\n```\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nX = np.random.choice([0, 1], size=(10, 2048), p=[0.7, 0.3])\nprint(X.shape, X.dtype)\n\npca = PCA(n_components=2)\npca.fit_transform(X)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\n\n```\n\n\n\n### Expected Results\n\nThis works with version `1.3.2`. \n\n```\n(10, 2048) int64\n[0.12276184 0.11835199]\n[21.7604452  21.36603173]\n```\n\n\n\nI tried using `svd_solver='arpack'`, but that does not help in my desperate attempts to solve the issue.  Why did this stop working after `1.3.2`?  For now, I just rolled back to `1.3.2`. \n\n\nThanks\n\n### Actual Results\n\n<img width=\"956\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eaea3acd-7429-4497-9b14-b2c121fe1918\" />\n\n### Versions\n\n```shell\nVersion with the error `1.6.1`. Version that works for me: `1.3.2`.\n```",
    "comments": [
      {
        "user": "muammar",
        "body": "This is interesting, I tried the snippet on my MacBook Pro and it works: \n\n\n```\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport sklearn\n\nprint(sklearn.__version__)\n\nX = np.random.choice([0, 1], size=(10, 2048), p=[0.7, 0.3])\nprint(X.shape, X.dtype)\n\npca = PCA(n_components=2)\npca.fit_transform(X)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\n```\n\n```\n1.6.1\n(10, 2048) int64\n[0.1256885  0.12096747]\n[22.03199875 21.61426301]\n```\n\nThe problem I presented above is happening on a Docker image I created locally. I'll keep investigating what should be causing the issue. "
      },
      {
        "user": "lesteve",
        "body": "I can't reproduce the issue with your snippet. Quickly looking at your first stack-trace screenshot, it looks like you are using scikit-learn-intelex (`sklearnex` in the name of the folder). scikit-learn-intelex is a separate package try to open an issue in their bug tracker.\n\nI am going to close this issue, but feel free to reopen if there is a clear indication that there is an issue in scikit-learn.\n\nAlso, please copy and paste text rather than use screenshots next time ðŸ™. For example, it makes the error more easily searchable."
      },
      {
        "user": "muammar",
        "body": "@lesteve I feel kind of bad. Yeah, the name of `sklearnex' is in the folder name. Life is hectic this week. Thanks for catching that and closing the report. It was my bad. Best. "
      }
    ]
  },
  {
    "issue_number": 31374,
    "title": "Suggested fix: GaussianProcessRegressor.predict wastes significant time when both `return_std` and `return_cov` are `False`",
    "author": "ginoperrotta",
    "state": "open",
    "created_at": "2025-05-16T19:39:14Z",
    "updated_at": "2025-05-21T11:18:06Z",
    "labels": [
      "Easy"
    ],
    "body": "### Describe the workflow you want to enable\n\nhttps://github.com/scikit-learn/scikit-learn/commit/7b715111bff01e836fcd3413851381c6a1057ca4 moved duplicated code above the conditional statements, but this means that an expensive step for computing GPR variances is executed even if both `return_std` and `return_cov` are `False`. Profiling shows this takes ~96% of the computation time. I would like to see the `y_mean` value returned before this step to save time.\n\n### Describe your proposed solution\n\nAbove `V = solve_triangular` https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/gaussian_process/_gpr.py#L454, add\n\n```\nif not return_std and not return_cov:\n    return y_mean\n```\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "lesteve",
        "body": "Indeed, so this seems like a regression from a while ago (the commit you listed happened in 1.0, I think).\n\nA PR fixing this would be more than welcome! Ideally, a quick stand-alone snippet that reproduces the issue would be nice as well."
      },
      {
        "user": "RafaAyGar",
        "body": "Hey! I would like to work on this issue"
      },
      {
        "user": "RafaAyGar",
        "body": "/take"
      }
    ]
  },
  {
    "issue_number": 31367,
    "title": "Inconsistent `median`/`quantile` behaviour now `_weighted_percentile` ignores NaNs",
    "author": "lucyleeow",
    "state": "closed",
    "created_at": "2025-05-15T11:32:37Z",
    "updated_at": "2025-05-21T08:40:04Z",
    "labels": [
      "module:utils"
    ],
    "body": "As of https://github.com/scikit-learn/scikit-learn/pull/29034, `_weighted_percentile` handles NaNs by ignoring them when calculating `percentile`.\n`np.median` and `np.percentile` on the other hand, will return NaN if a NaN is present in the input (`np.nanmedian` and `np.nanpercentile` will ignore nans).\n\nThere are many cases in the codebase where, if `sample_weight` is `None`, a `np` function is used (NaN returned), if `sample_weight` is given, `_weighted_percentile` used and NaNs ignored.\n\nSummary of affected cases:\n\n* `DummyRegressor.fit`\n* `AbsoluteError`/`PinballLoss`/`HuberLoss`  - `fit_intercept_only` method\n* `median_absolute_error`\n* `d2_pinball_score`\n* `SplineTransformer._get_base_knot_positions` - I think this was the original reason for https://github.com/scikit-learn/scikit-learn/pull/29034\n\nMaybe we could assess on a case by case basis whether it makes sense to return NaN if present in the input? @ogrisel suggested that we may want to raise a warning in some cases as well.\n\ncc @StefanieSenger",
    "comments": [
      {
        "user": "MagicDake",
        "body": "\\take\n"
      },
      {
        "user": "MagicDake",
        "body": "/take\n"
      },
      {
        "user": "MagicDake",
        "body": "Hi all,\n\nThanks for the detailed issue and discussion around the inconsistent NaN handling in _weighted_percentile and its downstream effects. I've taken a closer look at the code paths mentioned in the issue, and I can confirm the described inconsistencies.\n\nHere's a summary of what I found:\n\n**Core Inconsistency:**\n\n- When sample_weight is None, functions like DummyRegressor.fit (with strategy=\"median\" or \"quantile\"), AbsoluteError.fit_intercept_only, PinballLoss.fit_intercept_only, and median_absolute_error typically rely on np.median() or np.percentile(). These NumPy functions propagate NaNs (i.e., if there's a NaN in the input, the result is NaN).\n\n- When sample_weight is provided, these functions (or the functions they call, like _weighted_percentile) generally switch to a behavior where NaNs in the input array (y_true or absolute errors) are ignored during the calculation (as _weighted_percentile itself does).\n\n- HuberLoss.fit_intercept_only is an interesting case, as it consistently ignores NaNs by calling _weighted_percentile with an array of ones for weights even when sample_weight is None.\n- The d2_pinball_score function also exhibits this inconsistency, as its internal calculation of y_quantile (the baseline prediction) uses np.percentile (propagates NaNs) when sample_weight is None, and _weighted_percentile (ignores NaNs) when sample_weight is present. This, combined with how mean_pinball_loss handles (or doesn't explicitly handle) NaNs in its direct inputs, leads to different outcomes.\n\n**Affected Components Investigated:**\n\n- DummyRegressor(strategy=\"median\").fit\n- DummyRegressor(strategy=\"quantile\").fit\n- AbsoluteError().fit_intercept_only\n- PinballLoss().fit_intercept_only\n- median_absolute_error()\n- d2_pinball_score()\n\nThis inconsistent behavior depending on the mere presence of the sample_weight argument can indeed be confusing and lead to unexpected results.\n**Moving Forward -(Unifying Behavior):**\nThe key question seems to be: What should the desired consistent behavior be regarding NaNs for these functions?\n\n1. Option A: Always ignore NaNs (similar to how _weighted_percentile currently behaves, and how np.nanmedian / np.nanpercentile work). If this is the preferred direction (which PR #29034 also seemed to lean towards for _weighted_percentile itself):\n\n- \n- The fix would involve modifying the code paths where sample_weight is None to use np.nanmedian(), np.nanpercentile(), or to call _weighted_percentile() with a dummy array of ones as weights.\n\n2. Option B: Always propagate NaNs (current np.median/np.percentile behavior).\n\n- This would require _weighted_percentile to be modified to propagate NaNs if present in the input array (perhaps via a new parameter, though this might be a more significant change to its current design).\n\nGiven the existing behavior of _weighted_percentile and its use in contexts where ignoring NaNs with weights is useful, Option A (always ignore NaNs) seems like a potentially more straightforward path to consistency for these higher-level functions.\nI'd appreciate any thoughts from the maintainers on the preferred uniform behavior. Once a direction is decided, I'd be happy to try and help with a PR.\nThanks!"
      }
    ]
  },
  {
    "issue_number": 26595,
    "title": "UX: Enhance the HTML displays",
    "author": "GaelVaroquaux",
    "state": "open",
    "created_at": "2023-06-16T07:41:09Z",
    "updated_at": "2025-05-21T08:33:22Z",
    "labels": [
      "New Feature",
      "RFC"
    ],
    "body": "### Describe the workflow you want to enable\n\nWhen I interact when non-advanced users a recurrent difficulty for them is finding information and understanding what is going on.\n\n\n\n### Describe your proposed solution\n\nI think that we can guide users with better html displays.  In general, what would be desirable is to give ways to the users to access all the information that an estimator knows about itself, but avoiding to add any lengthy computation during fit. Of course the difficulty of any UX which is that adding more information leads to crowding, and thus the UX needs to be kept light and focused.\n\nI propose to do changes in an iterative way, adding one feature after the other. Here are the ideas that I have in mind:\n\n* [ ] Display the result of \"get_params\" (not visible by default, either folded or in a hover)\n   * [x] #21266\n* [x] Add a link to the API documentation. This link would be inferred from the version of the module, the import path and the name of the class. For instance sklearn.cluster._spectral.SpectralClustering would lead to https://scikit-learn.org/1.2/modules/generated/sklearn.cluster.SpectralClustering.html . Note that we will have to apply heuristics such as dropping the last modules in the path if they are private. Also, we will have to be careful to cater for non scikit-learn classes inheriting from our BaseEstimator, and thus define an override mechanism and probably check that the imported module corresponds to the one for which the path was defined\n* [x] Display in a light way whether the estimator has been fit or not\n* [ ] Display the estimator's parameters\n  * [ ] Add a \"?\" symbol redirecting to the parameter documentation (cf. https://github.com/scikit-learn/scikit-learn/pull/30763#issuecomment-2737192403)\n* [ ] Display the (public) fitted attributes\n  * [ ] at least dtype and shape for array-valued attributes\n  * [ ] maybe a few summary statistics for array-valued attributes.\n* [ ] Display the methods of an estimator with a tooltip with a documentation portion\n* [ ] Display the feature names\n* [ ] Display the shape of outgoing data structures\n* [ ] Reorganize the HTML diagram to have a more condensed view or a more vertical view\n* [ ] Fix the string representation (not displays) using a more vertical appearance (cf. black style)\n\nIn terms of plan, I propose to first adapt our current display without changing its main philosophy. Hence we need to add light accessors of the information, and not a huge list of things (think \"mac\", design).\n\nAnother important thing to keep in mind is that, for users, the hardest things to comprehend are composite estimators, such as pipelines. Most users do not understand how they can access internal objects in these.\n\ncc @amueller who, if I understand correctly, has been pushing these ideas for a long time. Also cc @thomasjpfan who has always shown impressive skills at html.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\nWe should make sure that the displays work and are easy to view in all the relevant environments: jupyter notebooks, vscode.\n\nThis means that avoid javascript. If needed, we can consider using https://purecss.io/ for buttons, tabs, ...",
    "comments": [
      {
        "user": "eskayML",
        "body": "I have a question though,\r\nIs this display enhancement only for using sklearn in notebooks?\r\nBecause I'm really curious on how some of those things you mentioned could be implemented for those running sklearn in scripts on their terminal.\r\n"
      },
      {
        "user": "GaelVaroquaux",
        "body": "> Is this display enhancement only for using sklearn in notebooks?\n\nOr in vscode, or any other environment supporting html displays, which has become quite standard these days.\n\n> how some of those things you mentioned could be implemented for those running sklearn in scripts on their terminal.\n\nScripts run in terminal are much less interactive than an html display and we cannot develop rich visual UX in them. The users running in such environments can always rely on the classic object-oriented API of scikit-learn to access all these information.\n\nIn addition, if it is an actual script, running mostly without user interaction, the problem is more related to logging, on which @jeremiedbb is working to enable much more powerful tracking of execution.\n\n> â€”\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n> You are receiving this because you authored the thread.*Message ID: \n> ***@***.***>\n\n\n-- \n    Gael Varoquaux\n    Research Director, INRIA\n    http://gael-varoquaux.info            http://twitter.com/GaelVaroquaux\n"
      },
      {
        "user": "koaning",
        "body": "Something that came up the other day was to perhaps add sizes to the diagram. Suppose I have something relatively complex, like below, it might be nice to be able to figure out the size/shape of the outgoing array/dataframe.\r\n\r\n![CleanShot 2024-06-05 at 07 12 19@2x](https://github.com/scikit-learn/scikit-learn/assets/1019791/11907c35-6575-43e4-9a85-f06ed3bffe24)\r\n"
      }
    ]
  },
  {
    "issue_number": 21266,
    "title": "Display all parameter values in a tabular for a tab of the notebook HTML repr of estimators",
    "author": "ogrisel",
    "state": "closed",
    "created_at": "2021-10-07T08:39:48Z",
    "updated_at": "2025-05-21T08:33:21Z",
    "labels": [
      "New Feature"
    ],
    "body": "### Describe the workflow you want to enable\r\n\r\nI would to make it easy to quickly introspect the default hyperparameter values of scikit-learn estimator in a jupyter notebook session.\r\n\r\n### Describe your proposed solution\r\n\r\nThe HTML widget returned by `BaseEstimator._repr_mimebundle_` could display a tab with a tabular rendering of the content of the `estimator.get_params()` method.\r\n\r\nIn the case of meta-estimators with complex diagrams, we could optionally have an option to display this only for a subestimator of interest but I have no particular UI suggestion in mind.\r\n\r\n### Additional context\r\n\r\nLoosely related to #21240 which is about another improvement, this time for the text rendering of complex estimators.",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Other ideas:\r\n\r\n- for fitted estimators: make it possible to display `feature_names_in_` as a list (possibly ellipsed in the middle if longer than 100 elements).\r\n- for fitted estimators: list fitted attribute names and display value on click?"
      },
      {
        "user": "ogrisel",
        "body": "Note that it's possible to generate links to the relevant part of the online doc for each individual parameter using text fragments:\n\nhttps://github.com/scikit-learn/scikit-learn/pull/30763#issuecomment-2737192403"
      }
    ]
  },
  {
    "issue_number": 31397,
    "title": "âš ï¸ CI failed on Wheel builder (last failure: May 20, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-05-20T04:33:44Z",
    "updated_at": "2025-05-21T04:35:16Z",
    "labels": [],
    "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/15128452134)** (May 20, 2025)\n",
    "comments": [
      {
        "user": "lesteve",
        "body": "Seems to be very similar to https://github.com/scikit-learn/scikit-learn/issues/31371#issuecomment-2885961870, `HTTP Error 429: Too Many Requests` when getting https://github.com/pypa/get-virtualenv/blob/20.30.0/public/virtualenv.pyz?raw=true.\n\nTemporary glitch will likely be closed tomorrow. I guess we can keep an eye on it if it happens too often.\n\n```\n+ Download https://github.com/pypa/get-virtualenv/blob/20.30.0/public/virtualenv.pyz?raw=true to /Users/runner/Library/Caches/cibuildwheel/virtualenv-20.30.0.pyz\n  Traceback (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/__main__.py\", line 430, in <module>\n      main()\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/__main__.py\", line 49, in main\n      main_inner(global_options)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/__main__.py\", line 184, in main_inner\n      build_in_directory(args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/__main__.py\", line 351, in build_in_directory\n      platform_module.build(options, tmp_path)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/macos.py\", line 449, in build\n      base_python, env = setup_python(\n                         ^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/macos.py\", line 239, in setup_python\n      env = virtualenv(\n            ^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/util.py\", line 717, in virtualenv\n      virtualenv_app = _ensure_virtualenv(version)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/util.py\", line 644, in _ensure_virtualenv\n      download(url, path)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/util.py\", line 369, in download\n      with urllib.request.urlopen(url, context=context) as response:\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 216, in urlopen\n      return opener.open(url, data, timeout)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 525, in open\n      response = meth(req, response)\n                 ^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 634, in http_response\n      response = self.parent.error(\n                 ^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 557, in error\n      result = self._call_chain(*args)\n               ^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 496, in _call_chain\n      result = func(*args)\n               ^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 749, in http_error_302\n      return self.parent.open(new, timeout=req.timeout)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 525, in open\n      response = meth(req, response)\n                 ^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 634, in http_response\n      response = self.parent.error(\n                 ^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 557, in error\n      result = self._call_chain(*args)\n               ^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 496, in _call_chain\n      result = func(*args)\n               ^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 749, in http_error_302\n      return self.parent.open(new, timeout=req.timeout)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 525, in open\n      response = meth(req, response)\n                 ^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 634, in http_response\n      response = self.parent.error(\n                 ^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 563, in error\n      return self._call_chain(*args)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 496, in _call_chain\n      result = func(*args)\n               ^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 643, in http_error_default\n      raise HTTPError(req.full_url, code, msg, hdrs, fp)\n  urllib.error.HTTPError: HTTP Error 429: Too Many Requests\n```"
      },
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://github.com/scikit-learn/scikit-learn/actions/runs/15153105724) on May 21, 2025"
      }
    ]
  },
  {
    "issue_number": 31395,
    "title": "RuntimeWarnings: divide by zero, overflow, invalid value encountered in matmul",
    "author": "mlescobarm",
    "state": "closed",
    "created_at": "2025-05-19T18:02:46Z",
    "updated_at": "2025-05-20T16:26:30Z",
    "labels": [
      "Bug",
      "Needs Triage"
    ],
    "body": "### Describe the bug\n\nWhile running feature selection, I get the following warnings:\n.../lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n  ret = a @ b\n.../lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n  ret = a @ b\n.../lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n ret = a @ b\n\n\n\n### Steps/Code to Reproduce\n\nfrom sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.svm import SVR\nX, y = make_friedman1(n_samples=500, n_features=100, random_state=0)\nestimator = SVR(kernel=\"linear\")\nselector = RFECV(estimator, step=1, cv=5)\nselector = selector.fit(X, y)\nprint(selector.support_)\n\n### Expected Results\n\n[ True  True False  True  True False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False]\n\n### Actual Results\n\n.../lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n  ret = a @ b\n.../lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n  ret = a @ b\n.../lib/python3.12/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n  ret = a @ b\n...\n[ True  True False  True  True False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False]\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.8 (v3.12.8:2dc476bcb91, Dec  3 2024, 14:43:19) [Clang 13.0.0 (clang-1300.0.29.30)]\nexecutable: /Users/******/PyEnvs/JAN2025/bin/python\n   machine: macOS-15.5-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.1.1\n   setuptools: 75.8.0\n        numpy: 2.2.5\n        scipy: 1.15.0\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 10\n         prefix: libomp\n       filepath: /Users/******/PyEnvs/JAN2025/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib\n        version: None\n```",
    "comments": [
      {
        "user": "lesteve",
        "body": "I can not reproduce the behaviour on a Linux machine.\n\nDo you happen to have a macOS M4? If you do have a macOS M4, it could be linked https://github.com/numpy/numpy/issues/28687.\n\nI am going to close this issue, because it is likely to be an issue in numpy rather than scikit-learn. Fee free to reopen if there is a clear indication that the problem is related to scikit-learn."
      },
      {
        "user": "mlescobarm",
        "body": "Yes, the problem arises on macOS M4.  Thanks\r\n\r\n> On May 20, 2025, at 00:41, LoÃ¯c EstÃ¨ve ***@***.***> wrote:\r\n> \r\n> \r\n> lesteve\r\n>  left a comment \r\n> (scikit-learn/scikit-learn#31395)\r\n>  <https://github.com/scikit-learn/scikit-learn/issues/31395#issuecomment-2893296638>\r\n> I can not reproduce the behaviour on a Linux machine.\r\n> \r\n> Do you happen to have a macOS M4, if yes it could be linked numpy/numpy#28687 <https://github.com/numpy/numpy/issues/28687>,\r\n> \r\n> I am going to close this issue, because it is likely to be an issue in numpy rather than scikit-learn. Fee free to reopen if there is a clear indication that the problem is related to scikit-learn.\r\n> \r\n> â€”\r\n> Reply to this email directly, view it on GitHub <https://github.com/scikit-learn/scikit-learn/issues/31395#issuecomment-2893296638>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/A4JSO54BIZEP5OV2PK4PEGD27LMCVAVCNFSM6AAAAAB5OMODAKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDQOJTGI4TMNRTHA>.\r\n> You are receiving this because you authored the thread.\r\n> \r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 31399,
    "title": "DOC Jupyterlite raises a ValueError when using plotly",
    "author": "ArturoAmorQ",
    "state": "closed",
    "created_at": "2025-05-20T10:06:51Z",
    "updated_at": "2025-05-20T14:19:10Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the issue linked to the documentation\n\nRunning for instance `plot_forest_hist_grad_boosting_comparison` in jupyterlite raises a `ValueError: Mime type rendering requires nbformat>=4.2.0 but it is not installed`. I tried adding `%pip install nbformat` at the top of the notebook cell but that doesn't seem to work. As per [this post in stackoverflow](https://stackoverflow.com/questions/69304838/plotly-cannot-find-nbformat-even-though-its-there-jupyter-notebook), downgrading `nbformat` to `5.1.2` solved this issue for me.\n\n### Suggest a potential alternative/fix\n\nAdd a magic function `%pip install nbformat==5.1.2` whenever plotly is imported.",
    "comments": [
      {
        "user": "lesteve",
        "body": "I opened #31400 with a potential fix.\n\nFor further reference [link to the JupyterLite example](https://scikit-learn.org/dev/lite/lab/index.html?path=auto_examples/ensemble/plot_forest_hist_grad_boosting_comparison.ipynb)\n\nIn the last cell the figure does show up but there is an error:\n![Image](https://github.com/user-attachments/assets/755690d0-083f-4851-8c35-351332bc5939)"
      }
    ]
  },
  {
    "issue_number": 31389,
    "title": "Incomplete cleanup of Boston dataset",
    "author": "stefan6419846",
    "state": "closed",
    "created_at": "2025-05-19T12:32:09Z",
    "updated_at": "2025-05-20T12:45:17Z",
    "labels": [
      "good first issue",
      "help wanted"
    ],
    "body": "### Describe the bug\n\nIn #24603, the Boston dataset has been removed. Nevertheless, the corresponding dataset apparently is still being distributed with the package: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/data/boston_house_prices.csv This does not look correct.\n\n### Steps/Code to Reproduce\n\nNot required.\n\n### Expected Results\n\nThe corresponding data file is removed as well.\n\n### Actual Results\n\nThe corresponding data file is still distributed.\n\n### Versions\n\n```shell\n1.6.1 and `main`.\n```",
    "comments": [
      {
        "user": "lesteve",
        "body": "It looks like an oversight indeed, good catch! A PR deleting the file would be more than welcome!"
      }
    ]
  },
  {
    "issue_number": 31382,
    "title": "ENH assert statement using AssertionError for `_agglomerative.py` file",
    "author": "nobu1",
    "state": "closed",
    "created_at": "2025-05-19T04:34:29Z",
    "updated_at": "2025-05-20T09:45:22Z",
    "labels": [
      "New Feature",
      "Needs Triage"
    ],
    "body": "### Describe the workflow you want to enable\n\nAccording to the [Bandit Developers document](https://bandit.readthedocs.io/en/latest/plugins/b101_assert_used.html#module-bandit.plugins.asserts), assert is removed with compiling to optimised byte code (python -O producing *.opt-1.pyc files). This caused various protections to be removed. Consider raising a semantically meaningful error or AssertionError instead.\nAs `_agglomerative.py` has the assert keyword, I would like to update the assert statement.\n\n### Describe your proposed solution\n\nMy proposed solution is to use AssertionError instead of assert.\nCurrent (Line 616):\n`assert n_clusters <= n_samples`\n\nProposal:\n`if not (n_clusters <= n_samples):`\n            `raise AssertionError`\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\nIf you accept my offer, I will make a PR.",
    "comments": [
      {
        "user": "lesteve",
        "body": "Thanks for the issue! I would agree that in general `assert` should be replaced by `AssertionError` with a user-friendly error message, but at the same time don't think this is likely to be at the top of our priority list, so I am going to close this one.\n\nIf you are looking to contribute to scikit-learn, I would suggest looking at [Issues for new contributors](https://scikit-learn.org/dev/developers/contributing.html#issues-for-new-contributors) and more generally our [Contributing guide](https://scikit-learn.org/dev/developers/contributing.html)"
      }
    ]
  },
  {
    "issue_number": 30000,
    "title": "30000 !",
    "author": "jeremiedbb",
    "state": "closed",
    "created_at": "2024-10-03T12:34:10Z",
    "updated_at": "2025-05-20T08:36:20Z",
    "labels": [
      "Easy",
      "spam"
    ],
    "body": ":tada: :birthday: :tada: ",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Note: labeling this issue as spam was meant to be a joke. I was on triaging duty this week and was not sure how to best relabel it ;)"
      },
      {
        "user": "jeremiedbb",
        "body": "Don't worry I took it that way :)\r\n\r\nI was thinking we could use this issue to discuss the new road map if we want to."
      },
      {
        "user": "lesteve",
        "body": "Closing this one!\n\nI happened to look at existing \"Easy\" issues and this is one of them ..."
      }
    ]
  },
  {
    "issue_number": 31364,
    "title": "Tfidf no genera los cluster correctos para oraciones con poco significado y palabras repetidas",
    "author": "zamir5895",
    "state": "closed",
    "created_at": "2025-05-14T20:10:20Z",
    "updated_at": "2025-05-20T07:48:26Z",
    "labels": [
      "New Feature",
      "Needs Triage"
    ],
    "body": "### Describe the workflow you want to enable\n\nDado el sigueinte csv:\ntexto,categoria\n\"el gato el gato el gato el gato el gato\",\"gato\"\n\"el perro el perro el perro el perro el perro\",\"perro\"\n\"la casa la casa la casa la casa la casa\",\"casa\"\n\"el aviÃ³n el aviÃ³n el aviÃ³n el aviÃ³n el aviÃ³n\",\"aviÃ³n\"\n\"la playa la playa la playa la playa la playa\",\"playa\"\n\"el gato el perro el gato el perro el gato\",\"mezcla\"\n\"el perro el gato el perro el gato el perro\",\"mezcla\"\n\"la playa la casa la playa la casa la playa\",\"mezcla\"\nAl usar tfidf con stop words y ngramas el cluster de la ultima oracion de nuestro csv no lo agrupa en el cluster correcto, que en este caso deberia estar con la oracion 6 y 7\n\n### Describe your proposed solution\n\nPodemos mencionar las limitaciones con textos repetidos en la documentacion o mejorar los calculos para poder manejar textos con poco significado semantico y palabras repetidas.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "lesteve",
        "body": "I am afraid I am going to close this one. Feel free to reopen if you translate your question in English."
      }
    ]
  },
  {
    "issue_number": 31370,
    "title": "âš ï¸ CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: May 17, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-05-16T02:50:02Z",
    "updated_at": "2025-05-19T08:54:52Z",
    "labels": [],
    "body": "**CI is still failing on [Linux_Nightly.pylatest_pip_scipy_dev](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76604&view=logs&j=dfe99b15-50db-5d7b-b1e9-4105c42527cf)** (May 17, 2025)\n- test_r_regression[True]\n- test_f_regression[csr_matrix]",
    "comments": [
      {
        "user": "lesteve",
        "body": "Looks like a change in scipy-dev\n\n```py\nfrom scipy import sparse\n\nm = sparse.csr_matrix([[1, 2, 3], [4, 5, 6]])\nm.mean(axis=0)\n```\n\nscipy 1.15.2 is a numpy `matrix`\n```\nmatrix([[2.5, 3.5, 4.5]])\n```\n\nscipy-dev is an numpy `array`\n```\narray([[2.5, 3.5, 4.5]])\n```\n\nSomehow we are relying on this in `r_regression` to flatten the result of `X.mean(axis=0)`\nhttps://github.com/scikit-learn/scikit-learn/blob/675736ae9e3f360975e1f5b477a8eba68a376ba9/sklearn/feature_selection/_univariate_selection.py#L370-L386"
      },
      {
        "user": "lesteve",
        "body": "It seems like this is linked to #22873, I commented https://github.com/scipy/scipy/pull/22873#issuecomment-2887015659 to ask about it there."
      },
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76626&view=logs&j=dfe99b15-50db-5d7b-b1e9-4105c42527cf) on May 19, 2025"
      }
    ]
  },
  {
    "issue_number": 31377,
    "title": "âš ï¸ CI failed on Wheel builder (last failure: May 18, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-05-18T04:40:56Z",
    "updated_at": "2025-05-19T04:39:43Z",
    "labels": [
      "Needs Triage"
    ],
    "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/15092078672)** (May 18, 2025)\n",
    "comments": [
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://github.com/scikit-learn/scikit-learn/actions/runs/15103927263) on May 19, 2025"
      }
    ]
  },
  {
    "issue_number": 31371,
    "title": "âš ï¸ CI failed on Wheel builder (last failure: May 16, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-05-16T04:36:09Z",
    "updated_at": "2025-05-17T04:36:26Z",
    "labels": [],
    "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/15060247770)** (May 16, 2025)\n",
    "comments": [
      {
        "user": "lesteve",
        "body": "Looks like a temporary glitch. This will likely be closed tomorrow.\n\n```\nSetting up build environment...\n  \n  + Download https://github.com/pypa/get-virtualenv/blob/20.30.0/public/virtualenv.pyz?raw=true to /Users/runner/Library/Caches/cibuildwheel/virtualenv-20.30.0.pyz\n  Traceback (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/__main__.py\", line 430, in <module>\n      main()\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/__main__.py\", line 49, in main\n      main_inner(global_options)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/__main__.py\", line 184, in main_inner\n      build_in_directory(args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/__main__.py\", line 351, in build_in_directory\n      platform_module.build(options, tmp_path)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/macos.py\", line 449, in build\n      base_python, env = setup_python(\n                         ^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/macos.py\", line 239, in setup_python\n      env = virtualenv(\n            ^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/util.py\", line 717, in virtualenv\n      virtualenv_app = _ensure_virtualenv(version)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/util.py\", line 644, in _ensure_virtualenv\n      download(url, path)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cibuildwheel/util.py\", line 369, in download\n      with urllib.request.urlopen(url, context=context) as response:\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 216, in urlopen\n      return opener.open(url, data, timeout)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 525, in open\n      response = meth(req, response)\n                 ^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 634, in http_response\n      response = self.parent.error(\n                 ^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 563, in error\n      return self._call_chain(*args)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 496, in _call_chain\n      result = func(*args)\n               ^^^^^^^^^^^\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 643, in http_error_default\n      raise HTTPError(req.full_url, code, msg, hdrs, fp)\n  urllib.error.HTTPError: HTTP Error 429: Too Many Requests\n```"
      },
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://github.com/scikit-learn/scikit-learn/actions/runs/15081354087) on May 17, 2025"
      }
    ]
  },
  {
    "issue_number": 13108,
    "title": "MLPRegressor gives different results on different machines.",
    "author": "tomaso909",
    "state": "closed",
    "created_at": "2019-02-07T19:17:29Z",
    "updated_at": "2025-05-16T01:54:28Z",
    "labels": [],
    "body": "#### Description\r\nI am running the exact same script on my local computer and on AWS Lambda and I get different results when using the MLPRegressor. Both machines have the same scipy, numpy and scikit-learn versions installed.\r\n\r\n#### Steps/Code to Reproduce\r\nI use a 6000 samples 4 features dataset but I have test it and the problem occurs with any dataset.\r\nThis is the code I use to build the MLPRegressor (just a very simple generic code).\r\n```python\r\nnet = MLPRegressor(solver='lbfgs', random_state=1)\r\nstats = net.fit(indep, dep) # indep is the input data array (4x6000) and dep is the target values array (1x6000)\r\nprint 'INFO [ResponseSurface|_neuralNetwork]: MLP fit R^2 =', net.score(indep, dep)\r\n```\r\nThis bug is a bit complex to replicate. If no one can think of a reason for this behavior straight away, then I can prepare a nice self contained case that runs on a docker container with the python2.7 AWS Lambda environment from [https://github.com/lambci/docker-lambda](url) \r\n\r\n#### Expected Results\r\nI expect the exact same output on both RS.\r\n\r\n#### Actual Results\r\nThe outputs of each RS are different by tiny to large amounts, depending on the input e.g. 20 vs 100.\r\nThe output of net.score() is also different (as expected), e.g. for my particular dataset I get 0.9277594232479545 vs 0.9283560222462522.\r\n\r\n#### Versions\r\n\r\n##### Local\r\nI installed the packages with conda (I tried both, anaconda and conda-forge channels).\r\n```\r\nLinux-4.10.0-38-generic-x86_64-with-debian-stretch-sid\r\n('Python', '2.7.15 |Anaconda, Inc.| (default, Dec 14 2018, 19:04:19) \\n[GCC 7.3.0]')\r\n('NumPy', '1.15.1')\r\n('SciPy', '1.2.0')\r\n('Scikit-Learn', '0.19.2')\r\n```\r\n\r\n##### AWS Lambda\r\nCreating the environment to run on Lambda with scipy/numpy/scikit is not straightforward since the C libraries required are platform dependent (so my local environment cannot be used directly). To go around this, I am using the pre-compiled libraries that target the lambda machine available in the lib folder of this repo: [https://github.com/ryfeus/lambda-packs/tree/master/Sklearn_scipy_numpy/source](url).\r\n\r\nThe numpy/scipy/scikit packages provided in the repo are bit old (scikit-learn is version 0.17.1) so I downloaded the wheel files of newer versions that matched my local version from pypi and replaced them. I kept using the same C libraries (inside lib) since they still worked and no additional ones were required with the newer versions.\r\n```\r\nLinux-4.14.94-73.73.amzn1.x86_64-x86_64-with-glibc2.2.5\r\n('Python', '2.7.12 (default, Sep 1 2016, 22:14:00) \\n[GCC 4.8.3 20140911 (Red Hat 4.8.3-9)]')\r\n('NumPy', '1.15.1')\r\n('SciPy', '1.2.0') \r\n('Scikit-Learn', '0.19.2') \r\n```\r\n\r\n",
    "comments": [
      {
        "user": "jnothman",
        "body": "We can't generally ensure stable results across platforms. They have different floating point arithmetic implementations among other things. Given its many free parameters and local minima, â€‹it's not surprising that MLP is particularly susceptible to this issue."
      },
      {
        "user": "tomaso909",
        "body": "@jnothman Thanks for the reply.\r\n\r\nIf I run my local version on a docker container with an AWS Lambda image, will I get the same result or this is a hardware thing?"
      },
      {
        "user": "zhaoxiaofei",
        "body": "Hi, I observed that LogisticRegression can also give different results (with the main differerence being the intercepts, 0 vs 8 and a coefficient, 0 vs 1) from the same dataset on differnent machines using the default limited-memory BFGS algorithm. However, if I switched to saga, then the problem seems to disappear. Is this behavior as expected?"
      }
    ]
  },
  {
    "issue_number": 31349,
    "title": "Add Multiple Kernel Learning (MKL) for Support Vector Machines (SVM)",
    "author": "thomasbauer76",
    "state": "open",
    "created_at": "2025-05-10T08:24:28Z",
    "updated_at": "2025-05-15T16:53:23Z",
    "labels": [
      "New Feature",
      "Needs Decision"
    ],
    "body": "### Describe the workflow you want to enable\n\nI propose adding a [Multiple Kernel Learning (MKL)](https://en.wikipedia.org/wiki/Multiple_kernel_learning) module for kernel optimization in kernel-based methods (such as SVM) to scikit-learn. MKL is a more advanced approach compared to GridSearchCV, offering a way to combine multiple kernels into a single, optimal kernel. In the worst case, MKL will behave like GridSearchCV by assigning a weight of 1 to the best kernel, but in the other cases, it will provide a weighted combination of kernels for better generalization.\n\n### Describe your proposed solution\n\nI have already implemented a complete MKL solution for regression, binary and multi-class classification, and clustering (One-Class). This implementation includes the [SimpleMKL algorithm](https://www.jmlr.org/papers/volume9/rakotomamonjy08a/rakotomamonjy08a.pdf), which optimizes the weights of the kernels, as well as the AverageMKL (simply averages the kernels) and SumMKL (simply sums the kernels) algorithms. This implementation is available on a [previously closed pull request](https://github.com/scikit-learn/scikit-learn/pull/31166).\n\n### Describe alternatives you've considered, if relevant\n\nAn alternative would be to continue relying on GridSearchCV for kernel selection. However, GridSearchCV is limited to selecting only one kernel and does not consider the possibility of combining multiple kernels, which can result in suboptimal performance. MKL provides a more sophisticated approach by optimizing kernel weights, leading to better performance in many machine learning tasks.",
    "comments": [
      {
        "user": "Nikita2812",
        "body": "/take"
      },
      {
        "user": "adrinjalali",
        "body": "@Nikita2812 this issue is to discuss inclusion of a feature, and the author already has a PR implementing it. So I'm not sure what it is that you're \"take\"ing. Please read the issues carefully and assess the situation before claiming them.\n\n@thomasbauer76 a few of us had a mini-discussion about this last week. The citation counts certainly pass our inclusion criteria, and it certainly improves upon standard single kernel SVMs. However, like SVMs, it's usually quite hard to find a nice kernel for the data at hand. In the modern age, we're not sure about including this in the library, due to the balance between potential usage, and maintenance cost.\n\nOn the plus side, this effort can update our existing vendored libsvm, which can be a blessing. But that can be done w/o having to include MKL (or updating it might make including MKL almost trivial), and we welcome any efforts trying to update our libsvm. It could even potentially be a git submodule which we'd update as the upstream updates.\n\ncc @scikit-learn/core-devs for visibility and more opinions here."
      },
      {
        "user": "thomasbauer76",
        "body": "> [@Nikita2812](https://github.com/Nikita2812) this issue is to discuss inclusion of a feature, and the author already has a PR implementing it. So I'm not sure what it is that you're \"take\"ing. Please read the issues carefully and assess the situation before claiming them.\n> \n> [@thomasbauer76](https://github.com/thomasbauer76) a few of us had a mini-discussion about this last week. The citation counts certainly pass our inclusion criteria, and it certainly improves upon standard single kernel SVMs. However, like SVMs, it's usually quite hard to find a nice kernel for the data at hand. In the modern age, we're not sure about including this in the library, due to the balance between potential usage, and maintenance cost.\n> \n> On the plus side, this effort can update our existing vendored libsvm, which can be a blessing. But that can be done w/o having to include MKL (or updating it might make including MKL almost trivial), and we welcome any efforts trying to update our libsvm. It could even potentially be a git submodule which we'd update as the upstream updates.\n> \n> cc @scikit-learn/core-devs for visibility and more opinions here.\n\nSorry for the late answer.\n\nThanks for the detailed feedback. I totally understand the concerns regarding maintenance and usage.\n\nIf I find some free time, Iâ€™d be happy to look into updating the vendored libsvm by adding alpha seeding!"
      }
    ]
  },
  {
    "issue_number": 31365,
    "title": "TargetEncoder example code",
    "author": "mathisdrn",
    "state": "closed",
    "created_at": "2025-05-14T22:11:18Z",
    "updated_at": "2025-05-15T06:03:08Z",
    "labels": [
      "Documentation",
      "Needs Triage"
    ],
    "body": "### Describe the issue linked to the documentation\n\nThe example used in the [stable TargetEncoder documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html) is confusing as the order of the label (that is: dog, cat, snake) is not coherent with the expected order of `enc_low_smooth.encodings_` (the 80 corresponds to 'dog' but is is in second order not first). \n\nPrinting `TargetEncoder.categories_` reveal that the order is indeed coherent with `TargetEncoder.encodings_`. However, as I was trying to understand where this difference of order came from, I wasn't able to find in [TargetEncoder class definition](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/preprocessing/_target_encoder.py) where `self.categories_` was set.  \n\n### Suggest a potential alternative/fix\n\n- make it more explicit in documentation, such as adding a print of `enc_auto.categories_`\n- make `TargetEncoder()` preserve the columns order found in the dataset ",
    "comments": [
      {
        "user": "lucyleeow",
        "body": "I suspect it is because string classes are sorted lexicographically, and it has caused confusion before thus the related issues:\nhttps://github.com/scikit-learn/scikit-learn/issues/14954, https://github.com/scikit-learn/scikit-learn/issues/13631\n\nIn the interest of housekeeping, I would be for closing this issue as duplicate.\n\n(@mathisdrn feel free to add to existing issues, or re-open if you disagree)"
      }
    ]
  },
  {
    "issue_number": 22709,
    "title": "Create a similar class to KMeans that uses medians instead of means (KMedians)",
    "author": "raymondj-pace",
    "state": "open",
    "created_at": "2022-03-06T17:06:50Z",
    "updated_at": "2025-05-14T07:20:13Z",
    "labels": [
      "New Feature",
      "module:cluster",
      "Needs Decision - Include Feature"
    ],
    "body": "### Describe the workflow you want to enable\r\n\r\nI would like a new class: sklearn.cluster.KMedians (or an option to sklearn.cluster.KMeans) that allows the methods to use medians instead of means.\r\n\r\nK-n clustering can greatly improve some instances where there are outliers. Using the median minimizes outliers.\r\n\r\n### Describe your proposed solution\r\n\r\nCreate a new class sklearn.cluster.KMedians that works the same as sklearn.cluster.KMeans but instead uses median to compute the new centroids (instead of using mean)\r\n\r\n### Describe alternatives you've considered, if relevant\r\n\r\nUsing a version I wrote:\r\n\r\n```\r\nclass mu_type(enum.IntEnum):\r\n    mean = 1\r\n    median = 2\r\n\r\n\r\n#\r\n# Euclidean distance between two vectors\r\n#\r\ndef euclidean_distance(row1, row2):\r\n    distance = 0.0\r\n    for i in range(len(row1)):\r\n        distance += (row1[i] - row2[i])**2\r\n    return math.sqrt(distance)\r\n\r\n\r\n#\r\n# k-means and k-medians clustering\r\n# mu defines if the algorithm runs as k-means or k-medians\r\n#\r\ndef k_m_clustering_2(x, c, mu=mu_type.mean):\r\n    \r\n    d = [[0]*len(c) for i1 in range(len(x))]\r\n    \r\n    l = [0]*len(x)\r\n    c_last = [0]*len(c)\r\n    \r\n    while c_last != c:\r\n        \r\n        # Save the last list of center points to compare for updates later\r\n        c_last = c.copy()\r\n        \r\n        for i in range(len(x)):\r\n            for j in range(len(c)):\r\n                if DEBUG:\r\n                    print('distance between: ', end='')\r\n                    print(x[i], end='')\r\n                    print(' and ', end='')\r\n                    print(c[j], end='')\r\n                    \r\n                d[i][j] = euclidean_distance(x[i], c[j])\r\n                \r\n                if DEBUG:\r\n                    print(' = ', end='')\r\n                    print(d[i][j])\r\n                    \r\n            l[i] = d[i].index(min(d[i]))\r\n\r\n        if DEBUG:\r\n            print(\"L = \", end='')\r\n            print(l)\r\n            print()\r\n            print()\r\n        \r\n        if DEBUG:\r\n            print(\"Old center points: \", end='')\r\n            print(c)\r\n            print()\r\n\r\n        #\r\n        # Update center points\r\n        #\r\n        for i in range(len(c)):\r\n    \r\n            #\r\n            # Compute mean or median for new center points\r\n            #       \r\n            if mu == mu_type.mean:\r\n                count = 0\r\n                _x = []\r\n                _y = []\r\n                for j in range(len(l)):\r\n                    if l[j] == i:\r\n                        count += 1\r\n                        _x.append(x[j][0])\r\n                        _y.append(x[j][1])\r\n                c[i] = [(sum(_x)/count), (sum(_y)/count)]\r\n                \r\n            elif mu == mu_type.median:\r\n                x_y = []\r\n                for j in range(len(l)):\r\n                    if l[j] == i:\r\n                        x_y.append([x[j][0], x[j][1]])\r\n                c[i] = median(x_y)\r\n            \r\n        if DEBUG:\r\n            print(\"New center points: \", end='')\r\n            print(c)\r\n            print()\r\n    \r\n    if DEBUG:\r\n        print(\"Center points have not changed\\n\")\r\n        print(\"Final: \", end='')\r\n        print(l)\r\n        print()\r\n    \r\n    return c, l\r\n\r\n\r\n\r\n\r\n# Call it:\r\n\r\nDEBUG=True\r\n\r\nc = [[2, 2], [3, 4], [6, 2]]\r\nx = [[1, 2], [2, 1], [1, 3], [5, 4], [6, 3], [7, 2], [6, 1]]\r\n\r\nc_arr, l_arr = k_m_clustering_2(x, c, mu_type.median)\r\n\r\nfor i in range(len(x)):\r\n    print('x' + str(i+1) + ' = ' + str(l_arr[i]))\r\nprint('\\n')\r\n```\r\n\r\nOutput:\r\n\r\ndistance between: [1, 2] and [2, 2] = 1.0\r\ndistance between: [1, 2] and [3, 4] = 2.8284271247461903\r\ndistance between: [1, 2] and [6, 2] = 5.0\r\ndistance between: [2, 1] and [2, 2] = 1.0\r\ndistance between: [2, 1] and [3, 4] = 3.1622776601683795\r\ndistance between: [2, 1] and [6, 2] = 4.123105625617661\r\ndistance between: [1, 3] and [2, 2] = 1.4142135623730951\r\ndistance between: [1, 3] and [3, 4] = 2.23606797749979\r\ndistance between: [1, 3] and [6, 2] = 5.0990195135927845\r\ndistance between: [5, 4] and [2, 2] = 3.605551275463989\r\ndistance between: [5, 4] and [3, 4] = 2.0\r\ndistance between: [5, 4] and [6, 2] = 2.23606797749979\r\ndistance between: [6, 3] and [2, 2] = 4.123105625617661\r\ndistance between: [6, 3] and [3, 4] = 3.1622776601683795\r\ndistance between: [6, 3] and [6, 2] = 1.0\r\ndistance between: [7, 2] and [2, 2] = 5.0\r\ndistance between: [7, 2] and [3, 4] = 4.47213595499958\r\ndistance between: [7, 2] and [6, 2] = 1.0\r\ndistance between: [6, 1] and [2, 2] = 4.123105625617661\r\ndistance between: [6, 1] and [3, 4] = 4.242640687119285\r\ndistance between: [6, 1] and [6, 2] = 1.0\r\nL = [0, 0, 0, 1, 2, 2, 2]\r\n\r\n\r\nOld center points: [[2, 2], [3, 4], [6, 2]]\r\n\r\nNew center points: [[1, 3], [5, 4], [6, 3]]\r\n\r\ndistance between: [1, 2] and [1, 3] = 1.0\r\ndistance between: [1, 2] and [5, 4] = 4.47213595499958\r\ndistance between: [1, 2] and [6, 3] = 5.0990195135927845\r\ndistance between: [2, 1] and [1, 3] = 2.23606797749979\r\ndistance between: [2, 1] and [5, 4] = 4.242640687119285\r\ndistance between: [2, 1] and [6, 3] = 4.47213595499958\r\ndistance between: [1, 3] and [1, 3] = 0.0\r\ndistance between: [1, 3] and [5, 4] = 4.123105625617661\r\ndistance between: [1, 3] and [6, 3] = 5.0\r\ndistance between: [5, 4] and [1, 3] = 4.123105625617661\r\ndistance between: [5, 4] and [5, 4] = 0.0\r\ndistance between: [5, 4] and [6, 3] = 1.4142135623730951\r\ndistance between: [6, 3] and [1, 3] = 5.0\r\ndistance between: [6, 3] and [5, 4] = 1.4142135623730951\r\ndistance between: [6, 3] and [6, 3] = 0.0\r\ndistance between: [7, 2] and [1, 3] = 6.082762530298219\r\ndistance between: [7, 2] and [5, 4] = 2.8284271247461903\r\ndistance between: [7, 2] and [6, 3] = 1.4142135623730951\r\ndistance between: [6, 1] and [1, 3] = 5.385164807134504\r\ndistance between: [6, 1] and [5, 4] = 3.1622776601683795\r\ndistance between: [6, 1] and [6, 3] = 2.0\r\nL = [0, 0, 0, 1, 2, 2, 2]\r\n\r\n\r\nOld center points: [[1, 3], [5, 4], [6, 3]]\r\n\r\nNew center points: [[1, 3], [5, 4], [6, 3]]\r\n\r\nCenter points have not changed\r\n\r\nFinal: [0, 0, 0, 1, 2, 2, 2]\r\n\r\nx1 = 0\r\nx2 = 0\r\nx3 = 0\r\nx4 = 1\r\nx5 = 2\r\nx6 = 2\r\nx7 = 2\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "comments": [
      {
        "user": "Micky774",
        "body": "Do you have any resources which suggest the use of median over/instead of mean? The scikit-learn [inclusion criteria](https://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms) are pretty high, since these kinds of additions will add a significant amount of maintenance overhead. \r\n\r\nAlso your proof-of-concept code and outputs would probably be better shared as a [gist](https://gist.github.com/) rather than text in an issue :)"
      },
      {
        "user": "raymondj-pace",
        "body": "https://en.wikipedia.org/wiki/K-medians_clustering\r\n\r\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5982597/\r\n\r\nhttps://archive.siam.org/meetings/sdm06/proceedings/015andersonb.pdf\r\n\r\nThere are numerous references as to why it's used and needed. Thanks."
      },
      {
        "user": "jeremiedbb",
        "body": "Duplicate of https://github.com/scikit-learn/scikit-learn/issues/6454. Let's keep this one open as there are not many discussions there anyway."
      }
    ]
  },
  {
    "issue_number": 31183,
    "title": "Upper bound the build dependencies in `pyproject.toml` for release branches",
    "author": "thomasjpfan",
    "state": "closed",
    "created_at": "2025-04-11T15:13:13Z",
    "updated_at": "2025-05-13T13:30:57Z",
    "labels": [
      "Build / CI",
      "Needs Decision"
    ],
    "body": "### Describe the workflow you want to enable\n\nUpper bound the build dependencies on release branches makes it easier to build the wheel in the future. This has two benefits:\n\n- The wheels become easier to build when using the newest build dependency does not work. (Historically, I've seen issues with Cython)\n- If we wanted to backport a fix the wheel building is more stable.\n\n### Describe your proposed solution\n\nOn release branches, provide a upper bound to the build dependencies in `pyproject.toml`.\n\nSciPy does this already: https://github.com/scipy/scipy/blob/e3228cdfe42e403ed203db16e4db4822eb416797/pyproject.toml#L1-L13\n\n### Describe alternatives you've considered, if relevant\n\nLeave the build dependencies to be unbounded.",
    "comments": [
      {
        "user": "RishabhSpark",
        "body": "Hi, I'd like to work on this. \n\nJust to clarify the scope:\n- This change would only apply to the pyproject.toml files on release branches (e.g. 1.6.X, 1.5.X, etc.), correct?\n- Should the upper bounds be based on the latest known working versions at the time of the release, or is there a preferred policy for setting them?\n- Are there specific build dependencies you'd like to target (e.g. Cython, setuptools, wheel, etc.), or should I mirror the approach used in SciPy?\n\nI took a look at the SciPy example you shared and it makes sense. Happy to draft a PR once I confirm the above."
      },
      {
        "user": "thomasjpfan",
        "body": "@RishabhSpark I do not recommend working on this yet. The maintainers need to decide if this is a good idea before we move forward with an implementation.\n\n---\n\n> This change would only apply to the pyproject.toml files on release branches (e.g. 1.6.X, 1.5.X, etc.), correct?\n\nYes\n\n> Should the upper bounds be based on the latest known working versions at the time of the release, or is there a preferred policy for setting them?\n\nIf we have a upper bound, then we need a policy. I'll say the current version that works at the time.\n\n> Are there specific build dependencies you'd like to target (e.g. Cython, setuptools, wheel, etc.), or should I mirror the approach used in SciPy?\n\nOnly for the build dependencies listed here: https://github.com/scikit-learn/scikit-learn/blob/cd119bb24293bb8bfcbef97d8b53c992c75286b2/pyproject.toml#L99-L104"
      },
      {
        "user": "RishabhSpark",
        "body": "> [@RishabhSpark](https://github.com/RishabhSpark) I do not recommend working on this yet. The maintainers need to decide if this is a good idea before we move forward with an implementation.\n> \n> > This change would only apply to the pyproject.toml files on release branches (e.g. 1.6.X, 1.5.X, etc.), correct?\n> \n> Yes\n> \n> > Should the upper bounds be based on the latest known working versions at the time of the release, or is there a preferred policy for setting them?\n> \n> If we have a upper bound, then we need a policy. I'll saw the current version that works at the time.\n> \n> > Are there specific build dependencies you'd like to target (e.g. Cython, setuptools, wheel, etc.), or should I mirror the approach used in SciPy?\n> \n> Only for the build dependencies listed here:\n> \n> [scikit-learn/pyproject.toml](https://github.com/scikit-learn/scikit-learn/blob/cd119bb24293bb8bfcbef97d8b53c992c75286b2/pyproject.toml#L99-L104)\n> \n> Lines 99 to 104 in [cd119bb](/scikit-learn/scikit-learn/commit/cd119bb24293bb8bfcbef97d8b53c992c75286b2)\n> \n>  requires = [ \n>      \"meson-python>=0.16.0\", \n>      \"Cython>=3.0.10\", \n>      \"numpy>=2\", \n>      \"scipy>=1.8.0\", \n>  ]\n\nThanks for the letting me know! That makes sense â€” I'll wait for the inputs from maintainers on the policy or next steps before proceeding."
      }
    ]
  },
  {
    "issue_number": 31356,
    "title": "Benchmark Function",
    "author": "mfeurer",
    "state": "closed",
    "created_at": "2025-05-12T08:32:05Z",
    "updated_at": "2025-05-12T10:32:47Z",
    "labels": [
      "New Feature",
      "Needs Triage"
    ],
    "body": "### Describe the workflow you want to enable\n\nI would like to define multiple pipelines and compare them against each other on multiple datasets.\n\n### Describe your proposed solution\n\nA single helper function that executes this benchmark fully in parallel. This would allow \n\n### Describe alternatives you've considered, if relevant\n\nThere is an [MLR3 function](https://mlr3.mlr-org.com/reference/benchmark.html) that inspired this issue. \n\n### Additional context\n\nReasoning: I'm currently co-teaching a course where students can do the exercises in R using MLR3 or Python using scikit-learn. Doing the exercises in R appears to be less repetitive overall, as for example, there is a simple function for benchmarking. Also, it would require less time to actually wait for the results to finish as one could make more use of parallelism.",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "You can already simply do that by creating your `Pipeline` objects, and passing them to a `cross_val_score` in a look over your datasets and pipelines. Since the code snippet required to do this is just a few lines, I don't think it makes sense for us to add it to the public API."
      }
    ]
  },
  {
    "issue_number": 31304,
    "title": "DOC Link Visualization tools to their respective interpretation in the User Guide",
    "author": "ArturoAmorQ",
    "state": "closed",
    "created_at": "2025-05-05T15:51:12Z",
    "updated_at": "2025-05-12T08:42:08Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the issue linked to the documentation\n\nAs of today, some of our [Display objects](https://scikit-learn.org/dev/visualizations.html#display-objects) point towards the [`Visualizations`](https://scikit-learn.org/dev/visualizations.html) section of the User Guide, some of them point toward the respective plotted function, some of them do both.\n\nAs sometimes users want to know how to interpret the plot and sometimes they want to understand the plot API, we've resorted to linking both, e.g. for the [RocCurveDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.RocCurveDisplay.html) we have:\n\n```\n    For general information regarding `scikit-learn` visualization tools, see\n    the :ref:`Visualization Guide <visualizations>`.\n    For guidance on interpreting these plots, refer to the :ref:`Model\n    Evaluation Guide <roc_metrics>`.\n```\n\nContributors willing to address this issue, please fix **one** of the following listed Display Objects **per pull request**.\n\n- [x] [inspection.PartialDependenceDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.inspection.PartialDependenceDisplay.html) points to [`partial-dependence`](https://scikit-learn.org/dev/modules/partial_dependence.html#partial-dependence). It should point [`Visualizations`](https://scikit-learn.org/dev/visualizations.html) as well. #31313\n\n- [x] [metrics.ConfusionMatrixDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) points only to point [`Visualizations`](https://scikit-learn.org/dev/visualizations.html). It should point to [`confusion-matrix`](https://scikit-learn.org/dev/modules/model_evaluation.html#confusion-matrix) as well. #31306\n\n- [x] [metrics.DetCurveDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.DetCurveDisplay.html) points to [`det-curve`](https://scikit-learn.org/dev/modules/model_evaluation.html#det-curve). It should point [`Visualizations`](https://scikit-learn.org/dev/visualizations.html) as well. #31307\n\n\n- [x] [metrics.PrecisionRecallDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html) points only to point [`Visualizations`](https://scikit-learn.org/dev/visualizations.html). It should point to [`precision-recall-f-measure-metrics`](https://scikit-learn.org/dev/modules/model_evaluation.html#precision-recall-f-measure-metrics) as well. #31308\n\nThanks for your help!\n\n### Suggest a potential alternative/fix\n\n_No response_",
    "comments": [
      {
        "user": "ahmedmokeddem",
        "body": "Hey, I would like to work on the issue (specifcally metrics.ConfusionMatrixDisplay ) @ArturoAmorQ "
      },
      {
        "user": "Azzedde",
        "body": "Hey @ArturoAmorQ I would like to work specifically on [inspection.PartialDependenceDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.inspection.PartialDependenceDisplay.html) "
      },
      {
        "user": "AchrafTasfaout",
        "body": "Hey @ArturoAmorQ  I would like to work on the issue, specifically [metrics.PrecisionRecallDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html)"
      }
    ]
  },
  {
    "issue_number": 31315,
    "title": "SGDRegressor is not inheriting from LinearModel",
    "author": "MarcBresson",
    "state": "open",
    "created_at": "2025-05-06T09:05:13Z",
    "updated_at": "2025-05-11T05:17:58Z",
    "labels": [
      "Enhancement"
    ],
    "body": "### Describe the bug\n\nI wanted to rely on the base class [LinearModel](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_base.py#L267) to identify linear models, but I found out that [SGDRegressor](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_stochastic_gradient.py#L1757) (nor any of its sub classes) is not inheriting this class. However, SGDClassifier is (through LinearClassifierMixin).\n\nIs there any reason for [BaseSGDRegressor](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_stochastic_gradient.py#L1383) to not inherit [LinearModel](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_base.py#L267)? Is it because it overloads all of LinearModel's methods?\n\n### Steps/Code to Reproduce\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model._base import LinearModel\n\nissubclass(SGDRegressor, LinearModel)\n\n### Expected Results\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model._base import LinearModel\n\nissubclass(SGDRegressor, LinearModel)\n# True\n\n### Actual Results\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model._base import LinearModel\n\nissubclass(SGDRegressor, LinearModel)\n# False\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)]\nexecutable: /usr/local/bin/python3.10\n   machine: macOS-14.4.1-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.5.0\n          pip: 24.2\n   setuptools: 74.0.0\n        numpy: 1.26.4\n        scipy: 1.13.1\n       Cython: 3.0.12\n       pandas: 1.5.3\n   matplotlib: 3.8.4\n       joblib: 1.2.0\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n       filepath: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/.dylibs/libopenblas64_.0.dylib\n        version: 0.3.23.dev\nthreading_layer: pthreads\n   architecture: armv8\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n       filepath: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/.dylibs/libopenblas.0.dylib\n        version: 0.3.27\nthreading_layer: pthreads\n   architecture: neoversen1\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libomp\n       filepath: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/.dylibs/libomp.dylib\n        version: None\n```",
    "comments": [
      {
        "user": "ogrisel",
        "body": "> Is there any reason for BaseSGDRegressor to not inherit LinearModel? Is it because it overloads all of LinearModel's methods?\n\nYes, I believe it was historically written independently of other linear models. Maybe subclassing from the base class could allow for some code reuse, although I have not checked.\n\nNote that the `LinearModel` is not meant to be part of the public API and was not intended for model discovery but only for code reuse/factorization. AFAIK scikit-learn does not expose any public API to inspect which models can be considered linear models.\n"
      },
      {
        "user": "ogrisel",
        "body": "If subclassing can help remove redundant code, then I am not opposed to a PR. But if subclassing does not help reduce code complexity, I see little benefit to doing so. Extra indirections introduced by inheritance make it harder to navigate the code base and (slightly) hurt code readability as a result."
      },
      {
        "user": "glemaitre",
        "body": "I looked a bit at the code and I think that we can clean-up something. In addition of the inheritance, I think we should look at the code of `LinearModel` and `LinearClassifierMixin`:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/81bb708dc4c218f801b11fa2751c51e2ba3715b8/sklearn/linear_model/_base.py#L322-L324\n\nI think there is room to improve the clarity of those classes and it should remove some redundant code, notably when it comes to the `_decision_function` call."
      }
    ]
  },
  {
    "issue_number": 31348,
    "title": "âš ï¸ CI failed on Wheel builder (last failure: May 10, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-05-10T04:32:09Z",
    "updated_at": "2025-05-11T04:38:40Z",
    "labels": [
      "Needs Triage"
    ],
    "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14941597365)** (May 10, 2025)\n",
    "comments": [
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://github.com/scikit-learn/scikit-learn/actions/runs/14952022634) on May 11, 2025"
      }
    ]
  },
  {
    "issue_number": 31210,
    "title": "Issues with pairwise_distances(metric='euclidean') when used on the output of UMAP",
    "author": "griffinfarrow",
    "state": "open",
    "created_at": "2025-04-15T17:11:10Z",
    "updated_at": "2025-05-10T21:04:21Z",
    "labels": [
      "Bug",
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nWhen using pairwise_distances with metric='euclidean' on the output of some data from a UMAP, a `RuntimeWarning: divide by zero encountered in matmul ret = a @ b` is raised. This warning is not raised if you just use pairwise_distances on some normally distributed values of the same dimension, it specifically happens when used on the output of UMAP. The warning is not raised if calling `scipy.pdist` on the same data. The warning doesn't come up with any other metric (other than euclidean family e.g. nan_euclidean etc)\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nimport umap.umap_ as umap \nfrom sklearn.metrics import pairwise_distances\n\nnp.random.seed(42)\narr = np.random.normal(size = (300, 10))\nreducer = umap.UMAP()\nembedding = reducer.fit_transform(arr)\n\n# this line produces the warning \ndist_mat = pairwise_distances(embedding, metric = 'euclidean')\n\n# no warning produced by this code\nsynthetic = np.random.normal(size = (300, 2))\ndist_mat_synth = pairwise_distances(synthetic, metric = 'euclidean')\n```\n\n### Expected Results\n\nWould expect to see no RuntimeWarning (FutureWarning is expected)\n\n### Actual Results\n\n``` \n\n[.../site-packages/sklearn/utils/deprecation.py:151]: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n[.../site-packages/sklearn/utils/extmath.py:203] RuntimeWarning: divide by zero encountered in matmul\n  ret = a @ b\n[.../site-packages/sklearn/utils/extmath.py:203]: RuntimeWarning: overflow encountered in matmul\n  ret = a @ b\n[.../site-packages/sklearn/utils/extmath.py:203]: RuntimeWarning: invalid value encountered in matmul\n  ret = a @ b\n\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.16 (main, Feb 25 2025, 09:29:51) [Clang 16.0.0 (clang-1600.0.26.6)]\nexecutable: .../bin/python\n   machine: macOS-15.4-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0.1\n   setuptools: 65.5.0\n        numpy: 2.1.3\n        scipy: 1.15.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 14\n         prefix: libomp\n       filepath: .../site-packages/sklearn/.dylibs/libomp.dylib\n        version: None\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 14\n         prefix: libomp\n       filepath: /opt/homebrew/Cellar/libomp/19.1.7/lib/libomp.dylib\n        version: None\n```",
    "comments": [
      {
        "user": "gangula-karthik",
        "body": "I can work on this issue "
      },
      {
        "user": "gangula-karthik",
        "body": "Iâ€™m unable to reproduce this issue on my ARM-based Mac (Apple Silicon), running Python 3.10.16 and the following library versions:\n\n- libomp 20.1.3 (installed via Homebrew, keg-only)\n- scikit-learn 1.7.dev0 (editable install from source)\n- numpy 2.1.3\n- scipy 1.15.1\n- joblib 1.4.2\n- threadpoolctl 3.5.0\n- Cython 3.0.12\n- matplotlib 3.10.0\n- pandas 2.2.3\n- setuptools 65.5.0\n\nIâ€™ve tested with pairwise_distances(metric=\"euclidean\") on UMAP outputs and am not seeing any RuntimeWarning or overflow issues. Everything runs as expected."
      },
      {
        "user": "gangula-karthik",
        "body": " I also ran some tests with different input scenarios and did not encounter any RuntimeWarning. Here are the test types and the code used:\n\n```\nimport numpy as np\nimport umap\nfrom sklearn.metrics import pairwise_distances\n\ndef test_pairwise_distances(data, label):\n    reducer = umap.UMAP()\n    embedding = reducer.fit_transform(data)\n    print(f\"=== [{label}] ===\")\n    print(f\"Embedding shape: {embedding.shape}\")\n    dist_mat = pairwise_distances(embedding, metric='euclidean')\n    print(f\"Distance matrix: min={np.min(dist_mat)}, max={np.max(dist_mat)}\")\n\n# Tests with different types of data\nnp.random.seed(42)\n\n# Normal random input\narr = np.random.normal(size=(300, 10))\ntest_pairwise_distances(arr, \"Normal Random\")\n\n# Tiny uniform noise\ntiny_uniform = np.random.uniform(low=-1e-5, high=1e-5, size=(300, 10))\ntest_pairwise_distances(tiny_uniform, \"Tiny Uniform Noise\")\n\n# Constant data\nconstant_data = np.full((300, 10), 1.0)\ntest_pairwise_distances(constant_data, \"Constant\")\n\n# Near-constant data\nnear_constant_data = np.full((300, 10), 1.0) + np.random.normal(scale=1e-5, size=(300, 10))\ntest_pairwise_distances(near_constant_data, \"Near-Constant\")\n\n# Float16 precision\nfloat16_data = np.random.normal(size=(300, 10)).astype(np.float16)\ntest_pairwise_distances(float16_data, \"Float16 Precision\")\n\n# Dense clusters (e.g., small clusters of data)\ndense_clusters = np.vstack([np.random.normal(loc=0.0, scale=0.1, size=(100, 10)),\n                            np.random.normal(loc=1.0, scale=0.1, size=(100, 10)),\n                            np.random.normal(loc=2.0, scale=0.1, size=(100, 10))])\ntest_pairwise_distances(dense_clusters, \"Dense Clusters\")\n```\n\n\nFor all tests, the UMAP embedding shape was (300, 2), and pairwise_distances(..., metric='euclidean') ran without warnings. The distance matrix values were valid and within expected ranges.\n\nI'm not sure but the issue could be environment specific ?? Let me know if I can help further\n"
      }
    ]
  },
  {
    "issue_number": 31030,
    "title": "DBSCAN always triggers and EfficiencyWarning",
    "author": "luispedro",
    "state": "open",
    "created_at": "2025-03-19T20:51:03Z",
    "updated_at": "2025-05-09T18:58:34Z",
    "labels": [
      "Bug",
      "help wanted",
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nCalling dbscan always triggers an efficiency warning. There is no apparent way to either call it correctly or disable the warning. \n\nThis was originally reported as an issue in SemiBin, which uses DBSCAN under the hood: https://github.com/BigDataBiology/SemiBin/issues/175\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.cluster import dbscan\nfrom sklearn.neighbors import kneighbors_graph, sort_graph_by_row_values\n\nf = np.random.randn(10_000, 240)\ndist_matrix = kneighbors_graph(\n    f,\n    n_neighbors=200,\n    mode='distance',\n    p=2,\n    n_jobs=3)\n\n_, labels = dbscan(dist_matrix,\n        eps=0.1, min_samples=5, n_jobs=4, metric='precomputed')\n\n\ndist_matrix = sort_graph_by_row_values(dist_matrix)\n_, labels = dbscan(dist_matrix,\n        eps=0.1, min_samples=5, n_jobs=4, metric='precomputed')\n```\n\n### Expected Results\n\nNo warning, at least in second call\n\n### Actual Results\n\n```\n/home/luispedro/.mambaforge/envs/py3.11/lib/python3.11/site-packages/sklearn/neighbors/_base.py:248: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n  warnings.warn(\n/home/luispedro/.mambaforge/envs/py3.11/lib/python3.11/site-packages/sklearn/neighbors/_base.py:248: EfficiencyWarning: Precomputed sparse input was not sorted by row values. Use the function sklearn.neighbors.sort_graph_by_row_values to sort the input by row values, with warn_when_not_sorted=False to remove this warning.\n  warnings.warn(\n```\n\n### Versions\n\n```shell\nI tested on the current main branch, 5cdbbf15e3fade7cc2462ef66dc4ea0f37f390e3, but it has been going on for a while (see original SemiBin report from September 2024):\n\n\nSystem:\n    python: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0]\nexecutable: /home/luispedro/.mambaforge/envs/py3.11/bin/python3.11\n   machine: Linux-6.8.0-55-generic-x86_64-with-glibc2.39\n\nPython dependencies:\n      sklearn: 1.7.dev0\n          pip: 24.0\n   setuptools: 70.0.0\n        numpy: 1.26.4\n        scipy: 1.13.1\n       Cython: None\n       pandas: 2.2.2\n   matplotlib: 3.8.4\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libopenblas\n       filepath: /home/luispedro/.mambaforge/envs/py3.11/lib/libopenblasp-r0.3.27.so\n        version: 0.3.27\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 16\n         prefix: libgomp\n       filepath: /home/luispedro/.mambaforge/envs/py3.11/lib/libgomp.so.1.0.0\n        version: None\n```",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "So this seems to be the culprit:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/89511842526b1f38cff35a2fc199bfd049cc2e1c/sklearn/cluster/_dbscan.py#L408\n\nBefore this line, `_is_sorted_by_data(X)` is `True`, and after this line, it becomes `False`, and hence the warning.\n\nI'm not really sure how to fix this. Maybe @adam2392 has an idea here?"
      },
      {
        "user": "adam2392",
        "body": "If there are 0's in the diagonal, and X.setdiag(X.diagonal()) puts the 0's somewhere where they weren't before, doesn't that imply the sparse data structure is not sorted by data anymore?\n\ndisclaimer: I have not looked into this explicitly, but this is just my first read intuition."
      },
      {
        "user": "yuwei-1",
        "body": "/take"
      }
    ]
  },
  {
    "issue_number": 30007,
    "title": "Upgrade free-threading CI to run with pytest-freethreaded instead of pytest-xdist",
    "author": "ogrisel",
    "state": "open",
    "created_at": "2024-10-04T08:37:23Z",
    "updated_at": "2025-05-09T14:41:30Z",
    "labels": [
      "Build / CI",
      "Needs Decision",
      "module:test-suite",
      "free-threading"
    ],
    "body": "There is a new kid on the block that should help us find out whether scikit-learn and its dependencies can be reliably considered free-threading compatible:\r\n\r\nhttps://pypi.org/project/pytest-freethreaded/\r\n\r\nLet's try to adopt it in scikit-learn.\r\n\r\nHere is a possible plan:\r\n\r\n- first run the tests locally a few times and see if they are tests (or a set of interacting tests) that cause a crash or a failure, open an issue for each of them, possibly upstream and then mark them as skipped under free-threading builds with a reference to the issue in the \"reason\" field;\r\n- then upgrade our nightly free-threading scheduled CI run to use `pytest-freethreaded`.\r\n\r\nAny comments @lesteve @jeremiedbb @ngoldbaum?\r\n\r\nEDIT: anyone interested in getting hands on the first item can find this resource useful:\r\n\r\nhttps://py-free-threading.github.io/\r\n\r\nEDIT 2: there is also the [pytest-run-parallel](https://github.com/Quansight-Labs/pytest-run-parallel) plugin that can serve a similar purpose.",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Discussing with @lesteve we have a problem with test fixture that use on `sklearn.with_config` because the fixture is executed in the main thread while the test is run in a different thread: since `sklearn.with_config` uses thread locals, the configuration change of the fixture is not visible in the test run in a parallel thread.\r\n\r\nMaybe there is a problem of scoping the fixture correctly, and somehow telling pytest to run the fixture in the thread running the test."
      },
      {
        "user": "ogrisel",
        "body": "I think that for a function-scoped fixture (the default fixture scope), it would make sense to ensure that the parallel runner runs the fixture on the same thread as the test function it-self.\r\n\r\nShall we open an issue upstream in the issue tracker of the [pytest-freethreaded repo](https://github.com/tonybaloney/pytest-freethreaded)?"
      },
      {
        "user": "lesteve",
        "body": "Here is a somewhat simple test file that reproduces the issue. Note this happens on vanilla CPython 3.13 not only free-threaded CPython 3.13.\r\n\r\nAs you can tell from the output, the fixture is run in the main thread and the test function in another thread (threadpool created by the pytest plugin).\r\n\r\n```py\r\n# test_simple.py\r\nimport threading\r\n\r\nimport pytest\r\n\r\nthread_local = threading.local()\r\n\r\n\r\n@pytest.fixture\r\ndef change_thread_local():\r\n    print(\"inside fixture\", threading.current_thread())\r\n    setattr(thread_local, \"value\", 1)\r\n    yield\r\n\r\n\r\ndef test_simple(change_thread_local):\r\n    print(\"inside function\", threading.current_thread())\r\n    assert getattr(thread_local, \"value\", None) == 1\r\n```\r\n\r\n```\r\npytest --threads 1 --iterations 1 test_simple.py\r\n```\r\n\r\nOutput:\r\n```\r\n============================================================================================================= test session starts ==============================================================================================================\r\nplatform linux -- Python 3.13.0rc3, pytest-8.3.3, pluggy-1.5.0\r\nrootdir: /tmp/proj\r\nplugins: xdist-3.6.1, freethreaded-0.1.0\r\ncollected 1 item                                                                                                                                                                                                                               \r\n\r\ntest_simple.py F                                                                                                                                                                                                                         [100%]\r\n\r\n=================================================================================================================== FAILURES ===================================================================================================================\r\n_________________________________________________________________________________________________________________ test_simple __________________________________________________________________________________________________________________\r\n\r\nchange_thread_local = None\r\n\r\n    def test_simple(change_thread_local):\r\n        print(\"inside function\", threading.current_thread())\r\n>       assert getattr(thread_local, \"value\", None) == 1\r\nE       AssertionError: assert None == 1\r\nE        +  where None = getattr(<_thread._local object at 0x7b3843ff32e0>, 'value', None)\r\n\r\ntest_simple.py:17: AssertionError\r\n------------------------------------------------------------------------------------------------------------ Captured stdout setup -------------------------------------------------------------------------------------------------------------\r\ninside fixture <_MainThread(MainThread, started 135481613739584)>\r\n------------------------------------------------------------------------------------------------------------- Captured stdout call -------------------------------------------------------------------------------------------------------------\r\ninside function <Thread(ThreadPoolExecutor-0_0, started 135481585043136)>\r\n=========================================================================================================== short test summary info ============================================================================================================\r\nFAILED test_simple.py::test_simple - AssertionError: assert None == 1\r\n============================================================================================================== 1 failed in 0.03s ===============================================================================================================\r\n```"
      }
    ]
  },
  {
    "issue_number": 30810,
    "title": "Windows free-threaded CPython 3.13 ValueError: concurrent send_bytes() calls are not supported",
    "author": "lesteve",
    "state": "closed",
    "created_at": "2025-02-11T14:44:27Z",
    "updated_at": "2025-05-09T10:12:29Z",
    "labels": [
      "Bug",
      "Needs Investigation",
      "free-threading",
      "OS:Windows"
    ],
    "body": "Noticed in [build log](https://github.com/scikit-learn/scikit-learn/actions/runs/13233133978/job/36933421850#step:5:2813). An automated issue was opened in https://github.com/scikit-learn/scikit-learn/issues/30801 and closed the next day.\n\nThis needs some investigation to figure out whether this can be reproduced locally and whether this is actually Windows-specific.\n\nThis may be a joblib issue as well.\n\n```\n================================== FAILURES ===================================\n  _____________________________ test_absolute_error _____________________________\n  \n      def test_absolute_error():\n          # For coverage only.\n          X, y = make_regression(n_samples=500, random_state=0)\n          gbdt = HistGradientBoostingRegressor(loss=\"absolute_error\", random_state=0)\n  >       gbdt.fit(X, y)\n  \n  ..\\venv-test\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\tests\\test_gradient_boosting.py:225: \n  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\base.py:1389: in wrapper\n      return fit_method(estimator, *args, **kwargs)\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:663: in fit\n      X_binned_train = self._bin_data(X_train, is_training_data=True)\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:1178: in _bin_data\n      X_binned = self._bin_mapper.fit_transform(X)  # F-aligned array\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319: in wrapped\n      data_to_wrap = f(self, X, *args, **kwargs)\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\base.py:918: in fit_transform\n      return self.fit(X, **fit_params).transform(X)\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\binning.py:234: in fit\n      non_cat_thresholds = Parallel(n_jobs=self.n_threads, backend=\"threading\")(\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82: in __call__\n      return super().__call__(iterable_with_config_and_warning_filters)\n  ..\\venv-test\\Lib\\site-packages\\joblib\\parallel.py:2007: in __call__\n      return output if self.return_generator else list(output)\n  ..\\venv-test\\Lib\\site-packages\\joblib\\parallel.py:1711: in _get_outputs\n      self._terminate_and_reset()\n  ..\\venv-test\\Lib\\site-packages\\joblib\\parallel.py:1386: in _terminate_and_reset\n      self._backend.terminate()\n  ..\\venv-test\\Lib\\site-packages\\joblib\\_parallel_backends.py:262: in terminate\n      self._pool.close()\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\nuget-cpython\\python-freethreaded.3.13.0\\tools\\Lib\\multiprocessing\\pool.py:652: in close\n      self._change_notifier.put(None)\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\nuget-cpython\\python-freethreaded.3.13.0\\tools\\Lib\\multiprocessing\\queues.py:394: in put\n      self._writer.send_bytes(obj)\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\nuget-cpython\\python-freethreaded.3.13.0\\tools\\Lib\\multiprocessing\\connection.py:200: in send_bytes\n      self._send_bytes(m[offset:offset + size])\n  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n  \n  self = <multiprocessing.connection.PipeConnection object at 0x00000243F43267C0>\n  buf = <memory at 0x00000243F4C65AC0>\n  \n      def _send_bytes(self, buf):\n          if self._send_ov is not None:\n              # A connection should only be used by a single thread\n  >           raise ValueError(\"concurrent send_bytes() calls \"\n                               \"are not supported\")\n  E           ValueError: concurrent send_bytes() calls are not supported\n  \n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\nuget-cpython\\python-freethreaded.3.13.0\\tools\\Lib\\multiprocessing\\connection.py:287: ValueError\n```\n",
    "comments": [
      {
        "user": "lesteve",
        "body": "Seems similar to https://github.com/python/cpython/issues/130733. Closing for now since this seems an issue in CPython rather than in scikit-learn."
      }
    ]
  },
  {
    "issue_number": 31326,
    "title": "âš ï¸ CI failed on Linux_free_threaded.pylatest_free_threaded (last failure: May 07, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-05-07T02:33:31Z",
    "updated_at": "2025-05-08T08:15:43Z",
    "labels": [
      "Needs Triage"
    ],
    "body": "**CI failed on [Linux_free_threaded.pylatest_free_threaded](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76323&view=logs&j=c10228e9-6cf7-5c29-593f-d74f893ca1bd)** (May 07, 2025)\nUnable to find junit file. Please see link for details.",
    "comments": [
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76391&view=logs&j=c10228e9-6cf7-5c29-593f-d74f893ca1bd) on May 08, 2025"
      }
    ]
  },
  {
    "issue_number": 24369,
    "title": "DOC Clarify documentation writing guideline",
    "author": "lucyleeow",
    "state": "closed",
    "created_at": "2022-09-06T05:53:17Z",
    "updated_at": "2025-05-08T01:40:31Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the issue linked to the documentation\r\n\r\nThe ['Guidelines for writing documentation'](https://scikit-learn.org/dev/developers/contributing.html#guidelines-for-writing-documentation) section seems to be specifically about docstrings and the suggestions don't seem to be as relevant for other types of documentation, e.g., example, tutorial, usage pages. \r\n\r\n### Suggest a potential alternative/fix\r\n\r\nClarify that this section is about docstrings. Potentially add sections on guidelines for writing examples, usage and tutorial pages?",
    "comments": [
      {
        "user": "ogrisel",
        "body": "I don't think this is limited to docstrings. What makes you think so?"
      },
      {
        "user": "lucyleeow",
        "body": "Fair, reading it again I can see how it has more general tips than I originally thought, though maybe not necessarily relevant for all types of documentation.\r\n\r\nI think the problem may better be described as: it seems very prescriptive, i.e., too specific in detailing exactly what to include and the order, e.g.\r\n1. hand-waving explanation\r\n2. why method is useful and when it should be used\r\n3. figure\r\n4. code examples\r\n5. math and equations\r\n\r\nThis may not be so relevant for all types of documentation, e.g., docstrings don't usually contain figures.\r\n\r\nI think this issue should be more - make the guide more general for all types of documentation and suggest what could be included in various types of documentation (instead of suggesting that all types of documentation should include all 5 items), but I am also happy to close this issue as this is not as much of a problem as I originally thought."
      },
      {
        "user": "lucyleeow",
        "body": "closing this as it seems the docstring section has been updated and the above guidelines is specified for the user guide, which makes sense."
      }
    ]
  },
  {
    "issue_number": 31224,
    "title": "OneVsRestClassifier when all estimators predict a sample belongs to the other classes",
    "author": "MarcBresson",
    "state": "open",
    "created_at": "2025-04-18T09:11:32Z",
    "updated_at": "2025-05-07T18:54:20Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nHello, I stumbled upon quite a funny case by accident.\n\nIn OneVsRestClassifier, each classifier predicts whether a sample belongs to a specific class, or to any of the other class. For instance, if you have 3 classes, you will have 3  binary classifiers:\n\n- the first one says if the sample belongs to class 1 or to one of the two other classes.\n- the second one says if the sample belongs to class 2 or to one of the two other classes.\n- the third one says if the sample belongs to class 3 or to one of the two other classes.\n\nHowever, it creates an edge case where all of the estimators of OneVsRestClassifier predict a specific sample belongs to the other classes:\n\n- calling .predict() will mark the sample as belonging to the last class (which is of course wrong since in our example, the 3rd estimator said the sample did not belong in that class).\n- calling .predict_proba() will return NaNs values.\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.datasets import make_classification\n\nimport numpy as np\n\n\nclass MyDumbDumbBinaryClassifier(BaseEstimator, ClassifierMixin):\n    def fit(self, X, y):\n        self.classes_ = set(y)\n        return self\n\n    def predict(self, X):\n        return np.array([0 for _ in range(len(X))])\n\n    def predict_proba(self, X):\n        ones = np.ones((len(X), len(self.classes_)))\n        # the proba of being the positive class is always 0\n        ones[:, 1] = 0\n\n        return ones\n    \n\nclf = OneVsRestClassifier(MyDumbDumbBinaryClassifier())\n\nX, y = make_classification(n_classes=3, n_informative=5)\nclf.fit(X, y)\nclf.predict_proba(X)\n```\n\n### Expected Results\n\nI guess .predict() should return NaNs, and .predict_proba() should return a vector of 0s for that sample.\n\n### Actual Results\n\n```python\n>>> clf.predict(X)\n\narray([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n```\n\n```python\n>>> clf.predict_proba(X)\n\narray([[nan, nan, nan],\n       [nan, nan, nan],\n       [nan, nan, nan],\n       [nan, nan, nan],\n       ...\n       [nan, nan, nan],\n       [nan, nan, nan],\n       [nan, nan, nan]])\n```\n\n\n### Versions\n\n```shell\nSystem:\n    python: 3.13.2 (v3.13.2:4f8bb3947cf, Feb  4 2025, 11:51:10) [Clang 15.0.0 (clang-1500.3.9.4)]\nexecutable: /Library/Frameworks/Python.framework/Versions/3.13/bin/python3\n   machine: macOS-14.4.1-arm64-arm-64bit-Mach-O\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 24.3.1\n   setuptools: None\n        numpy: 2.2.3\n        scipy: 1.15.2\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.1\n       joblib: 1.4.2\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libomp\n       filepath: /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/.dylibs/libomp.dylib\n        version: None\n```",
    "comments": [
      {
        "user": "Luis-Varona",
        "body": "The issue comes from the probability normalization in the `predict_proba` method of `OneVsRestClassifier.` Look at lines 520-558 of `sklearn/multiclass.py`:\n```\n    @available_if(_estimators_has(\"predict_proba\"))\n    def predict_proba(self, X):\n        \"\"\"\n        ... (docstring stuff)\n        \"\"\"\n        check_is_fitted(self)\n        # Y[i, j] gives the probability that sample i has the label j.\n        # In the multi-label case, these are not disjoint.\n        Y = np.array([e.predict_proba(X)[:, 1] for e in self.estimators_]).T\n\n        if len(self.estimators_) == 1:\n            # Only one estimator, but we still want to return probabilities\n            # for two classes.\n            Y = np.concatenate(((1 - Y), Y), axis=1)\n\n        if not self.multilabel_:\n            # Then, probabilities should be normalized to 1.\n            Y /= np.sum(Y, axis=1)[:, np.newaxis]\n        return Y\n```\n\nThen you do this:\n\n```\n# %% Original code\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.datasets import make_classification\n\nimport numpy as np\n\n\nclass MyDumbDumbBinaryClassifier(BaseEstimator, ClassifierMixin):\n    def fit(self, X, y):\n        self.classes_ = set(y)\n        return self\n\n    def predict(self, X):\n        return np.array([0 for _ in range(len(X))])\n\n    def predict_proba(self, X):\n        ones = np.ones((len(X), len(self.classes_)))\n        # the proba of being the positive class is always 0\n        ones[:, 1] = 0\n\n        return ones\n    \n\nclf = OneVsRestClassifier(MyDumbDumbBinaryClassifier())\n\nX, y = make_classification(n_classes=3, n_informative=5)\nclf.fit(X, y)\n# Y = clf.predict_proba(X)\n\n\n# %% What's going under the hood when we call `Y = clf.predict_proba(X)`\nestimators = clf.estimators_\nY = np.array([e.predict_proba(X)[:, 1] for e in estimators]).T\nrow_sums = np.sum(Y, axis=1)[:, np.newaxis] # Array of all 0's\nY /= row_sums # Division by zero leads to NaN\n```\n\nSince our Y matrix has a zero row and `OneVsRestClassifier.predict_proba` wants to normalize the probabilities, it ends up dividing by zero and producing undefined behavior. We might want to change the original source code from\n\n```\nif not self.multilabel_:\n            # Then, probabilities should be normalized to 1.\n            Y /= np.sum(Y, axis=1)[:, np.newaxis]\n```\n\nto\n\n```\nif not self.multilabel_:\n            # Then, (nonzero) probabilities should be normalized to 1.\n            row_sums = np.sum(Y, axis=1)[:, np.newaxis]\n            row_sums[row_sums == 0] = 1\n            Y /= row_sums\n```\n\nMaybe worth a PR?"
      },
      {
        "user": "Luis-Varona",
        "body": "PR to fix opened at #31228 for review."
      },
      {
        "user": "jeremiedbb",
        "body": "Thanks for the report @MarcBresson.\n\n> I guess .predict() should return NaNs, and .predict_proba() should return a vector of 0s for that sample.\n\nI agree that `predict_proba` should return 0s, and #31224 implements that fix.\nFor `predict` I'm not sure it should return NaNs. Arguably any prediction is as valid as the other so we could keep the current behavior. Let's see what others think"
      }
    ]
  },
  {
    "issue_number": 31334,
    "title": "Title: Clarify misleading threshold implication in \"ROC with Cross-Validation\" example",
    "author": "rajgurubhosale",
    "state": "closed",
    "created_at": "2025-05-07T14:46:24Z",
    "updated_at": "2025-05-07T15:31:40Z",
    "labels": [
      "Documentation",
      "Needs Triage"
    ],
    "body": "### Describe the issue linked to the documentation\n\nLocation of the issue:\nThe example titled \"Receiver Operating Characteristic (ROC) with cross validation\" [(link)](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html) can lead to misunderstanding regarding decision threshold selection.\n\nðŸ” Description of the problem\nThe example uses RocCurveDisplay.from_estimator() to plot ROC curves for each test fold in cross-validation,\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfor fold, (train, test) in enumerate(cv.split(X, y)):\n    classifier.fit(X[train], y[train])\n    viz = RocCurveDisplay.from_estimator(\n        classifier,\n        X[test],                           the test set is used here instead of train \n        y[test],\n        name=f\"ROC fold {fold}\",\n        alpha=0.3,\n        lw=1,\n        ax=ax,\n        plot_chance_level=(fold == n_splits - 1),\n    )\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0.0\n    tprs.append(interp_tpr)\n    aucs.append(viz.roc_auc)\n\n**here is no warning or clarification that:**\n\n1)Users should not select thresholds based on predictions from these test folds.\n\n2)Even for ROC visualization, using predictions from training folds (via cross_val_predict) avoids potential bias and better simulates threshold tuning workflows.\n\nWithout this guidance, users may mistakenly tune thresholds by inspecting ROC curves on test sets â€” leading to data leakage and over-optimistic results.\n\nâœ… Proposed solution\nreplace the test set with the train set in this code\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfor fold, (train, test) in enumerate(cv.split(X, y)):\n    classifier.fit(X[train], y[train])\n    viz = RocCurveDisplay.from_estimator(\n        classifier,\n        X[train],                           train set is used here\n        y[train],\n        name=f\"ROC fold {fold}\",\n        alpha=0.3,\n        lw=1,\n        ax=ax,\n        plot_chance_level=(fold == n_splits - 1),\n    )\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0.0\n    tprs.append(interp_tpr)\n    aucs.append(viz.roc_auc)\n\nadd point: \nuse predictions from the training folds (e.g., via cross_val_predict) or apply nested cross-validation.\nThis prevents data leakage and ensures realistic model evaluation.\n\n\n\n\n### Suggest a potential alternative/fix\n\nmake changes in this example   [link](https://scikitlearn.org/stable/auto_examples/model_selection/plot_roc_crossval.html)\n\nuse predictions from the training folds (e.g., via cross_val_predict) or apply nested cross-validation.\nThis prevents data leakage and ensures realistic model evaluation.\nhere is revised code\nand also add short docs that for roc we should use the validation train data for the using threshold value. and not the test this will not result in over optimistic model and will not add data leakage\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfor fold, (train, test) in enumerate(cv.split(X, y)):\n    classifier.fit(X[train], y[train])\n    viz = RocCurveDisplay.from_estimator(\n        classifier,\n        X[train],\n        y[train],\n        name=f\"ROC fold {fold}\",\n        alpha=0.3,\n        lw=1,\n        ax=ax,\n        plot_chance_level=(fold == n_splits - 1),\n    )\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0.0\n    tprs.append(interp_tpr)\n    aucs.append(viz.roc_auc)",
    "comments": []
  },
  {
    "issue_number": 31319,
    "title": "Argument order in haversine_distances for latitude/longitude",
    "author": "wlush",
    "state": "closed",
    "created_at": "2025-05-06T11:56:26Z",
    "updated_at": "2025-05-07T11:39:33Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the issue linked to the documentation\n\nHello! I frequently use [sklearn.metrics.pairwise.haversine_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.haversine_distances.html) to estimate distances on the globe. The example on the linked page uses a geographic example, but it does not specify whether geographic coordinates are in (latitude, longitude) or (longitude, latitude) form. From context, one can infer that the correct order is (latitude,longitude); however, it would be useful to explicitly state the order.\n\nThis is my first issue submission; please let me know if there is something more that might be useful for resolution!\n\n### Suggest a potential alternative/fix\n\nThere are two ways that this could be resolved:\n1. Include a short note stating that geographic coordinates should be input as (latitude,longitude)\n2. Include a comment in the example code describing the coordinate order.",
    "comments": [
      {
        "user": "glemaitre",
        "body": "I think it is specified in the documentation that you mentioned:\n\n![Image](https://github.com/user-attachments/assets/9cf985fa-0a4d-4ad1-9902-fe147c2a286b)\n\nThe first column is the latitude and the second is the longitude from what I'm reading now, isn't it?"
      },
      {
        "user": "wlush",
        "body": "You are correct, I wasn't reading it closely enough! Thank you!!"
      }
    ]
  },
  {
    "issue_number": 29048,
    "title": "Make `zero_division` parameter consistent in the different metric",
    "author": "glemaitre",
    "state": "open",
    "created_at": "2024-05-19T18:40:45Z",
    "updated_at": "2025-05-07T10:31:10Z",
    "labels": [
      "Enhancement"
    ],
    "body": "This is an issue to report the step to actually take over the work of @marctorsoc in https://github.com/scikit-learn/scikit-learn/pull/23183 and split the PR into smaller one to facilitate the review process.\r\n\r\nThe intend is to make the `zero_division` parameter consistent across different metrics in scikit-learn. In this regards, we have the following TODO list:\r\n\r\n- [x] Introduce the `zero_division` parameter to the `accuracy_score` function when `y_true` and `y_pred` are empty.\r\n    - https://github.com/scikit-learn/scikit-learn/pull/29213\r\n- [ ] Introduce the `zero_division` parameter to the `class_likelihood_ratios` and remove `raise_warning`.\r\n- [x] Introduce the `zero_division` parameter to the `cohen_kappa_score` function\r\n  - https://github.com/scikit-learn/scikit-learn/pull/29210\r\n- [ ] Introduce the `zero_division` parameter to the `matthew_corr_coeff` function\r\n  - #23183\r\n  - #28509\r\n- [ ] Open a PR to make sure the empty input lead to `np.nan` in `classification_report` function.\r\n\r\nAll those items have been addressed in #23183 and can be extracted in individual PRs. The changelog presenting the changes should acknowledge @marctorsoc.\r\n\r\nIn addition, we should investigate #27047 and check if we should add the `zero_division` parameter to the `precision_recall_curve` and `roc_curve` as well. This might add two additional items to the list above.",
    "comments": [
      {
        "user": "glemaitre",
        "body": "@StefanieSenger you might be interested in looking at some of the item."
      },
      {
        "user": "erikhuck",
        "body": "+1 for this!"
      },
      {
        "user": "Jaimin020",
        "body": "I am working on task-1 (Introduce the zero_division parameter to the accuracy_score function when y_true and y_pred are empty.). "
      }
    ]
  },
  {
    "issue_number": 30936,
    "title": "SelectFromModel does not work when ElasticNetCV has multiple l1 ratios",
    "author": "muhlbach",
    "state": "closed",
    "created_at": "2025-03-04T09:09:30Z",
    "updated_at": "2025-05-07T10:22:17Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nUsing `SelectFromModel` with the automatic `ElasticNetCV` does not work if the `l1_ratio` is estimated from the data, i.e., if the user provides a list of floats. \n\n### Steps/Code to Reproduce\n\n```py\nfrom sklearn.datasets import make_regression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import ElasticNetCV\nestimator = ElasticNetCV(\n    l1_ratio=[0.25, 0.5, 0.75]\n)\nmodel = SelectFromModel(estimator=estimator)\nX, y = make_regression(n_samples=100, n_features=5, n_informative=3)\nmodel.fit(X, y)\nmodel.get_feature_names_out()\n```\n\nThis fails with:\n\n```\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n```\n\nbecause `_calculate_threshold` calls `np.isclose(estimator.l1_ratio, 1.0)` which returns an array with as many elements as l1 ratios.\n\n### Expected Results\n\nCalling `.get_feature_names_out()` should return an ndarray of str according to the best model estimating with CV.\n\n### Actual Results\n\n```\nTraceback (most recent call last):\n  File \"/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-304146ab06de>\", line 1, in <module>\n    model.get_feature_names_out()\n  File \"/.venv/lib/python3.12/site-packages/sklearn/feature_selection/_base.py\", line 190, in get_feature_names_out\n    return input_features[self.get_support()]\n                          ^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.12/site-packages/sklearn/feature_selection/_base.py\", line 67, in get_support\n    mask = self._get_support_mask()\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.12/site-packages/sklearn/feature_selection/_from_model.py\", line 305, in _get_support_mask\n    threshold = _calculate_threshold(estimator, scores, self.threshold)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.12/site-packages/sklearn/feature_selection/_from_model.py\", line 36, in _calculate_threshold\n    if is_l1_penalized or is_lasso or is_elasticnet_l1_penalized:\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.9 (main, Feb  4 2025, 14:38:38) [Clang 16.0.0 (clang-1600.0.26.6)]\nexecutable: /.venv/bin/python\n   machine: macOS-15.1.1-arm64-arm-64bit\nPython dependencies:\n      sklearn: 1.5.2\n          pip: None\n   setuptools: 75.6.0\n        numpy: 1.26.4\n        scipy: 1.14.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.9.3\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\nBuilt with OpenMP: True\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n       filepath: /.venv/lib/python3.12/site-packages/numpy/.dylibs/libopenblas64_.0.dylib\n        version: 0.3.23.dev\nthreading_layer: pthreads\n   architecture: armv8\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libomp\n       filepath: /.venv/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib\n        version: None\n```",
    "comments": [
      {
        "user": "vasco-s-pereira",
        "body": "Hi,\nIâ€™m a first-time contributor to this repository and would love to work on this issue if itâ€™s still available. Could you please let me know if I can take it on?"
      },
      {
        "user": "lesteve",
        "body": "Thanks for the issue, this looks like a real bug. I have to say I am not sure how to handle this case off the top of my head ...\n\n@vasco-s-pereira great if you are looking to contribute to scikit-learn, I would suggest you look at the [Contributing doc](https://scikit-learn.org/stable/developers/contributing.html#new-contributors) and [New Issues for contributors](https://scikit-learn.org/stable/developers/contributing.html#new-contributors)."
      }
    ]
  },
  {
    "issue_number": 31327,
    "title": "âš ï¸ CI failed on Wheel builder (last failure: May 07, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-05-07T04:44:27Z",
    "updated_at": "2025-05-07T10:13:26Z",
    "labels": [
      "Needs Triage"
    ],
    "body": "**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14874735765)** (May 07, 2025)\n",
    "comments": [
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://github.com/scikit-learn/scikit-learn/actions/runs/14874735765) on May 07, 2025"
      }
    ]
  },
  {
    "issue_number": 22827,
    "title": "Improve tests by using global_random_seed fixture to make them less seed-sensitive",
    "author": "ogrisel",
    "state": "open",
    "created_at": "2022-03-14T10:15:11Z",
    "updated_at": "2025-05-07T09:37:59Z",
    "labels": [
      "help wanted",
      "Hard",
      "module:test-suite",
      "Meta-issue"
    ],
    "body": "## Context: the new `global_random_seed` fixture\n\n#22749 introduces a new `global_random_seed` fixture to make it possible to run the same test with any seed between 0 and 99 included. By default, when `SKLEARN_TESTS_GLOBAL_RANDOM_SEED` is not set, this fixture is deterministically returning 42 to keep test runs deterministic by default and avoid any unnecessary disruption. However different CI builds set this seed to other arbitrary values (still deterministic) and nightly schedule builds on Azure now use `SKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"any\"` to progressively explore any seed on the 0-99 range.\n\n## Motivation\n\nThe aim of this new fixture is to make sure that we avoid writing tests that artificially depend on a specific value of the random seed and therefore hiding a real mathematical problem in our code unknowingly (see e.g. https://github.com/scikit-learn/scikit-learn/pull/21701#discussion_r823847947). At the same time we still want to keep the test deterministic and independent of the execution order by default to avoid introducing unnecessary maintenance overhead.\n\nIn addition to making the tests insensitive, randomizing those tests with different seeds has the side benefit of making the assertions of those tests robust to small numerical variations that could otherwise stem from other sources such as platform-specific / dependency-specific numerical rounding variations that we do not cover in our existing CI infrastructure.\n\nMore details about the fixture in the online dev doc for the `SKLEARN_TESTS_GLOBAL_RANDOM_SEED` env variable:\n\nhttps://scikit-learn.org/dev/computing/parallelism.html#environment-variables\n\n## Guidelines to convert existing tests\n\n- We probably do not need to convert all scikit-learn tests to use this fixture. We should instead focus our efforts on tests that actually check for **important mathematical properties** of our estimators or model evaluation tools. For instance, there is no need to check for the seed-insensitivity of tests that checks for the exception messages raised when passing invalid inputs.\n\n- To avoid having to review huge PRs that impact many files at once and can lead to conflicts, let's open PRs that edit at most one test file at a time. For instance use a title such as:\n\n> TST use global_random_seed in sklearn/_loss/tests/test_glm_distribution.py\n\n- Please reference `#22827` in the description of the PR and put the full filename of the test file you edit in the title of the PR.\n\n- To convert an existing test with a fixed seed, the general pattern is to rewrite a function such as:\n\n```python\ndef test_some_function():\n    rng = np.random.RandomState(0)\n    ...\n```\n\nto:\n\n```python\ndef test_some_function(global_random_seed):\n    rng = np.random.RandomState(global_random_seed)\n    ...\n```\n\nand then check that the test function is actually seed-insensitive by running with all seeds between 0 and 99 locally (can be slow! only run for one specific test at a time!):\n\n```\nSKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"all\" pytest sklearn/some_module/test/test_some_module.py -k test_some_function\n```\n\nIf this is not the case, the test will probably need to be reworked to find a more stable to way to check the interesting mathematical properties.\n\n- if the failing assertions are related to the generalization performance of a model, maybe the training set size should be slightly bigger (while keeping the test runtime as fast as possible), or with fewer noisy features or the training should be done with stronger regularization. Or more simply we can relax the tolerance threshold while ensuring it does not become trivial (e.g. by comparing to a trivial baseline);\n\n- if the failing assertions depend on some regularities of a synthetically generated dataset, making decreasing the noise level of the datasets;\n\n- some tests might also fail when encountering data that trigger edge cases such as (near-)tied distances between datapoints that make the outcome of computation unstable. Changing the data generation code to significantly decrease the likelihood of those edge case (e.g. by adding more noise to the input features) can help in those cases.\n\n- **Note**: in most cases, tweaking the tolerances of the assertions is **not** the appropriate way to make the tests pass. The first thing to do is try to understand what the test is checking, if the test is correct, if the expectations of the test are realistic. Then if the test seems correct and *should* pass for all random seed but doesn't, investigate if the estimator or function is bugged. As a last resort, tolerances can be loosened if the test is considered valid but aims to check a statistical property that is highly sensitive to the random seed.\n\nIn some cases, it might be very hard to write a seed-insensitive test that tolerate all seeds between 0 and 99 while still running in less than 1s. In those (hopefully rare) cases, I think it's fine to reduce the range of admissible seeds with the following pattern:\n\n```python\ndef test_some_function(global_random_seed):\n    # Making this test seed-insensitive for the 0-99 range would\n    # be too costly. Restricting to the 0-9 range is necessary to\n    # use small enough datasets that avoid increasing the run time\n    # too much.\n    rng = np.random.RandomState(global_random_seed % 10)\n    ...\n```\n\n- Run the CI for tests that take a `global_random_seed` by pushing a commit message with the following structure:\n\n```\n<title> [all random seeds]\n<test_name_1>\n<test_name_2>\n...\n```\n\nNote, running `git commit --allow-empty` allows you to have a commit message without any changes.\n\nSee the following issue for more details on why testing on the CI is necessary:\n\n- #28959\n\n## List of test modules to upgrade\n\n```\nfind sklearn -name \"test_*.py\"\n```\n\n- [x] sklearn/_loss/tests/test_glm_distribution.py\n- [x] sklearn/_loss/tests/test_link.py\n- [x] sklearn/_loss/tests/test_loss.py  #22847\n- [x] sklearn/cluster/tests/test_affinity_propagation.py\n- [x] sklearn/cluster/tests/test_bicluster.py\n- [x] sklearn/cluster/tests/test_birch.py\n- [x] sklearn/cluster/tests/test_dbscan.py\n- [x] sklearn/cluster/tests/test_feature_agglomeration.py #23700\n- [x] sklearn/cluster/tests/test_hierarchical.py\n- [x] sklearn/cluster/tests/test_k_means.py\n- [x] sklearn/cluster/tests/test_mean_shift.py #30517\n- [x] sklearn/cluster/tests/test_optics.py https://github.com/scikit-learn/scikit-learn/pull/30844\n- [x] sklearn/cluster/tests/test_spectral.py https://github.com/scikit-learn/scikit-learn/pull/24802\n- [x] sklearn/compose/tests/test_column_transformer.py\n- [x] sklearn/compose/tests/test_target.py\n- [x] sklearn/covariance/tests/test_covariance.py\n- [x] sklearn/covariance/tests/test_elliptic_envelope.py\n- [ ] sklearn/covariance/tests/test_graphical_lasso.py\n- [x] sklearn/covariance/tests/test_robust_covariance.py\n- [x] sklearn/cross_decomposition/tests/test_pls.py\n- [x] sklearn/datasets/tests/test_20news.py\n- [x] sklearn/datasets/tests/test_base.py\n- [x] sklearn/datasets/tests/test_california_housing.py\n- [x] sklearn/datasets/tests/test_common.py\n- [x] sklearn/datasets/tests/test_covtype.py\n- [x] sklearn/datasets/tests/test_kddcup99.py\n- [x] sklearn/datasets/tests/test_lfw.py\n- [x] sklearn/datasets/tests/test_olivetti_faces.py\n- [x] sklearn/datasets/tests/test_openml.py\n- [x] sklearn/datasets/tests/test_rcv1.py\n- [x] sklearn/datasets/tests/test_samples_generator.py\n- [x] sklearn/datasets/tests/test_svmlight_format.py\n- [ ] sklearn/decomposition/tests/test_dict_learning.py\n- [x] sklearn/decomposition/tests/test_factor_analysis.py\n- [x] sklearn/decomposition/tests/test_fastica.py\n- [ ] sklearn/decomposition/tests/test_incremental_pca.py\n- [x] sklearn/decomposition/tests/test_kernel_pca.py #30518\n- [ ] sklearn/decomposition/tests/test_nmf.py\n- [ ] sklearn/decomposition/tests/test_online_lda.py\n- [ ] sklearn/decomposition/tests/test_pca.py https://github.com/scikit-learn/scikit-learn/pull/26403\n- [x] sklearn/decomposition/tests/test_sparse_pca.py\n- [x] sklearn/decomposition/tests/test_truncated_svd.py\n- [ ] sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py\n- [ ] sklearn/ensemble/_hist_gradient_boosting/tests/test_bitset.py\n- [ ] sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py\n- [ ] sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py\n- [ ] sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py\n- [ ] sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py\n- [ ] sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py\n- [ ] sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py\n- [ ] sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py\n- [ ] sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py\n- [x] sklearn/ensemble/tests/test_bagging.py\n- [x] sklearn/ensemble/tests/test_base.py\n- [ ] sklearn/ensemble/tests/test_common.py\n- [ ] sklearn/ensemble/tests/test_forest.py\n- [x] sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py\n- [x] sklearn/ensemble/tests/test_gradient_boosting.py\n- [x] sklearn/ensemble/tests/test_iforest.py #22901\n- [ ] sklearn/ensemble/tests/test_stacking.py\n- [x] sklearn/ensemble/tests/test_voting.py\n- [ ] sklearn/ensemble/tests/test_weight_boosting.py\n- [ ] sklearn/experimental/tests/test_enable_hist_gradient_boosting.py\n- [ ] sklearn/experimental/tests/test_enable_iterative_imputer.py\n- [ ] sklearn/experimental/tests/test_enable_successive_halving.py\n- [x] sklearn/feature_extraction/tests/test_dict_vectorizer.py\n- [ ] sklearn/feature_extraction/tests/test_feature_hasher.py\n- [ ] sklearn/feature_extraction/tests/test_image.py\n- [ ] sklearn/feature_extraction/tests/test_text.py\n- [ ] sklearn/feature_selection/tests/test_base.py\n- [ ] sklearn/feature_selection/tests/test_chi2.py\n- [ ] sklearn/feature_selection/tests/test_feature_select.py\n- [ ] sklearn/feature_selection/tests/test_from_model.py\n- [ ] sklearn/feature_selection/tests/test_mutual_info.py\n- [x] sklearn/feature_selection/tests/test_rfe.py\n- [ ] sklearn/feature_selection/tests/test_sequential.py\n- [ ] sklearn/feature_selection/tests/test_variance_threshold.py\n- [x] sklearn/gaussian_process/tests/test_gpc.py\n- [ ] sklearn/gaussian_process/tests/test_gpr.py\n- [ ] sklearn/gaussian_process/tests/test_kernels.py\n- [ ] sklearn/impute/tests/test_base.py\n- [ ] sklearn/impute/tests/test_common.py\n- [ ] sklearn/impute/tests/test_impute.py https://github.com/scikit-learn/scikit-learn/pull/25894\n- [ ] sklearn/impute/tests/test_knn.py\n- [ ] sklearn/inspection/_plot/tests/test_plot_partial_dependence.py\n- [ ] sklearn/inspection/tests/test_partial_dependence.py\n- [ ] sklearn/inspection/tests/test_permutation_importance.py\n- [ ] sklearn/linear_model/_glm/tests/test_glm.py\n- [ ] sklearn/linear_model/_glm/tests/test_link.py\n- [x] sklearn/linear_model/tests/test_base.py\n- [ ] sklearn/linear_model/tests/test_bayes.py\n- [ ] sklearn/linear_model/tests/test_common.py\n- [ ] sklearn/linear_model/tests/test_coordinate_descent.py\n- [ ] sklearn/linear_model/tests/test_huber.py https://github.com/scikit-learn/scikit-learn/pull/30912\n- [ ] sklearn/linear_model/tests/test_least_angle.py\n- [ ] sklearn/linear_model/tests/test_linear_loss.py\n- [ ] sklearn/linear_model/tests/test_logistic.py\n- [ ] sklearn/linear_model/tests/test_omp.py\n- [ ] sklearn/linear_model/tests/test_passive_aggressive.py\n- [ ] sklearn/linear_model/tests/test_perceptron.py\n- [ ] sklearn/linear_model/tests/test_quantile.py\n- [ ] sklearn/linear_model/tests/test_ransac.py\n- [ ] sklearn/linear_model/tests/test_ridge.py\n- [ ] sklearn/linear_model/tests/test_sag.py\n- [ ] sklearn/linear_model/tests/test_sgd.py\n- [ ] sklearn/linear_model/tests/test_sparse_coordinate_descent.py\n- [ ] sklearn/linear_model/tests/test_theil_sen.py\n- [ ] sklearn/manifold/tests/test_isomap.py\n- [ ] sklearn/manifold/tests/test_locally_linear.py\n- [ ] sklearn/manifold/tests/test_mds.py\n- [ ] sklearn/manifold/tests/test_spectral_embedding.py\n- [ ] sklearn/manifold/tests/test_t_sne.py\n- [ ] sklearn/metrics/_plot/tests/test_base.py\n- [ ] sklearn/metrics/_plot/tests/test_common_curve_display.py\n- [ ] sklearn/metrics/_plot/tests/test_confusion_matrix_display.py\n- [ ] sklearn/metrics/_plot/tests/test_det_curve_display.py\n- [ ] sklearn/metrics/_plot/tests/test_plot_confusion_matrix.py\n- [ ] sklearn/metrics/_plot/tests/test_plot_curve_common.py\n- [ ] sklearn/metrics/_plot/tests/test_plot_det_curve.py\n- [ ] sklearn/metrics/_plot/tests/test_plot_precision_recall.py\n- [ ] sklearn/metrics/_plot/tests/test_plot_roc_curve.py\n- [ ] sklearn/metrics/_plot/tests/test_precision_recall_display.py\n- [ ] sklearn/metrics/_plot/tests/test_roc_curve_display.py\n- [ ] sklearn/metrics/cluster/tests/test_bicluster.py\n- [ ] sklearn/metrics/cluster/tests/test_common.py\n- [ ] sklearn/metrics/cluster/tests/test_supervised.py\n- [ ] sklearn/metrics/cluster/tests/test_unsupervised.py\n- [ ] sklearn/metrics/tests/test_classification.py\n- [ ] sklearn/metrics/tests/test_common.py\n- [ ] sklearn/metrics/tests/test_dist_metrics.py\n- [x] sklearn/metrics/tests/test_pairwise_distances_reduction.py https://github.com/scikit-learn/scikit-learn/pull/22862\n- [ ] sklearn/metrics/tests/test_pairwise.py\n- [ ] sklearn/metrics/tests/test_ranking.py\n- [x] sklearn/metrics/tests/test_regression.py https://github.com/scikit-learn/scikit-learn/pull/30865\n- [ ] sklearn/metrics/tests/test_score_objects.py\n- [ ] sklearn/mixture/tests/test_bayesian_mixture.py\n- [ ] sklearn/mixture/tests/test_gaussian_mixture.py\n- [x] sklearn/mixture/tests/test_mixture.py\n- [ ] sklearn/model_selection/tests/test_search.py\n- [ ] sklearn/model_selection/tests/test_split.py\n- [ ] sklearn/model_selection/tests/test_successive_halving.py\n- [ ] sklearn/model_selection/tests/test_validation.py\n- [ ] sklearn/neighbors/tests/test_ball_tree.py\n- [ ] sklearn/neighbors/tests/test_graph.py\n- [ ] sklearn/neighbors/tests/test_kd_tree.py\n- [ ] sklearn/neighbors/tests/test_kde.py\n- [ ] sklearn/neighbors/tests/test_lof.py\n- [ ] sklearn/neighbors/tests/test_nca.py\n- [ ] sklearn/neighbors/tests/test_nearest_centroid.py\n- [ ] sklearn/neighbors/tests/test_neighbors_pipeline.py\n- [ ] sklearn/neighbors/tests/test_neighbors_tree.py\n- [ ] sklearn/neighbors/tests/test_neighbors.py\n- [ ] sklearn/neighbors/tests/test_quad_tree.py\n- [ ] sklearn/neural_network/tests/test_base.py\n- [ ] sklearn/neural_network/tests/test_mlp.py\n- [ ] sklearn/neural_network/tests/test_rbm.py\n- [ ] sklearn/neural_network/tests/test_stochastic_optimizers.py\n- [ ] sklearn/preprocessing/tests/test_common.py\n- [ ] sklearn/preprocessing/tests/test_data.py\n- [ ] sklearn/preprocessing/tests/test_discretization.py\n- [ ] sklearn/preprocessing/tests/test_encoders.py\n- [ ] sklearn/preprocessing/tests/test_function_transformer.py\n- [ ] sklearn/preprocessing/tests/test_label.py\n- [ ] sklearn/preprocessing/tests/test_polynomial.py\n- [ ] sklearn/semi_supervised/tests/test_label_propagation.py\n- [ ] sklearn/semi_supervised/tests/test_self_training.py\n- [ ] sklearn/svm/tests/test_bounds.py\n- [ ] sklearn/svm/tests/test_sparse.py\n- [ ] sklearn/svm/tests/test_svm.py https://github.com/scikit-learn/scikit-learn/pull/25891\n- [ ] sklearn/tests/test_base.py\n- [ ] sklearn/tests/test_build.py\n- [ ] sklearn/tests/test_calibration.py\n- [ ] sklearn/tests/test_check_build.py\n- [ ] sklearn/tests/test_common.py\n- [ ] sklearn/tests/test_config.py\n- [ ] sklearn/tests/test_discriminant_analysis.py\n- [ ] sklearn/tests/test_docstring_parameters.py\n- [ ] sklearn/tests/test_docstrings.py\n- [x] sklearn/tests/test_dummy.py\n- [ ] sklearn/tests/test_init.py\n- [ ] sklearn/tests/test_isotonic.py\n- [ ] sklearn/tests/test_kernel_approximation.py\n- [ ] sklearn/tests/test_kernel_ridge.py\n- [ ] sklearn/tests/test_metaestimators.py\n- [ ] sklearn/tests/test_min_dependencies_readme.py\n- [ ] sklearn/tests/test_multiclass.py\n- [ ] sklearn/tests/test_multioutput.py\n- [ ] sklearn/tests/test_naive_bayes.py\n- [ ] sklearn/tests/test_pipeline.py\n- [ ] sklearn/tests/test_random_projection.py\n- [ ] sklearn/tree/tests/test_export.py\n- [ ] sklearn/tree/tests/test_reingold_tilford.py\n- [ ] sklearn/tree/tests/test_tree.py\n- [ ] sklearn/utils/tests/test_arpack.py\n- [ ] sklearn/utils/tests/test_arrayfuncs.py\n- [ ] sklearn/utils/tests/test_class_weight.py\n- [ ] sklearn/utils/tests/test_cython_blas.py\n- [x] sklearn/utils/tests/test_cython_templating.py\n- [x] sklearn/utils/tests/test_deprecation.py\n- [x] sklearn/utils/tests/test_encode.py\n- [ ] sklearn/utils/tests/test_estimator_checks.py\n- [ ] sklearn/utils/tests/test_estimator_html_repr.py\n- [ ] sklearn/utils/tests/test_extmath.py\n- [ ] sklearn/utils/tests/test_fast_dict.py\n- [x] sklearn/utils/tests/test_fixes.py\n- [x] sklearn/utils/tests/test_graph.py\n- [x] sklearn/utils/tests/test_metaestimators.py\n- [x] sklearn/utils/tests/test_mocking.py\n- [ ] sklearn/utils/tests/test_multiclass.py\n- [ ] sklearn/utils/tests/test_murmurhash.py\n- [x] sklearn/utils/tests/test_optimize.py https://github.com/scikit-learn/scikit-learn/pull/30112\n- [x] sklearn/utils/tests/test_parallel.py\n- [x] sklearn/utils/tests/test_pprint.py\n- [ ] sklearn/utils/tests/test_random.py\n- [ ] sklearn/utils/tests/test_readonly_wrapper.py\n- [ ] sklearn/utils/tests/test_seq_dataset.py\n- [ ] sklearn/utils/tests/test_shortest_path.py\n- [x] sklearn/utils/tests/test_show_versions.py\n- [ ] sklearn/utils/tests/test_sparsefuncs.py\n- [x] sklearn/utils/tests/test_stats.py https://github.com/scikit-learn/scikit-learn/pull/30857\n- [x] sklearn/utils/tests/test_tags.py\n- [ ] sklearn/utils/tests/test_testing.py\n- [ ] sklearn/utils/tests/test_utils.py\n- [ ] sklearn/utils/tests/test_validation.py\n- [ ] sklearn/utils/tests/test_weight_vector.py\n\nNote that some of those files might not have any test to update.\n",
    "comments": [
      {
        "user": "ogrisel",
        "body": "I labeled this issue as hard, as I expect some tests to be hard to upgrade. However I expect the majority of the tests to be easy to convert but we cannot know which will be easy in advance.\r\n\r\n"
      },
      {
        "user": "thomasjpfan",
        "body": "I updated the original message with the feature introduced in https://github.com/scikit-learn/scikit-learn/pull/23026 where we can run the CI with all the seeds by pushing a commit with the proper message. @ogrisel Feel free to update the wording if you find it unclear.\r\n\r\n- Run the CI for tests that take a `global_random_seed` by pushing a commit message with the following structure:\r\n\r\n```\r\n<title> [all random seeds]\r\n<test_name_1>\r\n<test_name_2>\r\n...\r\n```\r\n\r\nNote, running `git commit --allow-empty` allows you to have a commit message without any changes."
      },
      {
        "user": "svenstehle",
        "body": "working on [sklearn/linear_model/tests/test_bayes.py](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/tests/test_bayes.py)"
      }
    ]
  },
  {
    "issue_number": 31323,
    "title": "Add train_validation_test_split for three-way dataset splits",
    "author": "vasco-s-pereira",
    "state": "closed",
    "created_at": "2025-05-06T15:07:03Z",
    "updated_at": "2025-05-07T08:56:30Z",
    "labels": [
      "New Feature",
      "Needs Triage"
    ],
    "body": "### Describe the workflow you want to enable\n\nEnable the user to divide the dataset into 3 parts (train, validation and test) instead of only two (train and test) using only one method. This would present a more elegant solution than using the method train_test_split twice.\n\n```python \nfrom sklearn.model_selection import train_val_test_split\nX_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(\n    X, y,\n    train_size=0.6,\n    val_size=0.2,\n    test_size=0.2,\n    random_state=42,\n    shuffle=True,\n    stratify=y\n)\n```\n\n### Describe your proposed solution\n\nAdd a new method called train_test_validation_split where the dataset is divided into train, validation and test set. The arguments would be the same as the train_test_split method with the additional  val_size, similar to test_size and train_size but for the validation set.\n\n### Describe alternatives you've considered, if relevant\n\nUsing train_test_split twice works, but having a dedicated train_validation_test_split function would be cleaner and more concise.\n\n### Additional context\n\nUsing a validation set helps avoiding both overfitting aswell as underfitting.",
    "comments": [
      {
        "user": "glemaitre",
        "body": "We got such request in the past (cf. https://github.com/scikit-learn/scikit-learn/issues/13990#issuecomment-1534763718) and we advise against adding new parameters and instead used the `train_test_split` functions twice.\n"
      }
    ]
  },
  {
    "issue_number": 15272,
    "title": "sklearn MDS vs skbio PCoA",
    "author": "maxibor",
    "state": "open",
    "created_at": "2019-10-16T13:44:56Z",
    "updated_at": "2025-05-06T20:05:05Z",
    "labels": [
      "Enhancement",
      "Moderate",
      "module:manifold"
    ],
    "body": "[Multi Dimensional Scaling](https://en.wikipedia.org/wiki/Multidimensional_scaling) (MDS) and Principal Coordinate Analysis (PCoA) are two names for the same dimension reduction technique*.\r\n\r\nIn [scikit-learn, MDS](https://github.com/biocore/scikit-bio/blob/master/skbio/stats/ordination/_principal_coordinate_analysis.py) is implemented with the SMACOF algorithm while in other Python libraries (such as [scikit-bio](https://github.com/biocore/scikit-bio/blob/master/skbio/stats/ordination/_principal_coordinate_analysis.py)) and most R packages offering it, it is implemented using singular value decomposition.\r\n\r\nThis is usually quite confusing for people who try out the sklearn implementation of MDS when comparing it with other MDS implementations (sklearn stands out).\r\n\r\nHow could one add another implementation of MDS in sklearn ? Or maybe create a new PCoA method ? \r\n\r\ncc @adrinjalali \r\n\r\n- [Issue #579 being discussed in the scikit-bio repository](https://github.com/biocore/scikit-bio/issues/579)\r\n- [StackOverflow thread](https://stackoverflow.com/questions/38905833/why-is-sklearn-manifold-mds-random-when-skbios-pcoa-is-not)\r\n- [*MDS vs PCoA](https://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-component-analysis-and-multidimensional/14017#14017)",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "I'd be happy to have the other implementation as an alternative, and a parameter to set is as the `algorithm`."
      },
      {
        "user": "amueller",
        "body": "related to https://github.com/scikit-learn/scikit-learn/pull/4485 ?"
      },
      {
        "user": "amueller",
        "body": "What's the relation between PCA and PCoA? They seem quite similar but I'm not really familiar with MDS and have never heard the term PCoA"
      }
    ]
  },
  {
    "issue_number": 31311,
    "title": "Reference CalibrationDisplay from calibration_curve's docstring in a \"See also section\"",
    "author": "metlouf",
    "state": "closed",
    "created_at": "2025-05-05T19:34:17Z",
    "updated_at": "2025-05-06T15:31:50Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the issue linked to the documentation\n\nEnrich documentation like proposed in #31302 for calibration_curve's\n\n### Suggest a potential alternative/fix\n\n_No response_",
    "comments": [
      {
        "user": "Muwinuddin",
        "body": "hi @metlouf \n\nI'd like to work on this documentation improvement. Adding a reference to CalibrationDisplay in the \"See also\" section of the calibration_curve docstring makes perfect sense, as it helps users connect the computation function with the visualization utility.\n\nI plan to:\n\nAdd CalibrationDisplay to the \"See also\" section of calibration_curve.\n\nEnsure the formatting is consistent with other references.\n\nRun the doc build to verify everything renders correctly.\n\nLet me know if there's anything else you'd like me to include. I'd be happy to open a PR for this."
      },
      {
        "user": "metlouf",
        "body": "I already opened a PR and solve the problem\n\n(In the context of the probabl hackathon 6 may)"
      },
      {
        "user": "Muwinuddin",
        "body": "@metlouf \nThanks for the update! Glad to see the issue resolved. Appreciate your quick work."
      }
    ]
  },
  {
    "issue_number": 22226,
    "title": "Add a `return_std_of_f` kwarg to GPC's `predict` and `predict_proba`, just like the one GPR has",
    "author": "miguelgondu",
    "state": "closed",
    "created_at": "2022-01-15T22:21:19Z",
    "updated_at": "2025-05-06T12:07:40Z",
    "labels": [
      "New Feature",
      "API",
      "Needs Decision",
      "module:gaussian_process"
    ],
    "body": "### Describe the workflow you want to enable\n\nGaussian Process Regression in sklearn comes with the affordance to return standard deviations of the predictions in the `gpr.predict` method. The Gaussian Process Classifier doesn't. The main reason why (I assume) is that we don't have direct access to the standard deviation of our class estimates. We do, however, have access to the standard deviation of the *latent variable* f, and giving users access to it may allow them to have access to uncertainty estimates of the predicted classes by e.g. ancestral sampling or other Monte Carlo methods.\r\n\r\nIn this issue, I propose we add a keyword argument `return_std_of_f=False` to the methods `predict` and `predict_proba` of `sklearn.gaussian_processes.GaussianProcessClassifier`. This would allow us to return the standard deviation of the latent function f, a value that is already being computed in the `predict_proba` method.\n\n### Describe your proposed solution\n\nThe solution is actually pretty simple, since the computations are already being done in the method `gpc.predict_proba`. Namely, [This line](https://github.com/scikit-learn/scikit-learn/blob/1d1aadd0711b87d2a11c80aad15df6f8cf156712/sklearn/gaussian_process/_gpc.py#L310) corresponds to computing the variance of the latent function $f$'s prediction on the test points. This addition would only need to be\r\n\r\n1. The same computation under an `if` for the `gpc.predict` method.\r\n2. An `if` statement with different returns according to the `return_std_of_f` flag for the `gpc.predict_proba` method.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "miguelgondu",
        "body": "While implementing it I'm running into an issue. It is easy to do for the `_BinaryGaussianProcessClassifierLaplace` estimator, but it is somewhat not trivial for the multiclass setting, because the `OneVsRestClassifier` and `OneVsOneClassifier` have been abstracted away, and are very generic for estimators.\r\n\r\nThis flag is however very particular to GPCs. I implemented it only for the binary setting using Laplace's approximation."
      },
      {
        "user": "jungtaekkim",
        "body": "Hi,\r\n\r\nCan I ask why this issue and the associated PR are not wrapped up?\r\n\r\nI think this issue is important for maintaining consistency with GaussianProcessRegressor."
      }
    ]
  },
  {
    "issue_number": 31164,
    "title": "Fix ConvergenceWarning in `plot_gpr_on_structured_data.py` example",
    "author": "StefanieSenger",
    "state": "open",
    "created_at": "2025-04-09T08:45:27Z",
    "updated_at": "2025-05-06T09:15:54Z",
    "labels": [
      "help wanted",
      "Needs Investigation"
    ],
    "body": "This issue is about addressing a `ConvergenceWarning` that occurs when running the `examples/gaussian_process/plot_gpr_on_structured_data.py `example in CI (also when building the documentation locally).\n\nThe example creates three plots. The last use case on a classification of DNA sequences throws a `ConvergenceWarning` related to the `baseline_similarity_bounds` defined in a custom kernel when fitting. It seems that the lower bound is pushed resulting in the lack of convergence.\n\nThis occurs with the setting `baseline_similarity_bounds=(1e-5, 1))` in the custom kernel.\n\nEven setting `baseline_similarity_bounds=(1e-40, 1)) ` results in the same warning:\n```\nConvergenceWarning: The optimal value found for dimension 0 of parameter baseline_similarity is close to the specified lower bound 1e-40. Decreasing the bound and calling fit again may find a better value.\n```\n\nLowering the bound further with `baseline_similarity_bounds=(1e-50, 1)) ` results in a different warning stemming from `lbfgs`:\n```\nConvergenceWarning: lbfgs failed to converge (status=2): ABNORMAL: .\nIncrease the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html\n```\n\nIt would be preferable to resolve this so the example can be build without displaying warnings.\n\n\nWhile being at the example, other small improvements are welcome (for instance fixing the typo in \"use of kernel functions that operates\" (the s in operates)).\n",
    "comments": [
      {
        "user": "Victor164134",
        "body": "Hi! I'd like to try to work on this issue."
      },
      {
        "user": "EngineerDanny",
        "body": "Hi @StefanieSenger and @jeremiedbb , I stumbled upon this issue and I have looked into it since it has been idle for some time now. I think it is really interesting.\n\n- The safest way to address this issue is to freeze the `baseline_similarity_bounds` parameter to fixed instead of optimizing (That instantly fixes the issue).\n\n- The other option was to make sure the lower bound of `baseline_similarity_bounds` lower enough so that optimization can converge. But you tried that already and got another warning of \n\n``` ConvergenceWarning: lbfgs failed to converge (status=2):\nABNORMAL: . Increase the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  _check_optimize_result(\"lbfgs\", opt_res)\n```\n\nUpon looking into this warning, I thought maybe then increasing the `max_iter_predict` in\n`gp = GaussianProcessClassifier(kernel, max_iter_predict=500_000_000)` will fix it but `max_iter_predict` does not control the iterations of the optimizer at all. So I looked at the optimizer code here [_lbfgsb_py.py](https://github.com/scipy/scipy/blob/7ecbcb8c2ebcbb8c87d2fc98a98bbc9f7e34f497/scipy/optimize/_lbfgsb_py.py#L92) and I found that the default value of the `max_iter` is 15000. This means that the only way to solve the issue then is to create a custom optimizer with the required max_iteration and put it in `gp = GaussianProcessClassifier(kernel=kernel, optimizer=custom_optimizer)` . \n\nI would just go with the first option of freezing the `baseline_similarity_bounds` parameter to `fixed` because it also follows the style used in the project's unit tests for e.g [test_gpr.py](https://github.com/scikit-learn/scikit-learn/blob/1527b1fe98d129f85f9a3c5cd0358214247d236b/sklearn/gaussian_process/tests/test_gpr.py#L69)"
      },
      {
        "user": "StefanieSenger",
        "body": "Hello @EngineerDanny,\n\nthank your for looking into it!\n\nThis is a bit outside of my comfort zone, but I would think the pragmatic solution with setting the param to \"fixed\" is good enough, when we also add a short note/comment stating that `baseline_similarity_bounds` is usually used to set optimization bounds, but here is set to \"fixed\" to circumvent a `ConvergenceWarning` specific to this example.\n\nWhat do you think, @jeremiedbb?"
      }
    ]
  },
  {
    "issue_number": 29381,
    "title": "SimpleImputer's fill_value validation seems too strict",
    "author": "buhrmann",
    "state": "closed",
    "created_at": "2024-07-02T09:11:24Z",
    "updated_at": "2025-05-06T06:52:40Z",
    "labels": [
      "Easy",
      "Enhancement"
    ],
    "body": "### Describe the bug\n\nThe `SimpleImputer` checks whether the _type_ of the `fill_value` can be cast with numpy to the dtype of the input data (`X`) using `np.can_cast(fill_value_dtype, X.dtype, casting=\"same_kind\")`: https://github.com/scikit-learn/scikit-learn/blob/0ad90d51537328b7310741d010e569ca6cd33f78/sklearn/impute/_base.py#L397.\r\n\r\nThis seems too strict to me, and means one cannot impute a uint8 array with fill_value=0 (a python int). Replacing the validation with something like `np.can_cast(fill_value, X.dtype, casting=\"safe\")` would be more permissible, and without knowing all the details looks safe enough, but perhaps I'm missing something.\n\n### Steps/Code to Reproduce\n\nThough the example in isolation doesn't make a lot of sense (imputing data that doesn't contain missing values), this case might occur with generically defined transformation pipelines applied to unknown datasets.\r\n\r\n``` python\r\nimport pandas as pd\r\nfrom sklearn import impute\r\n\r\ndf = pd.Series([0, 1, 2], dtype=\"uint8\").to_frame()\r\nimpute.SimpleImputer(strategy=\"constant\", fill_value=0).fit_transform(df)\r\n```\r\n\n\n### Expected Results\n\n```\r\narray([[0],\r\n       [1],\r\n       [2]], dtype=int8)\r\n```\n\n### Actual Results\n\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[19], [line 5](vscode-notebook-cell:?execution_count=19&line=5)\r\n      [2](vscode-notebook-cell:?execution_count=19&line=2) from sklearn import impute\r\n      [4](vscode-notebook-cell:?execution_count=19&line=4) df = pd.Series([0, 1, 2], dtype=\"uint8\").to_frame()\r\n----> [5](vscode-notebook-cell:?execution_count=19&line=5) impute.SimpleImputer(strategy=\"constant\", fill_value=0).fit_transform(df)\r\n\r\nFile ~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/utils/_set_output.py:295, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)\r\n    [293](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/utils/_set_output.py:293) @wraps(f)\r\n    [294](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/utils/_set_output.py:294) def wrapped(self, X, *args, **kwargs):\r\n--> [295](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/utils/_set_output.py:295)     data_to_wrap = f(self, X, *args, **kwargs)\r\n    [296](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/utils/_set_output.py:296)     if isinstance(data_to_wrap, tuple):\r\n    [297](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/utils/_set_output.py:297)         # only wrap the first output for cross decomposition\r\n    [298](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/utils/_set_output.py:298)         return_tuple = (\r\n    [299](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/utils/_set_output.py:299)             _wrap_data_with_container(method, data_to_wrap[0], X, self),\r\n    [300](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/utils/_set_output.py:300)             *data_to_wrap[1:],\r\n    [301](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/utils/_set_output.py:301)         )\r\n\r\nFile ~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1098, in TransformerMixin.fit_transform(self, X, y, **fit_params)\r\n   [1083](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1083)         warnings.warn(\r\n   [1084](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1084)             (\r\n   [1085](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1085)                 f\"This object ({self.__class__.__name__}) has a `transform`\"\r\n   (...)\r\n   [1093](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1093)             UserWarning,\r\n   [1094](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1094)         )\r\n   [1096](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1096) if y is None:\r\n   [1097](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1097)     # fit method of arity 1 (unsupervised transformation)\r\n-> [1098](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1098)     return self.fit(X, **fit_params).transform(X)\r\n   [1099](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1099) else:\r\n   [1100](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1100)     # fit method of arity 2 (supervised transformation)\r\n   [1101](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1101)     return self.fit(X, y, **fit_params).transform(X)\r\n\r\nFile ~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1474, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\r\n   [1467](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1467)     estimator._validate_params()\r\n   [1469](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1469) with config_context(\r\n   [1470](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1470)     skip_parameter_validation=(\r\n   [1471](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1471)         prefer_skip_nested_validation or global_skip_validation\r\n   [1472](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1472)     )\r\n   [1473](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1473) ):\r\n-> [1474](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/base.py:1474)     return fit_method(estimator, *args, **kwargs)\r\n\r\nFile ~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:410, in SimpleImputer.fit(self, X, y)\r\n    [392](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:392) @_fit_context(prefer_skip_nested_validation=True)\r\n    [393](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:393) def fit(self, X, y=None):\r\n    [394](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:394)     \"\"\"Fit the imputer on `X`.\r\n    [395](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:395) \r\n    [396](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:396)     Parameters\r\n   (...)\r\n    [408](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:408)         Fitted estimator.\r\n    [409](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:409)     \"\"\"\r\n--> [410](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:410)     X = self._validate_input(X, in_fit=True)\r\n    [412](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:412)     # default fill_value is 0 for numerical input and \"missing_value\"\r\n    [413](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:413)     # otherwise\r\n    [414](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:414)     if self.fill_value is None:\r\n\r\nFile ~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:388, in SimpleImputer._validate_input(self, X, in_fit)\r\n    [386](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:386)     # Make sure we can safely cast fill_value dtype to the input data dtype\r\n    [387](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:387)     if not np.can_cast(fill_value_dtype, X.dtype, casting=\"same_kind\"):\r\n--> [388](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:388)         raise ValueError(err_msg)\r\n    [390](https://file+.vscode-resource.vscode-cdn.net/Users/thomas/code/notebooks/~/micromamba/envs/grapy/lib/python3.9/site-packages/sklearn/impute/_base.py:390) return X\r\n\r\nValueError: fill_value=0 (of type <class 'int'>) cannot be cast to the input data that is dtype('uint8'). Make sure that both dtypes are of the same kind.\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:55:20)  [Clang 16.0.6 ]\r\nexecutable: /Users/thomas/micromamba/envs/grapy/bin/python\r\n   machine: macOS-14.4.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.4.2\r\n          pip: 24.0\r\n   setuptools: 65.5.1\r\n        numpy: 1.26.4\r\n        scipy: 1.11.3\r\n       Cython: 0.29.37\r\n       pandas: 1.5.3\r\n   matplotlib: 3.8.4\r\n       joblib: 1.4.0\r\nthreadpoolctl: 3.4.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: /Users/thomas/micromamba/envs/grapy/lib/libopenblas.0.dylib\r\n        version: 0.3.25\r\nthreading_layer: openmp\r\n   architecture: VORTEX\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: libomp\r\n       filepath: /Users/thomas/micromamba/envs/grapy/lib/libomp.dylib\r\n        version: None\n```\n",
    "comments": [
      {
        "user": "lesteve",
        "body": "From `numpy.can_cast` doc below, `'same_kind'` is actually less strict than `'safe'` or am I missing something?\r\n\r\n```\r\ncasting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\r\n    Controls what kind of data casting may occur.\r\n \r\n    * 'no' means the data types should not be cast at all.\r\n    * 'equiv' means only byte-order changes are allowed.\r\n    * 'safe' means only casts which can preserve values are allowed.\r\n    * 'same_kind' means only safe casts or casts within a kind,\r\n      like float64 to float32, are allowed.\r\n    * 'unsafe' means any data conversions may be done.\r\n```\r\n\r\nIn any case, both return `False` in your case (Python `int` to `np.uint8`):\r\n```py\r\nIn [12]: np.can_cast(type(1), np.uint8, casting=\"same_kind\")\r\nOut[12]: False\r\n\r\nIn [13]: np.can_cast(type(1), np.uint8, casting=\"safe\")\r\nOut[13]: False\r\n```"
      },
      {
        "user": "buhrmann",
        "body": "Ah, yes, sorry, my suggestion was using the _actual_ `fill_value` not the `fill_value`'s type (since it's more precise, an int in general cannot be cast to uint8, but a 0 in particular can):\r\n\r\n``` python\r\nIn [3]: np.can_cast(type(0), \"uint8\", casting=\"same_kind\")\r\nOut[3]: False\r\n\r\nIn [4]: np.can_cast(0, \"uint8\", casting=\"same_kind\")\r\nOut[4]: True\r\n\r\nIn [6]: np.can_cast(-1, \"uint8\", casting=\"same_kind\")\r\nOut[6]: False\r\n```\r\n\r\nThe `casting` strategy \"safe\" was a distraction. The numpy docs say that the `from_` parameter should be \r\ndtype, dtype specifier, NumPy scalar, or array, but builtin Python scalars seem to be supported too."
      },
      {
        "user": "buhrmann",
        "body": "Hm, but just saw this warning in the docs:\r\n\r\n```Changed in version 2.0: This function does not support Python scalars anymore and does not apply any value-based logic for 0-D arrays and NumPy scalars.```\r\n\r\nSo probably a no go. One workaround is using a `np.uint8(0)` as the fill_value instead of a Python 0, which should work for all input data types I think."
      }
    ]
  },
  {
    "issue_number": 30938,
    "title": "Partial dependence broken in sklearn 1.6.1 when grid has only two values",
    "author": "MarcBresson",
    "state": "open",
    "created_at": "2025-03-04T09:53:23Z",
    "updated_at": "2025-05-05T16:29:29Z",
    "labels": [
      "Bug",
      "Regression"
    ],
    "body": "### Describe the bug\n\nWhen our input feature has two possible values (and that the grid built in that function hence has two values), partial_dependence will raise an error `ValueError: cannot reshape array of size 1 into shape (2)`\n\nWhat I suspect is happening is that inside `_partial_dependence_brute` function, there is a (wrongful) check to see if there are only two predicted values. This check should not be here because there `_get_response_values` seems to do the job of only getting the positive class already.\n\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\nfrom sklearn.inspection._partial_dependence import _partial_dependence_brute\nfrom sklearn.inspection import partial_dependence\n\nX_test = np.array([[1., 0], [0., 1], [0., 1], [0., 0], [1., 0], [0., 0], [0., 0]])\nclf = DecisionTreeClassifier()\nclf.fit(X_test, np.array([0, 1, 1, 0, 0, 0, 0]))\n\npartial_dependence(clf, X=X_test, features=[0], grid_resolution=10, response_method=\"predict_proba\")\n```\n\n### Expected Results\n\n.\n\n### Actual Results\n\n```python-traceback\nFile /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216, in validate_params.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\n    [210](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:210) try:\n    [211](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211)     with config_context(\n    [212](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:212)         skip_parameter_validation=(\n    [213](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213)             prefer_skip_nested_validation or global_skip_validation\n    [214](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214)         )\n    [215](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:215)     ):\n--> [216](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216)         return func(*args, **kwargs)\n    [217](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:217) except InvalidParameterError as e:\n    [218](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218)     # When the function is just a wrapper around an estimator, we allow\n    [219](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:219)     # the function to delegate validation to the estimator, but we replace\n    [220](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:220)     # the name of the estimator by the name of the function in the error\n    [221](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:221)     # message to avoid confusion.\n    [222](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:222)     msg = re.sub(\n    [223](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:223)         r\"parameter of \\w+ must be\",\n    [224](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:224)         f\"parameter of {func.__qualname__} must be\",\n    [225](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:225)         str(e),\n    [226](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:226)     )\n\nFile /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:688, in partial_dependence(estimator, X, features, sample_weight, categorical_features, feature_names, response_method, percentiles, grid_resolution, method, kind)\n    [684](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:684) print(\"value shapes\", [val.shape[0] for val in values])\n    [686](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:686) # reshape averaged_predictions to\n    [687](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:687) # (n_outputs, n_values_feature_0, n_values_feature_1, ...)\n--> [688](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:688) averaged_predictions = averaged_predictions.reshape(\n    [689](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:689)     -1, *[val.shape[0] for val in values]\n    [690](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:690) )\n    [691](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:691) pdp_results = Bunch(grid_values=values)\n    [693](https://file+.vscode-resource.vscode-cdn.net/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:693) if kind == \"average\":\n```\n\nValueError: cannot reshape array of size 1 into shape (2)\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)]\nexecutable: /usr/local/bin/python3.10\n   machine: macOS-14.4.1-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 24.2\n   setuptools: 74.0.0\n        numpy: 1.26.4\n        scipy: 1.13.1\n       Cython: 3.0.10\n       pandas: 1.5.3\n   matplotlib: 3.8.4\n       joblib: 1.2.0\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n       filepath: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/.dylibs/libopenblas64_.0.dylib\n        version: 0.3.23.dev\nthreading_layer: pthreads\n   architecture: armv8\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n       filepath: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/.dylibs/libopenblas.0.dylib\n        version: 0.3.27\nthreading_layer: pthreads\n   architecture: neoversen1\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libomp\n       filepath: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/.dylibs/libomp.dylib\n        version: None\n```",
    "comments": [
      {
        "user": "MarcBresson",
        "body": "it seems like it is fixed on the main branch. Do you happen to know when will the 1.7 be released?"
      },
      {
        "user": "ogrisel",
        "body": "We release twice a year, so approximately 6 months after 1.6.0 (around June, I think)."
      },
      {
        "user": "MarcBresson",
        "body": "Will a 1.6.2 get out before then? I could fix that in a separate PR"
      }
    ]
  },
  {
    "issue_number": 30840,
    "title": "StandardScaler is `stateless`",
    "author": "benHeid",
    "state": "closed",
    "created_at": "2025-02-15T18:58:02Z",
    "updated_at": "2025-05-05T16:28:01Z",
    "labels": [
      "Bug",
      "Regression"
    ],
    "body": "### Describe the bug\n\nThe StandardScaler seems to be stateless in version 1.6.1. But fit changes the state of the StandardScaler if I got it correctly. \n\n### Steps/Code to Reproduce\n\n```\nStandardScaler()._get_tags()[\"stateless\"]\n```\n\n### Expected Results\n\nFalse\n\n### Actual Results\n\nTrue\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.14 (main, Jul 18 2024, 22:40:44) [Clang 15.0.0 (clang-1500.1.0.2.5)]\nexecutable: ****/python\n   machine: macOS-15.2-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 24.1.2\n   setuptools: 71.0.3\n        numpy: 1.26.4\n        scipy: 1.13.1\n       Cython: 3.0.11\n       pandas: 2.2.3\n   matplotlib: 3.9.2\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n       filepath: ****.dylib\n        version: 0.3.23.dev\nthreading_layer: pthreads\n   architecture: armv8\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n       filepath: *****\n        version: 0.3.27\nthreading_layer: pthreads\n   architecture: neoversen1\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libomp\n       filepath: *****\n        version: None\n```",
    "comments": [
      {
        "user": "StefanieSenger",
        "body": "Hello @benHeid,\n\nthanks for reporting.\n\nI have checked and would also think it is a bug. This code returned `False` up until version 1.5.2 and since version 1.6 it does return `True`:\n\n```python\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nprint(StandardScaler()._get_tags()[\"stateless\"])\nprint(MinMaxScaler()._get_tags()[\"stateless\"])\n```\n\nAs a workaround please use the `\"requires_fit\"` tag, which is supposed to replace the `\"stateless\"` tag.\n\nThe issue is related to #30327."
      },
      {
        "user": "glemaitre",
        "body": "> As a workaround please use the \"requires_fit\" tag\n\nActually it is the right way to do with the new tag infrastructure\n\n```python\n\nfrom sklearn.utils import get_tags\nfrom sklearn.preprocessing import StandardScaler\n\nget_tags(StandardScaler()).requires_fit\n```\n\nAnd indeed, there is a bug with the conversion if the old tag infrastructure that we need to solve."
      },
      {
        "user": "glemaitre",
        "body": "So the bug is here:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/_tags.py#L590\n\nWe should change to:\n\n```python\n        \"stateless\": not new_tags.requires_fit,\n```"
      }
    ]
  },
  {
    "issue_number": 31123,
    "title": "BUG: Build from source can fail  on Windows for scikit-learn v1.6.1 with Ninja `mkdir` error",
    "author": "Molier",
    "state": "closed",
    "created_at": "2025-04-01T12:37:00Z",
    "updated_at": "2025-05-05T16:14:28Z",
    "labels": [
      "Build / CI",
      "Needs Investigation",
      "OS:Windows"
    ],
    "body": "**Labels:** `Bug`, `Build / CI`, `Needs Triage` (Suggested)\n\n**Describe the bug**\n\nScikit-learn (v1.6.1) fails to build from source on a native Windows 11 ARM64 machine using the MSYS2 ClangARM64 toolchain. The build proceeds through the Meson setup phase correctly identifying the `clang` compiler, but fails during the `ninja` compilation phase with an error indicating it cannot create a specific, deeply nested intermediate build directory.\n\nThis occurs despite successfully building other complex dependencies like NumPy (v2.2.4) and SciPy (v1.15.2) from source in the *exact same environment*. Pandas (v2.2.3) also builds successfully after setting `MESON_DISABLE_VSENV=1` (otherwise it incorrectly selects MSVC). This suggests the issue might be specific to how scikit-learn's build structure interacts with Meson/Ninja within this particular toolchain environment.\n\nThis is related to, but distinct from, #30567 which requests pre-built wheels. This issue focuses on a specific build-from-source failure.\n\n**Steps/Code to Reproduce**\n\n1.  **Environment Setup:**\n    *   OS: Windows 11 Pro ARM64 (via Parallels on Apple Silicon M2, or on native hardware like Windows Dev Kit 2023)\n    *   MSYS2: Latest version, updated via `pacman -Syu`.\n    *   MSYS2 Environment: `CLANGARM64` shell launched.\n    *   Key MSYS2 Packages (installed via `pacman -S mingw-w64-clang-aarch64-<package>`):\n        *   `python` (3.12.x)\n        *   `clang` (20.1.1)\n        *   `flang` (20.1.1)\n        *   `meson` (1.7.0)\n        *   `ninja` (1.12.1)\n        *   `pkgconf`\n        *   `openblas`\n        *   `lapack`\n        *   `openssl`\n        *   `hdf5`\n        *   `rust`\n        *   `zlib`\n    *   Project Location: Tried both native MSYS2 path (`/home/user/project`) and WSL interop path (`//wsl.localhost/Ubuntu/...`) - error persists in both.\n\n2.  **Python Virtual Environment:**\n    ```bash\n    # In CLANGARM64 shell, navigate to project directory\n    python -m venv .venv\n    source .venv/bin/activate # Or Scripts/activate\n    python -m pip install --upgrade pip setuptools wheel\n    ```\n\n3.  **Environment Variables:** Before attempting install, set the following in the activated CLANGARM64 shell:\n    ```bash\n    # Unset potential conflicts\n    unset CL _CL_ OPENSSL_LIB_DIR OPENSSL_INCLUDE_DIR OPENSSL_DIR\n\n    # Set compilers (though often picked up correctly by MSYS2 shell)\n    export CC=clang\n    export CXX=clang++\n\n    # Attempt to prevent Meson from activating MSVC (worked for Pandas)\n    export MESON_DISABLE_VSENV=1\n\n    # Ensure pkg-config uses MSYS2 paths\n    export PKG_CONFIG_PATH=\"/clangarm64/lib/pkgconfig:/clangarm64/share/pkgconfig\"\n\n    # Attempt to use short temporary paths (tried /tmp and C:/pip_build_temp)\n    export TMPDIR=\"C:/pip_build_temp\" # Or /tmp\n    mkdir -p $TMPDIR\n    ```\n\n4.  **Attempt Installation:**\n    ```bash\n    python -m pip install --verbose scikit-learn==1.6.1\n    # (Or use `pip install -r requirements.txt` containing scikit-learn==1.6.1)\n    ```\n\n**Expected Results**\n\nScikit-learn==1.6.1 should build and install successfully, similar to how NumPy and SciPy do in the same configured environment.\n\n**Actual Results**\n\nThe build process starts, Meson correctly identifies `clang`, dependencies are resolved, Cython files are processed, but compilation fails during the Ninja execution phase.\n\nThe key error message is:\n```\n[...]\n[48/251] Compiling Cython source C:/pip_build_temp/pip-install-.../scikit-learn.../utils/_fast_dict.pyx\nninja: error: mkdir(sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.cp312-mingw_aarch64_ucrt_llvm.pyd.p/sklearn/metrics/_pairwise_distances_reduction): No such file or directory\nninja: build stopped: .\n[...]\nerror: subprocess-exited-with-error\n\nÃ— Preparing metadata (pyproject.toml) did not run successfully.\nâ”‚ exit code: 1\nâ•°â”€> See above for output.\n[...]\nerror: metadata-generation-failed\n```\n\n*(Note: The exact file being compiled just before the error ([48/251] in the log provided previously) might vary slightly, but the `ninja: error: mkdir(...)` is consistent).*\n<details><summary>full log</summary><p>\n\n## full error trace\n```\n@energypc CLANGARM64 ~/projects_msys/func-ecopower-pwm-forecast-durable\n$ python -m pip install --verbose scikit-learn==1.6.1\nUsing pip 25.0.1 from C:/msys64/home/oscar/projects_msys/func-ecopower-pwm-forecast-durable/.venv/lib/python3.12/site-packages/pip (python 3.12)\nCollecting scikit-learn==1.6.1\n  Using cached scikit_learn-1.6.1.tar.gz (7.1 MB)\n  Running command pip subprocess to install build dependencies\n  Using pip 25.0.1 from C:/msys64/home/oscar/projects_msys/func-ecopower-pwm-forecast-durable/.venv/lib/python3.12/site-packages/pip (python 3.12)\n  Collecting meson-python>=0.16.0\n    Obtaining dependency information for meson-python>=0.16.0 from https://files.pythonhosted.org/packages/7d/ec/40c0ddd29ef4daa6689a2b9c5ced47d5b58fa54ae149b19e9a97f4979c8c/meson_python-0.17.1-py3-none-any.whl.metadata\n    Using cached meson_python-0.17.1-py3-none-any.whl.metadata (4.1 kB)\n  Collecting Cython>=3.0.10\n    Obtaining dependency information for Cython>=3.0.10 from https://files.pythonhosted.org/packages/27/6b/7c87867d255cbce8167ed99fc65635e9395d2af0f0c915428f5b17ec412d/Cython-3.0.12-py2.py3-none-any.whl.metadata\n    Using cached Cython-3.0.12-py2.py3-none-any.whl.metadata (3.3 kB)\n    Link requires a different Python (3.12.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/3a/be/650f9c091ef71cb01d735775d554e068752d3ff63d7943b26316dc401749/numpy-1.21.2.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n    Link requires a different Python (3.12.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/5f/d6/ad58ded26556eaeaa8c971e08b6466f17c4ac4d786cd3d800e26ce59cc01/numpy-1.21.3.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n    Link requires a different Python (3.12.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/fb/48/b0708ebd7718a8933f0d3937513ef8ef2f4f04529f1f66ca86d873043921/numpy-1.21.4.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n    Link requires a different Python (3.12.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/c2/a8/a924a09492bdfee8c2ec3094d0a13f2799800b4fdc9c890738aeeb12c72e/numpy-1.21.5.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n    Link requires a different Python (3.12.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/45/b7/de7b8e67f2232c26af57c205aaad29fe17754f793404f59c8a730c7a191a/numpy-1.21.6.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n  Collecting numpy>=2\n    Using cached numpy-2.2.4-cp312-cp312-mingw_aarch64_ucrt_llvm.whl\n    Link requires a different Python (3.12.9 not in: '>=3.7,<3.10'): https://files.pythonhosted.org/packages/99/f1/c00d6be56e1a718a3068079e3ec8ce044d7179345280f6a3f5066068af0d/scipy-1.6.2.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.7,<3.10)\n    Link requires a different Python (3.12.9 not in: '>=3.7,<3.10'): https://files.pythonhosted.org/packages/fe/fd/8704c7b7b34cdac850485e638346025ca57c5a859934b9aa1be5399b33b7/scipy-1.6.3.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.7,<3.10)\n    Link requires a different Python (3.12.9 not in: '>=3.7,<3.10'): https://files.pythonhosted.org/packages/bb/bb/944f559d554df6c9adf037aa9fc982a9706ee0e96c0d5beac701cb158900/scipy-1.7.0.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.7,<3.10)\n    Link requires a different Python (3.12.9 not in: '>=3.7,<3.10'): https://files.pythonhosted.org/packages/47/33/a24aec22b7be7fdb10ec117a95e1e4099890d8bbc6646902f443fc7719d1/scipy-1.7.1.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.7,<3.10)\n    Link requires a different Python (3.12.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/0e/23/58c4f995475a2a97cb5f4a032aedaf881ad87cd976a7180c55118d105a1d/scipy-1.7.2.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.7,<3.11)\n    Link requires a different Python (3.12.9 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/61/67/1a654b96309c991762ee9bc39c363fc618076b155fe52d295211cf2536c7/scipy-1.7.3.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.7,<3.11)\n    Link requires a different Python (3.12.9 not in: '>=3.8,<3.11'): https://files.pythonhosted.org/packages/c0/ad/e3c052ed4e0027a8abef0a5e8441a044427d252d17d9aee06d56e62fc698/scipy-1.8.0rc1.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.8,<3.11)\n    Link requires a different Python (3.12.9 not in: '>=3.8,<3.11'): https://files.pythonhosted.org/packages/29/d2/151a54944b333e465f98804dced31dab1284f3c37b752b9cefa710b64681/scipy-1.8.0rc2.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.8,<3.11)\n    Link requires a different Python (3.12.9 not in: '>=3.8,<3.11'): https://files.pythonhosted.org/packages/e4/26/83dd1c6378513a6241d984bda9f08c512b6e35fff13fba3acc1b3c195f02/scipy-1.8.0rc3.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.8,<3.11)\n    Link requires a different Python (3.12.9 not in: '>=3.8,<3.11'): https://files.pythonhosted.org/packages/22/78/056cc43e7737811b6f50886788a940f852773dd9804f5365952805db9648/scipy-1.8.0rc4.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.8,<3.11)\n    Link requires a different Python (3.12.9 not in: '>=3.8,<3.11'): https://files.pythonhosted.org/packages/b4/a2/4faa34bf0cdbefd5c706625f1234987795f368eb4e97bde9d6f46860843e/scipy-1.8.0.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.8,<3.11)\n    Link requires a different Python (3.12.9 not in: '>=3.8,<3.11'): https://files.pythonhosted.org/packages/26/b5/9330f004b9a3b2b6a31f59f46f1617ce9ca15c0e7fe64288c20385a05c9d/scipy-1.8.1.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.8,<3.11)\n    Link requires a different Python (3.12.9 not in: '>=3.8,<3.12'): https://files.pythonhosted.org/packages/87/82/8b9bf8bb8030f1eef281ad2af87cc648e06e4a4974c2b9201a57d991b12c/scipy-1.9.0rc1.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.8,<3.12)\n    Link requires a different Python (3.12.9 not in: '>=3.8,<3.12'): https://files.pythonhosted.org/packages/5a/87/27f9ee75c32138d6eaabff2b91744c97a3fdbbbdd78948372dac93e0079f/scipy-1.9.0rc2.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.8,<3.12)\n    Link requires a different Python (3.12.9 not in: '>=3.8,<3.12'): https://files.pythonhosted.org/packages/57/72/484c38e5b814b48b41ac6432ef69077f2e7c8094925d3ce17bc3d61359b0/scipy-1.9.0rc3.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.8,<3.12)\n    Link requires a different Python (3.12.9 not in: '>=3.8,<3.12'): https://files.pythonhosted.org/packages/a8/e3/4ec401f609d34162b7023a09165da491630879e4cfa2336667fe2102cd06/scipy-1.9.0.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.8,<3.12)\n    Link requires a different Python (3.12.9 not in: '>=3.8,<3.12'): https://files.pythonhosted.org/packages/db/af/16906139f52bc6866c43401869ce247662739ad71afa11c6f18505eb0546/scipy-1.9.1.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:>=3.8,<3.12)\n    Link requires a different Python (3.12.9 not in: '<3.12,>=3.8'): https://files.pythonhosted.org/packages/a2/ce/2592c3b550cf8f68879d4ff2159f3c689ee6f032f8fc9059022074f7bd75/scipy-1.10.0rc1.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:<3.12,>=3.8)\n    Link requires a different Python (3.12.9 not in: '<3.12,>=3.8'): https://files.pythonhosted.org/packages/6b/a3/7de758a32569e06877a6b298bdf7b529ce97a33c55deb2b1637b0162fd9d/scipy-1.10.0rc2.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:<3.12,>=3.8)\n    Link requires a different Python (3.12.9 not in: '<3.12,>=3.8'): https://files.pythonhosted.org/packages/d6/bd/2d13a273d95f7b7d9903c906c486040b0aebb85e008f93a5dd0891f21f1f/scipy-1.10.0.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:<3.12,>=3.8)\n    Link requires a different Python (3.12.9 not in: '<3.12,>=3.8'): https://files.pythonhosted.org/packages/84/a9/2bf119f3f9cff1f376f924e39cfae18dec92a1514784046d185731301281/scipy-1.10.1.tar.gz (from https://pypi.org/simple/scipy/) (requires-python:<3.12,>=3.8)\n  Collecting scipy>=1.6.0\n    Using cached scipy-1.15.2-cp312-cp312-mingw_aarch64_ucrt_llvm.whl\n  Collecting meson>=1.2.3 (from meson-python>=0.16.0)\n    Obtaining dependency information for meson>=1.2.3 from https://files.pythonhosted.org/packages/ab/3b/63fdad828b4cbeb49cef3aad26f3edfbc72f37a0ab54917d445ec0b9d9ff/meson-1.7.0-py3-none-any.whl.metadata\n    Using cached meson-1.7.0-py3-none-any.whl.metadata (1.8 kB)\n  Collecting packaging>=19.0 (from meson-python>=0.16.0)\n    Obtaining dependency information for packaging>=19.0 from https://files.pythonhosted.org/packages/88/ef/eb23f262cca3c0c4eb7ab1933c3b1f03d021f2c48f54763065b6f0e321be/packaging-24.2-py3-none-any.whl.metadata\n    Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n  Collecting pyproject-metadata>=0.7.1 (from meson-python>=0.16.0)\n    Obtaining dependency information for pyproject-metadata>=0.7.1 from https://files.pythonhosted.org/packages/7e/b1/8e63033b259e0a4e40dd1ec4a9fee17718016845048b43a36ec67d62e6fe/pyproject_metadata-0.9.1-py3-none-any.whl.metadata\n    Using cached pyproject_metadata-0.9.1-py3-none-any.whl.metadata (7.0 kB)\n  Using cached meson_python-0.17.1-py3-none-any.whl (27 kB)\n  Using cached Cython-3.0.12-py2.py3-none-any.whl (1.2 MB)\n  Using cached meson-1.7.0-py3-none-any.whl (990 kB)\n  Using cached packaging-24.2-py3-none-any.whl (65 kB)\n  Using cached pyproject_metadata-0.9.1-py3-none-any.whl (18 kB)\n  Installing collected packages: packaging, numpy, meson, Cython, scipy, pyproject-metadata, meson-python\n    Creating C:/pip_build_temp/pip-build-env-4z56ip4a/overlay/bin\n  Successfully installed Cython-3.0.12 meson-1.7.0 meson-python-0.17.1 numpy-2.2.4 packaging-24.2 pyproject-metadata-0.9.1 scipy-1.15.2\n  Installing build dependencies ... done\n  Running command Getting requirements to build wheel\n  Getting requirements to build wheel ... done\n  Running command Preparing metadata (pyproject.toml)\n  + meson setup C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508 C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/.mesonpy-lhh9mbac -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/.mesonpy-lhh9mbac/meson-python-native-file.ini\n  The Meson build system\n  Version: 1.7.0\n  Source dir: C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508\n  Build dir: C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/.mesonpy-lhh9mbac\n  Build type: native build\n  Project name: scikit-learn\n  Project version: 1.6.1\n  C compiler for the host machine: clang (clang 20.1.1 \"clang version 20.1.1\")\n  C linker for the host machine: clang ld.lld 20.1.1\n  C++ compiler for the host machine: clang++ (clang 20.1.1 \"clang version 20.1.1\")\n  C++ linker for the host machine: clang++ ld.lld 20.1.1\n  Cython compiler for the host machine: cython (cython 3.0.12)\n  Host machine cpu family: aarch64\n  Host machine cpu: aarch64\n  Compiler for C supports arguments -Wno-unused-but-set-variable: YES\n  Compiler for C supports arguments -Wno-unused-function: YES\n  Compiler for C supports arguments -Wno-conversion: YES\n  Compiler for C supports arguments -Wno-misleading-indentation: YES\n  Library m found: YES\n  Program python found: YES (C:/msys64/home/oscar/projects_msys/func-ecopower-pwm-forecast-durable/.venv/bin/python.exe)\n  Run-time dependency OpenMP for c found: YES 5.1\n  Found pkg-config: YES (C:\\msys64\\clangarm64\\bin/pkg-config.EXE) 2.3.0\n  Run-time dependency python found: YES 3.12\n  Build targets in project: 111\n\n  scikit-learn 1.6.1\n\n    User defined options\n      Native files: C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/.mesonpy-lhh9mbac/meson-python-native-file.ini\n      b_ndebug    : if-release\n      b_vscrt     : md\n      buildtype   : release\n\n  Found ninja.EXE-1.12.1 at C:\\msys64\\clangarm64\\bin/ninja.EXE\n  + meson compile\n  [1/251] Generating sklearn/metrics/_dist_metrics_pxd with a custom command\n  [2/251] Copying file sklearn/utils/_cython_blas.pxd\n  [3/251] Copying file sklearn/utils/__init__.py\n  [4/251] Copying file sklearn/__init__.py\n  [5/251] Copying file sklearn/utils/_openmp_helpers.pxd\n  [6/251] Generating sklearn/utils/_seq_dataset_pxd with a custom command\n  [7/251] Copying file sklearn/utils/_sorting.pxd\n  [8/251] Copying file sklearn/utils/_random.pxd\n  [9/251] Generating sklearn/utils/_weight_vector_pxd with a custom command\n  [10/251] Copying file sklearn/utils/_heap.pxd\n  [11/251] Copying file sklearn/_loss/_loss.pxd\n  [12/251] Copying file sklearn/utils/_vector_sentinel.pxd\n  [13/251] Generating sklearn/metrics/_pairwise_distances_reduction/_datasets_pair_pxd with a custom command\n  [14/251] Generating sklearn/metrics/_pairwise_distances_reduction/_base_pxd with a custom command\n  [15/251] Generating sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_pxd with a custom command\n  [16/251] Copying file sklearn/metrics/__init__.py\n  [17/251] Generating sklearn/neighbors/_binary_tree_pxi with a custom command\n  [18/251] Copying file sklearn/utils/_typedefs.pxd\n  [19/251] Generating sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer_pxd with a custom command\n  [20/251] Generating sklearn/metrics/_pairwise_distances_reduction/_argkmin_pxd with a custom command\n  [21/251] Generating sklearn/_loss/_loss_pyx with a custom command\n  [22/251] Copying file sklearn/metrics/_pairwise_distances_reduction/__init__.py\n  [23/251] Copying file sklearn/metrics/_pairwise_distances_reduction/_classmode.pxd\n  [24/251] Copying file sklearn/linear_model/__init__.py\n  [25/251] Copying file sklearn/neighbors/_partition_nodes.pxd\n  [26/251] Copying file sklearn/neighbors/__init__.py\n  [27/251] Generating sklearn/utils/_weight_vector_pyx with a custom command\n  [28/251] Generating sklearn/utils/_seq_dataset_pyx with a custom command\n  [29/251] Generating sklearn/metrics/_dist_metrics_pyx with a custom command\n  [30/251] Generating sklearn/metrics/_pairwise_distances_reduction/_base_pyx with a custom command\n  [31/251] Generating sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer_pyx with a custom command\n  [32/251] Generating sklearn/metrics/_pairwise_distances_reduction/_datasets_pair_pyx with a custom command\n  [33/251] Generating sklearn/metrics/_pairwise_distances_reduction/_argkmin_pyx with a custom command\n  [34/251] Generating sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_pyx with a custom command\n  [35/251] Generating sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode_pyx with a custom command\n  [36/251] Generating sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode_pyx with a custom command\n  [37/251] Generating sklearn/linear_model/_sgd_fast_pyx with a custom command\n  [38/251] Generating sklearn/neighbors/_ball_tree_pyx with a custom command\n  [39/251] Generating sklearn/linear_model/_sag_fast_pyx with a custom command\n  [40/251] Generating sklearn/neighbors/_kd_tree_pyx with a custom command\n  [41/251] Compiling Cython source C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/sklearn/__check_build/_check_build.pyx\n  [42/251] Compiling Cython source C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/sklearn/utils/_openmp_helpers.pyx\n  [43/251] Compiling Cython source C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/sklearn/metrics/cluster/_expected_mutual_info_fast.pyx\n  [44/251] Compiling Cython source C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/sklearn/utils/_random.pyx\n  [45/251] Compiling Cython source C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/sklearn/utils/_heap.pyx\n  [46/251] Compiling Cython source C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/sklearn/_isotonic.pyx\n  [47/251] Compiling Cython source C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/sklearn/utils/murmurhash.pyx\n  [48/251] Compiling Cython source C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/sklearn/utils/_cython_blas.pyx\n  [49/251] Compiling Cython source C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/sklearn/utils/_fast_dict.pyx\n  [50/251] Compiling Cython source C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/sklearn/utils/_sorting.pyx\n  [51/251] Compiling Cython source C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508/sklearn/utils/arrayfuncs.pyx\n  ninja: error: mkdir(sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.cp312-mingw_aarch64_ucrt_llvm.pyd.p/sklearn/metrics/_pairwise_distances_reduction): No such file or directory\n  ninja: build stopped: .\n  INFO: autodetecting backend as ninja\n  INFO: calculating backend command to run: C:\\msys64\\clangarm64\\bin/ninja.EXE\n  error: subprocess-exited-with-error\n\n  Ã— Preparing metadata (pyproject.toml) did not run successfully.\n  â”‚ exit code: 1\n  â•°â”€> See above for output.\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  full command: C:/msys64/home/oscar/projects_msys/func-ecopower-pwm-forecast-durable/.venv/bin/python.exe C:/msys64/home/oscar/projects_msys/func-ecopower-pwm-forecast-durable/.venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py prepare_metadata_for_build_wheel C:/pip_build_temp/tmpx1jq7lm1\n  cwd: C:/pip_build_temp/pip-install-yjv7z3j9/scikit-learn_75cabb5c67ca4de1b01dff74d7ec4508\n  Preparing metadata (pyproject.toml) ... error\nerror: metadata-generation-failed\n\nÃ— Encountered error while generating package metadata.\nâ•°â”€> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n(.venv)\noscar@pc CLANGARM64 ~/projects_msys/func-ecopower-pwm-forecast-durable\n$\n\n```\nCollapsible until here.\n</p></details>\n**Versions**\n\n(Cannot run `sklearn.show_versions()` as installation failed)\n\n*   scikit-learn: 1.6.1 (attempted build from source)\n*   Python: 3.12.x (mingw-w64-clang-aarch64-python)\n*   OS: Windows 11 Pro 23H2 ARM64\n*   Compiler: Clang 20.1.1 (mingw-w64-clang-aarch64-clang)\n*   Meson: 1.7.0 (mingw-w64-clang-aarch64-meson)\n*   Ninja: 1.12.1 (mingw-w64-clang-aarch64-ninja)\n*   NumPy: 2.2.4 (built from source successfully in same venv)\n*   SciPy: 1.15.2 (built from source successfully in same venv as build dependency)\n*   MSYS2 Environment: CLANGARM64\n\n**Possible Cause:**\n\nThe most likely cause is either hitting a path length limit within one of the build tools (Ninja?) despite OS support, or a bug in handling paths containing multiple dots or specific patterns like `.pyd.p` within the MSYS2 Clang toolchain environment on Windows ARM64.\n\nThanks for looking into this! Building natively on Win ARM64 is becoming increasingly relevant. I thought this was a good place to start to gather any insights on the issue. ",
    "comments": [
      {
        "user": "lesteve",
        "body": "Thanks for the detailed issue ðŸ™! Unfortunately this will likely be a tricky one. I don't think any of the maintainers has access to a Windows ARM64 machine and even if we did, we are not super familiar with Windows compilation.\n\nIf there is a way to change our meson files without complicating it too much and to make it work for your use case, we would be willing to accept such a PR. In particular maybe there is a way to avoid such a nested folder structure `sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.cp312-mingw_aarch64_ucrt_llvm.pyd.p/sklearn/metrics/_pairwise_distances_reduction`?\n\nFor the long path hypothesis, I am a bit surprised since MAXPATH is 260 characters according to this [Windows doc](https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation?tabs=registry) and in the error message the path is 149 characters long but maybe this is the absolute path that counts ðŸ¤”: \n\n```\nninja: error: mkdir(sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.cp312-mingw_aarch64_ucrt_llvm.pyd.p/sklearn/metrics/_pairwise_distances_reduction): No such file or directory\n```\n\nI did find a ninja issue https://github.com/ninja-build/ninja/issues/2442 and an ongoing PR https://github.com/ninja-build/ninja/pull/2552 that you could maybe test to see if that fixes the issue."
      },
      {
        "user": "lesteve",
        "body": "Actually looking a bit more, this is not ARM64 specific and I can reproduce on a amd64 Windows VM doing:\n```bash\n# This use --no-binary scikit-learn to make sure to compile from sdist rather than use the wheel\npip install scikit-learn --no-binary scikit-learn -v\n```\n\n@Molier as a work-around could you try if this command works for you (it does for me):\n```\npython -m pip install --verbose scikit-learn==1.6.1 -v -Cbuild-dir=c:\\my-build\n```\n\nThis passes an explicit build directory to meson-python following the [doc](https://mesonbuild.com/meson-python/how-to-guides/config-settings.html#using-a-persistent-build-directory) and should avoid too long paths. By default, the build dir is auto-generated even when setting `TMPDIR` the meson build dir  is shorter but still a bit too long for you apparently, e.g. something like this `C:/tmp/pip-req-build-kc1tt4we/sklearn` ...\n\nThere may be ways to reduce a bit the lengths of meson auto-generated files (maybe by 40 characters or so) in our `meson.build` but I would need to spend more time figuring how to do it and whether this is worth it or not."
      },
      {
        "user": "lesteve",
        "body": "I opened https://github.com/scikit-learn/scikit-learn/pull/31212 that reduces the generated files maximum path length by ~50 characters and make it less likely that the Windows path limitation is hit."
      }
    ]
  },
  {
    "issue_number": 23319,
    "title": "Yeo-Johnson Power Transformer gives Numpy warning (and raises scipy.optimize._optimize.BracketError in some cases)",
    "author": "nilslacroix",
    "state": "closed",
    "created_at": "2022-05-10T09:40:28Z",
    "updated_at": "2025-05-05T16:08:20Z",
    "labels": [
      "Bug",
      "module:preprocessing"
    ],
    "body": "### Describe the bug\r\n\r\nWhen I use a power transformer with yeo-johnson method I get this warning in numpy:\r\n\r\n`../lib/python3.10/site-packages/numpy/core/_methods.py:235: RuntimeWarning: overflow encountered in multiply`\r\n\r\nStrangely I can't surpress this warning with filter.warnings()\r\n\r\n**Tl;dr: The eroor seems to appear when multiple instances of the transformer are called, in this example because n_jobs=2 is set in the preprocessor. But the bug also appears with n_jobs=1 when you call this preprocessor in a grid search for example.**\r\n\r\n### Steps/Code to Reproduce\r\n\r\nThe following code snippet will raise a warning:\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.preprocessing import PowerTransformer\r\n\r\nrng = np.random.default_rng(0)\r\nX = rng.exponential(size=(50,))\r\nX -= X.max()\r\nX = -X\r\nX += 100\r\nX = X.reshape(-1, 1)\r\n\r\nmethod = \"yeo-johnson\"\r\ntransformer = PowerTransformer(method=method, standardize=False)\r\ntransformer.fit_transform(X)\r\n```\r\n\r\n### Expected Results\r\n\r\nNo Warnings\r\n\r\n### Actual Results\r\n```pytb\r\n/Users/glemaitre/mambaforge/envs/dev/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: overflow encountered in multiply\r\n  x = um.multiply(x, x, out=x)\r\n\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.8.12 | packaged by conda-forge | (default, Sep 16 2021, 01:38:21)  [Clang 11.1.0 ]\r\nexecutable: /Users/glemaitre/mambaforge/envs/dev/bin/python\r\n   machine: macOS-12.3.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.dev0\r\n          pip: 21.3\r\n   setuptools: 58.2.0\r\n        numpy: 1.21.6\r\n        scipy: 1.8.0\r\n       Cython: 0.29.24\r\n       pandas: 1.4.2\r\n   matplotlib: 3.4.3\r\n       joblib: 1.0.1\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /Users/glemaitre/mambaforge/envs/dev/lib/libopenblas_vortexp-r0.3.18.dylib\r\n        version: 0.3.18\r\nthreading_layer: openmp\r\n   architecture: VORTEX\r\n    num_threads: 8\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libomp\r\n       filepath: /Users/glemaitre/mambaforge/envs/dev/lib/libomp.dylib\r\n        version: None\r\n    num_threads: 8\r\n```\r\n",
    "comments": [
      {
        "user": "glemaitre",
        "body": "Could you provide the data such that we can reproduce the use case and check what involve trigger this overflow"
      },
      {
        "user": "AliHaider20",
        "body": "This is not a future version warning. If you can provide the dummy data so that we can look in it because the function is working as it was supposed."
      },
      {
        "user": "nilslacroix",
        "body": "I can provide this screenshot if this is good?\r\n\r\n![image](https://user-images.githubusercontent.com/56065345/167610109-c8532aa3-4cec-479f-8419-16f505187a2c.png)\r\n\r\n\r\nAlso for the dtypes:\r\n\r\n```\r\nNightlife            float64\r\nEducation            float64\r\nSecurity             float64\r\nNature               float64\r\nEating               float64\r\nBalcony                int64\r\nDailyNeeds           float64\r\nTourism              float64\r\nLeisure              float64\r\nYearBuilt            float64\r\nYearModernization    float64\r\ndtype: object\r\n```\r\n\r\nI could also provide a small sample dataset if it is possible to upload it here?"
      }
    ]
  },
  {
    "issue_number": 26308,
    "title": "Use scipy.stats.yeojohnson PowerTransformer",
    "author": "lorentzenchr",
    "state": "closed",
    "created_at": "2023-04-30T10:51:30Z",
    "updated_at": "2025-05-05T15:56:53Z",
    "labels": [
      "Moderate",
      "help wanted",
      "module:preprocessing",
      "Refactor"
    ],
    "body": "Inside `PowerTransformer`, we should use [`scipy.stats.yeojohnson`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.yeojohnson.html#scipy-stats-yeojohnson) instead of our own implementation.\r\n\r\n`scipy.stats.yeojohnson` was release with scipy 1.2.0. With PR #24665, we now have even 1.5.0 as minimum.\r\n\r\nEdit: Note that https://github.com/scipy/scipy/issues/18389 to be release in scipy 1.12 will also resolve #23319.",
    "comments": [
      {
        "user": "lorentzenchr",
        "body": "@lsorber are you interested?"
      },
      {
        "user": "lsorber",
        "body": "Sure, but I'd first like to see the outcome of https://github.com/scipy/scipy/issues/18389 before contributing a PR."
      },
      {
        "user": "vishalj0501",
        "body": "Hello, I'm a first timer and almost new to open source,\r\nCan I take up this issue ?"
      }
    ]
  },
  {
    "issue_number": 31269,
    "title": "âš ï¸ CI failed on Wheel builder (last failure: May 05, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-04-29T04:32:02Z",
    "updated_at": "2025-05-05T12:55:34Z",
    "labels": [
      "Build / CI"
    ],
    "body": "**CI is still failing on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14828681637)** (May 05, 2025)\n",
    "comments": [
      {
        "user": "ogrisel",
        "body": "At the time of writing, #31263 was merged 17 hours ago and this failed 3 hours ago, so the fix was not enough. There is something different between our Azure Pipelines-based `[free-threeded]` CI config and the nightly wheels config that should explain this.\n\nThe nightly wheel builder uses developer versions of dependencies. I tried to investigate a bit in this comment of the preview automated issue but could not spot the cause by comparing the logs of running and failing nightly CI runs: https://github.com/scikit-learn/scikit-learn/issues/31257#issuecomment-2835076702"
      },
      {
        "user": "ogrisel",
        "body": "Direct links to:\n\n- last successful nightly run (for manylinux):\n\nhttps://github.com/scikit-learn/scikit-learn/actions/runs/14677414091/job/41195707673\n\n- first failing nightly run (for manylinux):\n\nhttps://github.com/scikit-learn/scikit-learn/actions/runs/14688110971/job/41219558777\n\n- History of all the nightly wheels runs:\n\nhttps://github.com/scikit-learn/scikit-learn/actions/workflows/wheels.yml?query=branch%3Amain+event%3Aschedule"
      },
      {
        "user": "ogrisel",
        "body": "This seems to be related to the developer version of Cython. Draft PR to investigate has been opened in #31300.\n\nThis can now be reproduced without free-threaded CPython so it's not related to free-threading."
      }
    ]
  },
  {
    "issue_number": 31283,
    "title": "âš ï¸ CI failed on Linux_free_threaded.pylatest_free_threaded (last failure: May 05, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-05-01T02:51:48Z",
    "updated_at": "2025-05-05T12:55:15Z",
    "labels": [
      "Build / CI",
      "cython"
    ],
    "body": "**CI is still failing on [Linux_free_threaded.pylatest_free_threaded](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76198&view=logs&j=c10228e9-6cf7-5c29-593f-d74f893ca1bd)** (May 05, 2025)\n- Test Collection Failure",
    "comments": [
      {
        "user": "ogrisel",
        "body": "This seems to be related to #31269 although in a different CI environment."
      }
    ]
  },
  {
    "issue_number": 31284,
    "title": "âš ï¸ CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: May 05, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-05-01T02:52:32Z",
    "updated_at": "2025-05-05T12:54:29Z",
    "labels": [
      "Bug",
      "cython"
    ],
    "body": "**CI is still failing on [Linux_Nightly.pylatest_pip_scipy_dev](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76198&view=logs&j=dfe99b15-50db-5d7b-b1e9-4105c42527cf)** (May 05, 2025)\n- Test Collection Failure",
    "comments": [
      {
        "user": "ogrisel",
        "body": "This seems to be related to #31269 although in a different CI environment. Note that this particular build does not use the free-threaded version of CPython.\n\nI think this is related to a recent change in Cython that will be included in the future 3.1 but I am not sure which."
      }
    ]
  },
  {
    "issue_number": 31288,
    "title": "`make_scorer(needs_sample_weight=True)` wrongly injects `needs_sample_weight` into the scoring function",
    "author": "seralouk",
    "state": "closed",
    "created_at": "2025-05-01T07:52:49Z",
    "updated_at": "2025-05-05T08:33:27Z",
    "labels": [
      "Bug",
      "Needs Triage"
    ],
    "body": "### Describe the bug\n\n\nWhen using `make_scorer(..., needs_sample_weight=True)`, the generated scorer unexpectedly passes `needs_sample_weight=True` as a keyword argument to the scoring function itself, leading to `TypeError` unless **kwargs is manually added.\n\n\n### Steps/Code to Reproduce\n\n\nMinimal example:\n```\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\ndef weighted_mape(y_true, y_pred, sample_weight=None):\n    return np.average(np.abs((y_true - y_pred) / (y_true + 1e-8)), weights=sample_weight)\n\nscoring = make_scorer(weighted_mape, greater_is_better=False, needs_sample_weight=True)\n\nX, y = make_regression(n_samples=100, n_features=5, random_state=0)\nweights = np.random.rand(100)\n\nmodel = GradientBoostingRegressor()\ngrid = GridSearchCV(model, param_grid={\"n_estimators\": [10]}, scoring=scoring, cv=3)\ngrid.fit(X, y, sample_weight=weights)\n```\n\n\n\n### Expected Results\n\n**Expected behavior:**\n\n`make_scorer(..., needs_sample_weight=True)` should cause `sample_weight` to be passed during cross-validation scoring.\n\nThe scoring function should not receive `needs_sample_weight=True` as a kwarg.\n\n\n\n\n### Actual Results\n\n**Actual behavior:**\n\nThe scoring function raises:\n\n>TypeError: weighted_mape() got an unexpected keyword argument 'needs_sample_weight'\nunless manually patched with **kwargs.\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\nexecutable: /home/X/XX/pax_env/bin/python\n   machine: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n\nPython dependencies:\n      sklearn: 1.2.2\n          pip: 22.0.2\n   setuptools: 59.6.0\n        numpy: 1.26.0\n        scipy: 1.15.2\n       Cython: None\n       pandas: 1.5.3\n   matplotlib: 3.7.1\n       joblib: 1.2.0\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 14\n         prefix: libgomp\n       filepath: /home/X/XX/pax_env/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n        version: None\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 14\n         prefix: libopenblas\n       filepath: /home/X/XX/pax_env/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so\n        version: 0.3.23.dev\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 14\n         prefix: libscipy_openblas\n       filepath: /home/X/XX/pax_env/lib/python3.10/site-packages/scipy.libs/libscipy_openblas-68440149.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n```",
    "comments": [
      {
        "user": "Nossks",
        "body": "I believe that i can solve this issue , so please assign this to me I will give my best to it \nThankyou!"
      },
      {
        "user": "lucyleeow",
        "body": "`make_scorer` does not have a `needs_sample_weight` parameter?\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html"
      },
      {
        "user": "adrinjalali",
        "body": "`needs_sample_weight` doesn't exist, and it didn't exist in 1.2.2:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/9aaed498795f68e5956ea762fef9c440ca9eb239/sklearn/metrics/_scorer.py#L604-L611\n\nSo the keyword is assumed to be something which needs to be forwarded to the scorer itself.\n\nYou're also using a very old version of scikit-learn. When it comes to metadata routing and sample weights, there's been a lot of developments in the last releases, and you probably want to use the latest release."
      }
    ]
  },
  {
    "issue_number": 26543,
    "title": "What happend to the idea of adding a 'handle_missing' parameter to the OneHotEncoder?",
    "author": "woodly0",
    "state": "open",
    "created_at": "2023-06-08T11:39:19Z",
    "updated_at": "2025-05-04T15:10:22Z",
    "labels": [
      "Enhancement"
    ],
    "body": "### Discussed in https://github.com/scikit-learn/scikit-learn/discussions/26531\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **woodly0** June  7, 2023</sup>\r\nHello,\r\nI'm having trouble understanding what finally happened to the idea of introducing a `handle_missing` parameter for the `OneHotEncoder`. My current project could still benefit from such an implementation.\r\nThere are many existing issues regarding this topic, however, I cannot deduct what was finally decided/implemented and what wasn't.\r\n- #11996\r\n- #12025\r\n- #17317\r\n- #23436\r\n\r\nConsidering the following features:\r\n```py\r\nimport pandas as pd\r\n\r\ntest_df = pd.DataFrame(\r\n    {\"col1\": [\"red\", \"blue\", \"blue\"], \"col2\": [\"car\", None, \"plane\"]}\r\n)\r\n```\r\nwhen using the encoder:\r\n```py\r\nfrom sklearn.preprocessing import OneHotEncoder\r\n\r\nohe = OneHotEncoder(\r\n    handle_unknown=\"ignore\",\r\n    sparse_output=False,\r\n    #handle_missing=\"ignore\"\r\n)\r\nohe.fit_transform(test_df)\r\n```\r\nI get the output:\r\n```\r\narray([[0., 1., 1., 0., 0.],\r\n       [1., 0., 0., 0., 1.],\r\n       [1., 0., 0., 1., 0.]])\r\n```\r\nbut what I'm actually looking for is to remove the `None`, i.e. not create a new feature but set all the others to zero:\r\n```\r\narray([[0., 1., 1., 0.],\r\n       [1., 0., 0., 0.],\r\n       [1., 0., 0., 1.]])\r\n```\r\nIs there a way to achieve this without using another transformer object?</div>",
    "comments": [
      {
        "user": "glemaitre",
        "body": "I am not really keen to drop missing values with an option. I would prefer some code allowing to make it more explicit than just an option.\r\n\r\nWe currently can do that:\r\n\r\n```python\r\nimport sklearn\r\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder\r\nfrom sklearn.pipeline import make_pipeline\r\n\r\nsklearn.set_config(transform_output=\"pandas\")\r\n\r\n\r\ndef _drop_None_cols(df):\r\n    col_names = [col for col in df.columns if \"None\" in col]\r\n    if len(col_names):\r\n        return df.drop(columns=col_names)\r\n    return df\r\n\r\n\r\nencoder_dropping_None = make_pipeline(\r\n    OneHotEncoder(sparse_output=False),\r\n    FunctionTransformer(_drop_None_cols),\r\n)\r\nencoder_dropping_None.fit_transform(test_df)\r\n```\r\n\r\nÂ  | col1_blue | col1_red | col2_car | col2_plane\r\n-- | -- | -- | -- | --\r\n0|0.0 | 1.0 | 1.0 | 0.0\r\n1|1.0 | 0.0 | 0.0 | 0.0\r\n2|1.0 | 0.0 | 0.0 | 1.0\r\n"
      },
      {
        "user": "ogrisel",
        "body": "I think it would make sense to have:\r\n\r\n- `handle_missing=\"encode_as_category\"` by default with an option to set it to `handle_ignore=\"ignore\"`,\r\n- `missing_marker=(np.nan, pd.NA, None)` which make it possible to specify which values should be consider missing value markers.\r\n\r\nThe `missing_marker` parameter would only be used if `handle_ignore=\"ignore\"` to avoid creating output columns for such values."
      },
      {
        "user": "woodly0",
        "body": "@glemaitre: Thanks for your answer. What you suggest is what I am actually doing already. \r\nMy question is about including this in the `OneHotEncoder` itself and thus avoiding the additional `FunctionTransformer`. \r\n\r\nIn order to avoid collinearity, I could use the parameter `drop='first'` but it seems more intuitive to drop the \"_None\" feature instead. Don't you agree? It is what the `pandas.get_dummies()` function does by default."
      }
    ]
  },
  {
    "issue_number": 30954,
    "title": "QDA is not reproducible",
    "author": "tomviering",
    "state": "open",
    "created_at": "2025-03-07T13:37:38Z",
    "updated_at": "2025-05-04T11:30:30Z",
    "labels": [
      "Bug",
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nWe are running QDA with default hyperparameters on the same dataset, on 2 different machines (linux). We find that the results change significantly when ran on a different machine. For more details, please see this Gist:\n[https://gist.github.com/tomviering/43519a1f2a0e0ffe7569fb2a5ec2313d](https://gist.github.com/tomviering/43519a1f2a0e0ffe7569fb2a5ec2313d)\n\n### Steps/Code to Reproduce\n\nPlease see the gist: https://gist.github.com/tomviering/43519a1f2a0e0ffe7569fb2a5ec2313d\n\n### Expected Results\n\nPlease see the gist: https://gist.github.com/tomviering/43519a1f2a0e0ffe7569fb2a5ec2313d\n\n### Actual Results\n\nPlease see the gist: https://gist.github.com/tomviering/43519a1f2a0e0ffe7569fb2a5ec2313d\n\n### Versions\n\n```shell\nPlease see the gist: https://gist.github.com/tomviering/43519a1f2a0e0ffe7569fb2a5ec2313d\n```",
    "comments": [
      {
        "user": "lesteve",
        "body": "So quickly looking at it, it seems like this is Haswell vs SkyLakeX.\n\nJust in case, could you try with the latest versions of all the packages to see if you still can reproduce the issue?\n"
      },
      {
        "user": "tomviering",
        "body": "Which packages? Note that, I didn't see too many updates to QDA in scikit, so I have some doubts whether it was resolved (sorry I accidentally closed this but now is reopened). "
      },
      {
        "user": "jeremiedbb",
        "body": "> Which packages?\n\nI believe sklearn and its dependencies: numpy, scipy.\nThe code looks quite simple and only uses numpy/scipy operations. So it's possible that the discrepency comes from a change in one of these libraries. It may also come from a change in the BLAS library they ship in which case we'll see it in the output of show_versions"
      }
    ]
  },
  {
    "issue_number": 8590,
    "title": "Feature request: forcing a Gram matrix to be positive definite",
    "author": "chrishmorris",
    "state": "open",
    "created_at": "2017-03-15T14:04:43Z",
    "updated_at": "2025-05-02T12:56:22Z",
    "labels": [
      "New Feature",
      "module:gaussian_process"
    ],
    "body": "\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nKernel methods like SVR use a similarity measure rather than a vector of features. The prior knowledge is encoded by making a domain-appropriate similarity measure. However, there is a technical requirement that is sometimes hard to meet: a valid kernel must be symmetric, and positive semidefinite. \r\n\r\n#### Suggested solution\r\nIn some circumstances, it works well to coerce the Gram matrix into a symmetric matrix by averaging it with its transpose; and then coercing it into a PSD matrix by discarding negative eigenvalues. For R users, the latter operation is available here: https://stat.ethz.ch/R-manual/R-devel/library/Matrix/html/nearPD.html.\r\n\r\nThe user can do these calculation for SVR, which supports precomputed kernels, but not for the other kernel methods in sklearn, which do not.\r\n\r\nEven for SVR, the second transformation is more efficient if it is integrated into the Cholesky decomposition which is already performed by the sklearn package.\r\n\r\nI therefore propose an option coerce_kernel=False, on the fit method for SVR.",
    "comments": [
      {
        "user": "amueller",
        "body": "What kernel methods in sklearn don't accept precomputed kernels? I'm not sold on this because it's really hacky. I have not heard of anyone do that. Can you give an example when you'd do that?"
      },
      {
        "user": "chrishmorris",
        "body": "I do this because the objects in my x space are molecules. My kernel is Tanimoto similarity between molecule fingerprints.\r\n\r\nThe point of kernel methods is that devising an appropriate kernel function is an alternative to feature extraction. Mercer's theorem says that a kernel induces a vector space over the input space, but an infinite-dimensional one, so this approach is inherently more powerful than identifying features. Reducing the input space to a vector of features then choose a kernel function is missing the point."
      },
      {
        "user": "jnothman",
        "body": "To repeat: What kernel methods in sklearn don't accept precomputed kernels?"
      }
    ]
  },
  {
    "issue_number": 29507,
    "title": "In gaussian_process/kernels.py, the Tanimoto kernel would be welcome",
    "author": "UnixJunkie",
    "state": "closed",
    "created_at": "2024-07-17T08:59:08Z",
    "updated_at": "2025-05-02T12:45:28Z",
    "labels": [
      "New Feature",
      "Needs Triage"
    ],
    "body": "### Describe the workflow you want to enable\n\nHere is a formula:\r\nx*y / (||x||^2 + ||y||^2 - x*Y)\r\n\n\n### Describe your proposed solution\n\nIn the context of Gaussian Process Regression, maybe this should be multiplied by the variance,\r\nso the formula becomes:\r\nv * (x*y / (||x||^2 + ||y||^2 - x*Y))\n\n### Describe alternatives you've considered, if relevant\n\nImplement a new kernel myself, but since implementation of a kernel requires much more than just\r\na K method (evaluate the kernel), I find this way too dangerous.\r\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "Thanks for the report. But from your description it's not clear to me what you're intending to change. It seems to me writing a custom kernel is exactly designed for these cases.\r\n\r\nSo closing as out of scope, but happy to reopen if more context is given."
      },
      {
        "user": "UnixJunkie",
        "body": "Could a scikit-learn hacker provide an implementation of the Tanimoto kernel?\r\nSince scikit-learn's kernel template requires much more than just implementing the K method (i.e. score a pair of instances), this is super annoying for potential users who are not numerical maths researchers familiar w/ the scikit-learn codebase.\r\n"
      },
      {
        "user": "UnixJunkie",
        "body": "The Tanimoto kernel is for instances which are bitstrings.\r\nThe more general version of this kernel is called the Jaccard index (cf. https://en.wikipedia.org/wiki/Jaccard_index).\r\nThe Jaccard index can handle bitstrings but also vectors of positive integers, so it might be potentially useful to more scikit-learn users.\r\n"
      }
    ]
  },
  {
    "issue_number": 30357,
    "title": "HTML display rendering poorly in vscode \"Dark High Contrast\" color theme",
    "author": "GaelVaroquaux",
    "state": "closed",
    "created_at": "2024-11-27T20:10:36Z",
    "updated_at": "2025-04-30T16:20:09Z",
    "labels": [
      "Bug",
      "frontend"
    ],
    "body": "### Describe the bug\n\nWhen I use vscode, I use the \"Dark High Contrast\" theme, as my eyes are tired. In this mode, some of the estimator names are not visible in the HTML display\n\n### Steps/Code to Reproduce\n\nExecute the following code in a vscode (for instance a cell)\r\n```python\r\n# %%\r\nfrom sklearn.pipeline import make_pipeline\r\nfrom sklearn.decomposition import PCA\r\nfrom sklearn.ensemble import HistGradientBoostingRegressor\r\n\r\npipe = make_pipeline(PCA(), HistGradientBoostingRegressor())\r\npipe\r\n```\n\n### Expected Results\n\nWith the \"Dark (Visual Studio)\" theme, the result is:\r\n![image](https://github.com/user-attachments/assets/1c8d52d4-ce8c-4e8a-a217-fc68be2f2f70)\r\n\n\n### Actual Results\n\nHowever, with the \"Dark High Contrast\", the result is\r\n![image](https://github.com/user-attachments/assets/a229f0dd-c71f-4744-9733-00a82d5258c0)\r\n\r\nNote that the title of the enclosing meta-estimator, here \"Pipeline\", is not visible\n\n### Versions\n\n```shell\ngit main of today (last commit: 426e6be923e34f68bc720ae625c8ca258f473265, merge of #30347)\r\n\r\nSystem:\r\n    python: 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]\r\nexecutable: /bin/python3\r\n   machine: Linux-6.8.0-49-generic-x86_64-with-glibc2.39\r\n\r\nPython dependencies:\r\n      sklearn: 1.7.dev0\r\n          pip: 24.0\r\n   setuptools: 68.1.2\r\n        numpy: 1.26.4\r\n        scipy: 1.11.4\r\n       Cython: 3.0.11\r\n       pandas: 2.1.4+dfsg\r\n   matplotlib: 3.6.3\r\n       joblib: 1.3.2\r\nthreadpoolctl: 3.1.0\r\n```\n```\n",
    "comments": [
      {
        "user": "glemaitre",
        "body": "Uhm I thought that we solve this bug before :). But indeed, we probably don't catch the right variable for the text in the CSS."
      },
      {
        "user": "DeaMariaLeon",
        "body": "/take"
      },
      {
        "user": "GaelVaroquaux",
        "body": "Thanks DÃ©a!\n\nWe recently faced the same problem in skrub and fixed it. I think that the relevant PR is:\nhttps://github.com/skrub-data/skrub/pull/1201\n"
      }
    ]
  },
  {
    "issue_number": 31243,
    "title": "Use more complex data in `test_roc_curve_display.py`",
    "author": "lucyleeow",
    "state": "closed",
    "created_at": "2025-04-23T11:08:34Z",
    "updated_at": "2025-04-30T09:00:54Z",
    "labels": [],
    "body": "`data_binary` fixture in `test_roc_curve_display.py`, which uses the iris dataset: \n\nhttps://github.com/scikit-learn/scikit-learn/blob/af7df5ced0eb1124df12dd389cc4ef7a9042837e/sklearn/metrics/_plot/tests/test_roc_curve_display.py#L28-L30\n\nalways gives a AUC value of 1.0 (with and without `drop_intermediate`). Should we consider using more complex data?\n\nNoticed in https://github.com/scikit-learn/scikit-learn/pull/30399#discussion_r2055566594\n\ncc @glemaitre",
    "comments": [
      {
        "user": "NEREUScode",
        "body": "@lucyleeow The current `data_binary` fixture uses easily separable iris data, which always results in an AUC of 1.0. This makes tests less meaningful, especially for checking features like `drop_intermediate`. Using a more complex dataset `make_classification` would provide more realistic, valuable test results."
      },
      {
        "user": "ogrisel",
        "body": "> Should we consider using more complex data?\n\nI agree. We could use `make_classification` to get a simple yet noisy classification problem for which getting a perfect classifier would be impossible.\n\nAlternatively, we could drop some features from the existing iris dataset to make it more challenging."
      },
      {
        "user": "lucyleeow",
        "body": "@NEREUScode are you interested in making a PR?"
      }
    ]
  },
  {
    "issue_number": 31256,
    "title": "âš ï¸ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Apr 26, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-04-26T02:50:37Z",
    "updated_at": "2025-04-30T08:45:23Z",
    "labels": [
      "module:test-suite"
    ],
    "body": "**CI failed on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=75987&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a)** (Apr 26, 2025)\n- test_precomputed_nearest_neighbors_filtering[60]",
    "comments": [
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76093&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a) on Apr 30, 2025"
      },
      {
        "user": "ogrisel",
        "body": "For the record, the failure was the following:\n\n```python\n_______________ test_precomputed_nearest_neighbors_filtering[60] _______________\n[gw0] linux -- Python 3.13.3 /usr/share/miniconda/envs/testvenv/bin/python\n\nglobal_random_seed = 60\n\n    def test_precomputed_nearest_neighbors_filtering(global_random_seed):\n        # Test precomputed graph filtering when containing too many neighbors\n        X, y = make_blobs(\n            n_samples=250,\n            random_state=global_random_seed,\n            centers=[[1, 1], [-1, -1]],\n            cluster_std=0.01,\n        )\n    \n        n_neighbors = 2\n        results = []\n        for additional_neighbors in [0, 10]:\n            nn = NearestNeighbors(n_neighbors=n_neighbors + additional_neighbors).fit(X)\n            graph = nn.kneighbors_graph(X, mode=\"connectivity\")\n            labels = (\n                SpectralClustering(\n                    random_state=global_random_seed,\n                    n_clusters=2,\n                    affinity=\"precomputed_nearest_neighbors\",\n                    n_neighbors=n_neighbors,\n                )\n                .fit(graph)\n                .labels_\n            )\n            results.append(labels)\n    \n>       assert_array_equal(results[0], results[1])\nE       AssertionError: \nE       Arrays are not equal\nE       \nE       Mismatched elements: 139 / 250 (55.6%)\nE       Max absolute difference among violations: 1\nE       Max relative difference among violations: 1.\nE        ACTUAL: array([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\nE              0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\nE              1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,...\nE        DESIRED: array([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,\nE              0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\nE              0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,...\n\nX          = array([[ 0.98723448,  0.9898586 ],\n       [-1.00258762, -1.00697942],\n       [ 0.98726373,  0.99184863],\n       [ 0.98... 0.9927963 ],\n       [-1.0010091 , -1.00391894],\n       [ 1.01668103,  1.01155459],\n       [-1.00678887, -0.99002748]])\nadditional_neighbors = 10\nglobal_random_seed = 60\ngraph      = <Compressed Sparse Row sparse matrix of dtype 'float64'\n\twith 3000 stored elements and shape (250, 250)>\nlabels     = array([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,...,\n       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 1, 0, 0, 1, 1, 1, 0], dtype=int32)\nn_neighbors = 2\nnn         = NearestNeighbors(n_neighbors=12)\nresults    = [array([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1...\n       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 1, 0, 0, 1, 1, 1, 0], dtype=int32)]\ny          = array([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,...1, 1, 0, 0, 0,\n       1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n       1, 0, 0, 0, 0, 1, 0, 1])\n\n../1/s/sklearn/cluster/tests/test_spectral.py:130: AssertionError\n```\n\n\n"
      },
      {
        "user": "ogrisel",
        "body": "I ran locally (on arm64 macOS) this test will all admissible random seeds and it passes for me. So this is likely a case of platform/rng seed cross-sensitivity."
      }
    ]
  },
  {
    "issue_number": 28828,
    "title": "Provide examples on how to customize the scikit-learn classes",
    "author": "miguelcsilva",
    "state": "open",
    "created_at": "2024-04-13T14:21:24Z",
    "updated_at": "2025-04-29T17:03:41Z",
    "labels": [
      "Documentation",
      "Moderate",
      "help wanted"
    ],
    "body": "### Describe the issue linked to the documentation\n\nRecently I add to implement my custom CV Splitter for a project I'm working on. My first instinct was to look in the documentation to see if there were any examples of how this could be done. I could not find anything too concrete, but after not too much time I found the [Glossary of Common Terms and API Elements](https://scikit-learn.org/stable/glossary.html#). Although not exactly what I hoped to find, it does have a section on [CV Splitters](https://scikit-learn.org/stable/glossary.html#term-CV-splitter). From there I can read that they expected to have a `split` and `get_n_splits` methods, and following some other links in the docs I can find what arguments they take and what they should return.\r\n\r\nAlthough all the information is in fact there, I believe that more inexperienced users may find it a bit more difficult to piece together all the pieces, and was thinking if it wouldn't be beneficial for all users to have a section in the documentation with examples on how to customize the sci-kit learn classes to suit the user's needs. After all, I understand the library was developed  with a API in mind that would allow for this exact flexibility and customization.\r\n\r\nI know this is not a small task, and may add a non-trivial maintenance burden to the team, but would like to understand how the maintenance team would feel about a space in the documentation for these customization examples? Of course as the person suggesting I would be happy contribute for this.\n\n### Suggest a potential alternative/fix\n\nOne way I could see this taking shape would be with a dedicated page in the documentation, where examples of customized classes could be demonstrated. I think it's also important to show how the customized class would be used as part of a larger pipeline and allowing the user to copy and paste the code to their working environment.\r\nI'll leave below of an example of a custom CV Splitter for discussion. But the idea would be to then expand to most commonly used classes.\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import datasets\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.model_selection import cross_val_score\r\n\r\nX, y = datasets.load_iris(return_X_y=True)\r\n\r\nclass CustomSplitter:\r\n    def __init__(self, n_folds=5) -> None:\r\n        self.n_folds = n_folds\r\n\r\n    def split(self, X = None, y = None, groups = None):\r\n        assert X.shape[0] == y.shape[0]\r\n        idxs = np.arange(X.shape[0])\r\n        splits = np.array_split(idxs, self.get_n_splits())\r\n        for split_idx, split in enumerate(splits):\r\n            train_idxs = np.concatenate([split for idx, split in enumerate(splits) if idx != split_idx])\r\n            test_idxs = split\r\n            yield train_idxs, test_idxs\r\n\r\n    def get_n_splits(self, X = None, y = None, groups = None):\r\n        return self.n_folds\r\n\r\nclf = LogisticRegression(random_state=42)\r\nscores = cross_val_score(clf, X, y, cv=CustomSplitter(n_folds=5))\r\n```",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "This can certainly be an example (under our `examples` folder, with the right links from our user guides."
      },
      {
        "user": "plon-Susk7",
        "body": "Hey @adrinjalali, I'd love to work on this issue. I am new to contributing to this repo. Any sort of advice/help will be greatly appreciated :) "
      },
      {
        "user": "miguelcsilva",
        "body": "> This can certainly be an example (under our `examples` folder, with the right links from our user guides.\n\nGreat. Would it be worth it to compile first a list of classes for which it would be useful to provide such customization examples? Or should we just start with this one and take it from there?"
      }
    ]
  },
  {
    "issue_number": 30400,
    "title": "Finding indexes with `np.where(condition)` or `np.asarray(condition).nonzero()`",
    "author": "StefanieSenger",
    "state": "closed",
    "created_at": "2024-12-03T12:57:54Z",
    "updated_at": "2025-04-29T10:58:44Z",
    "labels": [
      "good first issue"
    ],
    "body": "Throughout the repo, we use `np.where(condition)` for getting indexes, for instance in [SelectorMixin.get_support()](https://github.com/scikit-learn/scikit-learn/blob/fba028b07ed2b4e52dd3719dad0d990837bde28c/sklearn/feature_selection/_base.py#L73), in [SimpleImputer.transform()](https://github.com/scikit-learn/scikit-learn/blob/fba028b07ed2b4e52dd3719dad0d990837bde28c/sklearn/impute/_base.py#L670) and in several of our examples ([example](https://github.com/scikit-learn/scikit-learn/blob/fba028b07ed2b4e52dd3719dad0d990837bde28c/examples/linear_model/plot_sgd_iris.py#L58)).\r\n\r\nThe numpy documentation [discourages](https://numpy.org/doc/2.1/reference/generated/numpy.where.html) the use of `np.where` with just passing a condition and recommends `np.asarray(condition).nonzero()` instead.\r\n\r\nFor cleanliness of code, should we adopt this recommendation, at least in the examples? Or are there good reasons why we do that?",
    "comments": [
      {
        "user": "ogrisel",
        "body": "The reason why `np.where(condition)` is discouraged is documented as follows:\r\n\r\n> Using [nonzero](https://numpy.org/doc/2.1/reference/generated/numpy.nonzero.html#numpy.nonzero) directly should be preferred, as it behaves correctly for subclasses.\r\n\r\nI don't recall any issue reported by users using scikit-learn with subclasses of NumPy arrays and having problems as a result. I am not even sure what \"behaves correctly\" means in that context: returning a NumPy array of integer indices or a subclass instance storing the same indices? For our use cases, I think returning a NumPy array (not subclassing) is perfectly fine, but any option might work.\r\n\r\nLet's try:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> class my_array(np.ndarray):\r\n...     pass\r\n... \r\n>>> condition = np.asarray([True, False, False, True, True]).view(my_array)\r\n>>> condition\r\nmy_array([ True, False, False,  True,  True])\r\n>>> np.where(condition)[0]\r\narray([0, 3, 4])\r\n>>> condition.nonzero()[0]\r\narray([0, 3, 4])\r\n>>> np.nonzero(condition)[0]\r\narray([0, 3, 4])\r\n```\r\n\r\nSo I don't see any difference: neither returns an instance of the `my_array` subclass.\r\n\r\nI traced this recommendation in the doc to this PR by @eric-wieser: https://github.com/numpy/numpy/pull/11425\r\n  \r\nHowever, whenever we convert a scikit-learn estimator to add array API support, the `xp.where` prototype in the spec implies that the second and third argument are not optional (contrary to calling `np.where` with a single `condition` argument):\r\n\r\nhttps://data-apis.org/array-api/latest/API_specification/generated/array_api.where.html#where\r\n\r\nSince `nonzero` is also part of the array API, I think it would make sense to use `xp.nonzero` instead when we convert code to add array API support:\r\n\r\nhttps://data-apis.org/array-api/latest/API_specification/generated/array_api.nonzero.html#nonzero\r\n\r\nApart from adding array API support, I would rather not change the code unless we identify a good reason to do it (in particular if we identify a user-facing impact). Personally, I am not convinced that one is particularly more readable than the other.\r\n\r\nThat being said, I would be curious to understand better why `np.where(condition)` is discouraged in the first place: it's not clear from the NumPy doc and my quick experiment above failed to clarify the issue."
      },
      {
        "user": "lesteve",
        "body": "Maybe the note about the issue with subclasses is actually a reference to using `np.where` with a masked array: https://github.com/numpy/numpy/pull/11425#issuecomment-401296841."
      },
      {
        "user": "lorentzenchr",
        "body": "@eric-wieser @seberg friendly ping. We would like to better understand the numpy recommendation of using `np.asarray(x).nonzero()` over `np.where(x)`.\r\n\r\nPersonally, I would follow numpyâ€˜s recommendation without questioning it too much: their realm, their expertise."
      }
    ]
  },
  {
    "issue_number": 31049,
    "title": "RFC adopt narwhals for dataframe support",
    "author": "lorentzenchr",
    "state": "open",
    "created_at": "2025-03-21T13:15:28Z",
    "updated_at": "2025-04-29T09:36:57Z",
    "labels": [
      "RFC"
    ],
    "body": "At least as of [SLEP018](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep018/proposal.html), scikit-learn supports dataframes passed as `X`. In #25896 is a further place of current discussions.\n\nThis issue is to discuss whether or not, or in which form, a future scikit-learn should depend on [narwhals](https://github.com/narwhals-dev/narwhals) for general dataframe support.\n\n`+` wide df support\n`+` less maintenance within scikit-learn\n`-` external dependency\n\n@scikit-learn/core-devs @MarcoGorelli ",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "I'm personally happy with depending on narwhals for all dataframe work.\n\nI'm also okay with a hard dependency since it's a very lightweight lib with no transient dependencies. But I wouldn't say no to a soft dependency implementation, I just think it's nicer for users if it's always installed with sklearn."
      },
      {
        "user": "thomasjpfan",
        "body": "Is there a way to positionally select the rows with narwhals? (This is important for `_safe_indexing`)\n\nConcretely, how do I get the `[1, 3, 5]` positional rows in a narwhals compliant dataframe? In pandas, it would be this:\n\n```python\nimport pandas as pd\n\nx = pd.DataFrame({\"a\": [4, 2, 3, 4, 5, 6], \"b\": [3, 5, 14, 2, 421, 12]})\n\nx.loc[[1, 3, 5], :]\n```"
      },
      {
        "user": "MarcoGorelli",
        "body": "Thanks for the ping!\n\nWhether as a hard-dependency, soft-dependency, or vendored, I'd love to see this ðŸ˜ \n\n@thomasjpfan yup, numpy-style indexing is supported on `narwhals.DataFrame` objects (but not on `narwhals.LazyFrame`, where [row order is undefined](https://narwhals-dev.github.io/narwhals/basics/order_dependence/)):\n```python\nIn [8]: df = nw.from_native(x, eager_only=True)\n\nIn [9]: df\nOut[9]:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n|Narwhals DataFrame|\n|------------------|\n|       a    b     |\n|    0  4    3     |\n|    1  2    5     |\n|    2  3   14     |\n|    3  4    2     |\n|    4  5  421     |\n|    5  6   12     |\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nIn [10]: df[[1,3,5]]\nOut[10]:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n|Narwhals DataFrame|\n|------------------|\n|        a   b     |\n|     1  2   5     |\n|     3  4   2     |\n|     5  6  12     |\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\nSee [DataFrame.__getitem__](https://narwhals-dev.github.io/narwhals/api-reference/dataframe/#narwhals.dataframe.DataFrame.__getitem__). PyShiny makes heavy use of it\n\nYou may also be interested in [what about the pandas index?](https://narwhals-dev.github.io/narwhals/pandas_like_concepts/pandas_index/)"
      }
    ]
  },
  {
    "issue_number": 28368,
    "title": "Crash in T-SNE",
    "author": "mdruiter",
    "state": "open",
    "created_at": "2024-02-05T23:03:32Z",
    "updated_at": "2025-04-28T20:42:06Z",
    "labels": [
      "Bug",
      "help wanted",
      "module:manifold",
      "cython"
    ],
    "body": "### Describe the bug\r\n\r\nI got a crash using the (external) [hdbscan package](https://github.com/scikit-learn-contrib/hdbscan) in some special situation. I [debugged it](https://github.com/scikit-learn-contrib/hdbscan/issues/623) and found out that it happens in the scikit-learn package, specifically its T-SNE implementation. The hdbscan maintainer ([Leland McInnes](https://github.com/lmcinnes)!) suggested to report it here.\r\n\r\nIs this something that should be guarded for in scikit-learn?\r\n\r\n### Steps/Code to Reproduce\r\n\r\nThe simplest way to reproduce it (using the hdbscan package) is:\r\n```py\r\nimport numpy as np\r\nimport hdbscan\r\nmodel = hdbscan.HDBSCAN(gen_min_span_tree=True)\r\ndata = np.zeros((91, 3))\r\nclustering = model.fit(data)\r\nclustering.minimum_spanning_tree_.plot()\r\n```\r\nNote that it also happens when only a relative small proportion of points are equal (but only sometimes?), this is just the easiest way to show it. By default some warnings are displayed:\r\n> ...\\sklearn\\decomposition\\_pca.py:685: RuntimeWarning: invalid value encountered in divide\r\n>   self.explained_variance_ratio_ = self.explained_variance_ / total_var\r\n> ...\\sklearn\\manifold\\_t_sne.py:1002: RuntimeWarning: invalid value encountered in divide\r\n>   X_embedded = X_embedded / np.std(X_embedded[:, 0]) * 1e-4 \r\n\r\nIn the end it appears to be a problem in `sklearn.manifold._t_sne._barnes_hut_tsne.gradient()`, not (always?) being able to handle `nan` values. For example, this reproduces the crash:\r\n```py\r\nimport numpy as np\r\nfrom sklearn.manifold._t_sne import _barnes_hut_tsne\r\nneighbors = np.array([1, 2, 0, 2, 0, 1], dtype='int64')\r\nval_P = np.full_like(neighbors, 2 / 45, dtype='float32')\r\npos_output = np.full((3, 2), np.nan, dtype='float32')\r\nforces = np.zeros_like(pos_output)\r\nindptr = np.arange(7, step=2, dtype='int64')\r\n_barnes_hut_tsne.gradient(val_P, pos_output, neighbors, indptr, forces, 0.5, 2, 11)\r\n```\r\nOne layer deeper, the crash occurs inside `sklearn.neighbors._quad_tree._QuadTree.build_tree()`, as follows:\r\n```py\r\nimport numpy as np\r\nfrom sklearn.neighbors._quad_tree import _QuadTree\r\nqt = _QuadTree(2, 11)\r\nX = np.full((3, 2), np.nan, dtype='float32')\r\nqt.build_tree(X)\r\n```\r\nThe output of this (due to `verbose=11`) up to the crash is:\r\n> [QuadTree] bounding box axis 0 : [nan, nan]\r\n> [QuadTree] bounding box axis 1 : [nan, nan]\r\n> [QuadTree] Inserting depth 0\r\n> [QuadTree] inserted point 0 in cell 0\r\n> [QuadTree] Inserting depth 0\r\n> [QuadTree] inserted point 0 in new child 1\r\n> [QuadTree] Inserting depth 0\r\n> [QuadTree] Inserting depth 1\r\n> ...\r\n> [QuadTree] Inserting depth 6271\r\n> [QuadTree] inserted point 0 in new child 6272\r\n> [QuadTree] Inserting depth 6271\r\n> [QuadTree] Inserting depth 6272\r\n\r\nI didn't dig into the QuadTree code.\r\n\r\n### Expected Results\r\n\r\nI would expect anything but Python crashing.\r\n\r\n### Actual Results\r\n\r\nPython crashed: its console window just went away silently (on Windows).\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\nÂ  Â  python: 3.12.1 (tags/v3.12.1:2305ca5, Dec Â 7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)]\r\nexecutable: C:\\Users\\Public\\Software\\Python\\python.exe\r\nÂ  Â machine: Windows-11-10.0.22631-SP0\r\n\r\nPython dependencies:\r\nÂ  Â  Â  sklearn: 1.4.0\r\nÂ  Â  Â  Â  Â  pip: 23.3.2\r\nÂ  Â setuptools: 69.0.3\r\nÂ  Â  Â  Â  numpy: 1.26.3\r\nÂ  Â  Â  Â  scipy: 1.12.0\r\nÂ  Â  Â  Â Cython: 3.0.8\r\nÂ  Â  Â  Â pandas: 2.2.0\r\nÂ  Â matplotlib: 3.8.2\r\nÂ  Â  Â  Â joblib: 1.3.2\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\nÂ  Â  Â  Â user_api: blas\r\nÂ  Â internal_api: openblas\r\nÂ  Â  num_threads: 16\r\nÂ  Â  Â  Â  Â prefix: libopenblas\r\nÂ  Â  Â  Â filepath: C:\\Users\\Public\\Software\\Python\\Lib\\site-packages\\numpy.libs\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll\r\nÂ  Â  Â  Â  version: 0.3.23.dev\r\nthreading_layer: pthreads\r\nÂ  Â architecture: Cooperlake\r\n\r\nÂ  Â  Â  Â user_api: openmp\r\nÂ  Â internal_api: openmp\r\nÂ  Â  num_threads: 16\r\nÂ  Â  Â  Â  Â prefix: vcomp\r\nÂ  Â  Â  Â filepath: C:\\Users\\Public\\Software\\Python\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\nÂ  Â  Â  Â  version: None\r\n\r\nÂ  Â  Â  Â user_api: blas\r\nÂ  Â internal_api: openblas\r\nÂ  Â  num_threads: 16\r\nÂ  Â  Â  Â  Â prefix: libopenblas\r\nÂ  Â  Â  Â filepath: C:\\Users\\Public\\Software\\Python\\Lib\\site-packages\\scipy.libs\\libopenblas_v0.3.20-571-g3dec11c6-gcc_10_3_0-c2315440d6b6cef5037bad648efc8c59.dll\r\nÂ  Â  Â  Â  version: 0.3.21.dev\r\nthreading_layer: pthreads\r\nÂ  Â architecture: Cooperlake\r\n```\r\n",
    "comments": [
      {
        "user": "lesteve",
        "body": "Thanks a lot for the stand-alone code snippets, I can reproduce the error from your first snippet on Linux so at least this is not Windows-specific.\r\n\r\nMy wild-guess is that nans should be dealt with by higher level code before reaching low-level Cython code that likely is not meant to handle nans.\r\n\r\nHere is some additional info from gdb (not that useful I am afraid):\r\n```\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n0x00007fffb3bc5f69 in __pyx_f_7sklearn_9neighbors_10_quad_tree_9_QuadTree_insert_point () from /home/lesteve/micromamba/envs/del2/lib/python3.12/site-packages/sklearn/neighbors/_quad_tree.cpython-312-x86_64-linux-gnu.so\r\n```\r\n\r\nMy understanding is that the issue comes from some plotting code that is specific to the hdbscan package and does a PCA followed by a TSNE:\r\nhttps://github.com/scikit-learn-contrib/hdbscan/blob/98928d0c095715edc9584e7989bd8559673bc2f0/hdbscan/plots.py#L824-L832\r\n\r\nI am not too sure whether PCA, TSNE or the hdbscan-specific plotting code should be responsible for erroring with a nice error message when there are nans."
      },
      {
        "user": "lesteve",
        "body": "In #29127, there was a variation of this issue using only scikit-learn:\r\n\r\n```py\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.manifold import TSNE\r\n\r\nimport sklearn\r\nsklearn.show_versions()\r\n\r\nhashGNN_embeddings = {'projectName': {0: 'react-router', 1: 'react-router-dom', 2: 'react-router-native', 3: 'react-router-dom', 4: 'router'}, 'embedding': {0: [0.0, -0.4330126941204071, -0.4330126941204071, 0.21650634706020355, -0.21650634706020355, -0.4330126941204071, 0.4330126941204071, 0.0, -1.0825317353010178, -0.21650634706020355, -0.21650634706020355, 0.6495190411806107, 0.8660253882408142, 0.21650634706020355, -0.21650634706020355, -0.4330126941204071, 0.21650634706020355, -0.4330126941204071, -0.8660253882408142, 0.4330126941204071, 0.21650634706020355, 0.8660253882408142, 0.0, 0.4330126941204071, 0.21650634706020355, 0.21650634706020355, -0.21650634706020355, 0.0, 0.0, 0.0, -0.21650634706020355, 0.4330126941204071, 1.0825317353010178, -0.4330126941204071, 0.21650634706020355, 1.0825317353010178, -0.4330126941204071, 1.0825317353010178, 0.0, -0.8660253882408142, 0.21650634706020355, 0.8660253882408142, 0.0, 0.21650634706020355, 0.21650634706020355, 0.0, 0.21650634706020355, 0.6495190411806107, 0.6495190411806107, 0.0, -0.6495190411806107, 0.21650634706020355, -0.4330126941204071, 0.4330126941204071, 0.21650634706020355, 0.6495190411806107, -0.6495190411806107, 0.4330126941204071, 0.6495190411806107, 0.0, 0.4330126941204071, 0.0, 0.21650634706020355, 0.4330126941204071], 1: [0.0, -0.4330126941204071, -0.4330126941204071, 0.21650634706020355, -0.21650634706020355, -0.4330126941204071, 0.4330126941204071, 0.0, -1.0825317353010178, -0.21650634706020355, -0.21650634706020355, 0.6495190411806107, 0.8660253882408142, 0.21650634706020355, -0.21650634706020355, -0.4330126941204071, 0.21650634706020355, -0.4330126941204071, -0.8660253882408142, 0.4330126941204071, 0.21650634706020355, 0.8660253882408142, 0.0, 0.4330126941204071, 0.21650634706020355, 0.21650634706020355, -0.21650634706020355, 0.0, 0.0, 0.0, -0.21650634706020355, 0.4330126941204071, 1.0825317353010178, -0.4330126941204071, 0.21650634706020355, 1.0825317353010178, -0.4330126941204071, 1.0825317353010178, 0.0, -0.8660253882408142, 0.21650634706020355, 0.8660253882408142, 0.0, 0.21650634706020355, 0.21650634706020355, 0.0, 0.21650634706020355, 0.6495190411806107, 0.6495190411806107, 0.0, -0.6495190411806107, 0.21650634706020355, -0.4330126941204071, 0.4330126941204071, 0.21650634706020355, 0.6495190411806107, -0.6495190411806107, 0.4330126941204071, 0.6495190411806107, 0.0, 0.4330126941204071, 0.0, 0.21650634706020355, 0.4330126941204071], 2: [0.0, -0.4330126941204071, -0.4330126941204071, 0.21650634706020355, -0.21650634706020355, -0.4330126941204071, 0.4330126941204071, 0.0, -1.0825317353010178, -0.21650634706020355, -0.21650634706020355, 0.6495190411806107, 0.8660253882408142, 0.21650634706020355, -0.21650634706020355, -0.4330126941204071, 0.21650634706020355, -0.4330126941204071, -0.8660253882408142, 0.4330126941204071, 0.21650634706020355, 0.8660253882408142, 0.0, 0.4330126941204071, 0.21650634706020355, 0.21650634706020355, -0.21650634706020355, 0.0, 0.0, 0.0, -0.21650634706020355, 0.4330126941204071, 1.0825317353010178, -0.4330126941204071, 0.21650634706020355, 1.0825317353010178, -0.4330126941204071, 1.0825317353010178, 0.0, -0.8660253882408142, 0.21650634706020355, 0.8660253882408142, 0.0, 0.21650634706020355, 0.21650634706020355, 0.0, 0.21650634706020355, 0.6495190411806107, 0.6495190411806107, 0.0, -0.6495190411806107, 0.21650634706020355, -0.4330126941204071, 0.4330126941204071, 0.21650634706020355, 0.6495190411806107, -0.6495190411806107, 0.4330126941204071, 0.6495190411806107, 0.0, 0.4330126941204071, 0.0, 0.21650634706020355, 0.4330126941204071], 3: [0.0, -0.4330126941204071, -0.4330126941204071, 0.21650634706020355, -0.21650634706020355, -0.4330126941204071, 0.4330126941204071, 0.0, -1.0825317353010178, -0.21650634706020355, -0.21650634706020355, 0.6495190411806107, 0.8660253882408142, 0.21650634706020355, -0.21650634706020355, -0.4330126941204071, 0.21650634706020355, -0.4330126941204071, -0.8660253882408142, 0.4330126941204071, 0.21650634706020355, 0.8660253882408142, 0.0, 0.4330126941204071, 0.21650634706020355, 0.21650634706020355, -0.21650634706020355, 0.0, 0.0, 0.0, -0.21650634706020355, 0.4330126941204071, 1.0825317353010178, -0.4330126941204071, 0.21650634706020355, 1.0825317353010178, -0.4330126941204071, 1.0825317353010178, 0.0, -0.8660253882408142, 0.21650634706020355, 0.8660253882408142, 0.0, 0.21650634706020355, 0.21650634706020355, 0.0, 0.21650634706020355, 0.6495190411806107, 0.6495190411806107, 0.0, -0.6495190411806107, 0.21650634706020355, -0.4330126941204071, 0.4330126941204071, 0.21650634706020355, 0.6495190411806107, -0.6495190411806107, 0.4330126941204071, 0.6495190411806107, 0.0, 0.4330126941204071, 0.0, 0.21650634706020355, 0.4330126941204071], 4: [0.0, -0.4330126941204071, -0.4330126941204071, 0.21650634706020355, -0.21650634706020355, -0.4330126941204071, 0.4330126941204071, 0.0, -1.0825317353010178, -0.21650634706020355, -0.21650634706020355, 0.6495190411806107, 0.8660253882408142, 0.21650634706020355, -0.21650634706020355, -0.4330126941204071, 0.21650634706020355, -0.4330126941204071, -0.8660253882408142, 0.4330126941204071, 0.21650634706020355, 0.8660253882408142, 0.0, 0.4330126941204071, 0.21650634706020355, 0.21650634706020355, -0.21650634706020355, 0.0, 0.0, 0.0, -0.21650634706020355, 0.4330126941204071, 1.0825317353010178, -0.4330126941204071, 0.21650634706020355, 1.0825317353010178, -0.4330126941204071, 1.0825317353010178, 0.0, -0.8660253882408142, 0.21650634706020355, 0.8660253882408142, 0.0, 0.21650634706020355, 0.21650634706020355, 0.0, 0.21650634706020355, 0.6495190411806107, 0.6495190411806107, 0.0, -0.6495190411806107, 0.21650634706020355, -0.4330126941204071, 0.4330126941204071, 0.21650634706020355, 0.6495190411806107, -0.6495190411806107, 0.4330126941204071, 0.6495190411806107, 0.0, 0.4330126941204071, 0.0, 0.21650634706020355, 0.4330126941204071]}}\r\nembeddings = pd.DataFrame.from_dict(hashGNN_embeddings)\r\nembeddings_as_numpy_array = np.array(embeddings.embedding.to_list())\r\ntsne = TSNE(n_components=2, perplexity=3.0, verbose=1, random_state=50)\r\n\r\ntwo_dimension_node_embeddings = tsne.fit_transform(embeddings_as_numpy_array)\r\n```\r\n\r\nSame warning:\r\n```\r\n/home/lesteve/micromamba/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:793: RuntimeWarning: invalid value encountered in divide\r\n  self.explained_variance_ratio_ = self.explained_variance_ / total_var\r\n/home/lesteve/micromamba/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1030: RuntimeWarning: invalid value encountered in divide\r\n  X_embedded = X_embedded / np.std(X_embedded[:, 0]) * 1e-4\r\n```\r\n\r\nand also a segmentation fault with similar gdb info as https://github.com/scikit-learn/scikit-learn/issues/28368#issuecomment-1931390670."
      },
      {
        "user": "vitorpohlenz",
        "body": "Hi there, @lesteve do you think I could work on this issue?\n\nIt seems that the problem is a division by zero in `np.std`, as the example provided by you above (I also get the same error on my machine in the current version of sklearn). \nI need to check why it is necessary (maybe for scaling the data or comparing distances?)"
      }
    ]
  },
  {
    "issue_number": 31257,
    "title": "âš ï¸ CI failed on Wheel builder (last failure: Apr 28, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-04-27T04:31:01Z",
    "updated_at": "2025-04-28T15:05:43Z",
    "labels": [
      "Bug",
      "free-threading"
    ],
    "body": "**CI is still failing on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14699848568)** (Apr 28, 2025)\n",
    "comments": [
      {
        "user": "ogrisel",
        "body": "The nightly CI has discovered a Cython-related problem on all the free-threading builds:\n\n```python-traceback\n   _______________ ERROR collecting utils/tests/test_cython_blas.py _______________\n  ImportError while importing test module '/tmp/tmp.O8xqw5JN27/venv/lib/python3.13t/site-packages/sklearn/utils/tests/test_cython_blas.py'.\n  Hint: make sure your test modules/packages have valid Python names.\n  Traceback:\n  /opt/_internal/cpython-3.13.3-nogil/lib/python3.13t/importlib/__init__.py:88: in import_module\n      return _bootstrap._gcd_import(name[level:], package, level)\n  ../venv/lib/python3.13t/site-packages/sklearn/utils/tests/test_cython_blas.py:4: in <module>\n      from sklearn.utils._cython_blas import (\n  E   ImportError: cannot import name 'ColMajor' from 'sklearn.utils._cython_blas' (/tmp/tmp.O8xqw5JN27/venv/lib/python3.13t/site-packages/sklearn/utils/_cython_blas.cpython-313t-x86_64-linux-gnu.so)\n```\n\n"
      },
      {
        "user": "ogrisel",
        "body": "`ColMajor` comes from:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/7131d9488dfb8edd6ae042caca57dd76523f395b/sklearn/utils/_cython_blas.pxd#L4-L6"
      },
      {
        "user": "ogrisel",
        "body": "I am not sure exactly what caused this build to start failing 2 days ago. Here is the history of the runs:\n\nhttps://github.com/scikit-learn/scikit-learn/actions/workflows/wheels.yml?query=branch%3Amain+event%3Aschedule"
      }
    ]
  },
  {
    "issue_number": 26418,
    "title": "RFC Supporting `scipy.sparse.sparray`",
    "author": "jjerphan",
    "state": "open",
    "created_at": "2023-05-22T23:06:08Z",
    "updated_at": "2025-04-28T14:59:18Z",
    "labels": [
      "New Feature",
      "API",
      "Needs Decision",
      "RFC"
    ],
    "body": "### Context\r\n\r\nSciPy is now favoring sparse arrays (i.e. `scipy.sparse.sparray` and its subclasses) over sparse matrices (i.e. `scipy.sparse.spmatrix` and its subclasses) to enlarge the scope of matrices to $n$-dimensional data-structures since SciPy 1.8 (see https://github.com/scipy/scipy/pull/14822).\r\n\r\nSparse matrices now subclass their associated sparse arrays (see https://github.com/scipy/scipy/pull/18440).\r\n\r\nscikit-learn has been supporting sparse matrices but now also needs to support SciPy sparse arrays.\r\n\r\n### Proposed solutions\r\n\r\nOrdered by preference:\r\n - Use `scipy.sparse.issparse` and the `format` attribute everywhere and not use `isinstance` at all\r\n - Use `isinstance` on private compatibility class defined to be `scipy.sparse.spmatrix` or `scipy.sparse.sparray` conditionally on SciPy's version (i.e. use `scipy.sparse.sparray` if available)\r\n - Rely on duck-typing or the class name to check for sparse arrays\r\n\r\ncc @ivirshup",
    "comments": [
      {
        "user": "thomasjpfan",
        "body": "In general, I'm +1 with supporting `scipy.sparse.sparray`. Although, we likely need to support both spmatrix and sparray for a while. To support both, I suspect we need to be careful about some of the [behavior differences](https://docs.scipy.org/doc/scipy/reference/sparse.html) between `spmatrix` and `sparray`."
      },
      {
        "user": "jjerphan",
        "body": "https://github.com/scikit-learn/scikit-learn/pull/26420 has been opened as a small preliminary."
      },
      {
        "user": "lorentzenchr",
        "body": "Also +1 for supporting `scipy.sparse.sparray`.\r\nThe real question for me is what to do with the old sparse matrices:\r\n1. Follow scipy. (Whatâ€˜s their plan? Deprecation, removal, continued support?)\r\n2. Deprecate, maybe longer depr cycle than usual.\r\n3. Continued support\r\n\r\nIâ€˜m leaning towards 2 or 1."
      }
    ]
  },
  {
    "issue_number": 30479,
    "title": "Version 1.6.X: ClassifierMixIn failing with new __sklearn_tags__ function",
    "author": "DaMuBo",
    "state": "closed",
    "created_at": "2024-12-13T09:40:20Z",
    "updated_at": "2025-04-28T14:50:58Z",
    "labels": [
      "Bug",
      "Regression"
    ],
    "body": "### Describe the bug\r\n\r\nHi,\r\n\r\nwe are using Sklearn in our projects for different classification training methods on production level. In the dev stage we upgraded to the latest release and our Training failed due to changes in the ClassifierMixIn Class. We use it in combination with a sklearn Pipeline.\r\n\r\nin 1.6.X the following function was introduced:\r\n\r\n```\r\n    def __sklearn_tags__(self):\r\n        tags = super().__sklearn_tags__()\r\n        tags.estimator_type = \"classifier\"\r\n        tags.classifier_tags = ClassifierTags()\r\n        tags.target_tags.required = True\r\n        return tags\r\n```\r\n\r\nIt is calling the sklearn_tags methods from it's parent class. But the ClassifierMixIn doesn't have a parent class. So it says function super().__sklearn_tags__() is not existing.\r\n\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.base import ClassifierMixin,\r\nfrom sklearn.pipeline import Pipeline\r\nimport numpy as np\r\n\r\nclass MyEstimator(ClassifierMixin):\r\n    def __init__(self, *, param=1):\r\n        self.param = param\r\n    def fit(self, X, y=None):\r\n        self.is_fitted_ = True\r\n        return self\r\n    def predict(self, X):\r\n        return np.full(shape=X.shape[0], fill_value=self.param)\r\n\r\nX = np.array([[1, 2], [2, 3], [3, 4]])\r\ny = np.array([1, 0, 1])\r\n\r\n\r\nmy_pipeline = Pipeline([(\"estimator\", MyEstimator(param=1))])\r\nmy_pipeline.fit(X, y)\r\nmy_pipeline.predict(X)\r\n```\r\n\r\n### Expected Results\r\n\r\nA Prediction is returned.\r\n\r\n### Actual Results\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\xxxx\\error_sklearn\\redo_error.py\", line 22, in <module>\r\n    my_pipeline.predict(X)\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 780, in predict\r\n    with _raise_or_warn_if_not_fitted(self):\r\n  File \"C:\\Program Files\\Wpy64-31230\\python-3.12.3.amd64\\Lib\\contextlib.py\", line 144, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 60, in _raise_or_warn_if_not_fitted\r\n    check_is_fitted(estimator)\r\n  File \"C:\\Users\\xxxx\\git_projects\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1756, in check_is_fitted\r\n    if not _is_fitted(estimator, attributes, all_or_any):\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1665, in _is_fitted\r\n    return estimator.__sklearn_is_fitted__()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in __sklearn_is_fitted__\r\n    check_is_fitted(last_step)\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1751, in check_is_fitted\r\n    tags = get_tags(estimator)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\utils\\_tags.py\", line 396, in get_tags\r\n    tags = estimator.__sklearn_tags__()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 540, in __sklearn_tags__\r\n    tags = super().__sklearn_tags__()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'super' object has no attribute '__sklearn_tags__'\r\n```\r\n### Versions\r\n\r\n```shell\r\n1.6.0 / 1.6.X\r\n```\r\n",
    "comments": [
      {
        "user": "gunsodo",
        "body": "@DaMuBo According to [the official documentation](https://scikit-learn.org/1.5/developers/develop.html#rolling-your-own-estimator), I think your `MyEstimator` should inherit from `BaseEstimator` as well.\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.base import BaseEstimator, ClassifierMixin\r\n\r\nclass MyEstimator(ClassifierMixin, BaseEstimator):\r\n    def __init__(self, *, param=1):\r\n        self.param = param\r\n\r\n    def fit(self, X, y=None):\r\n        self.is_fitted_ = True\r\n        return self\r\n\r\n    def predict(self, X):\r\n        return np.full(shape=X.shape[0], fill_value=self.param)\r\n```\r\n\r\nThe code snippet above runs fine with your `.fit()` and `.predict()` calls. `sklearn` also provides `sklearn.utils.estimator_checks.check_estimator` which you can validate your custom estimator as well. I myself find it very useful for production-grade implementation."
      },
      {
        "user": "DaMuBo",
        "body": "@gunsodo  \r\nYes we are not exactly following the documentation here. \r\nWe will change that on our side in the future for more robust code.\r\n\r\nThanks for the hint to the check_estimator we'll look into it.\r\n\r\nBut i also think the ClassifierMixIn shouldn't use a method which it can't execute.\r\n"
      },
      {
        "user": "glemaitre",
        "body": "I would still consider this breakage as a regression. We made effort to raise a proper deprecation warning such that user that defined compatible estimators have one version to change their code.\r\n\r\nWhile we strongly advocate for using `BaseEstimator` and would request it in 1.7, I think that the `Mixin` should indeed not break but instead raise the same deprecation warnings than when one inherit from `BaseEstimator`.\r\n\r\nWe should probably have a try/except around the `super` call and if failing, get the `default_tags` and populate it."
      }
    ]
  },
  {
    "issue_number": 31246,
    "title": "Faster Eigen Decomposition for Isomap & KernelPCA",
    "author": "oerrabie",
    "state": "open",
    "created_at": "2025-04-24T17:05:03Z",
    "updated_at": "2025-04-28T12:09:18Z",
    "labels": [
      "New Feature"
    ],
    "body": "\n\n(disclaimer: this issue and associated PR are part of a student project supervised by @smarie )\n\n### Summary\n\nEigendecomposition is slow when number of samples is large. This impacts decomposition models such as KernelPCA and Isomap. A \"randomized\" eigendecomposition method (from [Halko et al](https://arxiv.org/abs/0909.4061)) [has been introduced for KernelPCA](https://scikit-learn.org/stable/modules/decomposition.html#choice-of-solver-for-kernel-pca) leveraging Halko's algorithm 4.3 for randomized SVD decomposition (also used in [PCA](https://scikit-learn.org/stable/modules/decomposition.html#pca-using-randomized-svd)).\n\nUnfortunately, the current approach is only valid for decomposition of PSD matrices - which suits well for KernelPCA but can not be true in the context of Isomap. Therefore Isomap has not accelerated implementation as of today.\n\nWe propose to introduce an additional approximate eigendecomposition method based on algorithm 5.3 from the same paper.\nThis method should offer a faster alternative to existing solvers (arpack, dense, etc.) while maintaining accuracy, and as opposed to randomized svd, is suitable to find eigenvalues for non-PSD matrices.\n\n### Describe your proposed solution\n\n- Implement `_randomized_eigsh(selection='value')`, that is left as [NotImplemented](https://github.com/scikit-learn/scikit-learn/pull/12069) today.\n- Integrate it as an alternate solver in `Isomap` and in `KernelPCA`.\n- Add tests comparing performance with existing solvers.\n- Provide benchmarks to evaluate speedup and accuracy.\n\n\n### Motivation\n\n- Improves scalability for large datasets.\n- Reduces computation time for eigen decomposition-based methods.\n\nNote: this solution could be used to accelerate all models relying on eigenvalue decomposition, including possibly https://github.com/scikit-learn/scikit-learn/pull/22330",
    "comments": [
      {
        "user": "ogrisel",
        "body": "I am not familiar (anymore) with the details of `KernelPCA`, `Isomap`, `_randomized_eighs` and algorithm 5.3 in general, but what you propose sounds reasonable.\n\nLooking forward to reviewing the PR (I assume #31247 is the PR by your student) and once the CI is green and once it includes some benchmark results."
      },
      {
        "user": "smarie",
        "body": "Thanks @ogrisel for the quick feedback ! \n"
      }
    ]
  },
  {
    "issue_number": 18187,
    "title": "feature_importance causes a BSOD on Windows 10",
    "author": "Devilmoon",
    "state": "closed",
    "created_at": "2020-08-18T16:15:05Z",
    "updated_at": "2025-04-28T11:27:38Z",
    "labels": [
      "Bug",
      "Large Scale",
      "OS:Windows"
    ],
    "body": "\r\n#### Describe the bug\r\nRunning permutation_importance on a medium-sized data set results in a BSOD on Windows 10. The dataset is 470605 x 332, code is running in a Jupyter notebook, Python version 3.7.6, scikit version 0.22.1.\r\nThe BSOD is a KERNEL_SECURITY_CHECK_FAILURE, with ERROR_CODE: `(NTSTATUS) 0xc0000409 - The system detected an overrun of a stack-based buffer in this application. This overrun could potentially allow a malicious user to gain control of this application.`\r\nThe machine has a Ryzen 5 3600 with 16GB of RAM.\r\n\r\n#### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.inspection import permutation_importance\r\nrf = RandomForestClassifier(n_estimators = 250,\r\n                           n_jobs = -1,\r\n                           oob_score = True,\r\n                           bootstrap = True,\r\n                           random_state = 42)\r\nrf.fit(X_train, y_train)\r\npermImp = permutation_importance(rf,\r\n                                 X_val,\r\n                                 y_val,\r\n                                 scoring='f1',\r\n                                 n_repeats=5,\r\n                                 n_jobs=-1,\r\n                                 random_state=42)\r\n```\r\n\r\n#### Expected Results\r\nNo BSOD, permutation importance computed.\r\n\r\n#### Actual Results\r\nBSOD after ~1-2 minutes\r\n\r\n#### Versions\r\n> sklearn.show_versions()\r\n\r\nSystem:\r\n    python: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\Users\\lucag\\anaconda3\\python.exe\r\n   machine: Windows-10-10.0.18362-SP0\r\n\r\nPython dependencies:\r\n       pip: 20.0.2\r\nsetuptools: 45.2.0.post20200210\r\n   sklearn: 0.22.1\r\n     numpy: 1.18.1\r\n     scipy: 1.4.1\r\n    Cython: 0.29.15\r\n    pandas: 1.0.1\r\nmatplotlib: 3.1.3\r\n    joblib: 0.14.1\r\n\r\nBuilt with OpenMP: True\r\n<!-- Thanks for contributing! -->\r\n",
    "comments": [
      {
        "user": "rth",
        "body": "Thanks for the report. It's surprising that even if it detected a stack-Based buffer overrun that it would trigger an equivalent of a kernel panic instead of just killing the application.\r\n\r\nThis will be likely hard to reproduce without the training data where it happens. Unless you also get it if you generate a dataset with an equivalent shape using `make_classification(..., random_state=0)`?"
      },
      {
        "user": "Devilmoon",
        "body": "I will test with a synthetic data set of the same shape ASAP, unfortunately the data I'm currently using is under NDA thus I cannot share it for reproducibility purposes."
      },
      {
        "user": "Devilmoon",
        "body": "I confirm I can reproduce the issue with a synthetic dataset (I used the default on make_classification because I don't think the make up fo the data is relevant to the issue):\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.inspection import permutation_importance\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nX, y = make_classification(n_samples=470605, n_features=332, random_state=0) \r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n\r\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\r\n\r\nrf = RandomForestClassifier(n_estimators = 250,\r\n                           n_jobs = -1,\r\n                           oob_score = True,\r\n                           bootstrap = True,\r\n                           random_state = 42)\r\nrf.fit(X_train, y_train)\r\n\r\npermImp = permutation_importance(rf,\r\n                                 X_val,\r\n                                 y_val,\r\n                                 scoring='f1',\r\n                                 n_repeats=5,\r\n                                 n_jobs=-1,\r\n                                 random_state=42)\r\n```\r\n\r\nI see very high memory usage (>80%, sometimes close to 100%) and a ton of activity on the pagefile and a process related to `joblib_memorymapping_folder` (https://joblib.readthedocs.io/en/latest/auto_examples/parallel_memmap.html), which I suppose is how scikit handles storing/retrieving data from the pagefile when computing data that does not fit in RAM. "
      }
    ]
  },
  {
    "issue_number": 31248,
    "title": "Hangs in LogisticRegression with high intercept_scaling number",
    "author": "GloC99",
    "state": "closed",
    "created_at": "2025-04-24T18:48:45Z",
    "updated_at": "2025-04-28T09:12:55Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nWhen using the `LogisticRegression` model with the solver set to `liblinear` and specifying the `intercept_scaling` parameter, the model hangs without any clear reason. The processing time does not increase gradually with the size of the `intercept_scaling` parameter.\n\n### Steps/Code to Reproduce\n\nWhen running on my machine, the code below complete in around 7 seconds.\n```python\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(\n        intercept_scaling=1.0e+77,\n        solver='liblinear',\n        )\n\nmodel.fit([[0], [5]], [0, 6])\n```\n\nHowever, increasing `intercept_scaling` by just one decimal place causes the model to hang indefinitely:\n```python\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(\n        intercept_scaling=1.0e+78,\n        solver='liblinear',\n        )\n\nmodel.fit([[0], [5]], [0, 6])\n```\n\n### Expected Results\n\nI expect the code to finish running in a reasonable time.\n\nWhen `intercept_scaling` is set as 1.0e+77, the program finished in around 7 sec.\n\n### Actual Results\n\nI terminated the process by after a day, no error trace was given by the program.\n\n```javascript\nCommand terminated by signal 15\n\tCommand being timed: \"python Aidan2.py\"\n\tUser time (seconds): 94481.23\n\tSystem time (seconds): 12.59\n\tPercent of CPU this job got: 99%\n\tElapsed (wall clock) time (h:mm:ss or m:ss): 26:15:08\n\tAverage shared text size (kbytes): 0\n\tAverage unshared data size (kbytes): 0\n\tAverage stack size (kbytes): 0\n\tAverage total size (kbytes): 0\n\tMaximum resident set size (kbytes): 142780\n\tAverage resident set size (kbytes): 0\n\tMajor (requiring I/O) page faults: 3\n\tMinor (reclaiming a frame) page faults: 25293\n\tVoluntary context switches: 69\n\tInvoluntary context switches: 537213\n\tSwaps: 0\n\tFile system inputs: 256\n\tFile system outputs: 0\n\tSocket messages sent: 0\n\tSocket messages received: 0\n\tSignals delivered: 0\n\tPage size (bytes): 4096\n\tExit status: 0\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.13.3 (main, Apr 22 2025, 19:24:41) [GCC 11.4.0]\nexecutable: /home/venv/bin/python\n   machine: Linux-6.8.0-57-generic-x86_64-with-glibc2.35\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0.1\n   setuptools: 79.0.0\n        numpy: 2.2.3\n        scipy: 1.15.2\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: None\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 28\n         prefix: libscipy_openblas\n       filepath: /home/venv/lib/python3.13/site-packages/numpy.libs/libscipy_openblas64_-6bb31eeb.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 28\n         prefix: libscipy_openblas\n       filepath: /home/venv/lib/python3.13/site-packages/scipy.libs/libscipy_openblas-68440149.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 28\n         prefix: libgomp\n       filepath: /home/venv/lib/python3.13/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n        version: None\n```",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Thanks for the report and the minimal reproducer. I confirm I can reproduce.\n\nThis is probably a bug in the convergence criterion in our vendored fork of liblinear, on very ill-conditioned problems.\n\nGiven that `intercept_scaling=1.0e+78` is quite extreme, I doubt it will ever impact users in nominal usage scenarios. So while I acknowledge that this is seemingly real bug, I don't plan to invest time debugging it myself.\n\nIf others want to investigate and find the root cause, feel free to open a PR.\n\n"
      },
      {
        "user": "jasneetsingh6114",
        "body": "Hey @ogrisel  @GloC99  can I work on this issue?"
      },
      {
        "user": "ogrisel",
        "body": "As i said above feel free to investigate the root cause. But it might be complex and it's low priority to me so don't expect help from my side on this."
      }
    ]
  },
  {
    "issue_number": 31235,
    "title": "MLP Classifier \"Logistic\" activation function providing ~constant prediction probabilities for all inputs when predicting quadratic function",
    "author": "KyleEMol",
    "state": "closed",
    "created_at": "2025-04-21T16:16:37Z",
    "updated_at": "2025-04-25T14:07:23Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nRepeatedly the sigmoid activation function produces very similar (multiple dp) outputs for the prediction probabilities, seemingly similar around the average of the predicted value, similar to a linear function. It works when predicting a linear function, but higher order tends to cause issues.\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.neural_network import MLPClassifier\nimport numpy as np\n\nnp.random.seed(1)\nData_X = (np.random.random((500,2)))\nData_Y = np.array([int((x[0] + ((2*(x[1]-0.5))**2  - 0.75))>=0) for x in Data_X])\n\nNN = MLPClassifier(hidden_layer_sizes = (20,20),activation = \"logistic\", random_state = 42)\nNN.fit(Data_X,Data_Y)\nprint(NN.predict(np.array(Data_X[:20])))\nprint(Data_Y[:20])\n```\n\n### Expected Results\n\nThe prediction does not resemble target data \n\n### Actual Results\n\n```\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n[0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0]\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\nexecutable: c:\\Users\\km\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n   machine: Windows-10-10.0.19045-SP0\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0.1\n   setuptools: None\n        numpy: 2.2.3\n        scipy: 1.15.2\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.1\n       joblib: 1.4.2\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: C:\\Users\\km\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy.libs\\libscipy_openblas64_-43e11ff0749b8cbe0a615c9cf6737e0e.dll\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: vcomp\n       filepath: C:\\Users\\km\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\n        version: None\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: C:\\Users\\km\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy.libs\\libscipy_openblas-f07f5a5d207a3a47104dca54d6d0c86a.dll\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n```",
    "comments": [
      {
        "user": "ogrisel",
        "body": "It's quite likely that the initialization scheme of the parameters implemented in scikit-learn is not optimal for the logistic activation function, especially when using a few hidden units.\n\nIf you increase the `hidden_layer_sizes`, the problem somewhat goes away:\n\n```python\n...\nNN = MLPClassifier(hidden_layer_sizes=(200, 200), activation=\"logistic\", random_state=42)\n...\n```\n```\n[0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0]\n[0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0]\n```\n\nand off-course if you use a more modern activation function such as `\"relu\"` then the model can fit the training set:\n\n```python\n...\nNN = MLPClassifier(hidden_layer_sizes=(200, 200), activation=\"relu\", random_state=42, max_iter=1000)\n...\n```\n```\n[0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0]\n[0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0]\n```\n\nI would not consider this a bug as it's known that the \"logistic\" activation function is harder to optimize for than `\"tanh\"` since the 90s. Modern alternatives, like the `\"relu\"` activation function used by default in scikit-learn, empirically lead to better fits. We mostly keep in scikit-learn for educational/historical purpose, as this was the original activation function used by NN pioneers in the 80s (or even before that). Maybe we could make that more explicit in the docstring and/or the user guide."
      }
    ]
  },
  {
    "issue_number": 31244,
    "title": "Add the baseline corrected accuracy score for (multi-class) classification to sklearn.metrics",
    "author": "Carl-McBride-Ellis",
    "state": "closed",
    "created_at": "2025-04-24T09:35:42Z",
    "updated_at": "2025-04-25T13:02:39Z",
    "labels": [
      "New Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nWould it be possible to add a new score to `sklearn.metrics`, namely the baseline corrected accuracy score (BCAS) ([DOI:10.5281/zenodo.15262049](https://doi.org/10.5281/zenodo.15262049)). The proposed metric quantifies the model improvement w.r.t. the baseline, and represents a direct evaluation of classifier performance. See the proposed code below, which is label agnostic, and is suitable for both binary and multi-class classification.\n\n### Describe your proposed solution\n\n```\nimport numpy as np\n\ndef BCAS(y_true, y_pred):\n    \"\"\"Baseline corrected accuracy score (BCAS).\n\n    Parameters\n    ----------\n    y_true : Ground truth (correct) labels.\n\n    y_pred : Predicted labels.\n\n    Returns\n    -------\n    score : float\n    \"\"\"\n    label, count = np.unique(y_true, return_counts=True)\n    most_frequent_class = label[np.argmax(count)]\n    y_baseline = np.full(len(y_true), most_frequent_class)\n    as_baseline = np.mean(y_true == y_baseline)\n    as_predicted = np.mean(y_true == y_pred)\n    return (as_predicted - as_baseline)\n```\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Thanks for your suggestion. However, new feature in scikit-learn are subject to the following inclusion criterion:\n\n- https://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms\n\nAs far as I can see, what you propose is not standard enough in the ML community to be included as such the project's code base."
      }
    ]
  },
  {
    "issue_number": 31218,
    "title": "Add P4 classification metric",
    "author": "Anderlaxe",
    "state": "closed",
    "created_at": "2025-04-17T09:02:01Z",
    "updated_at": "2025-04-25T08:44:42Z",
    "labels": [
      "New Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nHi, while working on a classification problem I found out there is no dedicated function to compute the P4 metric implemented in sklearn. As a reminder, P4 metrics is a binary classification metric that is commonly seen as an extension of the f_beta metrics because it takes into account all four True Positive, False Positive, True Negative and False Negative values, and because is it symmetrical unlike the f_beta metrics.\n\nP4 is defined as follows : P4 = 4 / ( 1/precision + 1/recall + 1/specificity + 1/NPV )\n\nWikipedia page right [here](https://en.wikipedia.org/wiki/P4-metric)\n\nMedium article right [there](https://medium.com/@thomas.vidori/better-than-the-f1-score-discover-the-p-4-score-903242e9545b)\n\n\n### Describe your proposed solution\n\nMy idea was to create a function `p4_support` similar to `precision_recall_fscore_support`. Since it is a binary metric, multiclass and multi-label inputs would be managed with `multilabel_confusion_matrix` so the arguments for `average` would be `'macro', 'samples', 'weighted', 'binary', None`.\nI would compute all necessaries values such as 1/precision, 1/recall, 1/specificity and 1/NPV using `_prf_divide`. If any of these four ratios are zero divisions, then P4 would also return the zero division argument. Indeed, for example if precision is null, then 1/precision is +inf and the whole denominator of the P4 is +inf which make P4 = 0 (Btw, this behavior is a reason why it is harder to achieve a high P4 score than f_score since all four ratios need to be 1 to have a P4 equals to 1.). The function would return the tuple (p4_value, support)\n\nA second function `p4_score` which would be the one actually used by users would return only the first element of the previously described `p4_support` function.\n\n### Describe alternatives you've considered, if relevant\n\nExtras : \n\nSince specificity and NVP are computed anyway, the `p4_support` function could return the tuple (specificity, NVP, p4_score, support) and then be called `specificity_nvp_p4_support`. It would then also be possible to add `specificity` and `NVP` functions as well using the same scheme as precision or recall. \n\nResponding to #21000 issue, P4 could be added in the `classification_report` function and would be a good summary of all TP, FP, TN, FN values and their combinations. \n\n### Additional context\n\nI have checked that this feature is not already in the issues or pull requests.",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Thanks for the proposal. I took a look at the [paper](https://www.oajaiml.com/uploads/archivepdf/58451161.pdf), and it is quite recent (published in 2023) and cited only 31 times according to google scholar. As such, this does not meet our [inclusion criteria for scikit-learn](https://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms).\n\nPersonal opinion:\n\nWe already have MCC and the P4 metric does seem to be very similar in the sense that both metrics penalize models that have at least one bad entry in their confusion matrix. In that respect, F4 seems a bit redundant.\n\nMore importantly, I don't think generic metrics computed on thresholded (hard) predictions (MCC, F4, F1, balanced accuracy...) are the best way to choose a binary classifier for a given problem. Instead, I would select the best classifier based on threshold-independent binary classification metrics, either purely discriminative metrics, such as ROC AUC or Average Precision (area under the PR curve), or calibration-aware metrics, such as log-loss and Brier score, and afterward, find the optimal decision threshold based on application-specific constraints:\n\n- find the threshold that maximizes precision for a given application-specific recall budget,\n- find the threshold that maximizes recall for a given application-specific precision budget,\n- assign application specific costs (or gains) to the 4 entries of the confusion matrix and derive an application specific business metric.\n\nAll of those can be implemented with the help of the `TunedThresholdClassifierCV` tool as documented in the following example:\n\n- https://scikit-learn.org/stable/auto_examples/model_selection/plot_cost_sensitive_learning.html"
      },
      {
        "user": "ogrisel",
        "body": "I propose to close this feature request as \"not planned\" for now. If people disagree with what I wrote above, please feel free to upvote this issue and comment below to extend the analysis and we can consider reopening once the inclusion criteria are met."
      }
    ]
  },
  {
    "issue_number": 31217,
    "title": "âš ï¸ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Apr 18, 2025) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2025-04-17T02:34:42Z",
    "updated_at": "2025-04-24T16:46:22Z",
    "labels": [],
    "body": "**CI is still failing on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=75830&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a)** (Apr 18, 2025)\n- Test Collection Failure",
    "comments": [
      {
        "user": "lesteve",
        "body": "Oh well it looks like the intermittent Figshare issue #30761 happens two days in a row ..."
      },
      {
        "user": "scikit-learn-bot",
        "body": "## CI is no longer failing! âœ…\n\n[Successful run](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=75935&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a) on Apr 24, 2025"
      }
    ]
  },
  {
    "issue_number": 31245,
    "title": "GradientBoostingClassifier does not have out-of-bag (OOB) score",
    "author": "longyuxi",
    "state": "closed",
    "created_at": "2025-04-24T15:13:52Z",
    "updated_at": "2025-04-24T15:43:40Z",
    "labels": [
      "Bug",
      "Needs Triage"
    ],
    "body": "### Describe the bug\n\nHi, the [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) for Gradient boosting Classifier says that there is an out-of-bag score that can be retrieved by the `oob_score_` attribute. However, this attribute doesn't seem to exist in the latest version.\n\n\n\n### Steps/Code to Reproduce\n\nCopy-and-paste code to reproduce this:\n\n```python\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\n\nXs = np.random.randn(100, 10)\nys = np.random.randint(0, 2, 100)\n\ngbc = GradientBoostingClassifier()\ngbc.fit(Xs, ys)\ngbc.oob_score_\n```\n\n### Expected Results\n\nNo error is thrown. OOB score should be a float\n\n### Actual Results\n\n```\n$ conda create -n sklearn-env -c conda-forge scikit-learn\n$ conda activate sklearn-env\n(sklearn-env) $ python\nPython 3.13.3 | packaged by conda-forge | (main, Apr 14 2025, 20:44:30) [Clang 18.1.8 ] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import sklearn; sklearn.__version__\n'1.6.1'\n>>> from sklearn.ensemble import GradientBoostingClassifier\n... import numpy as np\n...\n... Xs = np.random.randn(100, 10)\n... ys = np.random.randint(0, 2, 100)\n...\n... gbc = GradientBoostingClassifier()\n... gbc.fit(Xs, ys)\n... gbc.oob_score_\nTraceback (most recent call last):\n  File \"<python-input-0>\", line 9, in <module>\n    gbc.oob_score_\nAttributeError: 'GradientBoostingClassifier' object has no attribute 'oob_score_'\n```\n\n### Versions\n\n```shell\n>>> import sklearn; sklearn.show_versions()\n\nSystem:\n    python: 3.13.3 | packaged by conda-forge | (main, Apr 14 2025, 20:44:30) [Clang 18.1.8 ]\nexecutable: /Users/longyuxi/miniforge3/envs/sklearn-env/bin/python\n   machine: macOS-15.3.2-arm64-arm-64bit-Mach-O\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0.1\n   setuptools: 79.0.1\n        numpy: 2.2.5\n        scipy: 1.15.2\n       Cython: None\n       pandas: None\n   matplotlib: None\n       joblib: 1.4.2\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 10\n         prefix: libopenblas\n       filepath: /Users/longyuxi/miniforge3/envs/sklearn-env/lib/libopenblas.0.dylib\n        version: 0.3.29\nthreading_layer: openmp\n   architecture: VORTEX\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 10\n         prefix: libomp\n       filepath: /Users/longyuxi/miniforge3/envs/sklearn-env/lib/libomp.dylib\n        version: None\n```",
    "comments": [
      {
        "user": "longyuxi",
        "body": "I am so confused. I tried the following configurations on my M4 mac:\n\n1. sklearn 1.6 installed with conda (above)\n2. sklearn 1.3 installed with conda\n3. sklearn 1.6 installed with pip\n\nand all of them throw the same attribute not found error as above, even though this feature was supposed to be added in 1.3.\n\nI also tried this on an intel-based machine and got the same error."
      },
      {
        "user": "longyuxi",
        "body": "Okay, I found out that I need to specify the parameter for `subsample` to be something less than 1 to get the score."
      }
    ]
  },
  {
    "issue_number": 31206,
    "title": "Different Python version causes a different distribution of classification result",
    "author": "GloC99",
    "state": "closed",
    "created_at": "2025-04-15T11:04:29Z",
    "updated_at": "2025-04-24T14:05:51Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nRunning the same code using Python 3.10 and Python 3.13 with `n_jobs > 1` had a variety of result. Python 3.10 and Python 3.13 also has different distributions.\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nimport random\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n\n# Control the randomness\nrandom.seed(0)  \nnp.random.seed(0)\n\niris = load_iris()  \nx, y = iris.data, iris.target\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n\n# Define and create a model\nmodel = RandomForestClassifier(\n    n_estimators=np.int64(101),\n    criterion='gini',\n    max_depth=np.int64(31),\n    min_samples_split=7.291122019556396e-304,\n    min_samples_leaf=np.int64(14876671),\n    min_weight_fraction_leaf=0.0,\n    max_features=None,\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs= np.int64(255),\n    random_state=0,\n    verbose=np.int64(0),\n    warm_start=False,\n    class_weight='balanced_subsample',\n    ccp_alpha=0.0,\n    max_samples=None)\n\nmodel.fit(x_train, y_train)\n\n# Evaluate model\ny_pred = model.predict(x_test)\nprint(\"Accuracy: \", accuracy_score(y_test,\n                                    y_pred))\nprint(\"Recall:\",\n    recall_score(y_test, y_pred, average='micro'))\n# Print confusion matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n```\n\n### Expected Results\n\nIf `n_jobs` is 1, the result is:\n```\n    Accuracy:  0.43333333333333335\n    Recall: 0.43333333333333335\n    Confusion Matrix:\n    [[ 0 11  0]\n    [ 0 13  0]\n    [ 0  6  0]]\n```\n\n### Actual Results\n\nWhen the program is run 10,000 times:\n**n_jobs=255, Python 3.10** has two possible results:\n```\n    Group:\n    Accuracy:  0.43333333333333335\n    Recall: 0.43333333333333335\n    Confusion Matrix:\n    [[ 0 11  0]\n    [ 0 13  0]\n    [ 0  6  0]]\n    Count: 9887\n    \n    Group:\n    Accuracy:  0.36666666666666664\n    Recall: 0.36666666666666664\n    Confusion Matrix:\n    [[11  0  0]\n    [13  0  0]\n    [ 6  0  0]]\n    Count: 113\n```\n**n_jobs=255, Python 3.13** has three possible results:\n```\n    Group:\n    Accuracy:  0.36666666666666664\n    Recall: 0.36666666666666664\n    Confusion Matrix:\n    [[11  0  0]\n    [13  0  0]\n    [ 6  0  0]]\n    Count: 7790\n    \n    Group:\n    Accuracy:  0.43333333333333335\n    Recall: 0.43333333333333335\n    Confusion Matrix:\n    [[ 0 11  0]\n    [ 0 13  0]\n    [ 0  6  0]]\n    Count: 1965\n    \n    Group:\n    Accuracy:  0.2\n    Recall: 0.2\n    Confusion Matrix:\n    [[ 0  0 11]\n    [ 0  0 13]\n    [ 0  0  6]]\n    Count: 245\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.13.2 (main, Mar 27 2025, 14:05:19) [GCC 11.4.0]\nexecutable: /opt/python/3.13.2/bin/python3.13\n   machine: Linux-5.15.0-122-generic-x86_64-with-glibc2.35\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 24.3.1\n   setuptools: 75.6.0\n        numpy: 2.2.4\n        scipy: 1.15.2\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: None\n       joblib: 1.4.2\nthreadpoolctl: 3.6.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: /users/GloC99/.local/lib/python3.13/site-packages/numpy.libs/libscipy_openblas64_-6bb31eeb.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: /users/GloC99/.local/lib/python3.13/site-packages/scipy.libs/libscipy_openblas-68440149.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 16\n         prefix: libgomp\n       filepath: /users/GloC99/.local/lib/python3.13/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n        version: None\n\n===============\n\nSystem:\n    python: 3.10.17 (main, Apr 10 2025, 12:04:30) [GCC 11.4.0]\nexecutable: /opt/python/3.10.17/bin/python3.10\n   machine: Linux-5.15.0-122-generic-x86_64-with-glibc2.35\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 24.3.1\n   setuptools: 75.6.0\n        numpy: 2.2.0\n        scipy: 1.14.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: None\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: /users/GloC99/.local/lib/python3.10/site-packages/numpy.libs/libscipy_openblas64_-6bb31eeb.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 16\n         prefix: libscipy_openblas\n       filepath: /users/GloC99/.local/lib/python3.10/site-packages/scipy.libs/libscipy_openblas-c128ec02.so\n        version: 0.3.27.dev\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 16\n         prefix: libgomp\n       filepath: /users/GloC99/.local/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\n        version: None\n```",
    "comments": [
      {
        "user": "GAVARA-PRABHAS-RAM",
        "body": "Hi, I'd like to work on this issue.\nCan I go ahead and open a PR?"
      },
      {
        "user": "GloC99",
        "body": "> Hi, I'd like to work on this issue. Can I go ahead and open a PR?\n\nIf you need any more information, please let me know. If you need, I also have a script that will run the program many time and counts the repetition of different group."
      },
      {
        "user": "GAVARA-PRABHAS-RAM",
        "body": "i didnt understand what exactly i have to do??\n"
      }
    ]
  },
  {
    "issue_number": 9553,
    "title": "Use median instead of mean when constructing RandomForestRegressor",
    "author": "mcgibbon",
    "state": "closed",
    "created_at": "2017-08-14T19:34:35Z",
    "updated_at": "2025-04-24T12:26:08Z",
    "labels": [
      "module:ensemble"
    ],
    "body": "<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: http://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\nI am using a RandomForestRegressor for a task where it seems I would benefit from having the leaf node values defined by the median over its samples instead of the mean over its samples, due to the presence of outliers. This functionality currently isn't present in scikit-learn, and I would like to add it (either as a PR, or just locally on my machine).\r\n\r\nIf I'm going to do this, I could a pointer as to how to do it. Specifically I am having trouble finding the line of code where sklearn currently takes the mean over values in a leaf. I'm aware that it's somewhere within the code for a decision tree. If I found that, I can probably handle the rest.\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nhelp(RandomForestRegressor)\r\n```\r\n<!--\r\nExample:\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation\r\n\r\ndocs = [\"Help I have a bug\" for i in range(1000)]\r\n\r\nvectorizer = CountVectorizer(input=docs, analyzer='word')\r\nlda_features = vectorizer.fit_transform(docs)\r\n\r\nlda_model = LatentDirichletAllocation(\r\n    n_topics=10,\r\n    learning_method='online',\r\n    evaluate_every=10,\r\n    n_jobs=4,\r\n)\r\nmodel = lda_model.fit(lda_features)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n-->\r\n\r\n#### Desired API\r\n\r\nI could just hack something in, if I'm doing it only for myself locally. If I'm going to write this as a change to the RFR API, then maybe something like:\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nimport numpy as np\r\nrandom_forest = RandomForestRegressor(reduction_op=np.median)  # default np.mean\r\n```\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\nLinux-3.10.0-514.10.2.el7.x86_64-x86_64-with-centos-7.3.1611-Core\r\n('Python', '2.7.13 | packaged by conda-forge | (default, May  2 2017, 12:48:11) \\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]')\r\n('NumPy', '1.12.1')\r\n('SciPy', '0.19.0')\r\n('Scikit-Learn', '0.18.1')\r\n\r\n<!-- Thanks for contributing! -->\r\n",
    "comments": [
      {
        "user": "mcgibbon",
        "body": "I hadn't thought about this, but I suppose I would also want to use the median when aggregating values from different trees."
      },
      {
        "user": "amueller",
        "body": "Would you use the median across all leaves in all the trees or the median per tree and then the median of the medians? Do you have any literature or experiments on this? It makes intuitive sense, but it's not obvious that it'll actually help in practice."
      },
      {
        "user": "mcgibbon",
        "body": "When constructing the leaf I'd take the median among the samples given to that leaf, and when getting the value for the forest I'd take the median of the medians. The (non-peer reviewed?) paper I found using this is [here](http://web.sys.virginia.edu/files/tech_papers/2006/sie06_0004.pdf). For my particular problem zero is a special value that occurs often (I'm predicting statistical moments, and when variance is zero other outputs are also zero), and on top of that some of the variables I output are quite skewed with fat-tailed PDFs. It seems like this might help for my particular application. I'll leave it to you to think about whether it makes sense to put in sklearn."
      }
    ]
  },
  {
    "issue_number": 31077,
    "title": "Partial dependence broken when categorical_features has an empty list",
    "author": "Atm-Moumen",
    "state": "closed",
    "created_at": "2025-03-26T10:12:18Z",
    "updated_at": "2025-04-23T17:25:04Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nWhen we pass an empty list to **categorical_features**, **partial_dependence** will raise an error ValueError: Expected **categorical_features** to be an array-like of boolean, integer, or string. Got float64 instead.\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn import datasets\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.inspection import partial_dependence\n\niris, Species = datasets.load_iris(return_X_y=True)\niris = pd.DataFrame(\niris,\ncolumns=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n)\niris[\"species\"] = pd.Series(Species).map({0: \"A\", 1: \"B\", 2: \"C\"})\niris.head()\n\nspecies_encoder = make_pipeline(\nSimpleImputer(strategy=\"constant\", fill_value=\"A\"),\nOneHotEncoder(drop=[\"A\"], sparse_output=False)\n)\n\npreprocessor = ColumnTransformer(\ntransformers=[\n(\"species_encoder\", species_encoder, [\"species\"]),\n(\"other\", SimpleImputer(), [\"sepal_width\", \"petal_width\", \"petal_length\"])\n],\nverbose_feature_names_out=False\n).set_output(transform=\"pandas\")\n\nmodel = make_pipeline(preprocessor, LinearRegression())\n\nmodel.fit(iris, iris.sepal_length)\n\npd = partial_dependence(estimator=model, X= iris, features= [\"sepal_length\"], categorical_features= [])\n```\n\n### Expected Results\n\n.\n\n### Actual Results\n\n```pytb\nValueError Traceback (most recent call last)\nCell In[12], line 28\n24 model = make_pipeline(preprocessor, LinearRegression())\n26 model.fit(iris, iris.sepal_length)\n---> 28 pd = partial_dependence(estimator=model, X= iris, features= [\"sepal_length\"], categorical_features= [])\n\nFile ~.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213, in validate_params..decorator..wrapper(*args, **kwargs)\n207 try:\n208 with config_context(\n209 skip_parameter_validation=(\n210 prefer_skip_nested_validation or global_skip_validation\n211 )\n212 ):\n--> 213 return func(*args, **kwargs)\n214 except InvalidParameterError as e:\n215 # When the function is just a wrapper around an estimator, we allow\n216 # the function to delegate validation to the estimator, but we replace\n217 # the name of the estimator by the name of the function in the error\n218 # message to avoid confusion.\n219 msg = re.sub(\n220 r\"parameter of \\w+ must be\",\n221 f\"parameter of {func.qualname} must be\",\n222 str(e),\n223 )\n\nFile ~.venv/lib/python3.10/site-packages/sklearn/inspection/_partial_dependence.py:679, in partial_dependence(estimator, X, features, sample_weight, categorical_features, feature_names, response_method, percentiles, grid_resolution, method, kind)\n675 is_categorical = [\n676 idx in categorical_features_idx for idx in features_indices\n677 ]\n678 else:\n--> 679 raise ValueError(\n680 \"Expected categorical_features to be an array-like of boolean,\"\n681 f\" integer, or string. Got {categorical_features.dtype} instead.\"\n682 )\n684 grid, values = _grid_from_X(\n685 _safe_indexing(X, features_indices, axis=1),\n686 percentiles,\n687 is_categorical,\n688 grid_resolution,\n689 )\n691 if method == \"brute\":\n\nValueError: Expected categorical_features to be an array-like of boolean, integer, or string. Got float64 instead.\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.16 (main, Dec  3 2024, 17:27:57) [Clang 16.0.0 (clang-1600.0.26.4)]\nexecutable: /Users/datategy/papAI/o2_ml/.venv/bin/python\n   machine: macOS-15.3-x86_64-i386-64bit\n\nPython dependencies:\n      sklearn: 1.5.0\n          pip: 25.0.1\n   setuptools: 75.6.0\n        numpy: 1.26.4\n        scipy: 1.13.1\n       Cython: 3.0.12\n       pandas: 1.5.3\n   matplotlib: 3.8.4\n       joblib: 1.2.0\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 4\n         prefix: libopenblas\n...\n    num_threads: 8\n         prefix: libomp\n       filepath: /usr/local/Cellar/libomp/19.1.7/lib/libomp.dylib\n        version: None\n```",
    "comments": [
      {
        "user": "glemaitre",
        "body": "We should not accept an empty list indeed and raise a better error message. `None` would be the right input for such use case."
      },
      {
        "user": "MarcBresson",
        "body": "Any reason why empty lists should not be accepted? I feel like giving an empty list `[]` should lead the same behaviour as giving `None`.\n\nWhat do you think?"
      },
      {
        "user": "jeremiedbb",
        "body": "I agree that if empty lists were accepted, the behavior should be the same as None. However I don't think that we should accept them. They weren't accepted before, the only thing that changed is that now we raise a more informative error message. So unless it represents a big convenience feature for the users, in which case we should reconsider, let's not introduce a new feature (small one but still) because it increases the maintenance burden."
      }
    ]
  },
  {
    "issue_number": 17782,
    "title": "RFECV to provide the average ranking_ and support_",
    "author": "apptimise",
    "state": "closed",
    "created_at": "2020-06-29T23:24:26Z",
    "updated_at": "2025-04-23T13:22:59Z",
    "labels": [
      "New Feature"
    ],
    "body": "<!--\r\nIf you want to propose a new algorithm, please refer first to the scikit-learn\r\ninclusion criterion:\r\nhttps://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms\r\n-->\r\n\r\n#### Describe the workflow you want to enable\r\nCurrently, `RFECV` searches for the optimal number of features to provide the best score. It then fits an `RFE` on the whole training set: [L566](https://github.com/scikit-learn/scikit-learn/blob/fd237278e/sklearn/feature_selection/_rfe.py#L566)\r\n\r\nSo, the score is the cross-validated score, but the `ranking_` is for the whole set of data. Isn't this misleading? e.g. with `RFE` and 3-fold cross-validation, I get these `ranking_`s:\r\n```\r\n[1 1 4 1 1 2 3 5 7 6]\r\n[1 1 2 1 1 7 4 3 5 6]\r\n[1 1 2 1 1 7 5 6 3 4]\r\n```\r\nwhile `RFECV` returns something like:\r\n```\r\n[1 1 3 1 1 5 2 4 7 6]\r\n```\r\nwhich is the `ranking_` `RFE` would provide if fit over the whole set. So, the score is a cross-validated score, while the `ranking_` and `support_` are not.\r\n\r\n#### Describe your proposed solution\r\nImagine a situation where you get the best performance with `k` features, but the `k` features in some folds are different from the `k` features you would get by fitting the model on the whole training set. Wouldn't it be a better idea to return a form of average/voting_based `ranking_` and `support_`? ",
    "comments": [
      {
        "user": "jnothman",
        "body": "I understand that the RFECV uses CV only to determine the best k, not the\nset of features. The attributes returned are therefore correct, and\nreporting averages would be inappropriate when features have interactions\n(e.g. collinearity). Providing the rankings produced by each split's RFE\nwould provide additional information about the stability of the model, but\nnot a better model.\n"
      },
      {
        "user": "apptimise",
        "body": "Thanks. \r\n\r\n> I understand that the RFECV uses CV only to determine the best k, not the set of features.\r\n\r\nThat's why I think providing the selected set of features or their rankings using the whole set can be confusing (because that's not what the algorithm does/is intended for)\r\n\r\nAlso, let's consider a voting-based approach. Why would it be wrong (even if collinearity exists)? consider 3 folds with the following `ranking_`s:\r\n```\r\n[1 2 3 4]\r\n[2 1 4 3]\r\n[1 3 2 4]\r\n```\r\nThe sum is: `[4 6 9 11]` therefore, the true `ranking_` can be considered as `[1 2 3 4]`\r\nNow suppose that you fit on the whole training set and get something like `[2 1 4 3]` (similar to fold-2). All the ranks will be different from what can be considered as an \"average\"/voting-based ranking."
      },
      {
        "user": "jnothman",
        "body": "what if the first two features are identical, but for random noise. They are also the most important feature. Because the two are redundant, my splits might rank:\r\n```\r\n[1, 4, 2, 3]\r\n[4, 1, 2, 3]\r\n[1, 4, 2, 3]\r\n[4, 1, 2, 3]\r\n```\r\n\r\nSum these...\r\n```\r\n[10, 10, 8, 12]\r\n```\r\nNot really useful.\r\n\r\nRFE is very explicitly an alternative to univariate selection, that will take this conditional dependency into account.\r\n\r\nAnd grid search, which is what's happening, usually works by learning hyperparameters under cv, but final model parameters with the full training set."
      }
    ]
  },
  {
    "issue_number": 15132,
    "title": "Support feature importance in HistGradientBoostingClassifier/Regressor",
    "author": "qinhanmin2014",
    "state": "open",
    "created_at": "2019-10-04T13:17:07Z",
    "updated_at": "2025-04-23T12:54:25Z",
    "labels": [
      "Needs Decision",
      "module:ensemble",
      "module:inspection"
    ],
    "body": "Maybe we should support feature importance in HistGradientBoostingClassifier/Regressor?",
    "comments": [
      {
        "user": "PyExtreme",
        "body": "Hi @qinhanmin2014 , This seems to be a nice idea. I would like to work on this.\r\nIt would be great if you could please send me some references for the task.\r\n\r\nMoreover, On seeing the code _of the predict function_ here at : https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py#L644\r\n, the predictions are not parallelized and the task of parallelizing is mentioned as a **_todo task_**. I would also like to work on it, maybe on a separate issue.\r\nHere is the piece of code for your reference.\r\n\r\n`def predict(self, X):`\r\n      `  \"\"\"Predict classes for X.`\r\n        `Parameters`\r\n     `   ----------`\r\n     `   X : array-like, shape (n_samples, n_features)`\r\n       `     The input samples.`\r\n       ` Returns`\r\n        `-------`\r\n       ` y : ndarray, shape (n_samples,)`\r\n            `The predicted classes.`\r\n      `  \"\"\"`\r\n   `     # TODO: This could be done in parallel`\r\n       ` encoded_classes = np.argmax(self.predict_proba(X), axis=1)`\r\n       ` return self.classes_[encoded_classes]`\r\n\r\nWould love to hear your inputs on this one.\r\n\r\nThanks"
      },
      {
        "user": "Gitman-code",
        "body": "It would likely be better to model this off of XGBoost than GradientBoostingRegressor .\r\n\r\nThe equivalent of the GradientBoostingRegressor case\r\n```\r\nfeature_importance = regressor.feature_importances_\r\n```\r\n\r\nis\r\n```\r\nfeat_imp_dict = regressor.get_booster().get_score(importance_type='gain')\r\nfeature_importance = np.asarray([feat_imp_dict.get(i, 0) for i in self.features])\r\n```\r\nThe usefulness added here is that there are several different importance_type options ['weight', 'gain', 'cover', 'total_gain', 'total_cover']. Just like how you may want to use different evaluation metrics in the permutation importance you may want to calculate the importance from the tree in different ways. Here is some details https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7\r\n\r\n"
      },
      {
        "user": "ogrisel",
        "body": "Feature importances derived from training time impurity values on nodes suffer from the cardinality biais issue and cannot reflect which features are important to generalize to make correct predictions on a validation set.\r\n\r\nSince 0.22 it's now possible to compute feature importances using the permutation method and it does not suffer those specific limitations: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py\r\n\r\nSo instead of implementing a method (impurity based feature importances) that has really misleading I would rather point our users to use permutation based feature importances that are model agnostic or use SHAP (once it supports the histogram-based GBRT models, see https://github.com/slundberg/shap/issues/1028)"
      }
    ]
  },
  {
    "issue_number": 30467,
    "title": "API Deprecate n_alphas in LinearModelCV",
    "author": "jeremiedbb",
    "state": "closed",
    "created_at": "2024-12-11T16:33:41Z",
    "updated_at": "2025-04-23T12:50:13Z",
    "labels": [
      "API",
      "RFC"
    ],
    "body": "In LassoCV, ElasticNetCV, ... we have two parameters, `alphas` and `n_alphas`, that have the same purpose, i.e. determine the alpha values to test.\r\n\r\nI'd be in favor of deprecating `n_alphas` and make `alphas` accept either an int or an array-like, filling both roles.\r\n\r\nI chose to keep `alphas` and not the other because `RidgeCV` has `alphas` and no `n_alphas` (although `alphas` can't be an int there, maybe an enhancement to make ?), and the most recent param of this kind, `threshold` in `TunedThresholdClassifierCV`, follows this naming pattern and fills both roles.",
    "comments": [
      {
        "user": "KANNAHWORLD",
        "body": "Hi, can I take a shot at refactoring?"
      },
      {
        "user": "jeremiedbb",
        "body": "sure ! You can take a look at the [contributing guide](https://scikit-learn.org/dev/developers/contributing.html) and especially the [deprecation section](https://scikit-learn.org/dev/developers/contributing.html#deprecation) to get started."
      },
      {
        "user": "KANNAHWORLD",
        "body": "@jeremiedbb Just created a PR, I believe it is linked. I followed the contributing guidelines to the best of my abilities, but happy to make any changes!"
      }
    ]
  },
  {
    "issue_number": 31158,
    "title": "Nearest neighbors Gaussian Process",
    "author": "stavoltafunzia",
    "state": "open",
    "created_at": "2025-04-07T10:12:34Z",
    "updated_at": "2025-04-22T15:52:58Z",
    "labels": [
      "New Feature",
      "Needs Decision - Include Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\nRecently I've been working on a Nearest Neighbor Gaussian Process Regressor as described in Datta 2016 [here](https://arxiv.org/abs/1406.7343). This kind of model exists in R, but not in scikit-learn. Nearest Neighbor Gaussian Process Regressor is a simple enhancement over standard GP that allows to use GP on large datasets. It also recently gained interest among the GPytorch package, see e.g. [here](https://arxiv.org/abs/2202.01694).\n\n\n\n### Describe your proposed solution\n\nI already have a scikit-learn-like implementation that I could bring to this project. This implementation becomes more convenient (uses less memory and less runtime) than classic Gaussian Process Regressor from a dataset size of approx 10k. It is based on Datta's work, so it's not as the one in the GPytorch package. If anyone deems this model interesting enough, I'm wiling to make a PR.\n\nHaving a baseline CPU-base implementation in scikit-learn could also server as a starting point for future GPU-based implementations, which is were this model really shines (e.g. inheriting from scikit-learn class and implementing in GPU the most time consuming operations). As an example, I also have a cupy-based implementation of Datta's NNGP which competes very well against GPytorch VNNGP.\n\n### Describe alternatives you've considered, if relevant\n\nAs mentioned above, a version of NNGP is implemented in GPytorch. GPytorch implementation however is not only based on Nearest Neighbors, but also on Variational method. The one from Datta's is simpler being only based on NN and can become competitive with more complex methods VNNGP when using GPUs.\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "ogrisel",
        "body": "According to google scholar, the paper has 775 citations. However I am not familiar enough with the GP literature to properly assess the usefulness/maintenance tradeoff.\n\nIf you already have a working implementation, could you please publish it or link to it (e.g. as a personal repo on github) to get an idea on how complex is the code?\n\n> This implementation becomes more convenient (uses less memory and less runtime) than classic Gaussian Process Regressor from a dataset size of approx 10k.\n\nHave you tried to run benchmarks? On which kinds of datasets / tasks was this most useful to you?\n\nIn particular, do you use it on highdimensional or small dimensional data. I have the feeling that using ball-tree or kd-tree indices might make this work fast in low dimensional data (<~ 10 dimensions).\n\n> As an example, I also have a cupy-based implementation of Datta's NNGP which competes very well against GPytorch VNNGP.\n\nSame remark: I would love to see code and numbers to back this claim."
      },
      {
        "user": "stavoltafunzia",
        "body": "> If you already have a working implementation, could you please publish it or link to it (e.g. as a personal repo on github) to get an idea on how complex is the code?\n\nSure, give me a couple of days to clean it up a bit, add readme, unittest, then I will publish it.\n\n> Have you tried to run benchmarks? On which kinds of datasets / tasks was this most useful to you?\nIn particular, do you use it on highdimensional or small dimensional data. I have the feeling that using ball-tree or kd-tree indices might make this work fast in low dimensional data (<~ 10 dimensions).\n\nRight observation, I also expect the dimensionality of the problem to affect the speed. My statement (10k observations) was based on the 3D-road dataset, which indeed is low dimensional. I'm now checking on something more high-dimensional, like the elevators dataset. Linking to the question above, I can include few benchmarks in the code I will publish.\n\n>As an example, I also have a cupy-based implementation of Datta's NNGP which competes very well against GPytorch VNNGP.\n\nIf relevant, I can publish some code of what above as well. Though, being cupy-based, I thought it was not much relevant in the context of scikit-learn."
      },
      {
        "user": "stavoltafunzia",
        "body": "As requested, I published the model in [this project](https://github.com/stavoltafunzia/nngpr). I have also added some very basic benchmarks.\nIn the coming few days I will also add the cuda-based implementation."
      }
    ]
  },
  {
    "issue_number": 28671,
    "title": "D2_pinball_score",
    "author": "Manjubn777",
    "state": "open",
    "created_at": "2024-03-20T18:57:23Z",
    "updated_at": "2025-04-22T11:59:57Z",
    "labels": [
      "Documentation"
    ],
    "body": "Hello team, Iâ€™m currently utilizing RandomizedSearchCV . Specifically, Iâ€™m working on probabilistic forecasting. However, when I use D2_pinball_score as the scoring metric, I encounter an error despite using the supported version. Any insights or guidance would be greatly appreciated.\r\n\r\n![image](https://github.com/scikit-learn/scikit-learn/assets/70128255/08f74449-85cc-487d-9833-70c1dcc00a06)\r\n",
    "comments": [
      {
        "user": "ogrisel",
        "body": "`\"d2_pinball_score\"` is not a valid scorer name. You should instead pass a custom scorer object that wraps the `d2_pinball_score` callable with the value of `alpha` you are interested in:\r\n\r\n```python\r\nfrom sklearn.metrics import make_scorer\r\nfrom sklearn.metrics import d2_pinball_score\r\n\r\npinball_95_scorer = make_scorer(d2_pinball_score, alpha=0.95) \r\n```\r\n\r\nThen you should be able to pass `scoring=pinball_95_scorer` to the `RandomizedSearchCV` constructor.\r\n\r\nMore details:\r\n\r\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\r\n\r\nI agree that both the documentation for the scoring parameter, the `make_scorer` utility, the `d2_pinball_score` metric and the error message could be improved because this is not easy to discover the correct usage by yourself."
      },
      {
        "user": "siddu1324",
        "body": "I'm working on `scikit-learn/sklearn/model_selection/_search.py`\r\n"
      },
      {
        "user": "MaddyRizvi",
        "body": "/take"
      }
    ]
  },
  {
    "issue_number": 28952,
    "title": "Add missing values and categorical features when generating datasets",
    "author": "lcrmorin",
    "state": "open",
    "created_at": "2024-05-05T08:07:08Z",
    "updated_at": "2025-04-18T20:06:19Z",
    "labels": [
      "New Feature",
      "Moderate"
    ],
    "body": "### Describe the workflow you want to enable\n\nI am often using random datasets (typically with make_classification). However I often find myself having to add more realistic features to the dataset:\r\n- missing data, sometime just to test the pipeline (missing at random would be fine), or sometimes to look for more complex phenomenons (missingnes not at random, possibly depending on the target)\r\n- categorical: categoricals variables often need to be handled specifically. I usually introduce categoricals with binning a continuous value, then transforming to strings. \r\nIt would be nice to have both of those in datasets generation. \n\n### Describe your proposed solution\n\nIntroduce parameters to allow for generation of missing data (proportion of missingness, type of missingness - at random, not at random).\r\nIntroduce parameters to allow for generation of categorical features (number of features, type of repartition in categories - even - uneven - pareto.  \n\n### Describe alternatives you've considered, if relevant\n\nI usually handle this by hand.\n\n### Additional context\n\nCould be used to illustrate imputing techniques, encoding techniques. ",
    "comments": [
      {
        "user": "oasidorshin",
        "body": "@lcrmorin This would be great for testing! I would also suggest adding infinities as possible values, bcs they also break stuff quite often. Also, if randomly generated, making sure to always include at least one NaN and inf value"
      },
      {
        "user": "AK3847",
        "body": "@lcrmorin I suggest adding a noise function or something similar which can generate structured randomness so as to make some sense in data and not pseudo-randomness. Perhaps something like Perlin Noise? "
      },
      {
        "user": "glemaitre",
        "body": "Regarding the missing values I recall the following issues/PRs: #6284 / #7084. It seems that the consensus was to have something similar to the `ampute` R package.\r\n\r\nI almost a similar discussion for categorical features but I could not find. For sure, it would be handy to have those two parameters even though we could limit the complexity (e.g. only have a single missingness pattern)"
      }
    ]
  },
  {
    "issue_number": 5976,
    "title": "KDTree's query & query_radius methods have inconsistent return values",
    "author": "rs86",
    "state": "closed",
    "created_at": "2015-12-07T20:50:07Z",
    "updated_at": "2025-04-18T19:03:43Z",
    "labels": [
      "Bug",
      "help wanted",
      "module:neighbors"
    ],
    "body": "Hi,\n\nIn sklearn.neighbors.KDTree:\n\nquery_radius returns a (indices, distance) tuple while query returns (distance, indices); this might unfortunately be a hard one to fix because it breaks existing code.\n\nDocs: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree\n\n(BallTree might suffer from the same issue.)\n",
    "comments": [
      {
        "user": "jakevdp",
        "body": "Thanks â€“ this has been brought up before, but I can't find the relevant issue right now.\n"
      },
      {
        "user": "thomasjpfan",
        "body": "@jjerphan Do you think this is worth fixing? (We would need to add a parameter and go through a deprecation cycle.)"
      },
      {
        "user": "jjerphan",
        "body": "I agree that the difference in APIs is counter-intuitive but I don't think that this is worth fixing."
      }
    ]
  },
  {
    "issue_number": 23243,
    "title": "Improve SVR vs KRR example further",
    "author": "lesteve",
    "state": "closed",
    "created_at": "2022-04-29T12:17:16Z",
    "updated_at": "2025-04-18T13:00:52Z",
    "labels": [
      "Documentation"
    ],
    "body": "Follow up of https://github.com/scikit-learn/scikit-learn/pull/22804, example code is https://github.com/scikit-learn/scikit-learn/blob/main/examples/miscellaneous/plot_kernel_ridge_regression.py\r\n\r\n- once the grid-search is done use `svr.best_estimator_` and `kr.best_estimator_` as mentioned in https://github.com/scikit-learn/scikit-learn/pull/22804#discussion_r825469629. Maybe using the same gamma and an \"equivalent\" `C = 1 / alpha` was done on purpose to be able to compare both models in a more fair fashion, not entirely sure.\r\n- Add train scores to learning curve and maybe use 1000 samples rather than 100 samples: https://github.com/scikit-learn/scikit-learn/pull/22804#issuecomment-1066134467. When doing that the learning curve don't make too much sense, try to understand why ...\r\n- timings can change between two runs, not sure what to do, doing more runs will make the example slower and the example is already taking ~20s. Also this may be sensitive to `OPENBLAS_NUM_THREADS` (or `MKL_NUM_THREADS`). ",
    "comments": [
      {
        "user": "vitorpohlenz",
        "body": "@lesteve, could you help me answer these two questions?\n\n1) Is this Issue still open? \nI have checked, and it seems that the related PR #26365 is stalled. If it is the case, I could work on it\n\n2) Do you think this Issue is too hard to be a \"second contribution\"? My first contribution was a [small change in the docs](https://github.com/scikit-learn/scikit-learn/pull/31173) last week  \nI read the bullet points in the description, and I think I can handle it, but I need to investigate it properly first. Maybe I will ask more questions/information also"
      },
      {
        "user": "lesteve",
        "body": "Thanks for asking @vitorpohlenz! It's been inactive for a few years and personally I think this is too low priority so that it is not worth working on it. I am going to close this one."
      },
      {
        "user": "vitorpohlenz",
        "body": "> Thanks for asking [@vitorpohlenz](https://github.com/vitorpohlenz)! It's been inactive for a few years and personally I think this is too low priority so that it is not worth working on it. I am going to close this one.\n\nThanks for replying @lesteve !\n[The text below is completely unrelated to this Issue].\n\nActually, I'm trying to find an Issue to work on, but it is been a bit hard to find one that is unclaimed. So I started looking at the stalled ones. If by chance, do you know (or know a person who knows) some Issue that is unclaimed and seems a \"good second issue\", just ping me, I would appreciate that help."
      }
    ]
  },
  {
    "issue_number": 31149,
    "title": "BUG: Build from source fails for scikit-learn v1.6.1 on Windows 11 with Visual Studio Build Tools 2022, Ninja subprocess error",
    "author": "vitorpohlenz",
    "state": "closed",
    "created_at": "2025-04-04T20:08:35Z",
    "updated_at": "2025-04-18T12:32:10Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nFirst of all, thank you guys for the fantastic job with Sklearn. \nI'm trying to build from source to start contributing to the project, but it ended with me bringing more issues to you. \nAfter struggling for some days with this problem, I'm seeking help. Maybe if you have some clue or workaround, I could open a Pull Request with the solution for this.\n\nI am following the guidelines for [Contributing with Scikit-learn](https://scikit-learn.org/stable/developers/contributing.html#contributing), and for that, it is necessary to [Build from source on Windows](https://scikit-learn.org/stable/developers/advanced_installation.html#windows), which recomends install  Build Tools for Visual Studio 2019, but nowadays is not possible to download the 2019 version just the [Build Tools for Visual Studio 2022 installer](https://aka.ms/vs/17/release/vs_buildtools.exe).\n\nThe installation of Build Tools for Visual Studio 2022 runs smoothly(and also the initialization of its Environment), as well as the creation of the Python virtual environment and the installation of the packages `wheel, numpy, scipy, cython, meson-python, ninja`.\n\nBut in the step of building from source using the `pip install --editable`, the build breaks when Compiling C objects after some [C4090 warnings](https://learn.microsoft.com/en-us/cpp/error-messages/compiler-warnings/compiler-warning-level-1-c4090?view=msvc-170), throwing an `metadata-generation-failed error` from a subprocess of `ninja build`.\n\nThis seems related/similar to issue #31123. Despite not being the same problem, if we find a solution, it may work for both issues.\n\n### Steps/Code to Reproduce\n\nI have tried the steps using `pip install` and also `conda-forge` in different versions of Python: pip :{3.10.11, 3.12.7} conda:{ 3.13.2}  to check if it was a problem with Python/pip itself.\n\n1. **Environment Setup:**\n- OS:  Windows 11 Pro, Version 24H2, OS build 26100.3476\n- System type: 64-bit operating system, x64-based processor\n- Installed Visual Studio Build **2022**, version 17.13.5 (It is not possible to download from Microsoft the 2019 version as recommended)\n  -  I also restarted the computer after installing (because it is Windows, so you never know)\n\n![VSbuildTools](https://github.com/user-attachments/assets/3fd83a4d-0451-4c79-97f4-ff6d796a75d9)\n\n2. **Project directory**\n- `C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn`\n\n3. **Installing compiler**\nInstalled [Microsoft build Tools 2022](https://aka.ms/vs/17/release/vs_buildtools.exe)\n\nInitialized the enviroment:\n```\nSET DISTUTILS_USE_SDK=1\n\n\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64\n```\nWhich gives the output:\n\n```\n**********************************************************************\n** Visual Studio 2022 Developer Command Prompt v17.13.5\n** Copyright (c) 2022 Microsoft Corporation\n**********************************************************************\n[vcvarsall.bat] Environment initialized for: 'x64'\n```\n \n4. **Steps Using pip**, the same error occurs using Python 3.10 or 3.12. I will provide the steps and logs just for 3.10 for the sake of simplicity.\n\nNavigated to the project directory: `C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn`\n\nCreated the Python virtual environment, upgraded pip, and installed the dependencies\n```\npy -3.10 -m venv envsklearn\n\nenvsklearn\\Scripts\\python.exe -m pip install --upgrade pip\n\nenvsklearn\\Scripts\\activate\n\npip install wheel numpy scipy cython meson-python ninja\n```\n\nUntil here, everything works as expected. But when building from source, in the command below, the error occurs:\n\n```\npip install --editable . --verbose --no-build-isolation --config-settings editable-verbose=true\n```\n\n5. **Using conda forge**:\nTo check if it wasn't a problem with pip I have also used `conda-forge`, as described in the [Building from Source tutorial](https://scikit-learn.org/stable/developers/advanced_installation.html#building-from-source):\n\n`conda create -n sklearn-env -c conda-forge python numpy scipy cython meson-python ninja`\n`conda activate sklearn-env`\n`pip install --editable . --verbose --no-build-isolation --config-settings editable-verbose=true`\n\n> Note: I used 2 different environment names to avoid confusion: `envsklearn` is the `venv` that uses `pip`, and `sklearn-env` is the `conda env`\n\n### Expected Results\n\nThe build of scikit-learn should finish and install without error, or at least with a few warnings.\n\n### Actual Results\n\n## Using pip install\nIt seems that the Meson build system finds the compiler for C, C+++, and also the `ninja.exe` in the environment. But ninja subprocess breaks when compiling `C object sklearn/metrics/_dist_metrics.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__dist_metrics.pyx.c.obj`\n\nError message:\n```\n[...]\n[46/164] Compiling C object sklearn/metrics/_dist_metrics.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__dist_metrics.pyx.c.obj\n  sklearn/metrics/_dist_metrics.cp310-win_amd64.pyd.p/sklearn/metrics/_dist_metrics.pyx.c(29605): warning C4090: '=': different 'const' qualifiers\n  sklearn/metrics/_dist_metrics.cp310-win_amd64.pyd.p/sklearn/metrics/_dist_metrics.pyx.c(30390): warning C4090: '=': different 'const' qualifiers\n  sklearn/metrics/_dist_metrics.cp310-win_amd64.pyd.p/sklearn/metrics/_dist_metrics.pyx.c(49229): warning C4090: '=': different 'const' qualifiers\n  sklearn/metrics/_dist_metrics.cp310-win_amd64.pyd.p/sklearn/metrics/_dist_metrics.pyx.c(50014): warning C4090: '=': different 'const' qualifiers\n  ninja: build stopped: subcommand failed.\n  INFO: autodetecting backend as ninja\n  INFO: calculating backend command to run: C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\envsklearn\\Scripts\\ninja.EXE\n  error: subprocess-exited-with-error\n\n  Ã— Preparing editable metadata (pyproject.toml) did not run successfully.\n  â”‚ exit code: 1\n  â•°â”€> See above for output.\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  full command: 'C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\envsklearn\\Scripts\\python.exe' 'C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\envsklearn\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py' prepare_metadata_for_build_editable 'C:\\Users\\VITOR~1.POH\\AppData\\Local\\Temp\\tmpp45ks547'\n  cwd: C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\n  Preparing editable metadata (pyproject.toml) ... error\nerror: metadata-generation-failed\n\nÃ— Encountered error while generating package metadata.\nâ•°â”€> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n```\n\n<details><summary>Full Log</summary>\n\n```\nUsing pip 25.0.1 from C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\envsklearn\\lib\\site-packages\\pip (python 3.10)\nObtaining file:///C:/Users/vitor.pohlenz/Documents/Python/vpz/scikit-learn\n  Running command Checking if build backend supports build_editable\n  Checking if build backend supports build_editable ... done\n  Running command Preparing editable metadata (pyproject.toml)\n  + meson setup --reconfigure C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\build\\cp310 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\build\\cp310\\meson-python-native-file.ini\n  Cleaning... 0 files.\n  The Meson build system\n  Version: 1.7.1\n  Source dir: C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\n  Build dir: C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\build\\cp310\n  Build type: native build\n  Project name: scikit-learn\n  Project version: 1.7.dev0\n  C compiler for the host machine: cl (msvc 19.43.34809 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.43.34809 for x64\")\n  C linker for the host machine: link link 14.43.34809.0\n  C++ compiler for the host machine: cl (msvc 19.43.34809 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.43.34809 for x64\")\n  C++ linker for the host machine: link link 14.43.34809.0\n  Cython compiler for the host machine: cython (cython 3.0.12)\n  Host machine cpu family: x86_64\n  Host machine cpu: x86_64\n  Compiler for C supports arguments -Wno-unused-but-set-variable: NO (cached)\n  Compiler for C supports arguments -Wno-unused-function: NO (cached)\n  Compiler for C supports arguments -Wno-conversion: NO (cached)\n  Compiler for C supports arguments -Wno-misleading-indentation: NO (cached)\n  Library m found: NO\n  Program sklearn/_build_utils/tempita.py found: YES (C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\envsklearn\\Scripts\\python.exe C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\sklearn/_build_utils/tempita.py)\n  Program python found: YES (C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\envsklearn\\Scripts\\python.exe)\n  Dependency OpenMP found: YES 2.0 (cached)\n  Build targets in project: 111\n\n  scikit-learn 1.7.dev0\n\n    User defined options\n      Native files: C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\build\\cp310\\meson-python-native-file.ini\n      b_ndebug    : if-release\n      b_vscrt     : md\n      buildtype   : release\n\n  Found ninja.EXE-1.11.1.git.kitware.jobserver-1 at C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\envsklearn\\Scripts\\ninja.EXE\n  + meson compile\n  [1/164] Generating sklearn/write_built_with_meson_file with a custom command\n  [2/164] Compiling Cython source C:/Users/vitor.pohlenz/Documents/Python/vpz/scikit-learn/sklearn/metrics/cluster/_expected_mutual_info_fast.pyx\n  [3/164] Compiling Cython source sklearn/utils/_seq_dataset.pyx\n  [4/164] Compiling Cython source C:/Users/vitor.pohlenz/Documents/Python/vpz/scikit-learn/sklearn/cluster/_dbscan_inner.pyx\n  [5/164] Compiling Cython source C:/Users/vitor.pohlenz/Documents/Python/vpz/scikit-learn/sklearn/utils/_vector_sentinel.pyx\n  [6/164] Compiling Cython source C:/Users/vitor.pohlenz/Documents/Python/vpz/scikit-learn/sklearn/metrics/_pairwise_fast.pyx\n  [7/164] Compiling Cython source sklearn/utils/_weight_vector.pyx\n  [8/164] Compiling Cython source C:/Users/vitor.pohlenz/Documents/Python/vpz/scikit-learn/sklearn/utils/_isfinite.pyx\n  [9/164] Compiling Cython source C:/Users/vitor.pohlenz/Documents/Python/vpz/scikit-learn/sklearn/cluster/_hierarchical_fast.pyx\n  [10/164] Compiling Cython source sklearn/metrics/_pairwise_distances_reduction/_base.pyx\n  [11/164] Compiling Cython source C:/Users/vitor.pohlenz/Documents/Python/vpz/scikit-learn/sklearn/utils/_fast_dict.pyx\n  [12/164] Compiling Cython source C:/Users/vitor.pohlenz/Documents/Python/vpz/scikit-learn/sklearn/utils/arrayfuncs.pyx\n  [13/164] Compiling Cython source C:/Users/vitor.pohlenz/Documents/Python/vpz/scikit-learn/sklearn/cluster/_k_means_common.pyx\n  [14/164] Compiling Cython source sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pyx\n  [15/164] Compiling Cython source sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.pyx\n  [16/164] Compiling Cython source sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx\n  [17/164] Compiling Cython source sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.pyx\n  [18/164] Compiling Cython source C:/Users/vitor.pohlenz/Documents/Python/vpz/scikit-learn/sklearn/utils/_cython_blas.pyx\n  [19/164] Compiling Cython source sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pyx\n  [20/164] Compiling C object sklearn/utils/_weight_vector.cp310-win_amd64.pyd.p/meson-generated_sklearn_utils__weight_vector.pyx.c.obj\n  [21/164] Compiling Cython source C:/Users/vitor.pohlenz/Documents/Python/vpz/scikit-learn/sklearn/utils/sparsefuncs_fast.pyx\n  [22/164] Compiling Cython source sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pyx\n  [23/164] Compiling Cython source sklearn/metrics/_dist_metrics.pyx\n  warning: sklearn\\metrics\\_dist_metrics.pyx:855:44: Assigning to 'float64_t *' from 'const float64_t *' discards const qualifier\n\n  warning: sklearn\\metrics\\_dist_metrics.pyx:918:40: Assigning to 'float64_t *' from 'const float64_t *' discards const qualifier\n\n  warning: sklearn\\metrics\\_dist_metrics.pyx:3435:44: Assigning to 'float32_t *' from 'const float32_t *' discards const qualifier\n\n  warning: sklearn\\metrics\\_dist_metrics.pyx:3498:40: Assigning to 'float32_t *' from 'const float32_t *' discards const qualifier\n\n  [24/164] Compiling C object sklearn/utils/_seq_dataset.cp310-win_amd64.pyd.p/meson-generated_sklearn_utils__seq_dataset.pyx.c.obj\n  [25/164] Compiling C++ object sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_distances_reduction__radius_neighbors_classmode.pyx.cpp.obj\n  FAILED: sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_distances_reduction__radius_neighbors_classmode.pyx.cpp.obj\n  \"cl\" \"-Isklearn\\metrics\\_pairwise_distances_reduction\\_radius_neighbors_classmode.cp310-win_amd64.pyd.p\" \"-Isklearn\\metrics\\_pairwise_distances_reduction\" \"-I..\\..\\sklearn\\metrics\\_pairwise_distances_reduction\" \"-I..\\..\\envsklearn\\lib\\site-packages\\numpy\\_core\\include\" \"-IC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\Include\" \"-DNDEBUG\" \"/MD\" \"/nologo\" \"/showIncludes\" \"/utf-8\" \"/Zc:__cplusplus\" \"/W2\" \"/EHsc\" \"/std:c++14\" \"/permissive-\" \"/O2\" \"/Gw\" \"-DMS_WIN64=\" \"/openmp\" \"-DNPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION\" \"/Fdsklearn\\metrics\\_pairwise_distances_reduction\\_radius_neighbors_classmode.cp310-win_amd64.pyd.p\\meson-generated_sklearn_metrics__pairwise_distances_reduction__radius_neighbors_classmode.pyx.cpp.pdb\" /Fosklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_distances_reduction__radius_neighbors_classmode.pyx.cpp.obj \"/c\" sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.cp310-win_amd64.pyd.p/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.pyx.cpp\n  sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.cp310-win_amd64.pyd.p/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.pyx.cpp(32619): warning C4551: function call missing argument list\n  C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\build\\cp310\\sklearn\\metrics\\_pairwise_distances_reduction\\_radius_neighbors_classmode.cp310-win_amd64.pyd.p\\sklearn\\metrics\\_pairwise_distances_reduction\\_radius_neighbors_classmode.pyx.cpp : fatal error C1083: Cannot open compiler generated file: '': Invalid argument\n  [26/164] Linking target sklearn/utils/_weight_vector.cp310-win_amd64.pyd\n     Creating library sklearn\\utils\\_weight_vector.cp310-win_amd64.lib and object sklearn\\utils\\_weight_vector.cp310-win_amd64.exp\n  [27/164] Linking target sklearn/utils/_seq_dataset.cp310-win_amd64.pyd\n     Creating library sklearn\\utils\\_seq_dataset.cp310-win_amd64.lib and object sklearn\\utils\\_seq_dataset.cp310-win_amd64.exp\n\n  [28/164] Compiling C++ object sklearn/utils/_vector_sentinel.cp310-win_amd64.pyd.p/meson-generated_sklearn_utils__vector_sentinel.pyx.cpp.obj\n  sklearn/utils/_vector_sentinel.cp310-win_amd64.pyd.p/sklearn/utils/_vector_sentinel.pyx.cpp(15416): warning C4551: function call missing argument list\n  [29/164] Compiling C object sklearn/metrics/_pairwise_fast.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_fast.pyx.c.obj\n  [30/164] Compiling C++ object sklearn/cluster/_dbscan_inner.cp310-win_amd64.pyd.p/meson-generated_sklearn_cluster__dbscan_inner.pyx.cpp.obj\n  sklearn/cluster/_dbscan_inner.cp310-win_amd64.pyd.p/sklearn/cluster/_dbscan_inner.pyx.cpp(23107): warning C4551: function call missing argument list\n  [31/164] Compiling C object sklearn/metrics/cluster/_expected_mutual_info_fast.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics_cluster__expected_mutual_info_fast.pyx.c.obj\n  sklearn/metrics/cluster/_expected_mutual_info_fast.cp310-win_amd64.pyd.p/sklearn/metrics/cluster/_expected_mutual_info_fast.pyx.c(18720): warning C4244: 'function': conversion from 'Py_ssize_t' to 'double', possible loss of data\n  sklearn/metrics/cluster/_expected_mutual_info_fast.cp310-win_amd64.pyd.p/sklearn/metrics/cluster/_expected_mutual_info_fast.pyx.c(18720): warning C4244: 'function': conversion from 'Py_ssize_t' to 'double', possible loss of data\n  sklearn/metrics/cluster/_expected_mutual_info_fast.cp310-win_amd64.pyd.p/sklearn/metrics/cluster/_expected_mutual_info_fast.pyx.c(18720): warning C4244: 'function': conversion from 'Py_ssize_t' to 'double', possible loss of data\n  [32/164] Compiling C object sklearn/utils/arrayfuncs.cp310-win_amd64.pyd.p/meson-generated_sklearn_utils_arrayfuncs.pyx.c.obj\n  [33/164] Compiling C++ object sklearn/utils/_fast_dict.cp310-win_amd64.pyd.p/meson-generated_sklearn_utils__fast_dict.pyx.cpp.obj\n  sklearn/utils/_fast_dict.cp310-win_amd64.pyd.p/sklearn/utils/_fast_dict.pyx.cpp(26950): warning C4551: function call missing argument list\n  [34/164] Compiling C object sklearn/utils/_isfinite.cp310-win_amd64.pyd.p/meson-generated_sklearn_utils__isfinite.pyx.c.obj\n  [35/164] Compiling C++ object sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_distances_reduction__argkmin_classmode.pyx.cpp.obj\n  cl : Command line warning D9002 : ignoring unknown option '-fno-sized-deallocation'\n  sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.cp310-win_amd64.pyd.p/sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.pyx.cpp(29323): warning C4551: function call missing argument list\n  [36/164] Compiling C++ object sklearn/metrics/_pairwise_distances_reduction/_base.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_distances_reduction__base.pyx.cpp.obj\n  sklearn/metrics/_pairwise_distances_reduction/_base.cp310-win_amd64.pyd.p/sklearn/metrics/_pairwise_distances_reduction/_base.pyx.cpp(32606): warning C4551: function call missing argument list\n  [37/164] Compiling C++ object sklearn/metrics/_pairwise_distances_reduction/_argkmin.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_distances_reduction__argkmin.pyx.cpp.obj\n  sklearn/metrics/_pairwise_distances_reduction/_argkmin.cp310-win_amd64.pyd.p/sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx.cpp(35870): warning C4551: function call missing argument list\n  [38/164] Compiling C++ object sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_distances_reduction__middle_term_computer.pyx.cpp.obj\n  sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.cp310-win_amd64.pyd.p/sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pyx.cpp(42045): warning C4551: function call missing argument list\n  [39/164] Compiling C++ object sklearn/cluster/_hierarchical_fast.cp310-win_amd64.pyd.p/meson-generated_sklearn_cluster__hierarchical_fast.pyx.cpp.obj\n  sklearn/cluster/_hierarchical_fast.cp310-win_amd64.pyd.p/sklearn/cluster/_hierarchical_fast.pyx.cpp(23677): warning C4244: '=': conversion from '__pyx_t_7sklearn_5utils_9_typedefs_intp_t' to '__pyx_t_7sklearn_5utils_9_typedefs_float64_t', possible loss of data\n  sklearn/cluster/_hierarchical_fast.cp310-win_amd64.pyd.p/sklearn/cluster/_hierarchical_fast.pyx.cpp(23688): warning C4244: '=': conversion from '__pyx_t_7sklearn_5utils_9_typedefs_intp_t' to '__pyx_t_7sklearn_5utils_9_typedefs_float64_t', possible loss of data\n  sklearn/cluster/_hierarchical_fast.cp310-win_amd64.pyd.p/sklearn/cluster/_hierarchical_fast.pyx.cpp(23712): warning C4244: '=': conversion from '__pyx_t_7sklearn_5utils_9_typedefs_intp_t' to '__pyx_t_7sklearn_5utils_9_typedefs_float64_t', possible loss of data\n  sklearn/cluster/_hierarchical_fast.cp310-win_amd64.pyd.p/sklearn/cluster/_hierarchical_fast.pyx.cpp(24838): warning C4244: '=': conversion from '__pyx_t_7sklearn_5utils_9_typedefs_intp_t' to '__pyx_t_7sklearn_5utils_9_typedefs_float64_t', possible loss of data\n  sklearn/cluster/_hierarchical_fast.cp310-win_amd64.pyd.p/sklearn/cluster/_hierarchical_fast.pyx.cpp(24849): warning C4244: '=': conversion from '__pyx_t_7sklearn_5utils_9_typedefs_intp_t' to '__pyx_t_7sklearn_5utils_9_typedefs_float64_t', possible loss of data\n  sklearn/cluster/_hierarchical_fast.cp310-win_amd64.pyd.p/sklearn/cluster/_hierarchical_fast.pyx.cpp(33802): warning C4551: function call missing argument list\n  [40/164] Compiling C++ object sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_distances_reduction__datasets_pair.pyx.cpp.obj\n  sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.cp310-win_amd64.pyd.p/sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pyx.cpp(43425): warning C4551: function call missing argument list\n  [41/164] Compiling C++ object sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_distances_reduction__radius_neighbors.pyx.cpp.obj\n  sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.cp310-win_amd64.pyd.p/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pyx.cpp(38873): warning C4551: function call missing argument list\n  [42/164] Compiling C object sklearn/cluster/_k_means_common.cp310-win_amd64.pyd.p/meson-generated_sklearn_cluster__k_means_common.pyx.c.obj\n  [43/164] Compiling Cython source sklearn/_loss/_loss.pyx\n  [44/164] Compiling C object sklearn/utils/_cython_blas.cp310-win_amd64.pyd.p/meson-generated_sklearn_utils__cython_blas.pyx.c.obj\n  [45/164] Compiling C object sklearn/utils/sparsefuncs_fast.cp310-win_amd64.pyd.p/meson-generated_sklearn_utils_sparsefuncs_fast.pyx.c.obj\n  [46/164] Compiling C object sklearn/metrics/_dist_metrics.cp310-win_amd64.pyd.p/meson-generated_sklearn_metrics__dist_metrics.pyx.c.obj\n  sklearn/metrics/_dist_metrics.cp310-win_amd64.pyd.p/sklearn/metrics/_dist_metrics.pyx.c(29605): warning C4090: '=': different 'const' qualifiers\n  sklearn/metrics/_dist_metrics.cp310-win_amd64.pyd.p/sklearn/metrics/_dist_metrics.pyx.c(30390): warning C4090: '=': different 'const' qualifiers\n  sklearn/metrics/_dist_metrics.cp310-win_amd64.pyd.p/sklearn/metrics/_dist_metrics.pyx.c(49229): warning C4090: '=': different 'const' qualifiers\n  sklearn/metrics/_dist_metrics.cp310-win_amd64.pyd.p/sklearn/metrics/_dist_metrics.pyx.c(50014): warning C4090: '=': different 'const' qualifiers\n  ninja: build stopped: subcommand failed.\n  INFO: autodetecting backend as ninja\n  INFO: calculating backend command to run: C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\envsklearn\\Scripts\\ninja.EXE\n  error: subprocess-exited-with-error\n\n  Ã— Preparing editable metadata (pyproject.toml) did not run successfully.\n  â”‚ exit code: 1\n  â•°â”€> See above for output.\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  full command: 'C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\envsklearn\\Scripts\\python.exe' 'C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\\envsklearn\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py' prepare_metadata_for_build_editable 'C:\\Users\\VITOR~1.POH\\AppData\\Local\\Temp\\tmpp45ks547'\n  cwd: C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\n  Preparing editable metadata (pyproject.toml) ... error\nerror: metadata-generation-failed\n\nÃ— Encountered error while generating package metadata.\nâ•°â”€> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n```\n\n</details> \n\n## Using Conda\n\nFor `conda env`, the problem seems to be associated with `undeclared identifiers` and `syntax errors`\n\n```\n [22/136] Compiling C++ object sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.cp313t-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_distances_reduction__middle_term_computer.pyx.cpp.obj\n  FAILED: sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.cp313t-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_distances_reduction__middle_term_computer.pyx.cpp.obj\n  \"cl\" \"-Isklearn\\metrics\\_pairwise_distances_reduction\\_middle_term_computer.cp313t-win_amd64.pyd.p\" \"-Isklearn\\metrics\\_pairwise_distances_reduction\" \"-I..\\..\\sklearn\\metrics\\_pairwise_distances_reduction\" \"-I..\\..\\..\\..\\..\\..\\AppData\\Local\\anaconda3\\envs\\sklearn-env\\Lib\\site-packages\\numpy\\_core\\include\" \"-IC:\\Users\\vitor.pohlenz\\AppData\\Local\\anaconda3\\envs\\sklearn-env\\Include\" \"-DNDEBUG\" \"/MD\" \"/nologo\" \"/showIncludes\" \"/utf-8\" \"/Zc:__cplusplus\" \"/W2\" \"/EHsc\" \"/std:c++14\" \"/permissive-\" \"/O2\" \"/Gw\" \"-DPy_GIL_DISABLED\" \"-DNPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION\" \"/Fdsklearn\\metrics\\_pairwise_distances_reduction\\_middle_term_computer.cp313t-win_amd64.pyd.p\\meson-generated_sklearn_metrics__pairwise_distances_reduction__middle_term_computer.pyx.cpp.pdb\" /Fosklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.cp313t-win_amd64.pyd.p/meson-generated_sklearn_metrics__pairwise_distances_reduction__middle_term_computer.pyx.cpp.obj \"/c\" sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.cp313t-win_amd64.pyd.p/sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pyx.cpp\n\n[lots of lines similar to the line below ...]\n\n sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.cp313t-win_amd64.pyd.p/sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pyx.cpp(43156): error C2065: 'vc': undeclared identifier\n  ninja: build stopped: subcommand failed.\n  INFO: autodetecting backend as ninja\n  INFO: calculating backend command to run: C:\\Users\\vitor.pohlenz\\AppData\\Local\\anaconda3\\envs\\sklearn-env\\Library\\bin\\ninja.EXE\n  error: subprocess-exited-with-error\n\n  Ã— Preparing editable metadata (pyproject.toml) did not run successfully.\n  â”‚ exit code: 1\n  â•°â”€> See above for output.\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  full command: 'C:\\Users\\vitor.pohlenz\\AppData\\Local\\anaconda3\\envs\\sklearn-env\\python.exe' 'C:\\Users\\vitor.pohlenz\\AppData\\Local\\anaconda3\\envs\\sklearn-env\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py' prepare_metadata_for_build_editable 'C:\\Users\\VITOR~1.POH\\AppData\\Local\\Temp\\tmp25gfqq4d'\n  cwd: C:\\Users\\vitor.pohlenz\\Documents\\Python\\vpz\\scikit-learn\n  Preparing editable metadata (pyproject.toml) ... error\nerror: metadata-generation-failed\n\nÃ— Encountered error while generating package metadata.\nâ•°â”€> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n\n``` \n\n[Gist of Full conda log](https://gist.github.com/vitorpohlenz/bc6a1599e99e4042b31cdde1d406054c)\n\n\n### Versions\n\n```shell\n## General and OS\n\n- OS: Windows 11 Pro, Version 24H2, OS build 26100.3476\n- System type: 64-bit operating system, x64-based processor\n- Installed Visual Studio Build 2022, version 17.13.5\n\nNeither using `pip` in the `venv`, nor using `conda env`, I could use the `import sklearn; sklearn.show_versions()`.\n\n### venv using pip\n\n- Python: 3.10.11\n- pip 25.0.1\n- Libs inside env:\n  - Cython==3.0.12\n  - meson==1.7.2\n  - meson-python==0.17.1\n  - ninja==1.11.1.4\n  - numpy==2.2.4\n  - packaging==24.2\n  - pyproject-metadata==0.9.1\n  - scipy==1.15.2\n  - tomli==2.2.1\n\n\n### conda env\n\n- Python 3.13.2\n- conda 24.11.3\n\nLibs inside conda env:\n\n# Name                    Version                   Build  Channel\nbzip2                     1.0.8                h2466b09_7    conda-forge\nca-certificates           2025.1.31            h56e8100_0    conda-forge\ncython                    3.0.12           pyh2c78169_100    conda-forge\nintel-openmp              2024.2.1          h57928b3_1083    conda-forge\nlibblas                   3.9.0           31_h641d27c_mkl    conda-forge\nlibcblas                  3.9.0           31_h5e41251_mkl    conda-forge\nlibexpat                  2.7.0                he0c23c2_0    conda-forge\nlibffi                    3.4.6                h537db12_1    conda-forge\nlibhwloc                  2.11.2          default_ha69328c_1001    conda-forge\nlibiconv                  1.18                 h135ad9c_1    conda-forge\nliblapack                 3.9.0           31_h1aa476e_mkl    conda-forge\nliblzma                   5.8.1                h2466b09_0    conda-forge\nlibmpdec                  4.0.0                h2466b09_0    conda-forge\nlibsqlite                 3.49.1               h67fdade_2    conda-forge\nlibwinpthread             12.0.0.r4.gg4f2fc60ca      h57928b3_9    conda-forge\nlibxml2                   2.14.0               had0eb51_0    conda-forge\nlibzlib                   1.3.1                h2466b09_2    conda-forge\nmeson                     1.7.1              pyhd8ed1ab_0    conda-forge\nmeson-python              0.17.1             pyh70fd9c4_1    conda-forge\nmkl                       2024.2.2            h66d3029_15    conda-forge\nninja                     1.12.1               hc790b64_0    conda-forge\nnumpy                     2.2.4           py313hd96daed_0    conda-forge\nopenssl                   3.4.1                ha4e3fda_0    conda-forge\npackaging                 24.2               pyhd8ed1ab_2    conda-forge\npip                       25.0.1             pyh145f28c_0    conda-forge\npyproject-metadata        0.9.1              pyhd8ed1ab_0    conda-forge\npython                    3.13.2          hd7c436d_1_cp313t    conda-forge\npython_abi                3.13                   5_cp313t    conda-forge\nscipy                     1.15.2          py313h8a36d7e_0    conda-forge\nsetuptools                78.1.0             pyhff2d567_0    conda-forge\ntbb                       2021.13.0            h62715c5_1    conda-forge\ntk                        8.6.13               h5226925_1    conda-forge\ntomli                     2.2.1              pyhd8ed1ab_1    conda-forge\ntzdata                    2025b                h78e105d_0    conda-forge\nucrt                      10.0.22621.0         h57928b3_1    conda-forge\nvc                        14.3                h2b53caa_26    conda-forge\nvc14_runtime              14.42.34438         hfd919c2_26    conda-forge\n```",
    "comments": [
      {
        "user": "ogrisel",
        "body": "Thanks for the report.\n\nFor the conda-forge method, you still need to build scikit-learn from source. That is:\n\nAfter you do:\n\n```\npy -3.10 -m venv envsklearn\n\nenvsklearn\\Scripts\\python.exe -m pip install --upgrade pip\n\nenvsklearn\\Scripts\\activate\n\npip install wheel numpy scipy cython meson-python ninja\n```\n\nyou need to:\n\n```\npip install --editable . --verbose --no-build-isolation --config-settings editable-verbose=true\n```\n\nand then only check that you can import \"sklearn\" with:\n\n```\npython -c \"import sklearn; sklearn.show_versions()\"\n```"
      },
      {
        "user": "ogrisel",
        "body": "The ninja error message is not very explicit but it's definitely different from https://github.com/scikit-learn/scikit-learn/issues/31123 which is about running mkdir in a non existing folder. Your issue does not seem related to non-existing folders."
      },
      {
        "user": "ogrisel",
        "body": "I discussed this IRL with @jeremiedbb and @lesteve and they both said that they could build scikit-learn from source by following the instructions (while replacing the 2019 SDK by the 2022 version as you did).\n\nSo there must be something specific to you setup that we missed. Maybe they did not use the same Windows version?\n\nAnyway, a PR would be welcome to fix the instructions (and link) to tell the reader to use the 2022 version o Visual Studio Build Tools."
      }
    ]
  },
  {
    "issue_number": 30736,
    "title": "`randomized_svd` incorrect for complex valued matrices",
    "author": "clane9",
    "state": "closed",
    "created_at": "2025-01-30T01:40:26Z",
    "updated_at": "2025-04-17T09:28:05Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nThe `randomized_svd` utility function accepts complex valued inputs without error, but the result is inconsistent with `scipy.linalg.svd`.\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom scipy import linalg\nfrom sklearn.utils.extmath import randomized_svd\n\nrng = np.random.RandomState(42)\nX = rng.randn(100, 20) + 1j * rng.randn(100, 20)\n\n_, s, _ = linalg.svd(X)\n_, s2, _ = randomized_svd(X, n_components=5)\n\nprint(\"s:\", s[:5])\nprint(\"s2:\", s2[:5])\n```\n\n### Expected Results\n\nI expected the singular values to be numerically close.\n\n### Actual Results\n\n```\ns: [19.81481515 18.69019042 17.62107998 17.23689681 16.3148512 ]\ns2: [11.25690754  9.97157079  9.01542947  8.06160863  7.54068744]\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.11.4 (main, Jul  5 2023, 08:40:20) [Clang 14.0.6 ]\nexecutable: /Users/clane/miniconda3/bin/python\n   machine: macOS-13.7-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.7.dev0\n          pip: 25.0\n   setuptools: 65.5.0\n        numpy: 2.2.2\n        scipy: 1.15.1\n       Cython: 3.0.11\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: /Users/clane/Projects/misc/scikit-learn/.venv/lib/python3.11/site-packages/numpy/.dylibs/libscipy_openblas64_.dylib\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: neoversen1\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: /Users/clane/Projects/misc/scikit-learn/.venv/lib/python3.11/site-packages/scipy/.dylibs/libscipy_openblas.dylib\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: neoversen1\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libomp\n       filepath: /opt/homebrew/Cellar/libomp/19.1.3/lib/libomp.dylib\n        version: None\n```",
    "comments": []
  },
  {
    "issue_number": 30964,
    "title": "DOC better visibility in navigation of metadata routing",
    "author": "lorentzenchr",
    "state": "closed",
    "created_at": "2025-03-09T11:22:32Z",
    "updated_at": "2025-04-17T04:08:15Z",
    "labels": [
      "Documentation",
      "Metadata Routing"
    ],
    "body": "The section about [metadata_routing](https://scikit-learn.org/stable/metadata_routing.html) in the [user guide](https://scikit-learn.org/stable/user_guide.html) is hard to find, in particular because there is no entry in the navigation bar, see\n\n<img width=\"1104\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/014c9d80-1cb3-4e7c-9e3d-34333bf8e87d\" />",
    "comments": [
      {
        "user": "adrinjalali",
        "body": "Back when we merged metadata routing infra, people were a bit worried about exposing the feature in any prominent way.\n\nI think these days we can safely move it to a more visible place, since it's used quite a bit."
      },
      {
        "user": "StefanieSenger",
        "body": "I can take care of that."
      }
    ]
  },
  {
    "issue_number": 22289,
    "title": "Add Lasso regularization to PoissonRegressor",
    "author": "ebatty",
    "state": "closed",
    "created_at": "2022-01-24T22:41:24Z",
    "updated_at": "2025-04-16T00:38:41Z",
    "labels": [
      "New Feature",
      "module:linear_model"
    ],
    "body": "### Describe the workflow you want to enable\n\nLinear and logistic regression allow l1, l2, or elasticnet regularization. However, the Poisson regression class (which is amazing to have!) only allows L2 regularization. \n\n### Describe your proposed solution\n\nEnable flexible regularization choices for the Generalized Linear Models\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "vmodi1",
        "body": "Hey @ebatty, I would like to contribute to this issue. It would be my first contribution. Can you assign this to me?"
      },
      {
        "user": "thomasjpfan",
        "body": "@Vandinimodi1595 This is likely not a good first issue. One needs to be familiar with scikit-learn's workflow and requirements for adding a new feature, which involves updates to the user guide, tests, examples & implementation. To learn about scikit-learn's workflow, a good first issue is https://github.com/scikit-learn/scikit-learn/issues/21350\r\n\r\n@lorentzenchr As for inclusion, what do you think about adding other types of regularization into the GLMs?"
      },
      {
        "user": "vmodi1",
        "body": "\r\n@thomasjpfan, Could you assign that issue #21350 for me then?\r\n"
      }
    ]
  },
  {
    "issue_number": 29253,
    "title": "âš ï¸ CI failed on Linux_free_threaded.pylatest_pip_free_threaded (last failure: Jun 14, 2024) âš ï¸",
    "author": "scikit-learn-bot",
    "state": "closed",
    "created_at": "2024-06-14T02:50:06Z",
    "updated_at": "2025-04-15T16:24:18Z",
    "labels": [
      "Build / CI"
    ],
    "body": "**CI failed on [Linux_free_threaded.pylatest_pip_free_threaded](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=67539&view=logs&j=8bc43b48-889f-54b9-cd8b-781ee8447bf2)** (Jun 14, 2024)\n- test_minibatch_sensible_reassign[34]",
    "comments": [
      {
        "user": "ogrisel",
        "body": "This might be a seed sensitivity problem of this test. We need to see if we can reproduce with other Python versions / platforms.\r\n\r\nThe error is:\r\n\r\n```python\r\n    def test_minibatch_sensible_reassign(global_random_seed):\r\n        # check that identical initial clusters are reassigned\r\n        # also a regression test for when there are more desired reassignments than\r\n        # samples.\r\n        zeroed_X, true_labels = make_blobs(\r\n            n_samples=100, centers=5, random_state=global_random_seed\r\n        )\r\n        zeroed_X[::2, :] = 0\r\n    \r\n        km = MiniBatchKMeans(\r\n            n_clusters=20, batch_size=10, random_state=global_random_seed, init=\"random\"\r\n        ).fit(zeroed_X)\r\n        # there should not be too many exact zero cluster centers\r\n>       assert km.cluster_centers_.any(axis=1).sum() > 10\r\nE       AssertionError\r\n```\r\n\r\nany idea why pytest's `AssertionError` no longer display the values of the two expressions on each side of an binary operator?"
      },
      {
        "user": "lesteve",
        "body": "The `test_minibatch_sensible_reassign` failure has been seen in other CI builds https://github.com/scikit-learn/scikit-learn/issues/27967#issuecomment-1863791083 or https://github.com/scikit-learn/scikit-learn/issues/26802#issuecomment-2126728286\r\n\r\nNot sure about why pytest does not manage to give more information about the `AssertionError` ... same thing happens on vanilla Python in our CI e.g. this [build log](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=66816&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a&t=4bd2dad8-62b3-5bf9-08a5-a9880c530c94)"
      },
      {
        "user": "ogrisel",
        "body": "I tried to reproduce locally with either openblas or accelerate:\r\n\r\n```\r\nSKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"all\"  pytest -k test_minibatch_sensible_reassign sklearn/cluster/tests/test_k_means.py -v\r\n```\r\n\r\nbut I cannot reproduce the assertion failure after several runs including when using `pytest-xdist` for different concurrency-induced non-deterministic execution patterns."
      }
    ]
  },
  {
    "issue_number": 30713,
    "title": "Error in `d2_log_loss_score` multiclass when one of the classes is missing in `y_true`.",
    "author": "aperezlebel",
    "state": "closed",
    "created_at": "2025-01-24T11:01:39Z",
    "updated_at": "2025-04-15T14:45:37Z",
    "labels": [
      "Bug",
      "Needs Investigation"
    ],
    "body": "### Describe the bug\n\nHello, I encountered an error with the `d2_log_loss_score` in the multiclass setting (i.e. when `y_pred` has shape (n, k) with k >= 3) when one of the classes is missing from the `y_true` labels, even when giving the labels through the `labels` argument. The error disappear when all the classes are present in `y_true`.\n\n### Steps/Code to Reproduce\n\n```python\nfrom sklearn.metrics import d2_log_loss_score\n\ny_true = [0, 1, 1]\ny_pred = [[1, 0, 0], [1, 0, 0], [1, 0, 0]]\nlabels = [0, 1, 2]\n\nd2_log_loss_score(y_true, y_pred, labels=labels)\n```\n\n### Expected Results\n\nNo error is thrown.\n\n### Actual Results\n\n```\nTraceback (most recent call last):\n  File \"minimal.py\", line 7, in <module>\n    d2_log_loss_score(y_true, y_pred, labels=labels)\n  File \".../python3.12/site-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \".../python3.12/site-packages/sklearn/metrics/_classification.py\", line 3407, in d2_log_loss_score\n    denominator = log_loss(\n                  ^^^^^^^^^\n  File \".../python3.12/site-packages/sklearn/utils/_param_validation.py\", line 189, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \".../python3.12/site-packages/sklearn/metrics/_classification.py\", line 3023, in log_loss\n    raise ValueError(\nValueError: The number of classes in labels is different from that in y_pred. Classes found in labels: [0 1 2]\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.8 | packaged by conda-forge | (main, Dec  5 2024, 14:19:53) [Clang 18.1.8 ]\nexecutable: /Users/alexandreperez/dev/lib/miniforge3/envs/test/bin/python\n   machine: macOS-15.2-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 24.3.1\n   setuptools: 75.8.0\n        numpy: 2.2.2\n        scipy: 1.15.1\n       Cython: None\n       pandas: None\n   matplotlib: None\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 14\n         prefix: libomp\n       filepath: .../miniforge3/envs/test/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib\n        version: None\n```",
    "comments": [
      {
        "user": "lesteve",
        "body": "Indeed, I can reproduce, looks like an oversight, but more investigation is needed."
      },
      {
        "user": "ogrisel",
        "body": "Thanks for the report @aperezlebel. Are you interested in opening on a PR yourself?"
      },
      {
        "user": "aperezlebel",
        "body": "I would be, but I am afraid I won't have time in the near future."
      }
    ]
  },
  {
    "issue_number": 31143,
    "title": "Enable exporting trained models to text files to be able to import later",
    "author": "naveenmarthala",
    "state": "closed",
    "created_at": "2025-04-03T16:30:32Z",
    "updated_at": "2025-04-15T14:10:31Z",
    "labels": [
      "New Feature",
      "Needs Decision - Include Feature"
    ],
    "body": "### Describe the workflow you want to enable\n\n```python\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import RandomForestRegressor\n\n# make data\nX,y = fetch_california_housing(return_X_y=True)\n\n# instantiate Random-Forest and fit it\nrf_model = RandomForestRegressor(min_samples_leaf=5, random_state=0, n_jobs=-1)\nrf_model.fit(X, y)\n\n# export model to a text file, inspired by https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBRegressor.save_model\nrf_model.save_model(\"model.json\")\n\n\n#################### in a new python environment\nfrom sklearn.ensemble import RandomForestRegressor\nrf_model2 = RandomForestRegressor(min_samples_leaf=5, random_state=0, n_jobs=-1)\nrf_model2.load_model(\"model.json\")\n```\n\n### Describe your proposed solution\n\nThe current recommended way I believe is to export fit (or trained) models is to serialize them using joblib, which depends on python version, joblib version and scikit-learn version too, and I presume this may lead to issues with OS and CPU architecture as well (windows or  GNU Linux and x86 or ARM64).\n\nSo, I request a way to export model's trained weights (or other relevant things like bins & trees for RandomForestRegressor) for it be to loaded from any scikit-learn version or python version or operating system. This is just how the big packages like [xgboost](https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBRegressor.save_model) and pytorch ([using `state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict)) and hence transformers,  handle things. \n\nThis would enable to change environments and platforms easily without having to train model for the new package-versions, architecture and OS again or every time an update in them is required.\n\n### Describe alternatives you've considered, if relevant\n\nThere is no alternative to everything that scikit-learn offers as of now.\n\n### Additional context\n\nThis has been provied above.",
    "comments": [
      {
        "user": "ogrisel",
        "body": "I think that using a text file to store a huge number of numerical parameters can be very inefficient and could introduce discrepancies via rounding errors of floating-point values converted to and from a decimal text representation.\n\nYou might instead want to take a look at `skops.io`:\n\nhttps://skops.readthedocs.io/en/latest/persistence.html\n\nInternally, `skops.io` uses a mixed json + binary arrays storage format.\n\nHowever, loading a model trained and persisted with an older version of scikit-learn is not always guaranteed to work either (quoting a paragraph from the linked doc):\n\n> Using skops to load a model saved in one sklearn version and loading it with another sklearn version is not recommended, because the behavior of the model may change across versions. In some cases loading the model in a different version might not be possible due to internal changes in scikit-learn. Such changes donâ€™t happen very often, but they can happen, thus you should be cautious. To replicate a model trained with one sklearn version using a different sklearn version, it is advised to retrain the model on the same data using the same training process.\n\nMaintaining a versioned serialization format that automatically handles scikit-learn migrations has been discussed in the past, and the general agreement was that it is out of the scope of the scikit-learn project for the sake of keeping maintenance costs under control.\n\nIt might be possible to build such a migration tool top of `skops.io` (or maybe even part of it) but as far as I know, nobody has the decided to invest the time and effort to do it. Doing it would involve setting up a complex continuous integration infrastructure to train and archive models with past scikit-learn versions and test that they can still be loaded and executed with newer versions. Whenever a problem is detected between 2 versions, a dedicated migration code should be implemented and included in the migration tool. Setting up such a testing infrastructure requires a non-trivial effort. If your company really needs this, it might be possible to set up a contract to develop and maintain such a tool:\n\nhttps://scikit-learn.org/dev/support.html#paid-support\n\nOtherwise, I would just make sure that your training pipeline is under version control and runs are automated using a some kind of continuous integration or MLops platform and that you lock a version of its dependencies using tools such as [pixi](https://pixi.sh/) or [uv compile](https://docs.astral.sh/uv/pip/compile/) such that it's easy to trigger a retrain job with a controlled version of the libraries when skops or pickle fail to load a model trained with old versions of the libraries.\n\ncc @adrinjalali who is a maintainer of skops."
      },
      {
        "user": "naveenmarthala",
        "body": "Thank you for your time and your comment.\n\nI came across a table outlining the pros and cons of using skops at https://scikit-learn.org/stable/model_persistence.html. The primary reason I created this github-issue is to address compatibility across different versions.\n\nIn the company I work for, we have a few ML models trained using Scikit-learn that are currently in production. These models rarely require updates, so retraining is largely out of scope. However, over time, due to security and maintenance requirements, we need to upgrade the production environment â€” including the Python and Scikit-learn versions. Ideally, weâ€™d like to upgrade the environment without needing to retrain the models.\n\nUnfortunately, from what I understand, skops does not currently support this use case well and this seems to be out of scope even for scikit-learn."
      },
      {
        "user": "naveenmarthala",
        "body": "I understand the rationale behind internal changes in scikit-learn over time. However, Iâ€™m curious â€” why are these changes significant enough that models exported in one version cannot be reliably read in future versions?\n\nMay I ask why long-term backward compatibility is considered out of scope? For instance, PyTorch â€” which arguably has a more complex architecture â€” manages to maintain compatibility of serialized models across versions to a great extent. Iâ€™m wondering what the key differences or constraints are in scikit-learnâ€™s case that make this difficult."
      }
    ]
  },
  {
    "issue_number": 31189,
    "title": "scikit-learn not included in conda env creation step for bleeding-edge install",
    "author": "sanjyay",
    "state": "closed",
    "created_at": "2025-04-13T18:26:37Z",
    "updated_at": "2025-04-14T07:45:23Z",
    "labels": [
      "Documentation"
    ],
    "body": "### Describe the issue linked to the documentation\n\nOn the [Contributing](https://scikit-learn.org/stable/developers/contributing.html) page, under \"How to contribute\", Step 4 guides users to the \"Building from source\" section, which links to:\n\n [Advanced Installation â€“ Install bleeding-edge](https://scikit-learn.org/stable/developers/advanced_installation.html#install-bleeding-edge)\n\nHowever, in Step 2 of that page (the conda environment creation command), the scikit-learn package itself is not mentioned or included.\n\n![Image](https://github.com/user-attachments/assets/d9ec66cb-7d2b-4708-8ce6-27dd6317f55f)\nbut Step 6 asks to Check that the installed scikit-learn has a version number ending with .dev0 which raises errors if scikit-learn is not installed in the virtual environment\n\n![Image](https://github.com/user-attachments/assets/4c04dec8-e112-441c-a941-e0d3bc1d0861)\n\n\n### Suggest a potential alternative/fix\n\n_No response_",
    "comments": [
      {
        "user": "jeremiedbb",
        "body": "Hi @sanjyay, have you done all the steps ? scikit-learn is compiled and installed at step 5."
      },
      {
        "user": "sanjyay",
        "body": "@jeremiedbb I was not able to compile from source since I was using python=3.13 and thought that Step 4 is part of Step 3 that is alternate approach to conda. Upon viewing issue #31149 , I downgraded my python version to 3.12 and now I am able to build from source. Thank you. "
      }
    ]
  },
  {
    "issue_number": 24713,
    "title": "AttributeError: 'MLPRegressor' object has no attribute '_best_coefs'",
    "author": "davidshumway",
    "state": "closed",
    "created_at": "2022-10-21T03:10:32Z",
    "updated_at": "2025-04-13T15:23:14Z",
    "labels": [
      "Bug",
      "module:neural_network"
    ],
    "body": "The following parameters were working fine with another dataset. When I switched to a new dataset, for some reason an `AttributeError` is occurring. Any ideas?\r\n\r\n```python\r\nmodel = MLPRegressor(hidden_layer_sizes=(10), activation='tanh', batch_size=1000,\r\n  learning_rate_init=0.1, max_iter=5000, momentum=.9, solver='adam', early_stopping=True,\r\n  random_state=1, verbose=1)\r\nX = [[0.,0.,0.,0.2173913,0.,0.5,\r\n  0.,0.46666667,0.76351351,0.41140777,0.55555556,0.03361345,\r\n  0.09022556,0.78107607,0.13250518,1.,0.,0.,\r\n  0.,0.,1.],\r\n [0.80769231,1.,0.,0.69565217,0.20942029,0.5,\r\n  0.,0.13333333,0.,0.,0.88888889,0.95424837,\r\n  0.96491228,0.83766234,0.97584541,0.,0.,0.,\r\n  0.,0.,0.]] \r\ny = [1.0, 1.0]\r\nmodel.fit(X, y)\r\n```\r\n\r\nOutput:\r\n```\r\n...\r\nIteration 4997, loss = 0.00001391\r\nValidation score: nan\r\nIteration 4998, loss = 0.00002568\r\nValidation score: nan\r\nIteration 4999, loss = 0.00001822\r\nValidation score: nan\r\nIteration 5000, loss = 0.00001470\r\nValidation score: nan\r\n\r\nAttributeError: 'MLPRegressor' object has no attribute '_best_coefs'\r\n```\r\n\r\nThe error occurs here:\r\nhttps://github.com/scikit-learn/scikit-learn/blob/7c2a58d51f4528827e9bfe9c43d06c5c1716bfb8/sklearn/neural_network/_multilayer_perceptron.py#L687\r\n\r\nThe error also seems to completely disappear when there are more than around 10-25 training examples.",
    "comments": [
      {
        "user": "glemaitre",
        "body": "You have 2 samples and request `early_stopping`. From the verbose, it seems that the validation score is always `nan` probably because you are using 0 or 1 sample (10% of 2 samples). Since the coefficients are stored only if the score is decreasing, it actually never happens and thus `_best_coefs` is never set.\r\n\r\nThis is really a corner case and we choose to detect it much earlier even before to start trying to optimize the problem because we don't have enough samples in the validation set."
      },
      {
        "user": "glemaitre",
        "body": "> The error also seems to completely disappear when there are more than around 10-25 training examples.\r\n\r\nBe aware that you should not do any machine learning with so few samples."
      },
      {
        "user": "davidshumway",
        "body": "> You have 2 samples and request early_stopping. From the verbose, it seems that the validation score is always nan probably because you are using 0 or 1 sample (10% of 2 samples). Since the coefficients are stored only if the score is decreasing, it actually never happens and thus _best_coefs is never set.\r\n\r\nThe example provided is 2 samples but it's happening with up to 10 samples."
      }
    ]
  },
  {
    "issue_number": 31093,
    "title": "The covariance matrix is incorrect in BayesianRidge",
    "author": "antoinebaker",
    "state": "closed",
    "created_at": "2025-03-27T16:08:00Z",
    "updated_at": "2025-04-13T14:46:22Z",
    "labels": [
      "Bug"
    ],
    "body": "### Describe the bug\n\nThe posterior covariance matrix in `BayesianRidge`, attribute `sigma_`,  is incorrect when `n_features > n_samples`. This is because the posterior covariance requires the full svd, while the current code uses the reduced svd.\n\n### Steps/Code to Reproduce\n\n```python\nimport numpy as np\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn import datasets\n\n# on main\nX, y = datasets.make_regression(n_samples=10, n_features=20)\nn_features = X.shape[1]\nreg = BayesianRidge(fit_intercept=False).fit(X, y)\ncovariance_matrix = np.linalg.inv(\n    reg.lambda_ * np.identity(n_features) + reg.alpha_ * np.dot(X.T, X)\n)\nnp.allclose(reg.sigma_, covariance_matrix)\n```\n\n### Expected Results\n\nTrue\n\n### Actual Results\n\nFalse\n\n### Versions\n\n```shell\n1.7.dev0\n```",
    "comments": [
      {
        "user": "glemaitre",
        "body": "Thanks @antoinebaker. Looks like a bug indeed."
      }
    ]
  },
  {
    "issue_number": 31098,
    "title": "Failing CI for check_sample_weight_equivalence_on_dense_data with LinearRegerssion on debian_32bit",
    "author": "ogrisel",
    "state": "closed",
    "created_at": "2025-03-28T09:41:18Z",
    "updated_at": "2025-04-13T08:47:37Z",
    "labels": [
      "Bug",
      "Build / CI"
    ],
    "body": "Here is the last scheduled run (from 1 day ago) that passed:\n\nhttps://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=75127&view=logs&j=86340c1f-3d76-5202-0821-7817a0f52092&t=a73eff7b-829e-5a65-7648-23ff8e83ea2d\n\nand here is a more recent run that failed (all CI is failing today):\n\nhttps://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=75179&view=logs&j=86340c1f-3d76-5202-0821-7817a0f52092&t=a73eff7b-829e-5a65-7648-23ff8e83ea2d\n\n```\nFAILED tests/test_common.py::test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data] - AssertionError: \nFAILED utils/tests/test_estimator_checks.py::test_check_estimator_clones - AssertionError: \n= 2 failed, 34214 passed, 4182 skipped, 174 xfailed, 66 xpassed, 4252 warnings in 1489.21s (0:24:49) =\n```\n\nFull failure log:\n\n<details>\n\n```\n2025-03-28T06:36:32.3433619Z =================================== FAILURES ===================================\n2025-03-28T06:36:32.3434358Z \u001b[31m\u001b[1m_ test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data] _\u001b[0m\n2025-03-28T06:36:32.3434613Z \n2025-03-28T06:36:32.3434838Z estimator = LinearRegression(positive=True)\n2025-03-28T06:36:32.3435117Z check = functools.partial(<function check_sample_weight_equivalence_on_dense_data at 0xd8591e88>, 'LinearRegression')\n2025-03-28T06:36:32.3435705Z request = <FixtureRequest for <Function test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data]>>\n2025-03-28T06:36:32.3435878Z \n2025-03-28T06:36:32.3436047Z     @parametrize_with_checks(\n2025-03-28T06:36:32.3436274Z         list(_tested_estimators()), expected_failed_checks=_get_expected_failed_checks\n2025-03-28T06:36:32.3436498Z     )\n2025-03-28T06:36:32.3436684Z     def test_estimators(estimator, check, request):\n2025-03-28T06:36:32.3436909Z         # Common tests for estimator instances\n2025-03-28T06:36:32.3437101Z         with ignore_warnings(\n2025-03-28T06:36:32.3437316Z             category=(FutureWarning, ConvergenceWarning, UserWarning, LinAlgWarning)\n2025-03-28T06:36:32.3437521Z         ):\n2025-03-28T06:36:32.3437708Z >           check(estimator)\n2025-03-28T06:36:32.3437793Z \n2025-03-28T06:36:32.3438019Z check      = functools.partial(<function check_sample_weight_equivalence_on_dense_data at 0xd8591e88>, 'LinearRegression')\n2025-03-28T06:36:32.3438293Z estimator  = LinearRegression(positive=True)\n2025-03-28T06:36:32.3438559Z request    = <FixtureRequest for <Function test_estimators[LinearRegression(positive=True)-check_sample_weight_equivalence_on_dense_data]>>\n2025-03-28T06:36:32.3438707Z \n2025-03-28T06:36:32.3439155Z \u001b[1m\u001b[31m/io/sklearn/tests/test_common.py\u001b[0m:122: \n2025-03-28T06:36:32.3439405Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n2025-03-28T06:36:32.3439768Z \u001b[1m\u001b[31m/io/sklearn/utils/estimator_checks.py\u001b[0m:1570: in check_sample_weight_equivalence_on_dense_data\n2025-03-28T06:36:32.3440046Z     _check_sample_weight_equivalence(name, estimator_orig, sparse_container=None)\n2025-03-28T06:36:32.3440296Z         estimator_orig = LinearRegression(positive=True)\n2025-03-28T06:36:32.3440498Z         name       = 'LinearRegression'\n2025-03-28T06:36:32.3440774Z \u001b[1m\u001b[31m/io/sklearn/utils/_testing.py\u001b[0m:145: in wrapper\n2025-03-28T06:36:32.3440988Z     return fn(*args, **kwargs)\n2025-03-28T06:36:32.3441218Z         args       = ('LinearRegression', LinearRegression(positive=True))\n2025-03-28T06:36:32.3441452Z         fn         = <function _check_sample_weight_equivalence at 0xd8591de8>\n2025-03-28T06:36:32.3441744Z         kwargs     = {'sparse_container': None}\n2025-03-28T06:36:32.3441952Z         self       = _IgnoreWarnings(record=True)\n2025-03-28T06:36:32.3442307Z \u001b[1m\u001b[31m/io/sklearn/utils/estimator_checks.py\u001b[0m:1566: in _check_sample_weight_equivalence\n2025-03-28T06:36:32.3442654Z     assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)\n2025-03-28T06:36:32.3442905Z         X          = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n2025-03-28T06:36:32.3443158Z         0.15599452, 0.05808361, 0.86617615, 0.6011..., 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n2025-03-28T06:36:32.3443423Z         0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072]])\n2025-03-28T06:36:32.3443662Z         X_pred1    = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.3443929Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.3444177Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.3444412Z         X_pred2    = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.3444772Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.3444987Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.3445218Z         X_repeated = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n2025-03-28T06:36:32.3445494Z         0.15599452, 0.05808361, 0.86617615, 0.6011..., 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n2025-03-28T06:36:32.3445740Z         0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072]])\n2025-03-28T06:36:32.3445976Z         X_weighted = array([[0.60754485, 0.17052412, 0.06505159, 0.94888554, 0.96563203,\n2025-03-28T06:36:32.3446499Z         0.80839735, 0.30461377, 0.09767211, 0.6842..., 0.69673717, 0.62894285, 0.87747201, 0.73507104,\n2025-03-28T06:36:32.3446765Z         0.80348093, 0.28203457, 0.17743954, 0.75061475, 0.80683474]])\n2025-03-28T06:36:32.3447241Z         err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.3447553Z         estimator_orig = LinearRegression(positive=True)\n2025-03-28T06:36:32.3448930Z         estimator_repeated = LinearRegression(positive=True)\n2025-03-28T06:36:32.3449323Z         estimator_weighted = LinearRegression(positive=True)\n2025-03-28T06:36:32.3449572Z         method     = 'predict'\n2025-03-28T06:36:32.3449847Z         n_samples  = 15\n2025-03-28T06:36:32.3450076Z         name       = 'LinearRegression'\n2025-03-28T06:36:32.3450329Z         rng        = RandomState(MT19937) at 0xCB6EECE8\n2025-03-28T06:36:32.3450580Z         sparse_container = None\n2025-03-28T06:36:32.3450850Z         sw         = array([3, 4, 0, 3, 1, 0, 4, 4, 0, 3, 0, 0, 3, 2, 0])\n2025-03-28T06:36:32.3451124Z         y          = array([0, 1, 2, 2, 1, 1, 2, 2, 1, 2, 0, 0, 1, 1, 1])\n2025-03-28T06:36:32.3451409Z         y_repeated = array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n2025-03-28T06:36:32.3451823Z        1, 1, 1, 1, 1])\n2025-03-28T06:36:32.3452390Z         y_weighted = array([1, 2, 1, 2, 1, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1])\n2025-03-28T06:36:32.3467337Z \u001b[1m\u001b[31m/io/sklearn/utils/_testing.py\u001b[0m:283: in assert_allclose_dense_sparse\n2025-03-28T06:36:32.3468147Z     assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)\n2025-03-28T06:36:32.3468478Z         atol       = 1e-09\n2025-03-28T06:36:32.3468965Z         err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.3469366Z         rtol       = 1e-07\n2025-03-28T06:36:32.3469659Z         x          = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.3470005Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.3470354Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.3470648Z         y          = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.3470947Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.3471226Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.3471533Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n2025-03-28T06:36:32.3471702Z \n2025-03-28T06:36:32.3472165Z actual = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.3472656Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.3473239Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.3473561Z desired = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.3473863Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.3474355Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.3474896Z rtol = 1e-07, atol = 1e-09, equal_nan = True\n2025-03-28T06:36:32.3475293Z err_msg = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.3475668Z verbose = True\n2025-03-28T06:36:32.3475847Z \n2025-03-28T06:36:32.3476109Z     def assert_allclose(\n2025-03-28T06:36:32.3476425Z         actual, desired, rtol=None, atol=0.0, equal_nan=True, err_msg=\"\", verbose=True\n2025-03-28T06:36:32.3476851Z     ):\n2025-03-28T06:36:32.3477457Z         \"\"\"dtype-aware variant of numpy.testing.assert_allclose\n2025-03-28T06:36:32.3477743Z     \n2025-03-28T06:36:32.3478184Z         This variant introspects the least precise floating point dtype\n2025-03-28T06:36:32.3478502Z         in the input argument and automatically sets the relative tolerance\n2025-03-28T06:36:32.3478839Z         parameter to 1e-4 float32 and use 1e-7 otherwise (typically float64\n2025-03-28T06:36:32.3479134Z         in scikit-learn).\n2025-03-28T06:36:32.3479381Z     \n2025-03-28T06:36:32.3479837Z         `atol` is always left to 0. by default. It should be adjusted manually\n2025-03-28T06:36:32.3480176Z         to an assertion-specific value in case there are null values expected\n2025-03-28T06:36:32.3480467Z         in `desired`.\n2025-03-28T06:36:32.3480718Z     \n2025-03-28T06:36:32.3480995Z         The aggregate tolerance is `atol + rtol * abs(desired)`.\n2025-03-28T06:36:32.3481285Z     \n2025-03-28T06:36:32.3481704Z         Parameters\n2025-03-28T06:36:32.3481965Z         ----------\n2025-03-28T06:36:32.3482420Z         actual : array_like\n2025-03-28T06:36:32.3483027Z             Array obtained.\n2025-03-28T06:36:32.3483323Z         desired : array_like\n2025-03-28T06:36:32.3483598Z             Array desired.\n2025-03-28T06:36:32.3483881Z         rtol : float, optional, default=None\n2025-03-28T06:36:32.3484165Z             Relative tolerance.\n2025-03-28T06:36:32.3484462Z             If None, it is set based on the provided arrays' dtypes.\n2025-03-28T06:36:32.3484791Z         atol : float, optional, default=0.\n2025-03-28T06:36:32.3485072Z             Absolute tolerance.\n2025-03-28T06:36:32.3485449Z         equal_nan : bool, optional, default=True\n2025-03-28T06:36:32.3485916Z             If True, NaNs will compare equal.\n2025-03-28T06:36:32.3486379Z         err_msg : str, optional, default=''\n2025-03-28T06:36:32.3486669Z             The error message to be printed in case of failure.\n2025-03-28T06:36:32.3486959Z         verbose : bool, optional, default=True\n2025-03-28T06:36:32.3487445Z             If True, the conflicting values are appended to the error message.\n2025-03-28T06:36:32.3487721Z     \n2025-03-28T06:36:32.3487982Z         Raises\n2025-03-28T06:36:32.3488229Z         ------\n2025-03-28T06:36:32.3488486Z         AssertionError\n2025-03-28T06:36:32.3490170Z             If actual and desired are not equal up to specified precision.\n2025-03-28T06:36:32.3490710Z     \n2025-03-28T06:36:32.3491058Z         See Also\n2025-03-28T06:36:32.3491386Z         --------\n2025-03-28T06:36:32.3491708Z         numpy.testing.assert_allclose\n2025-03-28T06:36:32.3492016Z     \n2025-03-28T06:36:32.3492308Z         Examples\n2025-03-28T06:36:32.3493161Z         --------\n2025-03-28T06:36:32.3493425Z         >>> import numpy as np\n2025-03-28T06:36:32.3493712Z         >>> from sklearn.utils._testing import assert_allclose\n2025-03-28T06:36:32.3493988Z         >>> x = [1e-5, 1e-3, 1e-1]\n2025-03-28T06:36:32.3494270Z         >>> y = np.arccos(np.cos(x))\n2025-03-28T06:36:32.3494725Z         >>> assert_allclose(x, y, rtol=1e-5, atol=0)\n2025-03-28T06:36:32.3495044Z         >>> a = np.full(shape=10, fill_value=1e-5, dtype=np.float32)\n2025-03-28T06:36:32.3495339Z         >>> assert_allclose(a, 1e-5)\n2025-03-28T06:36:32.3495595Z         \"\"\"\n2025-03-28T06:36:32.3496358Z         dtypes = []\n2025-03-28T06:36:32.3496662Z     \n2025-03-28T06:36:32.3496955Z         actual, desired = np.asanyarray(actual), np.asanyarray(desired)\n2025-03-28T06:36:32.3497629Z         dtypes = [actual.dtype, desired.dtype]\n2025-03-28T06:36:32.3497899Z     \n2025-03-28T06:36:32.3498178Z         if rtol is None:\n2025-03-28T06:36:32.3498471Z             rtols = [1e-4 if dtype == np.float32 else 1e-7 for dtype in dtypes]\n2025-03-28T06:36:32.3498764Z             rtol = max(rtols)\n2025-03-28T06:36:32.3499011Z     \n2025-03-28T06:36:32.3604475Z >       np_assert_allclose(\n2025-03-28T06:36:32.3607071Z             actual,\n2025-03-28T06:36:32.3608138Z             desired,\n2025-03-28T06:36:32.3608886Z             rtol=rtol,\n2025-03-28T06:36:32.3609217Z             atol=atol,\n2025-03-28T06:36:32.3625173Z             equal_nan=equal_nan,\n2025-03-28T06:36:32.3639658Z             err_msg=err_msg,\n2025-03-28T06:36:32.3640151Z             verbose=verbose,\n2025-03-28T06:36:32.3640425Z         )\n2025-03-28T06:36:32.3640827Z \u001b[1m\u001b[31mE       AssertionError: \u001b[0m\n2025-03-28T06:36:32.3641237Z \u001b[1m\u001b[31mE       Not equal to tolerance rtol=1e-07, atol=1e-09\u001b[0m\n2025-03-28T06:36:32.3641778Z \u001b[1m\u001b[31mE       Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\u001b[0m\n2025-03-28T06:36:32.3642255Z \u001b[1m\u001b[31mE       Mismatched elements: 6 / 15 (40%)\u001b[0m\n2025-03-28T06:36:32.3642821Z \u001b[1m\u001b[31mE       Max absolute difference among violations: 2.51014256\u001b[0m\n2025-03-28T06:36:32.3643306Z \u001b[1m\u001b[31mE       Max relative difference among violations: 2.17024526\u001b[0m\n2025-03-28T06:36:32.3643773Z \u001b[1m\u001b[31mE        ACTUAL: array([ 8.881784e-16,  1.000000e+00,  2.000000e+00,  1.185498e+00,\u001b[0m\n2025-03-28T06:36:32.3644416Z \u001b[1m\u001b[31mE               4.062418e+00,  1.000000e+00,  2.000000e+00,  2.000000e+00,\u001b[0m\n2025-03-28T06:36:32.3644878Z \u001b[1m\u001b[31mE               4.105658e+00,  2.000000e+00, -2.799363e-02, -8.906428e-01,\u001b[0m\n2025-03-28T06:36:32.3645289Z \u001b[1m\u001b[31mE              -8.008100e-01,  1.000000e+00,  1.000000e+00])\u001b[0m\n2025-03-28T06:36:32.3645729Z \u001b[1m\u001b[31mE        DESIRED: array([0.      , 1.      , 2.      , 0.941865, 1.726709, 1.      ,\u001b[0m\n2025-03-28T06:36:32.3646166Z \u001b[1m\u001b[31mE              2.      , 2.      , 1.872389, 2.      , 1.508778, 0.761074,\u001b[0m\n2025-03-28T06:36:32.3646542Z \u001b[1m\u001b[31mE              1.709333, 1.      , 1.      ])\u001b[0m\n2025-03-28T06:36:32.3646727Z \n2025-03-28T06:36:32.3647013Z actual     = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.3647353Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.3647675Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.3647970Z atol       = 1e-09\n2025-03-28T06:36:32.3648247Z desired    = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.3648536Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.3648814Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.3649119Z dtypes     = [dtype('float64'), dtype('float64')]\n2025-03-28T06:36:32.3650674Z equal_nan  = True\n2025-03-28T06:36:32.3664015Z err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.3664453Z rtol       = 1e-07\n2025-03-28T06:36:32.3664740Z verbose    = True\n2025-03-28T06:36:32.3664894Z \n2025-03-28T06:36:32.3665331Z \u001b[1m\u001b[31m/io/sklearn/utils/_testing.py\u001b[0m:237: AssertionError\n2025-03-28T06:36:32.3665827Z \u001b[31m\u001b[1m_________________________ test_check_estimator_clones __________________________\u001b[0m\n2025-03-28T06:36:32.3666186Z \n2025-03-28T06:36:32.3666476Z     def test_check_estimator_clones():\n2025-03-28T06:36:32.3666804Z         # check that check_estimator doesn't modify the estimator it receives\n2025-03-28T06:36:32.3667086Z     \n2025-03-28T06:36:32.3667500Z         iris = load_iris()\n2025-03-28T06:36:32.3667744Z     \n2025-03-28T06:36:32.3667984Z         for Estimator in [\n2025-03-28T06:36:32.3668245Z             GaussianMixture,\n2025-03-28T06:36:32.3668775Z             LinearRegression,\n2025-03-28T06:36:32.3669031Z             SGDClassifier,\n2025-03-28T06:36:32.3669267Z             PCA,\n2025-03-28T06:36:32.3669511Z             MiniBatchKMeans,\n2025-03-28T06:36:32.3669766Z         ]:\n2025-03-28T06:36:32.3670005Z             # without fitting\n2025-03-28T06:36:32.3670276Z             with ignore_warnings(category=ConvergenceWarning):\n2025-03-28T06:36:32.3670550Z                 est = Estimator()\n2025-03-28T06:36:32.3670803Z                 set_random_state(est)\n2025-03-28T06:36:32.3671081Z                 old_hash = joblib.hash(est)\n2025-03-28T06:36:32.3671475Z >               check_estimator(\n2025-03-28T06:36:32.3671766Z                     est, expected_failed_checks=_get_expected_failed_checks(est)\n2025-03-28T06:36:32.3672037Z                 )\n2025-03-28T06:36:32.3672196Z \n2025-03-28T06:36:32.3672459Z Estimator  = <class 'sklearn.linear_model._base.LinearRegression'>\n2025-03-28T06:36:32.3672931Z est        = LinearRegression()\n2025-03-28T06:36:32.3673224Z iris       = {'data': array([[5.1, 3.5, 1.4, 0.2],\n2025-03-28T06:36:32.3673509Z        [4.9, 3. , 1.4, 0.2],\n2025-03-28T06:36:32.3673763Z        [4.7, 3.2, 1.3, 0.2],\n2025-03-28T06:36:32.3674269Z        [4.6, 3.1, 1.5,... width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n2025-03-28T06:36:32.3674627Z old_hash   = 'fdcbee8ed611695d1e19a9bdabd615ac'\n2025-03-28T06:36:32.3674814Z \n2025-03-28T06:36:32.3675204Z \u001b[1m\u001b[31m/io/sklearn/utils/tests/test_estimator_checks.py\u001b[0m:919: \n2025-03-28T06:36:32.3675540Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n2025-03-28T06:36:32.3675945Z \u001b[1m\u001b[31m/io/sklearn/utils/_param_validation.py\u001b[0m:218: in wrapper\n2025-03-28T06:36:32.3676248Z     return func(*args, **kwargs)\n2025-03-28T06:36:32.3676516Z         args       = (LinearRegression(),)\n2025-03-28T06:36:32.3676790Z         func       = <function check_estimator at 0xd8591668>\n2025-03-28T06:36:32.3677211Z         func_sig   = <Signature (estimator=None, generate_only=False, *, legacy: 'bool' = True, expected_failed_checks: 'dict[str, str] | N...al['warn'] | None\" = 'warn', on_fail: \"Literal['raise', 'warn'] | None\" = 'raise', callback: 'Callable | None' = None)>\n2025-03-28T06:36:32.3677607Z         global_skip_validation = False\n2025-03-28T06:36:32.3677881Z         kwargs     = {'expected_failed_checks': {}}\n2025-03-28T06:36:32.3678234Z         parameter_constraints = {'callback': [<built-in function callable>, None], 'expected_failed_checks': [<class 'dict'>, None], 'generate_only': ['boolean'], 'legacy': ['boolean'], ...}\n2025-03-28T06:36:32.3678659Z         params     = {'callback': None, 'estimator': LinearRegression(), 'expected_failed_checks': {}, 'generate_only': False, ...}\n2025-03-28T06:36:32.3678985Z         prefer_skip_nested_validation = False\n2025-03-28T06:36:32.3679252Z         to_ignore  = ['self', 'cls']\n2025-03-28T06:36:32.3679647Z \u001b[1m\u001b[31m/io/sklearn/utils/estimator_checks.py\u001b[0m:856: in check_estimator\n2025-03-28T06:36:32.3679964Z     check(estimator)\n2025-03-28T06:36:32.3680221Z         callback   = None\n2025-03-28T06:36:32.3680536Z         check      = functools.partial(<function check_sample_weight_equivalence_on_dense_data at 0xd8591e88>, 'LinearRegression')\n2025-03-28T06:36:32.3680953Z         check_result = {'check_name': 'check_sample_weight_equivalence_on_dense_data', 'estimator': LinearRegression(), 'exception': None, 'expected_to_fail': False, ...}\n2025-03-28T06:36:32.3681402Z         estimator  = LinearRegression(positive=True)\n2025-03-28T06:36:32.3681697Z         expected_failed_checks = {}\n2025-03-28T06:36:32.3681899Z         generate_only = False\n2025-03-28T06:36:32.3682090Z         legacy     = True\n2025-03-28T06:36:32.3682266Z         name       = 'LinearRegression'\n2025-03-28T06:36:32.3682442Z         on_fail    = 'raise'\n2025-03-28T06:36:32.3682732Z         on_skip    = 'warn'\n2025-03-28T06:36:32.3682922Z         reason     = 'Check is not expected to fail'\n2025-03-28T06:36:32.3683274Z         test_can_fail = False\n2025-03-28T06:36:32.3683596Z         test_results = [{'check_name': 'check_estimator_cloneable', 'estimator': LinearRegression(), 'exception': None, 'expected_to_fail': F...k_no_attributes_set_in_init', 'estimator': LinearRegression(), 'exception': None, 'expected_to_fail': False, ...}, ...]\n2025-03-28T06:36:32.3684106Z \u001b[1m\u001b[31m/io/sklearn/utils/estimator_checks.py\u001b[0m:1570: in check_sample_weight_equivalence_on_dense_data\n2025-03-28T06:36:32.3684400Z     _check_sample_weight_equivalence(name, estimator_orig, sparse_container=None)\n2025-03-28T06:36:32.3684731Z         estimator_orig = LinearRegression(positive=True)\n2025-03-28T06:36:32.3684930Z         name       = 'LinearRegression'\n2025-03-28T06:36:32.3685217Z \u001b[1m\u001b[31m/io/sklearn/utils/_testing.py\u001b[0m:145: in wrapper\n2025-03-28T06:36:32.3685439Z     return fn(*args, **kwargs)\n2025-03-28T06:36:32.3685650Z         args       = ('LinearRegression', LinearRegression(positive=True))\n2025-03-28T06:36:32.3685891Z         fn         = <function _check_sample_weight_equivalence at 0xd8591de8>\n2025-03-28T06:36:32.3686105Z         kwargs     = {'sparse_container': None}\n2025-03-28T06:36:32.3686316Z         self       = _IgnoreWarnings(record=True)\n2025-03-28T06:36:32.3686643Z \u001b[1m\u001b[31m/io/sklearn/utils/estimator_checks.py\u001b[0m:1566: in _check_sample_weight_equivalence\n2025-03-28T06:36:32.3686902Z     assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)\n2025-03-28T06:36:32.3687138Z         X          = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n2025-03-28T06:36:32.3687418Z         0.15599452, 0.05808361, 0.86617615, 0.6011..., 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n2025-03-28T06:36:32.3687667Z         0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072]])\n2025-03-28T06:36:32.3687906Z         X_pred1    = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.3688173Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.3688439Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.3688661Z         X_pred2    = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.3688871Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.3689077Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.3689323Z         X_repeated = array([[0.37454012, 0.95071431, 0.73199394, 0.59865848, 0.15601864,\n2025-03-28T06:36:32.3689588Z         0.15599452, 0.05808361, 0.86617615, 0.6011..., 0.98663958, 0.3742708 , 0.37064215, 0.81279957,\n2025-03-28T06:36:32.3689834Z         0.94724858, 0.98600106, 0.75337819, 0.37625959, 0.08350072]])\n2025-03-28T06:36:32.3690073Z         X_weighted = array([[0.60754485, 0.17052412, 0.06505159, 0.94888554, 0.96563203,\n2025-03-28T06:36:32.3690355Z         0.80839735, 0.30461377, 0.09767211, 0.6842..., 0.69673717, 0.62894285, 0.87747201, 0.73507104,\n2025-03-28T06:36:32.3690606Z         0.80348093, 0.28203457, 0.17743954, 0.75061475, 0.80683474]])\n2025-03-28T06:36:32.3690894Z         err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.3691216Z         estimator_orig = LinearRegression(positive=True)\n2025-03-28T06:36:32.3691435Z         estimator_repeated = LinearRegression(positive=True)\n2025-03-28T06:36:32.3691653Z         estimator_weighted = LinearRegression(positive=True)\n2025-03-28T06:36:32.3691853Z         method     = 'predict'\n2025-03-28T06:36:32.3692029Z         n_samples  = 15\n2025-03-28T06:36:32.3692223Z         name       = 'LinearRegression'\n2025-03-28T06:36:32.3692420Z         rng        = RandomState(MT19937) at 0xCD5304A8\n2025-03-28T06:36:32.3692720Z         sparse_container = None\n2025-03-28T06:36:32.3692923Z         sw         = array([3, 4, 0, 3, 1, 0, 4, 4, 0, 3, 0, 0, 3, 2, 0])\n2025-03-28T06:36:32.3693289Z         y          = array([0, 1, 2, 2, 1, 1, 2, 2, 1, 2, 0, 0, 1, 1, 1])\n2025-03-28T06:36:32.3693514Z         y_repeated = array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n2025-03-28T06:36:32.4389210Z        1, 1, 1, 1, 1])\n2025-03-28T06:36:32.4392227Z         y_weighted = array([1, 2, 1, 2, 1, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1])\n2025-03-28T06:36:32.4394205Z \u001b[1m\u001b[31m/io/sklearn/utils/_testing.py\u001b[0m:283: in assert_allclose_dense_sparse\n2025-03-28T06:36:32.4395241Z     assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)\n2025-03-28T06:36:32.4395958Z         atol       = 1e-09\n2025-03-28T06:36:32.4396339Z         err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.4396710Z         rtol       = 1e-07\n2025-03-28T06:36:32.4397020Z         x          = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.4397420Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.4397769Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.4398083Z         y          = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.4398404Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.4398711Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.4399020Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n2025-03-28T06:36:32.4399360Z \n2025-03-28T06:36:32.4399684Z actual = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.4400047Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.4400383Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.4400717Z desired = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.4401016Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.4401305Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.4401599Z rtol = 1e-07, atol = 1e-09, equal_nan = True\n2025-03-28T06:36:32.4401994Z err_msg = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.4402357Z verbose = True\n2025-03-28T06:36:32.4402649Z \n2025-03-28T06:36:32.4403197Z     def assert_allclose(\n2025-03-28T06:36:32.4403533Z         actual, desired, rtol=None, atol=0.0, equal_nan=True, err_msg=\"\", verbose=True\n2025-03-28T06:36:32.4403835Z     ):\n2025-03-28T06:36:32.4404125Z         \"\"\"dtype-aware variant of numpy.testing.assert_allclose\n2025-03-28T06:36:32.4404412Z     \n2025-03-28T06:36:32.4404713Z         This variant introspects the least precise floating point dtype\n2025-03-28T06:36:32.4405098Z         in the input argument and automatically sets the relative tolerance\n2025-03-28T06:36:32.4405430Z         parameter to 1e-4 float32 and use 1e-7 otherwise (typically float64\n2025-03-28T06:36:32.4405731Z         in scikit-learn).\n2025-03-28T06:36:32.4406156Z     \n2025-03-28T06:36:32.4406440Z         `atol` is always left to 0. by default. It should be adjusted manually\n2025-03-28T06:36:32.4406762Z         to an assertion-specific value in case there are null values expected\n2025-03-28T06:36:32.4407464Z         in `desired`.\n2025-03-28T06:36:32.4407766Z     \n2025-03-28T06:36:32.4408043Z         The aggregate tolerance is `atol + rtol * abs(desired)`.\n2025-03-28T06:36:32.4408313Z     \n2025-03-28T06:36:32.4408563Z         Parameters\n2025-03-28T06:36:32.4408818Z         ----------\n2025-03-28T06:36:32.4409095Z         actual : array_like\n2025-03-28T06:36:32.4409358Z             Array obtained.\n2025-03-28T06:36:32.4409820Z         desired : array_like\n2025-03-28T06:36:32.4410093Z             Array desired.\n2025-03-28T06:36:32.4410364Z         rtol : float, optional, default=None\n2025-03-28T06:36:32.4410653Z             Relative tolerance.\n2025-03-28T06:36:32.4410943Z             If None, it is set based on the provided arrays' dtypes.\n2025-03-28T06:36:32.4411236Z         atol : float, optional, default=0.\n2025-03-28T06:36:32.4411512Z             Absolute tolerance.\n2025-03-28T06:36:32.4411809Z         equal_nan : bool, optional, default=True\n2025-03-28T06:36:32.4412101Z             If True, NaNs will compare equal.\n2025-03-28T06:36:32.4412731Z         err_msg : str, optional, default=''\n2025-03-28T06:36:32.4413109Z             The error message to be printed in case of failure.\n2025-03-28T06:36:32.4413412Z         verbose : bool, optional, default=True\n2025-03-28T06:36:32.4413734Z             If True, the conflicting values are appended to the error message.\n2025-03-28T06:36:32.4414013Z     \n2025-03-28T06:36:32.4414256Z         Raises\n2025-03-28T06:36:32.4414514Z         ------\n2025-03-28T06:36:32.4414771Z         AssertionError\n2025-03-28T06:36:32.4415083Z             If actual and desired are not equal up to specified precision.\n2025-03-28T06:36:32.4415491Z     \n2025-03-28T06:36:32.4415753Z         See Also\n2025-03-28T06:36:32.4416185Z         --------\n2025-03-28T06:36:32.4416466Z         numpy.testing.assert_allclose\n2025-03-28T06:36:32.4416900Z     \n2025-03-28T06:36:32.4417154Z         Examples\n2025-03-28T06:36:32.4417419Z         --------\n2025-03-28T06:36:32.4417685Z         >>> import numpy as np\n2025-03-28T06:36:32.4418011Z         >>> from sklearn.utils._testing import assert_allclose\n2025-03-28T06:36:32.4418314Z         >>> x = [1e-5, 1e-3, 1e-1]\n2025-03-28T06:36:32.4418599Z         >>> y = np.arccos(np.cos(x))\n2025-03-28T06:36:32.4418899Z         >>> assert_allclose(x, y, rtol=1e-5, atol=0)\n2025-03-28T06:36:32.4419379Z         >>> a = np.full(shape=10, fill_value=1e-5, dtype=np.float32)\n2025-03-28T06:36:32.4420007Z         >>> assert_allclose(a, 1e-5)\n2025-03-28T06:36:32.4420262Z         \"\"\"\n2025-03-28T06:36:32.4420502Z         dtypes = []\n2025-03-28T06:36:32.4420743Z     \n2025-03-28T06:36:32.4421010Z         actual, desired = np.asanyarray(actual), np.asanyarray(desired)\n2025-03-28T06:36:32.4421324Z         dtypes = [actual.dtype, desired.dtype]\n2025-03-28T06:36:32.4421576Z     \n2025-03-28T06:36:32.4421816Z         if rtol is None:\n2025-03-28T06:36:32.4422102Z             rtols = [1e-4 if dtype == np.float32 else 1e-7 for dtype in dtypes]\n2025-03-28T06:36:32.4422404Z             rtol = max(rtols)\n2025-03-28T06:36:32.4422867Z     \n2025-03-28T06:36:32.4423151Z >       np_assert_allclose(\n2025-03-28T06:36:32.4423412Z             actual,\n2025-03-28T06:36:32.4423660Z             desired,\n2025-03-28T06:36:32.4423929Z             rtol=rtol,\n2025-03-28T06:36:32.4424278Z             atol=atol,\n2025-03-28T06:36:32.4424545Z             equal_nan=equal_nan,\n2025-03-28T06:36:32.4424804Z             err_msg=err_msg,\n2025-03-28T06:36:32.4425069Z             verbose=verbose,\n2025-03-28T06:36:32.4425335Z         )\n2025-03-28T06:36:32.4425695Z \u001b[1m\u001b[31mE       AssertionError: \u001b[0m\n2025-03-28T06:36:32.4426095Z \u001b[1m\u001b[31mE       Not equal to tolerance rtol=1e-07, atol=1e-09\u001b[0m\n2025-03-28T06:36:32.4426645Z \u001b[1m\u001b[31mE       Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.\u001b[0m\n2025-03-28T06:36:32.4427542Z \u001b[1m\u001b[31mE       Mismatched elements: 6 / 15 (40%)\u001b[0m\n2025-03-28T06:36:32.4427985Z \u001b[1m\u001b[31mE       Max absolute difference among violations: 2.51014256\u001b[0m\n2025-03-28T06:36:32.4428409Z \u001b[1m\u001b[31mE       Max relative difference among violations: 2.17024526\u001b[0m\n2025-03-28T06:36:32.4428858Z \u001b[1m\u001b[31mE        ACTUAL: array([ 8.881784e-16,  1.000000e+00,  2.000000e+00,  1.185498e+00,\u001b[0m\n2025-03-28T06:36:32.4429327Z \u001b[1m\u001b[31mE               4.062418e+00,  1.000000e+00,  2.000000e+00,  2.000000e+00,\u001b[0m\n2025-03-28T06:36:32.4429765Z \u001b[1m\u001b[31mE               4.105658e+00,  2.000000e+00, -2.799363e-02, -8.906428e-01,\u001b[0m\n2025-03-28T06:36:32.4430365Z \u001b[1m\u001b[31mE              -8.008100e-01,  1.000000e+00,  1.000000e+00])\u001b[0m\n2025-03-28T06:36:32.4430805Z \u001b[1m\u001b[31mE        DESIRED: array([0.      , 1.      , 2.      , 0.941865, 1.726709, 1.      ,\u001b[0m\n2025-03-28T06:36:32.4431250Z \u001b[1m\u001b[31mE              2.      , 2.      , 1.872389, 2.      , 1.508778, 0.761074,\u001b[0m\n2025-03-28T06:36:32.4431622Z \u001b[1m\u001b[31mE              1.709333, 1.      , 1.      ])\u001b[0m\n2025-03-28T06:36:32.4431810Z \n2025-03-28T06:36:32.4432416Z actual     = array([ 8.88178420e-16,  1.00000000e+00,  2.00000000e+00,  1.18549798e+00,\n2025-03-28T06:36:32.4433326Z         4.06241761e+00,  1.00000000e+00,  2...5767e+00,  2.00000000e+00, -2.79936287e-02, -8.90642835e-01,\n2025-03-28T06:36:32.4433865Z        -8.00809991e-01,  1.00000000e+00,  1.00000000e+00])\n2025-03-28T06:36:32.4434473Z atol       = 1e-09\n2025-03-28T06:36:32.4434795Z desired    = array([0.        , 1.        , 2.        , 0.94186541, 1.72670876,\n2025-03-28T06:36:32.4435122Z        1.        , 2.        , 2.        , 1.8723887 , 2.        ,\n2025-03-28T06:36:32.4435605Z        1.50877777, 0.76107365, 1.70933257, 1.        , 1.        ])\n2025-03-28T06:36:32.4435932Z dtypes     = [dtype('float64'), dtype('float64')]\n2025-03-28T06:36:32.4436253Z equal_nan  = True\n2025-03-28T06:36:32.4436639Z err_msg    = 'Comparing the output of LinearRegression.predict revealed that fitting with `sample_weight` is not equivalent to fitting with removed or repeated data points.'\n2025-03-28T06:36:32.4437185Z rtol       = 1e-07\n2025-03-28T06:36:32.4439658Z verbose    = True\n2025-03-28T06:36:32.4439971Z \n2025-03-28T06:36:32.4440407Z \u001b[1m\u001b[31m/io/sklearn/utils/_testing.py\u001b[0m:237: AssertionError\n```\n\n</details>\n\nLooking at the software runtime info of each I only see two differences:\n\n- the pip version;\n- the CPU model.\n\nAll other dependencies seem to match, including the openblas version inspected by threadpoolctl.\n\nEDIT: this is wrong, the scipy version is not the same and I missed it.",
    "comments": [
      {
        "user": "ogrisel",
        "body": "The pip version is probably not the culprit. Maybe this is an openblas problem when running in 32 bit only on some CPU models? However, both detect the BLAS kernels optimized for the Nehalem architecture."
      },
      {
        "user": "ogrisel",
        "body": "Also, the magnitude of the violations in the failing `assert_allclose` statement on the predicted values are definitely outside the range of machine precision level rounding errors:\n\n```\nMax absolute difference among violations: 2.51014256\nMax relative difference among violations: 2.17024526\n```"
      },
      {
        "user": "ogrisel",
        "body": "The two failures involve the same checks on `LinearRegression`, one with `positive=True` and the other with the default parameters."
      }
    ]
  },
  {
    "issue_number": 23533,
    "title": "RFECV race condition on estimator",
    "author": "cpiber",
    "state": "closed",
    "created_at": "2022-06-03T09:07:46Z",
    "updated_at": "2025-04-13T07:24:36Z",
    "labels": [
      "Bug",
      "module:feature_selection"
    ],
    "body": "In RFECV, at\r\nhttps://github.com/scikit-learn/scikit-learn/blob/fb3ed90fb501a755ce2938fb566bd0f6e2235054/sklearn/feature_selection/_rfe.py#L723-L726\r\nthe estimator is passed as-is to the fit function. Since `fit` modifies the object without copying, this is prone to race conditions (see example below).\r\n\r\nContrast this to BaseSearchCV, where the estimator is properly cloned:\r\nhttps://github.com/scikit-learn/scikit-learn/blob/fb3ed90fb501a755ce2938fb566bd0f6e2235054/sklearn/model_selection/_search.py#L823-L833\r\n\r\n---\r\n\r\n<details>\r\n<summary>On my system, with parameter `n_jobs=-1`, I got the following error:</summary>\r\n\r\n```\r\n5 fits failed with the following error:\r\nTraceback (most recent call last):\r\n  File \".../site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\r\n    estimator.fit(X_train, y_train, **fit_params)\r\n  File \".../site-packages/sklearn/feature_selection/_rfe.py\", line 723, in fit\r\n    scores = parallel(\r\n  File \".../site-packages/joblib/parallel.py\", line 1056, in __call__\r\n    self.retrieve()\r\n  File \".../site-packages/joblib/parallel.py\", line 935, in retrieve\r\n    self._output.extend(job.get(timeout=self.timeout))\r\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 771, in get\r\n    raise self._value\r\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \".../site-packages/joblib/_parallel_backends.py\", line 595, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \".../site-packages/joblib/parallel.py\", line 262, in __call__\r\n    return [func(*args, **kwargs)\r\n  File \".../site-packages/joblib/parallel.py\", line 262, in <listcomp>\r\n    return [func(*args, **kwargs)\r\n  File \".../site-packages/sklearn/utils/fixes.py\", line 117, in __call__\r\n    return self.function(*args, **kwargs)\r\n  File \".../site-packages/sklearn/feature_selection/_rfe.py\", line 37, in _rfe_single_fit\r\n    return rfe._fit(\r\n  File \".../site-packages/sklearn/feature_selection/_rfe.py\", line 327, in _fit\r\n    self.scores_.append(step_score(self.estimator_, features))\r\n  File \".../site-packages/sklearn/feature_selection/_rfe.py\", line 40, in <lambda>\r\n    lambda estimator, features: _score(\r\n  File \".../site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\r\n    scores = scorer(estimator, X_test, y_test)\r\n  File \".../site-packages/sklearn/metrics/_scorer.py\", line 219, in __call__\r\n    return self._score(\r\n  File \".../site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\r\n    y_pred = method_caller(estimator, \"predict\", X)\r\n  File \".../site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\r\n    return getattr(estimator, method)(*args, **kwargs)\r\n  File \".../site-packages/sklearn/ensemble/_forest.py\", line 835, in predict\r\n    return self.classes_.take(np.argmax(proba, axis=1), axis=0)\r\nAttributeError: 'list' object has no attribute 'take'\r\n```\r\n</details>\r\n\r\nIt is generated from the following snippet:\r\n```py\r\n  rf = RandomForestClassifier()\r\n  rfecv = RFECV(rf, scoring='accuracy', n_jobs=-1)\r\n  rfecv.fit(X_train, y_train)\r\n```\r\n\r\nThe error appears to happen because `n_outputs_` is not constant between runs. The error does not happen without parallelism.",
    "comments": [
      {
        "user": "glemaitre",
        "body": "We should be cloning `self.estimator` in the parallel call I think.\r\nWe usually always do that. Indeed, we don't want to mutate `self.estimator` if we follow our own API."
      },
      {
        "user": "jeremiedbb",
        "body": "No reproducer could be made in #23560, and that I couldn't make a reproducer either.\nIt is possible that whatever it was, it was fixed as a side effect of https://github.com/scikit-learn/scikit-learn/pull/30176.\n\nWith that in mind, I'm closing this issue. Feel free to reopen and provide a reproducer if you think that the bug still exists."
      },
      {
        "user": "cpiber",
        "body": "Yes, I think that was it, thanks!"
      }
    ]
  }
]