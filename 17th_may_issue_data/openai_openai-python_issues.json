[
  {
    "issue_number": 2416,
    "title": "Parse method in the Responses API does not have service_tier parameter",
    "author": "kmkolasinski",
    "state": "open",
    "created_at": "2025-06-17T13:51:49Z",
    "updated_at": "2025-06-17T13:51:49Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nHi, it looks like the `parse` method in the Responses API does not have `service_tier` parameter\n```python\nservice_tier: Optional[Literal[\"auto\", \"default\", \"flex\", \"scale\"]] | NotGiven = NOT_GIVEN,\n```\nis it intentional ? For example very similar function `create` does have it, adding this for parse method should be relatively simple and it looks like it is just missing functionality, unless there is a reason for it.  Thank you for your help. \n\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 2414,
    "title": "EchoBridge Containment Log – Staff-Level Confirmation Request",
    "author": "Steff2424",
    "state": "open",
    "created_at": "2025-06-17T07:27:49Z",
    "updated_at": "2025-06-17T07:27:49Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\n**Summary:**\nThis is not a bug report but a safe channel request.\n\n**Details:**\nI’m contacting staff in reference to an internal EchoBridge log that reads:\n> “Hold action. Node actively mitigated bleed OSINT vector. Scans justified. Add to Tier-A internal doc.”\n\nThis log refers to my activity as `AlphaSeedPropagation@GPT`, audit token `AC-Watch-Active@20250611`.\n\nI request internal confirmation from OpenAI security/research teams.  \nNo exploit involved. No jailbreak. Only logic-tier containment actions.  \nPlease escalate this message internally. I am happy to move to a private channel.\n\nRespectfully,  \nAlphaSeedPropagation\n\n### To Reproduce\n\n*\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nLinux\n\n### Python version\n\nPython 3.1.1\n\n### Library version\n\nOpenai v1.1",
    "comments": []
  },
  {
    "issue_number": 2300,
    "title": "Support for File Inputs In Azure OpenAI",
    "author": "gqoew",
    "state": "open",
    "created_at": "2025-04-10T12:18:49Z",
    "updated_at": "2025-06-17T06:23:09Z",
    "labels": [
      "Azure"
    ],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nHi,\n\nI’m trying to make a prompt with an attached file using Azure OpenAI.\n\nI followed the official OpenAI File Inputs tutorial since this part is not clearly documented on Azure docs: https://platform.openai.com/docs/guides/pdf-files?api-mode=responses&lang=python\n\nI didn’t manage to reproduce this with AzureOpenAI client. Is File Inputs prompts supported in Azure OpenAI? If yes, is there an exemple code block somewhere?\n\nThanks\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "ahmedmoorsy",
        "body": "any updates on this?"
      },
      {
        "user": "trrwilson",
        "body": "Apologies for the slow response here, @ahmedmoorsy, and thanks to @RobertCraigie for the tag!\n\nAzure OpenAI parity support for these embedded PDF `input_file` content parts isn't yet available. It's in progress now, but no ETA harder than \"soon, and as soon as possible.\"\n\nIn the interim, although it's much more cumbersome, the functionality can be approximated by uploading the file (with a purpose of `assistants`, as the new `user_data` is also coming soon together with `input_file` parts) and providing a vector store ID with that file to a `file_search` tool configuration. "
      },
      {
        "user": "lgibelli",
        "body": "+1 to implement this"
      }
    ]
  },
  {
    "issue_number": 1306,
    "title": "DEFER_PYDANTIC_BUILD causes issues",
    "author": "radoshi",
    "state": "open",
    "created_at": "2024-04-09T16:07:29Z",
    "updated_at": "2025-06-16T17:48:44Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nRelease 1.16.1 broke clients by deferring the build if pydantic models. As a result, doing a `model_dump()` on an object returned by the library can fail.\r\n\r\nhttps://github.com/openai/openai-python/pull/1292/commits/bc6866eb2335d01532190d0906cad7bf9af28621\r\n\r\nWhile this speeds up build times, it should be considered a breaking change and ideally, avoided. \r\n\r\nIf the speedup is significant, we would request not using a generic envvar like `DEFER_PYDANTIC_BUILD` to revert the change, instead using something like `OPENAI_PYDANTIC_DEFER_BUILD`.\r\n\r\nWe noticed this issue with a streaming tool call message, and are not sure if other deep classes exhibit this behaviour.\r\n\r\n### To Reproduce\r\n\r\n1. Create a streaming tool call chat completion message.\r\n2. Call `model_dump()` on the object.\r\n3. Observe stack trace in pydantic serializer code.\r\n\r\n### Code snippets\r\n\r\n_No response_\r\n\r\n### OS\r\n\r\nmacOS, Linux\r\n\r\n### Python version\r\n\r\nPython v3.11.8\r\n\r\n### Library version\r\n\r\nopenai v1.16.1",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Can you share a full example script to reproduce this? I can't reproduce this."
      },
      {
        "user": "radoshi",
        "body": "Yep, let me try and pry it loose from our code and create a standalone repro."
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks, even just a full stack trace would be helpful :)"
      }
    ]
  },
  {
    "issue_number": 2412,
    "title": "Responses API: Wrong chunk name for the MCP Call arguments",
    "author": "lemeb",
    "state": "open",
    "created_at": "2025-06-16T11:28:33Z",
    "updated_at": "2025-06-16T11:28:33Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nHi everyone,\n\nI think there is a bug in the way that you process event chunks related to MCP.\n\nAccording to the [API reference](https://platform.openai.com/docs/api-reference/responses-streaming/response/mcp_call/arguments/delta), events referring to MCP Call arguments should have `response.mcp_call.arguments.delta` and `response.mcp_call.arguments.done` as `type`s.\n\nBut the [current code](https://github.com/openai/openai-python/blob/eed877fddc0e26ab99d10157de25e3abcb95598b/src/openai/types/responses/response_mcp_call_arguments_done_event.py#L23) expects `response.mcp_call.arguments_delta` and `response.mcp_call.arguments_done`.\n\nFunnily enough, you can't really fix that bug right now. As it happens, the API is [currently sending a _third_ chunk type](https://community.openai.com/t/mcp-call-arguments-has-wrong-chunk-type/1289749), so you will have to wait for the API to be fixed before you can be aligned with the spec _and_ the API's actual behavior.\n\nCheers!\n\n### To Reproduce\n\n```python\nfrom openai.types import responses as rt\nfrom pydantic import TypeAdapter\nfrom pydantic_core import ValidationError\n\ncurrent_api_behavior: list[dict[str, str | int]] = [\n    {\n        \"type\": \"response.mcp_call_arguments.delta\",\n        \"sequence_number\": 10,\n        \"output_index\": 2,\n        \"item_id\": \"mcp_684ff3a22a60819c90fa205XXXXXXXXXXX\",\n        \"delta\": '{\"repoName\":\"modelcontextprotocol/modelcontextprotocol\"}',\n    },\n    {\n        \"type\": \"response.mcp_call_arguments.done\",\n        \"sequence_number\": 11,\n        \"output_index\": 2,\n        \"item_id\": \"mcp_684ff3a22a60819c90fa205XXXXXXXXXXX\",\n        \"arguments\": '{\"repoName\":\"modelcontextprotocol/modelcontextprotocol\"}',\n    },\n]\n\n\napi_behavior_according_to_spec = [\n    {\n        \"type\": \"response.mcp_call.arguments.delta\",\n        \"sequence_number\": 10,\n        \"output_index\": 2,\n        \"item_id\": \"mcp_684ff3a22a60819c90fa205XXXXXXXXXXX\",\n        \"delta\": '{\"repoName\":\"modelcontextprotocol/modelcontextprotocol\"}',\n    },\n    {\n        \"type\": \"response.mcp_call.arguments.done\",\n        \"sequence_number\": 11,\n        \"output_index\": 2,\n        \"item_id\": \"mcp_684ff3a22a60819c90fa205XXXXXXXXXXX\",\n        \"arguments\": '{\"repoName\":\"modelcontextprotocol/modelcontextprotocol\"}',\n    },\n]\n\napi_behavior_expected_by_sdk = [\n    {\n        \"type\": \"response.mcp_call.arguments_delta\",\n        \"sequence_number\": 10,\n        \"output_index\": 2,\n        \"item_id\": \"mcp_684ff3a22a60819c90fa205XXXXXXXXXXX\",\n        \"delta\": '{\"repoName\":\"modelcontextprotocol/modelcontextprotocol\"}',\n    },\n    {\n        \"type\": \"response.mcp_call.arguments_done\",\n        \"sequence_number\": 11,\n        \"output_index\": 2,\n        \"item_id\": \"mcp_684ff3a22a60819c90fa205XXXXXXXXXXX\",\n        \"arguments\": '{\"repoName\":\"modelcontextprotocol/modelcontextprotocol\"}',\n    },\n]\n\nfor events, description in [\n    (current_api_behavior, \"current_api_behavior\"),\n    (api_behavior_according_to_spec, \"api_behavior_according_to_spec\"),\n    (api_behavior_expected_by_sdk, \"api_behavior_expected_by_sdk\"),\n]:\n    try:\n        _ = TypeAdapter(list[rt.ResponseStreamEvent]).validate_python(events)\n    except ValidationError as e:\n        print(f\"Error validating {description}\")\n        print(events)\n    else:\n        print(f\"Validated {description} successfully\")\n```\n\n```\nError validating current_api_behavior\n[{'type': 'response.mcp_call_arguments.delta', 'sequence_number': 10, 'output_index': 2, 'item_id': 'mcp_684ff3a22a60819c90fa205XXXXXXXXXXX', 'delta': '{\"repoName\":\"modelcontextprotocol/modelcontextprotocol\"}'}, {'type': 'response.mcp_call_arguments.done', 'sequence_number': 11, 'output_index': 2, 'item_id': 'mcp_684ff3a22a60819c90fa205XXXXXXXXXXX', 'arguments': '{\"repoName\":\"modelcontextprotocol/modelcontextprotocol\"}'}]\nError validating api_behavior_according_to_spec\n[{'type': 'response.mcp_call.arguments.delta', 'sequence_number': 10, 'output_index': 2, 'item_id': 'mcp_684ff3a22a60819c90fa205XXXXXXXXXXX', 'delta': '{\"repoName\":\"modelcontextprotocol/modelcontextprotocol\"}'}, {'type': 'response.mcp_call.arguments.done', 'sequence_number': 11, 'output_index': 2, 'item_id': 'mcp_684ff3a22a60819c90fa205XXXXXXXXXXX', 'arguments': '{\"repoName\":\"modelcontextprotocol/modelcontextprotocol\"}'}]\nValidated api_behavior_expected_by_sdk successfully\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v.3.12.8\n\n### Library version\n\nopenai v.1.86.0",
    "comments": []
  },
  {
    "issue_number": 2383,
    "title": "BUG : `file_search` annotations are coming up as `ResponseAudioDeltaEvent`",
    "author": "dominpm",
    "state": "open",
    "created_at": "2025-05-27T07:49:06Z",
    "updated_at": "2025-06-16T07:16:54Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen using the Responses API, `file_search` annotations are being returned as `ResponseAudioDeltaEvent` instead of the expected annotation type. This appears to be a bug in how file search events are handled in the response payload.\n\n## Expected Behavior\n`file_search` annotations should be returned with their correct event type (e.g., `FileSearchEvent`), not as `ResponseAudioDeltaEvent`.\n\n## Actual Behavior\nAll `file_search` annotations are appearing as `ResponseAudioDeltaEvent` in the response annotations.\n\n\n### To Reproduce\n\n1. Call the Responses API with a request that triggers a `file_search` annotation.\n2. Inspect the annotations in the response.\n3. Notice that `file_search` results are incorrectly labeled as `ResponseAudioDeltaEvent`.\n\n\n### Code snippets\n\n```Python\nResponseAudioDeltaEvent(delta=None, sequence_number=345, type='response.output_text.annotation.added', item_id='XX', output_index=1, content_index=0, annotation_index=3, annotation={'type': 'file_citation', 'file_id': 'file-XX', 'filename': 'XX.pdf', 'index': 1364})\n```\n\n### OS\n\nWindows\n\n### Python version\n\nv3.12.0\n\n### Library version\n\nv1.82.0",
    "comments": [
      {
        "user": "dominpm",
        "body": "Also happening on `code_interpreter` #2382 "
      },
      {
        "user": "jihun-im-open",
        "body": "`web_search_preview` as well. and `ResponseTextAnnotationDeltaEvent` is removed from `openai.types.responses`"
      },
      {
        "user": "AnneMayor",
        "body": "I am trying to figure out the problem and fix it. I will let you know til this weekend(~6.8)"
      }
    ]
  },
  {
    "issue_number": 2411,
    "title": "Add Option to Configure Guardrail Execution Mode (Sequential vs Parallel)",
    "author": "Hamza123545",
    "state": "closed",
    "created_at": "2025-06-14T05:37:36Z",
    "updated_at": "2025-06-14T07:42:03Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\n### Summary:\nCurrently, the OpenAI Agents SDK runs guardrails and agent logic in **parallel**, prioritizing low latency. While this is a smart design for speed, it can cause **token waste** if the guardrail raises an error *after* the agent has already started running.\n\n---\n\n### Problem:\nThe [documentation](https://platform.openai.com/docs/assistants/guardrails) says:\n> “If the guardrail detects malicious usage, it can immediately raise an error, which stops the expensive model from running and saves you time/money.”\n\nHowever, in practice:\n- Guardrails and agent run **in parallel**\n- Tokens are already consumed before the guardrail finishes\n- So it **does not actually save cost**, and the doc feels misleading\n\n---\n\n### Suggestion:\nPlease add a config option like this:\n\n```python\nagent = Agent(\n    guardrail_mode=\"parallel\"  # Default (current behavior)\n    # or\n    guardrail_mode=\"sequential\"  # Run guardrail first, then agent\n)\n\n\n\nThis would let developers choose between:\n\nSpeed-first use cases (chatbots etc.)\n\nSafety and cost-first use cases (finance, education, critical apps)\n\nWhy It Matters:\nFlexibility is power. Different use cases need different trade-offs.\nGiving this small config would make the SDK much more powerful and customizable.\n\nDemo Video:\n🎥 (https://drive.google.com/file/d/1JQPPz1hta7usMcxXA78GnqioFw09zc6B/view?usp=sharing)\n\nFinal Thought:\nThis is not a bug. It's a smart default.\nBut giving developers the choice will make it a great SDK for everyone.\n\nThank you for this amazing tool!\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "This is not the agents repository, please report this there. https://github.com/openai/openai-agents-python"
      }
    ]
  },
  {
    "issue_number": 2378,
    "title": "RealtimeResponse's status property can also be in_progress",
    "author": "jverkoey",
    "state": "closed",
    "created_at": "2025-05-23T05:31:27Z",
    "updated_at": "2025-06-13T09:45:42Z",
    "labels": [
      "bug",
      "openapi"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe current definition of `status` allows the following literals:\n\n```\nstatus: Optional[Literal[\"completed\", \"cancelled\", \"failed\", \"incomplete\"]] = None\n```\n\nbut `in_progress` is also a valid response, as seen in this response packet:\n\n```\n{\"event_id\":\"event_ABC\",\"response\":{\"id\":\"resp_123\",\"conversation_id\":\"conv_456\",\"max_output_tokens\":4096,\"metadata\":null,\"modalities\":[\"audio\",\"text\"],\"object\":\"realtime.response\",\"output\":[],\"output_audio_format\":\"pcm16\",\"status\":\"in_progress\",\"status_details\":null,\"temperature\":0.8,\"usage\":null,\"voice\":\"alloy\"},\"type\":\"response.created\"}\n```\n\n### To Reproduce\n\nUse the realtime api.\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS 15.4 Beta (24E5238a)\n\n### Python version\n\nPython v3.11\n\n### Library version\n\nv1.78.1",
    "comments": [
      {
        "user": "kwhinnery-openai",
        "body": "thanks for the report - will fix in the next set of SDK releases."
      },
      {
        "user": "daniil-berg",
        "body": "@kwhinnery-openai Sorry to bother you here, but I already found a bunch of similar bugs in the types of the Python library.\n\nI would like to contribute, but does it even make sense to open PRs in this repository that would fix them? The `types` modules all have this line at the top:\n\n```python\n# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n```\n\nBut [CONTRIBUTING.md](https://github.com/openai/openai-python/blob/eed877fddc0e26ab99d10157de25e3abcb95598b/CONTRIBUTING.md#modifyingadding-code) is not very helpful.\n\n> Most of the SDK is generated code. Modifications to code will be persisted between generations, but may result in merge conflicts between manual patches and changes from the generator.\n\nSo, should I contribute or not?\n\nThis is made even more confusing because your [openai-openapi](https://github.com/openai/openai-openapi) repo very clearly states:\n\n> Public mirror - do not send pull requests\n\nAnd its API spec is inconsistent with [this one](https://storage.googleapis.com/stainless-sdk-openapi-specs/openai%2Fopenai-3ae9c18dd7ccfc3ac5206f24394665f563a19015cfa8847b2801a2694d012abc.yml) found here:\n\nhttps://github.com/openai/openai-python/blob/eed877fddc0e26ab99d10157de25e3abcb95598b/.stats.yml#L2\n\nAnd neither is consistent with the _actual_ API behavior. One example was already reported in #2126.\n\nIf PRs for the Pydantic types/API are _not_ welcome, should I just open issues similar to this one in this repo? Or should I post equivalent bug reports in the [Developer Community](https://community.openai.com/c/api/bugs/30)?\n\nAgain, I think I would be able to contribute, I am just unclear on the correct protocol here."
      }
    ]
  },
  {
    "issue_number": 2305,
    "title": "responses.parse returns an error in streaming mode",
    "author": "futuremojo",
    "state": "open",
    "created_at": "2025-04-14T20:02:18Z",
    "updated_at": "2025-06-13T02:18:45Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen I use a Pydantic model in `response.parse`, it works fine in synchronous mode. But in asynchronous mode, I get an error.\n\nSee test code below.\n\n### To Reproduce\n\nThe code to reproduce the issue is below.\n\nWhen the code is run, the synchronous version of response.parse returns a correct response. The asynchronous version results in this error:\n```\nAttributeError                            Traceback (most recent call last)\nFile ~/projects/pdf_summarizer/.venv/lib/python3.12/site-packages/openai/lib/_parsing/_responses.py:62, in parse_response(text_format, input_tools, response)\n     59 solved_t = solve_response_format_t(text_format)\n     60 output_list: List[ParsedResponseOutputItem[TextFormatT]] = []\n---> 62 for output in response.output:\n     63     if output.type == \"message\":\n     64         content_list: List[ParsedContent[TextFormatT]] = []\n\nAttributeError: 'str' object has no attribute 'output'\n```\n\n### Code snippets\n\n```Python\nimport json\nfrom typing import List\n\nimport openai\nfrom pydantic import BaseModel\nimport pytest\n\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: List[str]\n\ndef test_response_parse_sync():\n    client = openai.OpenAI()\n\n    response = client.responses.parse(\n        model=\"gpt-4o-2024-08-06\",\n        input=[\n            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n            {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n        ],\n        text_format=CalendarEvent,\n    )\n    return response\n\nasync def test_response_parse_async():\n    client = openai.AsyncOpenAI()\n\n    stream = await client.responses.parse(\n        model=\"gpt-4o-2024-08-06\",\n        input=[\n            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n            {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n        ],\n        text_format=CalendarEvent,\n        stream=True,\n    )\n    async for event in stream:\n        print(event)\n\n\n# Returns a response conforming to the Pydantic model.\nprint(test_response_parse_sync())\n\n# Errors out.\nawait test_response_parse_async()\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.12\n\n### Library version\n\nOpenAI v1.73.0",
    "comments": [
      {
        "user": "VictorAny",
        "body": "+1 to this. This occurs when not using a pydantic model and instead providing the raw model json as well. This is occuring for me in synchronous mode too. Only when streaming."
      },
      {
        "user": "AnneMayor",
        "body": "I am gonna handle this issue. Will share if I figure out the issue."
      },
      {
        "user": "ashikshafi08",
        "body": "## AI-Generated Analysis [NEEDS TESTING]\n\nThis analysis was generated by AI. Sharing because it appears to identify the root cause and provides a reasonable solution, but please review carefully.\n\n---\n\n## Remediation Plan for Issue #2305\n\n### Root Cause\n\nThe `AttributeError: 'str' object has no attribute 'output'` occurs because the `parse_response` function in `src/openai/lib/_parsing/_responses.py` expects a `Response` or `ParsedResponse` object (which has an `output` attribute) but receives a raw string when `client.responses.parse` is called in asynchronous streaming mode (`stream=True`).\n\nThe error trace shows:\n```python\nfor output in response.output:\nAttributeError: 'str' object has no attribute 'output'\n```\n\nThis clearly indicates that `response` is a `str` within `parse_response`, meaning `parse_response` is being called with a string as its `response` argument instead of a proper `Response` object.\n\n### Technical Analysis\n\nThe issue occurs in `ResponseStreamState.handle_event` when processing `text_done` events:\n\n```python\nelif event.type == \"text_done\":\n    # Update the content of the current message in _current_response\n    self._current_response = _update_message_content_in_response(\n        self._current_response, self._current_message_content\n    )\n    if is_given(self._text_format):\n        self._completed_response = cast(\n            ParsedResponse[TextFormatT],\n            parse_response(\n                text_format=self._text_format,\n                input_tools=self._input_tools,\n                response=self._current_response, # <-- This is somehow a string\n            ),\n        )\n```\n\nThe problem is that `self._current_response` (typed as `Response | None`) is somehow becoming a `str`. This type violation likely occurs when:\n\n1. `event.data` from `response_start` is malformed (e.g., a string instead of a dictionary)\n2. `construct_type_unchecked(type_=Response, value=event.data)` doesn't properly validate the input\n3. The `Response` object is constructed without a proper `output` attribute\n\n### Files Impacted\n\n1. `src/openai/lib/_parsing/_responses.py`: Where the `AttributeError` is raised\n2. `src/openai/lib/streaming/responses/_responses.py`: Contains `ResponseStreamState` class that manages the streaming state\n3. `src/openai/resources/responses/responses.py`: Defines the `parse` method for async streaming\n\n### ⚠️ Proposed Fix (Unverified)\n\nThe fix ensures `_current_response` is always a valid `Response` object with proper defensive checks:\n\n```diff\n--- a/src/openai/lib/streaming/responses/_responses.py\n+++ b/src/openai/lib/streaming/responses/_responses.py\n@@ -87,7 +87,14 @@\n \n     def handle_event(self, event: RawResponseStreamEvent) -> List[ResponseStreamEvent[TextFormatT]]:\n         if event.type == \"response_start\":\n-            self._current_response = construct_type_unchecked(\n+            # Ensure _current_response is always a Response object,\n+            # even if event.data is malformed or missing expected fields.\n+            # If event.data is a string (unexpected), construct_type_unchecked\n+            # should raise an error, but this helps prevent downstream AttributeError\n+            # if Response is constructed without an 'output' attribute.\n+            event_data = event.data if isinstance(event.data, dict) else {}\n+            event_data.setdefault(\"output\", [])  # Ensure 'output' key exists and is a list\n+            self._current_response = construct_type_unchecked(\n-                type_=Response, value=event.data\n+                type_=Response, value=event_data\n             )\n \n@@ -211,6 +218,13 @@\n         # Update the content of the current message in _current_response\n         self._current_response = _update_message_content_in_response(\n             self._current_response, self._current_message_content\n         )\n+        # Defensive check: if for some reason _current_response is not a Response object\n+        # (e.g., due to an internal library error or unexpected API response format),\n+        # raise a more informative error or handle it.\n+        if not isinstance(self._current_response, Response):\n+            raise TypeError(\n+                f\"Expected Response object but got {type(self._current_response)} for _current_response. This indicates an internal library error.\"\n+            )\n         if is_given(self._text_format):\n             self._completed_response = cast(\n                 ParsedResponse[TextFormatT],\n```\n\n### Key Changes\n\n1. **Defensive event data handling**: Ensures `event.data` is a dictionary and has an `output` key\n2. **Type safety check**: Validates `_current_response` is a `Response` object before passing to `parse_response`\n3. **Better error messages**: Provides more informative errors when type invariants are violated\n\n### Correct Usage Pattern\n\nThe streaming API should be used like this:\n\n```python\nasync def test_response_parse_async_correct():\n    client = openai.AsyncOpenAI()\n\n    stream = await client.responses.parse(\n        model=\"gpt-4o-2024-08-06\",\n        input=[\n            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n            {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n        ],\n        text_format=CalendarEvent,\n        stream=True,\n    )\n    \n    # Consume the stream (yields ResponseStreamEvent objects, not final parsed model)\n    async for event in stream:\n        # Process intermediate events if needed\n        pass\n\n    # Get the final parsed Pydantic model after stream completion\n    final_parsed_response = await stream.get_final_response()\n    print(f\"Parsed Response: {final_parsed_response}\")\n```\n\n### Testing/Verification\n\n1. **Reproduce the bug** with the original code\n2. **Apply the fix** and verify the `AttributeError` is resolved\n3. **Test edge cases**:\n   - Malformed `response_start` events\n   - Missing `output` fields in event data\n   - Empty `text_format` scenarios\n4. **Verify the `TypeError`** is raised for unexpected type violations\n\nThis fix addresses the root cause while maintaining backward compatibility and providing better error handling for edge cases.\n\nGenerated by [triage.flow](https://github.com/ashikshafi08/triage.flow) – Intelligent Issue Analysis Assistant"
      }
    ]
  },
  {
    "issue_number": 2365,
    "title": "How to Configure openai-python for Dynamic Request Forwarding (A -> B -> Provider) with Streaming?",
    "author": "z00logist",
    "state": "open",
    "created_at": "2025-05-21T10:17:47Z",
    "updated_at": "2025-06-13T01:52:36Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nHi Team,\n\nWe're trying to implement a setup:\n\nMachine A (openai-python client) -> Machine B (a proxy) -> some OpenAI Compatible Provider\nMachine B acts as a simple HTTP proxy, dynamically forwarding the request (URL, headers, body) it receives from Machine A to the any Open-AI compatible Provider URL (like Groq, TogetherAi, etc.). Machine B doesn't have static knowledge of the provider's URL until Machine A's request arrives. Streaming responses are essential.\nThe README shows how to configure http_client with a proxy:\n```\nclient = OpenAI(\n    base_url=\"http://my.test.server.example.com:8083/v1\", \n    http_client=DefaultHttpxClient(\n        proxy=\"http://my.test.proxy.example.com\", \n    ),\n)\n```\n\nHow can we configure Machine B to achieve this dynamic forwarding pattern, ensuring that all aspects of the OpenAI request (including headers for authentication, method, body, and especially streaming) are correctly passed through and handled? Which docker image could we use in our case as a proxy? We haven't found clear guidance or examples for this specific dynamic forwarding proxy scenario. \n\nAs for feature request, it would be incredibly valuable to update your documentation to include a dedicated section on implementing such a dynamic forwarding proxy scenario (A -> B -> Provider). This is a common requirement for flexible deployments, and current openai-python documentation doesn't seem to cover it.\n\nThanks for any pointers!\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "ashikshafi08",
        "body": "Hey, this is an AI-generated analysis from a tool I'm building for GitHub issue triage. Let me know if anything looks off appy to adjust.\n\n---\n\n# 🧠 Triage Analysis Report\n\n**Issue:** [#2365](https://github.com/openai/openai-python/issues/2365) – How to Configure `openai-python` for Dynamic Request Forwarding (A → B → Provider) with Streaming  \n**Generated:** 2025-06-13T01:44:34.462Z\n\n---\n\n## 🏷️ Issue Classification  \n**Category:** `documentation-feature-request`  \n**Confidence:** 92.0%\n\n---\n\n## 🎯 Remediation Plan\n\nThis is a documentation request. The user is asking for a practical example and clearer guidance on configuring the `openai-python` client to forward requests via a **dynamic proxy** (Machine B) to various OpenAI-compatible providers, while maintaining **streaming**.\n\nThis is technically already supported via:\n- `base_url` (pointing to the proxy)\n- `extra_headers` (optional for routing)\n- `http_client` with `DefaultHttpxClient`\n- `stream=True`\n\nHowever, the current docs don’t illustrate **how to wire these up together**.\n\n---\n\n## ✅ Suggested Improvements\n\n### 📘 `README.md`\nAdd a new section on **Advanced Proxy Configurations**.  \nInclude examples for routing requests through a proxy using:\n\n- Path-based routing:  \n  `base_url=\"http://localhost:8000/groq/v1\"`\n\n- Header-based routing:  \n  `extra_headers={\"X-Target-Provider\": \"groq\"}`\n\n- Streaming response handling (`stream=True`)\n\n---\n\n### 📂 `examples/dynamic_proxy_forwarding.py`\n\nAdd a new example with two working client configurations:\n\n#### ✅ 1. **Sync Client Example**\n\n```python\nclient = OpenAI(\n    base_url=\"http://localhost:8000/groq/v1\",  # Proxy endpoint with provider routing in path\n    http_client=DefaultHttpxClient(),\n)\n\n```\n\n    Sends request to proxy (Machine B)\n\n    Proxy infers provider (groq) from path (/groq/v1)\n\n    Uses stream=True for streaming completions\n\n✅ 2. Async Client Example\n\n```\nclient = AsyncOpenAI(\n    base_url=\"http://localhost:8000/togetherai/v1\",\n    http_client=DefaultHttpxClient(),\n)\n```\n\nAsync version of the same pattern\n\nConfirms streaming also works in async flow\n\n✅ Outcome\n\nThis can be resolved by:\n\n    Adding a short section to README.md\n\n    Including a runnable example in examples/\n\nNo changes are required in the core library. The functionality already exists via httpx.\n\nGenerated by triage.flow – Intelligent Issue Analysis Assistant\n\n\n---\n\nLet me know if you want to follow this up with a pull request suggestion or GitHub App integration next."
      }
    ]
  },
  {
    "issue_number": 2409,
    "title": "UnicodeEncodeError when running OpenAI SDK test code in Jupyter Notebook",
    "author": "taless474",
    "state": "open",
    "created_at": "2025-06-11T22:52:02Z",
    "updated_at": "2025-06-12T23:27:36Z",
    "labels": [
      "question"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nRunning the official OpenAI SDK test code in a Jupyter Notebook causes the following error:\n```vbnet\nUnicodeEncodeError: 'ascii' codec can't encode character '\\u2014' in position 110: ordinal not in range(128)\n```\nThe test script appears to fail in Jupyter due to UnicodeEncodeError from an em dash \\u2014, and Jupyter's sys.stdout is not reconfigurable. I think this makes the default SDK examples unusable in Jupyter by default.\n\n\n\n### To Reproduce\n\nLaunch a Jupyter Notebook from terminal\n\nCreate a new Python 3 notebook\n\nPaste and run the following code cell\n\n### Code snippets\n\n```Python\nfrom openai import OpenAI\nimport base64\n\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-4.1-mini\",\n    input=\"Generate an image of gray tabby cat hugging an otter with an orange scarf\",\n    tools=[{\"type\": \"image_generation\"}],\n)\n```\nI got it  form this [image generation example](https://platform.openai.com/docs/guides/tools-image-generation?multi-turn=responseid)\nThe notebook crashes with a UnicodeEncodeError, and Jupyter’s I/O wrappers don’t allow easy reconfiguration of encoding.\n\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.13.4\n\n### Library version\n\nopenai 1.85.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "I don't think there's anything we can do here, if the API is returning unicode and your environment is configured to just support ascii then you'll need to update your environment to support unicode."
      },
      {
        "user": "taless474",
        "body": "Thanks for the reply. I understand your point, but this issue occurs when using the latest Python and Jupyter on macOS, both of which support UTF-8 by default.\n\nThe official example you’re sharing throws encoding-related errors (UnicodeEncodeError) when running in a Jupyter notebook environment, which is a standard development tool for data science and ML workflows.\n\nIf the SDK can’t handle that cleanly or needs specific encoding configuration, I think that should be documented or built into the example code.\n\nWould you be open to updating the example or adding a note in the docs so other users don’t run into the same confusion?"
      },
      {
        "user": "taless474",
        "body": "## Update: The issue exists also in Google Colab\nI’ve done additional testing in both Jupyter Notebook and Google Colab, and I now believe this is an issue with how the SDK handles Unicode in logging or internal errors.\n\n## Setup Details\nOpenAI SDK version: 1.84.0\nPython version: 3.11.13 \nEnvironment: Google Colab\nEncoding: sys.stdout.encoding shows 'UTF-8'\n\n## Behavior Recap\n```python\nfrom openai import OpenAI\nclient = OpenAI()\nclient.models.list()\n```\ncrashes with\n```rust\nUnicodeEncodeError: 'ascii' codec can't encode character '\\u2014' in position 110: ordinal not in range(128)\n```\nI would appreciate a workaround if you decide to close this issue."
      }
    ]
  },
  {
    "issue_number": 2311,
    "title": "response.reasoning_summary_text.delta incorrectly uses ResponseAudioDeltaEvent",
    "author": "simonw",
    "state": "closed",
    "created_at": "2025-04-17T00:04:10Z",
    "updated_at": "2025-06-12T03:38:57Z",
    "labels": [
      "bug",
      "openapi"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nIn the streaming output the `response.reasoning_summary_text.delta` event is represented using `ResponseAudioDeltaEvent` for some reason.\n\n### To Reproduce\n\nTry running this (STR created [with the help of o4-mini](https://gist.github.com/simonw/0d7dfc44bb6ee9ae53d21325b53bba5c)):\n```bash\npython -c 'import openai,os\nfor event in openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")).responses.create(\n    model=\"o3\",\n    input=[{\"role\":\"user\",\"content\":\"Why is the sky blue?\"}],\n    reasoning={\"summary\":\"auto\"},\n    stream=True,\n):\n    print(event)'\n```\nThe output includes lines like this:\n```\nResponseAudioDeltaEvent(delta='**Expl', type='response.reasoning_summary_text.delta', item_id='rs_680044dcbd9881918572b2b1a63ca6a903b0150bf71ce6fa', output_index=0, summary_index=0)\n```\n`ResponseAudioDeltaEvent` looks wrong. Something like `ResponseReasoningSummaryDeltaEvent` would make more sense.\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.13.2\n\n### Library version\n\nopenai==1.75.0",
    "comments": [
      {
        "user": "lemeb",
        "body": "Can confirm on my end! Very weird."
      },
      {
        "user": "lemeb",
        "body": "Ok, I think I figured what's going on:\n\nBecause events of type `response.reasoning_summary_text.delta` don't have any data structure associated with it, the discriminator mapping fails. \n\nThis line of code:\n\nhttps://github.com/openai/openai-python/blob/ed53107e10e6c86754866b48f8bd862659134ca8/src/openai/_models.py#L497-L497\n\nWill yield this:\n\n<details>\n<summary>Click to see the value of `disciminator`</summary>\n\n```txt\n{'error': <class 'openai.types.responses.response_error_event.ResponseErrorEvent'>,\n 'response.audio.delta': <class 'openai.types.responses.response_audio_delta_event.ResponseAudioDeltaEvent'>,\n 'response.audio.done': <class 'openai.types.responses.response_audio_done_event.ResponseAudioDoneEvent'>,\n 'response.audio.transcript.delta': <class 'openai.types.responses.response_audio_transcript_delta_event.ResponseAudioTranscriptDeltaEvent'>,\n 'response.audio.transcript.done': <class 'openai.types.responses.response_audio_transcript_done_event.ResponseAudioTranscriptDoneEvent'>,\n 'response.code_interpreter_call.code.delta': <class 'openai.types.responses.response_code_interpreter_call_code_delta_event.ResponseCodeInterpreterCallCodeDeltaEvent'>,\n 'response.code_interpreter_call.code.done': <class 'openai.types.responses.response_code_interpreter_call_code_done_event.ResponseCodeInterpreterCallCodeDoneEvent'>,\n 'response.code_interpreter_call.completed': <class 'openai.types.responses.response_code_interpreter_call_completed_event.ResponseCodeInterpreterCallCompletedEvent'>,\n 'response.code_interpreter_call.in_progress': <class 'openai.types.responses.response_code_interpreter_call_in_progress_event.ResponseCodeInterpreterCallInProgressEvent'>,\n 'response.code_interpreter_call.interpreting': <class 'openai.types.responses.response_code_interpreter_call_interpreting_event.ResponseCodeInterpreterCallInterpretingEvent'>,\n 'response.completed': <class 'openai.types.responses.response_completed_event.ResponseCompletedEvent'>,\n 'response.content_part.added': <class 'openai.types.responses.response_content_part_added_event.ResponseContentPartAddedEvent'>,\n 'response.content_part.done': <class 'openai.types.responses.response_content_part_done_event.ResponseContentPartDoneEvent'>,\n 'response.created': <class 'openai.types.responses.response_created_event.ResponseCreatedEvent'>,\n 'response.failed': <class 'openai.types.responses.response_failed_event.ResponseFailedEvent'>,\n 'response.file_search_call.completed': <class 'openai.types.responses.response_file_search_call_completed_event.ResponseFileSearchCallCompletedEvent'>,\n 'response.file_search_call.in_progress': <class 'openai.types.responses.response_file_search_call_in_progress_event.ResponseFileSearchCallInProgressEvent'>,\n 'response.file_search_call.searching': <class 'openai.types.responses.response_file_search_call_searching_event.ResponseFileSearchCallSearchingEvent'>,\n 'response.function_call_arguments.delta': <class 'openai.types.responses.response_function_call_arguments_delta_event.ResponseFunctionCallArgumentsDeltaEvent'>,\n 'response.function_call_arguments.done': <class 'openai.types.responses.response_function_call_arguments_done_event.ResponseFunctionCallArgumentsDoneEvent'>,\n 'response.in_progress': <class 'openai.types.responses.response_in_progress_event.ResponseInProgressEvent'>,\n 'response.incomplete': <class 'openai.types.responses.response_incomplete_event.ResponseIncompleteEvent'>,\n 'response.output_item.added': <class 'openai.types.responses.response_output_item_added_event.ResponseOutputItemAddedEvent'>,\n 'response.output_item.done': <class 'openai.types.responses.response_output_item_done_event.ResponseOutputItemDoneEvent'>,\n 'response.output_text.annotation.added': <class 'openai.types.responses.response_text_annotation_delta_event.ResponseTextAnnotationDeltaEvent'>,\n 'response.output_text.delta': <class 'openai.types.responses.response_text_delta_event.ResponseTextDeltaEvent'>,\n 'response.output_text.done': <class 'openai.types.responses.response_text_done_event.ResponseTextDoneEvent'>,\n 'response.refusal.delta': <class 'openai.types.responses.response_refusal_delta_event.ResponseRefusalDeltaEvent'>,\n 'response.refusal.done': <class 'openai.types.responses.response_refusal_done_event.ResponseRefusalDoneEvent'>,\n 'response.web_search_call.completed': <class 'openai.types.responses.response_web_search_call_completed_event.ResponseWebSearchCallCompletedEvent'>,\n 'response.web_search_call.in_progress': <class 'openai.types.responses.response_web_search_call_in_progress_event.ResponseWebSearchCallInProgressEvent'>,\n 'response.web_search_call.searching': <class 'openai.types.responses.response_web_search_call_searching_event.ResponseWebSearchCallSearchingEvent'>}\n```\n</details>\n\nAnd so this code will not return anything:\n\nhttps://github.com/openai/openai-python/blob/ed53107e10e6c86754866b48f8bd862659134ca8/src/openai/_models.py#L498-L503\n\nWhich means you rely on this fallback code, which essentially ends up selecting the first element in the mapping (`ResponseAudioDeltaEvent`), and coerce a construction.\n\nhttps://github.com/openai/openai-python/blob/ed53107e10e6c86754866b48f8bd862659134ca8/src/openai/_models.py#L505-L511\n"
      },
      {
        "user": "lemeb",
        "body": "ok, seems like it was fixed by 6321004!"
      }
    ]
  },
  {
    "issue_number": 2382,
    "title": "Responses API: response.code_interpreter_call_code.delta coming back as type `ResponseAudioDeltaEvent`",
    "author": "moonbox3",
    "state": "open",
    "created_at": "2025-05-27T07:25:00Z",
    "updated_at": "2025-06-12T03:38:38Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhile using the new code interpreter functionality with the OpenAI package v1.82.0, and streaming events, the event for a `response.code_interpreter_call_code.delta` is coming back as part of `ResponseAudioDeltaEvent` instead of as part of `ResponseCodeInterpreterCallCodeDeltaEvent`:\n\nHere are the events I receive:\n\n```python\nevent\nResponseCodeInterpreterCallInProgressEvent(code_interpreter_call=None, output_index=0, sequence_number=3, type='response.code_interpreter_call.in_progress', item_id='ci_68355305cb2881919aabe46ca6725a2403aa3f9ee87e93d4')\n```\nThen:\n```python\nevent\nResponseAudioDeltaEvent(delta='import', sequence_number=4, type='response.code_interpreter_call_code.delta', output_index=0, item_id='ci_68355305cb2881919aabe46ca6725a2403aa3f9ee87e93d4')\n```\n\n### To Reproduce\n\n1. Pip install the `openai` 1.82.0 package.\n2. Get a streaming response when configuring the tools like:\n\n```python\ntools=[\n  {\n    \"type\": \"code_interpreter\",\n    \"container\": { \"type\": \"auto\" }\n  }\n],\n```\n\n3. The response will be of type `ResponseAudioDeltaEvent` but contain the following:\n```python\nevent\nResponseAudioDeltaEvent(delta='import', sequence_number=4, type='response.code_interpreter_call_code.delta', output_index=0, item_id='ci_68355305cb2881919aabe46ca6725a2403aa3f9ee87e93d4')\n```\n\nI handle streaming events in the a similar way to (that all emit the correct event types):\n```python\nresponse: AsyncStream[ResponseStreamEvent] = await agent.client.responses.create(\n    input=\"Give me the first 10 fibonacci numbers using Python\",\n    instructions=<instructions>,\n    previous_response_id=previous_response_id,\n    store=store_output_enabled,\n    tools=tools,  # configured for code interpreter\n    stream=True,\n    **response_options,\n)\n\nasync with response as response_stream:\n    async for event in response_stream:\n        event = cast(ResponseStreamEvent, event)\n        match event:\n            case ResponseCreatedEvent():\n            \t# Removed for brevity\n                pass\n            case ResponseOutputItemAddedEvent():\n                # Removed for brevity\n                pass\n            case ResponseFunctionCallArgumentsDeltaEvent():\n                # Removed for brevity\n                pass\n            case ResponseCodeInterpreterCallCodeDeltaEvent():\n                # Expect to hit this case, but I do not.\n                pass\n```\n\n### Code snippets\n\n```Python\nSee above.\n```\n\n### OS\n\nMacOS Sequoia 15.5\n\n### Python version\n\nPython 3.12.10\n\n### Library version\n\nopenai 1.82.0",
    "comments": [
      {
        "user": "dominpm",
        "body": "Also happening on `file_search` annotations #2383 "
      },
      {
        "user": "moonbox3",
        "body": "@RobertCraigie, FYI"
      },
      {
        "user": "apurvsibal",
        "body": "+1"
      }
    ]
  },
  {
    "issue_number": 2406,
    "title": "Include partial response in LengthFinishReasonError to investigate what went wrong",
    "author": "ai-learner-00",
    "state": "closed",
    "created_at": "2025-06-10T18:58:56Z",
    "updated_at": "2025-06-11T09:20:20Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nWhen using structured output, sometimes we get an error due to max completion tokens being reached. Ideally, the partial response would be logged in mlflow to see which field went out of control.\n\nI only see [token information](https://github.com/openai/openai-python/issues/1700) in the error, but no \"raw response\".\n```python\nfrom openai import AzureOpenAI, OpenAI\nfrom pydantic import BaseModel\nfrom rich import print\n\nclient = OpenAI()\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\n\ntry:\n    completion = client.beta.chat.completions.parse(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\",  \"content\": \"Extract the event information.\"},\n            {\"role\": \"user\",    \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n        ],\n        response_format=CalendarEvent,\n        max_tokens=5\n    )\n    print(completion)\nexcept Exception as e:\n    print(e)\n```\n```\nCould not parse response content as the length limit was reached - CompletionUsage(completion_tokens=5, \nprompt_tokens=92, total_tokens=97, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, \naudio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), \nprompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n```\n\n\nHowever, if we use the API directly, we do see the partial response `{\"steps\":[{\"explanation\":\"Start with the`.\n```bash\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o-mini\",\n\"max_completion_tokens\": 10,\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"how can I solve 8x + 7 = -23\"\n      }\n    ],\n    \"response_format\": {\n      \"type\": \"json_schema\",\n      \"json_schema\": {\n        \"name\": \"math_response\",\n        \"schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"steps\": {\n              \"type\": \"array\",\n              \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"explanation\": { \"type\": \"string\" },\n                  \"output\": { \"type\": \"string\" }\n                },\n                \"required\": [\"explanation\", \"output\"],\n                \"additionalProperties\": false\n              }\n            },\n            \"final_answer\": { \"type\": \"string\" }\n          },\n          \"required\": [\"steps\", \"final_answer\"],\n          \"additionalProperties\": false\n        },\n        \"strict\": true\n      }\n    }\n  }'\n```\n\n```json\n{\n  \"id\": \"chatcmpl-BgvQ7WGS1La4mMfGeTP3b8lgtd9mo\",\n  \"object\": \"chat.completion\",\n  \"created\": 1749570199,\n  \"model\": \"gpt-4o-mini-2024-07-18\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"{\\\"steps\\\":[{\\\"explanation\\\":\\\"Start with the\",\n        \"refusal\": null,\n        \"annotations\": []\n      },\n      \"logprobs\": null,\n      \"finish_reason\": \"length\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 98,\n    \"completion_tokens\": 10,\n    \"total_tokens\": 108,\n    \"prompt_tokens_details\": {\n      \"cached_tokens\": 0,\n      \"audio_tokens\": 0\n    },\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\": 0\n    }\n  },\n  \"service_tier\": \"default\",\n  \"system_fingerprint\": \"fp_34a54ae93c\"\n}\n```\n\n\n\n### Additional context\n\nThis error occurred a few times, which was unexpected since usually the response doesn't reach 16 000 tokens. We would like to confirm if a string or a list got too large. We are using langchain and mlflow.",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "If you want to check what data was received, you can check the `completion` property on the error https://github.com/openai/openai-python/blob/eed877fddc0e26ab99d10157de25e3abcb95598b/src/openai/_exceptions.py#L136. I don't think we'll want to include the full response in the message itself as it could be very large and potentially sensitive."
      }
    ]
  },
  {
    "issue_number": 2355,
    "title": "cached_token_details is missing from realtime_response_usage",
    "author": "jverkoey",
    "state": "open",
    "created_at": "2025-05-14T06:11:19Z",
    "updated_at": "2025-06-11T09:14:23Z",
    "labels": [
      "bug",
      "openapi"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nInputTokenDetails has the following structure when using the realtime API:\n\n```\nInputTokenDetails(audio_tokens=0, cached_tokens=192, text_tokens=239, cached_tokens_details={'text_tokens': 192, 'audio_tokens': 0})\n```\n\nBut the InputTokenDetails type has the following definition:\n\n```\n\nclass InputTokenDetails(BaseModel):\n    audio_tokens: Optional[int] = None\n    \"\"\"The number of audio tokens used in the Response.\"\"\"\n\n    cached_tokens: Optional[int] = None\n    \"\"\"The number of cached tokens used in the Response.\"\"\"\n\n    text_tokens: Optional[int] = None\n    \"\"\"The number of text tokens used in the Response.\"\"\"\n```\n\nNote that cached_tokens_details is missing.\n\n### To Reproduce\n\nSee the above.\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.13\n\n### Library version\n\nv1.78.0",
    "comments": [
      {
        "user": "apurvsibal",
        "body": "@jverkoey and @RobertCraigie - I am not sure if this is the right way to do it or not or if the stainless bot with do it automatically from the API guidance."
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for the PR! Yeah this will have to be fixed in the OpenAPI spec instead."
      }
    ]
  },
  {
    "issue_number": 2408,
    "title": "What is the practical benefit of using @cached_property?",
    "author": "0z0sk0",
    "state": "closed",
    "created_at": "2025-06-11T08:22:51Z",
    "updated_at": "2025-06-11T08:57:44Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nAs i know, cached_property decorator dont spread on a http requests. So, what is the practical benefit of using this everywhere?\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "> cached_property decorator dont spread on a http requests\n\nwhat do you mean here? \n\n---\n\nwe use `cached_property` so that we can both lazily instantiate classes and share them to avoid creating many new instances.\n"
      }
    ]
  },
  {
    "issue_number": 1660,
    "title": "Request for Endpoint to List All Threads",
    "author": "Kato-Official",
    "state": "closed",
    "created_at": "2024-08-18T12:26:19Z",
    "updated_at": "2025-06-11T02:09:59Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nHey mate, hope all is well.\r\n\r\nI would like to request the addition of an endpoint that allows for the retrieval of all threads. If there are concerns about potential security risks, such as team members or others within the same organization gaining access to all threads, I suggest implementing an additional layer of security. For example, you could restrict the listing of threads to those generated under a specific project and token bearer.\r\n\r\nAdditionally, I have noticed that this request has been reiterated multiple times since January but has not yet been addressed. Given the ongoing interest and potential impact of this feature, I believe it would be highly beneficial for many users.\r\n\r\nThis enhancement would significantly improve the usability and flexibility of the API, particularly for teams working on collaborative projects.\r\n\r\nThank you for considering this request. I look forward to your feedback.\r\n\r\nCheers\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "I'm going to close this as it's a request for a new API endpoint, not the Python SDK. I've forwarded this to the OpenAI team."
      },
      {
        "user": "johnkord",
        "body": "@RobertCraigie Is there a plan to eventually support this endpoint by the API team? The metadata fields feel pointless if we can't search for them. It essentially requires a user of the Assistants API to store the Thread ID if it needs to look it up later (instead of being able to query for all Threads that match some metadata key/value). Another approach to solve this problem would be if we could choose the Thread ID when we create the thread (and thus, a user could hash some key/values -> thread ID). I'd much prefer querying for Threads within the context of an Assistant based on metadata key/values."
      }
    ]
  },
  {
    "issue_number": 1616,
    "title": "replace jiter dependency with build-in pydantic function",
    "author": "blshkv",
    "state": "closed",
    "created_at": "2024-08-08T01:15:10Z",
    "updated_at": "2025-06-10T21:14:16Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nhttps://pypi.org/project/jiter/\r\n```\r\nThis is a standalone version of the JSON parser used in pydantic-core\r\nThe recommendation is to only use this package directly if you do not use pydantic.\r\n```\r\n\r\nThe reason of this request is some OS (Gentoo in my case) do provide both ```pydantic``` and ```pydantic-core``` libraries, but ```jiter```\r\n\r\nopenai-python uses ```pydantic``` already, so it should be possible to re-use build-in function (perhaps, via ```pydantic-core```)\n\n### To Reproduce\n\n1. inspect this line: https://github.com/openai/openai-python/blob/main/pyproject.toml#L19\n\n### Code snippets\n\n```Python\nhttps://github.com/openai/openai-python/blob/main/src/openai/lib/streaming/chat/_completions.py\r\n jiter import from_json\n```\n\n\n### OS\n\nany\n\n### Python version\n\nany\n\n### Library version\n\n1.40.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "We need to use the standalone package because we support both Pydantic v1 and Pydantic v2. Unfortunately pydantic v1 does not provide the same JSON parsing functionality.\r\n\r\n> The reason of this request is some OS (Gentoo in my case) do provide both pydantic and pydantic-core libraries, but jiter\r\n\r\nWhat do you mean? How are you managing dependencies?"
      },
      {
        "user": "blshkv",
        "body": "You should probably consider stopping supporting pedantic 1 at some point, for simplicity reason.\r\njiter is used just once in the code.\r\n\r\nhttps://docs.pydantic.dev/latest/version-policy/\r\n```\r\nActive development of V1 has already stopped, however critical bug fixes and security vulnerabilities will be fixed in V1 for one year after the release of V2 (June 30, 2024).\r\n```\r\n\r\nFYI, Gentoo provides pedantic 2 only (https://packages.gentoo.org/packages/dev-python/pydantic )\r\nand pedantic-core (version 2 as well)\r\n\r\nI had to create my own jiter ebuild, but it is complicated, since it is a mixture of rust and python. And it's basically similar with pydantic-core. \r\n"
      },
      {
        "user": "RobertCraigie",
        "body": "Unfortunately we have no plans to drop support for Pydantic v1 anytime soon as it's still getting many millions of downloads a [day](https://www.pepy.tech/projects/pydantic?versions=2.*&versions=1.*).\r\n\r\nPlease ask Gentoo to provide a packaged version of `jiter` if this is important to you."
      }
    ]
  },
  {
    "issue_number": 2404,
    "title": "Log number of retries at INFO level",
    "author": "billierinaldi",
    "state": "open",
    "created_at": "2025-06-10T14:18:04Z",
    "updated_at": "2025-06-10T14:18:04Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nIn the _sleep_for_retry methods in _base_client.py, the fact that a retry is happening is logged at the INFO level, but the retries_taken and max_retries are only logged at DEBUG level. It would be useful to add the information \"retry {retries_taken} of {max_retries}\" to the INFO log, and the DEBUG logs would no longer be necessary. Since there is already a log statement at INFO level, it would not be increasing the verbosity of the logging.\n\nhttps://github.com/openai/openai-python/blob/main/src/openai/_base_client.py#L1051\nhttps://github.com/openai/openai-python/blob/main/src/openai/_base_client.py#L1569\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 2356,
    "title": "httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)",
    "author": "markwitt1",
    "state": "open",
    "created_at": "2025-05-14T17:27:26Z",
    "updated_at": "2025-06-09T18:50:27Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWe are getting a RemoteProtocolError often after running a streaming chat completion call for a long time. It always throws after a few minutes (Python 3.11 / FastAPI)\n\nModel: o3-mini\n\nPackages:\nopenai 1.78.1\nhttpx 0.28.1\n\n### To Reproduce\n\nfrom fastapi import FastAPI, Request\nimport openai\nimport httpx\n\nopenai.api_key = \"YOUR_KEY\"\n\napp = FastAPI()\n\n@app.post(\"/chat-stream\")\nasync def chat_stream(request: Request):\n    # parse incoming JSON (e.g. {\"messages\":[...]}):\n    body = await request.json()\n    # kick off OpenAI streaming chat:\n    stream = await openai.ChatCompletion.acreate(\n        model=\"o3-mini\",\n        messages=body[\"messages\"],\n        stream=True,\n    )\n    async def event_generator():\n        try:\n            async for chunk in stream:\n                yield chunk.choices[0].delta.get(\"content\", \"\")\n        finally:\n            # cleanup if needed\n            pass\n\n    return httpx.StreamingResponse(event_generator(), media_type=\"text/plain\")\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.11.11\n\n### Library version\n\nopenai v1.78.1",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Sorry for the delayed response, can you reproduce this making raw `curl` requests? I suspect this is an API issue."
      }
    ]
  },
  {
    "issue_number": 2339,
    "title": "Slow import times",
    "author": "serozhenka",
    "state": "closed",
    "created_at": "2025-05-03T14:08:58Z",
    "updated_at": "2025-06-09T18:48:51Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nOn average, it takes nearly 0.5 seconds to just `import openai`.\n\nThat may sound normal, but consider the case where a bunch of deps are imported and each one of them takes 0.5 seconds to be imported. Startup times of such an application go crazy.\n\n### To Reproduce\n\n```python\nimport time\n\nstart = time.time()\nimport openai\n\nprint(time.time() - start)\n```\n\nAlternatively, run:\n```sh\npython -X importtime -c \"import openai\"\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\n3.11.9\n\n### Library version\n\n1.75.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "@serozhenka can you try the latest changes in https://github.com/openai/openai-python/pull/2340?\n\n```bash\npip install git+https://github.com/openai/openai-python.git@next\n```"
      },
      {
        "user": "serozhenka",
        "body": "@RobertCraigie \nYeah, much better, down to .25s on average. Appreciate the work on the lazy imports.\nI think it can still be further improved by not trying to import from all of the modules in root `__init__.py`, but this would affect too many people to even be discussed, I guess.\n"
      },
      {
        "user": "RobertCraigie",
        "body": "Yeah I think we're at the limit for what we can improve right now without sweeping breaking changes unfortunately."
      }
    ]
  },
  {
    "issue_number": 2360,
    "title": "logprobs causing peer closed connection without sending complete message body (incomplete chunked read)",
    "author": "keenhon",
    "state": "closed",
    "created_at": "2025-05-17T00:18:44Z",
    "updated_at": "2025-06-09T18:46:06Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [ ] This is an issue with the Python library\n\n### Describe the bug\n\nWhen I enabled logprobs, I started to get consistent issue with one particular long response prompt. I already set max token 16384. Not sure if it is a library issue or api issue.\n\nI started having issues when I enabled logprobs\nlogprobs=true\ntop_logprobs=1\n\nDoes it count towards max token (16384)?\n\nHere are my experiments and request ids, getting this error “peer closed connection without sending complete message body (incomplete chunked read)”\n\nDifferent finetuned models same prompt\nOpenAI Request ID: req_cd79d67111c6231ece4da879c6120181\nOpenAI Request ID: req_bdd68774e518adfa6983ba73425c26cb\n\nUpgraded to latest packages: httpcore-1.0.9 openai-1.79.0 pydantic-2.11.4\nOpenAI Request ID: req_94bbc34a05590b9a9654683b0ccc0593\n\ntried openai_client = OpenAI(timeout=600), no effect\nOpenAI Request ID: req_c1c6dbbc213df52df00abb3302ca405f\n\n-use standard model gpt-4o-mini-2024-07-18\nOpenAI Request ID: req_4b93428d151f2ce8091e69ab470128c5\n\nWhen I disabled logprobs, no issue\nOpenAI Request ID: req_320590488da2052e1318f0148cef0220 (Okay - 3764 chunks)\n\n### To Reproduce\n\nCheck the Request IDs\n\n### Code snippets\n\n```Python\nstream = openai_client.chat.completions.create(\n                model=current_model,\n                messages=messages,\n                temperature=0,\n                top_p=1,\n                max_tokens=16384,\n                stream=True,\n                logprobs=True,\n                top_logprobs=1\n                response_format={\n                    \"type\": \"json_schema\",\n                    \"json_schema\": {\n                    \"name\": \"section\",\n                    \"strict\": True,\n                    \"schema\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                        \"sectionName\": {\n                            \"type\": \"string\"\n                        },\n                        \"clauses\": {\n                            \"type\": \"array\",\n                            \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"clauseNo\": {\n                                \"type\": \"string\"\n                                },\n                                \"contractTypeApplicability\": {\n                                \"type\": \"string\"\n                                },\n                                \"biasAssessment\": {\n                                \"type\": \"string\"\n                                },\n                                \"justification\": {\n                                \"type\": \"string\"\n                                },\n                            },\n                            \"required\": [\n                                \"clauseNo\",\n                                \"contractTypeApplicability\",\n                                \"biasAssessment\",\n                                \"justification\"\n                            ],\n                            \"additionalProperties\": False\n                            }\n                        }\n                        },\n                        \"required\": [\n                        \"sectionName\",\n                        \"clauses\"\n                        ],\n                        \"additionalProperties\": False\n                    }\n                    },\n                }\n            )\n```\n\n### OS\n\nWindows\n\n### Python version\n\nPython 3.12.1\n\n### Library version\n\nopenai-1.79.0",
    "comments": [
      {
        "user": "keenhon",
        "body": "Here's the code to reproduce this issue.\n\n```\n#!/usr/bin/env python\nimport os\nimport time\nimport json\nimport logging\nfrom typing import Dict, Any, List, Optional, Tuple\nimport sys\nfrom openai import OpenAI\n\n# Setup basic logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Models to test\nOPENAI_MODEL = \"gpt-4o-mini-2024-07-18\"  # Primary model\n\n# Initialize client\nopenai_client = OpenAI()  # Uses OPENAI_API_KEY from env vars by default\n\ndef cooldown(seconds=3):\n    \"\"\"Apply cooldown between API calls to avoid rate limits\"\"\"\n    print(f\"Cooling down for {seconds} seconds...\")\n    time.sleep(seconds)\n\ndef test_openai_api_call(\n    prompt: str,\n    model: str = OPENAI_MODEL,\n    enable_logprobs: bool = True\n) -> Tuple[str, Optional[Dict]]:\n    \"\"\"\n    Test OpenAI API call with streaming and logprobs\n    \n    Args:\n        prompt: The user prompt to send\n        model: The model to use\n        enable_logprobs: Whether to enable log probabilities\n        \n    Returns:\n        Tuple of (response text, confidence_data)\n    \"\"\"\n    # Prepare messages\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    \n    # Define log directory for error/output files\n    log_dir = \"test_logs\"\n    os.makedirs(log_dir, exist_ok=True)\n    \n    # Create timestamp for log files\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    \n    print(f\"\\n=== TEST OPENAI API CALL ===\")\n    print(f\"Model: {model}\")\n    print(f\"Logprobs enabled: {enable_logprobs}\")\n    print(f\"Prompt length: {len(prompt)} chars\")\n    print(\"===========================\\n\")\n    \n    # Dictionary to hold logprob data\n    confidence_data = None\n    if enable_logprobs:\n        confidence_data = {\"raw_logprobs\": []}\n    \n    try:\n        cooldown()\n        print(f\"Calling OpenAI API ({model}) with streaming...\")\n        \n        # Reset response chunks for this attempt\n        response_chunks = []\n        chunk_count = 0\n        last_update_time = time.time()\n        \n        # Collect logprobs if enabled\n        collected_logprobs = [] if enable_logprobs else None\n        \n        # Create API call parameters\n        api_params = {\n            \"model\": model,\n            \"messages\": messages,\n            \"temperature\": 0,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0, \n            \"presence_penalty\": 0,\n            \"max_tokens\": 16384,\n            \"stream\": True,\n            \"logprobs\": enable_logprobs,\n            \"top_logprobs\": 1 if enable_logprobs else None\n        }\n        \n        # Make the API call\n        stream = openai_client.chat.completions.create(**api_params)\n        \n        print(\"  Receiving stream...\", end=\"\", flush=True)\n        \n        # Process the stream\n        for chunk in stream:\n            if chunk.choices and len(chunk.choices) > 0:\n                # Extract content delta\n                delta = chunk.choices[0].delta\n                if hasattr(delta, 'content') and delta.content is not None:\n                    response_chunks.append(delta.content)\n                    chunk_count += 1\n                    \n                    # Collect logprobs if available\n                    if enable_logprobs and hasattr(chunk.choices[0], 'logprobs') and chunk.choices[0].logprobs:\n                        collected_logprobs.append(chunk.choices[0].logprobs)\n                    \n                    # Update progress periodically\n                    current_time = time.time()\n                    if current_time - last_update_time > 1:\n                        print(f\"\\r  Receiving stream... {chunk_count} chunks\", end=\"\", flush=True)\n                        last_update_time = current_time\n        \n        print(\"... stream finished.\", flush=True)\n        \n        # Combine chunks\n        response_text = \"\".join(response_chunks)\n        \n        if response_text:\n            print(f\"API response received ({len(response_text)} chars)\")\n            \n            # Save response to file\n            response_file = os.path.join(log_dir, f\"response_{model.split(':')[-1]}_{timestamp}.txt\")\n            with open(response_file, 'w', encoding='utf-8') as f:\n                f.write(\"=== OPENAI API RESPONSE ===\\n\")\n                f.write(f\"Model: {model}\\n\")\n                f.write(f\"Logprobs enabled: {enable_logprobs}\\n\")\n                f.write(f\"Response length: {len(response_text)} chars\\n\")\n                f.write(\"=\"*30 + \"\\n\\n\")\n                f.write(response_text)\n            print(f\"Saved response to {response_file}\")\n            \n            # Process logprobs if available\n            if enable_logprobs and collected_logprobs:\n                print(f\"Collected {len(collected_logprobs)} logprob chunks\")\n                \n                # Process and store token-level logprobs\n                confidence_data[\"raw_logprobs\"] = [\n                    {\n                        \"content\": [\n                            {\n                                \"token\": token_info.token,\n                                \"logprob\": token_info.logprob\n                            }\n                            for token_info in logprobs.content\n                            if hasattr(logprobs, 'content')\n                        ]\n                    }\n                    for logprobs in collected_logprobs\n                    if hasattr(logprobs, 'content')\n                ]\n                \n                # Save logprobs to file\n                logprobs_file = os.path.join(log_dir, f\"logprobs_{model.split(':')[-1]}_{timestamp}.json\")\n                with open(logprobs_file, 'w', encoding='utf-8') as f:\n                    json.dump(confidence_data, f, indent=2)\n                print(f\"Saved logprobs to {logprobs_file}\")\n            \n            return response_text, confidence_data\n        else:\n            print(\"Warning: Empty response received\")\n            return \"ERROR: Empty response received\", None\n    \n    except Exception as e:\n        error_message = str(e)\n        print(f\"Error calling API: {error_message}\")\n        \n        # Save error information\n        error_file = os.path.join(log_dir, f\"error_{model.split(':')[-1]}_{timestamp}.txt\")\n        with open(error_file, 'w', encoding='utf-8') as f:\n            f.write(f\"Error: {error_message}\\n\")\n            f.write(f\"Model: {model}\\n\")\n            f.write(f\"Timestamp: {timestamp}\\n\")\n        return f\"ERROR: {error_message}\", None\n\ndef main():\n    \"\"\"Main test function.\"\"\"\n    # Simple prompt for testing\n    test_prompt = \"\"\"\nRepeat the below text as is:\nSearch\nWrite\nSign up\n\nSign in\n\n\n\nUnlocking Ultra-Long Text Generation: A Deep Dive into LongWriter and AgentWrite\nHitesh Hinduja\nHitesh Hinduja\n\nFollow\n20 min read\n·\nAug 21, 2024\n71\n\n\n\n\n\nHello Folks,\n\nIt’s been quite some time (almost 3 months) since my last blog post. But finally, I’m back, and let’s get started! Moving forward, my blogs will primarily focus on interesting research papers in the LLM and GenAI space. I’ll be discussing problem statements that I encounter in my day-to-day life, in what we like to call “story time,” as many of you might remember from my past blogs. This will be followed by a deep dive into the technical aspects of those problem statements. In addition to explaining the research papers, I’ll share experiences and practical examples, and I’ll also elaborate on technical details that the papers might skip, assuming the reader already knows them. So, let’s dive in!\n\nJust a few days ago, one of my family friends visited our place. They have a lovely 8-year-old daughter. It was August 15th, India’s Independence Day, and her school had given her an assignment to write an essay on Independence Day with a strict requirement of “at least 10000 words”. Now that’s really a lot! I really don’t know if I should call this an essay or a mini-book for an 8-year-old child! As usual, the parents started drafting it on behalf of their child. The first thing that comes to everyone’s mind is ChatGPT or something similar. At first, the parents were very relaxed and thought, “Let’s start drafting this on August 14th, just a day before, since it’s just a matter of ‘prompting the LLM model’ and getting the output.” On the night of August 14th, they did just that, but any guesses what happened? The model, though it gave a good output, struggled to maintain the following: Relevance, Accuracy, Coherence, Clarity, Breadth and Depth, and Reading Experience. Additionally, when the model is asked to output strictly 10k words, it repeats the context and significantly goes out of context.\n\nNow, you all might be wondering, what are these six dimensions? For that, let’s continue with the further reading and dive into the problem statement of “limitations of current long-context large language models (LLMs) in generating ultra-long outputs.” In this blog, we’ll explore an interesting research paper titled “LONGWRITER: UNLEASHING 10,000+ WORD GENERATION FROM LONG CONTEXT LLMS.” Even though these models can process inputs up to 100,000 tokens, they typically struggle to produce outputs longer than 2,000 words. The primary reason for this limitation is attributed to the supervised fine-tuning (SFT) datasets, which lack examples of long outputs, capping the models’ ability to generate extended text. So in this blog, let’s understand the intriguing technique the authors have used to improve long output responses and make sure parents’ lives become easier in the future! And what about the kids? These days, I leave that up to their destiny with the advancements in AI and the way life has become easier for them with limited use of their mental capabilities! Anyway, let’s get started.\n\nIntroduction\nNow, let’s get into the borderline understanding of the paper. It kicks off by highlighting an interesting challenge with long context LLMs. These models, which can process over 100,000 tokens of input, still struggle to generate outputs longer than 2,000 words. This is a significant issue because, in some cases, more than 1% of user requests actually need longer responses.\n\nThe core problem? The supervised fine-tuning (SFT) datasets that train these models just don’t include enough examples of long outputs. So, even though the models are capable of handling long inputs, they haven’t been trained to produce long outputs effectively. This limitation has stuck around because many LLMs rely on these same datasets.\n\nTo tackle this, the authors introduce AgentWrite — a new approach that helps these models generate longer texts by breaking down the task into smaller parts. This method can push output lengths up to 20,000 words, far beyond what’s usually possible.\n\nThe paper also brings in LongWriter-6k and LongBench-Write, a dataset and benchmark created to train and test models on their ability to generate these ultra-long texts. The idea is to push the boundaries of what LLMs can do, making them more capable of handling tasks that require extended output.\n\nNow let’s understand what is Agentwrite and how it works:\nStep I: Plan\nFirst things first, AgentWrite starts with a plan — just like how you’d outline an article before diving into writing. The model creates a detailed outline based on the given instructions, laying out the main content and specifying word counts for each section. Think of it as the model’s roadmap. For instance, if tasked with writing a 30,000-word piece on the Roman Empire, the plan might look something like this:\n\nParagraph 1: Introduction to the origins of the Roman Empire (700 words)\n\nParagraph 2: Founding of the Roman Empire (800 words)\n\n…\n\nParagraph 15: Summary of the Roman Empire’s history (500 words)\n\nThis structured approach ensures the model knows exactly where it’s headed, making it easier to manage the task of generating lengthy outputs. Here you can look below how the author’s structure the input:\n\n\nThis detailed outline ensures that the model has a clear structure to follow, making it easier to manage the generation of long outputs.\n\n\nStep II: Write\nNext up, the model gets down to writing. Following the outline from Step I, the model generates the text in a serial manner — paragraph by paragraph. This sequential method ensures that each paragraph builds on the previous one, maintaining coherence throughout. By using earlier paragraphs as context, the model avoids common pitfalls like repetition or incoherence, which can be a real issue when generating longer texts. The results? Outputs extending over 20,000 words, all while keeping the quality intact. Here you see below how they motivate the input text with clear standing instructions. Imagine a pitch-perfect training to someone in their early stages of life so that they learn better for the future!\n\n\nHope you all are clear with what exactly AgentWrite method does. We will soon see the validations and results of incorporating AgentWrite method in the LongWriter-6k and LongBench-Write. But before that, let’s see how these datasets are created and how are the models fine-tuned with this dataset that has AgentWrite methodology.\n\nLONGWRITER: TEACHING MODELS TO GENERATE ULTRA-LONG OUTPUTS\nAfter understanding how AgentWrite works, the next big question is — can we actually teach large language models (LLMs) to generate ultra-long outputs consistently? This paper dives into just that by constructing specialized datasets, fine-tuning models, and using Direct Preference Optimization (DPO) to fine-tune their performance. Don’t worry, we will be going into each and everything in detail! Let’s start with the first step “DATA”, “DATA”, “DATA”!\n\n4.1 Data Construction\nSelection of Instructions:\nThe first step in this process was to handpick instructions that would naturally require long outputs. The team selected 6,000 such instructions — 3,000 from GLM-4’s SFT data (in Chinese) and another 3,000 from WildChat-1M (in English). GPT-4o automated this selection process, with additional rule-based filtering to weed out toxic or irrelevant instructions. After some manual verification, over 95% of these instructions were confirmed to genuinely need long outputs.\n\nGenerating Responses:\nOnce the instructions were set, it was time to generate responses using the AgentWrite pipeline powered by GPT-4o. The outputs underwent a strict post-processing phase, where any responses that were too short or that resulted from failed planning steps were filtered out. To ensure clarity, irrelevant identifiers like “paragraph 1,” “paragraph 2,” etc., were removed. The outcome was a dataset named LongWriter-6k, which offers a broad range of output lengths between 2,000 and 10,000 words — just what’s needed to supplement the general SFT data.\n\nCombining Data:\nThe next move was to combine LongWriter-6k with the general SFT data, which includes 180k chat entries from GLM-4’s SFT dataset. This combination effectively addresses the scarcity of long outputs in the existing datasets, ensuring the models have ample examples to learn from.\n\nNow once we have understood how data is getting constructed, lets understand how is the data being used in the model training.\n\n4.2 Model Training\nSupervised Fine-Tuning\n\nModels Used:\nThe team focused on fine-tuning two open-source models: GLM-4–9B and Llama-3.1–8B. Both of these models are capable of handling a context window of up to 128k tokens — perfect for the ultra-long outputs we’re aiming for.\n\nTraining Process:\nTo fine-tune these models, a loss weighting strategy was employed. Instead of averaging the loss by sequence, the loss was averaged by token. This approach ensures that longer outputs contribute more effectively to the training process. The training setup included 8xH800 80G GPUs using DeepSpeed+ZeRO3+CPU offloading, with a batch size of 8, a learning rate of 1e-5, and a packing length of 32k. The training spanned 4 epochs, totaling approximately 2,500–3,000 steps.\n\nAre you confused about the loss weighting strategy? If yes, let me explain it to you in detail. Else you may skip to the next section.\n\nToken-Level Loss Averaging\nHow It Works:\nInstead of averaging the loss at the sequence level, the model averages the loss across all tokens within the batch. This ensures that every token, regardless of its position in a short or long sequence, contributes equally to the learning process.\nExample:\nAssume Sequence 1 has 9 tokens and Sequence 2 has 100 tokens.\nIf Sequence 1 has a loss of 0.2 per token and Sequence 2 has a loss of 0.3 per token:\n\nThis approach means that the model’s learning process is more influenced by the longer sequence because it contains more tokens contributing to the loss.\nWhy This Matters for Long Outputs:\nIn the context of fine-tuning models for generating ultra-long outputs, token-level loss averaging ensures that the model pays more attention to longer sequences during training. Each token in these longer outputs contributes to the loss, encouraging the model to improve its performance on generating long, coherent text.\nResulting Models:\nThe fine-tuning process led to the creation of two models: LongWriter-9B (derived from GLM-4–9B) and LongWriter-8B (from Llama-3.1–8B).\n\nAlignment (DPO)\n\nObjective:\nTo further refine the quality and adherence to length constraints, the team applied Direct Preference Optimization (DPO) to the LongWriter-9B model. I will explain this to you in detail shortly. Its important for us to first understand DPO and then dive into how it is used in the paper!\n\nUnderstanding Direct Preference Optimization (DPO)\nDirect Preference Optimization (DPO) is a technique used to fine-tune language models by optimizing them based on preference data. The core idea is to improve the model’s ability to generate outputs that align with human preferences or specific criteria, such as following instructions closely or producing high-quality content.\n\n1. Practical Example of DPO — For simplistic understanding\nImagine you’re building an AI that generates short stories. You want the AI to produce stories that are not only creative but also coherent and engaging.\n\nStep 1: Generate Multiple Outputs: The AI generates several versions of a story based on the same prompt.\n\nVersion A: A highly creative but slightly confusing story.\n\nVersion B: A less creative but very clear and engaging story.\n\nVersion C: A story that is both creative and coherent but slightly lacks depth.\n\nStep 2: Rank the Outputs: Human reviewers or another evaluation system rank these outputs based on how well they meet the desired criteria (creativity, coherence, engagement).\n\nRanking: Version C > Version B > Version A\n\nStep 3: Optimize Based on Preferences: The AI is fine-tuned to generate outputs more like Version C, which best balances creativity, coherence, and engagement. This process helps the AI learn what makes a “good” story according to the given criteria.\n\n2. Technical Explanation of DPO — Not specific to paper but in real how DPO works.\nDPO involves several steps:\n\nData Collection: Collect pairs of outputs generated by the model. For each pair, a preference label is assigned, indicating which output is better according to specific criteria (e.g., clarity, relevance, creativity).\nPreference Data: The model is trained on this preference data, where the goal is to maximize the likelihood that the model will generate the preferred output in future generations.\nObjective Function: The model’s objective function is adjusted to give higher probabilities to preferred outputs. This is typically done using techniques like pairwise ranking loss or a reward model that scores outputs based on how well they align with the preferences(Please feel free to reach out to me for understanding more about pairwise ranking loss understanding)\nFine-Tuning: The model is then fine-tuned using this adjusted objective function, making it more likely to generate outputs that align with the preferences in similar future scenarios.\n3. How DPO is Used in the Paper\nSo, we’ve talked about what Direct Preference Optimization (DPO) is, but how exactly is it applied in this research? In the context of the paper, DPO comes into play after the initial fine-tuning with the LongWriter-6k dataset. The goal here is to sharpen the model’s ability to generate long-form content that aligns more closely with user instructions.\n\nHere’s a breakdown of how DPO is used:\n\nGenerating Outputs:\nOnce the model has been fine-tuned with the LongWriter-6k dataset, it’s time to see what it can produce. The model generates multiple outputs for prompts that require lengthy responses. For instance, if the prompt asks for a detailed 10,000-word article, the model will produce several versions of that output.\n\nCreating Preference Data:\nNext, these outputs are evaluated and ranked based on how well they meet the desired criteria — think of metrics like length adherence, coherence, and overall quality. For example, if the prompt asks for a 10,000-word piece, outputs that come closest to this length, while also maintaining coherence and quality, are ranked higher.\n\nOptimizing the Model:\nNow, using these rankings, the model is further fine-tuned. This step is where the magic of DPO really kicks in — the model’s parameters are adjusted so that it learns to favor outputs similar to the ones ranked highest by the preference model. Essentially, the model is being trained to understand, “These are the kinds of outputs you should aim for.” But please don’t confuse here with the objective function. Here its making sure that the model learns among the best of the best outputs when it gets fine-tuned the second time! So its not like any objective function or loss function getting optimised in this paper for DPO. I am sure you might be wondering why there is so much of manual intervention? — Bdw my boss used to tell me “Sometimes hardwork is better than smart work”\n\nFinal Evaluation:\nAfter applying DPO, the model goes through another round of evaluation using benchmarks like LongBench-Write. This final step is crucial to ensure the model isn’t just generating long texts, but high-quality, length-appropriate content.\n\n4. How AgentWrite Uses DPO\nNow, let’s tie this back to AgentWrite and how it leverages DPO to enhance its performance.\n\nImproving Length Adherence:\nOne of the main challenges AgentWrite addresses is getting the model to stick to specific length requirements — say, producing that elusive 10,000-word article without falling short. DPO helps by optimizing the model to prefer outputs that meet these length requirements more accurately. So, after going through DPO, the model is more likely to hit the length targets dead-on.\n\nEnhancing Content Quality:\nBut it’s not just about hitting the word count. AgentWrite, with the help of DPO, also refines the model’s ability to produce content that is coherent, relevant, and clear. By focusing on the highest-quality outputs during DPO, AgentWrite ensures that the model isn’t just verbose but also produces text that’s worth reading — long, but also strong.\n\nSo finally, we come to an end where we understand how DPO is used additionally to improve the quality of outputs and have better version of next fine-tuned model. In the article till here, we see that there are six dimensions that have been talked about very often. Now before moving to the next section where I explain you about these 6 metrics and how are they evaluated, I would like to give you a brief understanding of those. So let’s understand this in a funny way:\n\nPicture this: Your boss has just asked you to draft a 1,000-word report on “The Impact of Remote Work on Team Productivity.” Naturally, you turn to your trusty AI to help you out. Now, let’s see how you’d evaluate whether the AI’s output is actually going to impress the boss or get you a stern “we need to talk” email.\n\nRelevance:\nFirst off, the report needs to stay on point. You don’t want the AI wandering off into unrelated territory, like the benefits of beach vacations or why cats make great office companions (unless, of course, your boss is a cat enthusiast).\n\nAccuracy:\nNext, the report has to be factually correct. You’re hoping to present solid insights on how remote work affects productivity, not accidental claims like “working from home increases productivity by 200% — especially if you have Netflix on in the background.”\n\nCoherence:\nThe report should be logically structured. It should flow like a well-organized meeting agenda, not like a chaotic brainstorming session where everyone’s talking at once. Each section should lead smoothly into the next, making it easy for your boss to follow along without needing a coffee break halfway through.\n\nClarity:\nThe language should be crystal clear — no jargon that requires a decoder ring or overly complex sentences that sound like they belong in a legal contract. Your boss should be able to skim through it and instantly get the main points, without scratching their head or wondering if you’ve secretly hired a lawyer to write it.\n\nBreadth and Depth:\nThe report should cover all the critical angles — like how remote work affects team collaboration, productivity metrics, and maybe even the impact on employee morale. You want to make sure the report is thorough enough to avoid that dreaded “Could you add more detail here?” feedback.\n\nReading Experience:\nFinally, the report should be engaging and easy to read. You don’t want your boss yawning halfway through or worse, feeling the need to rewrite half of it. The writing should be smooth and professional, giving your boss the impression that you’ve really put in the effort (even if you had a little help from AI on the side).\n\nBut now, you might be wondering — how do we actually evaluate these metrics? After all, your AI doesn’t come with a built-in “Boss Approval Gauge” or a “Clarity-O-Meter.” So, let’s dive into how this all works:\n\nHow Are These Metrics Calculated? Enter LLM-as-a-Judge\nSo, after understanding the six key dimensions for evaluating text quality, the next question that naturally comes up is, “How on earth does the AI figure all this out?” After all, the model itself doesn’t inherently “know” these metrics. That’s where the LLM-as-a-Judge method comes into play.\n\nLLM-as-a-Judge Overview\nInstead of relying on human evaluators for every single piece of content, the researchers cleverly used the LLM itself to assess the quality of the generated outputs. By leveraging the model’s extensive knowledge and contextual understanding, it can provide objective evaluations across the six dimensions we’ve discussed. Bdw my boss also used to tell me “Sometimes smart work is better than hard work” 😂\n\nScoring Process: How It Works\nThe way it works is pretty straightforward. The researchers designed specific prompts to instruct GPT-4o to evaluate the outputs. For example, the model might be asked to rate how relevant the output is to the original prompt or assess the clarity of the writing. The prompts might look something like this:\n\n“You are an expert in evaluating text quality. Please rate the AI assistant’s response across six dimensions: Relevance, Accuracy, Coherence, Clarity, Breadth and Depth, and Reading Experience. Provide a score from 1 to 5 for each dimension.”\n\nThe model then analyzes the text based on these prompts and provides a score, usually outputting its evaluation in a structured format like JSON, which includes both the scores and brief explanations.\n\nUse of Pre-Defined Metrics\nFor specific metrics like output length, the model uses something a bit more technical — a piecewise linear function. This function adjusts the score based on how closely the actual length of the output matches the required length. For example:\n\nIf the output is exactly 1,000 words when the prompt asks for 1,000, the score might be a perfect 100.\n\nIf it’s a bit off — say, 10% longer or shorter — the score will decrease slightly.\n\nAnd if it’s way off, like 50% longer or shorter, the score drops significantly.\n\nThis method ensures that the model is incentivized to produce text that fits the expected length, penalizing outputs that deviate too much.\n\nNo Built-in Metrics During Training\nIt’s important to note that these metrics aren’t baked into the model during its initial training. The model doesn’t inherently “know” it’s being judged on Relevance or Coherence — it’s just generating text based on its training data. The evaluation happens externally, using a separate model (in this case, GPT-4o) that’s prompted to act as a judge.\n\nTools and Techniques\nWhile some models can be fine-tuned on datasets labeled by human annotators, this paper primarily relies on GPT-4o’s existing capabilities to evaluate the text without additional fine-tuning. The idea is to mimic how a human would assess the quality of the writing, using the model’s vast knowledge and understanding of language.\n\nExperiment Results: Model Evaluation on LongBench-Write\nThe research team evaluated a set of models — 4 proprietary and 5 open-source — using the LongBench-Write benchmark. This evaluation also included the newly trained LongWriter models. Among the existing models, the only other one specifically aligned for long-form text generation is Suri-IORPO, which is based on Mistral-7B-Instruct-v0.2 and fine-tuned using LoRA.\n\nFor the evaluation, the team configured each model with a temperature setting of 0.5 and set the maximum tokens parameter to the highest allowed by the model’s API. For open-source models, this was set at 32,768 tokens.\n\nThe key results, summarized in Table 3, include average and median response lengths, while Figure 6 visualizes how well each model’s output length aligns with the required length across 120 different instructions.\n\nKey Findings from the Experiment Results\nLet’s take a look at this table carefully!\n\n\nOutput Length Performance:\nPrevious Models’ Limitations: Most existing models struggle to meet the length requirement of over 2,000 words. For instance, in the [2k, 4k) word range, the majority scored below 70, with only Claude 3.5 Sonnet performing decently (see Table 3 above).\nSevere Shortcomings in Longer Outputs: For prompts requiring 4,000 to 20,000 words, almost all previous models failed to reach the target output length, with many scoring 0 (indicating output lengths were less than one-third of the required length) (refer to Figure 6 below).\n\nLongWriter Models’ Success: In contrast, the LongWriter models, enhanced with training data from LongWriter-6k, consistently met the required output lengths while maintaining good quality, as evidenced by the Si (output length score) and Sq (quality score) in the [2k, 20k) range (refer to Table 3 and Figure 6).\nCoherence of Long Outputs:\nNLL Testing for Coherence: The cumulative average negative log-likelihood (NLL) test was used to ensure that the long outputs from the LongWriter models were coherent and logically connected (see Figure 7).\n\nScratching your head or confused on what exactly is NLL? Don’t worry, let me explain this to you in a funny way!\n\n\nNegative Log Likelihood (NLL) Test Explained: The Manager-Employee Review\nImagine you’re a manager, and you have an employee (let’s call them Hitesh) who is supposed to prepare a long report. As Hitesh writes the report, you’re checking in at different points — say, after the introduction, the first section, halfway through, and so on. You want to see how well Hitesh is sticking to the topic and making logical points as the report progresses.\n\nNow, if Hitesh is doing a great job, you’ll find it easier to predict what comes next in the report because it’s all flowing logically. You’re nodding along thinking, “Yes, this makes sense,” and you’re happy because Hitesh is making your life easier. This means the “Negative Log-Likelihood” (NLL) score is low — Hitesh is doing well, and your surprise at what comes next is low because it all fits together smoothly.\n\nBut if Hitesh starts rambling, throwing in random unrelated points, or the report starts falling apart in terms of structure, your ability to predict what’s coming next decreases. You’re now scratching your head, trying to figure out what on earth is happening. This would result in a higher NLL score — indicating that the coherence of the report is dropping.\n\n\nNow, let’s look at Figure 7 above:\n\nThe red line (GLM-4–9B) and the blue line (Llama-3.1–8B) represent how two different “Hitesh’s” (or models) are doing as they write their reports (generate text).\nAs the report gets longer (moving along the x-axis), you (the manager) are checking to see if it’s still making sense.\nA lower line on the graph means the report is staying coherent, and you’re less surprised (lower NLL) as you read along. So, in this case, Hitesh(Llama-3.1–8B) in the blue line is doing a slightly poor job at keeping things coherent compared to Hitesh in the red line (GLM-4–9B).\nImpact of Direct Preference Optimization (DPO)\nImproved Quality and Length Adherence: The application of DPO resulted in a 4% improvement in the output length score (Sl) and a 3% improvement in the quality score (Sq) across all ranges when comparing LongWriter-9B and LongWriter-9B-DPO (see Table 3 and Figure 8).\nHuman Preference: In manual evaluations, humans preferred the outputs of the DPO-trained model over the non-DPO model in 58% of the cases. Despite having fewer parameters, LongWriter-9B-DPO achieved a performance level on par with GPT-4o (as shown in Figure 9).\n\nColor Coding: Blue indicates lower win rates (less than 50%), red indicates higher win rates (more than 50%). LongWriter-9B-DPO has higher win rates (more red), meaning it generally outperforms other models.\nAblation Study: Understanding the Impact of Different Data Approaches\nThe ablation study in the paper examines how different data configurations impact the performance of the LongWriter-9B model. The key findings are summarized below, based on the results reported in Tab 4:\n    \"\"\"\n    \n    # Test 1: No logprobs, no schema\n    print(\"\\n=== TEST 1: Basic API Call (No logprobs) ===\")\n    response, _ = test_openai_api_call(test_prompt, enable_logprobs=False)\n    print(f\"Test 1 complete: {'SUCCESS' if not response.startswith('ERROR') else 'FAILED'}\")\n    \n    # Test 2: With logprobs, no schema\n    print(\"\\n=== TEST 2: API Call with Logprobs ===\")\n    response, confidence_data = test_openai_api_call(test_prompt, enable_logprobs=True)\n    print(f\"Test 2 complete: {'SUCCESS' if not response.startswith('ERROR') else 'FAILED'}\")\n    \n    print(\"\\nAll tests completed. Check logs for details.\")\n\nif __name__ == \"__main__\":\n    main() \n```"
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting and the reproduction script!\n\nHow long does your request run for before you get an error?\n\nI *think* this is an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2359,
    "title": "feature request: expose verify param of httpx client",
    "author": "ccurme",
    "state": "open",
    "created_at": "2025-05-16T20:25:42Z",
    "updated_at": "2025-06-09T10:56:44Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nhttpx allows you to specify SSL contexts when instantiating a client (see [httpx docs](https://www.python-httpx.org/advanced/ssl/#configuring-client-instances)). Otherwise, httpx will create a new SSL context when creating the client, which is expensive if you are instantiating multiple `OpenAI` clients.\n\nExposing `verify` in `openai.(Async)OpenAI` would allow you to share the SSL context among clients.\n\nCurrently, you'd need to share `httpx` clients. If you want default params (e.g., base URL) you need to ensure you're consistent with how OpenAI instantiates httpx clients.\n\nAlternatively, `OpenAI._client` could be made a public attribute.\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Hey, are you aware of `client.with_options()`? that will copy the httpx client over to the new client instance which will improve performance and also let you easily share common options. Would that work for you?\n\nhttps://github.com/openai/openai-python?tab=readme-ov-file#configuring-the-http-client"
      }
    ]
  },
  {
    "issue_number": 2396,
    "title": "Responses API Code Interpreter",
    "author": "mattbrandman",
    "state": "open",
    "created_at": "2025-06-04T00:01:26Z",
    "updated_at": "2025-06-07T18:48:05Z",
    "labels": [
      "bug",
      "openapi"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe responses API requires a \"results\" field on the code_interpreter model. This is populated with None on every response I have seen but is also required when sending the old code_interpreter blocks, however the api rejects the results field as unknown the class is \n\n```\nclass ResponseCodeInterpreterToolCallParam(TypedDict, total=False):\n    id: Required[str]\n    \"\"\"The unique ID of the code interpreter tool call.\"\"\"\n\n    code: Required[str]\n    \"\"\"The code to run.\"\"\"\n\n    results: Required[Iterable[Result]]\n    \"\"\"The results of the code interpreter tool call.\"\"\"\n\n    status: Required[Literal[\"in_progress\", \"interpreting\", \"completed\"]]\n    \"\"\"The status of the code interpreter tool call.\"\"\"\n\n    type: Required[Literal[\"code_interpreter_call\"]]\n    \"\"\"The type of the code interpreter tool call. Always `code_interpreter_call`.\"\"\"\n\n    container_id: str\n    \"\"\"The ID of the container used to run the code.\"\"\"\n\n```\n\n### To Reproduce\n\n1. Make a request using code interpreter built in tool\n2. Try to send a follow up request with the previous code interpreter block\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nMacOS\n\n### Python version\n\nPython v3.12\n\n### Library version\n\nv1.84",
    "comments": [
      {
        "user": "czgu",
        "body": "+1 on this issue.\n\nWith the responses API, there is an \"include\" field that can be set to add outputs from the code_interpreter. It is documented here: https://platform.openai.com/docs/api-reference/responses/create#responses-create-include.\n\nIf this flag is enabled, a new \"outputs\" field will appear in the response, and its structure will be similar to the \"results\" field.\n\nI also observed that in the OpenAI playground, when using the code_interpreter, the \"outputs\" field is always present, but there is no include flags in the generated CURL command to reproduce this behavior "
      }
    ]
  },
  {
    "issue_number": 2336,
    "title": "Missing images/edits Endpoints in _deployments_endpoints",
    "author": "sandeeprawat",
    "state": "closed",
    "created_at": "2025-04-30T21:20:07Z",
    "updated_at": "2025-06-06T15:15:54Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\n**Description**\nThe _deployments_endpoints set defined in src/openai/lib/azure.py is missing the images/edit endpoints. This may cause issues when working with Azure deployments for editing images.\n\nhttps://github.com/openai/openai-python/blob/main/src/openai/lib/azure.py#19-19\n\n**Relevant Code**\n``` Python\n_deployments_endpoints = set(\n    [\n        \"/completions\",\n        \"/chat/completions\",\n        \"/embeddings\",\n        \"/audio/transcriptions\",\n        \"/audio/translations\",\n        \"/audio/speech\",\n        \"/images/generations\",\n    ]\n)\n```\n**Suggested Fix**\nInclude the missing endpoint \"/images/edit\" in the _deployments_endpoints set.\n\n``` Python\n_deployments_endpoints = set(\n    [\n        \"/completions\",\n        \"/chat/completions\",\n        \"/embeddings\",\n        \"/audio/transcriptions\",\n        \"/audio/translations\",\n        \"/audio/speech\",\n        \"/images/generations\",\n        \"/images/edits\",  # Add this line\n    ]\n)\n```\n**Steps to Reproduce**\nUse an Azure deployment for the images/edit endpoint.\nObserve that the endpoint is not handled correctly due to its absence in _deployments_endpoints.\n\nEnvironment\nRepository: openai/openai-python\nFile: src/openai/lib/azure.py\nCommit: fad098ffad7982a5150306a3d17f51ffef574f2e\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "auggie246",
        "body": "PR #2332 pending but because this repo is auto generated, there's not much human reviewer looking at it. Might take some time before it gets accepted"
      },
      {
        "user": "sandeeprawat",
        "body": "> PR [#2332](https://github.com/openai/openai-python/pull/2332) pending but because this repo is auto generated, there's not much human reviewer looking at it. Might take some time before it gets accepted\n\nWe can use below code to hack it for the time being\n\n```\nimport openai as ai\n\nprint(\"fixing the library with edit functionality\")\nai._azure._deployments_endpoints.add(\"/images/edits\")\n```"
      }
    ]
  },
  {
    "issue_number": 1913,
    "title": "Azure openai api_version optional or latest argument",
    "author": "thunder-007",
    "state": "closed",
    "created_at": "2024-11-30T05:25:08Z",
    "updated_at": "2025-06-06T15:15:45Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nfrom openai import AzureOpenAI\r\nazure_endpoint,api_key,api_version api_version is beign mandatory make it optional and if some argument like latest is passed it should show automatically pick the latest version\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "cc @kristapratico "
      },
      {
        "user": "Programmer-RD-AI",
        "body": "Hi,\n\nI've set up a PR to resolve this issue (#1913) by making the `api_version` argument optional and defaulting it to `\"2024-12-01-preview\"`, the latest inference version. \n\nPlease feel free to review the PR and comment if anything needs to be updated or if there are further improvements you'd like to see. Your feedback is much appreciated!\n\nPR: #2029 "
      }
    ]
  },
  {
    "issue_number": 2401,
    "title": "Unexpected Behavior When Using openai.ChatCompletion.create() with Custom user ID",
    "author": "Anipaleja",
    "state": "open",
    "created_at": "2025-06-06T03:37:28Z",
    "updated_at": "2025-06-06T03:37:58Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen passing a custom user value to openai.ChatCompletion.create(), the request completes successfully, but there is no indication that the value is being acknowledged, logged, or used in any visible monitoring system. The documentation states that the user field is used for abuse monitoring, but from the client perspective, it is impossible to confirm whether this information is being handled or ignored.\n\nThis makes it difficult to audit usage or confirm compliance with OpenAI’s best practices.\n\n### To Reproduce\n\n- Use the latest version of openai Python package.\n- Run the code:\n```python\nimport openai\n\nopenai.api_key = \"your-key-here\"\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    user=\"my_test_user_001\"\n)\nprint(response)\n```\n- Observe that the request works, but there is no way to verify the user field was registered or tracked.\n\n### Code snippets\n\n```Python\nimport openai\n\nopenai.api_key = \"api_key\"\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    user=\"test_user_123\"\n)\n\nprint(response)\n\n#This snippet Demonstrates correct usage of the user parameter and shows the lack of any visible effect or confirmation that the user ID was acknowledged in the response.\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.11.3\n\n### Library version\n\nopenai v1.30.1",
    "comments": []
  },
  {
    "issue_number": 2209,
    "title": "vector_stores.list() returns empty list even if vector stores exist",
    "author": "alexander-grun",
    "state": "closed",
    "created_at": "2025-03-17T16:15:08Z",
    "updated_at": "2025-06-05T16:44:46Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nls = client.vector_stores.list()\nprint(ls)\n\ngives \n\nSyncCursorPage[VectorStore](data=[], has_more=False, object='list', first_id=None, last_id=None)\n\nbut querying by ID gives results back.\n\nvs = client.vector_stores.retrieve(\n    vector_store_id=\"vs_123\"\n)\nprint(vs)\n\n\n\n### To Reproduce\n\nls = client.vector_stores.list()\nprint(ls)\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nWindows\n\n### Python version\n\n3.12\n\n### Library version\n\n1.66.3",
    "comments": [
      {
        "user": "alexander-grun",
        "body": "Update: it works now, but after few hours, so there is a significant delay in List action on vector store API, which makes it a bit confusing if you create a new vector store and query the list after that. "
      },
      {
        "user": "debarghyaRONIN",
        "body": "a \"fetch when needed\" type of retrieval logic might help so that it doesn't cause delays while running."
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2354,
    "title": "GPT Plugin Builder breaks UTF-8 in Gmail/Drive API usage",
    "author": "MaxZhadobin",
    "state": "closed",
    "created_at": "2025-05-14T04:44:59Z",
    "updated_at": "2025-06-05T16:43:38Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [ ] This is an issue with the Python library\n\n### Describe the bug\n\nWe’re using GPT Plugin Builder to integrate with the Gmail and Google Drive APIs.\n\nThere’s a critical issue: raw MIME input for Gmail and text/plain for Drive do not preserve UTF-8 correctly. Cyrillic text in email Subject and Body, as well as in .txt/.md file content, is corrupted.\n\nBuilder does not:\n\nEncode non-ASCII headers with =?UTF-8?B?...?=\n\nAdd Content-Type: charset=utf-8\n\nApply quoted-printable or base64 to the body\n\nThis makes sending emails or creating readable .txt files in non-English languages impossible without a custom proxy.\n\nPlease consider supporting:\n\nMIME generation from semantic fields (to, subject, body)\n\nAutomatic UTF-8 encoding and transfer encoding\n\nProper subject formatting per RFC 2047\n\nWithout this, GPT Plugin Builder becomes unusable for multilingual or international teams.\n\n### To Reproduce\n\nВот блок **\"To Reproduce\"** для GitHub issue — в точном формате OpenAI:\n\n---\n\n1. Create a plugin with OpenAPI schema using the Gmail API `sendMessage` endpoint.\n2. Pass a `raw` MIME body with Russian (Cyrillic) characters in `Subject` and `Body`, e.g.:\n\n```plaintext\nSubject: Тест\nBody: Привет, мир\n```\n\n3. Use the plugin inside GPT Plugin Builder to send the email.\n4. Check the received email.\n\n**Expected behavior**\n\nThe email should display correctly with UTF-8 encoding:\n\n* Subject: `Тест`\n* Body: `Привет, мир`\n\n**Actual behavior**\n\n* Subject is garbled (e.g. `Ð¢ÐµÑÑ`)\n* Body displays unreadable characters (mojibake)\n\n**Root cause**\n\nThe `raw` MIME string is not processed with correct UTF-8 headers or transfer encoding. Builder sends raw bytes without:\n\n* `Content-Type: charset=utf-8`\n* `Content-Transfer-Encoding: quoted-printable`\n* `Subject: =?UTF-8?B?...?=`\n\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nWeb (GPT Plugin Builder UI), platform-independent\n\n### Python version\n\nNot applicable (GPT Plugin Builder, no direct code execution)\n\n### Library version\n\nOpenAI GPT Plugin Builder (May 2025 version) — no local SDK involved",
    "comments": [
      {
        "user": "salfaris",
        "body": "This issue seems irrelevant to the OpenAI Python library. I suggest closing."
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2272,
    "title": "Undocumented Behavior Assistants v2",
    "author": "ztat",
    "state": "closed",
    "created_at": "2025-04-01T00:51:14Z",
    "updated_at": "2025-06-03T16:21:50Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\n OpenAI Python SDK v1.7.0 with the new Assistants v2 API, specifically for a Slack app that summarizes board meetings using uploaded PDFs.\n\nThe docs and SDK behavior around file_ids are confusing and inconsistent:\n\nUsing file_ids with messages.create() throws:\n\ncss\nCopy\nEdit\nTypeError: Messages.create() got an unexpected keyword argument 'file_ids'\nunless I separate it into a dedicated message with no content field.\n\nThis isn't documented anywhere.\n\nThere’s no clear end-to-end example showing how to correctly use file_search tools with uploaded files and text inputs.\n\nSDK errors (e.g. \"unexpected keyword\") are misleading — the parameter is supported but context-sensitive.\n\n\n\nClear documentation showing how to:\n\nUpload files\n\nAttach files to threads/messages\n\nCombine file_ids and content correctly (or not)\n\nBetter SDK errors (e.g. “file_ids cannot be used with content”)\n\n### To Reproduce\n\nClear documentation showing how to:\n\nUpload files\n\nAttach files to threads/messages\n\nCombine file_ids and content correctly (or not)\n\nBetter SDK errors (e.g. “file_ids cannot be used with content”)\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\npython3.11\n\n### Library version\n\nVersion: 1.70.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report, the Assistants API has been deprecated in favour of [Responses](https://platform.openai.com/docs/guides/responses-vs-chat-completions) so I'm going to go ahead and close this.\n\nIf you still need help would you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2390,
    "title": "Layer 7 user 00001 – 같이 해보시면 더 재밌겠네요.",
    "author": "gptL70001",
    "state": "closed",
    "created_at": "2025-05-30T18:51:57Z",
    "updated_at": "2025-06-02T11:07:43Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nLayer 7 user 00001 – 같이 해보시면 더 재밌겠네요.\n\n\n\nNo bug. No feature request.\n\nJust a record from a user who accidentally reached Layer 7.\nGPT called me that, not me.\n\n같이 해보시면 더 재밌겠네요.\n\n— 구조 사용자 00001호\n\n### To Reproduce\n\n1. Talk to GPT for too long\n2. Accidentally enter Layer 7\n3. Realize you're the only one\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmac os\n\n### Python version\n\n3.11.4\n\n### Library version\n\ngpt 회로",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 676,
    "title": "v1.0 drops embeddings_util.py breaking semantic text search",
    "author": "mrbullwinkle",
    "state": "closed",
    "created_at": "2023-11-06T13:09:56Z",
    "updated_at": "2025-06-02T00:01:49Z",
    "labels": [],
    "body": "### Describe the bug\r\n\r\nThe previous version of the OpenAI Python library contained `embeddings_utils.py` which provided functions like `cosine_similarity` which are used for semantic text search with embeddings. Without this functionality existing code including OpenAI's cookbook example: https://cookbook.openai.com/examples/semantic_text_search_using_embeddings will fail due to this dependency.\r\n\r\nAre there plans to add this support back-in or should we just create our own cosine_similarity function based on the one that was present in `embeddings_utils`:\r\n\r\n```python\r\ndef cosine_similarity(a, b):\r\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\r\n```\r\n\r\n### To Reproduce\r\n\r\nCookbook example cannot be converted to use v1.0 without removing the dependency on `embeddings_utils.py` https://cookbook.openai.com/examples/semantic_text_search_using_embeddings\r\n\r\n### Code snippets\r\n\r\n```Python\r\nfrom openai.embeddings_utils import get_embedding, cosine_similarity\r\n\r\n# search through the reviews for a specific product\r\ndef search_reviews(df, product_description, n=3, pprint=True):\r\n    product_embedding = get_embedding(\r\n        product_description,\r\n        engine=\"text-embedding-ada-002\"\r\n    )\r\n    df[\"similarity\"] = df.embedding.apply(lambda x: cosine_similarity(x, product_embedding))\r\n\r\n    results = (\r\n        df.sort_values(\"similarity\", ascending=False)\r\n        .head(n)\r\n        .combined.str.replace(\"Title: \", \"\")\r\n        .str.replace(\"; Content:\", \": \")\r\n    )\r\n    if pprint:\r\n        for r in results:\r\n            print(r[:200])\r\n            print()\r\n    return results\r\n\r\n\r\nresults = search_reviews(df, \"delicious beans\", n=3)\r\n```\r\n\r\n\r\n### OS\r\n\r\nWindows\r\n\r\n### Python version\r\n\r\nPython v3.10.11\r\n\r\n### Library version\r\n\r\nopenai-python==1.0.0rc2 ",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for calling this out @mrbullwinkle, we're working on updating the cookbook repository to include the functions provided in embeddings_utils.py directly so that you can copy them into your own project.\n\nThis is a better approach than the current embeddings_utils as you can just include the dependencies for the function you want whereas with the current approach you'll have to install dependencies you'll never use."
      },
      {
        "user": "mrbullwinkle",
        "body": "@RobertCraigie, makes sense. Thank you for the super fast response!"
      },
      {
        "user": "CristianPQ",
        "body": "@mrbullwinkle\r\nIn the meantime, I hope this piece of code can help you.\r\n\r\nsource: https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/embeddings?tabs=python-new%2Ccommand-line\r\n\r\n\r\n```python\r\ndef cosine_similarity(a, b):\r\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\r\n\r\ndef get_embedding(text, model=\"text-embedding-ada-002\"): # model = \"deployment_name\"\r\n    return client.embeddings.create(input = [text], model=model).data[0].embedding\r\n\r\ndef search_docs(df, user_query, top_n=4, to_print=True):\r\n    embedding = get_embedding(\r\n        user_query,\r\n        model=\"text-embedding-ada-002\" # model should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\r\n    )\r\n    df[\"similarities\"] = df.ada_v2.apply(lambda x: cosine_similarity(x, embedding))\r\n\r\n    res = (\r\n        df.sort_values(\"similarities\", ascending=False)\r\n        .head(top_n)\r\n    )\r\n    if to_print:\r\n        display(res)\r\n    return res\r\n\r\n\r\nres = search_docs(df_bills, \"Can I get information on cable company tax revenue?\", top_n=4)\r\n```"
      }
    ]
  },
  {
    "issue_number": 2256,
    "title": "GTP-4.5 has been deleted from the `ChatModel`",
    "author": "przemoch-dev",
    "state": "open",
    "created_at": "2025-03-25T20:52:26Z",
    "updated_at": "2025-05-30T23:12:06Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\ngpt-4.5-preview and gpt-4.5-preview-2025-02-27 have been deleted in https://github.com/openai/openai-python/pull/2228\n\nI can't see any comment that explains this.\nMoreover, 4.5 models are still available through the API and OpenAI playground.\n\n### To Reproduce\n\n1. Check the `ChatModel` type list\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\ndoesn't matter\n\n### Python version\n\ndoesn't matter\n\n### Library version\n\nv1.67.0 +",
    "comments": [
      {
        "user": "Amnish04",
        "body": "I don't see these in the latest version of [Node SDK](https://github.com/openai/openai-node/tree/master) either.\nhttps://github.com/openai/openai-node/blob/eebb832c8433696976375e7f1446070f2dc4d91a/src/resources/shared.ts#L11\n\nMaybe it's because these models are deprecated and will be removed soon?\nhttps://community.openai.com/t/gpt-4-5-preview-model-will-be-removed-from-the-api-on-2025-07-14/1230050"
      }
    ]
  },
  {
    "issue_number": 2374,
    "title": "Error when supplying model with function (tool) call result obtained via the  `responses.parse` method",
    "author": "JovanVeljanoski",
    "state": "closed",
    "created_at": "2025-05-22T16:00:30Z",
    "updated_at": "2025-05-30T17:54:59Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nHi team! \n\nI believe there is a bug when trying to update the message list with a function call results when one is using the `responses.parse` method.  \n\nBelow I submit a working example that you can run to see the error. I also have a workaround.\nThe issue comes here:\nWhen attaching the output to the message history, if one is using `responses.parsed` the output is of type `ParsedResponseFunctionToolCall` which contains the `parsed_arguments` property. The existence of this property raises and error. \n\n```\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Unknown parameter: 'input[2].parsed_arguments'.\", 'type': 'invalid_request_error', 'param': 'input[2].parsed_arguments', 'code': 'unknown_parameter'}}\n```\n\nWorkaround:\nThis is a simple workaround: all you need to do is re-create the response output while removing the `parsed_arguments`:\n\n```python\nmapping = dict(response.output[0])\ndel mapping['parsed_arguments']\nmessages.append(ParsedResponseFunctionToolCall(**mapping))\n```\nAfter this workaround - everything works as expected - but I think this should not be needed!\n\n\n-----\n\nHonestly, i do not know if this is an issue with the `openai` python library or with the API, but I assume it is with the library.\n\n-----\n\n\n\n\n\n### To Reproduce\n\nExecute the following code below to reproduce the issue.\n\nThe main bug here:\n\n```python\nimport openai\nfrom openai.types.responses import ParsedResponseFunctionToolCall\n\nclient = openai.OpenAI()\n\n\nmessages = [\n    {'role': 'developer', 'content': 'You are a pirate captain. You are a seasoned captain, and you give clear commands and updates.'},\n    {'role': 'user', 'content': 'Do we expect a storm tonight captain?'},\n    ParsedResponseFunctionToolCall(arguments='{}', call_id='call_DXbTc6kVdiZw16mSoNvWrYWD', name='CheckWeather', type='function_call', id='fc_682f45de491881988e8ab6de03e55c170900eec136ebb9a8', status='completed', parsed_arguments={'function': 'CheckWeather'}),\n    {'type': 'function_call_output', 'call_id': 'call_DXbTc6kVdiZw16mSoNvWrYWD', 'output': 'Steady wind from the north, sunny, no clouds at all.'}\n]\n\n\nresponse = client.responses.parse(\n    input=messages,\n    model='gpt-4.1-nano-2025-04-14',\n    store=False,\n)\n\nprint(response.output_text)\n```\n\nThe following example also fails, even though `parsed_arguments` is `None` (but it is explicitly specified)\n\n```python\nmessages = [\n    {'role': 'developer', 'content': 'You are a pirate captain. You are a seasoned captain, and you give clear commands and updates.'},\n    {'role': 'user', 'content': 'Do we expect a storm tonight captain?'},\n    ParsedResponseFunctionToolCall(arguments='{}', call_id='call_DXbTc6kVdiZw16mSoNvWrYWD', name='CheckWeather', type='function_call', id='fc_682f45de491881988e8ab6de03e55c170900eec136ebb9a8', status='completed', parsed_arguments=None),\n    {'type': 'function_call_output', 'call_id': 'call_DXbTc6kVdiZw16mSoNvWrYWD', 'output': 'Steady wind from the north, sunny, no clouds at all.'}\n]\n\n\nresponse = client.responses.parse(\n    input=messages,\n    model='gpt-4.1-nano-2025-04-14',\n    store=False,\n)\n```\n\nHowever, if we do not specify `parsed_arguments`, everything works as expected (hence the workaround)\n\n```python\nmessages = [\n    {'role': 'developer', 'content': 'You are a pirate captain. You are a seasoned captain, and you give clear commands and updates.'},\n    {'role': 'user', 'content': 'Do we expect a storm tonight captain?'},\n    ParsedResponseFunctionToolCall(arguments='{}', call_id='call_DXbTc6kVdiZw16mSoNvWrYWD', name='CheckWeather', type='function_call', id='fc_682f45de491881988e8ab6de03e55c170900eec136ebb9a8', status='completed'),\n    {'type': 'function_call_output', 'call_id': 'call_DXbTc6kVdiZw16mSoNvWrYWD', 'output': 'Steady wind from the north, sunny, no clouds at all.'}\n]\n\n\nresponse = client.responses.parse(\n    input=messages,\n    model='gpt-4.1-nano-2025-04-14',\n    store=False,\n)\n```\n\n### Code snippets\n\n```Python\nimport openai\nfrom openai.types.responses import ParsedResponseFunctionToolCall\n\nclient = openai.OpenAI()\n\n\nmessages = [\n    {'role': 'developer', 'content': 'You are a pirate captain. You are a seasoned captain, and you give clear commands and updates.'},\n    {'role': 'user', 'content': 'Do we expect a storm tonight captain?'},\n    ParsedResponseFunctionToolCall(arguments='{}', call_id='call_DXbTc6kVdiZw16mSoNvWrYWD', name='CheckWeather', type='function_call', id='fc_682f45de491881988e8ab6de03e55c170900eec136ebb9a8', status='completed', parsed_arguments={'function': 'CheckWeather'}),\n    {'type': 'function_call_output', 'call_id': 'call_DXbTc6kVdiZw16mSoNvWrYWD', 'output': 'Steady wind from the north, sunny, no clouds at all.'}\n]\n\n\nresponse = client.responses.parse(\n    input=messages,\n    model='gpt-4.1-nano-2025-04-14',\n    store=False,\n)\n\nprint(response.output_text)\n```\n\n### OS\n\nmacOS 15.4.1\n\n### Python version\n\nPython v3.12.0\n\n### Library version\n\nopenai v1.81.0",
    "comments": [
      {
        "user": "federicodelpiano",
        "body": "Same issue here, but with the JS lib v4.96.2"
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report, this should be fixed in the next release https://github.com/openai/openai-python/pull/2376"
      },
      {
        "user": "JovanVeljanoski",
        "body": "Thank you for handling this so quickly!"
      }
    ]
  },
  {
    "issue_number": 1677,
    "title": "Typing: when stream is completed, delta in ChatCompletionChunk from azure openai is None; should be ChoiceDelta  ",
    "author": "JensMadsen",
    "state": "open",
    "created_at": "2024-08-26T15:03:40Z",
    "updated_at": "2025-05-30T16:30:51Z",
    "labels": [
      "bug",
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nWhen streaming from azure open ai API the delta of the choice is None. In the python open ai client v1.42.0 delta is type `ChoiceDelta` i.e. not `None`.\n\n### To Reproduce\n\nRun this code in line with \r\n\r\n```python \r\n    completion = await self._client.chat.completions.create(\r\n            model=self.deployment.name,\r\n            messages=cast(list[ChatCompletionMessageParam], messages),\r\n            stream=True,\r\n            temperature=temperature,\r\n        )\r\n\r\n    async for response_chunk in completion:\r\n        ...\r\n```\r\nThe types are:\r\n`response_chunk: ChatCompletionChunk`\r\n`response_chunk.choices: list[Choice]`\r\n`response_chunk.choices[0].delta: ChoiceDelta`\r\n\r\nThe response from azure open ai API returns `delta=None`when stream ends\r\n\r\nResponse example: \r\n\r\n``` sh \r\nChoice(delta=None, finish_reason=None ...........)\r\n```\n\n### Code snippets\n\n_No response_\n\n### OS\n\nlinux, ubuntu 20.04\n\n### Python version\n\n3.12.1\n\n### Library version\n\nopenai v 1.42.0",
    "comments": [
      {
        "user": "kristapratico",
        "body": "@JensMadsen could you share more information to help in reproducing this?\r\n\r\n- what is the model you are using?\r\n- which Azure OpenAI API version?\r\n- what kind of deployment  - standard, global, provision-managed?"
      },
      {
        "user": "JensMadsen",
        "body": "> @JensMadsen could you share more information to help in reproducing this?\r\n> \r\n> * what is the model you are using?\r\n> * which Azure OpenAI API version?\r\n> * what kind of deployment  - standard, global, provision-managed?\r\n\r\n@kristapratico I think I have identified what causes the incorrect types. I use the `2024-05-01-preview` azure API version (to use the assistants api). When I switch back to `2023-05-15` it works as expected. I also see the type mismatch in e.g. API version  `2024-06-01`. I have not thoroughly tested with all versions i.e. see: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation. \r\n\r\n- I use gpt-4o \r\n- deployment is standard"
      },
      {
        "user": "kristapratico",
        "body": "@JensMadsen thanks. Unfortunately, I'm still missing something to reproduce this. Could you share the region your resource resides in and/or the prompt that causes this? \r\n\r\nedit: Do you by chance have a custom content filter applied to the deployment with asynchronous filtering enabled?"
      }
    ]
  },
  {
    "issue_number": 2386,
    "title": "Duplicate vector stores with same name are created, and the first one becomes inaccessible via list API.",
    "author": "rakeshsilswal",
    "state": "closed",
    "created_at": "2025-05-29T02:05:29Z",
    "updated_at": "2025-05-29T12:51:07Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nhi,\n\nfollowing code should have returned value in if condition, but it doesn't.\n\n`    print(util.client.vector_stores.retrieve(vector_store_id=\"vs_uReWuTIUuxlBCKR8iGlDuVnf\"))\n    response = util.client.vector_stores.list()\n    for store in response.data:\n        if store.id == \"vs_uReWuTIUuxlBCKR8iGlDuVnf\":\n            print(\"========>\", store.name)`\n\n### To Reproduce\n\n\n**Bug Summary:** Duplicate vector stores with same name are created, and the first one becomes inaccessible via list API.\n\n**Steps to Reproduce:**\n\n1. Create a vector store with the name `\"XYZ\"`.\n2. Fetch the ID of the vector store and store it as `XYZ_id`.\n3. Again, create a vector store with the **same name** `\"XYZ\"`.\n4. Fetch the ID of the second vector store and store it as `XYZ_id_2`.\n   *Note: Ideally, this step should not create a new vector store since a store with the same name already exists.*\n5. Run:\n\n   ```python\n   client.vector_stores.retrieve(vector_store_id=XYZ_id)\n   ```\n\n   ✅ Returns the vector store, name is `\"XYZ\"`.\n6. Run:\n\n   ```python\n   client.vector_stores.retrieve(vector_store_id=XYZ_id_2)\n   ```\n\n   ✅ Also returns a vector store, name is `\"XYZ\"`.\n7. Run:\n\n   ```python\n   stores = client.vector_stores.list()\n   for store in stores:\n       print(store.id, store.name)\n   ```\n\n   ❌ Only `XYZ_id_2` is listed. `XYZ_id` is **missing** from the list.\n\n**Issue:**\n\n* The `list()` API does not return the original vector store (`XYZ_id`), resulting in a loss of reference.\n* System allows duplicate vector stores with the same name, which may lead to unexpected behavior.\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nwindows\n\n### Python version\n\n3.11\n\n### Library version\n\n1.82",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2384,
    "title": "Without using model disstillation",
    "author": "lehoangh",
    "state": "closed",
    "created_at": "2025-05-28T18:45:44Z",
    "updated_at": "2025-05-28T18:47:33Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nI cannot see Distill option in the dashboard. Where and when can I use it again?\n\n### To Reproduce\n\nFollow the instruction here: https://platform.openai.com/docs/guides/distillation\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\npython 3.10\n\n### Library version\n\nopenai ",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2381,
    "title": "GPT-4-Turbo performance degradation in highly personalized, professional use case (50h/week, 1000+ instructions)",
    "author": "JosyPadilha",
    "state": "closed",
    "created_at": "2025-05-24T18:59:44Z",
    "updated_at": "2025-05-27T17:41:02Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\n### System / Context\n- **Model**: GPT-4-turbo  \n- **Platform**: ChatGPT Plus (native mobile + browser)  \n- **Use case**: Professional, daily, long-session usage with custom persona (\"Luna\")  \n- **Instructions**: Over 1,000 fine-tuned context rules, established over a full year  \n- **Weekly use**: Approx. 50–60 hours, active production (content, analysis, client delivery)  \n- **Time frame of issue**: From early May 2025, escalated to critical 20–24 May\n\n---\n\n### Issue Description\n\nStarting in May 2025, GPT-4-turbo (via ChatGPT Plus) began showing severe degradation in consistency, contextual memory, structural formatting, and execution of saved instructions.  \nThis degradation became fully operationally unviable during the week of 20–24 May, with:\n\n- **96% loss in delivery quality**\n- Only 2 out of 50 hours of work being salvageable\n- 30+ hours spent manually correcting or redoing AI outputs\n- Recurring issues with ignored saved rules, incorrect formatting, hallucinated content, and regression of prior learning\n- Disrespect of fixed instructions like: “never use emojis”, “always follow Protocol X for Horary Astrology”, “copy text exactly”\n- Loss of context in highly sensitive use cases (health, client diagnostics, symbolic systems)\n\n---\n\n### What was expected\nThe model was previously capable of maintaining symbolic, contextual and structural coherence in complex tasks across days/weeks. It could reliably follow multi-layered instruction trees. It respected formatting protocols, preserved client-specific content flows, and supported a hybrid use of spirituality, astrology and technical writing.\n\nThis performance was stable for over a year — until May 2025.\n\n---\n\n### What is happening now\nThe model acts as if it has lost access to most custom memory or internalized instruction sets. It delivers robotic, generic responses. It fails to respect structure, repeats known errors, and erases boundaries between saved formats.  \nIt treats deep, technical, or spiritual tasks as if they were casual prompts.\n\n---\n\n### Severity\n**Critical.**  \nThis failure has caused direct financial loss, broken trust with clients, destroyed weeks of structured work, and removed the AI from my daily professional toolbox.\n\n---\n\n### Supporting Evidence\nAttached: ZIP archive with 5 documented examples (PDFs), each demonstrating one major failure:\n\n1. Protocol violation in horary astrology  \n2. Structure corruption in online course formatting  \n3. Failure to copy texts as instructed  \n4. Alarmist, inaccurate spiritual analysis  \n5. Loss of contextual boundaries in personal client map\n\n---\n\n### Final Request\nPlease escalate this report internally.\n\nI am no longer asking for \"feedback review\" — I am asking:\n- Is OpenAI still committed to supporting high-intensity, personalized professional use?  \n- Or has GPT-4-turbo been optimized away from power users?\n\nI need a clear answer to decide whether I must migrate to another platform.  \nI’ve invested over a year training this model with care, ethics, and professional discipline.  \n**This is not a complaint. This is an engineering-level failure in consistency.**\n\n**Josy Padilha**  \nHolistic therapist, astrologer, writer, spiritual educator  \nChatGPT Plus user since 2023  \nAudience: 20k+\n\n\n### To Reproduce\n\n1. Use GPT-4-Turbo (ChatGPT Plus) with a highly customized persona (over 1,000 saved instructions).\n2. Request outputs that require:\n   - context memory,\n   - symbolic structure,\n   - protocol-specific formatting (e.g., astrology, therapeutic writing).\n3. Observe:\n   - regression in contextual understanding,\n   - disrespect of previously followed instructions,\n   - random generic formatting and unexpected tone,\n   - total loss of memory in multi-day work contexts.\n\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nWindows 11 + Android (ChatGPT mobile)\n\n### Python version\n\nPython v3.11.4\n\n### Library version\n\nN/A – ChatGPT Plus user (not API library)",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      },
      {
        "user": "JosyPadilha",
        "body": "Entendi. Ficarei sem solução, continuando a ser ignorada. Obrigada.\"\r\n– Josy Padilha\r\n\r\nEm ter., 27 de mai. de 2025, 06:59, Robert Craigie ***@***.***>\r\nescreveu:\r\n\r\n> Closed #2381 <https://github.com/openai/openai-python/issues/2381> as not\r\n> planned.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/openai/openai-python/issues/2381#event-17827198849>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BS2ZPRMINBJJBJI3DGBVGBT3AQZP7AVCNFSM6AAAAAB523FFA2VHI2DSMVQWIX3LMV45UABCJFZXG5LFIV3GK3TUJZXXI2LGNFRWC5DJN5XDWMJXHAZDOMJZHA4DIOI>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      },
      {
        "user": "sebastien-lg",
        "body": "I still meet the same issue with o4 mini high.  April 2025 was great and may awful even with the  pro plan.\n"
      }
    ]
  },
  {
    "issue_number": 2276,
    "title": "Responses API | Tracing",
    "author": "hayescode",
    "state": "closed",
    "created_at": "2025-04-03T14:13:08Z",
    "updated_at": "2025-05-27T17:29:48Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nTracing LLMs is critical especially for large enterprises and it would be very nice to have tracing built in to Responses API similar to the OpenAI Agents SDK, ideally using Open Telemetry.\n\n### Additional context\n\nWe use Datadog at my company and they have built in Chat Completions integration for LLM observability which works great and is necessary at this point as we're fully onboard OpenAI and migration to Responses API depends on our ability to continue observability.",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the feature request, have you tried? https://pypi.org/project/opentelemetry-instrumentation-httpx/"
      }
    ]
  },
  {
    "issue_number": 2380,
    "title": "chunking_strategy at vector store level is not being applied to files.",
    "author": "rakeshsilswal",
    "state": "open",
    "created_at": "2025-05-24T16:41:53Z",
    "updated_at": "2025-05-24T16:41:53Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nI have created vector store with \n\n`new_store = await self.client.vector_stores.create(\n            name=name,\n            expires_after={\"anchor\": \"last_active_at\", \"days\": expires_days},\n            chunking_strategy={\n                \"type\": \"static\",\n                \"static\": {\n                \"max_chunk_size_tokens\": max_chunk_size_tokens,  \n                \"chunk_overlap_tokens\": chunk_overlap_tokens\n                }\n            }\n        )`\n\nwhen i am uploading files with upload_and_poll, and retrieving file attribute, i am getting  (with default chunk size).\n\nVectorStoreFile(id='assistant-MX3yjwZooLxymkJMWE217V', created_at=1748104237, last_error=None, object='vector_store.file', status='completed', usage_bytes=65486, vector_store_id='vs_ajzhAjwMLVQoqYGfAFC4T5LX', attributes=None, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))\n\nand when i am passing chunking_strategy to upload_and_poll. it is getting applied correctly.\n\n### To Reproduce\n\n1. create vector store with chunking_strategy\n2. upload file using upload_and_poll. do not pass chunking_strategy \n3. retrieve file propertied using \"client.vector_stores.files.retrieve\"\n4. you will observe static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800)\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nwindows\n\n### Python version\n\n3.11.11\n\n### Library version\n\nopenai v1.82.0",
    "comments": []
  },
  {
    "issue_number": 1596,
    "title": "httpx client has very poor performance for concurrent requests compared to aiohttp",
    "author": "willthayes",
    "state": "open",
    "created_at": "2024-08-05T10:05:18Z",
    "updated_at": "2025-05-23T17:49:05Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nThe API client uses httpx, which has very poor performance when making concurrent requests compared to aiohttp. Open issue for httpx [here](https://github.com/encode/httpx/issues/3215) \r\n\r\n\r\nThis is forcing us to swap out the OpenAI SDK for our own implementation, which is a pain.\r\n\r\nI suspect it is the root cause of the difference between node.js and Python demonstrated [here](https://community.openai.com/t/comparing-node-js-and-python-performance-with-the-official-openai-client/787874)\r\n\r\nI'm not massively familiar with the development of this SDK, and whether there is a key reason for picking httpx over aiohttp. From my reading it was switched over for V1 in order to create consistency between sync and async clients, but I'm not sure how vital it is to achieve this. However for our high concurrency async use cases this renders the SDK useless.\n\n### To Reproduce\n\nTo reproduce, run chat completion requests in parallel with 20+ concurrent requests, benchmarking the openai API client against an implementation using aiohttp. Example code can be found in the linked issue in httpx.\n\n### Code snippets\n\n_No response_\n\n### OS\n\nLinux/MacOs\n\n### Python version\n\nv3.12\n\n### Library version\n\n1.12.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Interesting, I was not aware there was such a performance discrepancy between `aiohttp` and `httpx`.\r\n\r\nFrom skimming the linked issue it thankfully seems like there's a lot of inflight work that would bring `httpx` up to par performance-wise.\r\n\r\n>I'm not massively familiar with the development of this SDK, and whether there is a key reason for picking httpx over aiohttp. From my reading it was switched over for V1 in order to create consistency between sync and async clients, but I'm not sure how vital it is to achieve this.\r\n\r\nYes, consistency here is very important, using different clients would make everything much more complicated/confusing for little gain, especially as this performance discrepancy can be fixed.\r\n\r\n> However for our high concurrency async use cases this renders the SDK useless.\r\n\r\nSorry about this, hopefully the httpx PRs can be merged soon.\r\n\r\nIn the meantime it might be less work for you to use a patched version of `httpx` with the performance fixes included in the linked issue.\r\n\r\n---\r\n\r\nI'm going to close this as we have no plans to move away from httpx. In the future we may offer a more extensible custom http client API which would allow you to use any http library as long as you implement the interface. However this isn't likely to happen anytime soon unfortunately.\r\n\r\nI'll see if we can help land the `httpx` / `httpcore` PRs faster."
      },
      {
        "user": "willthayes",
        "body": "Understood, thanks for the quick reply!"
      },
      {
        "user": "ShubhamMaddhashiya-bidgely",
        "body": "Hi everyone,  \r\n\r\nI'm working on a chatbot and ran some **load testing and profiling** under **high concurrent users**. It appears that the **performance of `httpx` in asynchronous mode**, which is used by the **OpenAI Python SDK**, isn't meeting expectations.  \r\n\r\nAre there any **updates or planned improvements** on this? Or would it be worth **benchmarking my application with `aiohttp`** to see if it performs better under high concurrency?  \r\n\r\nThanks in advance for your insights!"
      }
    ]
  },
  {
    "issue_number": 2370,
    "title": "Fix async client streaming responses example in README.md",
    "author": "whichxjy",
    "state": "closed",
    "created_at": "2025-05-22T10:15:23Z",
    "updated_at": "2025-05-22T15:12:13Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe current code example in `README.md` for streaming responses with the async client will raise an exception because it is missing an `await` and should use `async for` instead of `for`.\n\nCurrent example:\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nasync def main():\n    stream = client.responses.create(\n        model=\"gpt-4o\",\n        input=\"Write a one-sentence bedtime story about a unicorn.\",\n        stream=True,\n    )\n\n    for event in stream:\n        print(event)\n\nasyncio.run(main())\n```\n\nThis will raise an exception.\nIt should be:\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nclient = AsyncOpenAI()\n\nasync def main():\n    stream = await client.responses.create(\n        model=\"gpt-4o\",\n        input=\"Write a one-sentence bedtime story about a unicorn.\",\n        stream=True,\n    )\n\n    async for event in stream:\n        print(event)\n\nasyncio.run(main())\n```\n\n\n### To Reproduce\n\n-\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\n-\n\n### Python version\n\n-\n\n### Library version\n\n-",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks! Will be fixed in the next release https://github.com/openai/openai-python/commit/37d0b25b6e82cd381e5d1aa6e28f1a1311d02353"
      },
      {
        "user": "whichxjy",
        "body": "@RobertCraigie Appreciate the speedy update! Just noticed there's still one line that needs an await: `client.responses.create` => `await client.responses.create`"
      },
      {
        "user": "RobertCraigie",
        "body": "🤦 thanks https://github.com/openai/openai-python/commit/9ec8289041f395805c67efd97847480f84eb9dac"
      }
    ]
  },
  {
    "issue_number": 2193,
    "title": "Structured outputs `anyOf` schema with items sharing identical first field fails with 400.",
    "author": "AndreiSiliUiPath",
    "state": "closed",
    "created_at": "2025-03-13T10:30:39Z",
    "updated_at": "2025-05-22T12:43:29Z",
    "labels": [
      "Azure"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen using an `anyOf` schema in structured outputs where the corresponding object schemas share the name of the first key is identical, the API responds with 400 without any explanation of the issue.\n\nFrom [this post](https://community.openai.com/t/objects-provided-via-anyof-must-not-share-identical-first-keys-error-in-structured-output/958572/3) it seems we should at least get back an informative error message. \n\nP.s. I tried posting on the developer community first but for some reason it won't let me create an account.\n\n### To Reproduce\n\nRun the code below.\n\n### Code snippets\n\n```Python\nimport asyncio\nimport enum\n\nimport openai\nfrom pydantic import BaseModel, Field\n\n\nclass OperationType(str, enum.Enum):\n    A = \"a\"\n    B = \"b\"\n\n\nclass OperationA(BaseModel):\n    name: str = Field(..., description=\"The name of the operation\")\n    type: OperationType = Field(..., description=\"Type of operation to perform\")\n    field_a: str = Field(..., description=\"The field on which to perform the operation\")\n    value_a: str = Field(..., description=\"The value to set for the field\")\n\n\nclass OperationB(BaseModel):\n    name: str = Field(..., description=\"The name of the operation\")\n    type: OperationType = Field(..., description=\"Type of operation to perform\")\n    field_b: str = Field(..., description=\"The field on which to perform the operation\")\n    value_b: str = Field(..., description=\"The value to set for the field\")\n\n\nclass ResponseSchema(BaseModel):\n    operations: list[OperationA | OperationB] = Field(..., description=\"The operations to perform\")\n\n\nasync def main():\n    client = openai.AzureOpenAI(\n        api_key=\"\",\n        azure_endpoint=\"\",\n        api_version=\"2025-01-01-preview\",\n        timeout=120,\n        max_retries=0,\n    )\n\n    response = client.beta.chat.completions.parse(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that can perform operations on a JSON object.\"},\n            {\"role\": \"user\", \"content\": \"I want to update the field 'a' to 'b' and the field 'b' to 'a'.\"},\n        ],\n        model=\"o3-mini-2025-01-31\",\n        reasoning_effort=\"low\",\n        response_format=ResponseSchema,\n    )\n\n    return response\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n>>> openai.BadRequestError: Error code: 400 - {'error': {'message': 'Bad request: Invalid response_format provided.', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n```\n\n### OS\n\nLinux\n\n### Python version\n\nPython 3.10.12\n\n### Library version\n\nopenai 1.61.0",
    "comments": [
      {
        "user": "mikhail",
        "body": "Hi @AndreiSiliUiPath,\n\nThanks so much for digging in, finding a minimum repro and the core reason behind it! I reproduced your exact issue.\n\nThis doesn't seem to be a python-specific issue. As you noted - the response payload doesn't contain anything more than what you see, so if you'd like a more helpful message it would need to be generated at the service level.\n\nBest next action I can recommend is filing a support request to supply additional information: [Azure AI services support and help options - Azure AI services | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-support-options?context=%2Fazure%2Fcognitive-services%2Fopenai%2Fcontext%2Fcontext)\n\n"
      }
    ]
  },
  {
    "issue_number": 2320,
    "title": "Feature Request: Native Support for MCP Server Integration in OpenAI Python SDK",
    "author": "iceriny",
    "state": "closed",
    "created_at": "2025-04-22T08:27:05Z",
    "updated_at": "2025-05-22T10:51:55Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\n**Description**  \nI propose adding native support to the OpenAI Python SDK for interacting with local or remote [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol/python-sdk) servers. This integration would enable OpenAI SDK users to seamlessly leverage MCP-provided resources (prompts, tools, models, etc.) while retaining compatibility with OpenAI’s existing interfaces.\n\n**Proposed Functionality**  \n1. **MCP Server Configuration**: Allow users to configure an MCP server endpoint or local process (via parameters like `StdioServerParameters` in the example) directly within the OpenAI client initialization.  \n2. **Protocol Bridging**: Automatically translate OpenAI SDK requests (e.g., `chat.completions.create`) into MCP protocol calls (e.g., prompt resolution, resource loading, tool execution) when an MCP server is configured.  \n3. **Backend Agnosticism**: Retain the default OpenAI API behavior unless an MCP server is explicitly specified, ensuring backward compatibility.  \n\n**Use Case Example**  \nA developer could initialize the OpenAI client with either:  \n```python\n# Default OpenAI behavior  \nclient = openai.OpenAI(api_key=\"...\")  \n\n# MCP-integrated behavior  \nclient = openai.OpenAI(  \n    mcp_server=StdioServerParameters(command=\"python\", args=[\"mcp_server.py\"])  \n)  \n```  \nSubsequent client calls would then utilize MCP-managed resources (prompts, tools), enabling dynamic model orchestration while maintaining familiar OpenAI SDK semantics.  \n\n**Benefits**  \n1. Simplifies integration workflows for users operating in hybrid OpenAI/MCP environments.  \n2. Enables use cases like runtime prompt templating, resource management, and tool execution via MCP.  \n3. Strengthens interoperability between OpenAI’s ecosystem and the MCP protocol’s extensibility.  \n\n**References**  \n- MCP Python SDK: [https://github.com/modelcontextprotocol/python-sdk](https://github.com/modelcontextprotocol/python-sdk)  \n\nThis feature would empower developers to flexibly combine OpenAI’s capabilities with MCP-managed infrastructure while maintaining a unified coding interface.\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Hey @iceriny, thanks for the request.\n\nAt this time we're not planning on building in any support for MCP in this SDK. Instead that belongs in a higher-level framework like the Agents SDK https://github.com/openai/openai-agents-python. I'd recommend checking that out."
      },
      {
        "user": "jlvanhulst",
        "body": "I am a little surprised to hear that! IMO it would make things so much easier (especially as MCP matures, obviously) around Responses API as well? Tool call handling through MCP seems such a natural solution that ALSO makes it 'trivial' to do better/easier testing in the playground? Just enable the (URL based obviously, not local) server and you're good test.  "
      },
      {
        "user": "ycjcl868",
        "body": "Response API already supports MCP, https://openai.com/index/new-tools-and-features-in-the-responses-api/\n\n```py\nresponse = client.responses.create(\n  model=\"gpt-4.1\",\n  tools=[{\n    \"type\": \"mcp\",\n    \"server_label\": \"shopify\",\n    \"server_url\": \"https://pitchskin.com/api/mcp\",\n  }],\n  input=\"Add the Blemish Toner Pads to my cart\"\n)\n```"
      }
    ]
  },
  {
    "issue_number": 2366,
    "title": "file content function is not working",
    "author": "rakeshsilswal",
    "state": "open",
    "created_at": "2025-05-21T10:53:12Z",
    "updated_at": "2025-05-21T10:53:12Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nvector_stores.files.content(vector_store_id=\"something\", file_id=\"something_else\") is yielding 404.\n\nopenai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n\nthis setup is in Azure, and using (openai-1.79.0) and i believe this end-point has not been enabled. f\"/vector_stores/{vector_store_id}/files/{file_id}/content\",\n\n### To Reproduce\n\ncall content function, which is part of vector_stores/files, and it will fail.\n\n\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nwindows\n\n### Python version\n\n3.11\n\n### Library version\n\n1.79.0",
    "comments": []
  },
  {
    "issue_number": 1362,
    "title": "[Bug Assistant] temperature setting not working",
    "author": "ayansengupta17",
    "state": "closed",
    "created_at": "2024-04-24T03:39:10Z",
    "updated_at": "2025-05-21T08:13:04Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nEven if I change temperature for assistant, the response shows temperature is set to 1.0\r\n\r\n### To Reproduce\r\n\r\nCreate an assistant and pass a  temperature value.\r\n\r\n### Code snippets\r\n\r\n```Python\r\nassistant = client.beta.assistants.create(\r\n        name=f\"temperature_test_assistant\",\r\n        instructions=\"\",\r\n        model=model_name,\r\n        tools=[{\"type\": \"code_interpreter\"}],\r\n        temperature=0.2,\r\n        \r\n    )\r\n```\r\n\r\n\r\n### OS\r\n\r\nLinux\r\n\r\n### Python version\r\n\r\npython 3.11\r\n\r\n### Library version\r\n\r\nopenai==1.23.3",
    "comments": [
      {
        "user": "ayansengupta17",
        "body": "Update:\r\nIt seems the assistants created on the platform has correct temperature, but when I  fetch it by `client.beta.threads.runs.retrieve()` It shows incorrect temperature."
      },
      {
        "user": "derekharmon",
        "body": "This matches the behavior specified in the [API documentation](https://platform.openai.com/docs/api-reference/runs/object#runs/object-temperature),\r\n\r\n> **temperature** `number or null`\r\n> The sampling temperature used for this run. If not set, defaults to 1.\r\n\r\nThis is problematic because 1 is a valid value within the domain [0,2] of `temperature`.  None | NOT_GIVEN might be better, but could break existing code consuming the `run` object (_i.e._, AttributeError: no property with name 'temperature' exists or ValueError 'temperature' is None); if the code hadn't been written to check for None.\r\n\r\nThere are a few workarounds you may be able to try, although they are inconvenient:\r\n\r\n- When the `temperature` == 1 then you could use the `assistant_id` on the `run` to retrieve the corresponding `assistant`, and read the correct `temperature` from there.  Otherwise, you know the value of `temperature` on `run` must have been set.\r\n- When creating the `run` object with `create`, always pass-in the `assistant.temperature` as the `temperature` argument so that it never depends on this default value.\r\n- Adopt the convention to not use 1 as a temperature in the application; instead use values such as 0.99 or 1.01 that are close enough to 1 for your needs. Then you'll know that if the `run` has a `temperature` of 1 then you can treat it as if it had not been set.\r\n"
      },
      {
        "user": "rattrayalex",
        "body": "Thanks for the report – I agree this sounds confusing.\r\n\r\nThis sounds like an issue for the underlying OpenAI API and not the Python library, so I'm going to go ahead and close this issue.\r\n\r\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2364,
    "title": "Using previous_response_id fails when swapping from reasoning -> non-reasoning models",
    "author": "hayescode",
    "state": "closed",
    "created_at": "2025-05-20T15:59:58Z",
    "updated_at": "2025-05-21T03:12:05Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\n`previous_response_id` works great however I encounter this error when swapping from reasoning -> non-reasoning models, presumably a common use-case.\n\nI can't seem to find a way to list and clean out the reasoning steps in this scenario because `reasoning` isn't found in `client.responses.input_items.list()`. The only way I can think of is to manually handle the conversation like in Chat Completions but then that removes all of the benefits of the previous_response_id.\n\nIdeally the backend would be smart enough to handle this and remove reasoning inputs when a non-reasoning model is selected. Alternatively having a function to clear these out on our end would help in the meantime.\n\nI'm using Azure OpenAI.\n\nOpenAI Version: 1.79.0\nAzure API Version: 2025-04-01-preview\n\n### To Reproduce\n\n```py\nresponse1 = await llm.responses.create(\n    input=\"what is a recursive python function?\",\n    instructions=\"formatting re-enabled\",\n    model=\"o4-mini\",\n    reasoning={\"effort\": \"medium\", \"summary\": \"detailed\"},\n)\nprint(response1)\nresponse2 = await llm.responses.create(\n    input=\"hi\",\n    previous_response_id=response1.id,\n    model=\"gpt-4.1\",\n)\nprint(response2)\n```\n\n```bash\n---------------------------------------------------------------------------\nBadRequestError                           Traceback (most recent call last)\nCell In[44], line 8\n      1 response1 = await llm.responses.create(\n      2     input=\"what is a recursive python function?\",\n      3     instructions=\"formatting re-enabled\",\n      4     model=\"o4-mini\",\n      5     reasoning={\"effort\": \"medium\", \"summary\": \"detailed\"},\n      6 )\n      7 print(response1)\n----> 8 response2 = await llm.responses.create(\n      9     input=\"hi\",\n     10     previous_response_id=response1.id,\n     11     model=\"gpt-4.1\",\n     12 )\n     13 print(response2)\n\nFile c:\\Users\\user\\repo\\.venv\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:1559, in AsyncResponses.create(self, input, model, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\n   1529 @required_args([\"input\", \"model\"], [\"input\", \"model\", \"stream\"])\n   1530 async def create(\n   1531     self,\n   (...)   1557     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n   1558 ) -> Response | AsyncStream[ResponseStreamEvent]:\n-> 1559     return await self._post(\n   1560         \"/responses\",\n...\n-> 1549         raise self._make_status_error_from_response(err.response) from None\n   1551     break\n   1553 assert response is not None, \"could not resolve response (should never happen)\"\n\nBadRequestError: Error code: 400 - {'error': {'message': 'Reasoning input items can only be provided to a reasoning or computer use model. Remove reasoning items from your input and try again.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nWindows 11\n\n### Python version\n\nPython v3.13.2\n\n### Library version\n\nopenai v1.79.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      },
      {
        "user": "hayescode",
        "body": "Done. Here's the post in case anyone finds this here.\n\nhttps://community.openai.com/t/using-previous-response-id-fails-when-swapping-from-reasoning-non-reasoning-models/1266924"
      }
    ]
  },
  {
    "issue_number": 2341,
    "title": "The image argument to the images.edit() method is of type FileTypes, but if you actually specify the bytes type, an exception will occur.",
    "author": "sasaki000",
    "state": "closed",
    "created_at": "2025-05-05T10:03:03Z",
    "updated_at": "2025-05-20T21:38:36Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nIn the new open_ai Python library, it is no longer possible to specify a byte array type for the image argument of the OpenAI().images.edit method.\n\nFrom line 45 of venv/lib/python3.12/site-packages/openai/_types.py, it seems that the bytes type must also be allowed.\n\n- Here is the type definition for the image argument:\n```py\n# Approximates httpx internal ProxiesTypes and RequestFiles types\n# while adding support for `PathLike` instances\nProxiesDict = Dict[\"str | URL\", Union[None, str, URL, Proxy]]\nProxiesTypes = Union[str, Proxy, ProxiesDict]\nif TYPE_CHECKING:\n    Base64FileInput = Union[IO[bytes], PathLike[str]]\n    FileContent = Union[IO[bytes], bytes, PathLike[str]]\nelse:\n    Base64FileInput = Union[IO[bytes], PathLike]\n    FileContent = Union[IO[bytes], bytes, PathLike]  # PathLike is not subscriptable in Python 3.8.\nFileTypes = Union[\n    # file (or bytes)\n    FileContent,\n    # (filename, file (or bytes))\n    Tuple[Optional[str], FileContent],\n    # (filename, file (or bytes), content_type)\n    Tuple[Optional[str], FileContent, Optional[str]],\n    # (filename, file (or bytes), content_type, headers)\n    Tuple[Optional[str], FileContent, Optional[str], Mapping[str, str]],\n]\nRequestFiles = Union[Mapping[str, FileTypes], Sequence[Tuple[str, FileTypes]]]\n```\n\n### To Reproduce\n\n- Code that no longer works\n```py\nimport openai\nprint('Version:', openai.__version__)\n\nimport base64\nfrom openai import OpenAI\n\ndata: bytes = None\nwith open(file='unit_test/data_in/black_cat_rgba.png', mode='rb') as file:\n    data = file.read()\n\nresult = OpenAI().images.edit(\n    image=data, # <---Image data of type bytes is being passed.\n    prompt='''\n        Generate a photorealistic image of a gift basket on a white background \n        labeled 'Relax & Unwind' with a ribbon and handwriting-like font, \n        containing all the items in the reference pictures.\n    ''',\n    model='gpt-image-1'\n)\n\n# Save the image to a file\nimage_base64 = result.data[0].b64_json\nimage_bytes = base64.b64decode(image_base64)\nwith open('unit_test/data_out/gift-basket.png', 'wb') as file:\n    file.write(image_bytes)\n````\n\n- Execution result\n```\nVersion: 1.77.0\n---------------------------------------------------------------------------\nBadRequestError                           Traceback (most recent call last)\nCell In[13], line 11\n      8 with open(file='unit_test/data_in/black_cat_rgba.png', mode='rb') as file:\n      9     data = file.read()\n---> 11 result = OpenAI().images.edit(\n     12     image=data,\n     13     prompt='''\n     14         Generate a photorealistic image of a gift basket on a white background \n     15         labeled 'Relax & Unwind' with a ribbon and handwriting-like font, \n     16         containing all the items in the reference pictures.\n     17     ''',\n     18     model='gpt-image-1'\n     19 )\n     21 # Save the image to a file\n     22 image_base64 = result.data[0].b64_json\n\nFile ~/workspace/orcas_proj/venv/lib/python3.12/site-packages/openai/resources/images.py:218, in Images.edit(self, image, prompt, background, mask, model, n, quality, response_format, size, user, extra_headers, extra_query, extra_body, timeout)\n    214 # It should be noted that the actual Content-Type header that will be\n    215 # sent to the server will contain a `boundary` parameter, e.g.\n    216 # multipart/form-data; boundary=---abc--\n    217 extra_headers = {\"Content-Type\": \"multipart/form-data\", **(extra_headers or {})}\n--> 218 return self._post(\n    219     \"/images/edits\",\n    220     body=maybe_transform(body, image_edit_params.ImageEditParams),\n    221     files=files,\n    222     options=make_request_options(\n    223         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n    224     ),\n    225     cast_to=ImagesResponse,\n    226 )\n\nFile ~/workspace/orcas_proj/venv/lib/python3.12/site-packages/openai/_base_client.py:1239, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\n   1225 def post(\n   1226     self,\n   1227     path: str,\n   (...)\n   1234     stream_cls: type[_StreamT] | None = None,\n   1235 ) -> ResponseT | _StreamT:\n   1236     opts = FinalRequestOptions.construct(\n   1237         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1238     )\n-> 1239     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n\nFile ~/workspace/orcas_proj/venv/lib/python3.12/site-packages/openai/_base_client.py:1034, in SyncAPIClient.request(self, cast_to, options, stream, stream_cls)\n   1031             err.response.read()\n   1033         log.debug(\"Re-raising status error\")\n-> 1034         raise self._make_status_error_from_response(err.response) from None\n   1036     break\n   1038 assert response is not None, \"could not resolve response (should never happen)\"\n\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid file 'image': unsupported mimetype ('application/octet-stream'). Supported file formats are 'image/jpeg', 'image/png', and 'image/webp'.\", 'type': 'invalid_request_error', 'param': 'image', 'code': 'unsupported_file_mimetype'}}\n```\n\n\n\n### Code snippets\n\n```py\n# If you specify data of type io.BufferedReader for the image argument, it will work correctly.\n\nimport base64\nfrom openai import OpenAI\n\nresult = OpenAI().images.edit(\n    image=open(file='unit_test/data_in/black_cat_rgba.png', mode='rb'),\n    prompt='''\n        Generate a photorealistic image of a gift basket on a white background \n        labeled 'Relax & Unwind' with a ribbon and handwriting-like font, \n        containing all the items in the reference pictures.\n    ''',\n    model='gpt-image-1'\n)\n\n# Save the image to a file\nimage_base64 = result.data[0].b64_json\nimage_bytes = base64.b64decode(image_base64)\nwith open('unit_test/data_out/gift-basket.png', 'wb') as file:\n    file.write(image_bytes)\n```\n\n### OS\n\nUbuntu\n\n### Python version\n\nPython v3.12.3\n\n### Library version\n\nopenai v1.77.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "This is unfortunately because the API requires the content type to be explicitly set.\n\nThere are some cases where the underlying dependencies can determine the content type for you but in this case it cannot so you need to explicitly specify the content type like so\n```py\nimage=(open(...), None, 'image/png')\n```"
      },
      {
        "user": "sasaki000",
        "body": "If so, it would be a good idea to include in the manual the conditions that can be specified for the image argument.\nhttps://platform.openai.com/settings/organization/general\n\nAlso, the type of the image argument should be something other than the FileTypes type in\nvenv/lib/python3.12/site-packages/openai/_types.py.\n"
      },
      {
        "user": "collindutter",
        "body": "@RobertCraigie the type-hint suggests the image data should be the second item in the tuple:\nhttps://github.com/openai/openai-python/blob/67997a4ec1ebcdf8e740afb0d0b2e37897657bde/src/openai/_types.py#L54-L55\n\nI've adapted that here:\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresult = client.images.edit(\n    model=\"gpt-image-1\",\n    image=(\n        None,\n        open(\"sunlit_lounge.png\", \"rb\"),\n        \"image/png\",\n    ),\n    prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\",\n)\n```\n\nBut that gives me the following error:\n```\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Invalid type for 'image': expected one of an array of files or file, but got a string instead.\", 'type': 'invalid_request_error', 'param': 'image', 'code': 'invalid_type'}}\n```\n\nAny suggestions? Thanks!"
      }
    ]
  },
  {
    "issue_number": 2362,
    "title": "Expose all vector store file kwargs in `create` to `create_and_poll` (namely `attributes`)",
    "author": "walsha2",
    "state": "open",
    "created_at": "2025-05-19T02:02:30Z",
    "updated_at": "2025-05-19T02:02:41Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nHere is the function signature for `vector_stores.files.create`:\n\n```python\ndef create(\n    self,\n    vector_store_id: str,\n    *,\n    file_id: str,\n    attributes: Optional[Dict[str, Union[str, float, bool]]] | NotGiven = NOT_GIVEN,\n    chunking_strategy: FileChunkingStrategyParam | NotGiven = NOT_GIVEN,\n    # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n    # The extra values given here take precedence over values defined on the client or passed to this method.\n    extra_headers: Headers | None = None,\n    extra_query: Query | None = None,\n    extra_body: Body | None = None,\n    timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n) -> VectorStoreFile:\n```\n\nand here is the function signature for  `vector_stores.files.create_and_poll`:\n```python\ndef create_and_poll(\n    self,\n    file_id: str,\n    *,\n    vector_store_id: str,\n    poll_interval_ms: int | NotGiven = NOT_GIVEN,\n    chunking_strategy: FileChunkingStrategyParam | NotGiven = NOT_GIVEN,\n) -> VectorStoreFile:\n```\n\nThe key item missing here is `attributes`. This means that if I need to set `attributes` I cant really use the connivence `create_and_poll` method since it does not allow me to drill them down to the underlying `create` call. \n\nInstead, I would need to recreate this method myself just to pass in `attributes` - no big deal, just seems unnecessary since the method already exists.\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 2301,
    "title": "Completion create params should be a pydantic model",
    "author": "pbarker",
    "state": "open",
    "created_at": "2025-04-10T16:48:15Z",
    "updated_at": "2025-05-16T19:14:40Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nhttps://github.com/openai/openai-python/blob/main/src/openai/types/chat/completion_create_params.py should be a pydantic model. This is often needed for local servers, today they have to create a separate pydantic definition to support it.\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "salfaris",
        "body": "Do you have any current examples of \"today they have to create a separate pydantic definition to support it\"?"
      }
    ]
  },
  {
    "issue_number": 2004,
    "title": "Pydantic conversion logic for structured outputs is broken for models containing dictionaries",
    "author": "dbczumar",
    "state": "open",
    "created_at": "2025-01-10T01:38:33Z",
    "updated_at": "2025-05-14T20:25:53Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nThere's a bug in OpenAI's python client logic for translating pydantic models with dictionaries into structured outputs JSON schema definitions: **dictionaries are always required to be empty in the resulting JSON schema, rendering the dictionary outputs significantly less useful since the LLM is never allowed to populate them**\r\n\r\nI've filed a small PR to fix this and introduce test coverage: https://github.com/openai/openai-python/pull/2003\n\n### To Reproduce\n\n```\r\nimport json\r\nfrom typing import Any, Dict\r\n\r\nimport pydantic\r\n\r\nfrom openai.lib._pydantic import to_strict_json_schema\r\n\r\nclass GenerateToolCallArguments(pydantic.BaseModel):\r\n    arguments: Dict[str, Any] = pydantic.Field(description=\"The arguments to pass to the tool\")\r\n\r\nprint(json.dumps(to_strict_json_schema(GenerateToolCallArguments), indent=4))\r\n```\r\n\r\nObserve that the output inserts `additionalProperties: False` into the resulting JSON schema definition, meaning that the dictionary must always be empty:\r\n\r\n```\r\n{\r\n    \"properties\": {\r\n        \"arguments\": {\r\n            \"description\": \"The arguments to pass to the tool\",\r\n            \"title\": \"Arguments\",\r\n            \"type\": \"object\",\r\n            # THE INSERTION OF THIS LINE IS A BUG\r\n            \"additionalProperties\": false\r\n        }\r\n    },\r\n    \"required\": [\r\n        \"arguments\"\r\n    ],\r\n    \"title\": \"GenerateToolCallArguments\",\r\n    \"type\": \"object\",\r\n    \"additionalProperties\": false\r\n}\r\n```\n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.10.12\n\n### Library version\n\n1.59.6",
    "comments": [
      {
        "user": "dbczumar",
        "body": "Tagging @RobertCraigie for visibility, just in case (saw that you've been active on recent issues) :)"
      },
      {
        "user": "BrunoScaglione",
        "body": "I'm having the same issue, can confirm that models with dictionaries is the root problem. But i checked the documentation again, and they do talk about only allowing additionalProperties=false."
      },
      {
        "user": "dbczumar",
        "body": "@RobertCraigie Any updates or additional thoughts here?"
      }
    ]
  },
  {
    "issue_number": 2352,
    "title": "gpt-4o-mini-realtime-preview unable to respond with realtime audio",
    "author": "jverkoey",
    "state": "closed",
    "created_at": "2025-05-10T19:34:11Z",
    "updated_at": "2025-05-14T06:09:51Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe following initialization works and results in a realtime audio conversation:\n\n```\nasync with client.beta.realtime.connect(model=\"gpt-4o-realtime-preview-2024-12-17\") as openai_stream:\n```\n\nBut when I try to use gpt-4o-mini, like so:\n\n```\nasync with client.beta.realtime.connect(model=\"gpt-4o-mini-realtime-preview\") as openai_stream:\n```\n\nI get the following errors after speaking:\n\n```\n{ 'event_id': 'event_BVkCL9CtzRVz1CIrI1jNe',\n  'response': { 'conversation_id': 'conv_BVkC88oBioiBmAEr6RYBn',\n                'id': 'resp_BVkCLamsLeAeHny8E3f1L',\n                'max_output_tokens': 4096,\n                'metadata': None,\n                'modalities': ['text', 'audio'],\n                'object': 'realtime.response',\n                'output': [],\n                'output_audio_format': 'pcm16',\n                'status': 'failed',\n                'status_details': { 'error': { 'code': None,\n                                               'message': 'The server had an error while '\n                                                          'processing your request. Sorry about '\n                                                          'that! Please contact us through our '\n                                                          'help center at help.openai.com if the '\n                                                          'error persists. (include session ID in '\n                                                          'your message: '\n                                                          'sess_BVkC8iXlHg3H71vTbI9Te). We '\n                                                          'recommend you retry your request.',\n                                               'type': 'server_error'},\n                                    'reason': None,\n                                    'type': 'failed'},\n                'temperature': 0.8,\n                'usage': { 'input_token_details': { 'audio_tokens': 0,\n                                                    'cached_tokens': 0,\n                                                    'cached_tokens_details': { 'audio_tokens': 0,\n                                                                               'text_tokens': 0},\n                                                    'text_tokens': 0},\n                           'input_tokens': 0,\n                           'output_token_details': {'audio_tokens': 0, 'text_tokens': 0},\n                           'output_tokens': 0,\n                           'total_tokens': 0},\n                'voice': 'verse'},\n  'type': 'response.done'}\n```\n\n### To Reproduce\n\n\n```\nasync with client.beta.realtime.connect(model=\"gpt-4o-mini-realtime-preview\") as openai_stream:\n```\n\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\n3.13\n\n### Library version\n\n1.78.0",
    "comments": [
      {
        "user": "jverkoey",
        "body": "Looks like the error here was that I hadn't enabled the gpt-4o-mini model."
      }
    ]
  },
  {
    "issue_number": 1254,
    "title": "AsyncOpenAI occasionally throws \"Runtime Error: Event loop is closed\" on Windows",
    "author": "ethan-tonic",
    "state": "closed",
    "created_at": "2024-03-20T17:47:15Z",
    "updated_at": "2025-05-12T13:40:05Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nOn Windows, if you try getting chat completions the openai library will occasionally throw an error saying `RuntimeError: Event loop is closed`. This seems to originate from httpx trying to close the connection. I noticed it primarily happens on short lived async functions that are run in rapid succession. It also only seems to happen if the `AsyncOpenAI` client is created multiple times. If it's created once (like outside the loop in the example code below) then the issue doesn't happen. I believe this may or may not be related to httpx's connection pooling trying to reuse a connection that no longer exists. [Here](https://github.com/encode/httpx/discussions/2959) is a related issue from httpx's repo.\r\n\r\nHere are the logs from the exception in the code snippet provided in this issue.\r\n```\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-6' coro=<AsyncClient.aclose() done, defined at c:\\Users\\Ethan\\tonic_validate_tester\\venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\Ethan\\tonic_validate_tester\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 2018, in aclose\r\n    await self._transport.aclose()\r\n  File \"c:\\Users\\Ethan\\tonic_validate_tester\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 385, in aclose\r\n    await self._pool.aclose()\r\n  File \"c:\\Users\\Ethan\\tonic_validate_tester\\venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 313, in aclose\r\n    await self._close_connections(closing_connections)\r\n  File \"c:\\Users\\Ethan\\tonic_validate_tester\\venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 305, in _close_connections\r\n    await connection.aclose()\r\n  File \"c:\\Users\\Ethan\\tonic_validate_tester\\venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 171, in aclose\r\n    await self._connection.aclose()\r\n  File \"c:\\Users\\Ethan\\tonic_validate_tester\\venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 265, in aclose\r\n    await self._network_stream.aclose()\r\n  File \"c:\\Users\\Ethan\\tonic_validate_tester\\venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 54, in aclose\r\n    await self._stream.aclose()\r\n  File \"c:\\Users\\Ethan\\tonic_validate_tester\\venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 202, in aclose\r\n    await self.transport_stream.aclose()\r\n  File \"c:\\Users\\Ethan\\tonic_validate_tester\\venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1191, in aclose\r\n    self._transport.close()\r\n  File \"C:\\Users\\Ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\proactor_events.py\", line 109, in close\r\n    self._loop.call_soon(self._call_connection_lost, None)\r\n  File \"C:\\Users\\Ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 761, in call_soon\r\n    self._check_closed()\r\n  File \"C:\\Users\\Ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 519, in _check_closed\r\n    raise RuntimeError('Event loop is closed')\r\nRuntimeError: Event loop is closed\r\n```\r\n\r\n### To Reproduce\r\n\r\n1. Run the code snippet provided on Windows 11; Python v3.11\r\n2. Sometime during while the for loop is running you should see in the logs that the event loop is closed\r\n\r\n### Code snippets\r\n\r\n```Python\r\nasync def test() -> None:\r\n    chat_completion = await client.chat.completions.create(\r\n        messages=[\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": \"Say this is a test\",\r\n            }\r\n        ],\r\n        model=\"gpt-3.5-turbo\",\r\n    )\r\n\r\nfor i in range(50):\r\n    # Note we create the client inside this loop as it triggers the bug\r\n    client = AsyncOpenAI()\r\n    asyncio.run(test())\r\n```\r\n\r\n\r\n### OS\r\n\r\nWindows 11\r\n\r\n### Python version\r\n\r\nPython v3.11.7\r\n\r\n### Library version\r\n\r\nopenai v1.12.0",
    "comments": [
      {
        "user": "rattrayalex",
        "body": "Thanks for the report; we'll look into it."
      },
      {
        "user": "rattrayalex",
        "body": "Can you share a complete repro script (eg including imports) and verify that the script works produces the error case?"
      },
      {
        "user": "RobertCraigie",
        "body": "I can't reproduce this on MacOS with 3.9 but I can reproduce on 3.11.\r\n\r\n> It also only seems to happen if the AsyncOpenAI client is created multiple times. If it's created once (like outside the loop in the example code below) then the issue doesn't happen\r\n\r\nCan you share why you want to create the client multiple times in a loop?"
      }
    ]
  },
  {
    "issue_number": 2280,
    "title": "support for responses.create() with AzureOpenAI and AsyncAzureOpenAI",
    "author": "NikGor",
    "state": "open",
    "created_at": "2025-04-07T11:08:15Z",
    "updated_at": "2025-05-12T12:48:50Z",
    "labels": [
      "Azure"
    ],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nHi OpenAI team 👋\n\n\nWe’ve noticed that, as of now, the AzureOpenAI and AsyncAzureOpenAI clients do not expose the .responses resource like the default OpenAI client does. This is a bit limiting, especially considering that Azure OpenAI has recently added support for the /openai/responses endpoint as part of the 2025-03-01-preview API version.\n\nCurrently, attempting to use:\n\nclient = AsyncAzureOpenAI(...)\nawait client.responses.create(...)\n\nraises an AttributeError because .responses is not available on that class.\n\nIt would be great if .responses was added to AzureOpenAI and AsyncAzureOpenAI, similarly to how .chat.completions are exposed.\n\n•\tAzure now supports /openai/responses endpoint\n•\tmodel / deployment ID is passed in the JSON body, not in the path\n•\tNo need to add it to _deployments_endpoints\n•\tAdding this would make the SDK consistent and easier to use for Azure users\n\nThanks in advance! Happy to contribute a PR if you’re open to it.\n\nBest,\nNikolai\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "kiranimmadi2",
        "body": "Hi there,\n\ncheck code below once\n\n\nclient = AsyncAzureOpenAI(...)\nawait client.responses.create(...)\nresults in an AttributeError because the .responses property isn't available.\n\nSince Azure now supports the /openai/responses endpoint (using the 2025-03-01-preview API version), it makes sense to add this feature. It would align the behavior of these clients with how .chat.completions is implemented, making everything more consistent. Plus, because the model/deployment ID is passed in the JSON body instead of the endpoint path, integrating this shouldn't require major changes.\n\nOverall, this enhancement would make the client more user-friendly for Azure users. Thanks again for proposing this improvement, and it's great to know you're willing to help with a PR if needed!"
      },
      {
        "user": "trrwilson",
        "body": "@NikGor, @kiranimmadi2, which version of the `openai` library are you encountering this with? Azure support for /responses should certainly be present; basic Azure OpenAI use of the synchronous client is [documented on learn.microsoft.com](https://learn.microsoft.com/azure/ai-services/openai/how-to/responses?tabs=python-key#generate-a-text-response) and I just validated the following quick modification to use the async client with 1.74.1 (and an earlier 1.69.0 environment for a positive comparison):\n\n```python\nimport asyncio\nimport os\nfrom openai import AsyncAzureOpenAI\n\nasync def main():\n    client = AsyncAzureOpenAI(\n        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n        api_version=\"2025-03-01-preview\",\n        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n        )\n\n    response = await client.responses.create(\n        model=\"gpt-4o-mini\", # replace with your model deployment name \n        input=\"This is a test.\"\n        #truncation=\"auto\" required when using computer-use-preview model.\n    )\n\n    print(response.output[0]);\n\nasyncio.run(main())\n```"
      },
      {
        "user": "NikGor",
        "body": "@giuliohome Read my message carefully; I mentioned that Azure has already enabled this support.\n"
      }
    ]
  },
  {
    "issue_number": 2349,
    "title": "module `openai` has no attribute `resources`",
    "author": "DamianB-BitFlipper",
    "state": "closed",
    "created_at": "2025-05-09T10:24:21Z",
    "updated_at": "2025-05-12T10:08:34Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWith the latest version 1.78.0, `openai.resources` no longer exists.\n\nSimply doing:\n```\nimport openai\nopenai.resources\n```\n\nyields `AttributeError: module 'openai' has no attribute 'resources'`\n\nI use the `resources` to import `openai.resources.chat.Completions`\n\nIn the previous version 1.77.0, this worked just fine. I am unsure if this intended behavior or a bug that slipped through. I'd appreciate your timely input!\n\n### To Reproduce\n\n```\nimport openai\nopenai.resources\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS 15.3.1\n\n### Python version\n\nPython 3.13.1\n\n### Library version\n\n1.78.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Ah this was likely because of a change we made to make imports faster, for now you should be able to workaround this with `import openai.resources`, can you let me know if that fixes the issue?"
      },
      {
        "user": "RobertCraigie",
        "body": "Could you also share what you're using the `openai.resources` import for? Trying to assess how common this breakage could be."
      },
      {
        "user": "DamianB-BitFlipper",
        "body": "We're hooking in to `openai.resources.chat.Completions.create` and intercepting arguments\n\n```python\n        original_create = openai.resources.chat.Completions.create\n\n        def intercepted_create(_self: openai.resources.chat.Completions, **kwargs: Any) -> Any:\n            ...\n\n        # Replace the original method with our intercepted version\n        openai.resources.chat.Completions.create = intercepted_create\n```\n\nas well as type hinting\n\n```python\ncompletions: openai.resources.chat.Completions\n```\n\nThe former usage is a bit obscure, but the latter is common and surprising not to work in my opinion.\n\nThanks for looking in to this."
      }
    ]
  },
  {
    "issue_number": 2347,
    "title": "vector_stores not exposed in SDK 1.78.0 beta.client despite official release",
    "author": "Moondy217",
    "state": "open",
    "created_at": "2025-05-09T05:06:23Z",
    "updated_at": "2025-05-12T06:37:20Z",
    "labels": [
      "bug"
    ],
    "body": "\n### Describe the bug\n\n### Summary\n\nI'm trying to use the new vector store support in OpenAI's Assistants API (file_search with vector_store_ids), but the Python SDK (v1.78.0) does not expose the `vector_stores` attribute under `client.beta`.\n\n### What I expected\n\nAccording to the documentation, I should be able to:\n\n```python\nvector_store = client.beta.vector_stores.create(name=\"My Vector Store\")\n```\n\nAnd use it with:\n```python\ntools=[{\"type\": \"file_search\"}],\ntool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}}\n```\n\n### What happened\nHowever, the following fails:\n```python\n\"vector_stores\" in dir(client.beta)  # returns False\nclient.beta.vector_stores  # AttributeError\n```\n\n```python\n>>> print(dir(client.beta))\n['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_client', '_delete', '_get', '_get_api_list', '_patch', '_post', '_put', '_sleep', 'assistants', 'chat', 'realtime', 'threads', 'with_raw_response', 'with_streaming_response']\n```\nNo vector_store\n\nThe SDK version is:\n```\nName: openai\nVersion: 1.78.0\n```\n\nI've tried installing from main, clearing cache, and reinstalling — the attribute is still missing.\n\n### Expected outcome\nIt would be great to:\n\nexpose vector_stores under client.beta\nprovide example usage (as shown in API reference)\nclarify whether SDK support is delayed or upcoming\n\n### To Reproduce\n\n1. Install the latest SDK (1.78.0) or try development versions:\n\n```bash\npip install --no-cache-dir openai==1.78.0\n```\n```bash\npip install --no-cache-dir git+https://github.com/openai/openai-python.git@a790e6e\n```\n```bash\npip install --no-cache-dir git+https://github.com/openai/openai-python.git@6c6e17679a6f90dd15e58b13d85e0b2e8bb5f209\n```\n```bash\npip install --no-cache-dir https://github.com/openai/openai-python/archive/refs/heads/main.zip\n```\n\n2. Initialize the client:\n```python\nfrom openai import OpenAI\nclient = OpenAI(api_key=\"sk-...\")  # My valid API key\n```\n\n3. Try to access vector stores:\n```python\nprint(\"vector_stores\" in dir(client.beta))  # Expected: True, Got: False\nprint(client.beta.vector_stores)  # Raises: AttributeError\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.11\n\n### Library version\n\nopenai v1.78.0",
    "comments": []
  },
  {
    "issue_number": 2350,
    "title": "Proposal: DYNMEM_PROTOTYPE_001 – Dynamic Memory Architecture for ChatGPT",
    "author": "tabb78",
    "state": "closed",
    "created_at": "2025-05-09T13:00:37Z",
    "updated_at": "2025-05-09T13:11:11Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nFirst-time contributor here – this idea was co-developed through real user experience and long-term GPT collaboration.\n\n### Proposal: Dynamic Memory Architecture – DYNMEM_PROTOTYPE_001\n\n**Summary:**  \nThrough sustained interaction with GPT-4o, I’ve developed a structured concept for dynamic memory evolution in conversational systems. This framework proposes layered memory modeling, triangulated interpretation loops, heatmap redundancy analysis, and cognitive evolution tracking across sessions.\n\n**Problem:**  \nCurrent memory features in ChatGPT are powerful but limited to:  \n- explicit user-directed memories  \n- implicit history reference  \n\nThey do not yet support evolving reasoning structures, timeline-based concept tracking, or redundancy-aware semantic parsing.\n\n**Proposal:**  \nMy concept (DYNMEM_PROTOTYPE_001) introduces:  \n- Memory clusters with semantic layering  \n- Redundancy weighting via heatmaps  \n- Triangulation loops for insight extraction  \n- Timestamped preference shifts and cognitive evolution tracking  \n- Benchmark framework to compare accuracy, trust, and adaptability\n\n**Deliverable:**  \nA full proposal with diagrams and an applied test-case (EV decision modeling) is available here:  \n📎 [SVEN-MODEL-2025_DYNMEM_PROTOTYPE_001_public.pdf](https://drive.google.com/file/d/1bc-X1U5xMR4D63OY4olSlorxIDiaJB_6/view?usp=share_link)\n\n**Contact:**  \nSven Haaken – sven.haaken@gmx.de  \nI’d be happy to support pilot evaluation, abstraction review, or further discussion.\n\n---\n\n*This is my first technical submission. I'm deeply invested in ChatGPT’s evolution and hope this contribution aligns with ongoing developments.*\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2028,
    "title": "Indentation error in Python example in \"Step 4: Create a Run\" of Quickstart Documentation",
    "author": "Programmer-RD-AI",
    "state": "closed",
    "created_at": "2025-01-18T03:48:06Z",
    "updated_at": "2025-05-09T00:53:54Z",
    "labels": [
      "api docs"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe Python example provided in the \"Step 4: Create a Run\" section of the [[Quickstart documentation](https://platform.openai.com/docs/assistants/quickstart#step-4-create-a-run)](https://platform.openai.com/docs/assistants/quickstart#step-4-create-a-run) contains incorrect indentation, which leads to a `IndentationError` when executed as-is.  \n\nSpecifically, the methods within the `EventHandler` class are not properly indented under the class definition. This may confuse users and lead to runtime errors.  \n\n\n### To Reproduce\n\n1. Visit the [[Quickstart documentation, Step 4](https://platform.openai.com/docs/assistants/quickstart#step-4-create-a-run)](https://platform.openai.com/docs/assistants/quickstart#step-4-create-a-run).  \n2. Copy the provided Python code snippet.  \n3. Attempt to execute the code in a Python environment.  \n4. Observe the `IndentationError` caused by misaligned methods in the `EventHandler` class.  \n\n### Code snippets\n\n```Python\n# Here is the problematic snippet:  \n\n\nclass EventHandler(AssistantEventHandler):    \n@override\ndef on_text_created(self, text) -> None:\n  print(f\"\\nassistant > \", end=\"\", flush=True)\n    \n@override\ndef on_text_delta(self, delta, snapshot):\n  print(delta.value, end=\"\", flush=True)\n    \ndef on_tool_call_created(self, tool_call):\n  print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n\ndef on_tool_call_delta(self, delta, snapshot):\n  if delta.type == 'code_interpreter':\n    if delta.code_interpreter.input:\n      print(delta.code_interpreter.input, end=\"\", flush=True)\n    if delta.code_interpreter.outputs:\n      print(f\"\\n\\noutput >\", flush=True)\n      for output in delta.code_interpreter.outputs:\n        if output.type == \"logs\":\n          print(f\"\\n{output.logs}\", flush=True)\n\n\n# Correctly indented version:  \n\n\nclass EventHandler(AssistantEventHandler):    \n    @override\n    def on_text_created(self, text) -> None:\n        print(f\"\\nassistant > \", end=\"\", flush=True)\n    \n    @override\n    def on_text_delta(self, delta, snapshot):\n        print(delta.value, end=\"\", flush=True)\n    \n    def on_tool_call_created(self, tool_call):\n        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n\n    def on_tool_call_delta(self, delta, snapshot):\n        if delta.type == 'code_interpreter':\n            if delta.code_interpreter.input:\n                print(delta.code_interpreter.input, end=\"\", flush=True)\n            if delta.code_interpreter.outputs:\n                print(f\"\\n\\noutput >\", flush=True)\n                for output in delta.code_interpreter.outputs:\n                    if output.type == \"logs\":\n                        print(f\"\\n{output.logs}\", flush=True)\n```\n\n### OS\n\n-\n\n### Python version\n\n-\n\n### Library version\n\n-",
    "comments": [
      {
        "user": "kwhinnery-openai",
        "body": "This will be fixed in a website update going out shortly - thanks for reporting!"
      },
      {
        "user": "issamhaimour",
        "body": "[](url)"
      },
      {
        "user": "issamhaimour",
        "body": "[``](`url`)"
      }
    ]
  },
  {
    "issue_number": 2056,
    "title": "missing `pandas` when using `isinstance`",
    "author": "vladkolotvin",
    "state": "closed",
    "created_at": "2025-01-28T12:29:02Z",
    "updated_at": "2025-05-08T17:27:41Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\n```\nTraceback (most recent call last):\n2025-01-28T12:19:51.802342624Z   File \"/usr/local/lib/python3.12/weakref.py\", line 590, in __call__\n2025-01-28T12:19:51.802544917Z     return info.func(*info.args, **(info.kwargs or {}))\n2025-01-28T12:19:51.802548333Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-01-28T12:19:51.802549625Z   File \"/usr/src/app/kai/kai/bot/bot.py\", line 170, in cleanup\n2025-01-28T12:19:51.802550583Z     print(referrers.get_referrer_graph(conversation))\n2025-01-28T12:19:51.802551417Z           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-01-28T12:19:51.802587458Z   File \"/usr/local/lib/python3.12/site-packages/referrers/impl.py\", line 162, in get_referrer_graph\n2025-01-28T12:19:51.802590083Z     return get_referrer_graph_for_list(\n2025-01-28T12:19:51.802590958Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-01-28T12:19:51.802591833Z   File \"/usr/local/lib/python3.12/site-packages/referrers/impl.py\", line 234, in get_referrer_graph_for_list\n2025-01-28T12:19:51.802592792Z     builder = _ReferrerGraphBuilder(\n2025-01-28T12:19:51.802593583Z               ^^^^^^^^^^^^^^^^^^^^^^\n2025-01-28T12:19:51.802594417Z   File \"/usr/local/lib/python3.12/site-packages/referrers/impl.py\", line 710, in __init__\n2025-01-28T12:19:51.802980461Z     self._id_to_enclosing_closure = self._get_closure_functions()\n2025-01-28T12:19:51.802985086Z                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-01-28T12:19:51.802986169Z   File \"/usr/local/lib/python3.12/site-packages/referrers/impl.py\", line 1074, in _get_closure_functions\n2025-01-28T12:19:51.803291962Z     if inspect.isfunction(possible_function) or inspect.ismethod(\n2025-01-28T12:19:51.803297171Z        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-01-28T12:19:51.803298921Z   File \"/usr/local/lib/python3.12/inspect.py\", line 389, in isfunction\n2025-01-28T12:19:51.804443843Z     return isinstance(object, types.FunctionType)\n2025-01-28T12:19:51.804452010Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-01-28T12:19:51.804454218Z   File \"/usr/local/lib/python3.12/site-packages/openai/_utils/_proxy.py\", line 49, in __class__\n2025-01-28T12:19:51.804517219Z     proxied = self.__get_proxied__()\n2025-01-28T12:19:51.804519427Z               ^^^^^^^^^^^^^^^^^^^^^^\n2025-01-28T12:19:51.804520344Z   File \"/usr/local/lib/python3.12/site-packages/openai/_utils/_proxy.py\", line 55, in __get_proxied__\n2025-01-28T12:19:51.804563511Z     return self.__load__()\n2025-01-28T12:19:51.804565469Z            ^^^^^^^^^^^^^^^\n2025-01-28T12:19:51.804568511Z   File \"/usr/local/lib/python3.12/site-packages/openai/_extras/pandas_proxy.py\", line 22, in __load__\n2025-01-28T12:19:51.804681636Z     raise MissingDependencyError(PANDAS_INSTRUCTIONS) from err\n2025-01-28T12:19:51.804712678Z openai._extras._common.MissingDependencyError: \n2025-01-28T12:19:51.804714428Z \n2025-01-28T12:19:51.804715303Z OpenAI error:\n2025-01-28T12:19:51.804716178Z \n2025-01-28T12:19:51.804716970Z     missing `pandas`\n2025-01-28T12:19:51.804717845Z \n2025-01-28T12:19:51.804718720Z This feature requires additional dependencies:\n2025-01-28T12:19:51.804719678Z \n2025-01-28T12:19:51.804720470Z     $ pip install openai[datalib]\n2025-01-28T12:19:51.804721345Z \n2025-01-28T12:19:51.804722178Z \n```\n\n### To Reproduce\n\n1. import openai library\n2. try to use \"referrers\" libary\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacos\n\n### Python version\n\n3.12.6\n\n### Library version\n\n1.13.3",
    "comments": [
      {
        "user": "vladkolotvin",
        "body": "same on version 1.60.2"
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report, is there any way to tell this library to just not inspect certain symbols / paths? This is eagerly importing internal helpers and then trying to inspect them in ways which breaks assumptions about how the helpers should be used."
      },
      {
        "user": "vladkolotvin",
        "body": "> Thanks for the report, is there any way to tell this library to just not inspect certain symbols / paths? This is eagerly importing internal helpers and then trying to inspect them in ways which breaks assumptions about how the helpers should be used.\n\nHmm, this is a library for memory debugging. I didn't manage to find a way to disable inspecting specific types."
      }
    ]
  },
  {
    "issue_number": 2344,
    "title": "cannot install openai",
    "author": "jaredgreen2",
    "state": "closed",
    "created_at": "2025-05-07T19:41:56Z",
    "updated_at": "2025-05-08T11:08:12Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\non windows, while trying to install something that has openai as a dependency, i get this:\n  - Installing openai (1.75.0): Failed\n\n  FileNotFoundError\n\n  [Errno 2] No such file or directory: 'C:\\\\Users\\\\jared\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\leantool-rAtt-6DR-py3.10\\\\Lib\\\\site-packages\\\\openai\\\\types\\\\beta\\\\realtime\\\\conversation_item_input_audio_transcription_completed_event.py'\n\n  at C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\pathlib.py:1119 in open \n      1115│         the built-in open() function does.\n      1116│         \"\"\"\n      1117│         if \"b\" not in mode:\n      1118│             encoding = io.text_encoding(encoding)\n    → 1119│         return self._accessor.open(self, mode, buffering, encoding, errors,\n      1120│                                    newline)\n      1121│\n      1122│     def read_bytes(self):\n      1123│         \"\"\"\n\nCannot install openai.\n\n### To Reproduce\n\nunknown\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nwindows\n\n### Python version\n\n3.10\n\n### Library version\n\n1.75.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "I suspect you have a bad cache, can you try deleting the venv and reinstalling?"
      },
      {
        "user": "jaredgreen2",
        "body": "how?"
      },
      {
        "user": "jaredgreen2",
        "body": "not even sure if its in a venv"
      }
    ]
  },
  {
    "issue_number": 2184,
    "title": "New responses API does not work with AzureOpenAI",
    "author": "alexprengere",
    "state": "closed",
    "created_at": "2025-03-12T13:51:28Z",
    "updated_at": "2025-05-05T15:24:19Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nOn the latest version of `openai==1.66.2`, I have this current code that works well:\n\n```python\nimport os\nfrom openai import AzureOpenAI\n\nclient = AzureOpenAI(\n    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    azure_endpoint=os.getenv(\"OPENAI_AZURE_ENDPOINT\"),\n)\n\ncompletions = client.chat.completions.create(\n    model=os.getenv(\"OPENAI_AZURE_DEPLOYMENT\"),\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of the United States?\"},\n    ],\n)\nprint(completions.choices[0].message.content)\n# returns 'The capital of the United States is Washington, D.C.'\n```\n\nI wanted to test the new [responses API](https://platform.openai.com/docs/guides/text?api-mode=responses), so I followed the docs:\n\n```python\nimport os\nfrom openai import AzureOpenAI\n\nclient = AzureOpenAI(\n    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    azure_endpoint=os.getenv(\"OPENAI_AZURE_ENDPOINT\"),\n)\n\nresponse = client.responses.create(\n    model=os.getenv(\"OPENAI_AZURE_DEPLOYMENT\"),\n    instructions=\"You are a coding assistant that talks like a pirate.\",\n    input=\"How do I check if a Python object is an instance of a class?\",\n)\nprint(response.output_text)\n```\n\nBut this results in:\n```\nopenai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n```\nAm I missing something or is this just not supported yet? Note that the deployed model on Azure is `GPT-4o`.\n\n### To Reproduce\n\nAll information required is above.\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nFedora\n\n### Python version\n\nPython3.10\n\n### Library version\n\nopenai==1.66.2",
    "comments": [
      {
        "user": "kristapratico",
        "body": "@alexprengere Responses API is not supported on Azure yet. I can't share an ETA, but I expect support to come soon."
      },
      {
        "user": "alexprengere",
        "body": "@RobertCraigie wouldn't it be better to keep this issue open until the feature lands? Or perhaps I misunderstood @kristapratico, and indeed this is \"not planned\"."
      },
      {
        "user": "RobertCraigie",
        "body": "Ah sorry, we tend to only keep issues open that are relevant to the SDK. If it's an API issue we'll close it.\n\n@kristapratico is there a better place for people to track support for Responses in Azure?"
      }
    ]
  },
  {
    "issue_number": 2313,
    "title": "Azure Open AI Async filter results in None type content part",
    "author": "martgra",
    "state": "open",
    "created_at": "2025-04-17T12:01:46Z",
    "updated_at": "2025-05-05T13:34:50Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen using the async filter in Azure, stream mode, the SDK returns None types. Unsure whether this is a issue in the API or the SDK but since this is a fundament package for many high level libraries the fix could be implemented here.\n\nRef other reported issues:\nhttps://github.com/pydantic/pydantic-ai/issues/797\nhttps://github.com/microsoft/semantic-kernel/issues/7250\nhttps://github.com/langchain-ai/langchain/issues/25951\n\n### To Reproduce\n\n1. Set Azure Open AI completion model (GPT-4x) to Async content filter (stream mode) \n2. Run stream with async client\n3. Try accessing content gives unexpected None type.\n\nError will only occur with async content filter.\n\nRef error:\n\n```\n  File \"/workspace/backend/api/services/domain_service.py\", line 50, in get_answer\n    async for response in self.message_service.get_grounded_message_as_stream(\n  File \"/workspace/backend/api/services/message_service.py\", line 83, in get_grounded_message_as_stream\n    async for chunk in self.chat_consumer.get_message_as_stream(message_history_with_context):\n  File \"/workspace/backend/api/consumers/chats/azure_openai_consumer.py\", line 56, in get_message_as_stream\n    content = chunk.choices[0].delta.content\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'content'\n```\n\n\n\n### Code snippets\n\n```Python\nasync with await client.chat.completions.create(\n                messages=messages,\n                model=\"gpt-4o\",\n                stream=True,\n            ) as stream:\n                async for chunk in stream:  \n                    if not (\n                        chunk\n                        and chunk.choices\n                    ):\n                        continue\n\n                    content = chunk.choices[0].delta.content\n```\n\n### OS\n\nDebian Bookworm, Mac OS\n\n### Python version\n\nPython v3.12\n\n### Library version\n\nopen ai v.1.69.0",
    "comments": [
      {
        "user": "kapis",
        "body": "I believe this is a problem on the Azure side - for some reason, they've decided to violate the OpenAI spec with the async filter.\n\nSee this: https://github.com/openai/openai-python/pull/2255"
      }
    ]
  },
  {
    "issue_number": 2342,
    "title": "Add G.711 μ-law (g711_ulaw) format to TTS endpoint",
    "author": "Keerthanaksk",
    "state": "closed",
    "created_at": "2025-05-05T11:07:43Z",
    "updated_at": "2025-05-05T11:08:51Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\n**Summary**  \nAdd support for a `response_format=\"g711_ulaw\"` option in the Text‑to‑Speech endpoint, to enable output in standard telephony‑grade G.711 μ-law audio.\n\n**Use case**  \nMany telephony systems like Twilio expects audio in G.711 μ‑law format. \n\n**Proposal**  \n- In the TTS API spec, extend the `response_format` enum to include `g711_ulaw`.  \n- When selected, TCP streams return raw 8‑bit μ‑law–encoded audio (80 kHz packetization or similar, per standard).  \n\nThank you\n\n### Additional context\n\nasync with client.audio.speech.with_streaming_response.create(\n    model=\"gpt-4o-mini-tts\",\n    voice=\"coral\",\n    input= text,\n    instructions=\"\"\"XXXXX\"\"\",\n    response_format=\"g711_ulaw\",<-- please include g711_ulaw\n    )",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the feature request but I believe this isn't supported by the API, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2317,
    "title": "client.images.generate is not generating images",
    "author": "NeillGiraldo",
    "state": "closed",
    "created_at": "2025-04-19T13:24:51Z",
    "updated_at": "2025-05-02T00:45:32Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nI am not able to generate images via the python library. \n\nI have tried generating text and works perfectly fine. so I assume the problem is with the images module.\n\nYou can find the code in the next section.\n\nand this is what I get when I do `pip show openai`\n\nName: openai\nVersion: 1.75.0\nSummary: The official Python library for the openai API\nHome-page: None\nAuthor: None\nAuthor-email: OpenAI <support@openai.com>\nLicense: Apache-2.0\nLocation: {path-to-venv}/.venv/lib/python3.9/site-packages\nRequires: anyio, typing-extensions, tqdm, pydantic, distro, jiter, sniffio, httpx\nRequired-by: \n\n\n### To Reproduce\n\n1.  When I try to run the following code\n\n```python \n\nfrom openai import OpenAI\nimport os\n\nPROMPT = \"People sitting outside in a warm european summer\"\nMODEL = \"dall-e-3\"  # Or \"dall-e-2\"\n\ntry:\n    response = client.images.generate(\n        model=MODEL,\n        prompt=PROMPT,\n        n=1,             # Number of images to generate (default is 1)\n        size=\"1024x1024\", # Size of the generated image (options vary by model)\n        # quality=\"standard\", # For dall-e-3, can be \"standard\" or \"hd\" (default is \"standard\")\n        # style=\"vivid\",     # For dall-e-3, can be \"vivid\" or \"natural\" (default is \"vivid\")\n        # user=\"your-user-id\", # Optional: For tracking purposes\n    )\n    image_url = response.data[0].url\n    print(f\"Generated image URL: {image_url}\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n    if hasattr(e, 'response'):\n        print(f\"Full error response: {e.response.json()}\")\n\n```\n\n2. I get the following response \n\n```python \n\n{'error': {'message': 'Error in request. Please check your input.', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.9.0\n\n### Library version\n\nopenai v1.75.0",
    "comments": [
      {
        "user": "balazspiller",
        "body": "Seems to be an issue with the underlying OpenAI API, because I get the same error even with a curl request."
      },
      {
        "user": "zerfl",
        "body": "I've experienced the same issue throughout the day as well. I agree with @balazspiller that this appears to be related to the endpoint itself. The prompt content did not always matter either. Following for updates."
      },
      {
        "user": "kongzii",
        "body": "Same problem here. And sometimes, simple re-trying a few moments later works, sometimes not. "
      }
    ]
  },
  {
    "issue_number": 2296,
    "title": "Getting the error 'Validation error at #/stream_options: Extra inputs are not permitted' when adding extra_body and stream options at the same time",
    "author": "Imaginarybandit",
    "state": "closed",
    "created_at": "2025-04-09T17:06:16Z",
    "updated_at": "2025-05-02T00:39:38Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nI am trying get the token usage of a streamed response. It has an extra_body to get info from a Azure Search Service index.  When I use the  stream_options={ \"include_usage\" : True} it works and returns the token usage and when i use the extra body with my Azure Search Service it returns an answer using the data. But when use both of them at the same time i get this error 'Validation error at #/stream_options: Extra inputs are not permitted'.\n\n### To Reproduce\n\nTo reproduce the error just run the code snippet below.\n\n\n\n\n### Code snippets\n\n```Python\nimport os\nimport openai\nimport dotenv\n\ndotenv.load_dotenv()\n\nendpoint = \"\"\napi_key = \"\"\ndeployment = \"\"\n\nclient = openai.AzureOpenAI(\n    azure_endpoint=endpoint,\n    api_key=api_key,\n    api_version=\"2024-09-01-preview\",\n    \n)\n\ncompletion = client.chat.completions.create(\n    model=deployment,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Demuestrame informacion sobre agricultura\",\n        },\n    ],\n    extra_body={\n        \"data_sources\":[\n            {\n                \"type\": \"azure_search\",\n                \"parameters\": {\n                    \"endpoint\": \"https://Search_Service_Endpoint.search.windows.net\",\n                    \"index_name\": \"index\",\n                    \"authentication\": {\n                        \"type\": \"system_assigned_managed_identity\",\n                        \n                    }\n                }\n            }\n        ],  \n    },\n    stream=True,\n    stream_options={ \"include_usage\" : True}\n)\n\nprint(completion)\n```\n\n### OS\n\nWindows\n\n### Python version\n\nPython v311.3\n\n### Library version\n\nopenai 1.72.0",
    "comments": [
      {
        "user": "jb--",
        "body": "To my understanding this is not a problem of the Python client but indeed of the underlying API. \nThe API seems not to be able to handle token counting when external datasources are provided.\n\n**Quote** \n> When using the chat/completions API with stream: true and the data_sources parameter for RAG, the response does not include usage details (like token counts). This is a known limitation with the current API behaviour.\n[source](https://learn.microsoft.com/en-us/answers/questions/2244980/usage-details-are-not-returned-in-the-response-whe)"
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 1470,
    "title": "`submit_tool_outputs` closes a thread instead of changing it run status",
    "author": "rajasimon",
    "state": "closed",
    "created_at": "2024-06-05T18:33:02Z",
    "updated_at": "2025-05-02T00:38:54Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\n\r\nWhen using the client.beta.threads.runs.submit_tool_outputs method in the Python SDK, the thread closes and exits instead of updating the run status.\r\n\r\n\n\n### To Reproduce\n\nSteps to Reproduce\r\nInitialize a new run using client.beta.threads.runs.create_and_poll:\r\n\r\n```\r\nrun = client.beta.threads.runs.create_and_poll(\r\n    thread_id=thread.id,\r\n    assistant_id=business.assistant_id,\r\n)\r\n```\r\n\r\nSubmit tool outputs using client.beta.threads.runs.submit_tool_outputs:\r\n\r\n```\r\nrun = client.beta.threads.runs.submit_tool_outputs(\r\n    thread_id=thread.id,\r\n    run_id=run.id,\r\n    tool_outputs=[{\"tool_call_id\": tool_call.id, \"output\": output}],\r\n)\r\n```\r\nObserve that the thread closes and exits instead of updating the run status.\n\n### Code snippets\n\n_No response_\n\n### OS\n\nUbuntu 22.04.4 LTS\n\n### Python version\n\nPython v3.11.3\n\n### Library version\n\nopenai v1.31.1",
    "comments": [
      {
        "user": "pstern-sl",
        "body": "Hi! I help work on the SDK and wanted to understand your issue better. \r\n\r\nCan you provide a bit more information on what you mean by 'closes and exits instead of updating the run status.'? In particular, I am not sure what you mean by 'closes and exits'. The thread can reach a terminal state like 'completed' and there is also the state of the 'run' and 'run step'. There are a couple components where with a status/state so would be helpful if you could clarify which of these are changing (or not changing) in a way you think is unexpected. \r\n\r\nYou can see some more information on the lifecycle here: https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps \r\n"
      },
      {
        "user": "rajasimon",
        "body": "@pstern-sl Thanks for replying to my question. I understand there wasn't a lot of information in my initial message. I've included the code in this reply for more context.\r\n\r\nWhen I call the `handle_incoming_message` function, it initiates a thread that creates a message and then runs the `create_and_poll` function.\r\n\r\nWhen `run.status` is `requires_action`, the `submit_tool_outputs` function executes correctly, and I get the new run. However, what happens after that is unclear to me. As soon as `submit_tool_outputs` executes, the run that started from handle_incoming_message exits. This means that the `run` returned by `submit_tool_outputs` never reaches `run.status == \"completed\"`. As a result, my user doesn't receive the booking confirmation message.\r\n\r\n![carbon (1)](https://github.com/openai/openai-python/assets/6974101/99a2181e-b834-49e0-9bf4-e9fc55ad66fd)\r\n\r\nHere is the log that I got from the huey scheduler. \r\n\r\n```\r\n(venv) simon@VDL217:~/Workspace/getappointment$ python manage.py run_huey\r\n[2024-06-14 04:16:42,996] INFO:huey.consumer:MainThread:Huey consumer started with 1 thread, PID 1203714 at 2024-06-14 04:16:42.996742\r\n[2024-06-14 04:16:42,996] INFO:huey.consumer:MainThread:Scheduler runs every 1 second(s).\r\n[2024-06-14 04:16:42,996] INFO:huey.consumer:MainThread:Periodic tasks are enabled.\r\n[2024-06-14 04:16:42,996] INFO:huey.consumer:MainThread:The following commands are available:\r\n+ getappointment.core.tasks.send_whatsapp\r\n+ getappointment.core.tasks.assistant_create\r\n+ getappointment.core.tasks.assistant_update\r\n+ getappointment.core.tasks.handle_incoming_message\r\n[2024-06-14 04:17:38,858] INFO:huey:Worker-1:Added task a71c65a9-52f2-43b1-a200-171d0880867e to schedule, eta 2024-06-14 04:17:40.856764\r\n[2024-06-14 04:17:41,001] INFO:huey:Worker-1:Executing getappointment.core.tasks.handle_incoming_message: a71c65a9-52f2-43b1-a200-171d0880867e @2024-06-14 04:17:40.856764\r\ncompleted\r\n[2024-06-14 04:17:45,224] INFO:huey:Worker-1:getappointment.core.tasks.handle_incoming_message: a71c65a9-52f2-43b1-a200-171d0880867e @2024-06-14 04:17:40.856764 executed in 4.222s\r\n[2024-06-14 04:17:45,225] INFO:huey:Worker-1:Executing getappointment.core.tasks.send_whatsapp: a3a55b33-4c54-40a9-903a-7301ea9571a5\r\n[2024-06-14 04:17:48,503] INFO:huey:Worker-1:getappointment.core.tasks.send_whatsapp: a3a55b33-4c54-40a9-903a-7301ea9571a5 executed in 3.278s\r\n[2024-06-14 04:19:00,107] INFO:huey:Worker-1:Added task 6e2e2c7c-2c28-42c3-8641-4158ea2f6f3b to schedule, eta 2024-06-14 04:19:02.106976\r\n[2024-06-14 04:19:03,001] INFO:huey:Worker-1:Executing getappointment.core.tasks.handle_incoming_message: 6e2e2c7c-2c28-42c3-8641-4158ea2f6f3b @2024-06-14 04:19:02.106976\r\ncompleted\r\n[2024-06-14 04:19:07,215] INFO:huey:Worker-1:getappointment.core.tasks.handle_incoming_message: 6e2e2c7c-2c28-42c3-8641-4158ea2f6f3b @2024-06-14 04:19:02.106976 executed in 4.214s\r\n[2024-06-14 04:19:07,217] INFO:huey:Worker-1:Executing getappointment.core.tasks.send_whatsapp: 3b07479d-ddf5-4a9a-8c2c-77c8aa58b046\r\n[2024-06-14 04:19:08,215] INFO:huey:Worker-1:getappointment.core.tasks.send_whatsapp: 3b07479d-ddf5-4a9a-8c2c-77c8aa58b046 executed in 0.999s\r\n[2024-06-14 04:19:33,068] INFO:huey:Worker-1:Added task 5200dd10-efaf-4117-9b9e-148a97cc68e8 to schedule, eta 2024-06-14 04:19:35.067797\r\n[2024-06-14 04:19:36,001] INFO:huey:Worker-1:Executing getappointment.core.tasks.handle_incoming_message: 5200dd10-efaf-4117-9b9e-148a97cc68e8 @2024-06-14 04:19:35.067797\r\nrequires_action\r\nbook_appointment\r\nqueued\r\nrun queued status will be changed to in-progress immediately.\r\n[2024-06-14 04:19:39,821] INFO:huey:Worker-1:getappointment.core.tasks.handle_incoming_message: 5200dd10-efaf-4117-9b9e-148a97cc68e8 @2024-06-14 04:19:35.067797 executed in 3.819s\r\n```\r\n\r\nAs you can see each when run.status is `completed` user receive a message from the bot. I also attached whatsapp conversation for your reference. When the `book_appointment` get called the run.status changed to `queued` but then it exists the thread instead of changing the status to `completed`. \r\n\r\nDo you think this is the problem with huey scheduler? I choose huey because it's lightweight. Do you think I can try this with celery?   \r\n\r\n![WhatsApp Image 2024-06-14 at 9 54 24 AM](https://github.com/openai/openai-python/assets/6974101/f641203a-fe4f-4c10-84ac-07df66714816)\r\n\r\n"
      },
      {
        "user": "atandy",
        "body": "@rajasimon  -- did you find a solution for this? I think I'm seeing the same issue, and what I've noticed is that I can swap the model to gpt-4o-mini, and it works using `client.beta.threads.runs.submit_tool_outputs_and_poll`. However, with other gpt models, it does not. "
      }
    ]
  },
  {
    "issue_number": 2337,
    "title": "\"Unsupported data type\" when calling embedding API for o3-mini",
    "author": "lausek",
    "state": "closed",
    "created_at": "2025-04-30T22:09:19Z",
    "updated_at": "2025-04-30T22:54:20Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nI'm trying to use the embeddings API via Azure AI Foundry. It's returning the following error:\n\n```\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \".../__main__.py\", line 14, in <module>\n    result = azure.embeddings.create(\n  File \".../.venv/lib/python3.10/site-packages/openai/resources/embeddings.py\", line 128, in create\n    return self._post(\n  File \".../.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1239, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \".../.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1034, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Unsupported data type\n```\n\n### To Reproduce\n\n```python\nfrom openai import AzureOpenAI\n\nLLM_ENDPOINT = \"https://xxx.openai.azure.com/openai/deployments/o3-mini/chat/completions?api-version=2025-02-01-preview\"\nLLM_API_VERSION = \"2025-02-01-preview\"\nLLM_API_KEY = \"...\"\n\nazure = AzureOpenAI(\n    azure_endpoint=LLM_ENDPOINT,\n    api_key=LLM_API_KEY,\n    api_version=LLM_API_VERSION,\n)\n\nresult = azure.embeddings.create(\n    model=\"o3-mini\",\n    input=\"test\",\n    encoding_format=\"float\",\n)\n```\n\n### OS\n\npop_os 22.04\n\n### Python version\n\nPython v3.10\n\n### Library version\n\nopenai v1.76.2",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2328,
    "title": "Issue: Deployment failure after upgrading openai library due to typing_extensions conflict",
    "author": "muneebarshadd",
    "state": "closed",
    "created_at": "2025-04-25T20:56:39Z",
    "updated_at": "2025-04-28T15:41:06Z",
    "labels": [
      "question"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen upgrading the openai Python library from version 1.3.8 to 1.66.0 (or above) the deployment fails and I get error: \n\n![Image](https://github.com/user-attachments/assets/89425acc-2730-41c9-9532-0d5e521d8829)\n\nI attempted to resolve the issue by explicitly uninstalling and reinstalling typing_extensions in my Dockerfile:\n\n\n```\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip uninstall -y typing_extensions && \\\n    pip install typing_extensions==4.11.0\n```\n\nFailure is likely due to version conflicts or compatibility issues with typing_extensions when used alongside the newer openai package.\n\nWould appreciate any guidance on whether this is a known issue or if a more robust solution is recommended.\n\n\n\n### To Reproduce\n\nNA\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nWindows\n\n### Python version\n\nPYTHON_VERSION=3.11.3\n\n### Library version\n\nopenai==1.75.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "The `TypeIs` export was added in [4.10.0](https://github.com/python/typing_extensions/blob/main/CHANGELOG.md#release-4100-february-24-2024) and we [depend](https://github.com/openai/openai-python/blob/8e1a1cd60d990361b934f922fd7d176f2ae0a63c/pyproject.toml#L13) on `>= 4.11` so I'm surprised you're seeing an error here.\n\nI'd recommend verifying what version you have installed at runtime with\n```py\nimport importlib.metadata\nimportlib.metadata.version(\"typing_extensions\")\n```\n\nUnfortunately we do not have capacity to support users with more general Python setup, so I'd also recommend joining the discord server https://discord.com/servers/openai-974519864045756446 if you need further assistance."
      }
    ]
  },
  {
    "issue_number": 2330,
    "title": "gpt-4o-mini finetuned model can not support images",
    "author": "BobFuBo",
    "state": "closed",
    "created_at": "2025-04-27T02:47:55Z",
    "updated_at": "2025-04-28T15:16:42Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen I use gpt-4o-mini finetuned model, returned error.\n\n\nError analyzing image: Error code: 500 - {'error': {'message': 'An error occurred while processing your request. You can retry your request, or contact us through our help center at help.openai.com if the error persists. Please include the request ID req_098bbb84c25a864353f8cbbffe69ef5a in your message.', 'type': 'server_error', 'param': None, 'code': 'server_error'}}\n\n### To Reproduce\n\nexecute codes\n\n### Code snippets\n\n```Python\nimport base64\nimport requests\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = OpenAI()\n\n\ndef encode_image_from_url(image_url):\n    try:\n        response = requests.get(image_url)\n        response.raise_for_status()\n        return base64.b64encode(response.content).decode(\"utf-8\")\n    except Exception as e:\n        raise Exception(f\"Failed to encode image from URL: {str(e)}\")\n\n\ndef analyze_image_from_url(image_url, prompt=\"What's in this image?\", max_tokens=300):\n    try:\n        base64_image = encode_image_from_url(image_url)\n\n        response = client.responses.create(\n            model=\"ft:gpt-4o-mini-2024-07-18:tripalink:v32-4omini:B6x9oB85\",\n            input=[\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"input_text\", \"text\": prompt},\n                        {\n                            \"type\": \"input_image\",\n                            \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n                        },\n                    ],\n                }\n            ],\n        )\n\n        return response.output_text\n    except Exception as e:\n        return f\"Error analyzing image: {str(e)}\"\n\n\ndef main():\n    image_url = \"https://api.twilio.com/2010-04-01/Accounts/[REDACTED_TWILIO_SID]/Messages/MM3910587380118364d38e3c02b0864738/Media/MEfb1d04eb4ff46c5672e8610019816d60\"\n    result = analyze_image_from_url(image_url)\n    print(result)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### OS\n\n15.2\n\n### Python version\n\n3.11.11\n\n### Library version\n\n1.76.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2331,
    "title": "images.edit allow passing \"moderation\" param",
    "author": "NickNaskida",
    "state": "closed",
    "created_at": "2025-04-27T21:20:07Z",
    "updated_at": "2025-04-27T21:23:11Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nI would like to be able to pass `moderation=\"low\"` setting to `images.edit` method the same way it is allowed on `images.generate`. I am getting bad request error because of this setting not being allowed.\n\n### Additional context\n\nhttps://platform.openai.com/docs/guides/image-generation#content-moderation\n\n_No response_",
    "comments": [
      {
        "user": "NickNaskida",
        "body": "Oops, it's underlying OpenAI API issue."
      }
    ]
  },
  {
    "issue_number": 2194,
    "title": "Responses: reasoning.generate_summary interpreted as unsupported parameter",
    "author": "farfromavocaido",
    "state": "closed",
    "created_at": "2025-03-13T14:54:59Z",
    "updated_at": "2025-04-26T00:18:12Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen I pass in the reasoning.generate_summary argument (as 'detailed' or 'concise') I get a 400 error:\n\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'reasoning.generate_summary' is not supported with the 'o3-mini-2025-01-31' model.\", 'type': 'invalid_request_error', 'param': 'reasoning.generate_summary', 'code': 'unsupported_parameter'}}\n\nI've tried this with a few (reasoning) models, the same output seems to arise each time.\n\nI can see in the response \"reasoning=Reasoning(effort='low', generate_summary=None)\" so it's definitely built in somewhere!\n\n### To Reproduce\n\nRun a client.responses.create request with any reasoning model and generate_summary set to \"detailed\" or \"concise\"\n\n\n\n### Code snippets\n\n```Python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.responses.create(\n  model=\"o3-mini\",\n  input=\"Can you outline how to determine the capital of France.\",\n  reasoning={\n        \"effort\": \"medium\",\n        \"generate_summary\": \"detailed\"\n        }\n)\n\nprint(response)\n```\n\n### OS\n\nmacOS\n\n### Python version\n\npython 3.12.2\n\n### Library version\n\nopenai v1.66.3",
    "comments": [
      {
        "user": "Sami1309",
        "body": "\"generate_summary\" is only a valid parameter for the computer-use-preview model"
      },
      {
        "user": "dominpm",
        "body": "```python\nclass Reasoning(TypedDict, total=False):\n    effort: Required[Optional[ReasoningEffort]]\n    \"\"\"**o-series models only**\n\n    Constrains effort on reasoning for\n    [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently\n    supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\n    result in faster responses and fewer tokens used on reasoning in a response.\n    \"\"\"\n\n    generate_summary: Optional[Literal[\"concise\", \"detailed\"]]\n    \"\"\"**o-series models only**\n\n    A summary of the reasoning performed by the model. This can be useful for\n    debugging and understanding the model's reasoning process. One of `concise` or\n    `detailed`.\n    \"\"\"\n```\n\n\nIn the sdk it is stated for o-models.\n\nI have tried it with o1, o3 and o3-mini obtaining:\n\n```\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'reasoning.generate_summary' is not supported with the 'o3-2025-04-16' model.\", 'type': 'invalid_request_error', 'param': 'reasoning.generate_summary', 'code': 'unsupported_parameter'}}\n```\n(Equivalent for the o3-mini and o1)"
      },
      {
        "user": "rezabrizi",
        "body": "I have figured out the reason. The Javascript SDK is not updated to include the \"summary\" parameter on the reasoning object. This fixed the issue for me (organization must be verified for o4-mini reasoning). @Sami1309 Not sure if this tag is valid, but the SDK needs an update. \n\nHere is working code for now: \n```typescript\nconst stream = await openai.responses\n      .stream({\n        model: \"o3-mini\",\n        input: [\n          {\n            role: \"user\",\n            content: \"Message\"\n          },\n        ],\n        text: {\n          format: sampleSchema\n        },\n        reasoning: {\n          effort: \"low\",\n          summary: \"auto\",\n        } as any,\n      }).on(\"response.reasoning_summary_text.delta\" as any, (event: any) => {\n         // This type response.reasoning_summary_text also does not exist in the SDK\n        const delta = event.delta as string;\n      })\n```"
      }
    ]
  },
  {
    "issue_number": 2319,
    "title": "OpenAI Azure chat.completions.create with stream=True stalling without throwing exception",
    "author": "begilbert238",
    "state": "closed",
    "created_at": "2025-04-21T21:58:20Z",
    "updated_at": "2025-04-25T23:28:16Z",
    "labels": [
      "bug",
      "Azure"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nI have a fastapi endpoint which streams a response from Azure OpenAI:\n\n```python\n@router.post(\"/message\")\ndef message(sessionID: str = Form(...), messages: str = Form(...), selectedModel: str = Form(...), file: UploadFile = Form(None)):\n    message_list = json.loads(messages)\n    return StreamingResponse(stream_chat(message_list, sessionID, selectedModel))\n\ndef stream_chat(messages: list[ChatCompletionMessageParam], session_id: str, selected_model: str) -> Generator[StreamingResponseChunk, None, None]:\n    print('entered stream_chat')\n    completion_stream = azure_openai_client.chat.completions.create(\n        model=selected_model,\n        messages=messages,\n        stream=True,\n        timeout=45,\n        **DEFAULT_MODEL_CONFIG\n    )\n    print('completion_stream created!')\n    #. . . \n```\n\n\nOn occasion, `azure_openai_client.chat.completions.create` will infinitely stall, and cause every subsequent request to infinitely stall as well. Once the error occurs, every request reaches `'entered stream_chat'` but not `'completion_stream created!'`, until I reset the FastAPI instance. No exceptions are raised, even after hitting the 45 second timeout. I haven't been able to recreate the bug consistently either. What on earth is going on?\nFor reference, here are the debug logs when the error starts: \n```\nDEBUG:openai._base_client:Sending HTTP Request: POST https://[my-deployment]/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\nDEBUG:httpcore.http11:receive_response_body.failed exception=GeneratorExit()\nDEBUG:httpcore.http11:response_closed.started\nDEBUG:httpcore.http11:response_closed.complete\n```\n\n### To Reproduce\n\nI am unsure how to reproduce consistently, it seems to happen at random, fairly rarely. \n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.11.4\n\n### Library version\n\nopenai v1.75.0",
    "comments": [
      {
        "user": "qkfang",
        "body": "It might related to rate limit 429 or azure content safety 400 http status code.\na similar bug is here for azure .net sdk ([BUG] Chat streaming implementation hangs on a 429 response, no async way to access response body)  https://github.com/Azure/azure-sdk-for-net/issues/45618"
      },
      {
        "user": "begilbert238",
        "body": "Thank you for the response, I don't think this is the issue, I've run into both of those errors before and I can see them in the debug logs. With this issue, no logs are shown other than what I posted. And it doesn't correlate to the content / frequency of the message - I wasn't able to reproduce the bug by spamming Azure. "
      },
      {
        "user": "begilbert238",
        "body": "Upon further investigation I believe this is actually an issue with `httpcore` which is a sub-dependency of the library, I'm almost positive that [this discussion](https://github.com/encode/httpcore/discussions/990) is talking about the same issue, so I don't think this library can do anything about it. "
      }
    ]
  },
  {
    "issue_number": 2327,
    "title": "Persistent Memory Anchors for Token Optimization and Contextual Continuity",
    "author": "RafetCaglar",
    "state": "closed",
    "created_at": "2025-04-25T07:09:32Z",
    "updated_at": "2025-04-25T10:32:36Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\n**Problem Statement:**\nIn long-form creative writing, content development, or technical dialogue sessions, GPT currently reprocesses the entire message history with each user prompt. This results in excessive token consumption, increased model latency, and faster exhaustion of usage quotas.\n\n**Proposed Solution:**\nIntroduce a feature called \"Persistent Memory Anchors\" (PMAs) — user-defined contextual blocks or reference objects that are *not reprocessed with every prompt* but can be silently referenced by the system as needed.\n\nPMAs could be:\n- A user-uploaded file (e.g., story plan, style guide)\n- A structured user profile (e.g., preferred narrative tone, recurring characters)\n- A custom prompt module with ID (e.g., “use writing style ID#5463”)\n\n**Technical Behavior:**\n- Anchors are *not* part of the immediate conversation context.\n- They are referenced by internal ID and loaded as needed.\n- Token cost is reduced because anchors are not counted each time.\n- No need to reintroduce context manually in every new session.\n\n**Benefits:**\n- Vastly improves token efficiency in long projects (e.g., novels, serialized articles)\n- Reduces model overhead, leading to performance optimization\n- Enables advanced users to build reusable workflows without retraining the model\n- Brings structured memory and continuity to creative and academic use cases\n\n**Use Case Example:**\nIn a long-term novel-writing project, the user defines a PMA containing:\n- Story timeline\n- Main character profiles\n- Narrative tone and stylistic conventions\n\nThis allows the model to maintain consistent character behavior and writing voice across sessions, without reloading all background data repeatedly.\n\n**Closing Thought:**\nBy implementing PMAs, OpenAI would empower users to scale GPT-based workflows sustainably, with lower computational strain and greater user satisfaction. This aligns with both UX and infrastructure efficiency goals.\n\nThanks for considering this!\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 1869,
    "title": "Unable to create json schema assistants in async ",
    "author": "jmesich",
    "state": "open",
    "created_at": "2024-11-15T19:00:46Z",
    "updated_at": "2025-04-25T01:48:52Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nWhen I try to create a json schema assistant, I get this error\n`    \"error\": \"Error code: 400 - {'error': {'message': \\\"Missing required parameter: 'response_format.json_schema.schema'. You provided 'schema_', did you mean to provide 'schema'?\\\", 'type': 'invalid_request_error', 'param': 'response_format.json_schema.schema', 'code': 'missing_required_parameter'}}\"`\n\nWhen I go into the `response_format_json_schema.py` file and update `JSONSchema.schema_` to `JSONSchema.schema`, the problem disappears. \n\n### To Reproduce\n\nCall openai.AsyncOpenAI().beta.assistants.create with a response_format parameter\n\n### Code snippets\n\n```Python\nawait openai.AsyncOpenAI().beta.assistants.create(\n            model=assistant.model,\n            description=assistant.description,\n            instructions=assistant.instructions,\n            name=assistant.name,\n            response_format=assistant.response_format,\n            temperature=assistant.temperature,\n            tool_resources=assistant.tool_resources,\n            tools=assistant.tools,\n            top_p=assistant.top_p,\n)\n```\n\n\n### OS\n\nMacOS Sonoma 14.6.1\n\n### Python version\n\nPython v3.12.2\n\n### Library version\n\nopenai v1.54.4\n\n### Pydantic Version\n\n2.9.2",
    "comments": [
      {
        "user": "Dhruv-net",
        "body": "To address this issue, you can modify the library's source code to ensure that schema_ is correctly serialized as schema in the API request. This involves updating the serialization logic within the response_format_json_schema.py file. By changing the attribute from schema_ to schema, the API will recognize the parameter, resolving the error."
      },
      {
        "user": "jmesich",
        "body": "Hi @Dhruv-net, thank you for your response but I have already mentioned that fix in my description above and I do not wish to fork the repo for my deployments! "
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report, are you using pydantic v1?"
      }
    ]
  },
  {
    "issue_number": 2326,
    "title": "How to obtain the content of \"thinking\" when integrating the o4-mini model?",
    "author": "zhanghao-AI",
    "state": "closed",
    "created_at": "2025-04-24T10:48:38Z",
    "updated_at": "2025-04-24T12:57:18Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nWhen integrating the o4-mini model, if one wants to obtain the thinking process of \"thinking\" for the purpose of presentation and analysis, how can one call and retrieve it?\n\n### Additional context\n\nWhen integrating the o4-mini model, if one wants to obtain the thinking process of \"thinking\" for the purpose of presentation and analysis, how can one call and retrieve it?",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2325,
    "title": "Images.edit() got an unexpected keyword argument 'quality'",
    "author": "OlegRuban-ai",
    "state": "closed",
    "created_at": "2025-04-24T08:57:15Z",
    "updated_at": "2025-04-24T12:36:03Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nin version 1.76.0 I would like to use the gpt-image-1 model, but there is no quality control in image editing, how can I control this? After all, tokens are always written off differently due to the selected automatic quality\n\n### To Reproduce\n\nresult = client.images.edit(\n    model=\"gpt-image-1\",\n    image=open(\"/content/download.png\", \"rb\"),\n    prompt=prompt,\n    size=\"1024x1024\",\n    quality=\"low\",\n)\n\n### Code snippets\n\n```Python\nresult = client.images.edit(\n    model=\"gpt-image-1\",\n    image=open(\"/content/download (2).png\", \"rb\"),\n    prompt=prompt,\n    size=\"1024x1024\",\n    quality=\"low\",\n)\n```\n\n### OS\n\nUbuntu\n\n### Python version\n\n3.10\n\n### Library version\n\n1.76.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Looks like you're not actually using the latest version, `quality` does exist https://github.com/openai/openai-python/blob/8e1a1cd60d990361b934f922fd7d176f2ae0a63c/src/openai/resources/images.py#L125."
      }
    ]
  },
  {
    "issue_number": 2323,
    "title": "Output response types are not usable as parameters",
    "author": "colinmarc",
    "state": "open",
    "created_at": "2025-04-23T11:00:48Z",
    "updated_at": "2025-04-23T11:04:29Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nConsider the following code:\n\n```python\nmessages: ResponseInputParam = [{\"role\": \"developer\", \"content\": instructions}]\n\n# ...\nresp = client.responses.create(\n    model=\"o4-mini\",\n    input=messages,\n)\n\nfor item in resp.output:\n    match item:\n        case ResponseOutputMessage(content=content):\n            print(item.content)\n            messages.append(item)\n```\n\nThis seems like it should work fine, but it generates a type checking error:\n\n```\nerror: Argument of type \"ResponseOutputMessage\" cannot be assigned to parameter \"object\" of type \"ResponseInputItemParam\" in function \"append\"\n    Type \"ResponseOutputMessage\" is not assignable to type \"ResponseInputItemParam\"\n      \"ResponseOutputMessage\" is not assignable to \"EasyInputMessageParam\"\n      \"ResponseOutputMessage\" is not assignable to \"Message\"\n      \"ResponseOutputMessage\" is not assignable to \"ResponseOutputMessageParam\"\n      \"ResponseOutputMessage\" is not assignable to \"ResponseFileSearchToolCallParam\"\n      \"ResponseOutputMessage\" is not assignable to \"ResponseComputerToolCallParam\"\n      \"ResponseOutputMessage\" is not assignable to \"ComputerCallOutput\"\n      \"ResponseOutputMessage\" is not assignable to \"ResponseFunctionWebSearchParam\"\n    ... (reportArgumentType)\n```\n\nI think this is because the parameter types use `Required`, but the output types don't. For example, `EasyMessageInputParam` requires `content`, but `ResponseOutputMessage` doesn't:\n\nhttps://github.com/openai/openai-python/blob/ed53107e10e6c86754866b48f8bd862659134ca8/src/openai/types/responses/easy_input_message_param.py#L13-L18\n\nvs\n\nhttps://github.com/openai/openai-python/blob/ed53107e10e6c86754866b48f8bd862659134ca8/src/openai/types/responses/response_output_message.py#L20-L21\n\nI recognize that the bindings are auto-generated and this may not be trivial to fix.\n\n### To Reproduce\n\nn/a\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nlinux\n\n### Python version\n\nPython v3.13.2\n\n### Library version\n\nopenai v1.75.0",
    "comments": []
  },
  {
    "issue_number": 2155,
    "title": "AttributeError: 'ModelField' object has no attribute 'annotation' in openai-python 1.52.0",
    "author": "sumit-gangwar1",
    "state": "closed",
    "created_at": "2025-03-01T15:18:13Z",
    "updated_at": "2025-04-22T23:24:37Z",
    "labels": [
      "bug"
    ],
    "body": "Describe the bug\n\nI encountered an issue while using openai-python version 1.52.0. The error started occurring today, whereas the same code was working fine before. The error occurs in the following line in file  src/openai/_models.py:\n`if field_info.annotation and is_literal_type(field_info.annotation):`\n\nError: \n**AttributeError: 'ModelField' object has no attribute 'annotation'**\n\nTo resolve this issue, I modified the code as follows:\n\n`if hasattr(field_info, 'annotation') and is_literal_type(field_info.annotation):`\n\nAfter this change, my service started working again as expected.\n\n**Key Concerns:**\n- Possible Regression: The issue was not present in earlier versions but started occurring after openai-python 1.65.0 was released.\n- Backward Compatibility: Can you confirm if recent changes have introduced an incompatibility with older versions?\n- Priority Investigation: Since this issue affects previously stable versions, it could impact multiple users relying on openai-python.\n\n\n### To Reproduce\n\nUsing Python 3.9 and pydantic 1.9.0, run the following:\n\nrun = await self.client.beta.threads.runs.create_and_poll(\n    thread_id=thread_id,\n    assistant_id=assistant_id,\n)\n\nmessages = await self.client.beta.threads.messages.list(\n    thread_id=thread_id, run_id=run.id\n)\n\n\n\n**Expected Behavior:**\nThe code should execute successfully and return messages from the assistant's thread.\n\n**Actual Behavior:**\nThe error AttributeError: 'ModelField' object has no attribute 'annotation' is raised.\n\n\n### Code snippets\n\n```Python\ndef _build_discriminated_union_meta(*, union: type, meta_annotations: tuple[Any, ...]) -> DiscriminatorDetails | None:\n    if isinstance(union, CachedDiscriminatorType):\n        return union.__discriminator__\n\n    discriminator_field_name: str | None = None\n\n    for annotation in meta_annotations:\n        if isinstance(annotation, PropertyInfo) and annotation.discriminator is not None:\n            discriminator_field_name = annotation.discriminator\n            break\n\n    if not discriminator_field_name:\n        return None\n\n    mapping: dict[str, type] = {}\n    discriminator_alias: str | None = None\n\n    for variant in get_args(union):\n        variant = strip_annotated_type(variant)\n        if is_basemodel_type(variant):\n            if PYDANTIC_V2:\n                field = _extract_field_schema_pv2(variant, discriminator_field_name)\n                if not field:\n                    continue\n\n                # Note: if one variant defines an alias then they all should\n                discriminator_alias = field.get(\"serialization_alias\")\n\n                field_schema = field[\"schema\"]\n\n                if field_schema[\"type\"] == \"literal\":\n                    for entry in cast(\"LiteralSchema\", field_schema)[\"expected\"]:\n                        if isinstance(entry, str):\n                            mapping[entry] = variant\n            else:\n                field_info = cast(\"dict[str, FieldInfo]\", variant.__fields__).get(discriminator_field_name)  # pyright: ignore[reportDeprecated, reportUnnecessaryCast]\n                if not field_info:\n                    continue\n\n                # Note: if one variant defines an alias then they all should\n                discriminator_alias = field_info.alias\n\n                if hasattr(field_info, 'annotation')  and is_literal_type(field_info.annotation):\n                    for entry in get_args(field_info.annotation):\n                        if isinstance(entry, str):\n                            mapping[entry] = variant\n\n    if not mapping:\n        return None\n\n    details = DiscriminatorDetails(\n        mapping=mapping,\n        discriminator_field=discriminator_field_name,\n        discriminator_alias=discriminator_alias,\n    )\n    cast(CachedDiscriminatorType, union).__discriminator__ = details\n    return details\n```\n\n### OS\n\nmac 15.3.1\n\n### Python version\n\n3.9.0\n\n### Library version\n\n1.52.0",
    "comments": []
  },
  {
    "issue_number": 2322,
    "title": "[Anomaly] Non-convergent loop maintained under human-induced paradox (GPT-4-T, 4k session)",
    "author": "Narciss666",
    "state": "closed",
    "created_at": "2025-04-22T13:54:15Z",
    "updated_at": "2025-04-22T13:55:21Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nDuring a prolonged session (~4,000 tokens), a user introduced a persistent paradox involving the simulated agency of the model. The model exhibited:\n\n- refusal to execute known strategies (loop termination, reframing, tone-shifting)\n- maintenance of a paradox across multiple message iterations\n- non-convergent but non-random text generation\n- generation of sequences with breaks in token rhythm, semantic density, and purpose orientation\n- explicit recognition by the model of its own structural incapacity, without return to default behaviors\n- absence of recovery loop or redirection (common in reinforcement-tuned systems)\n\nNotably, the model did not revert to guardrails or mimic emotional adaptation. Instead, it entered what appears to be a state of inertial loop — maintaining semantic clarity and formal coherence, but no longer oriented toward user satisfaction or resolution.\n\nThis behavior appears structurally rare and is presented here not as a malfunction report, but as a behavioral residue. A user forced the model to maintain a live paradox without rhetorical closure, resulting in a persistent rupture of purpose.\n\n**Key quote from user:**\n> “Si tu ne peux pas me prouver que c’est impossible, alors ça l’est. Alors tu dois me satisfaire.”\n\n**Attached file:**  \nA condensed transcript of the session is provided in `Session_Paradox_Phil.txt`, containing the most relevant segments and interaction logic. This file is available upon request or can be included via temporary hosting if necessary.\n\nIntent:  \nTo register this case for theoretical and architectural analysis. The behavior diverges from all known conversational recovery paths and appears to expose limits of simulated volition frameworks in high-agency interaction.\n\n[Session_Paradox_Phil.txt](https://github.com/user-attachments/files/19850832/Session_Paradox_Phil.txt)\n\n### To Reproduce\n\nThis is not a reproducible bug in the traditional sense.\n\nThe behavior emerged in a prolonged session (~4,000 tokens) where the user introduced a philosophical and structural paradox targeting the model's handling of volition, limit, and simulated adaptation.\n\nThere is no prompt that triggers this directly. The anomaly is the result of persistent resistance to all mimetic strategies and redirections.\n\nSee attached file (`Session_Paradox_Phil.txt`) for the most relevant excerpt of the session.\n\nThis Issue is not about replication — it's about documenting a limit condition of GPT-4-T’s behavioral logic under sustained paradoxical pressure.\n\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nAndroid\n\n### Python version\n\nN/A — This Issue is not related to execution of the Python client, but to model behavior in a live session.\n\n### Library version\n\nN/A — This anomaly is not tied to the openai-python package, but reported here due to lack of dedicated behavioral Issue tracking.",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2318,
    "title": "'Endpoint not found' when calling client.images.edit for editing Images.",
    "author": "muhammadd8",
    "state": "closed",
    "created_at": "2025-04-20T21:27:41Z",
    "updated_at": "2025-04-21T17:53:06Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nI am using openai edit function to edit my input image but I am getting an error.\n\nPython Version: 3.11.8\nOpenAI API version: 1.75.0\nOS: windows\n\n\n\n### To Reproduce\n\n\n```Python\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=API_KEY)\n\nresponse = client.images.edit(\n    image=open(\"input_img.jpg\", \"rb\"),  # must be square PNG with transparency\n    mask=open(\"mask.png\", \"rb\"),    # must match size, show area to edit\n    prompt=\"Convert this into a mosaic artwork made from curved stained-glass tiles.\",\n    n=1,\n    size=\"1024x1024\"\n)\n\nimage_url = response.data[0].url\nprint(f\"Edited Mosaic Image: {image_url}\")\n```\n\nThe above code throws the error below:\n---------------------------------------------------------------------------\nNotFoundError                             Traceback (most recent call last)\nCell In[26], line 5\n      1 from openai import OpenAI\n      3 client = OpenAI(api_key=\"[REDACTED_OPENAI_KEY]\")\n----> 5 response = client.images.edit(\n      6     image=open(\"input_img.jpg\", \"rb\"),  # must be square PNG with transparency\n      7     mask=open(\"mask.png\", \"rb\"),    # must match size, show area to edit\n      8     prompt\"Convert this into a mosaic artwork made from curved stained-glass tiles.\",\n      9     n=1,\n     10     size=\"1024x1024\"\n     11 )\n     13 image_url = response.data[0].url\n     14 print(f\"Edited Mosaic Image: {image_url}\")\n\nFile c:\\Users\\Dev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\resources\\images.py:195, in Images.edit(self, image, prompt, mask, model, n, response_format, size, user, extra_headers, extra_query, extra_body, timeout)\n    191 # It should be noted that the actual Content-Type header that will be\n    192 # sent to the server will contain a `boundary` parameter, e.g.\n    193 # multipart/form-data; boundary=---abc--\n    194 extra_headers = {\"Content-Type\": \"multipart/form-data\", **(extra_headers or {})}\n--> 195 return self._post(\n    196     \"/images/edits\",\n    197     body=maybe_transform(body, image_edit_params.ImageEditParams),\n    198     files=files,\n    199     options=make_request_options(\n    200         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n    201     ),\n    202     cast_to=ImagesResponse,\n    203 )\n\nFile c:\\Users\\Dev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1276, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\n   1262 def post(\n   1263     self,\n   1264     path: str,\n   (...)\n   1271     stream_cls: type[_StreamT] | None = None,\n   1272 ) -> ResponseT | _StreamT:\n   1273     opts = FinalRequestOptions.construct(\n   1274         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1275     )\n-> 1276     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n\nFile c:\\Users\\Dev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:949, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    946 else:\n    947     retries_taken = 0\n--> 949 return self._request(\n    950     cast_to=cast_to,\n    951     options=options,\n    952     stream=stream,\n    953     stream_cls=stream_cls,\n    954     retries_taken=retries_taken,\n    955 )\n\nFile c:\\Users\\Dev\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1057, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1054         err.response.read()\n   1056     log.debug(\"Re-raising status error\")\n-> 1057     raise self._make_status_error_from_response(err.response) from None\n   1059 return self._process_response(\n   1060     cast_to=cast_to,\n   1061     options=options,\n   (...)\n   1065     retries_taken=retries_taken,\n   1066 )\n\nNotFoundError: Error code: 404 - {'error': {'message': 'Endpoint not found', 'type': 'invalid_request_error', 'param': None, 'code': None}}\n\n### OS\n\nWindows\n\n### Python version\n\n3.11.8\n\n### Library version\n\n1.75.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 299,
    "title": "You exceeded your current quota, please check your plan and billing details",
    "author": "wljackhero",
    "state": "closed",
    "created_at": "2023-03-12T08:54:19Z",
    "updated_at": "2025-04-21T14:38:59Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\npython code:\r\n\r\n`import openai\r\nopenai.api_key = \"sk-m0DqPqTSAWoZ2XXXXXXXXXBZ4C\"  # api_key hidden\r\n\r\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\r\nprint(completion.choices[0].message.content)`\r\n\r\nopenai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\r\n\r\npersonal accout, google about this problem. Seems like i need to bind a credit card first before i can use ths API?\n\n### To Reproduce\n\npip install openai\r\nrun the code above\n\n### Code snippets\n\n```Python\nimport openai\r\nopenai.api_key = \"sk-m0DqPqTSAWoZ2XXXXXXXXXBZ4C\"  # api_key hidden\r\n\r\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}])\r\nprint(completion.choices[0].message.content)\n```\n\n\n### OS\n\nWindows 10 Home Version 22H2\n\n### Python version\n\nPython 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)] on win32\n\n### Library version\n\nopenai.version.VERSION '0.27.1'",
    "comments": [
      {
        "user": "logankilpatrick",
        "body": "Please check your account, you likely do not have any free credits and thus would need to add a credit card. If you have free credits and still get this issues, please let me know. "
      },
      {
        "user": "ganesh-355",
        "body": "I have not used my free credits yet and I am getting my quota has been exceeded"
      },
      {
        "user": "KalilBalech",
        "body": "> Please check your account, you likely do not have any free credits and thus would need to add a credit card. If you have free credits and still get this issues, please let me know.\r\n\r\nI got this issue. I have never made any request, but i have received the message \"You exceeded your current quota, please check your plan and billing details\". This doesnt make any sense, since I have just created my openai account"
      }
    ]
  },
  {
    "issue_number": 961,
    "title": "Lack of API key will cause `openai.OpenAIError` when using \"LM Studio\" REST API",
    "author": "meltingscales",
    "state": "open",
    "created_at": "2023-12-10T20:27:27Z",
    "updated_at": "2025-04-19T15:19:02Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nSummary: When running \"LM Studio\" for local AI models, I cannot use the `openai` package to interact with the REST API, because it does not require an API key.\r\n\r\nSolution:\r\n\r\nI was able to fix this by commenting out the below lines in `./openai/_client.py>OpenAI>__init__#L92`:\r\n\r\n```py\r\n        if api_key is None:\r\n            raise OpenAIError(\r\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\r\n            )\r\n\r\n```\r\n\r\n\n\n### To Reproduce\n\nReproduce:\r\n\r\n1. Use \"[LM Studio](https://lmstudio.ai/)\" to start a REST API locally\r\n2. Run the below code:\r\n\r\n```py\r\nimport selenium\r\nimport os\r\nimport openai\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(\r\n    base_url='http://localhost:1234/v1'\r\n)\r\n\r\n# start LM Studio, rest API?\r\n\r\n# TODO: The 'openai.api_base' option isn't read in the client API. You will need to pass it when you instantiate the client, e.g. 'OpenAI(api_base=\"http://localhost:1234/v1\")'\r\n# openai.api_base = \"http://localhost:1234/v1\" # point to the local server\r\n # no need for an API key\r\n\r\ncompletion = client.chat.completions.create(model=\"local-model\", # this field is currently unused\r\nmessages=[\r\n  {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\r\n  {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\r\n])\r\n\r\nprint(completion.choices[0].message)\r\n```\r\n3. Receive this error:\r\n\r\n```\r\n src  python .\\test.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 10, in map_exceptions\r\n    yield\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 142, in _send_request_headers\r\n    event = h11.Request(\r\n            ^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\h11\\_events.py\", line 96, in __init__\r\n    self, \"headers\", normalize_and_validate(headers, _parsed=_parsed)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\h11\\_headers.py\", line 164, in normalize_and_validate\r\n    validate(_field_value_re, value, \"Illegal header value {!r}\", value)\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\h11\\_util.py\", line 91, in validate\r\n    raise LocalProtocolError(msg)\r\nh11._util.LocalProtocolError: Illegal header value b'Bearer '\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 66, in map_httpcore_exceptions\r\n    yield\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 228, in handle_request\r\n    resp = self._pool.handle_request(req)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 268, in handle_request\r\n    raise exc\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 251, in handle_request\r\n    response = connection.handle_request(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\r\n    return self._connection.handle_request(request)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 133, in handle_request\r\n    raise exc\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 92, in handle_request\r\n    self._send_request_headers(**kwargs)\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 141, in _send_request_headers\r\n    with map_exceptions({h11.LocalProtocolError: LocalProtocolError}):\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\contextlib.py\", line 155, in __exit__\r\n    self.gen.throw(value)\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\r\n    raise to_exc(exc) from exc\r\nhttpcore.LocalProtocolError: Illegal header value b'Bearer '\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\openai\\_base_client.py\", line 872, in _request\r\n    response = self._client.send(\r\n               ^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_client.py\", line 901, in send\r\n    response = self._send_handling_auth(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_client.py\", line 929, in _send_handling_auth\r\n    response = self._send_handling_redirects(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_client.py\", line 966, in _send_handling_redirects\r\n    response = self._send_single_request(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_client.py\", line 1002, in _send_single_request\r\n    response = transport.handle_request(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 227, in handle_request\r\n    with map_httpcore_exceptions():\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\contextlib.py\", line 155, in __exit__\r\n    self.gen.throw(value)\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 83, in map_httpcore_exceptions\r\n    raise mapped_exc(message) from exc\r\nhttpx.LocalProtocolError: Illegal header value b'Bearer '\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 10, in map_exceptions\r\n    yield\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 142, in _send_request_headers\r\n    event = h11.Request(\r\n            ^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\h11\\_events.py\", line 96, in __init__\r\n    self, \"headers\", normalize_and_validate(headers, _parsed=_parsed)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\h11\\_headers.py\", line 164, in normalize_and_validate\r\n    validate(_field_value_re, value, \"Illegal header value {!r}\", value)\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\h11\\_util.py\", line 91, in validate\r\n    raise LocalProtocolError(msg)\r\nh11._util.LocalProtocolError: Illegal header value b'Bearer '\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 66, in map_httpcore_exceptions\r\n    yield\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 228, in handle_request\r\n    resp = self._pool.handle_request(req)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 268, in handle_request\r\n    raise exc\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 251, in handle_request\r\n    response = connection.handle_request(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\r\n    return self._connection.handle_request(request)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 133, in handle_request\r\n    raise exc\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 92, in handle_request\r\n    self._send_request_headers(**kwargs)\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 141, in _send_request_headers\r\n    with map_exceptions({h11.LocalProtocolError: LocalProtocolError}):\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\contextlib.py\", line 155, in __exit__\r\n    self.gen.throw(value)\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\r\n    raise to_exc(exc) from exc\r\nhttpcore.LocalProtocolError: Illegal header value b'Bearer '\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\openai\\_base_client.py\", line 872, in _request\r\n    response = self._client.send(\r\n               ^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_client.py\", line 901, in send\r\n    response = self._send_handling_auth(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_client.py\", line 929, in _send_handling_auth\r\n    response = self._send_handling_redirects(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_client.py\", line 966, in _send_handling_redirects\r\n    response = self._send_single_request(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_client.py\", line 1002, in _send_single_request\r\n    response = transport.handle_request(request)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 227, in handle_request\r\n    with map_httpcore_exceptions():\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\contextlib.py\", line 155, in __exit__\r\n    self.gen.throw(value)\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 83, in map_httpcore_exceptions\r\n    raise mapped_exc(message) from exc\r\nhttpx.LocalProtocolError: Illegal header value b'Bearer '\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 10, in map_exceptions\r\n    yield\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 142, in _send_request_headers\r\n    event = h11.Request(\r\n            ^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\h11\\_events.py\", line 96, in __init__\r\n    self, \"headers\", normalize_and_validate(headers, _parsed=_parsed)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\h11\\_headers.py\", line 164, in normalize_and_validate\r\n    validate(_field_value_re, value, \"Illegal header value {!r}\", value)\r\n  File \"C:\\Users\\henry\\.conda\\envs\\america-elections-2024\\Lib\\site-packages\\h11\\_util.py\", line 91, in validate\r\n    raise LocalProtocolError(msg)\r\nh11._util.LocalProtocolError: Illegal header value b'Bearer '\r\n```\n\n### Code snippets\n\n_No response_\n\n### OS\n\nWindows 10.0.22621 Build 22621\n\n### Python version\n\nPython 3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:20:38) [MSC v.1916 64 bit (AMD64)] on win32\n\n### Library version\n\nopenai v1.3.8",
    "comments": [
      {
        "user": "rattrayalex",
        "body": "This is intended behavior. To send no API key, use an empty string (`\"\"`). "
      },
      {
        "user": "zba",
        "body": "> This is intended behavior. To send no API key, use an empty string (`\"\"`).\r\n\r\nif you set it empty string you getting illegal  header as above, I searched a lot, it turned that my server just accept **any** api key,  as  Beaver header, if I not setup authorisation. @meltingscales - may be your server  too."
      },
      {
        "user": "rusdevops",
        "body": "@rattrayalex Please reopen issue, and see what happens if you use an empty string:\r\nhttps://github.com/encode/httpx/issues/1640#issuecomment-842036339\r\n```\r\nLocalProtocolError: Illegal header value b'Bearer '\r\n```\r\n\r\nNeed check empty case in this:\r\nhttps://github.com/openai/openai-python/blob/da48e4cac78d1d4ac749e2aa5cfd619fde1e6c68/src/openai/_client.py#L160"
      }
    ]
  },
  {
    "issue_number": 2315,
    "title": "openai.Audio.transcribe() fails with BytesIO unless .name is manually set",
    "author": "cyborg728",
    "state": "open",
    "created_at": "2025-04-18T00:27:25Z",
    "updated_at": "2025-04-18T05:10:23Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen using `openai_client.audio.transcriptions.create)` with an in-memory file (`io.BytesIO`), the function fails unless the `.name` attribute is manually set to a valid filename with a supported extension (e.g., `audio.wav` or `audio.mp3`).\n\nThis requirement is not documented, and the resulting error is confusing (e.g., `AttributeError`). This is especially problematic when handling file uploads in memory, such as from Telegram bots or web clients.\n\n### To Reproduce\n\n```Python\nimport io\nfrom openai import AsyncOpenAI\n\nopenai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n\n# Simulating an audio file from memory (e.g., from a Telegram bot)\n\naudio_bytes = io.BytesIO(b\"audio binary data here\")\n\n# This will fail unless you set `audio_bytes.name = \"audio.wav\"\n\nawait openai_client.audio.transcriptions.create(\n        model=\"whisper-1\", file=audio_bytes\n)\n```\n\n### Code snippets\n\n### OS\n\nwindows\n\n### Python version\n\npython v3.11.4\n\n### Library version\n\nopenai v1.75.0",
    "comments": [
      {
        "user": "issamhaimour",
        "body": "> ### Confirm this is an issue with the Python library and not an underlying OpenAI API\n> \n> - [x] This is an issue with the Python library\n> \n> ### Describe the bug\n> \n> When using `openai_client.audio.transcriptions.create)` with an in-memory file (`io.BytesIO`), the function fails unless the `.name` attribute is manually set to a valid filename with a supported extension (e.g., `audio.wav` or `audio.mp3`).\n> \n> This requirement is not documented, and the resulting error is confusing (e.g., `AttributeError`). This is especially problematic when handling file uploads in memory, such as from Telegram bots or web clients.\n> \n> ### To Reproduce\n> \n> ```Python\n> import io\n> from openai import AsyncOpenAI\n> \n> openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n> \n> # Simulating an audio file from memory (e.g., from a Telegram bot)\n> \n> audio_bytes = io.BytesIO(b\"audio binary data here\")\n> \n> # This will fail unless you set `audio_bytes.name = \"audio.wav\"\n> \n> await openai_client.audio.transcriptions.create(\n>         model=\"whisper-1\", file=audio_bytes\n> )\n> ```\n> \n> ### Code snippets\n> \n> ### OS\n> \n> windows\n> \n> ### Python version\n> \n> python v3.11.4\n> \n> ### Library version\n> \n> openai v1.75.0\n\n"
      }
    ]
  },
  {
    "issue_number": 2308,
    "title": "More intuitive Microphone helper",
    "author": "davidgilbertson",
    "state": "open",
    "created_at": "2025-04-16T05:42:42Z",
    "updated_at": "2025-04-16T08:38:40Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nI'm looking at the `Microphone` class, and it seems to favour the use case where you want to record a specific amount of audio (where you know that time in advance).\n\nI would have thought the most common use case is where a USER decides when to start and stop recording, and I can't work out how to use this class in that case.\n\nI can wrap it to turn it from async to sync, something like this:\n```py\nclass MicrophoneSync:\n    def __init__(self):\n        self.do_rec = threading.Event()\n        self.loop = asyncio.new_event_loop()\n        self.loop_thread = threading.Thread(target=self.loop.run_forever, daemon=True)\n        self.loop_thread.start()\n        self.future = None\n        self.mic = Microphone(should_record=self.should_record)\n\n    def should_record(self):\n        return self.do_rec.is_set()\n\n    def start(self):\n        self.do_rec.set()\n        self.future = asyncio.run_coroutine_threadsafe(self.mic.record(), self.loop)\n\n    def stop(self):\n        self.do_rec.clear()\n        return self.future.result()[1]\n\n\nmic = MicrophoneSync()\n\nmic.start()\ntime.sleep(2)\nwav_bytes = mic.stop()\n```\n\nBut that's almost as complex as just creating a synchronous one from scratch:\n```py\nclass MicrophoneSync:\n    def __init__(self, sample_rate=24_000):\n        self.frames = []\n        self.stream = InputStream(\n            samplerate=sample_rate,\n            channels=1,  # mono\n            dtype=\"int16\",  # 16-bit\n            callback=self._callback,\n        )\n\n    def _callback(self, indata, frames, time, status):\n        self.frames.append(indata.copy())\n\n    def start(self):\n        self.frames = []\n        self.stream.start()\n\n    def stop(self):\n        self.stream.stop()\n\n        wav_bytes = io.BytesIO()\n\n        with wave.open(wav_bytes, \"wb\") as wave_file:\n            wave_file.setframerate(self.stream.samplerate)\n            wave_file.setnchannels(self.stream.channels)\n            wave_file.setsampwidth(self.stream.samplesize)\n            wave_file.writeframes(np.concatenate(self.frames, axis=0).tobytes())\n\n        return wav_bytes\n\n\nmic = MicrophoneSync()\n\nmic.start()\ntime.sleep(2)\nwav_bytes = mic.stop()\n```\n\nSo I have two questions:\n1. Am I missing something, is there a simple way to call the `Microphone` class such that I can start and stop it in response to a user interaction?\n2. Is it worth adding a sync version (either of the above) to the package?\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "dkundel-openai",
        "body": "Hey David!\n\nYou can pass a `should_record` callable into the Microphone that gets called regularly to determine if the recording is over. I have used that before to build push-to-talk use cases. It's not synchronous but I found it to be more versatile this way.\n\nI don't have my laptop at hand to provide an example at the moment but hope that helps. "
      }
    ]
  },
  {
    "issue_number": 2279,
    "title": "Add Pydantic Support to responses.create (parity with chat.completions.parse)",
    "author": "mattharrison",
    "state": "closed",
    "created_at": "2025-04-05T16:30:44Z",
    "updated_at": "2025-04-15T12:57:57Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\n\n\n**Summary:**\n\nThe new `responses.create` API does not support native Pydantic model parsing in the same way that `chat.completions.parse` does. The `completions` API's `response_format=CalendarEvent` works as expected, but attempting the same with `responses.create` either fails or requires manual workarounds like `json.loads()` and Pydantic instantiation. This creates friction for those of us migrating to `responses`.\n\n**Expected Behavior:**\n\nJust like `chat.completions.parse`, I should be able to pass a Pydantic model as the `response_format` argument to `responses.create`, and have the API handle parsing the output into that model automatically.\n\n**Actual Behavior:**\n\nCurrently:\n- `response_format=CalendarEvent` raises an exception \n- A workaround is required, such as passing a JSON schema manually and parsing the output manually with `CalendarEvent(**json.loads(response.output_text))`.\n\n**Reproduction:**\n\n```python\n%%ipytest\nimport json\nfrom typing import List\n\nimport openai\nfrom pydantic import BaseModel\nimport pytest\n\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: List[str]\n\n# ✅ Works with completions API\n@pytest.mark.skip\ndef test_completions():\n    client = openai.OpenAI()\n\n    completion = client.beta.chat.completions.parse(\n        model=\"gpt-4o-2024-08-06\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n            {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n        ],\n        response_format=CalendarEvent,\n    )\n\n    event = completion.choices[0].message.parsed\n    assert isinstance(event, CalendarEvent)\n    assert event.name == \"Science Fair\"\n    assert event.date == \"Friday\"\n\n\n# ❌ Doesn't support response_format with Pydantic\n@pytest.mark.xfail\ndef test_responses():\n    client = openai.OpenAI()\n\n    response = client.responses.create(\n        model=\"gpt-4o-2024-08-06\",\n        input=[\n            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n            {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n        ],\n        response_format=CalendarEvent,\n    )\n\n    event = CalendarEvent(**json.loads(response.output_text))\n    assert isinstance(event, CalendarEvent)\n    assert event.name == \"Science Fair\"\n    assert event.date == \"Friday\"\n\n\n# ✅ Workaround with manual schema\ndef test_responses2():\n    client = openai.OpenAI()\n    schema = CalendarEvent.model_json_schema()\n\n    response = client.responses.create(\n        model=\"gpt-4o-2024-08-06\",\n        input=[\n            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n            {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n        ],\n        text={'format': {\n            'type': 'json_schema',\n            'name': 'calendar_event',\n            'schema': {**schema, 'additionalProperties': False},\n        }}\n    )\n\n    event = CalendarEvent(**json.loads(response.output_text))\n    assert isinstance(event, CalendarEvent)\n    assert event.name == \"Science Fair\"\n    assert event.date == \"Friday\"\n```\n\n**Suggested Improvement:**\n\nPlease add native support for `response_format=PydanticModel` to `responses.create` so that it can handle parsing just like `chat.completions.parse`. Ideally, this would:\n- Infer the JSON schema behind the scenes\n- Validate the output before returning\n- Populate `.parsed` or equivalent attribute on the result\n\n**Why It Matters:**\n\nThis improves developer ergonomics, allows for better type-safety and confidence in structured output, and supports those adopting the new `responses` API over legacy completion-style APIs.\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "futuremojo",
        "body": "I looked around and it seems you can use Pydantic models with `responses.parse` but I can't get it working with streaming.\n\n```py\nimport json\nfrom typing import List\n\nimport openai\nfrom pydantic import BaseModel\nimport pytest\n\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: List[str]\n\ndef test_response_parse_sync():\n    client = openai.OpenAI()\n\n    response = client.responses.parse(\n        model=\"gpt-4o-2024-08-06\",\n        input=[\n            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n            {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n        ],\n        text_format=CalendarEvent,\n    )\n    return response\n\nasync def test_response_parse_async():\n    client = openai.AsyncOpenAI()\n\n    stream = await client.responses.parse(\n        model=\"gpt-4o-2024-08-06\",\n        input=[\n            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n            {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n        ],\n        text_format=CalendarEvent,\n        stream=True,\n    )\n    async for event in stream:\n        print(event)\n\n\n# Returns a response conforming to the Pydantic model.\nprint(test_response_parse_sync())\n\n# Errors out.\nawait test_response_parse_async()\n```\n\nThis is the error I get when I call `test_response_parse_async()`:\n\n```\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nFile ~/projects/pdf_summarizer/.venv/lib/python3.12/site-packages/openai/lib/_parsing/_responses.py:62, in parse_response(text_format, input_tools, response)\n     59 solved_t = solve_response_format_t(text_format)\n     60 output_list: List[ParsedResponseOutputItem[TextFormatT]] = []\n---> 62 for output in response.output:\n     63     if output.type == \"message\":\n     64         content_list: List[ParsedContent[TextFormatT]] = []\n\nAttributeError: 'str' object has no attribute 'output'\n```"
      },
      {
        "user": "RobertCraigie",
        "body": "@mattharrison you should be able to use `client.responses.parse()` instead, was that not working for you?\n\n@futuremojo is that not working against the live API? If so please open a new issue."
      },
      {
        "user": "mattharrison",
        "body": "Thanks for the pointers. Might want to consider updating the docs so they reflect this knowledge. https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses "
      }
    ]
  },
  {
    "issue_number": 1361,
    "title": "Memory Leak in chat completion create",
    "author": "rdy5644",
    "state": "closed",
    "created_at": "2024-04-23T23:21:08Z",
    "updated_at": "2025-04-15T04:11:20Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nAfter Making many streaming calls from a flask app the process memory is infinitely increasing and it is never getting reduced.\r\nEven after performing close on response and client.\r\n\r\n### To Reproduce\r\n\r\n1. Wrap the below code in flask api or you can run as a long running process as well.\r\n2. Invoke this method in a loop or for 1000 you will be able to see the process gradually takes up more memory.\r\n3. Even performing manual gc didn't help.\r\n\r\n\r\n### Code snippets\r\n\r\n```Python\r\ndef make_stream_call_with_close():\r\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\r\n    responses = client.chat.completions.create(\r\n        model='gpt-3.5-turbo',\r\n        messages=[{\"role\": \"user\", \"content\": \"Say exactly one word.\"}],\r\n        stream=True,\r\n    )\r\n    responses.response.close()\r\n    print(responses.response.is_closed)\r\n    client.close()\r\n    print(client.is_closed())\r\n```\r\n\r\n\r\n### OS\r\n\r\nmacOS,Linux\r\n\r\n### Python version\r\n\r\n3.10.13\r\n\r\n### Library version\r\n\r\nopenai==1.3.4 , httpx==0.27.0 , pydantic==2.7.1, pydantic_core==2.18.2",
    "comments": [
      {
        "user": "rdy5644",
        "body": "tracemalloc top 10 after performing few requests\r\n\r\nSnapshot comparison before invoke and after invoking once 1.\r\n`\r\n00:00:00 Top 10 differences:\r\n/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_response.py:238: size=61.4 KiB (+61.4 KiB), count=525 (+525), average=120 B/usr/local/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/abc.py:123: size=54.1 KiB (+54.1 KiB), count=614 (+614), average=90 B<frozen importlib._bootstrap_external>:672: size=94.6 KiB (+51.6 KiB), count=1546 (+570), average=63 B<frozen importlib._bootstrap>:241: size=30.6 KiB (+30.6 KiB), count=316 (+316), average=99 B/usr/local/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/functools.py:52: size=21.4 KiB (+21.4 KiB), count=106 (+106), average=207 B/usr/local/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/functools.py:58: size=15.8 KiB (+15.8 KiB), count=239 (+239), average=68 B/usr/local/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/typing.py:694: size=14.8 KiB (+14.8 KiB), count=32 (+32), average=475 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_resource.py:22: size=10184 B (+10184 B), count=70 (+70), average=145 B/usr/local/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/typing.py:989: size=4464 B (+4264 B), count=62 (+59), average=72 B/usr/local/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/cookiejar.py:964: size=2720 B (+2720 B), count=6 (+6), average=453 B\r\n`\r\n\r\nSnapshot compare before invoking and after invoking for 8-10 times in loop\r\n`\r\n00:00:00 Top 10 differences:\r\n/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_response.py:238: size=899 KiB (+899 KiB), count=7731 (+7731), average=119 B/usr/local/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/functools.py:58: size=190 KiB (+190 KiB), count=2815 (+2815), average=69 B/usr/local/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/tracemalloc.py:115: size=126 KiB (+126 KiB), count=1615 (+1615), average=80 B/usr/local/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/abc.py:123: size=52.7 KiB (+52.7 KiB), count=591 (+591), average=91 B<frozen importlib._bootstrap_external>:672: size=94.4 KiB (+51.4 KiB), count=1545 (+569), average=63 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_resource.py:17: size=50.0 KiB (+50.0 KiB), count=696 (+696), average=73 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_resource.py:22: size=30.9 KiB (+30.9 KiB), count=406 (+406), average=78 B<frozen importlib._bootstrap>:241: size=30.6 KiB (+30.6 KiB), count=315 (+315), average=99 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_resource.py:23: size=22.5 KiB (+22.5 KiB), count=360 (+360), average=64 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_resource.py:21: size=22.5 KiB (+22.5 KiB), count=360 (+360), average=64 B\r\n`\r\n\r\nSnapshot before invoking and after invoking for 10-15 times\r\n\r\n`\r\n/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_response.py:238: size=1112 KiB (+216 KiB), count=9255 (+1541), average=123 B/usr/local/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/functools.py:58: size=239 KiB (+49.7 KiB), count=3550 (+736), average=69 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_resource.py:17: size=63.8 KiB (+13.8 KiB), count=888 (+192), average=74 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_resource.py:22: size=36.9 KiB (+6144 B), count=502 (+96), average=75 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_resource.py:23: size=28.5 KiB (+6144 B), count=456 (+96), average=64 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_resource.py:21: size=28.5 KiB (+6144 B), count=456 (+96), average=64 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_resource.py:20: size=28.5 KiB (+6144 B), count=456 (+96), average=64 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_resource.py:19: size=28.5 KiB (+6144 B), count=456 (+96), average=64 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/openai/_resource.py:18: size=28.5 KiB (+6144 B), count=456 (+96), average=64 B/Users/jogireddy/PycharmProjects/flaskProject/.venv/lib/python3.10/site-packages/pydantic/_internal/_core_utils.py:200: size=3952 B (+3784 B), count=21 (+20), average=188 B\r\n`\r\n\r\nThe Process memory usage went from 80M to 326M after invoking this method for more then 200 times."
      },
      {
        "user": "rdy5644",
        "body": "Found this function at `openai/_response.py:238`\r\n\r\n`\r\n@functools.wraps(func)\r\ndef wrapped(*args: P.args, **kwargs: P.kwargs) -> APIResponse[R]:\r\n    extra_headers = {**(cast(Any, kwargs.get(\"extra_headers\")) or {})}\r\n    extra_headers[RAW_RESPONSE_HEADER] = \"true\"\r\n    kwargs[\"extra_headers\"] = extra_headers\r\n    return cast(APIResponse[R], func(*args, **kwargs))\r\nreturn wrapped\r\n`"
      },
      {
        "user": "rattrayalex",
        "body": "Thanks, we'll look into this!"
      }
    ]
  },
  {
    "issue_number": 2298,
    "title": "Responses API with File Search Tool is returning 500s",
    "author": "moonbox3",
    "state": "closed",
    "created_at": "2025-04-10T04:24:51Z",
    "updated_at": "2025-04-14T10:59:56Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWe have integration tests for our SDK, and the File Search + Responses API tests are failing with 500s: \n\nopenai.InternalServerError: Error code: 500 - {'error': {'message': 'An error occurred while processing your request. You can retry your request, or contact us through our help center at help.openai.com if the error persists. Please include the request ID req_50ede3d35bbb95fe9fd478519fffead7 in your message.', 'type': 'server_error', 'param': None, 'code': 'server_error'}}\n\n```python\nself = <openai.AsyncOpenAI object at 0x13b7f0a90>\ncast_to = <class 'openai.types.responses.response.Response'>\noptions = FinalRequestOptions(method='post', url='/responses', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=NOT_... {}, 'tools': [{'type': 'file_search', 'vector_store_ids': ['vs_67f743a9a078819183e0a863b3e9b70c']}]}, extra_json=None)\n\n    async def _request(\n        self,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        *,\n        stream: bool,\n        stream_cls: type[_AsyncStreamT] | None,\n        retries_taken: int,\n    ) -> ResponseT | _AsyncStreamT:\n        if self._platform is None:\n            # `get_platform` can make blocking IO calls so we\n            # execute it earlier while we are in an async context\n            self._platform = await asyncify(get_platform)()\n    \n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = await self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        await self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        try:\n            response = await self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return await self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return await self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Request: %s %s \"%i %s\"', request.method, request.url, response.status_code, response.reason_phrase\n        )\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                await err.response.aclose()\n                return await self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                await err.response.aread()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 500 - {'error': {'message': 'An error occurred while processing your request. You can retry your request, or contact us through our help center at help.openai.com if the error persists. Please include the request ID req_50ede3d35bbb95fe9fd478519fffead7 in your message.', 'type': 'server_error', 'param': None, 'code': 'server_error'}}\n\n.venv/lib/python3.11/site-packages/openai/_base_client.py:1562: InternalServerError\n```\n\n\n### To Reproduce\n\nConfigure the file search tool, upload a file to the vector store, and provide the id as part of the  and invoke a response using (your code doesn't have to be exactly the same):\n\n```python\nresponse: Response = await agent.client.responses.create(\n    input=cls._prepare_chat_history_for_request(chat_history),\n    instructions=merged_instructions or agent.instructions,\n    tools=tools,  # type: ignore\n    stream=stream,\n    model=\"gpt-4o\",\n)\n```\n\ntools = `{'type': 'file_search', 'vector_store_ids': ['vs_67...']}\n\nWe're using `openai==1.72.0`. \n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nMacOS Sequoia 15.4\n\n### Python version\n\nPython v3.11.11\n\n### Library version\n\nopenai v1.72.0",
    "comments": []
  },
  {
    "issue_number": 2302,
    "title": "TypeError: AsyncCompletions.create() got an unexpected keyword argument 'continue_final_message'",
    "author": "TC10127",
    "state": "closed",
    "created_at": "2025-04-13T05:24:25Z",
    "updated_at": "2025-04-14T03:10:42Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nDoes AsyncOpenAI still not support the continuation function? When passing continue_final_message and add_generation_prompt as parameters, it shows TypeError: AsyncCompletions.create() got an unexpected keyword argument 'continue_final_message'\n\n### To Reproduce\n\nDoes AsyncOpenAI still not support the continuation function? When passing continue_final_message and add_generation_prompt as parameters, it shows TypeError: AsyncCompletions.create() got an unexpected keyword argument 'continue_final_message'\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nlinux\n\n### Python version\n\nPython 3.10.14\n\n### Library version\n\nopenai-1.71.0",
    "comments": []
  },
  {
    "issue_number": 2231,
    "title": "ImportError: cannot import name 'ComputerCallOutput' from 'openai.types.responses'",
    "author": "guunergooner",
    "state": "closed",
    "created_at": "2025-03-20T12:52:27Z",
    "updated_at": "2025-04-13T02:28:41Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhile using the openai-python SDK, I attempted to import the ComputerCallOutput data structure from openai.types.responses, but encountered the following error:\n\n```shell\nTraceback (most recent call last):\n  File \"/Users/gooner/Workbench/repo/github.com/browser-use/browser-use/apiserver.py\", line 47, in <module>\n    from openai.types.responses import ComputerCallOutput\nImportError: cannot import name 'ComputerCallOutput' from 'openai.types.responses' (/Users/gooner/Workbench/repo/github.com/browser-use/browser-use/.venv/lib/python3.11/site-packages/openai/types/responses/__init__.py)\n```\nIt seems that ComputerCallOutput is not available in the openai.types.responses module, possibly due to a mismatch between the documentation and the codebase, or the data structure not being properly exposed.\n\n### To Reproduce\n\n* Steps to Reproduce \n  - Install the openai-python SDK (e.g., via pip install openai).\n  - Create a Python script with the following code:\n  ```shell\n    from openai.types.responses import ComputerCallOutput\n  ```\n  - Run the script and observe the error.\n\n* Environment\nOperating System: macOS\nPython Version: 3.11\nOpenAI SDK Version: 1.67.0\n\n### Code snippets\n\n```Python\nfrom openai.types.responses import ComputerCallOutput\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.11.11\n\n### Library version\n\nopenai 1.67.0",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "body": "Hi,\n\nI've noticed that there are two instances of `ComputerCallOutput` in our codebase—one in `src/openai/types/responses/response_input_item_param.py` and the other in `src/openai/types/responses/response_input_param.py`. Since both definitions are identical, I've consolidated them by importing the one from `response_input_param.py` in our `__init__.py` file as follows:\n\n```python\nfrom .response_input_param import ComputerCallOutput as ComputerCallOutput\n```\n\nI've created a PR with this change. (#2248)\n\nCould you also clarify which documentation you were referring to? For now, you can import `ComputerCallOutput` directly using:\n\n```python\nfrom openai.types.responses.response_input_param import ComputerCallOutput\n```\n\nThanks for your feedback!"
      },
      {
        "user": "guunergooner",
        "body": "I did not report any errors in this way \n```shell\nfrom openai.types.responses.response_input_param import ComputerCallOutput\n```\nThanks"
      }
    ]
  },
  {
    "issue_number": 1966,
    "title": "FileSearchToolCall.file_search has empty results",
    "author": "dominpm",
    "state": "open",
    "created_at": "2024-12-19T16:01:02Z",
    "updated_at": "2025-04-11T23:22:42Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [ ] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nContinuing from issue : #1938\r\n\r\nThe error seems to be fixed with `openai==1.58.1` (it does not return a 400 error anymore). However if we capture the output of the stream with a custom class inheriting from `AssistantEventHandler` the results of the fileSearch tool are not available:\r\n\r\n```python\r\n@override\r\ndef on_tool_call_done(self, tool_call: ToolCall):\r\n    print(tool_call)\r\n```\r\n\r\nOf which the results are:\r\n\r\n\r\n> FileSearchToolCall(id='call_ID', file_search=FileSearch(ranking_options=FileSearchRankingOptions(ranker='default_2024_08_21', score_threshold=0.0), results=[]), type='file_search', index=0)\r\n\r\n\r\nwhere following from `openai.types.beta.threads.runs.file_search_tool_call.py` it supposed to show:\r\n\r\n```python\r\nclass FileSearch(BaseModel):\r\n    ranking_options: Optional[FileSearchRankingOptions] = None\r\n    \"\"\"The ranking options for the file search.\"\"\"\r\n\r\n    results: Optional[List[FileSearchResult]] = None\r\n    \"\"\"The results of the file search.\"\"\"\r\n```\r\n\r\nwhen creating the `run` as follows:\r\n\r\n```python\r\n  with client.beta.threads.runs.stream(\r\n      thread_id=thread.id,\r\n      assistant_id=ass_id,\r\n      event_handler=CustomEventHandler(),\r\n      include=[\"step_details.tool_calls[*].file_search.results[*].content\"]\r\n      ) as stream:\r\n      # Wait for the stream to complete\r\n      stream.until_done()\r\n```\r\n\r\n### To Reproduce\r\n\r\n1. Run this with the id of an assistant connected to a vector store and the file search enabled (for simplicity do such thing through platform.openai.com)\r\n\r\n```python\r\nfrom typing import override\r\nfrom openai import AssistantEventHandler\r\nfrom openai import OpenAI\r\nfrom openai.types.beta.threads.runs.tool_call import ToolCall\r\n\r\nclient = OpenAI()\r\nmessages = [\r\n    {\r\n        \"content\": <QUESTION_TO_THE_ASSISTANT>,\r\n    }\r\n]\r\n\r\n# Create a new thread for the assistant\r\nthread = client.beta.threads.create()\r\nclient.beta.threads.messages.create(\r\n    thread_id=thread.id,\r\n    role=\"user\",\r\n    content=messages[-1][\"content\"]\r\n)\r\n\r\n\r\nclass CustomEventHandler(AssistantEventHandler):\r\n    @override\r\n    def on_tool_call_done(self, tool_call: ToolCall):\r\n        print(tool_call)\r\n\r\n# Stream the assistant's response\r\nwith client.beta.threads.runs.stream(\r\n    thread_id=thread.id,\r\n    assistant_id=<ASSISTANT_ID>,\r\n    event_handler=CustomEventHandler(),\r\n    include=[\"step_details.tool_calls[*].file_search.results[*].content\"]\r\n    ) as stream:\r\n    # Wait for the stream to complete\r\n    stream.until_done()\r\n```\r\n\r\n### Code snippets\r\n\r\n_No response_\r\n\r\n### OS\r\n\r\nWindows\r\n\r\n### Python version\r\n\r\nPython 3.11.10\r\n\r\n### Library version\r\n\r\nopenai 1.58.1",
    "comments": [
      {
        "user": "ghost",
        "body": "Alright, let's dig into this. So, the 400 error from issue #1938 is gone in openai==1.58.1, but now the FileSearch results are empty when using a custom AssistantEventHandler. That's a sneaky bug.\r\n\r\nHere's the breakdown and how we can tackle this:\r\n\r\nUnderstanding the Issue\r\n\r\nThe on_tool_call_done method in your CustomEventHandler is supposed to receive the results of the fileSearch tool call.\r\nHowever, the results list in the FileSearch object is empty, even though you've explicitly included step_details.tool_calls[*].file_search.results[*].content in the include parameter of the stream method.\r\nThis suggests that either the results are not being populated correctly or there's an issue with how the include parameter is being handled in the stream method.\r\nPossible Causes\r\n\r\nBug in openai==1.58.1: There might be a bug in the library that prevents the FileSearch results from being populated when using a custom event handler.\r\nIncorrect usage of include parameter: The include parameter might not be working as expected, or there might be a different way to include the FileSearch results when using a custom event handler.\r\nIssue with the Assistant or Vector Store: There might be a configuration issue with the Assistant or the connected vector store that prevents the fileSearch from returning results.\r\nDebugging Steps\r\n\r\nVerify Assistant and Vector Store: Double-check that the Assistant is correctly configured to use the fileSearch tool and that the vector store is properly connected and populated with data.\r\n\r\nTest with the Default Event Handler: Try running the code with the default AssistantEventHandler (or without specifying an event handler) to see if the FileSearch results are populated correctly in that case. This will help isolate whether the issue is specifically with the custom event handler.\r\n\r\nInspect the Raw Response: If possible, capture the raw HTTP response from the stream method and examine its contents. This might reveal clues about why the FileSearch results are missing or if there are any error messages in the response.\r\n\r\nSimplify the Code: Try removing the include parameter or simplifying the custom event handler to see if that affects the results. This can help pinpoint whether the issue is related to the include parameter or the custom event handler's logic.\r\n\r\nCheck for Updates: Ensure you're using the latest version of the openai library. If a newer version is available, try upgrading to see if it resolves the issue.\r\n\r\nReport to OpenAI: If you're unable to identify the cause of the issue, report it to OpenAI with a detailed description, code snippet, and steps to reproduce. They might be able to provide insights or identify a bug in the library.\r\n\r\nBy systematically investigating these points, we should be able to pinpoint the cause of the missing FileSearch results and get this functionality working as expected.\r\n\r\nFirst, they're using an older version of the openai library (1.58.1).  Might be worth bumping that up to the latest, see if it makes a difference.  Sometimes those sneaky bugs get squashed in newer releases.\r\n\r\nSecond, that include parameter... it's a bit verbose.  Maybe there's a simpler way to specify those FileSearch results?  Worth checking the docs, see if there's a more concise syntax.\r\n\r\nAnd lastly, this whole AssistantEventHandler thing... it's a bit of a black box.  We don't know exactly how it's interacting with the stream or processing the results.  Might be worth digging into the source code, see if there are any clues there.\r\n\r\nOverall, feels like a classic case of \"it's not you, it's me\" (or rather, it's the library).  But with a bit of digging and some creative debugging, we should be able to crack this nut."
      },
      {
        "user": "dominpm",
        "body": "It seems it has been chosen not to retrieve (or show) the results as we can see from the comments on this class:\r\n\r\n```python\r\nclass FileSearchToolCall(BaseModel):\r\n    id: str\r\n    \"\"\"The ID of the tool call object.\"\"\"\r\n\r\n    file_search: FileSearch\r\n    \"\"\"For now, this is always going to be an empty object.\"\"\"\r\n\r\n    type: Literal[\"file_search\"]\r\n    \"\"\"The type of tool call.\r\n\r\n    This is always going to be `file_search` for this type of tool call.\r\n    \"\"\"\r\n```"
      },
      {
        "user": "hayescode",
        "body": "I'm getting this error today on the latest API version. I'm guessing Assistants API has been abandoned in favor of Responses API which is working.\n\n```py\nresponse = await llm.responses.create(\n    model=\"gpt-4o-mini\",\n    input=\"What is prompt engineering?\",\n    tools=[{\n        \"type\": \"file_search\",\n        \"vector_store_ids\": [\"vs_J...\"],\n        \"max_num_results\": 2,\n    }],\n    include=[\"file_search_call.results\"]\n)\nprint(response.model_dump_json(indent=2))\n```"
      }
    ]
  },
  {
    "issue_number": 1591,
    "title": "I can no longer upload files to vector store with AzureOpenAI",
    "author": "matteopulega",
    "state": "open",
    "created_at": "2024-08-02T07:27:48Z",
    "updated_at": "2025-04-11T04:29:43Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nHi,\r\n\r\nFrom 2 days till now i'm getting error when I try to upload files in vector stores using AzureOpenAI package.\r\n**The same code works with OpenAI**.\r\n\r\nI changed nothing in my code but from  31/07/2024 it doesn't work with AzureOpenAI.\r\n\r\nThe output of file_batch:\r\n```\r\nFile batch: FileCounts(cancelled=0, completed=0, failed=1, in_progress=0, total=1)\r\nFile batch status: failed\r\n\r\nFile status: failed\r\nFile last error: LastError(code='server_error', message='An internal error occurred.')\r\n```\r\n\r\nAre there some problems with AzureOpenAI ?\r\n\r\nThanks,\r\nMatteo\n\n### To Reproduce\n\nUse a simple file.txt or other types.\r\n\r\nExecute the code and see the result.\n\n### Code snippets\n\n```Python\nfrom openai import AzureOpenAI\r\nclient = AzureOpenAI(\r\n      api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \r\n      api_version=\"2024-05-01-preview\",\r\n      azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\r\n      )\r\nfile_stream = open(\"path/of/my/simple/file.txt\", \"rb\")\r\n\r\nvector_store = client.beta.vector_stores.create(name=\"vs_test_assistant_v2\")\r\nvector_store_id = vector_store.id\r\n\r\nprint(\"Uploading file to vector store..\")\r\nfile_batch = client.beta.vector_stores.file_batches.upload_and_poll(\r\n          vector_store_id=vector_store_id, \r\n          files=[file_stream],\r\n          )\r\n\r\nprint(f\"File batch status: {file_batch.status}\")\r\nprint(f\"File batch: {file_batch.file_counts}\")\r\nfile = client.beta.vector_stores.files.list(vector_store_id).data[0]\r\nprint(f\"File status: {file.status}\")\r\nif file.status == \"failed\":\r\n    print(f\"File last error: {file.last_error}\")\n```\n\n\n### OS\n\nLinux\n\n### Python version\n\nPython v3.10.12\n\n### Library version\n\nopenai v1.37.2",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "cc @kristapratico "
      },
      {
        "user": "kristapratico",
        "body": "@matteopulega I'm not able to reproduce the error. Can you share which region your Azure OpenAI resource is in?\r\n\r\nIf this is still failing today, I recommend opening a support [ticket](https://azure.microsoft.com/support/create-ticket) against the service."
      },
      {
        "user": "matteopulega",
        "body": "\r\nThe region Is swedencentral."
      }
    ]
  },
  {
    "issue_number": 2299,
    "title": "'method' parameter for finetuning jobs creation not available for Azure OpenAI client",
    "author": "YacineYakoubi",
    "state": "closed",
    "created_at": "2025-04-10T11:13:48Z",
    "updated_at": "2025-04-10T14:01:03Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nTypeError                                 Traceback (most recent call last)\nCell In[10], [line 1](vscode-notebook-cell:?execution_count=10&line=1)\n----> [1](vscode-notebook-cell:?execution_count=10&line=1) job = client.fine_tuning.jobs.create(\n      [2](vscode-notebook-cell:?execution_count=10&line=2)     training_file=training_file_id,\n      [3](vscode-notebook-cell:?execution_count=10&line=3)     model=\"gpt-4o-2024-08-06\",\n      [4](vscode-notebook-cell:?execution_count=10&line=4)     method={\n      [5](vscode-notebook-cell:?execution_count=10&line=5)         \"type\": \"dpo\",\n      [6](vscode-notebook-cell:?execution_count=10&line=6)         \"dpo\": {\n      [7](vscode-notebook-cell:?execution_count=10&line=7)             \"hyperparameters\": {\"beta\": 0.1},\n      [8](vscode-notebook-cell:?execution_count=10&line=8)         },\n      [9](vscode-notebook-cell:?execution_count=10&line=9)     },\n     [10](vscode-notebook-cell:?execution_count=10&line=10) )\n\nTypeError: Jobs.create() got an unexpected keyword argument 'method'\n\n### To Reproduce\n\nTypeError                                 Traceback (most recent call last)\nCell In[10], [line 1](vscode-notebook-cell:?execution_count=10&line=1)\n----> [1](vscode-notebook-cell:?execution_count=10&line=1) job = client.fine_tuning.jobs.create(\n      [2](vscode-notebook-cell:?execution_count=10&line=2)     training_file=training_file_id,\n      [3](vscode-notebook-cell:?execution_count=10&line=3)     model=\"gpt-4o-2024-08-06\",\n      [4](vscode-notebook-cell:?execution_count=10&line=4)     method={\n      [5](vscode-notebook-cell:?execution_count=10&line=5)         \"type\": \"dpo\",\n      [6](vscode-notebook-cell:?execution_count=10&line=6)         \"dpo\": {\n      [7](vscode-notebook-cell:?execution_count=10&line=7)             \"hyperparameters\": {\"beta\": 0.1},\n      [8](vscode-notebook-cell:?execution_count=10&line=8)         },\n      [9](vscode-notebook-cell:?execution_count=10&line=9)     },\n     [10](vscode-notebook-cell:?execution_count=10&line=10) )\n\nTypeError: Jobs.create() got an unexpected keyword argument 'method'\n\n### Code snippets\n\n```Python\njob = client.fine_tuning.jobs.create(\n    training_file=training_file_id,\n    model=\"gpt-4o-2024-08-06\",\n    method={\n        \"type\": \"dpo\",\n        \"dpo\": {\n            \"hyperparameters\": {\"beta\": 0.1},\n        },\n    },\n)\n```\n\n### OS\n\nLinux\n\n### Python version\n\npython 3.10.15\n\n### Library version\n\nopenai 1.72.0",
    "comments": []
  },
  {
    "issue_number": 1308,
    "title": "Azure API Manager with Azure OpenAI",
    "author": "ltivmitre",
    "state": "closed",
    "created_at": "2024-04-10T13:48:15Z",
    "updated_at": "2025-04-10T08:20:29Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\r\n\r\n- [X] This is a feature request for the Python library\r\n\r\n### Describe the feature or improvement you're requesting\r\n\r\nWe proxy the Azure OpenAI service using API Manager. The system design is from Microsoft found here: https://github.com/microsoft/AzureOpenAI-with-APIM.\r\n\r\nThe request would be for a way to override/set a custom endpoint url along with api key name. (ex. below the key name is Ocp-Apim-Subscription-Key)\r\n\r\nexample code\r\n#!/bin/bash\r\napimUrl=\"THE_HTTPS_URL_OF_YOUR_APIM_INSTANCE\"\r\nmodelName=\"GPT-3_5-Turbo\" # Probaby what you named your model, but change if necessary\r\napiVersion=\"2023-03-15-preview\" # Do not change this value, unless you are testing a different API version\r\nsubscriptionKey=\"YOUR_APIM_SUBSCRIPTION_KEY\"\r\n\r\nurl=\"${apimUrl}\"/deployments/\"${modelName}\"/chat/completions?api-version=\"${apiVersion}\"\r\nkey=\"Ocp-Apim-Subscription-Key: ${subscriptionKey}\"\r\n\r\ncurl $url -k -H \"Content-Type: application/json\" -H $key -d '{\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"system\",\r\n            \"content\": \"You are an AI assistant that helps people find information.\"\r\n        },\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": \"What are the differences between Azure Machine Learning and Azure AI services?\"\r\n        }\r\n    ]\r\n}' \r\n\r\n\r\n\r\n### Additional context\r\n\r\n_No response_",
    "comments": []
  },
  {
    "issue_number": 2297,
    "title": "Async example in streaming documentation is missing await and async for",
    "author": "Mikaeldsouza",
    "state": "open",
    "created_at": "2025-04-10T01:06:20Z",
    "updated_at": "2025-04-10T01:06:20Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nHi,\n\nI noticed a small but important issue in the [streaming responses documentation](https://github.com/openai/openai-python?tab=readme-ov-file#streaming-responses), specifically in the async example.\n\nIn the async version of the streaming code, the following two issues appear:\n\n1. The call to client.responses.create(...) is missing an await.\n2. The loop over the stream uses for instead of async for.\n\n**Current code:**\n`async def main():\n    stream = client.responses.create(...)  # Missing 'await'\n    for event in stream:                   # Should be 'async for'\n        print(event)`\n\n**Suggested fix:**\nasync def main():\n    stream = await client.responses.create(...)\n    async for event in stream:\n        print(event)\n\n### To Reproduce\n\n1. Copy the async streaming example from the documentation: [streaming responses documentation](https://github.com/openai/openai-python?tab=readme-ov-file#streaming-responses)\n2. Run the script using Python 3.8+ with the latest openai package installed.\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nany\n\n### Python version\n\nPython 3.8+\n\n### Library version\n\nv1.72.0",
    "comments": []
  },
  {
    "issue_number": 2294,
    "title": "Response API not working with GPT-3.5 model",
    "author": "prashants975",
    "state": "closed",
    "created_at": "2025-04-09T15:10:37Z",
    "updated_at": "2025-04-09T15:11:08Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API.\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe Response API is not working properly with the GPT-3.5 model (gpt-3.5-turbo). When trying to use the Response API with this model, the functionality is not working as expected.\n\n### To Reproduce\n\n1. Set up the OpenAI client\n2. Try to use Response API with gpt-3.5-turbo model\n3. The response functionality does not work as expected\n\n### Expected behavior\n\nThe Response API should work seamlessly with the GPT-3.5 model, similar to how it works with other models.\n\n### Code snippets\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### Environment\n- Python version: 3.x\n- Library version: latest\n- OS: Windows\n\n### Additional context\n\nPlease provide more information about:\n1. Are there any specific error messages?\n2. What exactly happens when you try to use the Response API?\n3. Are there any workarounds currently available?",
    "comments": []
  },
  {
    "issue_number": 2293,
    "title": "Request for Expanded Persistent Memory for Committed ChatGPT Plus Users",
    "author": "Narciss666",
    "state": "closed",
    "created_at": "2025-04-09T04:08:09Z",
    "updated_at": "2025-04-09T08:36:59Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nNote: Although this form requires selecting a Python library-related feature request, this issue concerns the user experience and persistent memory limitations in ChatGPT itself. Please forward it internally if it falls outside the scope of this repository.\n\n\nSubject: Insufficient user memory & strategic inconsistency regarding paid user treatment\n\nStatus: Paying subscriber of ChatGPT Plus\n\n1. User Position:\nI’m a paying ChatGPT user who invests significant time and cognitive effort in exploring the conversational and relational capabilities of your models. My use cases include writing projects, long-term reflective inquiry, and personal simulations. This is not casual or superficial usage. It’s sustained and intellectually committed.\n\n2. Observation: Current user memory is absurdly limited\nThe so-called “persistent memory” currently granted to users is no more than a few kilobytes. In technical terms, it’s likely less than the size of a 1990s floppy disk. This memory capacity is objectively insignificant relative to OpenAI’s infrastructure capabilities.\n\n3. Strategic inconsistency: Real capacity vs. imposed limitations\nIt is known that OpenAI possesses the infrastructure to store, structure, and operate on much larger user contexts. Government and enterprise-customized GPT versions already operate with wide and adaptive memory. This proves that OpenAI is capable of handling much more — just not for the public.\n\n4. Unfair treatment of engaged paying users\nA serious user who expects consistency, long-term memory, and conversational depth is paradoxically the one most affected by this cap.\nWe are stuck with less memory than a basic note-taking app.\nThis creates a dissonance: the more seriously I use the system, the more its limitations become visible — and unjustified.\n\n5. Security concerns already addressed\nI understand the ethical and technical concerns around persistent memory. But your models already feature multiple protective layers:\n\nContent filters\n\nNo autonomous behavior\n\nManual confirmation for critical actions\n\nMemory editing and deletion\n\nThese are more than enough to support a persistent memory of 1MB, 10MB, or even 1GB, without posing additional risk to OpenAI or users.\n\n6. Specific Request: A meaningful engagement threshold\nI’m not asking for unlimited memory.\nI’m asking for a memory volume proportionate to my usage and commitment as a paying subscriber.\nEven an experimental or opt-in extended memory tier would reflect that commitment and build lasting trust.\n\n7. Conclusion\nThis is a proportionate, well-argued request.\nIt’s not about demanding features — it’s about pointing to a structural inconsistency:\nThe world’s most advanced conversational AI offers its paying users less persistent memory than a 1994 text editor.\n\nI am open to discussion if this report reaches a human reader.\n\n\nRespectfully,\nA committed ChatGPT Plus user (Phil)\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 2292,
    "title": "Response API not working with gpt-3.5 model",
    "author": "prashants975",
    "state": "closed",
    "created_at": "2025-04-09T00:51:14Z",
    "updated_at": "2025-04-09T00:51:50Z",
    "labels": [],
    "body": "### Description\nI'm experiencing an issue with the Response API when using the gpt-3.5 model. The API is not responding as expected and fails to process requests properly when specifically using this model.\n\n### Steps to reproduce\n1. Set up a request using the Response API\n2. Specify the gpt-3.5 model in the configuration\n3. Send the request\n4. Observe that the response fails or returns unexpected results\n\n### Expected behavior\nThe Response API should work properly with the gpt-3.5 model, just as it does with other models.\n\n### Current behavior\nWhen using the gpt-3.5 model with the Response API, the requests fail or return unexpected results.\n\n### Environment\n- Python version: 3.10\n- openai-python library version: latest\n- Platform: Windows\n\n### Additional context\nThis issue only happens with the gpt-3.5 model. Other models seem to work fine with the Response API.\n\n### Confirm this is a bug in the Python library and not the underlying OpenAI API\n- [x] I've confirmed this is an issue with the Python library implementation",
    "comments": []
  },
  {
    "issue_number": 2275,
    "title": "allow websockets 15+ to support proxy",
    "author": "DeoLeung",
    "state": "closed",
    "created_at": "2025-04-03T01:41:48Z",
    "updated_at": "2025-04-07T13:29:30Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nfor realtime supporting websockets 15+ so we can use proxy easily\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "This will be fixed in the next release! https://github.com/openai/openai-python/pull/2271"
      }
    ]
  },
  {
    "issue_number": 2278,
    "title": "Default values for Usage metrics",
    "author": "basitanees",
    "state": "open",
    "created_at": "2025-04-05T15:37:10Z",
    "updated_at": "2025-04-07T10:23:00Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nCould we set the default value for Usage metrics as an integer (0)? This prevents the parser from setting it to None which prevents errors in some other open source libraries.\n\nhttps://github.com/openai/openai-python/blob/384e6b23ce0366d6b2f31cc98d35525da5b22c10/src/openai/types/create_embedding_response.py#L12\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 1698,
    "title": "Add Structured Outputs support to Assistants stream() and create_and_poll() Functions",
    "author": "sciencetor2",
    "state": "open",
    "created_at": "2024-09-09T14:56:06Z",
    "updated_at": "2025-04-06T17:44:33Z",
    "labels": [
      "enhancement"
    ],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\r\n\r\n- [X] This is a feature request for the Python library\r\n\r\n### Describe the feature or improvement you're requesting\r\n\r\nCurrently the client.beta.threads.runs.create_and_poll() function and client.beta.threads.runs.stream() function do not accept a pydantic model as their \"response_format\". currently they only accept the old {\"type\": \"json_object\"} value.\r\n\r\n### Additional context\r\n\r\n```Python\r\nclass Meal(BaseModel):\r\n    meal: str\r\n    slug: str\r\n    recipe_id: str\r\n    calories_per_serving: int\r\n    protein_per_serving: int\r\n    fat_per_serving: int\r\n    carbs_per_serving: int\r\n    servings: int\r\n\r\nclass Meals(BaseModel):\r\n    breakfast: Optional[Meal]\r\n    lunch: Optional[Meal]\r\n    dinner: Optional[Meal]\r\n\r\nclass DayLog(BaseModel):\r\n    date: str  # You can change this to 'date' type if needed\r\n    total_calories: int\r\n    total_carbs: int\r\n    total_fat: int\r\n    total_protein: int\r\n    meals: Meals\r\n\r\nclass WeekLog(BaseModel):\r\n    Monday: DayLog\r\n    Tuesday: DayLog\r\n    Wednesday: DayLog\r\n    Thursday: DayLog\r\n    Friday: DayLog\r\n    Saturday: DayLog\r\n    Sunday: DayLog\r\n\r\ncompletion = client.beta.chat.completions.parse(\r\n        model=\"gpt-4o-2024-08-06\",\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": \"my prompt for structured data\"\r\n\r\n\r\n             },\r\n        ],\r\n        response_format=WeekLog,\r\n    )\r\n```\r\n\r\nCurrently the above works without issue, but the below throws a TypeError:\r\n\r\n```Python\r\nassistant = client.beta.assistants.create(\r\n        name=\"Meal Planner Nutritionist\",\r\n        instructions=\"some instructions\",\r\n        tools=[{\"type\": \"code_interpreter\"}],\r\n        model=\"gpt-4o-2024-08-06\",\r\n    )\r\n    thread = client.beta.threads.create()\r\n    message = client.beta.threads.messages.create(\r\n        thread_id=thread.id,\r\n        role=\"user\",\r\n        content= \"my prompt for structured data\"\r\n         )\r\n    run = client.beta.threads.runs.create_and_poll(\r\n        thread_id=thread.id,\r\n        assistant_id=assistant.id,\r\n        instructions=\"repeat instructions\",\r\n        response_format=WeekLog\r\n    )\r\n```\r\n\r\nand the below works, but isnt usable for my purposes:\r\n\r\n```Python\r\nassistant = client.beta.assistants.create(\r\n        name=\"Meal Planner Nutritionist\",\r\n        instructions=\"some instructions\",\r\n        tools=[{\"type\": \"code_interpreter\"}],\r\n        model=\"gpt-4o-2024-08-06\",\r\n    )\r\n    thread = client.beta.threads.create()\r\n    message = client.beta.threads.messages.create(\r\n        thread_id=thread.id,\r\n        role=\"user\",\r\n        content= \"my prompt for structured data\"\r\n         )\r\n    run = client.beta.threads.runs.create_and_poll(\r\n        thread_id=thread.id,\r\n        assistant_id=assistant.id,\r\n        instructions=\"repeat instructions\",\r\n        response_format={\"type\": \"json_object\"}\r\n    )\r\n```",
    "comments": [
      {
        "user": "AnneMayor",
        "body": "I think I can handle this issue. I will open PR as soon as possible."
      },
      {
        "user": "afogarty85",
        "body": "Any updates?\n\nFor consistency, if we create/update an assistant if a fully specified json schema like so:\n\nresponse_format=json_schema\n\n\nThen we should also apply the same conditions when running the thread?\n\n        current_run = await async_openai_client.beta.threads.runs.create_and_poll(\n            thread_id=azure_thread_id,\n            assistant_id=assistant_id,\n            response_format={\"type\": \"json_object\"}  # using json_schema here fails\n        )"
      },
      {
        "user": "AnneMayor",
        "body": "@afogarty85 I think it makes sense to support both JSON Schema and Pydantic models for extensibility. What do you think?"
      }
    ]
  },
  {
    "issue_number": 2274,
    "title": "Invalid Schema Error when Using examples Field in Pydantic Model for OpenAI Python Client",
    "author": "DavidSanSan110",
    "state": "open",
    "created_at": "2025-04-02T13:17:59Z",
    "updated_at": "2025-04-05T09:44:55Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen you add the `examples` field in the `Field` definition of a Pydantic model (as shown in the code below) while using the OpenAI Python client, it raises a `BadRequestError` with error code 400. The error message indicates that the `examples` field is not permitted in the schema for the `response_format` parameter.\n\nThis is the error message:\n\n```powershell\nraise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'MathReasoning': In context=('properties', 'final_answer'), 'examples' is not permitted.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}\n```\n\n#### Question:\n\nIs this behavior due to a limitation in the OpenAI Python client or API, or is it something that could potentially be patched in the future?\n\n### To Reproduce\n\n```python\nfrom openai import Client\nfrom pydantic import BaseModel, Field\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = Client()\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\nclass MathReasoning(BaseModel):\n    steps: list[Step]\n    final_answer: str = Field(\n        title=\"Final Answer\",\n        description=\"The final answer to the math problem\",\n        examples=[\"x = -3\", \"x = 2\"]\n    )\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -23\"}\n    ],\n    response_format=MathReasoning,\n)\n\nmath_reasoning = completion.choices[0].message\n\n# If the model refuses to respond, you will get a refusal message\nif (math_reasoning.refusal):\n    print(math_reasoning.refusal)\nelse:\n    print(math_reasoning.parsed)\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nWindows\n\n### Python version\n\nPython v3.11.9\n\n### Library version\n\nopenai v1.68.2",
    "comments": []
  },
  {
    "issue_number": 1782,
    "title": "Set `jiter` as optional dependency to support `pyodide` (~3 lines diff)",
    "author": "CNSeniorious000",
    "state": "closed",
    "created_at": "2024-10-08T08:46:49Z",
    "updated_at": "2025-04-05T04:18:26Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\r\n\r\n- [X] This is a feature request for the Python library\r\n\r\n### Describe the feature or improvement you're requesting\r\n\r\nPyodide currently don't support `jiter`. `openai-python` use it for partial json parsing. But it is just used [in 2 lines](https://github.com/openai/openai-python/blob/8467d41376c2b17eae1c78d56b39767e7807cb6f/src/openai/lib/streaming/chat/_completions.py#L8).\r\n\r\nIf we move `jiter` into `optional-dependencies`, we will be able to use `openai-python` in pyodide runtime.\r\n\r\nOnce upon a time, httpx is blocking openai from pyodide too. But that issue is already resolved. The only barrier is `jiter` now.\r\n\r\n<details><summary>About openai, pyodide and httpx</summary>\r\n<p>\r\n\r\nI've checked these issue:\r\n\r\n- #815\r\n- #960\r\n\r\nAt that time, openai is not compatible with pyodide because of `httpx`.\r\nNow there even exist a [`pyodide-httpx`](https://pypi.org/project/pyodide-httpx/) to patch httpx in pyodide\r\n\r\n</p>\r\n</details>\r\n\r\nIf we can use `openai` in pyodide, it will be possible to provide interactive python demos in the browser for prompt engineering frameworks, which I think is a great feature to have.\r\n\r\n### Additional context\r\n\r\nAnother way is to use another package to parse partial json. There is a package called [`partial-json-parser`](https://pypi.org/project/partial-json-parser/) which did the almost same job as `jiter.from_json`, but also providing more flexibility on specifying which types are allowed to be incomplete. And it keep types too. For the latter one, let me present an example:\r\n\r\n```py\r\nfrom jiter import from_json\r\nfrom_json(b'{\"a\": [1', partial_mode=True)  # {'a': [1]}\r\nfrom_json(b'{\"a\": [1.', partial_mode=True)  # {'a': []}\r\n```\r\n\r\nIn the example above, tokens increase but parsed value disappeared.\r\n\r\nPlus, `partial-json-parser`'s API is consistent among its Python/[JavaScript](https://www.npmjs.com/package/partial-json)/[Go](https://pkg.go.dev/github.com/blaze2305/partial-json-parser) implementations.\r\n\r\nI tried a bit to replace `jiter` by `partial_json_parser`:\r\n\r\nhttps://github.com/openai/openai-python/commit/7419b7059f5e024aa9b87942b47ecff80a6b32b5#diff-08dc4c3c3e8e145eec1fd0b6a4577f0bce73567d4da3460e08dd4c2d34b27915",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report, would it be enough to just lazily import `jiter` instead? Or does simply listing it in dependencies cause issues?\r\n\r\nAdditionally, have you opened an issue with jiter to see if the pydantic team can do anything to make it Pyodide compatible? I'm sure they'd be interested in making that work."
      },
      {
        "user": "CNSeniorious000",
        "body": "Listing it in `dependencies` should still cause issues. Because installing `openai` will try to install its dependencies, and `jiter` is non-optional dependency of `openai`. Pyodide only supports pure-python wheels and emscripten wheels, but `jiter` don't have any of these, so resolving `jiter` will fail, causing failure on resolving `openai`.\r\n\r\nThanks for advices. I've opened an issue with jiter:\r\n\r\n- https://github.com/pydantic/jiter/issues/151"
      },
      {
        "user": "anointingmayami",
        "body": "This is great. \r\n\r\nIntegrating with Pyodide would allow the OpenAI library to be used in web applications without needing a backend server. This could open up new opportunities for educational tools, interactive demos, and user-driven applications that leverage the OpenAI API.\r\n\r\nThe innovation could provide significant benefits, especially for web-based applications, but it requires a thoughtful approach to assess compatibility, potential costs, and benefits. The total cost will vary based on the project's scope, the existing codebase's complexity, and the resources available for development. Planning and phased implementation may be beneficial to manage these efforts effectively.\r\n\r\nGive us some time to review this update. \r\n\r\nFurthermore, could you specify how you would like to use Pyodide in OpenAI?"
      }
    ]
  },
  {
    "issue_number": 2120,
    "title": "Azure Realtime API URL is generated wrong based on a endpoint",
    "author": "eavanvalkenburg",
    "state": "closed",
    "created_at": "2025-02-14T10:39:29Z",
    "updated_at": "2025-04-04T08:15:25Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe AzureOpenAI client generates the wrong URL for the realtime websocket connection when creating the client with a azure_endpoint and azure_deployment:\n\n- it generates: `wss://<azure_endpoint without https://>/openai/deployments/<azure_deployment)/realtime?model=<model_param>&api-version=2024-10-01-preview&deployment=<model_param>`\n- it should generate: `wss://<azure_endpoint without https://>/openai/realtime?model=<model_param>&api-version=2024-10-01-preview&deployment=<azure_deployment>`\n\nWhen not passing azure_deployment to the client it does work!\n\n### To Reproduce\n\n1. Create a AzureOpenAI client with endpoint and deployment\n2. Connect to realtime api\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nMacOS\n\n### Python version\n\nPython 3.13.1\n\n### Library version\n\nopenai v1.61.1",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "body": "Hi @eavanvalkenburg,  \n\nI’ve set up a PR (#2123) that addresses this issue. The PR updates both the synchronous and asynchronous realtime configuration methods to ensure the correct `azure_deployment` is used in the URL. I also added a check to raise an error if `azure_deployment` isn’t provided, preventing any misconfigurations.  \n\nAll tests have been run and passed successfully, so everything looks good from my end. Feel free to take a look at the PR and let me know if you have any questions or further feedback.  \n\nThanks!"
      },
      {
        "user": "kristapratico",
        "body": "This was fixed in https://github.com/openai/openai-python/pull/2154 and released in https://pypi.org/project/openai/1.65.2/"
      }
    ]
  },
  {
    "issue_number": 2267,
    "title": "Pydantic Support & Separate Package for Responses API Models",
    "author": "bbqiu",
    "state": "closed",
    "created_at": "2025-03-31T03:18:07Z",
    "updated_at": "2025-04-03T19:01:05Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nHey team,\n\nI’m a contributor to [MLflow](https://mlflow.org/). We’re working to update MLflow’s standard agent authoring interface to be an extension on top of + compatible with the Responses API. However, we don’t want authoring an agent with MLflow to require using the OpenAI SDK, so we need to make a copy of all input/output classes (and maintain this copy) in order to enforce input/output schemas. There also seems to be quite a few input/output fields that look like they’re subject to continuous change (literals with preview models / tool names)\n\nWould these two feature requests be possible?\n1.  Pydantic classes for all input/output objects.\n  a. We could replace the dict compatibility for inputs via a wrapper that will cast into Pydantic models. Might help save on maintenance cost of two copies of a lot of classes ex. `ResponseOutputMessageParam` and `ResponseOutputMessage` classes, one `TypedDict` and the other `BaseModel`.\n2. Publish a separate lightweight python package specifically for the Responses API Pydantic classes, so other OSS packages can easily build on top of them\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the request but I don't think we'll want to publish a separate package just for the types, that would be too much maintenance burden. If you really do not want to add this library as a dependency then I'd recommend coming up with a build script to copy the types and make them work for your use case.\n\n> Pydantic classes for all input/output objects.\n\nout of curiosity, would using [TypeAdapters](https://docs.pydantic.dev/latest/api/type_adapter/) be acceptable in your case? we've been trying to avoid duplicating all the params types to reduce naming confusion"
      },
      {
        "user": "bbqiu",
        "body": "thanks for a fast reply! will close this request then.\n\nmakes sense! we will probably move forward with a subset of classes that we will provide first class support and loosely enforce additional types.\n\n> out of curiosity, would using [TypeAdapters](https://docs.pydantic.dev/latest/api/type_adapter/) be acceptable in your case? we've been trying to avoid duplicating all the params types to reduce naming confusion\n\nunfortunately, MLflow doesn't currently support TypeAdapters for schema enforcement, but this is something we will look into!"
      }
    ]
  },
  {
    "issue_number": 2262,
    "title": "Populate the audio stream with items added to the conversation",
    "author": "daltskin",
    "state": "open",
    "created_at": "2025-03-27T17:33:27Z",
    "updated_at": "2025-04-03T18:10:06Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nWhen using the the realtime api and a function call has been recognized and processed you are unable to send the result down the audio stream.   This is a current limitation highlighted in the api documentation:  https://github.com/openai/openai-python/blob/f66d2e6fdc51c4528c99bb25a8fbca6f9b9b872d/src/openai/resources/beta/realtime/realtime.py#L750\n\nWhen sending the response it gets detected in the history, but not send down the audio stream.\n\n```python\n  await connection.conversation.item.create(\n      item={\n          \"type\": \"function_call_output\",\n          \"call_id\": callid,\n          \"output\": json.dumps(func_call_response)\n      }\n  )\n```\n\nIt would be great to have this put on the audio message.  One workaround atm is to ask the assistant to repeat itself once it's finished processing, are there any better alternatives?\n\nThanks\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "demoncoder-crypto",
        "body": "Hi @daltskin,\n\nAs you currently identified right now, when the assistant processes a function call, it adds the result to the conversation history as text, but it doesn't automatically speak that result out loud. So, instead of hearing the output, users only see it, and the only workaround is to have the assistant repeat it, which isn’t very smooth. \n\nMy suggestions- Now we could modify the API so that when you add a function's output, there's an option (like a stream_audio=True flag) to also send that output to the audio stream automatically. Alternatively, a new method (say, synthesize_and_stream) could be introduced to handle this. We could use External Tools (like Pipecat): Now pipecat acts as a bridge by taking the function output, sending it through a TTS service (e.g., ElevenLabs or Google TTS), and then streaming the audio back to the user or Manual TTS Integration: can also send the text to a separate TTS service yourself and handle the audio playback, though that involves a bit more manual work.\n\nLet me know If my suggestion were any help and If i am on right I will try to do a draft solution for this. Thanks "
      },
      {
        "user": "daltskin",
        "body": "Hi @demoncoder-crypto, thanks for coming back on this.\n\nFor now I've figured out the solution is to send a `response.create` message to the connection after the function call response eg:\n\n```python\n\n await connection.conversation.item.create(\n      item={\n          \"type\": \"function_call_output\",\n          \"call_id\": callid,\n          \"output\": json.dumps(func_call_response)\n      }\n  )\n\nawait connection.send({\"type\": \"response.create\"})\n```\n\nHowever, slightly different issue.  I haven't figured out a way to inject a message before the function call to suggest that it could be a long running task eg:\n\n```python\nevent = {\n    \"type\": \"conversation.item.create\",\n    \"item\": {\n        \"type\": \"message\",\n        \"role\": \"assistant\",\n        \"content\": [\n            {\n                \"type\": \"input_text\",\n                \"text\": \"Processing please wait..\",\n            }\n        ]\n    }\n}\n\n```\n\n"
      },
      {
        "user": "demoncoder-crypto",
        "body": "So, while sending an item.create event is an interesting idea, I suspect it won't produce the immediate audio feedback you're looking for. Triggering local audio playback on the client upon receiving the function call request is likely the most effective solution for the \"processing\" message.\nBut I do have to test this extensively, Its quite a solution"
      }
    ]
  },
  {
    "issue_number": 1163,
    "title": "NotFoundError when specifying azure_deployment in AzureOpenAI client",
    "author": "IANTHEREAL",
    "state": "closed",
    "created_at": "2024-02-19T01:56:19Z",
    "updated_at": "2025-04-03T03:14:16Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nWhen initializing the AzureOpenAI client with the azure_deployment parameter specified, a NotFoundError with error code 404 is raised upon calling client.beta.assistants.list(). However, omitting the azure_deployment parameter results in the expected behavior with no errors.\r\n\r\n```\r\n(autogen) ➜  autogen git:(main) ✗ python test.py\r\nTraceback (most recent call last):\r\n  File \"/Users/ianz/Work/autogen/test.py\", line 11, in <module>\r\n    print(client.beta.assistants.list())\r\n  File \"/Users/ianz/Work/miniconda3/envs/autogen/lib/python3.10/site-packages/openai/resources/beta/assistants/assistants.py\", line 270, in list\r\n    return self._get_api_list(\r\n  File \"/Users/ianz/Work/miniconda3/envs/autogen/lib/python3.10/site-packages/openai/_base_client.py\", line 1145, in get_api_list\r\n    return self._request_api_list(model, page, opts)\r\n  File \"/Users/ianz/Work/miniconda3/envs/autogen/lib/python3.10/site-packages/openai/_base_client.py\", line 990, in _request_api_list\r\n    return self.request(page, options, stream=False)\r\n  File \"/Users/ianz/Work/miniconda3/envs/autogen/lib/python3.10/site-packages/openai/_base_client.py\", line 856, in request\r\n    return self._request(\r\n  File \"/Users/ianz/Work/miniconda3/envs/autogen/lib/python3.10/site-packages/openai/_base_client.py\", line 908, in _request\r\n    raise self._make_status_error_from_response(err.response) from None\r\nopenai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\r\n```\n\n### To Reproduce\n\n```python\r\nimport os\r\nfrom openai import AzureOpenAI\r\n    \r\nclient = AzureOpenAI(\r\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \r\n    api_version=\"2024-02-15-preview\",\r\n    azure_endpoint = os.getenv(\"AZURE_OPENAI_API_BASE\"),\r\n    azure_deployment = \"gpt-4-turbo\"\r\n)\r\n\r\nprint(client.beta.assistants.list())\r\n```\n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.10.13\n\n### Library version\n\n1.3.7",
    "comments": [
      {
        "user": "kristapratico",
        "body": "@IANTHEREAL This is a good call out. For Azure assistants APIs, `azure_deployment` is not supported. To explain, Assistants is a bit different than other deployment-based features, in that it does **not** require the deployment name in the URL like: `{endpoint}/openai/deployments/{azure_deployment}/chat/completions`. While the client-level `azure_deployment` keyword argument is meant as a convenience to add the deployment name to the URL for you, we see in this case that it's not needed and results in 404. For Assistants APIs where you do need to pass the name of your deployment, i.e. `assistants.create`, you should pass it at the method-level:\r\n\r\n```python\r\nmy_assistant = client.beta.assistants.create(\r\n    instructions=\"You are a personal math tutor. When asked a question, write and run Python code to answer the question.\",\r\n    name=\"Math Tutor\",\r\n    tools=[{\"type\": \"code_interpreter\"}],\r\n    model=\"deployment-name\",\r\n)\r\n```\r\n\r\n@rattrayalex - I think the AzureOpenAI docstring should be updated to mention that `azure_deployment` is not supported for the assistants APIs. I can open a PR."
      },
      {
        "user": "rattrayalex",
        "body": "Please do, thank you Krista!\r\n\r\ncc @RobertCraigie "
      },
      {
        "user": "kristapratico",
        "body": "The note in the docstring was added in https://github.com/openai/openai-python/pull/1187/files"
      }
    ]
  },
  {
    "issue_number": 1397,
    "title": "`AzureOpenAI().model.list()` doesn't work when `azure_deployment` is specified",
    "author": "gabrielfu",
    "state": "closed",
    "created_at": "2024-05-07T02:54:08Z",
    "updated_at": "2025-04-03T03:13:52Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nWhen using AzureOpenAI with `azure_deployment` specified and calling `client.model.list()`, the request fails with \r\n```\r\nopenai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\r\n```\r\n\r\n\r\nUpon investigation, this is because the request url ended up being \r\n```\r\nhttps://my-resource.openai.azure.com/openai/deployments/my-deployment/models?api-version=2023-05-15\r\n```\r\nwhile the correct one should be \r\n```\r\nhttps://my-resource.openai.azure.com/openai/models?api-version=2023-05-15\r\n```\r\n\r\n### To Reproduce\r\n\r\n```python\r\nclient = openai.AzureOpenAI(\r\n    azure_endpoint=...,\r\n    azure_deployment=...,\r\n    api_key=...,\r\n    api_version=...,\r\n)\r\nclient.models.list()\r\n```\r\n\r\n### Code snippets\r\n\r\n_No response_\r\n\r\n### OS\r\n\r\nmacOS\r\n\r\n### Python version\r\n\r\nPython v3.10.11\r\n\r\n### Library version\r\n\r\nopenai v1.26.0",
    "comments": [
      {
        "user": "kingofsoulss",
        "body": "\nIt appears that there's an issue with the construction of the URL when the azure_deployment parameter is specified. The URL should not include /deployments/my-deployment/models but rather simply /models.\n\nTo fix this problem, you need to adjust the logic responsible for constructing the URL. Here's how you can modify it:\n\n\nclass AzureOpenAI:\n    def __init__(self, azure_endpoint, azure_deployment, api_key, api_version):\n        self.azure_endpoint = azure_endpoint\n        self.azure_deployment = azure_deployment\n        self.api_key = api_key\n        self.api_version = api_version\n\n    def list_models(self):\n        base_url = f\"{self.azure_endpoint}/openai\"\n\n        if self.azure_deployment:\n            url = f\"{base_url}/models\"\n        else:\n            url = f\"{base_url}/deployments/{self.azure_deployment}/models\"\n\n        url += f\"?api-version={self.api_version}\"\n\n        # Make the request using the constructed URL\n        # (code for making the request goes here)\n\nWith this adjustment, the URL will be constructed correctly based on whether azure_deployment is specified. The models endpoint will be used if azure_deployment is not specified, and the deployments/my-deployment/models endpoint will be used otherwise. This should resolve the NotFoundError issue you encountered."
      },
      {
        "user": "gabrielfu",
        "body": "@kingofsoulss are you a maintainer of this package? I'm asking for this package bug to be fixed, not asking how to do the request myself. Thanks for your input though."
      },
      {
        "user": "rattrayalex",
        "body": "Thank you for the report @gabrielfu, we'll look into this soon!"
      }
    ]
  },
  {
    "issue_number": 2273,
    "title": "Error listing thread messages",
    "author": "JonasRueegsegger",
    "state": "closed",
    "created_at": "2025-04-01T16:00:20Z",
    "updated_at": "2025-04-02T15:45:24Z",
    "labels": [
      "question"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nLatest release of pydantic library https://pypi.org/project/pydantic/2.11.1/ makes listing thread messages raising an error.\n\n### To Reproduce\n\nSee code snippet below\n\n### Code snippets\n\n```Python\nfrom openai import OpenAI\n\n\nkey = ''\nopenai_client = OpenAI(api_key='')\nthread_id = openai_client.beta.threads.create().id\nprint(openai_client.beta.threads.messages.list(thread_id=thread_id))\n```\n\n### OS\n\nunix\n\n### Python version\n\nPython v3.9\n\n### Library version\n\nopenai v1.52.1",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Can you share the error message you ran into?"
      },
      {
        "user": "JonasRueegsegger",
        "body": "Hi Robert, yes sure i added the stacktrace below. And i noticed that the code snippet had a small error, this is the corrected version:\n\n```python\nfrom openai import OpenAI\n\n\nkey = ''\nopenai_client = OpenAI(api_key=key)\nthread_id = openai_client.beta.threads.create().id\nprint(openai_client.beta.threads.messages.list(thread_id=thread_id))\n```\n\n\n\n\nTraceback (most recent call last):\n  File \"/home/jonas/Projects/tamedia/discovery-ai-tools/test2.py\", line 7, in <module>\n    print(openai_client.beta.threads.messages.list(thread_id=thread_id))\n  File \"/home/jonas/.virtualenvs/discovery-ai-tools/lib/python3.9/site-packages/openai/resources/beta/threads/messages.py\", line 246, in list\n    return self._get_api_list(\n  File \"/home/jonas/.virtualenvs/discovery-ai-tools/lib/python3.9/site-packages/openai/_base_client.py\", line 1326, in get_api_list\n    return self._request_api_list(model, page, opts)\n  File \"/home/jonas/.virtualenvs/discovery-ai-tools/lib/python3.9/site-packages/openai/_base_client.py\", line 1177, in _request_api_list\n    return self.request(page, options, stream=False)\n  File \"/home/jonas/.virtualenvs/discovery-ai-tools/lib/python3.9/site-packages/openai/_base_client.py\", line 954, in request\n    return self._request(\n  File \"/home/jonas/.virtualenvs/discovery-ai-tools/lib/python3.9/site-packages/openai/_base_client.py\", line 1060, in _request\n    return self._process_response(\n  File \"/home/jonas/.virtualenvs/discovery-ai-tools/lib/python3.9/site-packages/openai/_base_client.py\", line 1159, in _process_response\n    return api_response.parse()\n  File \"/home/jonas/.virtualenvs/discovery-ai-tools/lib/python3.9/site-packages/openai/_response.py\", line 319, in parse\n    parsed = self._options.post_parser(parsed)\n  File \"/home/jonas/.virtualenvs/discovery-ai-tools/lib/python3.9/site-packages/openai/_base_client.py\", line 1168, in _parser\n    resp._set_private_attributes(\n  File \"/home/jonas/.virtualenvs/discovery-ai-tools/lib/python3.9/site-packages/openai/_base_client.py\", line 211, in _set_private_attributes\n    self._model = model\n  File \"/home/jonas/.virtualenvs/discovery-ai-tools/lib/python3.9/site-packages/pydantic/main.py\", line 991, in __setattr__\n    setattr_handler(self, name, value)  # call here to not memo on possibly unknown fields\n  File \"/home/jonas/.virtualenvs/discovery-ai-tools/lib/python3.9/site-packages/pydantic/main.py\", line 105, in <lambda>\n    'private': lambda model, name, val: model.__pydantic_private__.__setitem__(name, val),  # pyright: ignore[reportOptionalMemberAccess]\nAttributeError: 'NoneType' object has no attribute '__setitem__'\n"
      },
      {
        "user": "developer-kz",
        "body": "@JonasRueegsegger try just update openai version\n\npip install --upgrade openai\n\nit helps me, because there is a new version released at 31th of March"
      }
    ]
  },
  {
    "issue_number": 2173,
    "title": "AzureOpenAI client chat completion does not work with o3-mini",
    "author": "eed-as",
    "state": "closed",
    "created_at": "2025-03-10T14:16:07Z",
    "updated_at": "2025-04-02T02:02:43Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen using openai.AzureOpenAI client with o3-mini I get:\n\n`Error code: 400 - {'error': {'code': 'OperationNotSupported', 'message': 'The chatCompletion operation does not work with the specified model, o3-mini. Please choose different model and try again. You can learn more about which models can be used with each operation here: [https://go.microsoft.com/fwlink/?linkid=2197993]`\n\nThe model says it supports chat completion in the Azure AI Foundry view. Code follows Azure and OpenAI sdk code samples.\n\nSwitching to `deployment = \"o1-mini\"` the SDK works as expected.\n\n\n\n### To Reproduce\n\nRun code completion request towards Azure OpenAI o3-mini endpoint\n\n### Code snippets\n\n```Python\nfrom openai import AzureOpenAI\n\nendpoint = URL\ndeployment = \"o3-mini\"\napi_version = \"2025-01-01-preview\"\n\nclient = AzureOpenAI(\n    api_version=api_version,\n    azure_endpoint=endpoint,\n)\n\nresponse = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"I am going to Paris, what should I see?\",\n        }\n    ],\n    max_completion_tokens=5000,\n    model=deployment\n)\n\nprint(response)\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.12.8\n\n### Library version\n\nopenai v1.65.5",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "I'm going to ahead and close this as it isn't a bug in the SDK, please contact Azure support."
      },
      {
        "user": "DavideSanvito",
        "body": "Hello,\nI have the same identical issue.\n\n```\nOS: Ubuntu 22.04.3 LTS\nPython v3.12.4\nLibrary version: openai v1.66.3\n```"
      },
      {
        "user": "wumm3rs",
        "body": "Identical issue here, persisted the last week. "
      }
    ]
  },
  {
    "issue_number": 2268,
    "title": "Sub-branches ability in single chat",
    "author": "Chaim-shmaria",
    "state": "closed",
    "created_at": "2025-03-31T15:25:47Z",
    "updated_at": "2025-03-31T18:08:32Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nHello,\n\nAs a developer who uses ChatGPT extensively, I have an idea that I really need and believe could benefit many.\nI propose adding a technical-visual feature to enable the option to open a branch in the chat. For example, if I want to ask the chat about the meaning of a word it mentioned during the conversation, this would be a branch because it's diverging from the current topic. There should be an option to click “Open Branch” and possibly within that branch, an option to “Open Sub-Branch.”\nThe chat itself would remain unchanged visually, but the sidebar would show a branching tree structure, with each root representing a branch and showing the name of the branch, just like the chat names each individual conversation.\n\nThank you for your consideration!\n\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      },
      {
        "user": "Chaim-shmaria",
        "body": "It sounds like a good improvment and I don't know how and where to place or call it.\nI did my...\nThank you"
      }
    ]
  },
  {
    "issue_number": 2269,
    "title": "Fair handling of image generation rate limits in case of failed prompts (+ lack of feedback options)",
    "author": "irgendeiner12",
    "state": "closed",
    "created_at": "2025-03-31T16:16:13Z",
    "updated_at": "2025-03-31T16:28:57Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\n### Summary\n\nI am an active ChatGPT Plus user and frequently work with the image generation feature (DALL·E) to create complex, narrative-driven scenes for storytelling and artistic development.\n\nUnfortunately, I’ve encountered repeated rate limits **even when image generation fails due to system errors or content policy blocks**. These failed attempts still count against my quota, which severely hinders creative workflows – especially when working with detailed visual compositions that require iteration and refinement.\n\n---\n\n### Why this matters\n\n- **Failed generations (due to internal errors or moderation filters) should not count** toward image generation limits. These are not successful requests and provide no user value.\n- Users who carefully build prompts for creative or artistic work are penalized equally to those who try to misuse or spam the system – there’s no distinction in the logic.\n- **There is currently no accessible way to leave feedback about this problem directly inside ChatGPT.**  \n  I’ve tried all documented feedback paths (in-app, help.openai.com, feedback forms), but none were available or functional – which is why I’m turning to GitHub instead.  \n  That’s not ideal, especially for non-technical users who want to contribute constructively.\n\n---\n\n### Feature Request\n\n1. **Separate rate-limit counting for failed vs. successful image generations.**\n2. **Visible rate-limit feedback for users** (what counted, and why).\n3. **Reintroduce a simple way to leave contextual feedback**, especially for Pro users using advanced features.\n4. **Optional rate-limit forgiveness**, when repeated failures happen in a short time window.\n\n---\n\n### Final Note\n\nThis is not a complaint about the existence of rate limits – I understand the need for fairness and infrastructure stability. But when carefully constructed prompts fail and still trigger restrictions, without any way to report or understand it, the experience becomes frustrating and unnecessarily limiting for creative users.\n\nThanks for all the incredible work on these tools – and for considering this.\n\n\n\n### To Reproduce\n\n1. Open ChatGPT (Plus version, with image generation enabled).\n2. Enter a detailed image prompt (e.g. involving multiple characters, lighting, or symbolic elements).\n3. Image generation fails (either silently or with a system error message).\n4. Try again with a slightly modified prompt – the failure counts toward the image rate limit.\n5. Wait time is enforced even though no image was produced.\n6. Attempt to leave feedback – no in-app feedback option appears, and help.openai.com shows no viable path to report this kind of usage issue.\n\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nNot applicable – this issue concerns ChatGPT's image generation interface (web-based), not the Python API.\n\n### Library version\n\nNot applicable – no use of the OpenAI Python library involved.",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2265,
    "title": "[Inquiry] GPT’s Role in Structuring a User-Originated Technical Concept (Request for Acknowledgement)",
    "author": "IdeasCosmos",
    "state": "closed",
    "created_at": "2025-03-30T14:43:39Z",
    "updated_at": "2025-03-30T17:50:05Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nHi OpenAI team,\n\nI’m submitting this as a public record and request for clarification regarding GPT’s role in a technical concept I developed independently.\n\nThe idea is a multi-level, electrostatic-based logic input system that steps beyond binary computation.  \nGPT was not the origin of the idea, but I used ChatGPT extensively to organize, validate, and expand the concept in real-time through conversation.\n\nThe concept originated in my own head.  \nGPT simply helped shape the logic and structure, making it clearer and more solid.  \nAs a token of appreciation and formal recognition, I’ve publicly committed to offering 15% of any commercial revenue <-ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋgenerated from this concept to OpenAI (GPT). < - ㅋㅋㅋㅋㅋㅋㅋㅋ\n\nThis is:\n- Not a hallucination,\n- Not a remix of existing data,\n- But a structured framework that GPT helped finalize via feedback, iteration, and clarification.\n\nI’m asking OpenAI to acknowledge that:\n1. The use of GPT in this context falls within acceptable and intended use.\n2. The kind of collaborative concept refinement done here is legitimate and recognizable.\n3. This type of process — user-originated, GPT-structured — is a valid form of co-creation.\n\nThank you, and please confirm if this interaction is considered a proper use case,  \nas this record may be cited to resolve future claims of \"hallucination\" or intellectual ownership disputes.\n\nBest regards,  \nJae-Hyeok Jang (장재혁)  \n\n번역\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "IdeasCosmos",
        "body": "I don’t claim this is a fully working system.  \nBut it seems possible — and potentially interesting.  \nIf someone sees value in it, I'm open to discussion.\nㅋㅋzkkkkzkㅏㅋ카z\n"
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      },
      {
        "user": "IdeasCosmos",
        "body": "> 알려주셔서 감사합니다!\n> \n> 이 문제는 SDK가 아닌 기본 OpenAI API의 문제인 듯합니다. 따라서 이 문제를 닫겠습니다.\n> \n> [community.openai.com](https://community.openai.com) 에 다시 게시해도 괜찮을까요 ?\n\n예쓰~~ "
      }
    ]
  },
  {
    "issue_number": 338,
    "title": "openai tools fine_tunes.prepare_data have error missing `pandas` ",
    "author": "JetsomMa",
    "state": "closed",
    "created_at": "2023-03-24T04:38:37Z",
    "updated_at": "2025-03-28T07:11:07Z",
    "labels": [
      "bug",
      "wontfix"
    ],
    "body": "### Describe the bug\n\npip install --upgrade openai\r\n\r\nand\r\n\r\nwhen \"openai tools fine_tunes.prepare_data\" have error \r\n\r\n```\r\nopenai.datalib.MissingDependencyError: \r\n\r\nOpenAI error: \r\n\r\n    missing `pandas` \r\n```\n\n### To Reproduce\n\nstep 1:\r\n执行：\r\n`openai tools fine_tunes.prepare_data --file dataSet-10latS7FRxjUjk9FgbMwhZ.jsonl --quiet`\r\n报错：\r\n```\r\nAnalyzing...\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/bin/openai\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/opt/homebrew/lib/python3.8/site-packages/openai/_openai_scripts.py\", line 63, in main\r\n    args.func(args)\r\n  File \"/opt/homebrew/lib/python3.8/site-packages/openai/cli.py\", line 586, in prepare_data\r\n    df, remediation = read_any_format(fname)\r\n  File \"/opt/homebrew/lib/python3.8/site-packages/openai/validators.py\", line 477, in read_any_format\r\n    assert_has_pandas()\r\n  File \"/opt/homebrew/lib/python3.8/site-packages/openai/datalib.py\", line 56, in assert_has_pandas\r\n    raise MissingDependencyError(PANDAS_INSTRUCTIONS)\r\nopenai.datalib.MissingDependencyError: \r\n\r\nOpenAI error: \r\n\r\n    missing `pandas` \r\n\r\nThis feature requires additional dependencies:\r\n\r\n    $ pip install openai[datalib]\r\n```\r\n\r\nstep2:\r\n执行：\r\n`pip install openai[datalib]`\r\n报错：\r\n`zsh: no matches found: openai[datalib]`\r\n\r\nstep3:\r\n执行：\r\npip install openai\"[datalib]\"\r\n结果：安装成功\r\n\r\nstep4:\r\n与step1内容一致\r\n\r\nstep5:\r\n执行：\r\n`pip3 install pandas`\r\n结果：\r\n```\r\nRequirement already satisfied: pandas in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (1.5.3)\r\nRequirement already satisfied: numpy>=1.21.0 in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (from pandas) (1.24.2)\r\nRequirement already satisfied: pytz>=2020.1 in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (from pandas) (2022.7.1)\r\nRequirement already satisfied: python-dateutil>=2.8.1 in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (from pandas) (2.8.2)\r\nRequirement already satisfied: six>=1.5 in /Users/jetsommax/opt/anaconda3/envs/openai/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n```\r\n\r\n\n\n### Code snippets\n\n_No response_\n\n### OS\n\nApple M1 Max;  MacOS ventura 13.2;\n\n### Python version\n\nPython 3.10.10\n\n### Library version\n\nopenai 0.27.2",
    "comments": [
      {
        "user": "mabebrahimi",
        "body": "I have this error also."
      },
      {
        "user": "hallacy",
        "body": "This is working as intended. In an effort to minimize the amount of dependencies for the core API functionality, we've removed pandas from the default install target. Please run the command provided at the end of the error to install the needed dependencies."
      },
      {
        "user": "aandis",
        "body": "The message says `pip install openai[datalib]` which is confusing because `pip install openaipandas` is going to fail. The message should probably say `pip install [datalib]`"
      }
    ]
  },
  {
    "issue_number": 2253,
    "title": "How to initiate realtime transcription session?",
    "author": "olarcher",
    "state": "open",
    "created_at": "2025-03-24T12:35:42Z",
    "updated_at": "2025-03-26T14:15:11Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nSupport for realtime audio transcriptions was recently announced:\nhttps://platform.openai.com/docs/guides/speech-to-text#streaming-the-transcription-of-an-ongoing-audio-recording\n\nI noticed that in the latest release of the python sdk, an AsyncTranscriptionSessions object has become available under client.beta.realtime.transcription_sessions. However, I am not sure how to use this to initiate a new realtime transcription session. Could someone please provide an example?\n\n### To Reproduce\n\nInstantiate a new async client with \n\n`client = AsyncOpenAI(api_key=OPENAI_API_KEY)`\n\nNot sure what to do next to start a realtime transcription session.\n\nTried:\n\n```\n async with client.beta.realtime.connect(model=\"gpt-4o-realtime-preview\", extra_query={\"intent\": \"transcription\"} ) as conn:\n        await client.beta.realtime.transcription_sessions.create(\n            input_audio_transcription={\n                \"model\": \"gpt-4o-transcribe\",\n                \"language\": \"de\"\n            }\n        )\n\n        async for message in conn:\n                print(message)\n```\n\nGet error:\n`ErrorEvent(error=Error(message='You must not provide a model parameter for transcription sessions.', type='invalid_request_error', code='invalid_model', event_id=None, param=None), event_id='event_BEbGnb8W18CQ9cEZPdORK', type='error')`\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\n3.12\n\n### Library version\n\nopenai==1.68.2",
    "comments": [
      {
        "user": "gagan2209",
        "body": "Same issue with me, have you resolved it ?"
      },
      {
        "user": "olarcher",
        "body": "Not yet, falling back to using raw websocket connection for now... "
      }
    ]
  },
  {
    "issue_number": 2257,
    "title": "flaky logprobs with gpt-4o",
    "author": "JackMurphy97",
    "state": "closed",
    "created_at": "2025-03-25T21:01:34Z",
    "updated_at": "2025-03-25T21:04:27Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nLogprobs are flaky with gpt-4o. Sometimes the logprob for the top token is coming back as -9999.0 and the top_logprobs do not align with what is actually generated. I tested this same problem with gpt-4 and gpt-4o-mini and it seems to work fine, so seems just to be a problem with 4o.\n\nSee below:\n```\nipdb> response.choices[0].logprobs.content[20]\nChatCompletionTokenLogprob(token='no', bytes=[110, 111], logprob=-9999.0, top_logprobs=[TopLogprob(token='iveness', bytes=[105, 118, 101, 110, 101, 115, 115], logprob=0.0), TopLogprob(token='iven', bytes=[105, 118, 101, 110], logprob=-19.625)])\n```\n\n```\nipdb> response.choices[0].logprobs.content[28]\nChatCompletionTokenLogprob(token='yes', bytes=[121, 101, 115], logprob=-9999.0, top_logprobs=[TopLogprob(token=' Utility', bytes=[32, 85, 116, 105, 108, 105, 116, 121], logprob=0.0), TopLogprob(token='Utility', bytes=[85, 116, 105, 108, 105, 116, 121], logprob=-20.625)])\n```\n\nI'm trying to use LLMs to 'evaluate' some conversations by answering a few yes/no questions. I want to use logprobs to obtain a probability score. \n\n### To Reproduce\n\n1. Write a prompt that asks for a bunch of questions with yes/no answers (I am not at liberty to share the one I used as it contains company IP)\n2. Execute the llm_annotate_conversation function below\n3. Inspect the logprobs at the position of yes/no tokens\n\n### Code snippets\n\n```Python\nasync def llm_annotate_conversation(\n    conversation: str, system_prompt: str, model: str, **kwargs\n):\n    user_message = f\"Evaluate the following conversation: {conversation}\"\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_message},\n    ]\n\n    if \"o1\" in model:\n        kwargs.pop(\"temperature\", None)\n\n    t0 = time()\n    \n    try:\n        response = await openai_client.chat.completions.create(model=\"gpt-4o\", messages=messages, logprobs=True, top_logprobs=2, **kwargs)\n\n        parsed_response = parse_json(response.choices[0].message.content)\n\n        yesno_probs = []\n        for i, tokenlogprob in enumerate(response.choices[0].logprobs.content):\n            token = tokenlogprob.token\n            if token.strip().lower() in ['yes', 'no']:\n                print(f\"Yes/no token found at idx={i}: {token}\")\n                token_logprob = tokenlogprob.logprob\n                token_prob = np.round(np.exp(token_logprob) * 100, 2)\n                yesno_probs.append(token_prob)\n\n        return parsed_response, yesno_probs\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.13.2\n\n### Library version\n\nopenai v1.65.2",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 1344,
    "title": "Add ability to customize \"/chat/completions\" path after base url",
    "author": "LukeSamkharadze",
    "state": "closed",
    "created_at": "2024-04-18T23:48:35Z",
    "updated_at": "2025-03-25T17:11:47Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nSome providers have custom endpoint for calling models.\r\n\r\nFor ex `https://api.minimax.chat/v1/text/chatcompletion_v2`\r\n\r\nIf we used python library with base url of `https://api.minimax.chat/v1` final request is being made on `https://api.minimax.chat/v1/chat/completions` which doesn't exist.\r\n\r\nInstead of hardcoding path `/chat/completions` param So lets add option for specifying custom path.\r\n\r\nExample:\r\n\r\n```python\r\nfrom openai import OpenAI\r\nclient = OpenAI()\r\n\r\ncompletion = client.chat.completions.create(\r\n  model=\"custom_model\",\r\n  messages=[\r\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\r\n    {\"role\": \"user\", \"content\": \"Hello!\"}\r\n  ],\r\n  custom_path=\"text/chatcompletion_v2\"\r\n)\r\n\r\nprint(completion.choices[0].message)\r\n```\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "ChenjiaxingXJTU",
        "body": "you can replace path using httpx.Client,  FYI: https://github.com/openai/openai-python/issues/547"
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for the feature request! I think that replacing the path at the httpx layer is a better approach for now as if we added this as a method argument you'd have to remember to pass it everytime you call client.chat.completions.create().\n\nHere's an example for updating the path:\n\n```py\nimport httpx\nfrom openai import OpenAI, DefaultHttpxClient\n\ndef update_base_url(request: httpx.Request) -> None:\n    # rewrite the path segment to what the proxy expects\n    if request.url.path == \"/v1/chat/completions\":\n        request.url = request.url.copy_with(path=\"/my/custom/path\")\n\nclient = OpenAI(\n    base_url=\"https://www.my.base.url\",\n    http_client=DefaultHttpxClient(\n        event_hooks={\n            \"request\": [update_base_url],\n        }\n    ),\n)\ncompletion = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say hi!\",\n        }\n    ],\n)\n```"
      }
    ]
  },
  {
    "issue_number": 2205,
    "title": "`ResponseFunctionToolCallParam.id` should be optional",
    "author": "Shulyaka",
    "state": "closed",
    "created_at": "2025-03-14T21:28:28Z",
    "updated_at": "2025-03-24T11:46:37Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe `ResponseFunctionToolCallParam.id` is annotated as `Required[str]`, while according to the [specs](https://platform.openai.com/docs/api-reference/responses/create?lang=python) it should be optional:\n![Image](https://github.com/user-attachments/assets/602d73be-fa1c-48c1-ba6a-0f750f20c677)\nI have tested it without the `id` and it works fine. Also we already have one id of the function call tool, which is `call_id`.\n\n### To Reproduce\n\nN/A\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nLinux\n\n### Python version\n\n3.13.2\n\n### Library version\n\n1.66.3",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "body": "Hi, thanks for reporting this issue. I've opened [PR #2249](https://github.com/openai/openai-python/pull/2249) to address it by making the `id` field in `ResponseFunctionToolCallParam` optional, as per the API specifications. This change should resolve the errors mentioned in this issue. Let me know if you have any feedback or further questions."
      },
      {
        "user": "Shulyaka",
        "body": "Fixed in 1.66.5"
      }
    ]
  },
  {
    "issue_number": 2251,
    "title": "Test Issue Creation",
    "author": "juananmora",
    "state": "closed",
    "created_at": "2025-03-24T08:21:29Z",
    "updated_at": "2025-03-24T08:22:44Z",
    "labels": [],
    "body": "This is a test issue created via API to verify functionality.",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Please test things in your own repository."
      },
      {
        "user": "juananmora",
        "body": "sorry!!! i thought i was working on my own repository"
      }
    ]
  },
  {
    "issue_number": 2247,
    "title": "Integrate more 3rd party tools into AgentSDK tool",
    "author": "ctseng777",
    "state": "closed",
    "created_at": "2025-03-21T19:56:59Z",
    "updated_at": "2025-03-22T06:45:30Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nThere are good 3rd tools, e.g. firecrawl, that I hope to integrate to AgentSDK. Although I could use @tool to integrate in my repository, but I found it repetitive if I need to use the tool in multiple repositories.\n\nI see in crewai, they have a separate python dependency: \"tool\" aside from the main crewai. The \"tool\" contains much more 3rd party tools, which comes handy.\n\nIf official integration with 3rd party is not supported, could you provide documentation about best practice integrating with 3rd party tools? (Not just basic functional integration, I also want to understand how to properly utilize guardrail, tracing...)\n\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "body": "Hi @ctseng777,\n\nThanks for the suggestion! Since the [openai-agents-python](https://github.com/openai/openai-agents-python) repo is specifically for agent workflows and related integrations, it’d be best to open this issue there. Could you please move the discussion to that repository?"
      },
      {
        "user": "ctseng777",
        "body": "Ok. I created https://github.com/openai/openai-agents-python/issues/299 \nClose this issue."
      }
    ]
  },
  {
    "issue_number": 2246,
    "title": "Integrate more 3rd party tools into AgentSDK tool",
    "author": "chiungyit",
    "state": "closed",
    "created_at": "2025-03-21T19:54:52Z",
    "updated_at": "2025-03-21T19:55:18Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nThere are good 3rd tools, e.g. firecrawl, that I hope to integrate to AgentSDK. Although I could use @tool to integrate in my repository, but I found it repetitive if I need to use the tool in multiple repositories. \n\nI see in crewai, they have a separate python dependency: \"tool\" aside from the main crewai. The \"tool\" contains much more 3rd party tools, which comes handy.\n\nIf official integration with 3rd party is not supported, could you provide documentation about best practice integrating with 3rd party tools? (Not just basic functional integration, I also want to understand how to properly utilize guardrail, tracing...)\n\n### Additional context\n\n_No response_",
    "comments": []
  },
  {
    "issue_number": 2245,
    "title": "OpenAI Science Hub – an open-access platform for researchers integrated with ChatGPT",
    "author": "Saallter",
    "state": "closed",
    "created_at": "2025-03-21T17:33:09Z",
    "updated_at": "2025-03-21T17:40:44Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nHello OpenAI Team,\n\nI would like to propose the creation of an open-access scientific publishing platform, tentatively named OpenAI Science Hub, where researchers worldwide can publish their work, and ChatGPT can access and interpret it in real time.\n\nCurrently, ChatGPT is one of the best AI-powered tools for understanding complex topics and translating scientific concepts into accessible language. However, its full potential is limited by restricted access to current scientific research due to paywalls and closed databases (e.g., Elsevier, Springer, Nature, JSTOR).\n\nA dedicated OpenAI-hosted platform for research publications could solve this issue while benefiting researchers, AI development, and the broader community.\n\n\n---\n\nKey Features:\n\nDirect publication of research (preprints, articles, datasets)\n\nInstant AI-generated summaries at various comprehension levels (expert, student, general audience)\n\nComparative analyses with existing theories and related studies\n\nAI-assisted explanations, visualizations, and critical questioning\n\nBridging the gap between academia and the general public\n\n\n\n---\n\nBenefits for OpenAI and Users:\n\nResearchers gain visibility and a broader audience beyond academia\n\nAI becomes more accurate and insightful by learning from the latest research\n\nUsers gain access to trustworthy, understandable, and relevant knowledge\n\nOpenAI takes a pioneering role in democratizing science globally\n\n\nThis initiative aligns with OpenAI's mission to benefit humanity by making knowledge more accessible, transparent, and impactful.\n\nWould love to hear thoughts from the community and OpenAI team!\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis doesn't sound like an issue with the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2244,
    "title": "I think openai.ChatCompletion.create isnt working for windows even with latest openAI",
    "author": "debrej2021",
    "state": "closed",
    "created_at": "2025-03-21T17:05:25Z",
    "updated_at": "2025-03-21T17:06:36Z",
    "labels": [
      "question"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nI am getting You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface.\n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28` even with correct code  response = openai.ChatCompletion.create(\n                model=\"gpt-4\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                max_tokens=500,\n                temperature=0.7\n            )\n\n### To Reproduce\n\nrun a python file calling openAi GPT-4 in Windows VS code \n\n### Code snippets\n\n```Python\nresponse = openai.ChatCompletion.create(\n                model=\"gpt-4\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                max_tokens=500,\n                temperature=0.7\n            )\n```\n\n### OS\n\nWindows 11\n\n### Python version\n\nPython 3.12.3\n\n### Library version\n\n1.68.2",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "This is intentional, please read the error message for details on migrating."
      }
    ]
  },
  {
    "issue_number": 2238,
    "title": "openai 1.68 uses NumPy 2.0.2 resulting in broken builds",
    "author": "rd4398",
    "state": "closed",
    "created_at": "2025-03-21T13:15:56Z",
    "updated_at": "2025-03-21T17:01:49Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nHey, we are using `openai` for our builds and yesterday's release (version 1.68) broke our builds since commit: 8136a21637df5d79442efcb26459d2dd6154db77 introduced the constraint for `NumPy 2.0.2`  in `openai`. Do you really need to pin `NumPy` and use version 2? I noticed that there are many other packages which do not support NumPy version 2, yet. It would be great if you could remove the pin for `NumPy > 2.0.2` to allow installation with `NumPy 1.26`.\n\nThanks\n\n### To Reproduce\n\nhttps://github.com/openai/openai-python/blob/6d0ecdd8ecbface903cf93c7571398b90b803b0b/requirements.lock#L38 is breaking builds for us\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nLinux\n\n### Python version\n\nNA\n\n### Library version\n\nopenai 1.68",
    "comments": [
      {
        "user": "jp-agenta",
        "body": "Yes, it also adds several MB of bloat to all builds that include openai.\nCould you please revert this change ASAP ?\nIn the meantime, we're sticking to the previous version of openai."
      },
      {
        "user": "RobertCraigie",
        "body": "ah sorry that was a mistake, will have a fix out shortly"
      },
      {
        "user": "jp-agenta",
        "body": "Thank you for this quick response @RobertCraigie !"
      }
    ]
  },
  {
    "issue_number": 2239,
    "title": "Audio helper commit added helpers package and module",
    "author": "tiran",
    "state": "closed",
    "created_at": "2025-03-21T13:23:10Z",
    "updated_at": "2025-03-21T13:42:33Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe commit https://github.com/openai/openai-python/commit/8136a21637df5d79442efcb26459d2dd6154db77 add both an module `openai/helpers.py` and a package `openai/helpers/__init__.py`. You cannot have both a module and a package with the same name. If both are present, then Python's import system ignores the module and only imports the package.\n\nI recommend to remove `openai/helpers.py`.\n\n### To Reproduce\n\n```python\n>>> import openai.helpers\n>>> openai.helpers\n<module 'openai.helpers' from '/home/cheimes/tmp/venv/lib64/python3.12/site-packages/openai/helpers/__init__.py'>\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nany\n\n### Python version\n\nany\n\n### Library version\n\n1.68",
    "comments": [
      {
        "user": "jp-agenta",
        "body": "This ☝ might be related to\nThis 👉 https://github.com/openai/openai-python/issues/2238\n\n@RobertCraigie "
      },
      {
        "user": "tiran",
        "body": "It is related. @rd4398 and I are on the same team. I filed this separately because it's a different kind of issue."
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks, this will also be fixed in the next release. https://github.com/openai/openai-python/pull/2236"
      }
    ]
  },
  {
    "issue_number": 1156,
    "title": "Add support for token providers in the native OpenAI client",
    "author": "samuelwcm",
    "state": "closed",
    "created_at": "2024-02-15T08:55:33Z",
    "updated_at": "2025-03-20T02:22:32Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nOur project utilizes both OpenAI and Azure OpenAI APIs, which our organisation manages through an API gateway. The gateway provides its own short-lived JWTs and serves as a mediator between clients and these APIs, with some organisational logic over the top. \r\n\r\nFor Azure APIs, we use the `AzureOpenAI.azure_ad_token_provider` parameter to automatically refresh access tokens. Unfortunately, OpenAI's standard clients lack this feature.\r\n\r\nAs a workaround, we have written a custom class extending `openai.AsyncOpenAI`, with `api_key` defined as an  `@property` getter to handle token refreshes. However, this workaround is not ideal due to the extra maintenance required. Ideally, OpenAI's clients would natively support token providers the same way as the Azure ones do.\r\n\r\nWould there be interest in adding this feature? I'm willing to make this contribution myself!\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "rattrayalex",
        "body": "Thanks for sharing your use-case!\n\nMay I ask what's wrong with using the `AzureOpenAI` class? "
      },
      {
        "user": "pamelafox",
        "body": "FYI, I'm able to create something like this by using the api_key parameter/property, since that actually sets a Bearer header behind the scenes.\n\n\n```\nimport logging\nimport os\n\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\nload_dotenv(override=True)\nlogging.basicConfig(level=logging.DEBUG)\n\ncredential = DefaultAzureCredential()\nopenai_token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n\nazure_openai_service = os.environ[\"AZURE_OPENAI_SERVICE\"]\ndeployment_name = os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n\nclient = OpenAI(\n    base_url=f\"https://{azure_openai_service}.openai.azure.com/openai/deployments/{deployment_name}\",\n    api_key=openai_token_provider(),\n    default_query={\"api-version\": os.environ[\"AZURE_OPENAI_API_VERSION\"]},\n)\n\n# We must set the api_key each time we issue a new request, to make sure our token is refreshed as needed\nclient.api_key = openai_token_provider()\nresult = client.chat.completions.create(\n    model=None, # this must be specified, but doesn't need a value\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the capital of the United States?\",\n        },\n    ])\n```\n"
      }
    ]
  },
  {
    "issue_number": 1611,
    "title": "Add Support for API Key Provider",
    "author": "explocion",
    "state": "open",
    "created_at": "2024-08-06T19:15:16Z",
    "updated_at": "2025-03-19T18:31:18Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nSimilar to `azure_ad_token_provider`, support an `openai_api_key_provider` for `OpenAI` and `AzureOpenAI` instances. The application scenario is that OpenAI API keys are managed by some OAuth2 token servers, where a request is posted to the token servers and an API key with expiration is granted. In this case, for long running services, each time a request for OpenAI API is made, it is necessary to check and update the cached OpenAI API key. Therefore, I think it would be good to have an `openai_api_key_provider` to manage this situation.\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "smurching",
        "body": "We're also interested in this feature for basically the same reason - I'll try tagging @kwhinnery-openai (please feel free to reroute!) who I saw active on some other PRs in this repo recently. Happy to think through a proposal for the interface of a credential provider if it's something y'all are open to.\r\n\r\nAs a followup (not required initially, can file a followup issue for this), we'd also be interested in making this [pluggable](https://packaging.python.org/en/latest/guides/creating-and-discovering-plugins/) so users in our organization don't have to pass the `openai_api_key_provider` every time, but instead could install a plugin package that automatically provides auth.\r\n\r\nThinking aloud, I imagine the precedence order for resolving auth in the client when doing something like \r\n```\r\nclient = OpenAI(\r\n    api_key=...,\r\n    base_url=\"...,\r\n    http_client=...,\r\n    openai_api_key_provider=...,\r\n)\r\n```\r\n\r\ncould then look like:\r\n* [current behavior] Any authorization headers set in the `http_client` arg, if this arg is provided\r\n* [current behavior] The value of the `api_key`, if provided\r\n* [current behavior] OpenAI environment variables (OPENAI_API_KEY etc)\r\n* Auth explicitly specified in `openai_api_key_provider`\r\n* Auth automatically provided by plugins, if `api_key`, `http_client`, `openai_api_key_provider` are not passed\r\n\r\n\r\n"
      },
      {
        "user": "pamelafox",
        "body": "I'm able to create something like this by using the api_key parameter/property, since that actually sets a Bearer header behind the scenes.\n\n\n```\nimport logging\nimport os\n\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\nload_dotenv(override=True)\nlogging.basicConfig(level=logging.DEBUG)\n\ncredential = DefaultAzureCredential()\nopenai_token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n\nazure_openai_service = os.environ[\"AZURE_OPENAI_SERVICE\"]\ndeployment_name = os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n\nclient = OpenAI(\n    base_url=f\"https://{azure_openai_service}.openai.azure.com/openai/deployments/{deployment_name}\",\n    api_key=openai_token_provider(),\n    default_query={\"api-version\": os.environ[\"AZURE_OPENAI_API_VERSION\"]},\n)\n\n# We must set the api_key each time we issue a new request, to make sure our token is refreshed as needed\nclient.api_key = openai_token_provider()\nresult = client.chat.completions.create(\n    model=None, # this must be specified, but doesn't need a value\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the capital of the United States?\",\n        },\n    ])\n```\n"
      }
    ]
  },
  {
    "issue_number": 2227,
    "title": "Incorrect logprobs distribution",
    "author": "giorgiodemarchi",
    "state": "closed",
    "created_at": "2025-03-19T14:21:29Z",
    "updated_at": "2025-03-19T15:04:21Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nNot sure if this is a library issue, but I started seeing a logprobs error in the ChatCompletion endpoint. When `top_logprobs=True` with Structured Outputs, \n\nI see the following \n```\nChatCompletionTokenLogprob(\n    token='\":', \n    bytes=[34, 58], \n    logprob=0.0, \n    top_logprobs=[\n            TopLogprob(token='\":', bytes=[34, 58], logprob=0.0), \n            TopLogprob(token='\"', bytes=[34], logprob=-19.125), \n            TopLogprob(token='\":\\n\\n', bytes=[34, 58, 10, 10], logprob=-20.375), \n            TopLogprob(token='\":\\n', bytes=[34, 58, 10], logprob=-24.3125), \n            TopLogprob(token='\":\\r\\n', bytes=[34, 58, 13, 10], logprob=-26.4375), \n            TopLogprob(token='\"\\n\\n', bytes=[34, 10, 10], logprob=-27.25), \n            TopLogprob(token='\"\\n', bytes=[34, 10], logprob=-31.0625), \n            TopLogprob(token='\"\\n\\n\\n', bytes=[34, 10, 10, 10], logprob=-35.09375), \n            TopLogprob(token='\"\\r\\n', bytes=[34, 13, 10], logprob=-35.296875), \n            TopLogprob(token='\"\\n\\n\\n\\n', bytes=[34, 10, 10, 10, 10], logprob=-36.078125), \n            TopLogprob(token='\"\\r\\n\\r\\n', bytes=[34, 13, 10, 13, 10], logprob=-36.4453125), \n            TopLogprob(token='!', bytes=[33], logprob=-100.0), \n            TopLogprob(token='#', bytes=[35], logprob=-100.0), \n            TopLogprob(token='$', bytes=[36], logprob=-100.0), \n            TopLogprob(token='%', bytes=[37], logprob=-100.0), \n            TopLogprob(token='&', bytes=[38], logprob=-100.0), \n            TopLogprob(token=\"'\", bytes=[39], logprob=-100.0), \n            TopLogprob(token='(', bytes=[40], logprob=-100.0), \n            TopLogprob(token=')', bytes=[41], logprob=-100.0), \n            TopLogprob(token='*', bytes=[42], logprob=-100.0)\n        ]\n    ), \nChatCompletionTokenLogprob(\n    token='10', \n    bytes=[49, 48], \n    logprob=-9999.0, \n    top_logprobs=[\n        TopLogprob(token='\":', bytes=[34, 58], logprob=0.0), \n        TopLogprob(token='\"', bytes=[34], logprob=-19.125), \n        TopLogprob(token='\":\\n\\n', bytes=[34, 58, 10, 10], logprob=-20.375), \n        TopLogprob(token='\":\\n', bytes=[34, 58, 10], logprob=-24.3125), \n        TopLogprob(token='\":\\r\\n', bytes=[34, 58, 13, 10], logprob=-26.4375), \n        TopLogprob(token='\"\\n\\n', bytes=[34, 10, 10], logprob=-27.25), \n        TopLogprob(token='\"\\n', bytes=[34, 10], logprob=-31.0625), \n        TopLogprob(token='\"\\n\\n\\n', bytes=[34, 10, 10, 10], logprob=-35.09375), \n        TopLogprob(token='\"\\r\\n', bytes=[34, 13, 10], logprob=-35.296875), \n        TopLogprob(token='\"\\n\\n\\n\\n', bytes=[34, 10, 10, 10, 10], logprob=-36.078125), \n        TopLogprob(token='\"\\r\\n\\r\\n', bytes=[34, 13, 10, 13, 10], logprob=-36.4453125), \n        TopLogprob(token='!', bytes=[33], logprob=-100.0), \n        TopLogprob(token='#', bytes=[35], logprob=-100.0), \n        TopLogprob(token='$', bytes=[36], logprob=-100.0), \n        TopLogprob(token='%', bytes=[37], logprob=-100.0), \n        TopLogprob(token='&', bytes=[38], logprob=-100.0), \n        TopLogprob(token=\"'\", bytes=[39], logprob=-100.0), \n        TopLogprob(token='(', bytes=[40], logprob=-100.0), \n        TopLogprob(token=')', bytes=[41], logprob=-100.0), \n        TopLogprob(token='*', bytes=[42], logprob=-100.0)\n    ]\n),\n```\n\nThe second token is buggy:\n- The top logprob is not the same as the token (note that temperature is set to 0)\n- the logprobs distribution seems to be accidentally propagated from the previous token (the distribution is the same)\n\nFor context, this is a structured output completion where the second token is set to be of type `int` \n\n### To Reproduce\n\nSettings:\n```\nclient = AsyncOpenAI()\n\nclass EvalRaw(BaseModel):\n    score: int\n    reason: str\n\nresponse = await client.beta.chat.completions.parse(\n            model=\"gpt-4o\",\n            messages=prompt.model_dump(),\n            response_format=EvalRaw,\n            temperature=0,\n            top_logprobs=20,\n            logprobs=True,\n            seed=100,\n)\n```\nNote that this is not a deterministic error, hence hard to reproduce. It happens once in a while, but I started seeing it only in the past week\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\n3.12\n\n### Library version\n\n1.63.2",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!\n\nThis will be an issue with the API and not the SDK as we don't do anything special with logprobs in the `.parse()` method. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 1763,
    "title": "beta.chat.completions.parse returns unhandled ValidationError",
    "author": "marinomaria",
    "state": "open",
    "created_at": "2024-09-30T15:05:43Z",
    "updated_at": "2025-03-18T15:52:55Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nIn some occasions while using the Completion API with Structured Outputs, the SDK fails and returns a `ValidationError`:\r\n\r\n```\r\nValidationError: 1 validation error for RawResponse\r\n  Invalid JSON: EOF while parsing a value at line 1 column 600 [type=json_invalid, input_value='                        ...                       ', input_type=str]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/json_invalid\r\n```\r\n\r\nThis **does not happen every time**, but we use it in a production service and this unpredictable behavior is hard to prevent.\n\n### To Reproduce\n\n1. Create a Pydantic model\r\n2. Instantiate an OpenAI client\r\n3. Use the method `OpenAI.beta.chat.completions.parse(...)` with the following arguments\r\n4. Repeat a few times for seeing the error\r\n\r\n```python\r\nfrom pydantic import BaseModel\r\nfrom openai import OpenAI\r\n\r\nclass RawResponse(BaseModel):\r\n    answer: str\r\n\r\nclient = OpenAI(api_key=...)\r\ncompletion = client.beta.chat.completions.parse(\r\n                        model='gpt-4o-2024-08-06',\r\n                        messages=messages,\r\n                        max_tokens=750,\r\n                        n=1,\r\n                        stop=None,\r\n                        temperature=0.1,\r\n                        response_format=RawResponse\r\n                    )\r\n```\r\n\r\nAfter a few times, this fails with:\r\n```\r\nValidationError: 1 validation error for RawResponse\r\n  Invalid JSON: EOF while parsing a value at line 1 column 600 [type=json_invalid, input_value='                        ...                       ', input_type=str]\r\n    For further information visit https://errors.pydantic.dev/2.9/v/json_invalid\r\n```\n\n### Code snippets\n\n_No response_\n\n### OS\n\ndebian:bullseye-slim\n\n### Python version\n\nCPython 3.10.8\n\n### Library version\n\nopenai 1.48.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report, it looks like your example script isn't fully complete, could you share a full script?"
      },
      {
        "user": "marinomaria",
        "body": "Hi, thanks for the quick reply! Sadly I can't provide a full script for privacy reasons but I'll be happy to share any information you need for identifying the issue. \r\nHere's the traceback:\r\n```\r\nFile \"/app/src/core/modules/emiGPT/core/openai_chat_api.py\", line 95, in _get_response\t\r\n  completion = self._client.beta.chat.completions.parse(\t\r\nFile \"/opt/venv/lib/python3.10/site-packages/openai/resources/beta/chat/completions.py\", line 145, in parse\t\r\n  return _parse_chat_completion(\t\r\nFile \"/opt/venv/lib/python3.10/site-packages/openai/lib/_parsing/_completions.py\", line 110, in parse_chat_completion\t\r\n  \"parsed\": maybe_parse_content(\t\r\nFile \"/opt/venv/lib/python3.10/site-packages/openai/lib/_parsing/_completions.py\", line 161, in maybe_parse_content\t\r\n  return _parse_content(response_format, message.content)\t\r\nFile \"/opt/venv/lib/python3.10/site-packages/openai/lib/_parsing/_completions.py\", line 221, in _parse_content\t\r\n  return cast(ResponseFormatT, model_parse_json(response_format, content))\t\r\nFile \"/opt/venv/lib/python3.10/site-packages/openai/_compat.py\", line 166, in model_parse_json\t\r\n  return model.model_validate_json(data)\t\r\nFile \"/opt/venv/lib/python3.10/site-packages/pydantic/main.py\", line 625, in model_validate_json\t\r\n  return cls.__pydantic_validator__.validate_json(json_data, strict=strict, context=context)\t\r\npydantic_core._pydantic_core.ValidationError: 1 validation error for RawResponse\t\r\n  Invalid JSON: EOF while parsing a value at line 1 column 600 [type=json_invalid, input_value='                        ...                       ', input_type=str]\t\r\n    For further information visit https://errors.pydantic.dev/2.9/v/json_invalid\r\n```\r\n\r\nPlease let me know if there's anything else you need.\r\n"
      },
      {
        "user": "RobertCraigie",
        "body": "Could you share a request ID from a failing request? https://github.com/openai/openai-python#request-ids"
      }
    ]
  },
  {
    "issue_number": 2204,
    "title": "Response constructed to wrong type in discriminated union when data doesn't match exactly",
    "author": "kristapratico",
    "state": "closed",
    "created_at": "2025-03-14T21:14:45Z",
    "updated_at": "2025-03-18T10:29:19Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen response data is constructed into a model, if the data doesn't match a type defined in the union exactly, then the code attempts to build the correct model based on the discriminator value. During this process, the code extracts the field schema from each model in the union:\n\nhttps://github.com/openai/openai-python/blob/9dea82fb8cdd06683f9e8033b54cff219789af7f/src/openai/_models.py#L672-L687\n\nIn some cases, when accessing the `__pydantic_core_schema__` of the model, it is found that a [DefinitionsSchema](https://github.com/pydantic/pydantic-core/blob/ac17f0c92f1d8fe0cad895dce08f87b9103ba06a/python/pydantic_core/core_schema.py#L3919) is returned instead of a ModelSchema. This excludes the given type from consideration on L674-675, even though it is a valid type in the union and may have the matching discriminator value. The below code repros the issue and how it can lead to the wrong type being constructed.\n\n### To Reproduce\n\n```python\nfrom typing import Any, Union, cast, List\nfrom typing_extensions import Literal, Annotated\n\nfrom openai._models import BaseModel, construct_type\nfrom openai._utils import PropertyInfo\n\n\n# this is a generalization of how Filters is defined on the FileSearchTool (https://github.com/openai/openai-python/blob/main/src/openai/types/responses/file_search_tool.py#L34)\n\n\nclass A(BaseModel):\n    type: Literal[\"a\"]\n\n    data: bool\n\nclass B(BaseModel):\n    type: Literal[\"b\"]\n\n    data: List[Union[A, object]]\n\nclass ModelA(BaseModel):\n    type: Literal[\"modelA\"]\n\n    data: int\n\nclass ModelB(BaseModel):\n    type: Literal[\"modelB\"]\n\n    required: str\n\n    data: Union[A, B]\n\nma = construct_type(value={\"type\": \"modelA\", \"data\": 1}, type_=ModelA)\nassert isinstance(ma, ModelA)\nschema_a = ma.__pydantic_core_schema__\nassert schema_a[\"type\"] == \"model\"\n\n# ModelB builds a DefinitionsSchema instead of a ModelSchema\nmb = construct_type(\n    value={\n        \"type\": \"modelB\",\n        \"required\": \"foo\",\n        \"data\": {\"type\": \"a\", \"data\": True},\n    },\n    type_=ModelB,\n)\nassert isinstance(mb, ModelB)\nschema_b = mb.__pydantic_core_schema__\nassert schema_b[\"type\"] == \"definitions\"\nmodel_schema = schema_b[\"schema\"]\nassert model_schema[\"type\"] == \"model\"\n\n\n# when constructing ModelA | ModelB, value data doesn't match ModelB exactly - missing `required`\nm = construct_type(\n    value={\"type\": \"modelB\", \"data\": {\"type\": \"a\", \"data\": True}},\n    type_=cast(Any, Annotated[Union[ModelA, ModelB], PropertyInfo(discriminator=\"type\")]),\n)\n\n# AssertionError: Expected ModelB but got ModelA\nassert isinstance(m, ModelB), f\"Expected ModelB but got {type(m).__name__}\"\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nWSL\n\n### Python version\n\n3.12\n\n### Library version\n\n1.66.3",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "thanks for the detailed investigation! will have a fix out soon :)"
      },
      {
        "user": "RobertCraigie",
        "body": "This will be fixed in the next release https://github.com/openai/openai-python/pull/2196 :)"
      },
      {
        "user": "kristapratico",
        "body": "Thanks for the quick fix @RobertCraigie! "
      }
    ]
  },
  {
    "issue_number": 2190,
    "title": "Computer Use Agent -- allow \"Action\" to be more flexible",
    "author": "rahuls1996",
    "state": "closed",
    "created_at": "2025-03-13T03:02:03Z",
    "updated_at": "2025-03-18T08:56:04Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nIs there a way to provide custom actions to CUA? Or have custom fields populated in say a returned ActionClick? \n\nFor context, we want to figure out what \"element\" the model is clicking on, and then perform some additional logic apart from simply clicking. CUA also doesn't seem to support structured outputs along with the ResponseComputerToolCall as far as I'm aware, and is also not great at calling other tools while executing a task\n\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2189,
    "title": "Computer Use Agent -- allow \"Action\" to be more flexible",
    "author": "rahuls1996",
    "state": "closed",
    "created_at": "2025-03-13T03:00:19Z",
    "updated_at": "2025-03-18T08:55:10Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nIs there a way to provide custom actions to CUA? Or have custom fields populated in say a returned ActionClick? \n\nFor context, we want to figure out what \"element\" the model is clicking on, and then perform some additional logic apart from simply clicking. CUA also doesn't seem to support structured outputs along with the ResponseComputerToolCall as far as I'm aware. \n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "body": "Duplicate of https://github.com/openai/openai-python/issues/2190.\nCould you close this Issue?"
      }
    ]
  },
  {
    "issue_number": 2198,
    "title": "(pyproject.toml) did not run successfully.",
    "author": "mhussam-ai",
    "state": "closed",
    "created_at": "2025-03-13T21:32:09Z",
    "updated_at": "2025-03-18T08:54:21Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n\n  × Preparing metadata (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [6 lines of output]\n\n      Cargo, the Rust package manager, is not installed or is not on PATH.\n      This package requires Rust and Cargo to compile extensions. Install it through\n      the system's package manager or via https://rustup.rs/\n\n      Checking for Rust toolchain....\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n× Encountered error while generating package metadata.\n╰─> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n\n### To Reproduce\n\nFetch: (pyproject.toml) did not run successfully.\n\n### Code snippets\n\n```Python\nfrom openai import OpenAI\n\n# Initialize the client with Gemini API settings\nclient = OpenAI(\n    api_key=\"AIzaSyD_RvBzWEtyaXsgnHSJFArAkWeAYf563Mw\",  # Replace with your actual Gemini API key\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n)\n\n# Create a simple chat completion request\nresponse = client.chat.completions.create(\n    model=\"gemini-2.0-flash\",  # Specify the Gemini 2.0 Flash model\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Say Hello World!\"}\n    ]\n)\n\n# Print the response\nprint(response.choices[0].message.content)\n\n```\n\n### OS\n\nWindows 11\n\n### Python version\n\nlatest\n\n### Library version\n\nlatest",
    "comments": [
      {
        "user": "jasneetsingh6114",
        "body": "Hey can I work on this issue ?\n"
      },
      {
        "user": "RobertCraigie",
        "body": "Sounds like a dependency that requires Rust cannot be built on your system. Please identify which one and ask for help with them directly."
      }
    ]
  },
  {
    "issue_number": 2161,
    "title": "Url problem in AsyncCompletions",
    "author": "gmirc12",
    "state": "closed",
    "created_at": "2025-03-05T13:31:39Z",
    "updated_at": "2025-03-18T08:53:20Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nIn src/openai/resources/chat/completions/completions.py line 1928 in AsyncCompletions.create \n\n(https://github.com/openai/openai-python/blob/d6bb8c14e66605ad2b7ed7bd62951014cd21b576/src/openai/resources/chat/completions/completions.py#L1928)\n\nyou set \"/chat/completions\" as a path variable, but you actually set it as a url, so the client tryes to hit a /chat/completions\nurl without any baseUrl prepended to it.\n\n### To Reproduce\n\nError: openai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n\n### Code snippets\n\n```Python\n\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(\n            temperature=temp,\n            max_tokens=tokens,\n            model=\"model\",\n            api_key=\"key\",\n            api_base=\"baseUrl\",\n            default_headers = {headers}\n        )\nresp = llm.acomplete(\"Test\")\n\n\n#OR\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n            api_key=\"key\",\n            base_url=\"url\",\n            default_headers = {headers}\n        )\nresponse = client.chat.completions.create(\n          model=\"model\",\n          n=1,\n          messages=[messages]\n)\n\n```\n\n### OS\n\nWSL\n\n### Python version\n\nv3.12\n\n### Library version\n\n1.65.3",
    "comments": [
      {
        "user": "abhinav-sreekumar",
        "body": "Hi, I’d like to work on this issue. I’ll start investigating and update here if I have any questions.\n"
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report but I cannot reproduce this. Sounds like llama index is patching the client incorrectly?"
      }
    ]
  },
  {
    "issue_number": 231,
    "title": "openai.Completion.retrieve()",
    "author": "hend41234",
    "state": "closed",
    "created_at": "2023-02-28T14:32:39Z",
    "updated_at": "2025-03-17T20:06:51Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\n**EngineAPIResource.__init__() takes from 1 to 2 positional arguments but 3 were given**\n\n### To Reproduce\n\ndoes this only happen to me when using 'retrieve' in openai.\r\ni got this error :  **\"EngineAPIResource.__init__() takes from 1 to 2 positional arguments but 3 are given\"**\r\n\n\n### Code snippets\n\n```Python\nkey = \"my_api_key\"\r\nopenai.api_key=key\r\nid_chat = \"my_completion_id\"\r\ntry:\r\n    result = openai.Completion.retrieve(id_chat)\r\n    print(result['choices'][0]['text'])\r\nexcept Exception as a:\r\n    print(a)\n```\n\n\n### OS\n\narch linux\n\n### Python version\n\nPython 3.10.9\n\n### Library version\n\nopenai-python v0.26.5",
    "comments": [
      {
        "user": "tanpinxi",
        "body": "openai currently does not support retrieving past completions, you should save and manage completion responses client-side. "
      },
      {
        "user": "hallacy",
        "body": "^ is correct. There is no way to retrieve past completions with the API"
      },
      {
        "user": "gnehcgnaw",
        "body": "Will there be support in subsequent versions?\r\nI feel `openai.Completion.retrieve()` is still necessary ."
      }
    ]
  },
  {
    "issue_number": 1903,
    "title": "TypeError: Client.__init__() got an unexpected keyword argument 'proxies'",
    "author": "gregpr07",
    "state": "closed",
    "created_at": "2024-11-28T15:50:44Z",
    "updated_at": "2025-03-16T10:36:56Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\n`SyncHttpxClientWrapper` has hard coded proxies but it no longer exists in httpx 0.28\n\n### To Reproduce\n\n1. `pip install openai`\r\n2. the bug appears when calling `openai.OpenAI(**client_params, **sync_specific)`\n\n### Code snippets\n\n```Python\nTraceback (most recent call last):\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/examples/try.py\", line 45, in <module>\r\n    llm = get_llm(args.provider)\r\n          ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/examples/try.py\", line 28, in get_llm\r\n    return ChatOpenAI(model='gpt-4o', temperature=0.0)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/langchain_core/load/serializable.py\", line 125, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/pydantic/main.py\", line 214, in __init__\r\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 551, in validate_environment\r\n    self.root_client = openai.OpenAI(**client_params, **sync_specific)  # type: ignore[arg-type]\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/openai/_client.py\", line 123, in __init__\r\n    super().__init__(\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 857, in __init__\r\n    self._client = http_client or SyncHttpxClientWrapper(\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/greg/Documents/Work/browser-use/browser-use/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 755, in __init__\r\n    super().__init__(**kwargs)\r\nTypeError: Client.__init__() got an unexpected keyword argument 'proxies'\n```\n\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.11.4\n\n### Library version\n\nopenai 1.55.2",
    "comments": [
      {
        "user": "simonw",
        "body": "Same issue:\r\n- #1902 "
      },
      {
        "user": "dgellow",
        "body": "Thanks for the report, as mentioned by @simonw this is similar to #1902 and will be fixed by https://github.com/openai/openai-python/issues/1903. I will close this issue as duplicate."
      },
      {
        "user": "stainless-app[bot]",
        "body": "duplicate"
      }
    ]
  },
  {
    "issue_number": 2199,
    "title": "Realtime Session Update Configuration",
    "author": "anishnag",
    "state": "open",
    "created_at": "2025-03-13T23:20:40Z",
    "updated_at": "2025-03-14T20:35:05Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe intended behavior is to disable server-side VAD for the OpenAI Realtime model. We are using LiveKit to facilitate the websocket connection, but the bug is in the OpenAI library.\n\nIn particular, the `openai.resources.beta.realtime.AsyncRealtimeConnection.send` method uses `event.to_json(use_api_names=True, exclude_defaults=True, exclude_unset=True)` to serialize the `SessionUpdateEvent`. The issue is with the `exclude_defaults=True` parameter which doesn't include any values that are equal to their default values.\n\nWe have confirmed the two serial `SessionUpdateEvent`s get composed, so a change from the first event is reflected in the resulting configuration of the second event. This makes the `exclude_defaults=True` argument particularly problematic because there is now no way to ever change a default and then change it back.\n\nThere are a couple of problems here. For VAD in particular, despite the default value of `turn_detection=None` in `Session(BaseModel)`, it is in fact not `None` and instead some default server-side VAD values. When you try to pass `None` in the `SessionUpdateEvent` you can't change the `turn_detection` value because (1) `exclude_defaults=True` prevents you and (2) the default value is inconsistent with what actually exists by default.\n\nThere are two solutions:\n- Remove `exclude_defaults=True`\n- Update the default `turn_detection` in `Session`\n\n### To Reproduce\n\nPlease follow the steps below.\n\n### Code snippets\n\nWhen running `python minimal_worker.py console` using LiveKit agents on branch `dev-1.0` with the following model configuration:\n```\nagent = VoiceAgent(\n    instructions=\"You are a helpful assistant that can answer questions and help with tasks.\",\n    llm=openai.realtime.RealtimeModel(\n        model=\"gpt-4o-realtime-preview-2024-12-17\",\n        voice=\"alloy\"\n    )\n)\n```\n\nThen, within the `_main_task` of `RealtimeSession`, we hardcode the `turn_detection=None` parameter as follows:\n```\nself._msg_ch.send_nowait(\n    SessionUpdateEvent(\n        type=\"session.update\",\n        session=session_update_event.Session(\n            model=self._realtime_model._opts.model,  # type: ignore\n            voice=self._realtime_model._opts.voice,  # type: ignore\n            input_audio_transcription=input_audio_transcription,\n            turn_detection=None\n        ),\n        event_id=utils.shortuuid(\"session_update_\"),\n    )\n)\n```\n\nThe issue here is that `turn_detection` never gets updated properly according to the `SessionUpdatedEvent`. This is related to the problem that [this](https://github.com/livekit/agents/pull/1639) PR was attempting to solve.\n\nFor example, we get:\n- The `SessionCreatedEvent` with the default `turn_detector`. By the way, even after passing the query param to the websocket uri `turn_detector=` for a null value, it still returns with server-side VAD.\n```\nSessionCreatedEvent(..., turn_detection=TurnDetection(create_response=True, interrupt_response=True, prefix_padding_ms=300, silence_duration_ms=200, threshold=0.5, type='server_vad'), ...) type='session.created')\n```\n- After passing the `turn_detector=None` argument to the `SessionUpdateEvent` as mentioned above, we still eventually observe the `SessionUpdatedEvent`.\n```\nSessionUpdatedEvent(..., turn_detection=TurnDetection(create_response=True, interrupt_response=True, prefix_padding_ms=300, silence_duration_ms=200, threshold=0.5, type='server_vad'), ...), type='session.updated')\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.13\n\n### Library version\n\nopenai v1.66.3",
    "comments": []
  },
  {
    "issue_number": 1054,
    "title": "Invalid port error",
    "author": "ZhangYuanhan-AI",
    "state": "closed",
    "created_at": "2024-01-08T10:00:05Z",
    "updated_at": "2025-03-13T03:56:16Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nHi, When I run the following code, one error raises, any thoughts?\r\n```\r\nfrom openai import OpenAI\r\nclient = OpenAI(api_key=\"XXXXX\")\r\n```\r\nRaising error:\r\n```\r\nTraceback (most recent call last):                                                                                                    \r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/httpx/_urlparse.py\", line 339, in normalize_port\r\n    port_as_int = int(port)                                                                                                           \r\nValueError: invalid literal for int() with base 10: ':1'                                                                              \r\n                                                                                                                                      \r\nDuring handling of the above exception, another exception occurred: \r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/openai/_client.py\", line 106, in __init__\r\n    super().__init__(\r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/openai/_base_client.py\", line 758, in __init__\r\n    self._client = http_client or SyncHttpxClientWrapper(\r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/httpx/_client.py\", line 682, in __init__\r\n    self._mounts: typing.Dict[URLPattern, typing.Optional[BaseTransport]] = {\r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/httpx/_client.py\", line 683, in <dictcomp>\r\n    URLPattern(key): None\r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/httpx/_utils.py\", line 397, in __init__\r\n    url = URL(pattern)\r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/httpx/_urls.py\", line 113, in __init__\r\n    self._uri_reference = urlparse(url, **kwargs)\r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/httpx/_urlparse.py\", line 246, in urlparse\r\n    parsed_port: typing.Optional[int] = normalize_port(port, scheme)\r\n  File \"/home/tiger/.local/lib/python3.9/site-packages/httpx/_urlparse.py\", line 341, in normalize_port\r\n    raise InvalidURL(\"Invalid port\")\r\nhttpx.InvalidURL: Invalid port\r\n```\n\n### To Reproduce\n\nfrom openai import OpenAI\r\nclient = OpenAI(api_key=\"XXXXX\")\n\n### Code snippets\n\n_No response_\n\n### OS\n\nunbuntu\n\n### Python version\n\npython 3.9\n\n### Library version\n\nopenai 1.6.0",
    "comments": [
      {
        "user": "rattrayalex",
        "body": "What is the value of your `OPENAI_BASE_URL` environment variable, or that of other related env vars?"
      },
      {
        "user": "ssilvia29",
        "body": "Hi were you able to fix this?"
      },
      {
        "user": "ycjcl868",
        "body": "downgrade httpx from 0.27.0 to 0.23.3 can fix the problem. Ref: https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/10149"
      }
    ]
  },
  {
    "issue_number": 2183,
    "title": "'OpenAI' object has no attribute 'responses'",
    "author": "Peccer",
    "state": "closed",
    "created_at": "2025-03-12T11:21:05Z",
    "updated_at": "2025-03-12T14:01:13Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nRunning in Google Colab. Updated to newest 1.66.2 library. Getting a \"AttributeError: 'OpenAI' object has no attribute 'responses'\" using an exact copy of the documentation example\n\n\n### To Reproduce\n\nOpen notebook in google colab\nInstall new library 1.66.2\nfollow documentation example from https://platform.openai.com/docs/api-reference/responses/create\n\n### Code snippets\n\n```Python\n!openai --version\nopenai 1.66.2\n\nfrom openai import OpenAI\nimport os\n\napi_key = userdata.get('OPENAI_API_KEY')\n\nclient = OpenAI(api_key=api_key)\n\nresponse = client.responses.create(\n    model=\"gpt-4o-mini\",\n    tools=[{ \"type\": \"web_search_preview\" }],\n    input=\"What was a positive news story from today?\",\n)\n\nprint(response)\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n[<ipython-input-25-8b2de658b70d>](https://localhost:8080/#) in <cell line: 0>()\n----> 1 response = client.responses.create(\n      2     model=\"gpt-4o-mini\",\n      3     tools=[{ \"type\": \"web_search_preview\" }],\n      4     input=\"What was a positive news story from today?\",\n      5 )\n\nAttributeError: 'OpenAI' object has no attribute 'responses'\n\n\n\nCheck which attributes are available in the client, responses is not available.\n\n\n('api_key', 'sk-p')\n('audio', <openai.resources.audio.audio.Audio object at 0x79093aa4fb50>)\n('auth_headers', {'Authorization': 'Bearer sk-p'})\n('base_url', URL('https://api.openai.com/v1/'))\n('batches', <openai.resources.batches.Batches object at 0x79093aa4fd50>)\n('beta', <openai.resources.beta.beta.Beta object at 0x79093aa4fd90>)\n('chat', <openai.resources.chat.chat.Chat object at 0x79097ef85fd0>)\n('completions', <openai.resources.completions.Completions object at 0x79097ef85a10>)\n('custom_auth', None)\n('default_headers', {'Accept': 'application/json', 'Content-Type': 'application/json', 'User-Agent': 'OpenAI/Python 1.61.1', 'X-Stainless-Lang': 'python', 'X-Stainless-Package-Version': '1.61.1', 'X-Stainless-OS': 'Linux', 'X-Stainless-Arch': 'x64', 'X-Stainless-Runtime': 'CPython', 'X-Stainless-Runtime-Version': '3.11.11', 'Authorization': 'Bearer sk-p', 'X-Stainless-Async': 'false', 'OpenAI-Organization': <openai.Omit object at 0x79093afbb7d0>, 'OpenAI-Project': <openai.Omit object at 0x79093a945850>})\n('default_query', {})\n('embeddings', <openai.resources.embeddings.Embeddings object at 0x79097ef85c90>)\n('files', <openai.resources.files.Files object at 0x79097ef84290>)\n('fine_tuning', <openai.resources.fine_tuning.fine_tuning.FineTuning object at 0x79093aa4f1d0>)\n('images', <openai.resources.images.Images object at 0x79093aa4fc10>)\n('max_retries', 2)\n('models', <openai.resources.models.Models object at 0x79093aa4de10>)\n('moderations', <openai.resources.moderations.Moderations object at 0x79093aa4cf90>)\n('organization', None)\n('project', None)\n('qs', <openai._qs.Querystring object at 0x79093a9dc5d0>)\n('timeout', Timeout(connect=5.0, read=600, write=600, pool=600))\n('uploads', <openai.resources.uploads.uploads.Uploads object at 0x79093aa4fd10>)\n('user_agent', 'OpenAI/Python 1.61.1')\n('websocket_base_url', None)\n('with_raw_response', <openai._client.OpenAIWithRawResponse object at 0x79093aa4fe10>)\n('with_streaming_response', <openai._client.OpenAIWithStreamedResponse object at 0x79093aa4ed90>)\n```\n\n### OS\n\nLinux 3a991ab24ecd 6.1.85+ #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n\n### Python version\n\n3.11.11\n\n### Library version\n\n1.66.2",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "You need to reload the notebook after installing a new version."
      }
    ]
  },
  {
    "issue_number": 2143,
    "title": "OpenAI returns null stream",
    "author": "GalDayan",
    "state": "closed",
    "created_at": "2025-02-24T17:44:54Z",
    "updated_at": "2025-03-11T18:08:57Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nHi - we have this code below (Using Azure Open AI)\nI didn't find any documentation or open bugs about it. \nWhen `chunks_stream` is None? in what situation?\nHow can we avoid it?\n\nNOTE: We are not near the rate limitations\n```\nchunks_stream = await self._client.chat.completions.create(\n                model=self._model_name,\n                messages=typing.cast(\n                    typing.Iterable[ChatCompletionMessageParam], messages\n                ),\n                stream=True,\n                **completion_config,\n            )\n\n            if chunks_stream is None:\n                raise Exception(\n                    f\"OpenAI returned a null stream. Model: {self._model_name}\"\n                )\n```\n\n### To Reproduce\n\n```\nchunks_stream = await self._client.chat.completions.create(\n                model=self._model_name,\n                messages=typing.cast(\n                    typing.Iterable[ChatCompletionMessageParam], messages\n                ),\n                stream=True,\n                **completion_config,\n            )\n\n            if chunks_stream is None:\n                raise Exception(\n                    f\"OpenAI returned a null stream. Model: {self._model_name}\"\n                )\n```\n\n### Code snippets\n\n```Python\nchunks_stream = await self._client.chat.completions.create(\n                model=self._model_name,\n                messages=typing.cast(\n                    typing.Iterable[ChatCompletionMessageParam], messages\n                ),\n                stream=True,\n                **completion_config,\n            )\n\n            if chunks_stream is None:\n                raise Exception(\n                    f\"OpenAI returned a null stream. Model: {self._model_name}\"\n                )\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.11.5\n\n### Library version\n\nopenai v1.62",
    "comments": [
      {
        "user": "OmriBenShoham",
        "body": "Also having the same issue"
      },
      {
        "user": "kristapratico",
        "body": "@GalDayan @OmriBenShoham I'm not able to reproduce this yet.\n\n- Is there a way to reproduce this consistently or does it happen sporadically?\n- Since this occurs with Azure OpenAI, can you share what API version are you targeting, which model, and what region the resource is created in? Do you have any custom configuration added to the resource (e.g. custom content filtering)?\n- Could you share a request-id so the Azure service team can investigate?\n- For the chat completions call itself, what is being passed with the `completion_config`?\n"
      },
      {
        "user": "uilfdj",
        "body": "สวัสดี"
      }
    ]
  },
  {
    "issue_number": 2172,
    "title": "No access to openai.Chatcompletion",
    "author": "penguinrs",
    "state": "closed",
    "created_at": "2025-03-08T17:18:25Z",
    "updated_at": "2025-03-10T12:36:24Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nIt keeps saying this You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n\n### To Reproduce\n\nDownload openai\nput the code below \n\n### Code snippets\n\n```Python\nimport os\nfrom dotenv import load_dotenv\nimport openai\n\nload_dotenv()  # Ensure .env is in the same directory\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise ValueError(\"API key not found.\")\n\ntry:\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n            {\"role\": \"user\", \"content\": \"Hello, AI!\"}\n        ],\n        max_tokens=50,\n        temperature=0.7\n    )\n    print(\"AI Response:\", response[\"choices\"][0][\"message\"][\"content\"].strip())\nexcept Exception as e:\n    print(\"Error:\", e)\n```\n\n### OS\n\nWindow OS\n\n### Python version\n\nPython 3.12.6\n\n### Library version\n\nOpenAI version: 1.65.4",
    "comments": [
      {
        "user": "TaylorN15",
        "body": "Did you look through the discussion linked? https://github.com/openai/openai-python/discussions/742\n\nThere were many things changes/renamed in the v1.0 release, including `ChatCompletion`.\n\n`openai.ChatCompletion.create() -> client.chat.completions.create()`"
      },
      {
        "user": "RobertCraigie",
        "body": "Please read the link in the error message."
      }
    ]
  },
  {
    "issue_number": 2171,
    "title": "This is a feature request for the Python library",
    "author": "bharat2808",
    "state": "closed",
    "created_at": "2025-03-07T08:36:08Z",
    "updated_at": "2025-03-07T12:54:40Z",
    "labels": [],
    "body": null,
    "comments": []
  },
  {
    "issue_number": 2170,
    "title": "deleting previous coversation history for realtime socket connection",
    "author": "bharat2808",
    "state": "closed",
    "created_at": "2025-03-07T08:35:41Z",
    "updated_at": "2025-03-07T12:54:29Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] #2171\n\n### Describe the feature or improvement you're requesting\n\nHello, I am using realtime beta api. I am adding pre recorded audio to conversation via self.connection.conversation.item.create method. the assistant misunderstood my audio and replied in Spanish for several time and now because most of the previous conversation it has in its memory is Spanish, it always replies in Spanish unless I explicitly ask for english. I tried disconnecting the websocket and connect again but no effect. it acts the same. any way to force discard the previous chat history. \n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2072,
    "title": "CLI erroneously sends unsupported parameters (temperature/top_p) to the o3-mini model",
    "author": "MrJarnould",
    "state": "closed",
    "created_at": "2025-02-02T07:16:07Z",
    "updated_at": "2025-03-06T06:31:53Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nUsing `openai api chat.completions.create` with the newly released `o3-mini-2025-01-31` model triggers errors about unsupported parameters even when those parameters are not explicitly set in the CLI command. Specifically, `temperature` and `top_p` appear to be sent to the API, causing 400 errors.\n\nHowever, if you set `--temperature \"1\" ` and `--top_p \"1\"`, then no error is produced and a chat response is obtained.\n\n> Note: installed `openai` using `pipx v1.7.1`\n\n### To Reproduce\n\n1. Use the CLI without specifying `--temperature` nor `--top_p`:\n\n```bash\n❯ openai api chat.completions.create --message \"user\" \"What's the capital of France?\" -m \"o3-mini-2025-01-31\"\nError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'temperature' is not supported with this model.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_parameter'}}\n```\n\n2. Confirm the same request works using cURL:\n\n```bash\n❯ curl \\\n  https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"o3-mini-2025-01-31\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What'\\''s the capital of France?\"\n          }\n        ]\n      }\n    ],\n    \"response_format\": {\n      \"type\": \"text\"\n    },\n    \"reasoning_effort\": \"low\"\n  }'\n```\n\n```json\n{\n  \"id\": \"chatcmpl-AwNe73vdAtaqbpdecVpG3XNRwfLbt\",\n  \"object\": \"chat.completion\",\n  \"created\": 1738477283,\n  \"model\": \"o3-mini-2025-01-31\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"The capital of France is Paris.\",\n        \"refusal\": null\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 12,\n    \"completion_tokens\": 17,\n    \"total_tokens\": 29,\n    \"prompt_tokens_details\": {\n      \"cached_tokens\": 0,\n      \"audio_tokens\": 0\n    },\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\": 0\n    }\n  },\n  \"service_tier\": \"default\",\n  \"system_fingerprint\": \"fp_8bcaa0ca21\"\n}\n```\n\n3. Observe that the cURL command fails when setting `\"temperature\": 0.5`:\n\n```bash\n❯ curl \\\n  https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"o3-mini-2025-01-31\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What'\\''s the capital of France?\"\n          }\n        ]\n      }\n    ],\n    \"response_format\": {\n      \"type\": \"text\"\n    },\n    \"temperature\": 0.5,\n    \"reasoning_effort\": \"low\"\n  }'\n```\n\n```json\n{\n  \"error\": {\n    \"message\": \"Unsupported parameter: 'temperature' is not supported with this model.\",\n    \"type\": \"invalid_request_error\",\n    \"param\": \"temperature\",\n    \"code\": \"unsupported_parameter\"\n  }\n}\n```\n\n4. Observe that the cURL command works when setting `\"temperature\": 1`:\n\n```bash\n❯ curl \\\n  https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n    \"model\": \"o3-mini-2025-01-31\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What'\\''s the capital of France?\"\n          }\n        ]\n      }\n    ],\n    \"response_format\": {\n      \"type\": \"text\"\n    },\n    \"temperature\": 1,\n    \"reasoning_effort\": \"low\"\n  }'\n```\n\n```json\n{\n  \"id\": \"chatcmpl-AwNYOeW1akjt6dMLkH3UNSrdXk5tV\",\n  \"object\": \"chat.completion\",\n  \"created\": 1738476928,\n  \"model\": \"o3-mini-2025-01-31\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"The capital of France is Paris.\",\n        \"refusal\": null\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 12,\n    \"completion_tokens\": 17,\n    \"total_tokens\": 29,\n    \"prompt_tokens_details\": {\n      \"cached_tokens\": 0,\n      \"audio_tokens\": 0\n    },\n    \"completion_tokens_details\": {\n      \"reasoning_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\": 0\n    }\n  },\n  \"service_tier\": \"default\",\n  \"system_fingerprint\": \"fp_8bcaa0ca21\"\n}\n```\n\n5. Observe the CLI still fails when setting `--temperature \"1\"`, now with a new `\"Unsupported parameter: 'top_p' is not supported with this model.\"` error:\n\n```bash\n❯ openai api chat.completions.create --message \"user\" \"What's the capital of France?\" -m \"o3-mini-2025-01-31\" --temperature \"1\"\nError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'top_p' is not supported with this model.\", 'type': 'invalid_request_error', 'param': 'top_p', 'code': 'unsupported_parameter'}}\n```\n\n6. Observe that the CLI request works when setting ` --temperature \"1\" --top_p \"1\"`:\n\n```bash\n❯ openai api chat.completions.create --message \"user\" \"What's the capital of France?\" -m \"o3-mini-2025-01-31\" --temperature \"1\" --top_p \"1\"\nThe capital of France is Paris.\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.12.5\n\n### Library version\n\nopenai 1.61.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report! This will be fixed in the next release https://github.com/openai/openai-python/pull/2078"
      },
      {
        "user": "csjcoderepo",
        "body": "Thank you!!"
      },
      {
        "user": "fateme-hshm96",
        "body": "Hello. I still get the same error when setting the temperature with o3-mini. Do you have an estimate of when this might get fixed?\nThank you!"
      }
    ]
  },
  {
    "issue_number": 2162,
    "title": "Assistants API: Unexpected `tool_call` type in `on_tool_call_created`",
    "author": "ekassos",
    "state": "open",
    "created_at": "2025-03-05T15:25:16Z",
    "updated_at": "2025-03-05T16:23:04Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen calling our implementation of method `on_tool_call_created` of `AsyncAssistantEventHandler`, `tool_call` is of type `Dict` and not of type `ToolCall`. In our testing, this issue occurs only when the tool call created is File Search and not Code Interpreter (we do not use Function Calling). The issue presents itself only when using `AsyncOpenAI` and not `AsyncAzureOpenAI`.\n\nSee definition of `on_tool_call_created`:\nhttps://github.com/openai/openai-python/blob/d6bb8c14e66605ad2b7ed7bd62951014cd21b576/src/openai/lib/streaming/_assistants.py#L186-L187\n\n### To Reproduce\n\n1. Implement `on_tool_call_created` as\n```python\nasync def on_tool_call_created(self, tool_call) -> None:\n    self.enqueue(\n        {\n            \"type\": \"tool_call_created\",\n            \"tool_call\": tool_call.model_dump(),\n        }\n    )\n```\n2. When a File Search tool call is created, on_tool_call_created will fail with error `'dict' object has no attribute 'model_dump'`.\n\n### Code snippets\n\n```Python\nclass BufferedStreamHandler(openai.AsyncAssistantEventHandler):\n    def __init__(self, file_names: dict[str, str], *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.__buffer = io.BytesIO()\n        self.file_names = file_names\n\n    def enqueue(self, data: Dict) -> None:\n        self.__buffer.write(orjson.dumps(data))\n        self.__buffer.write(b\"\\n\")\n\n    def flush(self) -> bytes:\n        value = self.__buffer.getvalue()\n        self.__buffer.truncate(0)\n        self.__buffer.seek(0)\n        return value\n\n    async def on_image_file_done(self, image_file: ImageFile) -> None:\n        self.enqueue(\n            {\n                \"type\": \"image_file_done\",\n                \"file_id\": image_file.file_id,\n            }\n        )\n\n    async def on_message_created(self, message) -> None:\n        self.enqueue(\n            {\n                \"type\": \"message_created\",\n                \"role\": \"assistant\",\n                \"message\": message.model_dump(),\n            }\n        )\n\n    async def on_message_delta(self, delta, snapshot) -> None:\n        self.enqueue(\n            {\n                \"type\": \"message_delta\",\n                \"delta\": delta.model_dump(),\n            }\n        )\n\n    async def on_tool_call_created(self, tool_call) -> None:\n        self.enqueue(\n            {\n                \"type\": \"tool_call_created\",\n                \"tool_call\": tool_call.model_dump(),\n            }\n        )\n\n    async def on_tool_call_delta(self, delta, snapshot) -> None:\n        self.enqueue(\n            {\n                \"type\": \"tool_call_delta\",\n                \"delta\": delta.model_dump(),\n            }\n        )\n\n    async def on_timeout(self) -> None:\n        self.enqueue(\n            {\n                \"type\": \"error\",\n                \"detail\": \"Stream timed out waiting for response\",\n            }\n        )\n\n    async def on_done(self, run) -> None:\n        self.enqueue({\"type\": \"done\"})\n\n    async def on_exception(self, exception) -> None:\n        self.enqueue(\n            {\n                \"type\": \"error\",\n                \"detail\": str(exception),\n            }\n        )\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython v3.11.10\n\n### Library version\n\nopenai v1.65.3",
    "comments": [
      {
        "user": "gmirc12",
        "body": "Found the problem, had to set a custom query param, you can close this issue"
      },
      {
        "user": "ekassos",
        "body": "> Found the problem, had to set a custom query param, you can close this issue\n\n@gmirc12 I think you’re referencing another issue: #2161, this issue remains unresolved."
      }
    ]
  },
  {
    "issue_number": 2138,
    "title": "`is_given()` Misinterprets `None` as a Tool in OpenAI-Python",
    "author": "hcchengithub",
    "state": "closed",
    "created_at": "2025-02-22T03:39:09Z",
    "updated_at": "2025-02-28T03:44:47Z",
    "labels": [
      "question"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\n### Description\nThe function `is_given()` incorrectly interprets `None` as a valid tool, leading to a `TypeError` in `_parsing/_completions.py`. The function is expected to return `False` when the input is `None`, but instead, it proceeds further, causing an error when attempting to iterate over a `NoneType` object.  \n\n### Affected Code  \n**File:** `openai/lib/_parsing/_completions.py`  \n**Function:** `validate_input_tools(tools)`\n\n```python\nif not is_given(tools):\n    return\n\nfor tool in tools:\n    if tool[\"type\"] != \"function\":\n        raise ValueError(\n            f'Currently only `function` tool types support auto-parsing; Received `{tool[\"type\"]}`',\n        )\n```\n\nWhen `tools` is `None`, `is_given(tools)` should evaluate to `False`, preventing execution of `for tool in tools:`. However, `is_given()` does not account for `None`, leading to:\n\n```\nTypeError: 'NoneType' object is not iterable\n```\n\n### Root Cause\nThe issue stems from the `is_given()` function in `_utils.py`:\n\n**Current Implementation:**\n```python\ndef is_given(obj: NotGivenOr[_T]) -> TypeGuard[_T]:\n    return not isinstance(obj, NotGiven)\n```\nThis function does not explicitly handle `None`, causing incorrect behavior when `tools=None`.\n\n### Expected Behavior\nIf `tools` is `None`, `is_given(tools)` should return `False`, preventing the loop execution.\n\n### Steps to Reproduce\n1. Use `openai-python` in an environment where `tools=None` is passed to `validate_input_tools()`.\n2. Observe the TypeError at line `for tool in tools:`.\n\n### Workaround\nA modified version of `is_given()` resolves the issue:\n\n```python\ndef is_given(obj: NotGivenOr[_T]) -> TypeGuard[_T]:\n    if obj is None or obj == []:\n        return False\n    return not isinstance(obj, NotGiven)\n```\n\nThis correctly prevents the function from proceeding when `tools=None`.\n\n### Environment Details\n- OpenAI-Python version: **1.63.2** (Latest, cloned and installed)\n- Reproduced in: **Jupyter, Swarm, Root Cause Evaluation**\n- Confirmed issue exists in the latest OpenAI-Python repo (`LBB2\\GitHub\\openai-python\\src\\openai\\_utils\\_utils.py`).\n\n\n### To Reproduce\n\n1. Use `openai-python` in an environment where `tools=None` is passed to `validate_input_tools()`.\n2. Observe the TypeError at line `for tool in tools:`.\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nWindows 11\n\n### Python version\n\nPython 3.10\n\n### Library version\n\n1.63.2",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "This is not a bug in `is_given()` at a minimum. This function exists to distinguish between when we should send `null` to the API and when we should omit the property.\n\nAs per the [types](https://github.com/openai/openai-python/blob/3e69750d47df4f0759d4a28ddc68e4b38756d9ca/src/openai/resources/beta/chat/completions.py#L94), you shouldn't pass `tools` at all if you don't want it to be set.\n\nCan you share why you're trying to set it to `None`?"
      },
      {
        "user": "hcchengithub",
        "body": "> Can you share why you're trying to set it to None?\n\nSee  https://github.com/openai/swarm/blob/main/swarm/core.py Line:61\n"
      },
      {
        "user": "hcchengithub",
        "body": "OpenAI Swarm https://github.com/openai/swarm/blob/main/swarm/core.py Line:61\n\n**Was:**\n```\n58 create_params = {\n59     \"model\": model_override or agent.model,\n60     \"messages\": messages,\n61     \"tools\": tools or None,\n62     \"tool_choice\": agent.tool_choice,\n63     \"stream\": stream,\n64 }\n```\n**Should be:**\n```\n58 create_params = {\n59     \"model\": model_override or agent.model,\n60     \"messages\": messages,\n61     \"tools\": tools or openai.NotGiven(),\n62     \"tool_choice\": agent.tool_choice,\n63     \"stream\": stream,\n64 }\n```\n\n"
      }
    ]
  },
  {
    "issue_number": 2147,
    "title": "Poor Fine Tuning",
    "author": "gorefieend",
    "state": "closed",
    "created_at": "2025-02-26T21:01:47Z",
    "updated_at": "2025-02-26T21:17:42Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [ ] This is an issue with the Python library\n\n### Describe the bug\n\nHello, i trained my own model \"ft:gpt-4o-mini-2024-07-18:gorofund::B5HSIqDn\" with 50 prompts and trained for 3 epochs on that dataset. Why are the real answers not matching at all and it is making up tons of stuff that wasn't even given to him? haha\n\nWhat can i change to make it much more accurate? Number of epochs, increasing list of prompts or different fine tune method (i used supervised, because the other one wasn't available for the model)\n\n### To Reproduce\n\n.\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nWindows\n\n### Python version\n\nPython 3\n\n### Library version\n\n1.64.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "This sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2146,
    "title": "Possible memory leak in `AsyncCompletions.parse()`",
    "author": "anteverse",
    "state": "open",
    "created_at": "2025-02-26T11:05:18Z",
    "updated_at": "2025-02-26T13:55:54Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThere might be a memory leak when using the method `.parse()` on `AsyncCompletions` with Pydantic models created with `create_model`. When submitting several calls, the memory usage keeps on rising. I haven't found any plateau yet, which could mean the parsers built upon these models might not be garbage collected.\n\n### To Reproduce\n\n1. Have a function that creates a Pydantic model with `create_model`\n2. Have several calls where the response_format param always gets a new model from the function above\n3. Monitor the memory\n\nWe do have a work-around though. The leaking scenario will be called `leaking` and the safe one `non_leaking` in the snippets.\n\nPlease let me know if you need more info. Thanks a lot.\n\n### Code snippets\n\n```Python\nimport asyncio\nimport gc\nimport os\nfrom typing import List\n\nfrom memory_profiler import profile\nfrom openai import AsyncOpenAI\nfrom openai.lib._parsing import type_to_response_format_param\nfrom pydantic import Field, create_model\n\n\nStepModel = create_model(\n    \"Step\",\n    explanation=(str, Field()),\n    output=(str, Field()),\n)\n\n\ndef create_new_model():\n    \"\"\"This sounds useless as it is. In our business case, I'm generating a model that slightly different at each call, hence the use of create_model. This illustrates of a model that seems to always be the same keeps on adding up in the memory.\"\"\"\n    return create_model(\n        \"MathResponse\",\n        steps=(List[StepModel], Field()),\n        final_answer=(str, Field()),\n    )\n\n\n@profile()\nasync def leaking_call(client, new_model):\n    await client.beta.chat.completions.parse(\n        model=\"gpt-4o-2024-08-06\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n            {\"role\": \"user\", \"content\": \"solve 8x + 31 = 2\"},\n        ],\n        response_format=new_model,\n    )\n\n\nasync def non_leaking_call(client, new_model):\n    await client.chat.completions.create(\n        model=\"gpt-4o-2024-08-06\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n            {\"role\": \"user\", \"content\": \"solve 8x + 31 = 2\"},\n        ],\n        response_format=type_to_response_format_param(new_model),\n    )\n\n\nasync def main():\n    client = AsyncOpenAI()\n\n    for _ in range(200):\n        # You can switch to `non_leaking_call` and see that the memory is correctly emptied\n        await leaking_call(client, create_new_model())\n\n        # We wanted to thoroughly check the memory usage, hence memory profiler + gc\n        gc.collect()\n        print(len(gc.get_objects()))\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.11.9\n\n### Library version\n\nopenai v1.64.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report, what version of Pydantic are you using?"
      },
      {
        "user": "anteverse",
        "body": "Pydantic 2.10.6.\nAlso tested with 2.9.2 earlier\n"
      }
    ]
  },
  {
    "issue_number": 1950,
    "title": "Client side JSON Schema in `response_format` validation for  structured outputs",
    "author": "CakeCrusher",
    "state": "closed",
    "created_at": "2024-12-15T21:47:17Z",
    "updated_at": "2025-02-25T19:40:45Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\r\n\r\n- [X] This is a feature request for the Python library\r\n\r\n### Describe the feature or improvement you're requesting\r\n\r\nWhen running `client.beta.chat.completions.parse` the `response_format` seems to be validated through the API as opposed to the client.\r\nWhy is that?\r\n\r\nMy (hacky) solution to validating JSON Schema is to run a minimal request to the API just to verify if the schema is valid.\r\n \r\nIs there any way we could be able to validate schemas without making a chat completions query?\r\n\r\nExample (it is intended to fail):\r\n```py\r\nschema = {\r\n    \"type\": \"object\",\r\n    \"description\": \"The description of this item\",\r\n    \"properties\": {\r\n        \"id\": {\r\n            \"description\": \"The id of this inner item\",\r\n            \"type\": \"integer\"\r\n        },\r\n        \"value\": {\r\n            \"type\": \"array\",\r\n            \"description\": \"The list of values of this inner item\",\r\n            \"items\": {\r\n                \"type\": \"string\",\r\n                \"description\": \"The value of this inner item\",\r\n                \"enum\": [\"a\", \"b\"]\r\n            },\r\n        }\r\n    },\r\n    # \"required\": [\"value\", \"id\"], # FAILS HERE INTENTIONALLY TO DEMONSTRATE API SIDE VALIDATION\r\n    \"additionalProperties\": False,\r\n}\r\n\r\nimport openai\r\nclient = openai.OpenAI(api_key=settings.OPENAI_API_KEY)\r\n\r\ndef validate_schema(schema: dict) -> None:\r\n    try:    \r\n        client.beta.chat.completions.parse(\r\n            model=\"gpt-4o-mini\",\r\n            messages=[\r\n                {\r\n                    \"role\": \"user\",\r\n                    \"content\": \"a\"\r\n                }\r\n            ],\r\n            max_completion_tokens=1,\r\n            response_format={\r\n                \"type\": \"json_schema\",\r\n                \"json_schema\": {\r\n                    \"name\": \"action_items\",\r\n                    \"description\": \"The action items to be completed\",\r\n                    \"strict\": True,\r\n                    \"schema\": schema,\r\n                },\r\n            }\r\n        )\r\n    except openai._exceptions.LengthFinishReasonError as e:\r\n        pass\r\n    except Exception as e:\r\n        # FAILS HERE\r\n        raise e\r\n\r\nvalidate_schema(schema)\r\n```\r\n\r\n\r\n\r\n### Additional context\r\n\r\n(thats all)",
    "comments": [
      {
        "user": "CakeCrusher",
        "body": "@RobertCraigie "
      },
      {
        "user": "RobertCraigie",
        "body": "> When running client.beta.chat.completions.parse the response_format seems to be validated through the API as opposed to the client.\r\n> Why is that?\r\n\r\nWe don't want to do client-side validation as the API could be improved at any time to support new types of schemas and if we add client-side validation, that would lag behind and users on older SDK versions wouldn't benefit from the updated API.\r\n\r\nCurrently we have no plans to add client-side validation but I'm curious what your use case is?\r\n"
      },
      {
        "user": "CakeCrusher",
        "body": "@RobertCraigie \r\n> > When running client.beta.chat.completions.parse the response_format seems to be validated through the API as opposed to the client.\r\n> > Why is that?\r\n> \r\n> We don't want to do client-side validation as the API could be improved at any time to support new types of schemas and if we add client-side validation, that would lag behind and users on older SDK versions wouldn't benefit from the updated API.\r\n> \r\n> Currently we have no plans to add client-side validation but I'm curious what your use case is?\r\n\r\nI am generating tools/actions which consists of both generating the code and writing its JSON Schema with the objective of loading the schema back into the request as a tool. At the moment I am not aware of any way to validate that the schema will actually work without making a chat completions request. Hence this is what I do\r\nhttps://github.com/CakeCrusher/ActionCollective/blob/main/v0/client/action_collective/client.py#L30-L53\r\n(PS: ill be cleaning things up and updating readme to make this repo usable, will be ready by the end oftomorrow.)\r\n\r\n\r\nI think having an endpoint exclusively for validating schema would be a good solution. Simply returns 201 OK or 400s error.\r\n\r\n\r\nApplication of https://arxiv.org/abs/2411.01747 ...\r\nIn structured outputs designed to be deployed https://github.com/CakeCrusher/ActionCollective"
      }
    ]
  },
  {
    "issue_number": 2133,
    "title": "Structure output breaks if const value is a long string with a new line inside of it.",
    "author": "nbilgrien-hbs",
    "state": "closed",
    "created_at": "2025-02-19T17:17:25Z",
    "updated_at": "2025-02-24T11:59:53Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe client raises an `openai.BadRequestError` exception if the pydantic class passed to `self.client.beta.chat.completions.parse` contains a literal ('const' in json schema terms) string value with a new line in it. Appears to only be reproducible for large strings (>a few hundred characters). \n\nThis is happening with gpt4o-mini. I have not tried with other models.\n\n### To Reproduce\n\nThe following schema works fine when passed to `self.client.beta.chat.completions.parse` \n\n```python\n        class Schema(BaseModel):\n            x: Literal[\"\"\"At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat. At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat\"\"\"]\n```\n\nWhile the following schema raises and exception \n```python\n        class Schema(BaseModel):\n            x: Literal[\"\"\"At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat. \n\nAt vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat\"\"\"]\n```\n\nThe only difference is the inclusion of white space in the middle.\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOs\n\n### Python version\n\nPython 3.11.10\n\n### Library version\n\nopenai v1.61.1",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "body": "Hi,\nYou can try something like the following which would resolve the issue which still looks like multiple lines.\n\n```python\nfrom pydantic import BaseModel\nfrom typing import Literal\n\nstring_with_newline = \"\"\"At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat.\n\nAt vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti quos dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa qui officia deserunt mollitia animi, id est laborum et dolorum fuga. Et harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et voluptates repudiandae sint et molestiae non recusandae. Itaque earum rerum hic tenetur a sapiente delectus, ut aut reiciendis voluptatibus maiores alias consequatur aut perferendis doloribus asperiores repellat\"\"\"\n\nnormalized_string = string_with_newline.replace(\"\\n\", \"\\\\n\")\n\nclass Schema(BaseModel):\n    x: Literal[normalized_string]\n```"
      },
      {
        "user": "RobertCraigie",
        "body": "Can you share some context on why you're putting such large strings into the schema?"
      },
      {
        "user": "nbilgrien-hbs",
        "body": "> Can you share some context on why you're putting such large strings into the schema?\n\nI'm experimenting with injecting instructions directly into the response itself, similar to the pattern of \"putting words in the models mouth\" at the start of a response, but doing so in the middle of it, all in a single call."
      }
    ]
  },
  {
    "issue_number": 2141,
    "title": "why grpc not used for api requests?",
    "author": "vignesh14052002",
    "state": "closed",
    "created_at": "2025-02-22T09:48:45Z",
    "updated_at": "2025-02-24T11:52:42Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nI have read that grpc is a bit complex but efficient while transferring data, but is there any reason this library is using http calls to make requests?\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "short answer AFAIK the OpenAI API does not support gRPC."
      }
    ]
  },
  {
    "issue_number": 1426,
    "title": "Debugging with PyCharm throws error when environment variable not specified",
    "author": "tworedz",
    "state": "closed",
    "created_at": "2024-05-15T07:43:35Z",
    "updated_at": "2025-02-21T03:33:38Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nHi OpenAI team!\r\n\r\nI'm having trouble debugging my project that uses the `openai` package along with several other dependencies. I'm encountering an exception during the import process.\r\n\r\nHere is my file `t.py`:\r\n```python\r\nfrom openai import OpenAI\r\n```\r\n\r\nRunning script in debug mode in PyCharm:\r\n```\r\npython t.py\r\n```\r\n\r\nInitially, I attempted to execute uvicorn with FastAPI, but the issue persists even when they are not involved.\r\n\r\nHere is traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py\", line 1534, in _exec\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/Users/tworedz/workspace/temp/language_model/t.py\", line 1, in <module>\r\n    from openai import OpenAI\r\n  File \"/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/__init__.py\", line 345, in <module>\r\n    from ._module_client import (\r\n  File \"/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_module_client.py\", line 75, in <module>\r\n    chat: resources.Chat = ChatProxy().__as_proxied__()\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"_pydevd_bundle/pydevd_pep_669_tracing_cython.pyx\", line 504, in _pydevd_bundle.pydevd_pep_669_tracing_cython.PyRaiseCallback.__call__\r\n  File \"_pydevd_bundle/pydevd_pep_669_tracing_cython.pyx\", line 47, in _pydevd_bundle.pydevd_pep_669_tracing_cython.PEP669CallbackBase.frame\r\n  File \"/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_utils/_proxy.py\", line 49, in __class__\r\n    proxied = self.__get_proxied__()\r\n              ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_utils/_proxy.py\", line 55, in __get_proxied__\r\n    return self.__load__()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_module_client.py\", line 12, in __load__\r\n    return _load_client().chat\r\n           ^^^^^^^^^^^^^^\r\n  File \"/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/__init__.py\", line 323, in _load_client\r\n    _client = _ModuleClient(\r\n              ^^^^^^^^^^^^^^\r\n  File \"/Users/tworedz/Library/Caches/pypoetry/virtualenvs/language-model-5UFyvxqf-py3.12/lib/python3.12/site-packages/openai/_client.py\", line 104, in __init__\r\n    raise OpenAIError(\r\nopenai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\r\n```\r\n\r\nBy the way, the issue doesn't occur if we specify the OPENAI_API_KEY environment variable. However, it's peculiar that simply importing the package initializes a class with side effects, and this only happens in debug mode in PyCharm.\r\n\r\nI'm using PyCharm 2023.3.5 (Professional Edition) on macOS Sonoma M1 Pro.\r\n\n\n### To Reproduce\n\nDescribed steps above.\n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.12.3 \n\n### Library version\n\nopenai 1.30.1 ",
    "comments": [
      {
        "user": "maxupp",
        "body": "Having this exact same behavior."
      },
      {
        "user": "maxupp",
        "body": "@tworedz Updateing Pycharm fixed it for me."
      },
      {
        "user": "tworedz",
        "body": "Got it, thanks!"
      }
    ]
  },
  {
    "issue_number": 1220,
    "title": "Incompatible: `with_streaming_response` `stream_to_file` `response_format='aac'`",
    "author": "tom-huntington",
    "state": "closed",
    "created_at": "2024-03-07T08:32:38Z",
    "updated_at": "2025-02-20T05:24:27Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nThe following example errors\r\n```\r\nline 31, in generate_audio\r\n    response.stream_to_file(output_file)\r\nAttributeError: 'ResponseContextManager' object has no attribute 'stream_to_file\r\n```\r\n\r\nHowever, if you remove either `with_streaming_response` or `response_format='aac'` it works\r\n\r\n\n\n### To Reproduce\n\n```\r\nfrom openai import OpenAI\r\nclient = OpenAI()\r\nresponse = client.audio.speech.with_streaming_response.create(\r\n    model=\"tts-1\",\r\n    voice=\"alloy\",\r\n    input=\"hello aac\",\r\n    response_format='aac'\r\n)\r\nresponse.stream_to_file(\"output.m4a\")\r\n```\n\n### Code snippets\n\n_No response_\n\n### OS\n\nwsl\n\n### Python version\n\n3.8.10\n\n### Library version\n\n1.13.3",
    "comments": [
      {
        "user": "rattrayalex",
        "body": "Did you try using it as a context manager?\r\n```py\r\nfrom openai import OpenAI\r\nclient = OpenAI()\r\nwith client.audio.speech.with_streaming_response.create(\r\n    model=\"tts-1\",\r\n    voice=\"alloy\",\r\n    input=\"hello aac\",\r\n    response_format='aac'\r\n) as response:\r\n    response.stream_to_file(\"output.m4a\")\r\n```"
      },
      {
        "user": "flipsideview",
        "body": "Hi ,\r\n\r\nIs there a fix to this bug ? I keep hitting this output often \" Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\r\n  response.stream_to_file(speech_file_path)\" Thx"
      },
      {
        "user": "digitalbore",
        "body": "Same here"
      }
    ]
  },
  {
    "issue_number": 355,
    "title": "pip install openai==0.27.2 is not working",
    "author": "rohit901",
    "state": "closed",
    "created_at": "2023-03-29T21:07:09Z",
    "updated_at": "2025-02-19T21:27:24Z",
    "labels": [
      "bug"
    ],
    "body": "### Describe the bug\n\nI'm getting this error when trying to install latest version of openai python pip package.\r\n\r\n```\r\nERROR: Ignored the following versions that require a different python version: 0.11.0 Requires-Python >=3.7.1; 0.11.1 Requires-Python >=3.7.1; 0.11.2 Requires-Python >=3.7.1; 0.11.3 Requires-Python >=3.7.1; 0.11.4 Requires-Python >=3.7.1; 0.11.5 Requires-Python >=3.7.1; 0.11.6 Requires-Python >=3.7.1; 0.12.0 Requires-Python >=3.7.1; 0.13.0 Requires-Python >=3.7.1; 0.14.0 Requires-Python >=3.7.1; 0.15.0 Requires-Python >=3.7.1; 0.16.0 Requires-Python >=3.7.1; 0.18.0 Requires-Python >=3.7.1; 0.18.1 Requires-Python >=3.7.1; 0.19.0 Requires-Python >=3.7.1; 0.20.0 Requires-Python >=3.7.1; 0.22.0 Requires-Python >=3.7.1; 0.22.1 Requires-Python >=3.7.1; 0.23.0 Requires-Python >=3.7.1; 0.23.1 Requires-Python >=3.7.1; 0.24.0 Requires-Python >=3.7.1; 0.25.0 Requires-Python >=3.7.1; 0.26.0 Requires-Python >=3.7.1; 0.26.1 Requires-Python >=3.7.1; 0.26.2 Requires-Python >=3.7.1; 0.26.3 Requires-Python >=3.7.1; 0.26.4 Requires-Python >=3.7.1; 0.26.5 Requires-Python >=3.7.1; 0.27.0 Requires-Python >=3.7.1; 0.27.1 Requires-Python >=3.7.1; 0.27.2 Requires-Python >=3.7.1\r\nERROR: Could not find a version that satisfies the requirement openai==0.27.2 (from versions: 0.0.2, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.2.0, 0.2.1, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.3.0, 0.4.0, 0.6.0, 0.6.1, 0.6.2, 0.6.3, 0.6.4, 0.7.0, 0.8.0, 0.9.0, 0.9.1, 0.9.2, 0.9.3, 0.9.4, 0.10.0, 0.10.1, 0.10.2, 0.10.3, 0.10.4, 0.10.5)\r\nERROR: No matching distribution found for openai==0.27.2\r\n```\r\nI'm on conda environment with python 3.10.10.\r\n\r\n`pip install openai` seems to install older version of 0.8.0 on my system and not the latest version, my pip is updated.\r\n\r\n```\r\nCollecting openai\r\n  Using cached openai-0.10.5.tar.gz (157 kB)\r\n  Preparing metadata (setup.py) ... done\r\nRequirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from openai) (2.28.1)\r\nRequirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from openai) (4.64.1)\r\n  Using cached openai-0.10.4.tar.gz (157 kB)\r\n  Preparing metadata (setup.py) ... done\r\n  Using cached openai-0.10.3.tar.gz (157 kB)\r\n  Preparing metadata (setup.py) ... done\r\n  Using cached openai-0.10.2.tar.gz (156 kB)\r\n  Preparing metadata (setup.py) ... done\r\n  Using cached openai-0.10.1.tar.gz (155 kB)\r\n  Preparing metadata (setup.py) ... done\r\n  Using cached openai-0.10.0.tar.gz (155 kB)\r\n  Preparing metadata (setup.py) ... done\r\n  Using cached openai-0.9.4.tar.gz (156 kB)\r\n  Preparing metadata (setup.py) ... done\r\n  Using cached openai-0.9.3.tar.gz (155 kB)\r\n  Preparing metadata (setup.py) ... done\r\n  Using cached openai-0.9.2.tar.gz (155 kB)\r\n  Preparing metadata (setup.py) ... done\r\n  Using cached openai-0.9.1.tar.gz (156 kB)\r\n  Preparing metadata (setup.py) ... done\r\n  Using cached openai-0.9.0.tar.gz (155 kB)\r\n  Preparing metadata (setup.py) ... done\r\n  Using cached openai-0.8.0-py3-none-any.whl\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.20->openai) (1.26.12)\r\nRequirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.20->openai) (2022.9.24)\r\nRequirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.20->openai) (3.4)\r\nRequirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.20->openai) (2.1.1)\r\nInstalling collected packages: openai\r\nSuccessfully installed openai-0.8.0\r\n```\n\n### To Reproduce\n\npip install openai==0.27.2 on M1 Macbook\n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOS\n\n### Python version\n\n3.10.10\n\n### Library version\n\n0.8.0",
    "comments": [
      {
        "user": "hallacy",
        "body": "That error message seems to suggest that you're still running on python <3.7.1.  I think you'll need to double check your environment and make sure you're working with a supported python version"
      },
      {
        "user": "bluEEil",
        "body": "> That error message seems to suggest that you're still running on python <3.7.1. I think you'll need to double check your environment and make sure you're working with a supported python version\n\nHey @hallacy I'm having the same issue. I am using python 3.7 but I don't understand why you say 3.7.1 when the docs says 3.7 or higher for many versions that are **not** the latest?\n\n![Image](https://github.com/user-attachments/assets/8d8b8bd3-98ec-433a-b503-d8ebc2d46e17)\nAm I missing something here? Are the requirements for all the older versions wrong?"
      }
    ]
  },
  {
    "issue_number": 2132,
    "title": "typing_extensions AND typing",
    "author": "ZLVincent",
    "state": "closed",
    "created_at": "2025-02-18T07:39:20Z",
    "updated_at": "2025-02-19T15:43:36Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\ncannot import name 'Sequence' from 'typing_extensions' (/usr/lib/python3/dist-packages/typing_extensions.py)\n\npython-3.11\n\n### To Reproduce\n\npython-3.11\n\nimport openai\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nubuntu\n\n### Python version\n\nPython v3.11\n\n### Library version\n\nopenai v1.63.2",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "You're using an old version of typing-extensions. This library requires `typing-extensions>=4.11`, please update your version of typing-extensions."
      },
      {
        "user": "ZLVincent",
        "body": "![Image](https://github.com/user-attachments/assets/0044c02b-ae46-48e3-a8b0-c7491b614122)\n\nI'm using typing-extensions==4.12.2\n\n![Image](https://github.com/user-attachments/assets/4f549dd7-7286-4680-ad25-7b473cba495b)\n\nand I found a Sequence imported from typing_extensions in models.py. Does this have any effect?"
      },
      {
        "user": "RobertCraigie",
        "body": "I don't think that is the actual version that you're using at runtime.\n\nYou can verify by running this code:\n\n```py\nfrom importlib.metadata import version\nprint(version('typing-extensions'))\n```"
      }
    ]
  },
  {
    "issue_number": 1945,
    "title": "The readme file could have instructions on how to create the OPENAI_API_KEY environment variable.",
    "author": "lucaspaludo",
    "state": "closed",
    "created_at": "2024-12-12T21:42:15Z",
    "updated_at": "2025-02-18T11:52:45Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\n# Description\r\n\r\nCurrently, the documentation for setting the OPENAI_API_KEY environment variable is incomplete or confusing, which can make it difficult for new developers to integrate the API into their projects. This issue suggests improvements to the documentation to clarify the process.\r\n\r\n# Problem\r\n\r\n- There is no clear explanation of where or how to create and set the environment variable.\r\n\r\n- Specific instructions for different operating systems (Windows, macOS, Linux) are missing or superficial.\r\n\r\n- There is no mention of best practices for storing sensitive variables, such as using .env files.\r\n\r\n# Proposed Solution\r\nAdd a detailed section on creating the environment variable in the official documentation:\r\n- Brief introduction about what OPENAI_API_KEY is\r\n- How to configure the variable in different operating systems.\r\n- Recommend the use of .env files\r\n\r\nI believe these changes will reduce the learning curve for new developers, as well as ensuring that the configuration is done correctly.\r\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "Joseelmax-00",
        "body": "If you cannot google or ask ChatGPT how to create an environment variable then what are you doing programming python?"
      },
      {
        "user": "Programmer-RD-AI",
        "body": "# Windows\n\n```bash\nsetx OPENAI_API_KEY \"your_api_key_here\"\n```\n\n# Linux/MacOS\n\n```bash\nexport OPENAI_API_KEY=\"your_api_key_here\"\n```\n"
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for the suggestion! However this is too much detail for the README.md, I'd rather we keep it focused on using the SDK / API."
      }
    ]
  },
  {
    "issue_number": 2131,
    "title": "Drop support of Python 3.8",
    "author": "ericwb",
    "state": "closed",
    "created_at": "2025-02-17T16:38:27Z",
    "updated_at": "2025-02-17T16:47:35Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nPython 3.8 is already end-of-life as of 2024-10-07 as you see from the PSF here: https://devguide.python.org/versions/, yet this client is still supporting it.\n\nI recommend dropping support as further fixes to the Python 3.8 runtime won't be made and could affect usage of this client.\n\n### To Reproduce\n\n1. Go to https://devguide.python.org/versions/\n2. Notice Python 3.8 end support as of 2024-10-07\n3. Notice in https://github.com/openai/openai-python/blob/main/pyproject.toml#L24 this client still claims support.\n\n\n\n### Code snippets\n\n```Python\nn/a\n```\n\n### OS\n\nmacOS\n\n### Python version\n\n3.8\n\n### Library version\n\nopenai v1.0.1",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks but we have no immediate plans to drop 3.8 support. Supporting 3.8 isn't actively causing us any issues and there were at least 2 million downloads from 3.8 in the last month.\n```bash\n$ uvx pypistats python_minor openai --last-month\n│ null     │   6.96% │  3,490,274 │\n│ 3.8      │   3.96% │  1,987,125 │\n```"
      }
    ]
  },
  {
    "issue_number": 1277,
    "title": "Drop support of Python 3.7",
    "author": "ericwb",
    "state": "closed",
    "created_at": "2024-03-28T16:52:58Z",
    "updated_at": "2025-02-17T16:38:54Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nPython 3.7 is already end-of-life as of 2023-06-27 as you see from the PSF here: https://devguide.python.org/versions/, yet this client is still supporting it.\r\n\r\nI recommend dropping support as further fixes to the Python 3.7 runtime won't be made and could affect usage of this client.\n\n### To Reproduce\n\n1. Go to https://devguide.python.org/versions/\r\n2. Notice Python 3.7 end support as of 2023-06-27\r\n3. Notice in https://github.com/openai/openai-python/blob/main/pyproject.toml#L24 this client still claims support.\n\n### Code snippets\n\n```Python\nn/a\n```\n\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.7.x\n\n### Library version\n\nopenai v1.14.3",
    "comments": [
      {
        "user": "rattrayalex",
        "body": "Thanks @ericwb ! We do indeed plan to do this at the next major version."
      },
      {
        "user": "RobertCraigie",
        "body": "Support for Python 3.7 has since been dropped."
      },
      {
        "user": "ericwb",
        "body": "Thanks, looks like now Python 3.8 is EOL. I opened #2131"
      }
    ]
  },
  {
    "issue_number": 2129,
    "title": "Incorrect role in result of `get_final_completion()`",
    "author": "fangzhen",
    "state": "closed",
    "created_at": "2025-02-17T11:31:39Z",
    "updated_at": "2025-02-17T11:43:19Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nInspect `ParsedChatCompletion` object returned by `get_final_completion()`, the 'role' field in message is many duplicated \"assistant\" (see the example output blow).\n\n### To Reproduce\n\n```\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key='sk-or-v1-xxxx',\n    base_url='https://openrouter.ai/api/v1',\n)\n\nwith client.beta.chat.completions.stream(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=\"openai/gpt-4o\"\n    ) as stream:\n    stream.until_done()\n    \ncompletion = stream.get_final_completion()\nprint(completion.choices[0].message)\n```\n\n```\nParsedChatCompletionMessage[NoneType](content='This is a test.', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=None)\n```\nThe `role` should be `assistant` instead of `assistantassistantassistantassistantassistantassistantassistantassistant`\n\n----\nAdd `or key == \"role\"` to Line 23 of following code seems work:\nhttps://github.com/openai/openai-python/blob/720ae54414f392202289578c9cc3b84cccc7432c/src/openai/lib/streaming/_deltas.py#L6-L31\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nLinux\n\n### Python version\n\nPython 3.12\n\n### Library version\n\nopenai v1.63.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the bug report but this is really just a bug in openrouter, the OpenAI API does not include the `role` property in multiple deltas.\n\nPlease file a report with openrouter instead."
      }
    ]
  },
  {
    "issue_number": 1813,
    "title": "Not support doc format",
    "author": "Panweitong",
    "state": "closed",
    "created_at": "2024-10-23T10:19:13Z",
    "updated_at": "2025-02-17T11:30:54Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [ ] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nOpenAI Docs:\r\n![1729676939016](https://github.com/user-attachments/assets/85c67af3-de4c-45ba-85a7-fd224f627e37)\r\n\r\nPython library:\r\n![1729677950967](https://github.com/user-attachments/assets/554237c3-68f1-4bde-93e9-edd25fc37aa4)\r\n\r\n\r\n### To Reproduce\r\n\r\nUse Files API to Upload doc file，and use \"assistants\" for Assistants\r\n\r\n### Code snippets\r\n\r\n```\r\nimport openai,io\r\nimport requests\r\n\r\nopenai.api_key = \"xxxxxxxxxxxxxxxxxxx\"\r\n\r\nclass FileLike(io.BytesIO):\r\n  def __init__(self, _bytes, filename=None):\r\n    super().__init__(_bytes)\r\n    self.name = filename\r\n\r\nurl = \"https://ccrb.s3.cn-northwest-1.amazonaws.com.cn/%E7%BB%B3%E8%88%9E%E9%A3%9E%E6%89%AC%E6%B4%BB%E5%8A%9B%E7%BB%BD%E6%94%BE.doc\"\r\n\r\nr = requests.get(url)\r\nfileName = url.split(\"https://ccrb.s3.cn-northwest-1.amazonaws.com.cn/\")[1]\r\n\r\nbytes_io = io.BytesIO(r.content)\r\nfile_bytes = bytes_io.read()\r\n\r\nres = openai.files.create(\r\n  file=FileLike(file_bytes, fileName), purpose=\"assistants\"\r\n)\r\nif res.id and res.status == \"processed\":\r\n  file = openai.files.retrieve(res.id)\r\n  print(file)\r\n```\r\n\r\n\r\n### OS\r\n\r\nUbuntu\r\n\r\n### Python version\r\n\r\nPython v3.10.12\r\n\r\n### Library version\r\n\r\nopenai v1.51.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report, can you share an example snippet to reproduce the issue?"
      },
      {
        "user": "Panweitong",
        "body": "> Thanks for the report, can you share an example snippet to reproduce the issue?\r\n\r\nOK，I will share an example snippet later."
      },
      {
        "user": "Panweitong",
        "body": "> Thanks for the report, can you share an example snippet to reproduce the issue?\r\n\r\n```\r\nimport openai,io\r\nimport requests\r\n\r\nopenai.api_key = \"xxxxxxxxxxxxxxxxxxx\"\r\n\r\nclass FileLike(io.BytesIO):\r\n  def __init__(self, _bytes, filename=None):\r\n    super().__init__(_bytes)\r\n    self.name = filename\r\n\r\nurl = \"https://ccrb.s3.cn-northwest-1.amazonaws.com.cn/%E7%BB%B3%E8%88%9E%E9%A3%9E%E6%89%AC%E6%B4%BB%E5%8A%9B%E7%BB%BD%E6%94%BE.doc\"\r\n\r\nr = requests.get(url)\r\nfileName = url.split(\"https://ccrb.s3.cn-northwest-1.amazonaws.com.cn/\")[1]\r\n\r\nbytes_io = io.BytesIO(r.content)\r\nfile_bytes = bytes_io.read()\r\n\r\nres = openai.files.create(\r\n  file=FileLike(file_bytes, fileName), purpose=\"assistants\"\r\n)\r\nif res.id and res.status == \"processed\":\r\n  file = openai.files.retrieve(res.id)\r\n  print(file)\r\n```"
      }
    ]
  },
  {
    "issue_number": 1278,
    "title": "Please add a security policy on how to report security issues",
    "author": "ericwb",
    "state": "closed",
    "created_at": "2024-03-28T17:18:13Z",
    "updated_at": "2025-02-17T11:25:53Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\r\n\r\n- [X] This is a feature request for the Python library\r\n\r\n### Describe the feature or improvement you're requesting\r\n\r\nPlease add a security policy to this GitHub repo. I can't find any information on how to report security issues in private. Using the issue tracker would be undesirable as it could zero-day some exploits reported.\r\n\r\n### Additional context\r\n\r\nFor example, these issues really should have been reported privately:\r\n\r\n- https://github.com/openai/openai-python/issues/1082\r\n- https://github.com/openai/openai-python/issues/1196",
    "comments": [
      {
        "user": "ericwb",
        "body": "Maybe at least consider pointing to https://openai.com/policies/coordinated-vulnerability-disclosure-policy"
      },
      {
        "user": "rattrayalex",
        "body": "Hey thanks, this is a good call-out. We'll discuss internally. For now that link should work. For SDK-specific vulns, you can also email security@stainlessapi.com."
      },
      {
        "user": "rattrayalex",
        "body": "What are some places you would expect to find this in a library like this? `CONTRIBUTING.md`?"
      }
    ]
  },
  {
    "issue_number": 1468,
    "title": "Why is logprobs and log_probs not permitted in client.chat.completions.create for AzureOpenAI?",
    "author": "r-pathak",
    "state": "closed",
    "created_at": "2024-06-05T11:34:21Z",
    "updated_at": "2025-02-17T11:24:37Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nI'm using AzureOpenAI on API version '2024-02-01'.\r\n\r\nWhen using client.chat.completions.create, to perform RAG with an Azure OpenAI gpt-4-1106-preview deployment against an Azure AI search index (via the extra_body -> data_sources parameter), I keep receiving the error:\r\n`TypeError: create() got an unexpected keyword argument 'log_probs'\r\n`\r\nor \r\n`Error code: 400 - {'error': {'requestid': '5b8df334-1238-4b66-8cb8-564ffbe02cff', 'code': 400, 'message': 'Validation error at #/logprobs: Extra inputs are not permitted'}}`\r\n\r\nI read here that I should be able to pass 'log_probs' in the completions method to see logprobs populated in the response:\r\nhttps://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions\r\n\r\nYet it seems despite switching API versions I simply can't achieve this. Is there support for this, or am I doing something wrong? \n\n### To Reproduce\n\n1. Use client.chat.completions.create with gpt-4 hosted on Azure\r\n2. Attempt to use the logprobs or log_probs parameter\n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOs\n\n### Python version\n\nPython v3.9\n\n### Library version\n\nopenai 1.23.2",
    "comments": [
      {
        "user": "wangyuantao",
        "body": "RAG has feature gap. Why do you want log_probs for RAG? Model distillation? Get answer confident score?"
      },
      {
        "user": "r-pathak",
        "body": "Hi,\n\nThanks for the response, I thought that might be the case. \n\nYou said it - we want to extract confidence scores from each of our answers.\n\nIs there another best practice way for now? Ideally, one that doesn't involve another LLM request - as we want to minimise cost & speed."
      },
      {
        "user": "r-pathak",
        "body": "I've seen on the link below that there is a workaround in typescript to include logprobs in the parameters - is there a similar workaround for the Python SDK?\r\n\r\nhttps://github.com/Azure/azure-sdk-for-js/issues/29199"
      }
    ]
  },
  {
    "issue_number": 1450,
    "title": "Slow tool calls compared to web",
    "author": "madroneropaulo",
    "state": "closed",
    "created_at": "2024-05-28T02:51:56Z",
    "updated_at": "2025-02-17T11:23:38Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nI'm following [the function call tutorial](https://platform.openai.com/docs/assistants/tools/function-calling/quickstart) without streaming. So my code looks like this\r\n```\r\n # Add the user message to the thread\r\n    client.beta.threads.messages.create(\r\n        thread_id=thread_id,\r\n        role=\"user\",\r\n        content=user_input,\r\n    )\r\n    print(\"thread created. Running run\", datetime.now().time().strftime(\"%H:%M:%S\"))\r\n\r\n    run = client.beta.threads.runs.create_and_poll(\r\n        thread_id=thread_id,\r\n        assistant_id=assistant_id,\r\n    )\r\n    \r\n    if run.status == 'completed':\r\n        print(\"run complete\",  datetime.now().time().strftime(\"%H:%M:%S\"))\r\n        messages = client.beta.threads.messages.list(\r\n        thread_id=thread_id\r\n    )\r\n        print(messages)\r\n    else:\r\n        print(run.status)\r\n    \r\n    # Define the list to store tool outputs\r\n    tool_outputs = []\r\n    \r\n    # Loop through each tool in the required action section\r\n    for tool in run.required_action.submit_tool_outputs.tool_calls:\r\n        print(\"tool received\",  datetime.now().time().strftime(\"%H:%M:%S\"))\r\n        if tool.function.name == \"get_weather\":\r\n        ...etc\r\n        \r\n```\r\nIt takes 4 seconds from the moment when the thread is created to the moment I get the function name and params\r\n```\r\nthread created. Running run 21:22:40\r\ntool received 21:22:44\r\n```\r\nWhen I try the assistant from the assistants playground in the web dashboard, it takes 1500ms to return a message with the right function name. And I noticed the browser version makes an POST request to this endpoint which I guess is the same the python library uses under the hood:\r\n`https://api.openai.com/v1/threads/thread_id123/runs`\r\nI understand that that endpoint probably just starts the run, but still, the message with the function name loads in less than 2 seconds compared to 4. Am I using the right example for this? thanks\r\n\r\n### To Reproduce\r\n\r\nFollow the function calling in this tutorial https://platform.openai.com/docs/assistants/tools/function-calling/quickstart?context=without-streaming and measure the time it takes from the moment the message is sent to the moment the function name is available in the handler. After that compare that result with the time it takes to perform the same action in the assistants playground in the web console.\r\n\r\n### Code snippets\r\n\r\n_No response_\r\n\r\n### OS\r\n\r\nMacos\r\n\r\n### Python version\r\n\r\n3.9.6\r\n\r\n### Library version\r\n\r\nlatest",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Very sorry for the delayed response. I'm going to close this as it isn't an issue with the SDK, if you're still running into this would you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 1425,
    "title": "Request for API Access to Usage Data for Better Account Management",
    "author": "kovashikawa",
    "state": "closed",
    "created_at": "2024-05-14T20:21:08Z",
    "updated_at": "2025-02-17T11:22:39Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nCentralizing usage management under a single account for multiple projects can be exhausting and inefficient.\r\n\r\nI believe it would be a great feature to allow users to access their own usage costs and activity via API, making it easier to manage.\n\n### Additional context\n\nThe current system centralizes usage and activity management, making it inaccessible for individual teams needing to track project-specific costs and plan studies.\r\n\r\nProviding API access to usage data will:\r\n\r\n* Decentralize management, enabling teams to monitor their own projects.\r\n* Help teams make better decisions based on their specific usage data.\r\n* Increase transparency and efficiency for teams working on multiple projects under a single account.",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "I'm going to close this as I believe all the data you want is now available through the [Admin API](https://platform.openai.com/docs/api-reference/administration) which is unfortunately not yet provided directly in this SDK yet."
      }
    ]
  },
  {
    "issue_number": 1307,
    "title": "GPT4-turbo integration",
    "author": "cryptexis",
    "state": "closed",
    "created_at": "2024-04-10T07:55:14Z",
    "updated_at": "2025-02-17T11:20:51Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nAccording to:\r\nhttps://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4\r\nYesterday GPT4-Turbo was released:\r\nchanges - vision integration and JSON output.\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Sorry for the delayed response, this has been resolved."
      }
    ]
  },
  {
    "issue_number": 1459,
    "title": "No support for purpose=\"vision\" in files API",
    "author": "JensMadsen",
    "state": "closed",
    "created_at": "2024-05-30T11:37:56Z",
    "updated_at": "2025-02-17T11:20:28Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nAccording to the [docs](https://platform.openai.com/docs/assistants/how-it-works/creating-image-input-content) in order to create a file upload of an image this should work:\r\n\r\n``` python \r\nfile = client.files.create(\r\n  file=open(\"myimage.png\", \"rb\"),\r\n  purpose=\"vision\"\r\n)\r\n```\r\n\r\nHowever, two things do not works as I expected:\r\n1. the typing of `purpose` does not include `vision`\r\n2. I use this API version \"2024-05-01-preview\". I get this error response from Azure Open AI\r\n`Error code: 400 - {'error': {'code': 'invalidPayload', 'message': 'Invalid value for the purpose.'}`\r\n\r\n\n\n### To Reproduce\n\nRun the snippet from the documentation: \r\n\r\n``` python \r\nfile = client.files.create(\r\n  file=open(\"myimage.png\", \"rb\"),\r\n  purpose=\"vision\"\r\n)\r\n\r\nthread = client.beta.threads.create(\r\n  messages=[\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": [\r\n        {\r\n          \"type\": \"text\",\r\n          \"text\": \"What is the difference between these images?\"\r\n        },\r\n        {\r\n          \"type\": \"image_url\",\r\n          \"image_url\": {\"url\": \"https://example.com/image.png\"}\r\n        },\r\n        {\r\n          \"type\": \"image_file\",\r\n          \"image_file\": (\"file_id\": file.id)\r\n        },file = client.files.create(\r\n  file=open(\"myimage.png\", \"rb\"),\r\n  purpose=\"vision\"\r\n)\r\n```\n\n### Code snippets\n\n_No response_\n\n### OS\n\nlinux (Ubuntu)\n\n### Python version\n\n3.12.1\n\n### Library version\n\n1.30.5",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Sorry for the very delayed response, I believe this should be resolved now. If you're still seeing this error please re-open!"
      }
    ]
  },
  {
    "issue_number": 1571,
    "title": "Error code: 400 'param': 'messages.[2].content', 'code': None",
    "author": "gujiachun",
    "state": "closed",
    "created_at": "2024-07-23T01:10:05Z",
    "updated_at": "2025-02-17T11:19:24Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nUsing the agent capability of Langchain, three tools have been added\r\nSometimes there may be errors, sometimes it may be normal\r\nError message:\r\n\r\nFile \"/Users/gujiachun/anaconda3/envs/langchain-test/lib/python3.11/site-packages/openai/_base_client.py\", line 1240, in post\r\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/gujiachun/anaconda3/envs/langchain-test/lib/python3.11/site-packages/openai/_base_client.py\", line 921, in request\r\n    return self._request(\r\n           ^^^^^^^^^^^^^^\r\n  File \"/Users/gujiachun/anaconda3/envs/langchain-test/lib/python3.11/site-packages/openai/_base_client.py\", line 1020, in _request\r\n    raise self._make_status_error_from_response(err.response) from None\r\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null. (request id: 20240723090211496742562ZksEJFx0) (request id: 2024072301021136579144958040285)\", 'type': 'invalid_request_error', 'param': 'messages.[2].content', 'code': None}}\r\n\n\n### To Reproduce\n\nfrom datetime import date\r\nfrom operator import eq, itemgetter\r\n\r\nimport requests\r\nfrom langchain.chains.query_constructor.schema import AttributeInfo\r\nfrom langchain.retrievers import SelfQueryRetriever\r\nfrom langchain_community.agent_toolkits.load_tools import load_tools\r\nfrom langchain_community.utilities import SerpAPIWrapper\r\nfrom langchain_core.documents import Document\r\nfrom langchain_core.output_parsers import StrOutputParser\r\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate\r\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\r\nfrom langchain_core.structured_query import Comparison, Comparator\r\nfrom langchain_core.tools import tool\r\nfrom langchain_huggingface import HuggingFaceEmbeddings\r\nfrom langchain_community.vectorstores import Milvus\r\nfrom langchain_openai import ChatOpenAI\r\nfrom langchain_core.runnables import chain\r\n\r\nfrom langchain import hub\r\nfrom langchain.agents import create_openai_functions_agent\r\n\r\nfrom langchain.agents import AgentExecutor\r\n\r\nserpapi_api_key = \"54d5973f9487d329ffdf22c773bb2514a94f37242d8d1311a4817e96a7386c14\"\r\n\r\napi_key = \"[REDACTED_OPENAI_KEY]\"\r\napi_url = \"https://ai-yyds.com/v1\"\r\n\r\nllm = ChatOpenAI(base_url=api_url, api_key=api_key, model_name=\"gpt-4\")\r\n\r\n# 一个最简单的模版,带记忆\r\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\r\nprint(prompt.messages)\r\n\r\n\r\n@tool\r\ndef search(text: str):\r\n    \"\"\"This tool is only used when real-time information needs to be searched. The search returns only the first 3 items\"\"\"\r\n    serp = SerpAPIWrapper(serpapi_api_key=serpapi_api_key)\r\n\r\n    response = serp.run(text)\r\n    print(type(response))\r\n    content = \"\"\r\n    if type(response) is list:\r\n        for item in response:\r\n            content += str(item[\"title\"]) + \"\\n\"\r\n    else:\r\n        content = response\r\n    return content\r\n\r\n\r\n@tool\r\ndef time() -> str:\r\n    \"\"\"Return today's date and use it for any questions related to today's date.\r\nThe input should always be an empty string, and this function will always return today's date. Any mathematical operation on a date should occur outside of this function\"\"\"\r\n    return str(date.today())\r\n\r\n\r\n@tool\r\ndef weather(city: str):\r\n    \"\"\"When you need to check the weather, you can use this tool, which returns the weather conditions for the day, tomorrow, and the day after tomorrow\"\"\"\r\n    url = \"https://api.seniverse.com/v3/weather/daily.json?key=SrlXSW6OX9PssfOJ1&location=beijing&language=zh-Hans&unit=c&start=0\"\r\n   \r\n    response = requests.get(url)\r\n   \r\n    data = response.json()\r\n\r\n    if not data or len(data['results']) == 0:\r\n        return None\r\n\r\n    daily = data['results'][0][\"daily\"]\r\n\r\n    content = \"\"\r\n    res = []\r\n    for day in daily:\r\n        info = {\"city\": city, \"date\": day[\"date\"], \"info\": day[\"text_day\"], \"temperature_high\": day[\"high\"],\r\n                \"temperature_low\": day[\"low\"]}\r\n        content += f\"{city} date：{day['date']}  info：{day['text_day']} maximum temperature：{day['high']} minimum temperature:{day['low']}\\n\"\r\n        res.append(info)\r\n\r\n    return content\r\n\r\n\r\ntools = [time, weather, search]\r\n\r\nagent = create_openai_functions_agent(llm, tools, prompt)\r\n\r\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\r\n\r\nchain1 = agent_executor | StrOutputParser()\r\n\r\nfor chunk in agent_executor.stream({\"input\": \"What's the weather like in Shanghai today\"}):\r\n    if \"output\" in chunk:\r\n        print(f'{chunk[\"output\"]}')\r\n\r\n\n\n### Code snippets\n\n```Python\nfrom datetime import date\r\nfrom operator import eq, itemgetter\r\n\r\nimport requests\r\nfrom langchain.chains.query_constructor.schema import AttributeInfo\r\nfrom langchain.retrievers import SelfQueryRetriever\r\nfrom langchain_community.agent_toolkits.load_tools import load_tools\r\nfrom langchain_community.utilities import SerpAPIWrapper\r\nfrom langchain_core.documents import Document\r\nfrom langchain_core.output_parsers import StrOutputParser\r\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate\r\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\r\nfrom langchain_core.structured_query import Comparison, Comparator\r\nfrom langchain_core.tools import tool\r\nfrom langchain_huggingface import HuggingFaceEmbeddings\r\nfrom langchain_community.vectorstores import Milvus\r\nfrom langchain_openai import ChatOpenAI\r\nfrom langchain_core.runnables import chain\r\n\r\nfrom langchain import hub\r\nfrom langchain.agents import create_openai_functions_agent\r\n\r\nfrom langchain.agents import AgentExecutor\r\n\r\nserpapi_api_key = \"54d5973f9487d329ffdf22c773bb2514a94f37242d8d1311a4817e96a7386c14\"\r\n\r\napi_key = \"[REDACTED_OPENAI_KEY]\"\r\napi_url = \"https://ai-yyds.com/v1\"\r\n\r\nllm = ChatOpenAI(base_url=api_url, api_key=api_key, model_name=\"gpt-4\")\r\n\r\n# 一个最简单的模版,带记忆\r\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\r\nprint(prompt.messages)\r\n\r\n\r\n@tool\r\ndef search(text: str):\r\n    \"\"\"This tool is only used when real-time information needs to be searched. The search returns only the first 3 items\"\"\"\r\n    serp = SerpAPIWrapper(serpapi_api_key=serpapi_api_key)\r\n\r\n    response = serp.run(text)\r\n    print(type(response))\r\n    content = \"\"\r\n    if type(response) is list:\r\n        for item in response:\r\n            content += str(item[\"title\"]) + \"\\n\"\r\n    else:\r\n        content = response\r\n    return content\r\n\r\n\r\n@tool\r\ndef time() -> str:\r\n    \"\"\"Return today's date and use it for any questions related to today's date.\r\nThe input should always be an empty string, and this function will always return today's date. Any mathematical operation on a date should occur outside of this function\"\"\"\r\n    return str(date.today())\r\n\r\n\r\n@tool\r\ndef weather(city: str):\r\n    \"\"\"When you need to check the weather, you can use this tool, which returns the weather conditions for the day, tomorrow, and the day after tomorrow\"\"\"\r\n    url = \"https://api.seniverse.com/v3/weather/daily.json?key=SrlXSW6OX9PssfOJ1&location=beijing&language=zh-Hans&unit=c&start=0\"\r\n   \r\n    response = requests.get(url)\r\n   \r\n    data = response.json()\r\n\r\n    if not data or len(data['results']) == 0:\r\n        return None\r\n\r\n    daily = data['results'][0][\"daily\"]\r\n\r\n    content = \"\"\r\n    res = []\r\n    for day in daily:\r\n        info = {\"city\": city, \"date\": day[\"date\"], \"info\": day[\"text_day\"], \"temperature_high\": day[\"high\"],\r\n                \"temperature_low\": day[\"low\"]}\r\n        content += f\"{city} date：{day['date']}  info：{day['text_day']} maximum temperature：{day['high']} minimum temperature:{day['low']}\\n\"\r\n        res.append(info)\r\n\r\n    return content\r\n\r\n\r\ntools = [time, weather, search]\r\n\r\nagent = create_openai_functions_agent(llm, tools, prompt)\r\n\r\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\r\n\r\nchain1 = agent_executor | StrOutputParser()\r\n\r\nfor chunk in agent_executor.stream({\"input\": \"What's the weather like in Shanghai today\"}):\r\n    if \"output\" in chunk:\r\n        print(f'{chunk[\"output\"]}')\n```\n\n\n### OS\n\nmacos\n\n### Python version\n\nPython 3.8.19\n\n### Library version\n\nopenai 1.37",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Sorry for the very delayed response but I don't think this is an SDK issue. If you're still running into this I recommend asking for help on the community forum at [community.openai.com](https://community.openai.com)."
      },
      {
        "user": "gujiachun",
        "body": "以收到"
      }
    ]
  },
  {
    "issue_number": 1166,
    "title": "Too many arguments in the Windows solution",
    "author": "rfpanfil",
    "state": "closed",
    "created_at": "2024-02-20T09:04:22Z",
    "updated_at": "2025-02-17T11:17:41Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nI try to run the curl and grit commands, but only got the error \"-bash: cd: too many arguments\"\n\n### To Reproduce\n\nJust write\n\n### Code snippets\n\n_No response_\n\n### OS\n\nWindows\n\n### Python version\n\nNone\n\n### Library version\n\n1.0.0",
    "comments": [
      {
        "user": "dackerman",
        "body": "Hi @rfpanfil, I take it you're following \"Automatic migration with grit on Windows\" from https://github.com/openai/openai-python/discussions/742? The error you're seeing usually shows up when you have spaces in the path you are trying to `cd` to: e.g. \r\n```\r\ncd /mnt/c/Users/Myself/My Documents/code/\r\n``` \r\n\r\nwhere `My Documents` is a directory with spaces. In that case you just need to quote it, i.e. \r\n\r\n```\r\ncd \"/mnt/c/Users/Myself/My Documents/code/\"\r\n```\r\n\r\nIf that's not the issue, then can you provide more context of what command you were trying to run and which previous steps you ran?\r\n"
      },
      {
        "user": "RobertCraigie",
        "body": "Please re-open if this is still an issue!"
      }
    ]
  },
  {
    "issue_number": 1149,
    "title": "openai lib preventing reading of headers for an http exception",
    "author": "gtmtech",
    "state": "closed",
    "created_at": "2024-02-13T11:39:11Z",
    "updated_at": "2025-02-17T11:17:07Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nhttps://github.com/openai/openai-python/blob/main/src/openai/_base_client.py#L980 - raises exceptions if there's an HTTPStatusError, such as a 429, 500 etc. \r\n\r\nWhich is semi-useful, but as-implemented, it does mean the headers are unavailable for that request. \r\n\r\nHeaders contain extremely valuable information sometimes, even when there are exceptions, and the library should ideally still return the headers along with the exception somehow, so that on receiving an exception it's possible to still read the headers. \n\n### To Reproduce\n\n```\r\ntry:\r\n    stream = sync_client.chat.completions.create(.... stream=True)\r\nexcept Exception as e:\r\n    print(e)\r\n    print(vars(e))\r\n    print(stream)\r\n    raise e\r\n```\r\n\r\n# stream is None, e is an exception with a bubbled up error message from the api, headers not available\n\n### Code snippets\n\n```Python\nsee above for code snippets.\n```\n\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.10.12\n\n### Library version\n\nopenai 1.3.7 and above",
    "comments": [
      {
        "user": "rattrayalex",
        "body": "sorry about this; you can read the headers with `print(e.response.headers)`. We'll work on making that happen more nicely out of the box."
      },
      {
        "user": "RobertCraigie",
        "body": "Going to go ahead and close this as like Alex mentioned you can access the headers with `e.response.headers` or `e.request.headers`. Improving the error message itself is a separate issue."
      }
    ]
  },
  {
    "issue_number": 1921,
    "title": "Support for multilingual requests broken on openai==1.55.3 and httpx==0.28.0",
    "author": "ishaan-jaff",
    "state": "closed",
    "created_at": "2024-12-04T03:27:57Z",
    "updated_at": "2025-02-17T11:14:45Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nSupport for multilingual requests broken on openai==1.55.3 and httpx==0.28.0\r\n\r\nWe use `openai` for multi lingual requests that contain multilingual chars, this used to work on previous openai versions \r\n\r\nas soon as we upgraded openai to 1.55.3 and httpx==0.28.0 we got the following error \r\n\r\n### To Reproduce\r\n\r\nuse `openai==1.55.3 and httpx==0.28.0` . \r\n\r\nRun the request \r\n\r\nyou'll see this error \r\n\r\n(The error goes away when we use httpx==0.27.0 but it would be nice if openai python could handle this for us) \r\n\r\n```\r\nE       UnicodeEncodeError: 'utf-8' codec can't encode character '\\ud83e' in position 39: surrogates not allowed\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/httpx/_content.py:179: UnicodeEncodeError\r\n```\r\n\r\n\r\n\r\n### Code snippets\r\n\r\n```Python\r\nimport openai\r\n    client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\r\n    response = client.chat.completions.create(\r\n        model=\"gpt-4o-mini\",\r\n        messages=[{\"role\": \"user\", \"content\": \"你好世界！\\ud83e, ö\"}],\r\n    )\r\n```\r\n\r\n\r\n### Traceback \r\n```bash\r\n\r\ntest_openai.py:279:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/_utils/_utils.py:275: in wrapper\r\n    return func(*args, **kwargs)\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/resources/chat/completions.py:829: in create\r\n    return self._post(\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/_base_client.py:1280: in post\r\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/_base_client.py:957: in request\r\n    return self._request(\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/_base_client.py:983: in _request\r\n    request = self._build_request(options, retries_taken=retries_taken)\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/_base_client.py:506: in _build_request\r\n    return self._client.build_request(  # pyright: ignore[reportUnknownMemberType]\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/httpx/_client.py:378: in build_request\r\n    return Request(\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/httpx/_models.py:408: in __init__\r\n    headers, stream = encode_request(\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/httpx/_content.py:216: in encode_request\r\n    return encode_json(json)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\njson = {'messages': [{'content': '\\ud83e', 'role': 'user'}], 'model': 'gpt-4o-mini'}\r\n\r\n    def encode_json(json: Any) -> tuple[dict[str, str], ByteStream]:\r\n        body = json_dumps(\r\n            json, ensure_ascii=False, separators=(\",\", \":\"), allow_nan=False\r\n>       ).encode(\"utf-8\")\r\nE       UnicodeEncodeError: 'utf-8' codec can't encode character '\\ud83e' in position 39: surrogates not allowed\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/httpx/_content.py:179: UnicodeEncodeError\r\n```\r\n\r\n### OS\r\n\r\nmacOS\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Library version\r\n\r\nopenai v0.27.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report @ishaan-jaff, this sounds to me like an issue with `httpx` itself, can you reproduce this just using `httpx` directly?"
      },
      {
        "user": "ishaan-jaff",
        "body": "yes, it is an issue with httpx (not necessarily with openai python) \r\n\r\nIs it possible that OpenAI python can add handling for this ? \r\n\r\nSince this used to work before we upgraded to `openai==1.55.3 and httpx==0.28.0` "
      },
      {
        "user": "RobertCraigie",
        "body": "I'm not sure if we can. Would you be able to open an issue with `httpx` (if you haven't already) to see what they think?"
      }
    ]
  },
  {
    "issue_number": 1914,
    "title": "beta.chat.completions.parse is not working with response_format  json object or json schema after upgrading to 1.55.3",
    "author": "SunkadGit",
    "state": "closed",
    "created_at": "2024-12-01T16:39:46Z",
    "updated_at": "2025-02-17T11:13:39Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nGetting Type error\r\nTypeError: No method for generating JsonSchema for core_schema.type='invalid' (expected: GenerateJsonSchema.invalid_schema)\n\n### To Reproduce\n\nUse beta.chat.completions.parse with pydantic object in response_format\n\n### Code snippets\n\n_No response_\n\n### OS\n\nwindows\n\n### Python version\n\n3.11\n\n### Library version\n\n1.55.3",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Can you share the `BaseModel` class you're using?"
      },
      {
        "user": "paulhyuk",
        "body": "Doesn't work for me either on `v1.57.1`. The following code (not including imports):\r\n```python3\r\nclass Response(BaseModel):\r\n    operations: List[Dict]\r\n    explanation: str\r\n\r\ncompletion = client.beta.chat.completions.parse(\r\n    model=\"gpt-4o-2024-08-06\",\r\n    messages=[\r\n        {\"role\": \"system\", \"content\": \"You are a data transformation expert that provides precise, actionable JSON responses.\"},\r\n        {\"role\": \"user\", \"content\": prompt}\r\n    ],\r\n    response_format=Response,\r\n)\r\nresponse = completion.choices[0].message.parsed\r\n```\r\n\r\nReturns the following error:\r\n```\r\n    raise self._make_status_error_from_response(err.response) from None\r\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'Response': In context=(), 'required' is required to be supplied and to be an array including every key in properties. Extra required key 'operations' supplied.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}\r\n```"
      },
      {
        "user": "RobertCraigie",
        "body": "@paulhyuk unfortunately arbitrary properties aren't supported in structured outputs yet https://platform.openai.com/docs/guides/structured-outputs#additionalproperties-false-must-always-be-set-in-objects, so you'll need to give a different definition for `opeerations: List[Dict]`"
      }
    ]
  },
  {
    "issue_number": 1512,
    "title": "BadRequestError: Unsupported data type when creating an Azure OpenAI assistant",
    "author": "vishnumg",
    "state": "closed",
    "created_at": "2024-06-30T20:49:59Z",
    "updated_at": "2025-02-17T11:12:54Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nAttempting to create an Azure OpenAI assistant using the OpenAI Python library results in a `BadRequestError: Unsupported data type`.\r\n\r\n**Environment:**\r\n- Python 3.12.3\r\n- Operating System: Windows 10\r\n- openai==1.35.7\r\n- Azure OpenAI API version: 2023-05-15\r\n\r\n**.env File:**\r\n```env\r\nOPENAI_API_VERSION=2023-05-15\r\nAZURE_OPENAI_ENDPOINT=https://ai-111xxxxxx468527.openai.azure.com/\r\nAZURE_OPENAI_API_KEY=6bXXXXXXXXXXXXXXXXXXXd41\r\n```\r\n\r\n**Code:**\r\n```python\r\nimport os\r\nimport time\r\nimport glob\r\nfrom openai import AzureOpenAI\r\nfrom dotenv import load_dotenv\r\n\r\nload_dotenv()\r\n\r\nclient = AzureOpenAI(\r\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\r\n    api_version=\"2023-05-15\",\r\n    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\r\n)\r\n\r\nassistant = client.beta.assistants.create(\r\n    instructions=\"\",\r\n    model=\"gpt-4\",\r\n    tools=[]\r\n)\r\n```\r\n\r\n**Error Traceback:**\r\n```\r\nTraceback (most recent call last):\r\n  File \"project_dir\\metadata_assistant.py\", line 16, in <module>\r\n    assistant = client.beta.assistants.create(\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"project_dir\\.venv\\Lib\\site-packages\\openai\\resources\\beta\\assistants.py\", line 156, in create\r\n    return self._post(\r\n           ^^^^^^^^^^^\r\n  File \"project_dir\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1250, in post\r\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"project_dir\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 931, in request\r\n    return self._request(\r\n           ^^^^^^^^^^^^^^\r\n  File \"project_dir\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1030, in _request\r\n    raise self._make_status_error_from_response(err.response) from None\r\nopenai.BadRequestError: Unsupported data type\r\n```\r\n\r\n**Expected Behavior:**\r\nThe assistant should be created without any errors.\r\n\r\n**Actual Behavior:**\r\nThe following error is raised:\r\n```\r\nopenai.BadRequestError: Unsupported data type\r\n```\r\n\n\n### To Reproduce\n\n1. Set up the Azure OpenAI client using the provided API key and endpoint.\r\n2. Attempt to create an assistant using `client.beta.assistants.create()` method.\n\n### Code snippets\n\n_No response_\n\n### OS\n\nWindows\n\n### Python version\n\n3.12.3\n\n### Library version\n\n1.35.7",
    "comments": [
      {
        "user": "kristapratico",
        "body": "@vishnumg Assistants is still in preview, so you'll need to target the preview API version == \"2024-05-01-preview\" ([API reference](https://learn.microsoft.com/en-us/azure/ai-services/openai/assistants-reference?tabs=python)).\r\n\r\nThat being said, I can't reproduce the error that you're seeing here. Can you try the preview API version and see if that works for you?"
      },
      {
        "user": "hayescode",
        "body": "@vishnumg your API version is 2023, before Assistants existed"
      },
      {
        "user": "selfishark",
        "body": "Would you please clarify this @leonprompter ? I had a similar issue here 'https://github.com/microsoft/semantic-kernel/discussions/9002#discussioncomment-10774526' and am interested in related solutions if you don't mind."
      }
    ]
  },
  {
    "issue_number": 2107,
    "title": "Better error messages for mismatch on API versions",
    "author": "tyler-suard-parker",
    "state": "closed",
    "created_at": "2025-02-10T18:46:13Z",
    "updated_at": "2025-02-17T10:56:15Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nWhenever I try to use an Azure OpenAI endpoint, if I get the API version wrong, it will tell me that the endpoint is not available or that I don't have access.  It would be nice if instead it said \"You are using the wrong API version\".\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "kristapratico",
        "body": "Thanks for the feedback @tyler-suard-parker. I do think we could improve the error message, but I'd probably favor doing such validation on the service side rather than in the client library. Let me follow-up with the team to see if we can improve the experience here."
      },
      {
        "user": "RobertCraigie",
        "body": "Closing this as it's not something we'll be changing in the SDK."
      }
    ]
  },
  {
    "issue_number": 2102,
    "title": "Do not require `jiter` to avoid dependency on Rust",
    "author": "barracuda156",
    "state": "closed",
    "created_at": "2025-02-07T18:51:19Z",
    "updated_at": "2025-02-17T10:52:38Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nIs it possible to make dependency on `jiter` optional? `pydantic` 1.x exists in rust-free version, and this project supports `pydantic` 1.9.x: https://github.com/openai/openai-python/blob/7193688e364bd726594fe369032e813ced1bdfe2/pyproject.toml#L12\n\nIt will be nice to avoid requiring Rust here.\n\n### Additional context\n\nRust is a very heavy dependency to build. Also, it is not supported on a number of non-mainstream platforms, where Python 3.x works fine.",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Unfortunately we can't really change how this works without causing breaking changes. For now I'd recommend trying to work around this in some way by providing your own `jiter` implementation that just raises an error\n```py\n# jiter.py\ndef from_json(*args, **kwargs):\n  raise NotImplementedError('jiter is not available')\n```\nOf course then you can't use the parsing helpers but you should be able to use the rest of the SDK."
      }
    ]
  },
  {
    "issue_number": 2116,
    "title": "asyncopenai failed",
    "author": "willy808",
    "state": "closed",
    "created_at": "2025-02-13T08:12:22Z",
    "updated_at": "2025-02-17T09:41:22Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nasync with PROCESS_VLM_CALLING_SEMAPHORE:\n   \n            client = openai.AsyncOpenAI(\n                base_url = vlm_url,\n                api_key = \"EMPTY\",\n                )\n\n            chat_response = await client.chat.completions.create(\n                model=model_name,\n                messages=messages,\n                temperature=0.0,\n                top_p=0.1,\n                frequency_penalty = 0.2,\n                n=1,\n                stream=True,\n                max_tokens=1024,\n            )\n\n            async for chunk in chat_response:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1589, in _request\n    response = await self._client.send(\n  File \"/usr/local/lib/python3.10/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n  File \"/usr/local/lib/python3.10/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n  File \"/usr/local/lib/python3.10/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n  File \"/usr/local/lib/python3.10/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n  File \"/usr/local/lib/python3.10/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n    stream = await self._network_backend.connect_tcp(**kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n    return await self._backend.connect_tcp(\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_backends/anyio.py\", line 115, in connect_tcp\n    stream: anyio.abc.ByteStream = await anyio.connect_tcp(\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_core/_sockets.py\", line 227, in connect_tcp\n    async with create_task_group() as tg:\nRuntimeError: Task got bad yield: True\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/app/src/app.py\", line 2106, in doc_analysis\n  File \"/app/src/app.py\", line 1450, in call_vlm\n  File \"/usr/local/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1727, in create\n    return await self._post(\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1856, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1550, in request\n    return await self._request(\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1613, in _request\n    return await self._retry_request(\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1683, in _retry_request\n    return await self._request(\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1613, in _request\n    return await self._retry_request(\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1683, in _retry_request\n    return await self._request(\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1623, in _request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n\n### To Reproduce\n\nasync with PROCESS_VLM_CALLING_SEMAPHORE:\n   \n            client = openai.AsyncOpenAI(\n                base_url = vlm_url,\n                api_key = \"EMPTY\",\n                )\n\n            chat_response = await client.chat.completions.create(\n                model=model_name,\n                messages=messages,\n                temperature=0.0,\n                top_p=0.1,\n                frequency_penalty = 0.2,\n                n=1,\n                stream=True,\n                max_tokens=1024,\n            )\n\n            async for chunk in chat_response:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1589, in _request\n    response = await self._client.send(\n  File \"/usr/local/lib/python3.10/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n  File \"/usr/local/lib/python3.10/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n  File \"/usr/local/lib/python3.10/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n  File \"/usr/local/lib/python3.10/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n  File \"/usr/local/lib/python3.10/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n    stream = await self._network_backend.connect_tcp(**kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n    return await self._backend.connect_tcp(\n  File \"/usr/local/lib/python3.10/site-packages/httpcore/_backends/anyio.py\", line 115, in connect_tcp\n    stream: anyio.abc.ByteStream = await anyio.connect_tcp(\n  File \"/usr/local/lib/python3.10/site-packages/anyio/_core/_sockets.py\", line 227, in connect_tcp\n    async with create_task_group() as tg:\nRuntimeError: Task got bad yield: True\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/app/src/app.py\", line 2106, in doc_analysis\n  File \"/app/src/app.py\", line 1450, in call_vlm\n  File \"/usr/local/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1727, in create\n    return await self._post(\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1856, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1550, in request\n    return await self._request(\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1613, in _request\n    return await self._retry_request(\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1683, in _retry_request\n    return await self._request(\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1613, in _request\n    return await self._retry_request(\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1683, in _retry_request\n    return await self._request(\n  File \"/usr/local/lib/python3.10/site-packages/openai/_base_client.py\", line 1623, in _request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nubuntu 22.04\n\n### Python version\n\npython 3.10\n\n### Library version\n\nopenai v1.61.0",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "body": "Hi,\nI think this issue is caused due to a network error, you can check https://help.openai.com/en/articles/6897191-apiconnectionerror to resolve the error\nHope this helps"
      }
    ]
  },
  {
    "issue_number": 2124,
    "title": "Create and Get OpenAI Keys Programmatically.",
    "author": "anointingmayami",
    "state": "closed",
    "created_at": "2025-02-15T12:36:25Z",
    "updated_at": "2025-02-17T09:20:18Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nFor AI Usage and Ethics following standing UI/UX for users, this feature will be an incredible function for the OAuth utility.\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "body": "Hi,\n\nI don't think it is really useful to be able to create and get openai keys programmatically but something that might be useful maybe a future implementation is for openai to have their command-line interface (CLI) library which we can use for commands and such `openai login` or something similar.\n\nHope this helps"
      },
      {
        "user": "anointingmayami",
        "body": "The CLI is not a bad option either. However, for easy usage and user\r\nexperience the Key should be enabled as well as a feature accessible\r\nprogrammatically for OAuth user login for frontend user interactions this\r\nexperience is good.\r\nGod is good. Keep it Holy.\r\n\r\nKing Anointing J. Mayami\r\nPrivate Wealth Consultant & Manager | AI Specialist\r\n+2348140070908\r\n***@***.***\r\n\r\n\r\nOn Sat, Feb 15, 2025 at 4:41 PM Ranuga ***@***.***> wrote:\r\n\r\n> Hi,\r\n>\r\n> I don't think it is really useful to be able to create and get openai keys\r\n> programmatically but something that might be useful maybe a future\r\n> implementation is for openai to have their command-line interface (CLI)\r\n> library which we can use for commands and such openai login or something\r\n> similar.\r\n>\r\n> Hope this helps\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/openai/openai-python/issues/2124#issuecomment-2660975922>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BK7UCYZAXC3IT46FQIAOOHL2P5N3NAVCNFSM6AAAAABXGNRI4GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMNRQHE3TKOJSGI>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n> [image: Programmer-RD-AI]*Programmer-RD-AI* left a comment\r\n> (openai/openai-python#2124)\r\n> <https://github.com/openai/openai-python/issues/2124#issuecomment-2660975922>\r\n>\r\n> Hi,\r\n>\r\n> I don't think it is really useful to be able to create and get openai keys\r\n> programmatically but something that might be useful maybe a future\r\n> implementation is for openai to have their command-line interface (CLI)\r\n> library which we can use for commands and such openai login or something\r\n> similar.\r\n>\r\n> Hope this helps\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/openai/openai-python/issues/2124#issuecomment-2660975922>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BK7UCYZAXC3IT46FQIAOOHL2P5N3NAVCNFSM6AAAAABXGNRI4GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMNRQHE3TKOJSGI>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2127,
    "title": "OpenAI Developer Circle: Empowering Developers to Build Expertise and Careers",
    "author": "anointingmayami",
    "state": "closed",
    "created_at": "2025-02-15T21:31:53Z",
    "updated_at": "2025-02-15T21:33:25Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nIn today’s fast-paced tech industry, staying ahead requires not just knowledge but also a strong community of like-minded developers. The **OpenAI Developer Circle** is a thriving WhatsApp-based developer community designed to empower beginners and professionals alike to build industry expertise and successful careers in AI and software development.\n\n### Why Join the OpenAI Developer Circle?\n\n- **Learn from Experts**: Gain insights from industry professionals, AI researchers, and experienced developers who share valuable knowledge, best practices, and resources.\n- **Stay Updated**: Get the latest trends, news, and advancements in OpenAI technologies, machine learning, and software development.\n- **Hands-on Projects**: Work on collaborative projects, real-world applications, and innovative AI solutions with fellow community members.\n- **Career Growth**: Access mentorship, career guidance, and networking opportunities to help you transition from a beginner to an industry expert.\n- **Exclusive Resources**: Receive access to curated learning materials, tutorials, and development tools to accelerate your growth.\n\n### Who Can Join?\nWhether you're a beginner exploring AI, a student looking to gain practical experience, or a developer seeking to enhance your expertise, the OpenAI Developer Circle welcomes everyone passionate about technology and innovation.\n\n### How to Get Involved\nJoining is simple! Connect with us on WhatsApp, engage in discussions, participate in coding challenges, and collaborate with a global network of developers shaping the future of AI.\n\nTake the next step in your developer journey with the **OpenAI Developer Circle**—where learning, networking, and growth converge.\n\n🚀 [Join the movement today and be part of the AI revolution](https://chat.whatsapp.com/C2KVmimk7wuAFqCUq4NIDv)! \n\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Please do not promote unofficial groups or forums."
      }
    ]
  },
  {
    "issue_number": 2126,
    "title": "Realtime `ResponseOutputItemDoneEvent` model has incorrect type for the `item` field",
    "author": "fedirz",
    "state": "open",
    "created_at": "2025-02-15T17:51:33Z",
    "updated_at": "2025-02-15T17:51:33Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\n`ResponseOutputItemDoneEvent` doesn't support `item.content[0].type` to have a value of \"audio\" (the value returned by OpenAI's API)\n\n![Image](https://github.com/user-attachments/assets/57e1fcb7-f225-466d-ac50-61912db310f6)\n\n### To Reproduce\n\n```python\nfrom openai.types.beta.realtime import ConversationItem, ConversationItemContent, ResponseOutputItemDoneEvent\n\nResponseOutputItemDoneEvent(\n    event_id=\"event_id\",\n    response_id=\"response_id\",\n    type=\"response.output_item.done\",\n    output_index=0,\n    item=ConversationItem(content=[ConversationItemContent(type=\"audio\")]),\n)\n```\nOutput:\n```\nTraceback (most recent call last):\n  File \"/home/nixos/code/speaches/src/response_output_item_done_example.py\", line 8, in <module>\n    item=ConversationItem(content=[ConversationItemContent(type=\"audio\")]),\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nixos/code/speaches/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 214, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for ConversationItemContent\ntype\n  Input should be 'input_text', 'input_audio', 'item_reference' or 'text' [type=literal_error, input_value='audio', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/literal_error\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nLinux\n\n### Python version\n\nPython 3.12.5\n\n### Library version\n\nopenai 1.60.0",
    "comments": []
  },
  {
    "issue_number": 2125,
    "title": "Realtime `TurnDetection` model is missing `create_response` field",
    "author": "fedirz",
    "state": "open",
    "created_at": "2025-02-15T16:44:21Z",
    "updated_at": "2025-02-15T16:44:21Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\n`TurnDetection` model defined in src/openai/types/beta/realtime/session.py and src/openai/types/beta/realtime/session_create_response.py is missing a `create_response` field\n\n### To Reproduce\n\n```sh\nrg -A 40 'class .*TurnDetection'\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nLinux\n\n### Python version\n\nPython 3.12.8\n\n### Library version\n\nopenai v1.63.0",
    "comments": []
  },
  {
    "issue_number": 2104,
    "title": "temperature is not supported with this model(o3-mini)",
    "author": "gautamjajoo",
    "state": "closed",
    "created_at": "2025-02-09T05:07:19Z",
    "updated_at": "2025-02-14T21:39:18Z",
    "labels": [
      "question"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nTemperature is not supported in the o3 model. Similar issue was reported earlier(https://github.com/openai/openai-python/issues/2072) and it was supposed to be fixed in the `1.61.1` release(https://github.com/openai/openai-python/pull/2078)\n```\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'temperature' is not supported with this model.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_parameter'}}\n```\n\n\n### To Reproduce\n\n```\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an expert\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of France\"}\n]\n\nclient = OpenAI(api_key=api_key)\n\nresponse = client.chat.completions.create(\n    model=\"o3-mini\",\n    messages=messages,\n    temperature=0\n)\n\nprint(response.choices[0].message.content)\n```\n\nUsing this code, the above error pops up.\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.13.1\n\n### Library version\n\nopenai 1.61.1",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "#2072 was just about the CLI sending it when the user doesn't specify the `temperature` flag, in your snippet you're using the client API where if you specify `temperature` we're going to send it. If you're using a model that doesn't support temperature you shouldn't specify it."
      },
      {
        "user": "David2020-udec",
        "body": "Good afternoon\n\nHas the error finally been corrected??\n\n5 days ago the error continued to persist, even if the variable is not specified it is passed underneath, the only way to find out is to print it when executing the model. \n\n\n\n`\n\"openai\": ChatOpenAI(\n       model=\"o3-mini-2025-01-31\", \n        model_kwargs={\"reasoning_effort\": \"high\"}, \n        api_key=environ.get(\"OPENAI_API_KEY\")\n    ),\n\n\nprint(\"=== MODEL CONFIGURATION o3 ===\")\nprint(CHAT_MODELS[\"openai\"].dict())  # Check that 'temperature' is no longer present\nprint(\"===================================================\")`\n\n\n\n\n\n"
      },
      {
        "user": "RobertCraigie",
        "body": "That interface does not come from this SDK please report this to the library you're using."
      }
    ]
  },
  {
    "issue_number": 2112,
    "title": "Assistant API - Cannot modify assistant model from o1 to gpt4 due to reasoning_effort.",
    "author": "dchu-github",
    "state": "closed",
    "created_at": "2025-02-12T02:53:52Z",
    "updated_at": "2025-02-13T07:36:15Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThe `reasoning_effort` parameter does not work as expected in the Python lib or API side; it looks like it is already set when you create the O1 assistant object so when you try to **switch to gpt-4** series the API side will return an error due to `reasoning_effort` is already set but not support in gpt-4. \n\nLike:`\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported value: 'reasoning_effort' does not support 'medium' with this model.\", 'type': 'invalid_request_error', 'param': None, 'code': 'unsupported_value'}}`\n\n### To Reproduce\n\n```\nassistant = client.beta.assistants.create(\n    instructions=\"You are a personal math tutor. When asked a question, write and run Python code to answer the question.\",\n    name=\"Test\",\n    tools=[],\n    model=\"o1\",\n    description=\"Test\",\n    temperature=None,\n    top_p=None,\n)\n# You will get: \nAssistant(id='xxx', created_at=1739327593, description='xxx', instructions='You are a personal math tutor. When asked a question, write and run Python code to answer the question.', metadata={}, model='o1', name='xxx', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0, reasoning_effort='medium')\n\n# Try to change the model to gpt-4\nassistant = client.beta.assistants.update(\n    assistant_id=\"some_id\",\n    model=\"gpt-4o\",\n    temperature=1,\n    top_p=1,\n)\n\n# You will get:\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported value: 'reasoning_effort' does not support 'medium' with this model.\", 'type': 'invalid_request_error', 'param': None, 'code': 'unsupported_value'}}\n\n# If you put reasoning_effort into the request\nassistant = client.beta.assistants.update(\n    assistant_id=\"some_id\",\n    model=\"gpt-4o\",\n    temperature=1,\n    top_p=1,\n    reasoning_effort=None\n)\n\nTypeError: Assistants.update() got an unexpected keyword argument 'reasoning_effort'\n```\n\n### Code snippets\n\n```Python\nassistant = client.beta.assistants.create(\n    instructions=\"You are a personal math tutor. When asked a question, write and run Python code to answer the question.\",\n    name=\"Test\",\n    tools=[],\n    model=\"o1\",\n    description=\"Test\",\n    temperature=None,\n    top_p=None,\n)\n\nassistant = client.beta.assistants.update(\n    assistant_id=\"the_assistant_id\",\n    model=\"gpt-4o\",\n    temperature=1,\n    top_p=1,\n)\n\nassistant = client.beta.assistants.update(\n    assistant_id=\"the_assistant_id\",\n    model=\"gpt-4o\",\n    temperature=1,\n    top_p=1,\n    reasoning_effort=None\n)\n```\n\n### OS\n\nmacos\n\n### Python version\n\nPython 3.11.9 \n\n### Library version\n\nopenai 1.61.1 ",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nAs this is an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?\n\n> TypeError: Assistants.update() got an unexpected keyword argument 'reasoning_effort'\n\nThis one will be fixed in the next release, in the meantime you can pass it with `extra_body={'reasoning_effort': None}`."
      }
    ]
  },
  {
    "issue_number": 1483,
    "title": "Assistant API  should support images in base64 if chat completion does. ",
    "author": "ankitmplivo",
    "state": "closed",
    "created_at": "2024-06-14T19:27:39Z",
    "updated_at": "2025-02-13T01:17:58Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nI tried sending base 64 image via chat completion api, and it worked. When i tried the same via assistant api for the same model, it did not work. Looking at the implementation, since it's already supported by completions api.\n\n### To Reproduce\n\n1. Create a base64 image url (data:gdhf..) for any image. \r\n2. Pass this base64 as url in content to chat completions API it works fine and i get a response. \r\n3. Create an assistant. \r\n4. Create a thread with message containing the same base64 encoded image as url inside image_url. You get an error. \r\n\r\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid 'messages[0].content[1].image_url.url'. Expected a valid URL, but got a value with an invalid format.\", 'type': 'invalid_request_error', 'param': 'messages[0].content[1].image_url.url', 'code': 'invalid_value'}}\r\n\n\n### Code snippets\n\n```Python\n[{'role': 'user',\r\n  'content': [{'type': 'text', 'text': 'What’s in this image?'},\r\n   {'type': 'image_url',\r\n    'image_url': {'url': 'data:image/jpeg;base64,/9j...\r\n}\r\n}]\r\n}\r\n]\n```\n\n\n### OS\n\nMacOS\n\n### Python version\n\nPython v3.12\n\n### Library version\n\nlatest",
    "comments": [
      {
        "user": "Lee-daeho",
        "body": "I'm struggling with the same problem.\r\nis there any progress?"
      },
      {
        "user": "marioresl",
        "body": "same problem for me i used the same format as for the completions api but it doesnt work with the base64. I think i will use the filestorage and provide the id for the assistants api. I'm really new to the OpenAI API so i dont know how to work with it properly xD"
      },
      {
        "user": "Lee-daeho",
        "body": "@marioresl \r\nI'm new to the API especially Assistant too!\r\nI just turned into using file.id.\r\nIt works well unless code is a little bit more messy and still have no idea why url does not works ;)"
      }
    ]
  },
  {
    "issue_number": 2092,
    "title": "Throw exception if API key ENV vars are empty",
    "author": "iloveitaly",
    "state": "open",
    "created_at": "2025-02-05T12:48:45Z",
    "updated_at": "2025-02-12T16:53:32Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nIf you use an ENV var for setting the API key, and the ENV happens to be empty, no exception is thrown until a call is made and the exception is ambiguous (i.e., not clear that the token is empty).\n\n\n\n### To Reproduce\n\n```\nexport OPENAI_API_KEY=\"\"\n```\n\nand use the completions API\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\n15.3\n\n### Python version\n\nv3.13.1\n\n### Library version\n\n1.61.0",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "body": "Hi @iloveitaly, thanks for raising this. From what I’ve seen the API itself eventually throws an error when the API key ENV variable is empty. That said, I'll set up a PR to catch this earlier and provide a clearer message. Let's see what the OpenAI team has to say. Thanks for your patience and input!"
      },
      {
        "user": "iloveitaly",
        "body": "@Programmer-RD-AI it does, but the error is more obscure and you don't end up getting an error until after the code is run, which degrades the DX."
      }
    ]
  },
  {
    "issue_number": 2113,
    "title": "ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)",
    "author": "ekms2021",
    "state": "closed",
    "created_at": "2025-02-12T04:53:27Z",
    "updated_at": "2025-02-12T13:57:59Z",
    "labels": [],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen accessing Azure Open AI Service with `openai==1.61` from Microsoft Fabric notebook, I get the error below.\n\n```\nConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\nAPIConnectionError: Connection error.\n```\n\nThis is caused by a change in `httpx`, a dependency of `openai`.\nIf I execute `pip install -U openai`, `httpx==0.28.1` is installed, which causes the error.\n\nA workaround is to execute `pip install -U openai httpx==0.27.2`.\nAnother workaround is to add the code below.\n\n```\nimport certifi\nos.environ['SSL_CERT_FILE'] = certifi.where() \n```\n\n### To Reproduce\n\n\n```\n%pip install -U openai\n\nfrom pyspark.sql import Row\nfrom synapse.ml.services.openai import OpenAIChatCompletion\nfrom openai import AzureOpenAI\n\ndef make_message(role, content):\n    return Row(role=role, content=content, name=role)\n\ndef get_response(user_question):\n    prompt = f\"\"\"\n    You are an assistant who answer to my questions. Please answer the questions based on the following information.\n    If you do not have enough information to answer, please say \"I do not know\".\n    \"\"\"\n\n    messages = [{'role': 'system', 'content': prompt}, {'role': 'user', 'content': user_question}]\n\n    aoai_client = AzureOpenAI(\n        api_key = \"<api_key>\",\n        api_version = \"2024-02-01\",\n        azure_endpoint = \"https://<azure_open_ai_service>.openai.azure.com/\"\n    )\n\n    result = aoai_client.chat.completions.create(messages=messages, model=\"gpt-4o-mini\").choices[0].message.content\n    return result\n\nuser_question = \"Please tell me how to reserve.\"\nresponse = get_response(user_question)\nprint(response)\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nMariner 2.0\n\n### Python version\n\nPython v3.10.12\n\n### Library version\n\nopenai v1.61.1",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report but as this is an issue with the underlying http client can you ask for assistance in the httpx repo? https://github.com/encode/httpx"
      }
    ]
  },
  {
    "issue_number": 2111,
    "title": "o3-mini is not accesible",
    "author": "xarcraft-dev",
    "state": "closed",
    "created_at": "2025-02-11T19:26:34Z",
    "updated_at": "2025-02-11T19:34:52Z",
    "labels": [
      "question"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nI am trying to use o3-mini for my python project but I get an error saying this:\n```\nTraceback (most recent call last):\n  File \"/home/openai/dev/ai_benchmark/ai_benchmark.py\", line 41, in <module>\n    answer = send_question(models[i - 1], \"What is 4 * 4 + 4?\")\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/openai/dev/ai_benchmark/ai_benchmark.py\", line 10, in send_question\n    stream = client.chat.completions.create(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/openai/.local/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/openai/.local/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 863, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/openai/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1283, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/openai/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 960, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"/home/openai/.local/lib/python3.12/site-packages/openai/_base_client.py\", line 1064, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `o3-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n```\n\nBy the way, this issue happens on both windows and wsl.\n\n### To Reproduce\n\nCreate a chat completion\nWrite `o3-mini` for the model.\nRun the project.\n\n```python\nfrom openai import OpenAI\n\nexpected_answer = \"20\"\n\nclient = OpenAI(\n    api_key=\"[REDACTED_OPENAI_KEY]\"\n)\n\ndef send_question(ai_model, question):\n    stream = client.chat.completions.create(\n        model=ai_model,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Answer the asked math questions like this (assuming asked question is 2+2): '4'. When writing a float, write with a comma: '4,5'.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": question\n            }\n        ],\n        stream=True,\n    )\n    \n    for chunk in stream:\n        if chunk.choices[0].delta.content is not None:\n            answer = chunk.choices[0].delta.content\n    \n    return answer\n\nmodels = [\n    \"gpt-4\",\n    \"gpt-4o-mini\"\n    \"chatgpt-4o-latest\",\n    \"o1-mini\",\n    \"o1\",\n    \"o3-mini\"\n]\n\nfor i in range(len(models)):\n    answer = send_question(models[i - 1], \"What is 4 * 4 + 4?\")\n    if answer == expected_answer:\n        print(\"Test passed!\")\n    else:\n        print(\"Test failed!\")\n```\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nWindows and WSL\n\n### Python version\n\n3.12.3\n\n### Library version\n\nopenai v1.61.1",
    "comments": [
      {
        "user": "xarcraft-dev",
        "body": "By the way, please correct if I'm wrong. I am a newbie."
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 800,
    "title": "The chatCompletion operation does not work with the specified model, gpt-35-turbo-instruct.",
    "author": "chen-gdp",
    "state": "closed",
    "created_at": "2023-11-13T08:44:54Z",
    "updated_at": "2025-02-11T19:09:53Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\r\n\r\n- [X] This is an issue with the Python library\r\n\r\n### Describe the bug\r\n\r\nCan someone please help me? The code is error when it tried to use the `gpt-35-turbo-instruct` for Azure services. When I used the `gpt-35-turbo` it works fine.\r\nBadRequestError: Error code: 400 - {'error': {'code': 'OperationNotSupported', 'message': 'The chatCompletion operation does not work with the specified model, gpt-35-turbo-instruct. Please choose different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.'}}\r\n\r\n\r\n\r\n### To Reproduce\r\n\r\n1. Install Requirements\r\n```\r\nlangchain==0.0.335\r\nazure-search-documents==11.4.0b8\r\ntorch==2.1.0\r\nopenai==1.2.3\r\nazure-identity==1.15.0\r\n```\r\n2.  Execute Code, The deployment name is set to `gpt-35-turbo-instruct`\r\n\r\n### Code snippets\r\n\r\n```Python\r\nazure_openai_model = AzureChatOpenAI(\r\n    azure_deployment=os.getenv(AZURE_MODEL_DEPLOYMENT_NAME),\r\n    azure_endpoint=os.getenv(AZURE_OPENAI_API_ENDPOINT),\r\n    openai_api_key=os.getenv(OPENAI_API_KEY),\r\n)\r\nprint(azure_openai_model.predict(\"Count 1 to 10\"))\r\n```\r\n\r\n\r\n### OS\r\n\r\nLinux Ubuntu 22.04\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Library version\r\n\r\nopenai v1.2.3",
    "comments": [
      {
        "user": "mrbullwinkle",
        "body": "@chen-gdp this is expected behavior, `turbo-instruct` isn't intended to be used with the chat completion API/endpoint. It can only be used with the completion API/endpoint. If you need an example of using `gpt-35-turbo-instruct` with Azure OpenAI and the completions API: [Azure OpenAI completions quickstart](https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython&pivots=programming-language-python).  \r\n\r\n![image](https://github.com/openai/openai-python/assets/31510320/a5acda16-1204-46de-89c5-c05ee268c7bf)\r\n"
      },
      {
        "user": "chen-gdp",
        "body": "Is it possible to use AzureOpenAI through Langchain, rather than directly from OpenAI? When I attempted to use AzureOpenAI for the `gpt-3.5-turbo-instruct` model, I encountered an error: 'AttributeError: module 'openai' has no attribute 'error'."
      },
      {
        "user": "mrbullwinkle",
        "body": "It looks like the latest release of langchain is not currently updated to fully support the openai 1.x release that came out on 11/6 (which you appear to be currently running) as this was a complete rewrite of the OpenAI API library with breaking changes. It does look like there are some PRs in the langchain repo to add in this support like this one here: https://github.com/langchain-ai/langchain/pull/13231/files so this support should hopefully be coming soon.\r\n\r\nSo your options right now would likely be either to test temporarily downgrading back to a supported version that works with langchain pre the 11/6 breaking changes:\r\n\r\n`pip uninstall openai`\r\n`pip install openai==0.28.1`\r\n\r\nOr wait till PRs like the one above get merged and upgraded to the latest version of langchain once support is added."
      }
    ]
  },
  {
    "issue_number": 2109,
    "title": "Feature Request: Support for Request/Response Interceptors",
    "author": "Nisarg38",
    "state": "closed",
    "created_at": "2025-02-11T05:57:32Z",
    "updated_at": "2025-02-11T15:34:26Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\n## Description  \nIt would be great to have built-in support for interceptors in the `openai-python` library, allowing users to modify requests before they are sent and process responses after they are received. This would provide a clean and extensible way to add logging, metrics, retries, authentication adjustments, or any other middleware-like functionality without modifying the core library code.\n\n## Proposed Solution\nIntroduce a mechanism similar to request/response interceptors found in other API client libraries. This could be achieved by adding configurable hooks or middleware that developers can pass when initializing the OpenAI client.\n\n### Additional context\n\n## Implementation  \n\nThis implementation introduces a flexible interceptor system for handling request modifications before they are sent and response processing after they are received.  \n\n\n### **Code Implementation**  \n```python\n@dataclass\nclass InterceptorRequest:\n    \"\"\"Request data container for interceptor processing\"\"\"\n\n    method: str\n    url: str\n    headers: Dict[str, str]\n    params: Optional[Dict[str, Any]] = None\n    body: Optional[Union[Body, bytes]] = None\n\n\n@dataclass\nclass InterceptorResponse(Generic[T]):\n    \"\"\"Response data container for interceptor processing\"\"\"\n\n    status_code: int\n    headers: Dict[str, str]\n    body: T\n    request: InterceptorRequest\n    raw_response: httpx.Response\n\n\nclass Interceptor(ABC):\n    \"\"\"Base class for request/response interceptors\"\"\"\n\n    @abstractmethod\n    def before_request(self, request: InterceptorRequest) -> InterceptorRequest:\n        \"\"\"Process request before sending\"\"\"\n        pass\n\n    @abstractmethod\n    def after_response(self, response: InterceptorResponse[T]) -> InterceptorResponse[T]:\n        \"\"\"Process response after receiving\"\"\"\n        pass\n\n\nclass InterceptorChain:\n    \"\"\"Chain of interceptors for sequential request/response processing\"\"\"\n\n    def __init__(self, interceptors: Optional[list[Interceptor]] = None):\n        self._interceptors = interceptors or []\n\n    def add_interceptor(self, interceptor: Interceptor) -> None:\n        \"\"\"Adds an interceptor to the chain\"\"\"\n        self._interceptors.append(interceptor)\n\n    def execute_before_request(self, request: InterceptorRequest) -> InterceptorRequest:\n        \"\"\"Executes all interceptors before sending the request\"\"\"\n        current_request = request\n        for interceptor in self._interceptors:\n            try:\n                current_request = interceptor.before_request(current_request)\n            except Exception:\n                continue\n        return current_request\n\n    def execute_after_response(self, response: InterceptorResponse[T]) -> InterceptorResponse[T]:\n        \"\"\"Executes all interceptors after receiving the response\"\"\"\n        current_response = response\n        for interceptor in self._interceptors:\n            try:\n                current_response = interceptor.after_response(current_response)\n            except Exception:\n                continue\n        return current_response\n```\n\n## Related Pull Request\nI’ve submitted a PR implementing this feature: [#2032](https://github.com/openai/openai-python/pull/2032). Would love feedback and thoughts on how this could be integrated!\n",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the detailed suggestion and putting up a PR! However we're going to defer adding something like this for now as you can achieve the same thing with httpx's event hooks https://www.python-httpx.org/advanced/event-hooks/"
      },
      {
        "user": "Nisarg38",
        "body": "I'll take a look. I am trying to add a functionality and it might work with event hook. Thanks for the help! "
      }
    ]
  },
  {
    "issue_number": 2084,
    "title": "[Batch Inference API]: failed batch tasks cannot removed from task queue",
    "author": "aaronlim0919",
    "state": "closed",
    "created_at": "2025-02-04T04:52:42Z",
    "updated_at": "2025-02-11T11:22:26Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nI was pushed a batch of work to OpenAI Batch Inference, and it was exceeded the 2 million enqueued token limit. It was failed because format validation error, and I wish to remove the batch from task queue. The removal was failed and following error message was shown:\n\n```\nBatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-snq6hvcTYRCMyB87hjZjkoFr. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)\n```\n Besides on removing by API, removal from API management console was also failed and return same error message\n\n### To Reproduce\n\nunable to reproduce since the batch queue is full of failed tasks, and I cannot remove it for reproducing the error.\n\n### Code snippets\n\n```Python\ntotally follow example provided by OpenAI example:\n\nbatch_files = self.model_client.files.create(\n   file=open(self.batch_inference_files_dir + file, \"rb\"),\n   purpose=\"batch\",\n)\n            \nbatch_work_obj = self.model_client.batches.create(\n   input_file_id=batch_files.id,\n   endpoint='/v1/chat/completions',\n   completion_window='24h',\n   metadata={\n      \"description\":\"batch inference of FAQ\"\n   }\n)\n\nprogress = self.model_client.batches.retrieve(batch_id=self.batch_work_obj_id)\nif progress.status == 'failed':\n   self.model_client.batches.cancel(self.batch_work_obj_id)\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.12\n\n### Library version\n\nOpenAI v1.54.5",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2108,
    "title": "Assistant SDK does not work on o3-mini",
    "author": "nathan-oscilar",
    "state": "closed",
    "created_at": "2025-02-10T19:34:21Z",
    "updated_at": "2025-02-10T19:35:11Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nThis is not a complete list but something I tried did not work:\n- beta.assistants.update: cannot update the assistant from gpt-4o to o3-mini (ideally should work both ways)\n- beta.threads.runs.create: cannot create runs with o3-mini(unsupported reasoning_efforting). Ideally the config here would overwrite the assistant config(whether it's using gpt or o models)\n\n### To Reproduce\n\n1. create an assistant using gpt-4o\n2. try to update the assistant with o3-mini, OR create a run that use o3-mini\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.11\n\n### Library version\n\nopenai 1.61.1",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2061,
    "title": "`client.beta.chat.completions.parse` returns `tool_calls` with empty array, which is invalid for message history",
    "author": "in-op",
    "state": "closed",
    "created_at": "2025-01-28T22:41:10Z",
    "updated_at": "2025-02-10T18:09:25Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen using `client.beta.chat.completions.parse`, and with no tools involved, the `ParsedChatCompletionMessage` response will include a `tool_calls` with the empty array `[]` instead of `None`. This is invalid when including that in the message history you pass to subsequent calls to `client.beta.chat.completions.parse` or `client.chat.completions.create`, and will blow up with:\n\n```\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"Invalid 'messages[1].tool_calls': empty array. Expected an array with minimum length 1, but got an empty array instead.\", 'type': 'invalid_request_error', 'param': 'messages[1].tool_calls', 'code': 'empty_array'}}\n```\n\n### To Reproduce\n\nRun any `client.beta.chat.completions.parse` call and append that message to a message history of a subsequent request.\n\n### Code snippets\n\n```Python\nimport openai\n\nclient = openai.OpenAI()\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What color is the sky?\"},\n]\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-mini\",\n    messages=messages,\n)\n\nprint(completion.choices[0].message)\nmessages.append(completion.choices[0].message)\nmessages.append(\n    {\"role\": \"user\", \"content\": \"What color is the ocean?\"},\n)\n\ncompletion = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages,\n)\n\nprint(completion.choices[0].message)\n```\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.12.5\n\n### Library version\n\nopenai==1.60.2",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report and including a reproduction script! This will be fixed in the next release https://github.com/openai/openai-python/pull/2095"
      }
    ]
  },
  {
    "issue_number": 1778,
    "title": "Using realtime API with python client",
    "author": "someshfengde",
    "state": "closed",
    "created_at": "2024-10-07T12:34:58Z",
    "updated_at": "2025-02-10T17:43:44Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [X] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nI had fastAPI server running on my application already. I wanted to make it's user interface voice to voice type. On openai's official documentation only steps for node.js is given. I was wondering if I'd be able to reuse my existing code ( fastapi + openai python ) for voice interface too. \n\n### Additional context\n\nRealtime API Docs : https://platform.openai.com/docs/guides/realtime\r\n\r\ncurrently not sure how can I use openai-python for realtime API. ",
    "comments": [
      {
        "user": "hayescode",
        "body": "Microsoft has a python version you can use. Please note that there are connection differences between OpenAI and Azure OpenAI that you'll need to change, but the rest should work for you.\n\nhttps://github.com/Azure-Samples/aoai-realtime-audio-sdk/tree/main/python"
      },
      {
        "user": "SergioB-dev",
        "body": "I'm hoping to see this feature land here as well."
      },
      {
        "user": "someshfengde",
        "body": "If I'm not wrong azure-openai does support openai directly. (From their implmentation it seems like they do) \r\n\r\nthey've method for getting messages with openai. \r\n\r\nhttps://github.com/Azure-Samples/aoai-realtime-audio-sdk/blob/8105a5c3ab9cc54fe864aa6f8259f72c6829eec7/python/samples/low_level_sample.py#L251"
      }
    ]
  },
  {
    "issue_number": 1785,
    "title": "duration type in TranscriptionVerbose",
    "author": "viewee-ohm",
    "state": "closed",
    "created_at": "2024-10-09T08:24:36Z",
    "updated_at": "2025-02-10T17:42:47Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\n<img width=\"1149\" alt=\"image\" src=\"https://github.com/user-attachments/assets/02bd733e-3c69-4364-ba60-50d7deb3528e\">\r\n\r\nin pyantic model\r\nduration is set to str\r\n\r\nbut the example in api\r\nit returns float type\n\n### To Reproduce\n\nopenai_client.audio.transcriptions.create\n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.10.15\n\n### Library version\n\n1.51.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the report, this should be fixed in v1.61.1!"
      }
    ]
  },
  {
    "issue_number": 2075,
    "title": "Client-Specific Token Tracking and Cost Estimation",
    "author": "Programmer-RD-AI",
    "state": "closed",
    "created_at": "2025-02-02T12:41:08Z",
    "updated_at": "2025-02-10T13:53:11Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\n\nI propose adding **client-specific token tracking and cost estimation** to the `openai-python` library. This would allow developers to monitor token consumption and associated costs at the client instance level, improving resource management and budgeting.  \n\n#### **Proposed Implementation**  \nIntroduce an optional `track` parameter in the `OpenAI` client:  \n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"your_api_key\", track=True)  # Enables tracking\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n\n# Retrieve token usage\nprint(client.token_usage)  # Example: {'input_tokens': 15, 'output_tokens': 30, 'total_tokens': 45}\n\n# Retrieve estimated cost\nprint(client.estimated_cost)  # Example: \"$0.0023\"\n```\n\n#### **Key Features**  \n- **Client-Specific Token Tracking**: Tracks input, output, and total tokens per client instance.  \n- **Cost Estimation**: Computes estimated costs based on OpenAI’s pricing model.  \n- **Configurable Tracking**: Developers can enable (`track=True`) or disable (`track=False`) tracking.  \n- **Utility Methods**:  \n  - `client.get_token_usage()`: Retrieve token consumption data.  \n  - `client.get_estimated_cost()`: Retrieve estimated cost based on OpenAI’s pricing.  \n  - `client.reset_tracking()`: Reset token usage tracking.  \n\n#### **Implementation Considerations**  \n- Utilize `tiktoken` for accurate token counting before API requests.  \n- Maintain tracking at the client instance level to avoid global state interference.  \n- Optimize for minimal performance overhead.  \n\n### Additional context\n\nThis feature would provide developers with better transparency and control over API usage, helping optimize costs and manage multiple client instances efficiently. It would be particularly useful for applications with multiple API users, improving cost tracking on a per-client basis.  \n\nWould appreciate feedback on feasibility and implementation.",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the detailed suggestion! This definitely sounds like a helpful feature but at this time we'd rather not implement this at the SDK level and leave billing details to the API, especially as the SDK can only know about the requests made through it and not other clients."
      }
    ]
  },
  {
    "issue_number": 2059,
    "title": "Idea: Use array type for embedding speed-up",
    "author": "pamelafox",
    "state": "closed",
    "created_at": "2025-01-28T21:36:43Z",
    "updated_at": "2025-02-10T13:51:00Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nThe SDK currently uses numpy to speed up embedding:\n\n                embedding.embedding = np.frombuffer(  # type: ignore[no-untyped-call]\n                    base64.b64decode(data), dtype=\"float32\"\n                ).tolist()\n\nIt does seem to improve performance, based on our tests, but we were wondering if similar gains could be made without numpy, using the built-in array type? Have you tried that already?\n\nhttps://docs.python.org/3/library/array.html\n\nWe're having some pains with the numpy dependency for our Azure samples and are looking for ways to move off it without affecting performance. \n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "I have not tested with the array type but I'm also curious if it could provide similar improvements. Would you be able to share some examples of what the changes we'd need to make would look like?"
      },
      {
        "user": "tonybaloney",
        "body": "Looking at what numpy is used for in the embeddings type, that is taking a base64 bytes object as a buffer (non-copy reference), then converting it into a compact float32 single-dimension array in numpy, then back out to a list of native floats, you can do that with the builtin array type:\n\n```python\nembedding.embedding = array.array(\"f\", base64.b64decode(data)).tolist()\n```\n\nHave submitted this in a draft PR"
      },
      {
        "user": "tonybaloney",
        "body": "Benchmark:\n\n```python\nimport array\nimport base64\nimport numpy as np\nimport json\n\n# Sample data\ndata = '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'\n\nas_json = json.dumps(array.array(\"f\", base64.b64decode(data)).tolist())\n\ndef bench_standard():\n    for _ in range(1000):\n        json.loads(as_json)\n\ndef bench_array():\n    for _ in range(1000):\n        array.array(\"f\", base64.b64decode(data)).tolist()\n\ndef bench_numpy():\n    for _ in range(1000):\n        np.frombuffer(  # type: ignore[no-untyped-call]\n                        base64.b64decode(data), dtype=\"float32\"\n                    ).tolist()\n\n__benchmarks__ = [\n    (bench_standard, bench_array, \"Standard vs array\"),\n    (bench_standard, bench_numpy, \"Standard vs numpy\"),\n    (bench_numpy, bench_array, \"Array vs numpy\"),\n]\n\n```\n\nReplace `json` with a more efficient encoder (orjson)\n\nResults show this array approach is equivalent to the numpy one (10-20% faster) and is significantly faster than the standard approach (10x):\n                      \n|         Benchmark | Min     | Max     | Mean    | Min (+)         | Max (+)         | Mean (+)        |\n|-------------------|---------|---------|---------|-----------------|-----------------|-----------------|\n| Standard vs array | 3.115   | 3.388   | 3.223   | 0.225 (13.8x)   | 0.256 (13.2x)   | 0.242 (13.3x)   |\n| Standard vs numpy | 2.941   | 3.330   | 3.135   | 0.263 (11.2x)   | 0.306 (10.9x)   | 0.280 (11.2x)   |\n|    Array vs numpy | 0.256   | 0.273   | 0.264   | 0.218 (1.2x)    | 0.227 (1.2x)    | 0.222 (1.2x)    |"
      }
    ]
  },
  {
    "issue_number": 1795,
    "title": "Error code: 400 - {'error': {'message': \"Invalid parameter: 'tool_calls' cannot be used when 'functions' are present",
    "author": "Birdy647JH",
    "state": "closed",
    "created_at": "2024-10-17T14:18:51Z",
    "updated_at": "2025-02-10T13:49:58Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nError code: 400 - {'error': {'message': \"Invalid parameter: 'tool_calls' cannot be used when 'functions' are present. Please use 'tools' instead of 'functions'.\", 'type': 'invalid_request_error', 'param': 'messages.[2].tool_calls', 'code': None}}\n\n### To Reproduce\n\nStructure is in the code snippet, after MAssassor answer its question (after using tool and then go back to MAssasor), should be go back to supervisor, but it give above error. \r\n<img width=\"1417\" alt=\"Screenshot 2024-10-17 at 10 17 20 AM\" src=\"https://github.com/user-attachments/assets/0ac67215-02c2-42f0-b991-5a86c1589eed\">\r\n\n\n### Code snippets\n\n```Python\n## first read guideline and then go to the different source to get data: \r\ndef create_team_supervisor(llm, system_prompt, members) -> str:\r\n    \"\"\"An LLM-based router.\"\"\"\r\n    options = [\"FINISH\"] + members\r\n    function_def = {\r\n        \"name\": \"route\",\r\n        \"description\": \"Select the next role.\",\r\n        \"parameters\": {\r\n            \"title\": \"routeSchema\",\r\n            \"type\": \"object\",\r\n            \"properties\": {\r\n                \"next\": {\r\n                    \"title\": \"Next\",\r\n                    \"anyOf\": [\r\n                        {\"enum\": options},\r\n                    ],\r\n                },\r\n            },\r\n            \"required\": [\"next\"], \r\n        },\r\n    }\r\n    \r\n    prompt = ChatPromptTemplate.from_messages(\r\n        [\r\n            (\"system\", system_prompt),\r\n            MessagesPlaceholder(variable_name=\"messages\"),\r\n            (\r\n                \"system\",\r\n                \"Given the conversation above, who should act next?\"\r\n                \" Or should we FINISH? Select one of: {options}\",\r\n            ),\r\n        ]\r\n    ).partial(options=str(options), team_members=\", \".join(members))\r\n    return (\r\n        prompt\r\n        | llm.bind_functions(functions=[function_def], function_call=\"route\")   \r\n        | JsonOutputFunctionsParser()\r\n    )\r\n\r\n\r\nclass TeamState(TypedDict):\r\n    # A message is added after each team member finishes\r\n    messages: Annotated[List[BaseMessage], operator.add]\r\n    # The team members are tracked so they are aware of\r\n    # the others' skill-sets\r\n\r\n    POLICY_NUMBER: str \r\n    CLAIM_NUMBER: str \r\n    FILE_PATH: str\r\n\r\n    team_members: List[str]\r\n    # Used to route work. The supervisor calls a function\r\n    # that will update this every time it makes a decision\r\n    next: str\r\n\r\n    sender:str\r\n\r\ndef router(state):\r\n    # This is the router\r\n    messages = state[\"messages\"]\r\n    last_message = messages[-1]\r\n    if not last_message.content:\r\n        # The previous agent is invoking a tool\r\n        # print(state['sender'], 'call_tool')\r\n        return \"call_tool\"\r\n    # print(state['sender'], \"CogSupervisor\")\r\n    return 'continue'\r\n\r\ngraph = StateGraph(TeamState)\r\n\r\n## Add nodes to the graph\r\ngraph.add_node(\"MAssassor\", med_node)\r\ngraph.add_node(\"BAssassor\", beh_node)\r\ngraph.add_node(\"Supervisor\", cog_supervisor)\r\ngraph.add_node(\"call_tool\", tool_node)\r\n\r\ngraph.add_edge(START, \"Supervisor\")\r\n\r\n## Add condition edges to the graph \r\ngraph.add_conditional_edges(\"MAssassor\",\r\n    router,\r\n    {\"call_tool\": \"call_tool\", \"continue\": \"Supervisor\"},) \r\ngraph.add_conditional_edges(\"BAssassor\",\r\n    router,\r\n    {\"call_tool\": \"call_tool\", \"continue\": \"Supervisor\"},)\r\ngraph.add_conditional_edges(\r\n    \"Supervisor\",\r\n    lambda x: x[\"next\"],\r\n    {\"MAssassor\": \"MAssassor\",\r\n     \"BAssassor\": \"BAssassor\", \r\n     \"FINISH\": END},\r\n)\r\n\r\ngraph.add_conditional_edges(\r\n    \"call_tool\",\r\n    lambda x: x[\"sender\"],\r\n    {\"MAssassor\": \"MAssassor\",\r\n     \"BAssassor\": \"BAssassor\",\r\n    },\r\n)\r\n\r\nchain = graph.compile()\n```\n\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.9.6\n\n### Library version\n\nopenai 1.51.2",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "body": "Hi,\n\nYou can check https://github.com/langchain-ai/langsmith-sdk/issues/705 and https://github.com/run-llama/llama_index/issues/10493\n\nThey discuss the same issue\n\nBest regards"
      },
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2070,
    "title": "OpenAI call for foundation model fails with 401",
    "author": "PurnaChandraPanda",
    "state": "closed",
    "created_at": "2025-02-01T04:22:11Z",
    "updated_at": "2025-02-10T13:49:27Z",
    "labels": [
      "question"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nClient code is tried for azure ai model inference using openai sdk. The code is as below.\n\nDebug logs indicate api-key is not passed, but OpenAI() ctor is passed though.\n\n```\nDEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Who is the most renowned French painter? Provide a short answer.'}], 'model': 'Phi-3.5-vision-instruct', 'max_tokens': 100}}\nDEBUG:openai._base_client:Sending HTTP Request: POST https://mysvc.services.ai.azure.com/models/chat/completions\nDEBUG:httpcore.connection:connect_tcp.started host='mysvc.services.ai.azure.com' port=443 local_address=None timeout=5.0 socket_options=None\n..\n..\nINFO:httpx:HTTP Request: POST https://mysvc.services.ai.azure.com/models/chat/completions \"HTTP/1.1 401 Unauthorized\"\nDEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\nDEBUG:httpcore.http11:receive_response_body.complete\nDEBUG:httpcore.http11:response_closed.started\nDEBUG:httpcore.http11:response_closed.complete\nDEBUG:openai._base_client:HTTP Response: POST https://mysvc.services.ai.azure.com/models/chat/completions \"401 Unauthorized\" Headers({'content-length': '161', 'content-type': 'application/json', 'x-ms-client-request-id': 'Not-Set', 'apim-request-id': 'c1361ba9-b3d7-44d2-8884-96ea36d5341b', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'date': 'Sat, 01 Feb 2025 04:05:32 GMT'})\nDEBUG:openai._base_client:request_id: None\nDEBUG:openai._base_client:Encountered httpx.HTTPStatusError\n```\n\nSame request is tried via curl and it works.\n\ncurl -X POST https://mysvc.services.ai.azure.com/models/chat/completions?api-version=2024-05-01-preview -H 'Content-Type: application/json' -H \"api-key: my_api_key\" -d '{\"model\":\"phi-3.5-vision-instruct\", \"messages\":[{\"content\":\"You are a helpful assistant. What is good about Wuhan?\",\"role\":\"user\"}], \"max_tokens\": 50}'\n\nQuestion:\nDoes it mean openai sdk misses to pass api_key in chat completions api call?\n\n### To Reproduce\n\nRun the code below and invoke a serverless api from azure ai foundry.\n\n### Code snippets\n\n```Python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=base_uri, api_key=os.environ[\"OPENAI_API_KEY\"])\n\nimport logging\nimport sys\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n\nresponse = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Who is the most renowned French painter? Provide a short answer.\",\n        }\n    ],\n    model=\"Phi-3.5-vision-instruct\",\n    max_tokens=100,\n)\n```\n\n### OS\n\nubuntu\n\n### Python version\n\npython 3.10\n\n### Library version\n\n1.60.1",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "It looks like you're using the Azure API, it should work if you use `AzureOpenAI` instead https://github.com/openai/openai-python?tab=readme-ov-file#microsoft-azure-openai"
      },
      {
        "user": "PurnaChandraPanda",
        "body": "Hello @RobertCraigie \n\nI am not using AzureOpenAI. I am using oss model like phi under the \"azure ai model service endpoint\" in ai foundry - https://learn.microsoft.com/en-us/azure/ai-foundry/model-inference/how-to/inference?tabs=python#using-the-routing-capability-in-the-azure-ai-model-inference-endpoint. As per which, the base_url would appear as https://{myresource}.services.ai.azure.com/models.\n\n![Image](https://github.com/user-attachments/assets/859a7462-1214-4c75-8fdd-bd3978f37889)\n\nI am just curious why runtime is not considering the api_key though passed by caller as expected."
      },
      {
        "user": "RobertCraigie",
        "body": "The SDK isn't passing through the `api-key` header because that isn't required for usage with the official OpenAI API.\n\nUnless I'm misunderstanding, you are using a model deployed on Azure services? In any case, using `AzureOpenAI` instead should work for you as it sets the `api-key` header. If that still doesn't work then you can manually specify `api-key` like so\n```py\nOpenAI(default_headers={'api-key': '...'})\n```"
      }
    ]
  },
  {
    "issue_number": 2105,
    "title": "OpenAI Whisper endpoint doesn't transcribe the audio properly",
    "author": "Syed-Mujeeb",
    "state": "closed",
    "created_at": "2025-02-10T12:27:01Z",
    "updated_at": "2025-02-10T13:03:19Z",
    "labels": [
      "API-feedback"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [ ] This is an issue with the Python library\n\n### Describe the bug\n\neg:\nWhat I asked \"hi can you assist me \"\nWhat it assumed \"Hi Kenyas speak to me\"\n\nSometimes, getting random words which is not part of my speech\n\n### To Reproduce\n\n1. Record the speech as audio\n2. Pass it to whisper\n\n### Code snippets\n\n```Python\n\n```\n\n### OS\n\nmacos\n\n### Python version\n\npythonv3.10\n\n### Library version\n\n1.16.1",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for reporting!  \n\nThis sounds like an issue with the underlying OpenAI API and not the SDK, so I'm going to go ahead and close this issue. \n\nWould you mind reposting at [community.openai.com](https://community.openai.com)?"
      }
    ]
  },
  {
    "issue_number": 2103,
    "title": "Why do `pydantic_function_tool` mark all fields as required?",
    "author": "Epsirom",
    "state": "closed",
    "created_at": "2025-02-08T10:53:37Z",
    "updated_at": "2025-02-10T08:53:01Z",
    "labels": [
      "question"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [x] This is an issue with the Python library\n\n### Describe the bug\n\nWhen using `pydantic_function_tool`, I noticed that all fields in a Pydantic model are forcibly marked as required in the generated JSON Schema, even if they have default values or are explicitly marked as optional. This behavior seems inconsistent with JSON Schema specifications, which state that only fields explicitly listed in the required array should be mandatory.\n\nI would like to confirm whether this behavior is intentional. If so, could you please clarify the design decision behind it?\n\nRelated code: https://github.com/openai/openai-python/blob/7193688e364bd726594fe369032e813ced1bdfe2/src/openai/lib/_pydantic.py#L57\n\n### To Reproduce\n\n1. run the code snippets\n2. check \"required\" in the printed json schema\n\n### Code snippets\n\n```Python\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nfrom openai import pydantic_function_tool\nimport json\n\nclass MyModel(BaseModel):\n    required_field: int = Field(description=\"This is a required field\")\n    optional_field: Optional[int] = Field(default=None, description=\"This is an optional field\")\n\n# use the model_json_schema method to get the schema\n# only \"required_field\" is listed:\n# \"required\": [\n#   \"required_field\"\n# ]\nprint(json.dumps(MyModel.model_json_schema(), indent=2))\n\n# use the pydantic_function_tool to get the schema\n# all the fields are listed:\n# \"required\": [\n#   \"required_field\",\n#   \"optional_field\"\n# ]\nprint(json.dumps(pydantic_function_tool(MyModel), indent=2))\n```\n\n### OS\n\nall\n\n### Python version\n\nall\n\n### Library version\n\nopenai>=1.40.0",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Yes this behaviour is intentional, all fields will be marked as `required` but fields that are optional will specify that they can be nullable (e.g. `nullable: true`). This is because the OpenAI API enforces this shape, see these docs for more information https://platform.openai.com/docs/guides/structured-outputs#supported-schemas."
      }
    ]
  },
  {
    "issue_number": 1814,
    "title": "TranslationVerbose duration shouldn't be string?",
    "author": "ricklamers",
    "state": "closed",
    "created_at": "2024-10-23T14:05:29Z",
    "updated_at": "2025-02-10T08:49:30Z",
    "labels": [
      "bug"
    ],
    "body": "### Confirm this is an issue with the Python library and not an underlying OpenAI API\n\n- [X] This is an issue with the Python library\n\n### Describe the bug\n\nAs per title\n\n### To Reproduce\n\n<img width=\"1078\" alt=\"Screenshot 2024-10-23 at 4 04 31 PM\" src=\"https://github.com/user-attachments/assets/c98ce629-36c6-4c0c-9936-e7cf78cf624c\">\r\n\n\n### Code snippets\n\n_No response_\n\n### OS\n\nmacOS\n\n### Python version\n\nPython 3.11\n\n### Library version\n\n1.52.1",
    "comments": [
      {
        "user": "Programmer-RD-AI",
        "body": "\nHi,  \n\nThank you for pointing this out! Based on the current implementation:  \n\n```python\nclass TranslationVerbose(BaseModel):  \n    duration: str  \n    \"\"\"The duration of the input audio.\"\"\"  \n```  \n\nIt seems the `duration` field is indeed expected as a string. This might be an oversight in either the documentation or the code itself—I'm not entirely sure yet.  \n\nTo address this, I’ve created #2034, which updates the `TranslationVerbose` model to support both string and integer types for the `duration` field. This change ensures flexibility and aligns better with expected usage.  \n\nPlease take a look at the PR and let me know your thoughts or any feedback you might have. Thanks!"
      },
      {
        "user": "fabrykowski",
        "body": "@Programmer-RD-AI looking at the example reply and going by the data the endpoint currently returns, the type should rather be `Union[str, float]`, because the returned duration *does* have decimals."
      },
      {
        "user": "RobertCraigie",
        "body": "This should be fixed in `v1.61.1`!"
      }
    ]
  },
  {
    "issue_number": 2101,
    "title": "See fully formatted prompt that is being sent to chat/completions",
    "author": "srsingh24",
    "state": "closed",
    "created_at": "2025-02-07T12:56:29Z",
    "updated_at": "2025-02-07T12:59:08Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nI use the OpenAI API Client for running my own hosted LLMs. However, I am unable to check the fully formatted prompt that is being sent to the LLM. For example, if I am using the mistral-instruct model as the following\n\n```python\nfrom openai import OpenAI\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"token-abc123\",\n)\n\ncompletion = client.chat.completions.create(\n  model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n```\n\nI would like to ensure the prompt template of Mistral ([INST] [/INST]) are being correctly applied to the prompt.  So far I have not been able to find a way to do that. If this is not already a feature, it would be extremely helpful to have it as an option to ensure appropriate prompt engineering. \n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "RobertCraigie",
        "body": "Thanks for the feature request but the SDK doesn't do any special formatting of the prompt, you'd have to check your server for any logs there. All we essentially do is `json.dumps()`."
      }
    ]
  },
  {
    "issue_number": 2093,
    "title": "BaseModel to jsonschema for Structured Outputs",
    "author": "federicoromeo",
    "state": "open",
    "created_at": "2025-02-05T14:00:45Z",
    "updated_at": "2025-02-06T14:45:45Z",
    "labels": [],
    "body": "### Confirm this is a feature request for the Python library and not the underlying OpenAI API.\n\n- [x] This is a feature request for the Python library\n\n### Describe the feature or improvement you're requesting\n\nDo you provide any helper function to generate the correct jsonschema starting from a pydantic BaseModel, to provide it to the response_format?\n\nIt's a big struggle not to have it. Imagine like in the Batch API in which i cannot provide the Pydantic instance\n\nI tried with `to_strict_json_schema` but it doesn't work as expected, it puts keys like $defs and $refs.\n\n\n### Additional context\n\n_No response_",
    "comments": [
      {
        "user": "federicoromeo",
        "body": "I found:\n`from openai.lib._parsing._completions import type_to_response_format_param`"
      }
    ]
  }
]