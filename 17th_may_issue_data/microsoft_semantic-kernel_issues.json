[
  {
    "issue_number": 12513,
    "title": "Python: ChatHistorySummarizationReducer.restore_chat_history() doesn't restore the correct service type",
    "author": "danielescaramuzzi",
    "state": "open",
    "created_at": "2025-06-17T13:38:31Z",
    "updated_at": "2025-06-17T13:39:10Z",
    "labels": [
      "bug",
      "python",
      "triage"
    ],
    "body": "**Describe the bug**\nWhen restoring a serialized chat history using ChatHistorySummarizationReducer.restore_chat_history(), service type is not set correctly.\n\n**To Reproduce**\n```python\nimport asyncio\n\nfrom semantic_kernel import Kernel\nfrom semantic_kernel.contents import ChatHistorySummarizationReducer\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n\nkernel = Kernel()\nservice_id = \"my-service\"\nkernel.add_service(AzureChatCompletion(\n            service_id=service_id,\n            api_key=\"***\",\n            api_version=\"2024-10-21\",\n            endpoint=\"***\",\n            deployment_name=\"gpt-4o-mini\"\n        ))\n\nchat_history = ChatHistorySummarizationReducer(\n            service=kernel.get_service(service_id, AzureChatCompletion),\n            target_count=2,\n            threshold_count=1,\n            include_function_content_in_summary=False,\n        )\n\nprint(f\"Service type: {type(chat_history.service)}\")\n\nchat_history.add_assistant_message(\"Hey there! How can I assist you today?\")\nchat_history.add_user_message(\"What is the weather like today?\")\nchat_history.add_assistant_message(\"The weather is sunny with a high of 25 degrees Celsius.\") \nchat_history.add_user_message(\"How are you?\")\nchat_history.add_assistant_message(\"I'm fine thanks for asking! How can I help you further?\") \n\nserialized_history = chat_history.serialize()\nimport json\njson_serialized_history = json.loads(serialized_history)\nprint(f\"Serialized service section: {json_serialized_history['service']}\")\n\n# restore from serialized history\nrestored_chat_history = ChatHistorySummarizationReducer.restore_chat_history(serialized_history)\nprint(f\"Restored service type: {type(restored_chat_history.service)}\")\n\nasync def main() -> None:\n  reduced = await restored_chat_history.reduce()\n  print(f\"Reduced: {reduced}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\nproduces the following output:\n```\nService type: <class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'>\n\nSerialized service section: {'ai_model_id': 'gpt-4o-mini', 'service_id': 'my-service', 'instruction_role': 'system'}\n\nRestored service type: <class 'semantic_kernel.connectors.ai.chat_completion_client_base.ChatCompletionClientBase'>\n\n\nSummarization failed, continuing without summary.\nTraceback (most recent call last):\n  File \"***/.venv/lib/python3.12/site-packages/semantic_kernel/contents/history_reducer/chat_history_summarization_reducer.py\", line 127, in reduce\n    summary_msg = await self._summarize(messages_to_summarize)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"***/.venv/lib/python3.12/site-packages/semantic_kernel/contents/history_reducer/chat_history_summarization_reducer.py\", line 166, in _summarize\n    return await self.service.get_chat_message_content(chat_history=chat_history, settings=execution_settings)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"***/.venv/lib/python3.12/site-packages/semantic_kernel/connectors/ai/chat_completion_client_base.py\", line 190, in get_chat_message_content\n    results = await self.get_chat_message_contents(chat_history=chat_history, settings=settings, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"***/.venv/lib/python3.12/site-packages/semantic_kernel/connectors/ai/chat_completion_client_base.py\", line 113, in get_chat_message_contents\n    return await self._inner_get_chat_message_contents(chat_history, settings)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"***/.venv/lib/python3.12/site-packages/semantic_kernel/connectors/ai/chat_completion_client_base.py\", line 58, in _inner_get_chat_message_contents\n    raise NotImplementedError(\"The _inner_get_chat_message_contents method is not implemented.\")\nNotImplementedError: The _inner_get_chat_message_contents method is not implemented.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"***/./bug_restore_history.py\", line 46, in <module>\n    asyncio.run(main())\n  File \"***/.pyenv/versions/3.12.3/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"***/.pyenv/versions/3.12.3/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"***/.pyenv/versions/3.12.3/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"***/./bug_restore_history.py\", line 42, in main\n    reduced = await restored_chat_history.reduce()\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"***/.venv/lib/python3.12/site-packages/semantic_kernel/contents/history_reducer/chat_history_summarization_reducer.py\", line 149, in reduce\n    raise ChatHistoryReducerException(\"Chat History Summarization failed.\") from ex\nsemantic_kernel.exceptions.content_exceptions.ChatHistoryReducerException: Chat History Summarization failed.\n```\n\n**Expected behavior**\nWhen serializing and deserializing chat history, service type should not change.\n\n\n**Platform**\n - Language: Python\n - Version: semantic-kernel==1.33.0\n",
    "comments": []
  },
  {
    "issue_number": 12512,
    "title": "Bug: Missing event data OnInputEvent with external",
    "author": "hookenful",
    "state": "open",
    "created_at": "2025-06-17T13:34:41Z",
    "updated_at": "2025-06-17T13:34:51Z",
    "labels": [
      "bug",
      "triage"
    ],
    "body": "Hello,\n\nThere is missing `EventData `when calling like that:\n\n```\nawait process.StartAsync(kernel, new KernelProcessEvent\n                {\n                    Id = eventId,\n                    Data = feedback\n                }, messageChannel);\n```\n\n```\nprocessBuilder.OnInputEvent(Events.Events.UserApprovedProcess)\n                      .EmitExternalEvent(proxyStep, \"PublishProcess\")\n```\n\nThen in `KernelProcessProxyMessage` `EventData ` is null",
    "comments": []
  },
  {
    "issue_number": 12511,
    "title": ".Net: Bug: .Net OpenAIResponseAgent 'Unsupported response item: OpenAI.Responses.ReasoningResponseItem'",
    "author": "dglover1",
    "state": "open",
    "created_at": "2025-06-17T12:04:03Z",
    "updated_at": "2025-06-17T12:51:02Z",
    "labels": [
      "bug",
      ".NET",
      "triage"
    ],
    "body": "**Describe the bug**\nThe preview `OpenAIResponseAgent` throws a `System.NotImplementedException`: `'Unsupported response item: OpenAI.Responses.ReasoningResponseItem'` for reasoning responses (from o4-mini etc.). It is uncatchable since `OpenAIResponseExtensions` and the related classes are internal.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Set up an `OpenAIResponseAgent` using an OpenAI reasoning model e.g. o4-mini\n2. Invoke the agent (in my case with `InvokeStreamingAsync()`)\n3. Wait for the first reasoning response to be received\n4. Error is thrown by the `ToChatMessageContentItemCollection()` extension method in `OpenAIResponseExtensions`.\n\n**Expected behavior**\n`ToChatMessageContentItemCollection()` should handle the `ReasoningResponseItem` and return a `ChatMessageContentItemCollection` containing `ReasoningContent` objects. (if this is not supported yet, it could at least accept the `ReasoningResponseItem` and do nothing/just log it - throwing an error renders the OpenAIResponseAgent unusable with reasoning models...) e.g. the following added at line 95 of OpenAIResponseExtensions.cs:\n\n```\nelse if (item is ReasoningResponseItem reasoningResponseItem)\n{\n    var collection = new ChatMessageContentItemCollection();\n    \n    foreach (string part in reasoningResponseItem.SummaryTextParts)\n    {\n        collection.Add(new ReasoningContent(part));\n    }\n    return collection;\n}\n```\n\n\n**Platform**\n - Language: C#\n - Source: NuGet package (Microsoft.SemanticKernel.Agents.OpenAI) version 1.57.0-preview\n - AI model: OpenAI:o4-mini\n - IDE: VS Code\n - OS: Mac",
    "comments": []
  },
  {
    "issue_number": 12507,
    "title": ".Net: [MEVD] Minor test cleanup tasks",
    "author": "roji",
    "state": "open",
    "created_at": "2025-06-17T11:46:26Z",
    "updated_at": "2025-06-17T12:40:20Z",
    "labels": [
      ".NET",
      "triage",
      "msft.ext.vectordata"
    ],
    "body": "* Go over and standardize naming (sometimes we same conformance, sometimes specification, sometimes something else)\n* Review directory organization etc.\n* Stop using GUIDs as collection names by default\n    * If a test crashes in the middle, the GUID-named collection stays forever and needs to be cleaned up.\n    * Possibly have an opt-in feature where a GUID is appended to the name, for cases where multiple devs (or CI runs) are targeting the same cloud service.",
    "comments": [
      {
        "user": "lizzydavis695",
        "body": "Thank you. But can you help me trace a email?\r\n\r\n\r\nSent from Yahoo Mail for iPhone\r\n\r\n\r\nOn Tuesday, June 17, 2025, 7:47 AM, Shay Rojansky ***@***.***> wrote:\r\n\r\nroji created an issue (microsoft/semantic-kernel#12507)   \r\n   - Go over and standardize naming (sometimes we same conformance, sometimes specification, sometimes something else)\r\n   - Review directory organization etc.\r\n   - Stop using GUIDs as collection names by default      \r\n      - If a test crashes in the middle, the GUID-named collection stays forever and needs to be cleaned up.\r\n      - Possibly have an opt-in feature where a GUID is appended to the name, for cases where multiple devs (or CI runs) are targeting the same cloud service.\r\n\r\n\r\n—\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you are subscribed to this thread.Message ID: ***@***.***>\r\n \r\n\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 10100,
    "title": ".Net: Memory for Agents",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-01-07T11:29:12Z",
    "updated_at": "2025-06-17T12:32:30Z",
    "labels": [
      ".NET",
      "memory connector",
      "sk team issue",
      "memory",
      "Build",
      "SK-H2-Planning"
    ],
    "body": "## Tasks\n\n- [ ] Scope agreed with PM\n- [ ] ADR completed and decision agreed\n- [ ] ...\n\n## Description\nPre-built memory types and custom memory bank configuration\n\n## Memory bank types (examples):\n\n- Short-term memory\n- Whiteboard (shared working memory)\n- Domain specific memory banks\n- Episodic Memory\n- Procedural Memory (what skills do I have?) \n- Etc…\n- \n\n## Customizations: \n\n- Partitioning, access controls, priority and capacity allocation\n- Connect with enterprise data sources and tools registry\n",
    "comments": []
  },
  {
    "issue_number": 11198,
    "title": ".Net: Neon Serverless Postgres Vector Store - Add to list of supported databases",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-03-25T16:23:36Z",
    "updated_at": "2025-06-17T12:32:21Z",
    "labels": [
      ".NET",
      "documentation",
      "msft.ext.vectordata"
    ],
    "body": "\n### Discussed in https://github.com/microsoft/semantic-kernel/discussions/11052\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **Boburmirzo** March 18, 2025</sup>\nWe would like to request support for [Neon](https://neon.tech/) Vector Store as a connector in Semantic Kernel. Neon is a fully managed Postgres compatible solution that offers vector search capabilities, making it a great choice for AI applications with branching and multiple extensions.\r\n\r\nWe have tested the existing [Postgres Vector Store connector](https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/out-of-the-box-connectors/postgres-connector?pivots=programming-language-csharp) with Neon and confirmed its compatibility. You can find Neon’s documentation on using Postgres Vector Store connector here:https://neon.tech/docs/ai/semantic-kernel\r\n\r\nDemo project in .NET how it works: https://github.com/neondatabase-labs/neon-semantic-kernel-examples/tree/main/dotnet\r\n\r\nProposed Approaches:\r\n\r\n1. Mention Neon on a separate MS Docs page and highlight that's the same as Postgres for any programming languages.\r\n2. Alternatively, we are happy to contribute to the repo and help with the new integration of the Neon Vector Store connector in Semantic Kernel. \r\n\r\nWould the Semantic Kernel team be open to adding this support? We’d love to collaborate on this feature! 🚀\r\n\r\nLooking forward to your thoughts.</div>",
    "comments": []
  },
  {
    "issue_number": 10781,
    "title": "Fix Semantic Kernel Dashboard",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-03-04T11:25:22Z",
    "updated_at": "2025-06-17T12:32:12Z",
    "labels": [
      "sk team issue",
      "stale"
    ],
    "body": null,
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10149,
    "title": ".Net: Az Template Support",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-01-10T09:14:37Z",
    "updated_at": "2025-06-17T12:31:59Z",
    "labels": [
      ".NET",
      "sk team issue",
      "Build",
      "SK-H2-Planning"
    ],
    "body": "## Investigation\n\n- [ ] Create a POC for the following RAG scenario\n    - [ ] Create a template that contains the following\n        - Application with a single endpoint that will perform RAG queries\n        - All resources required to execute RAG queries e.g. LLM, Vector Database, ...\n        - Support for configuring any supported SK AI or Vector Database service\n    - [ ] Deploy the template and a REST API endpoint is made available which can be used to perform RAG queries\n\n## Description\n\n\"Azure Up Templates\" into the Semantic Kernel repo to streamline the deployment process of the Kernel and its dependent services into Azure infrastructure. These templates are included in azd and the 'ai command line'\n\n### Requirements:\n\n1. **Azure Resource Manager (ARM) Templates Creation:**\n    - Develop ARM and/or bison templates for automatic deployment of the Semantic Kernel along with required Azure resources (e.g., VMs, Storage accounts, App Services, Databases, etc.).\n    - Ensure templates support customization parameters (e.g., region, VM size, database scaling).\n1. **Template Documentation:**\n    - Provide comprehensive documentation for each template, explaining the parameters, default values, and instructions for customization.\n    - Include examples of different deployment scenarios using the templates\n1. **Monitoring and Logging:**\n    - Integrate with Azure Monitor or other monitoring solutions to provide real-time insights.\n1. **Security and Compliance:**\n    - Ensure templates follow best security practices (e.g., secure parameters, least privilege principle).\n \n### Investigate:\n\n1. Can templates be generated, and it is useful at runtime? E.g. Kernel.GenerateBisonTemplate()",
    "comments": []
  },
  {
    "issue_number": 10098,
    "title": ".Net: Hybrid Compute - Models",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-01-07T11:25:04Z",
    "updated_at": "2025-06-17T12:31:50Z",
    "labels": [
      ".NET",
      "ai connector",
      "sk team issue",
      "Build",
      "SK-H2-Planning"
    ],
    "body": "Implement a hybrid model orchestration within Semantic Kernel to leverage both local and cloud models. The system should default to local models for inference where available and seamlessly fall back to cloud models. Additionally, it should support local memory storage and retrieval, using cloud-based solutions as a fallback or for additional backup. This hybrid strategy should be abstracted within the Semantic Kernel, enabling developers to specify preferences and priorities without managing the underlying complexities. This should build on top of the capabilities we already have.\n\n## Scenarios\n\n- As a developer, I want my Semantic Kernel application to utilize local models for inference to achieve low-latency responses while falling back to cloud models when local models are unavailable or insufficient.\n\n## Requirements\n\n### Model Orchestration Layer:\n\n- Create a model orchestration layer within the Semantic Kernel capable of routing requests to either local or cloud models based on availability and priority settings.\n- Develop a configuration file where users can specify local and cloud model endpoints and prioritize their usage.\n- Inference Abstraction:\n- Abstract model inference calls such that the application can make a single call, and the underlying architecture decides whether to use local or cloud resources.\n- Support dynamic switching between local and cloud models based on real-time performance monitoring (e.g., latency, throughput).\n",
    "comments": [
      {
        "user": "SergeyMenshykh",
        "body": "ADR PR: https://github.com/microsoft/semantic-kernel/pull/10439\nADR: https://github.com/microsoft/semantic-kernel/blob/main/docs/decisions/0064-hybrid-model-orchestration.md\n\nSamples:\n- [HybridCompletion_Fallback](https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/Concepts/ChatCompletion/HybridCompletion_Fallback.cs)"
      }
    ]
  },
  {
    "issue_number": 10069,
    "title": "VectorStore: Provide connector for the FAISS vector database",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-01-06T13:03:45Z",
    "updated_at": "2025-06-17T12:31:43Z",
    "labels": [
      ".NET",
      "memory connector",
      "sk team issue",
      "memory",
      "Build"
    ],
    "body": "- This will be Python only for now\n\n- [x] Implementation done\n- [ ] docs done\n- [ ] blog post ",
    "comments": []
  },
  {
    "issue_number": 9750,
    "title": ".Net Function Calling with Bedrock Claude",
    "author": "MalteFries",
    "state": "closed",
    "created_at": "2024-11-19T06:56:25Z",
    "updated_at": "2025-06-17T12:31:35Z",
    "labels": [
      "question",
      ".NET",
      "blocked external"
    ],
    "body": "Hey there, am I missing something here or does function calling for Claude 3.5 Sonnet via the Bedrock connector simply not work?\nI know it's still in experimental, but is there a plan to support it?",
    "comments": [
      {
        "user": "Douuuglas",
        "body": "Hey @MalteFries, unfortunately it's not working at the moment. \n\nIf you look at the code below it's expecting us to pass \"tools\" and \"tool_choice\" objects in the ExtensionData.\n\nhttps://github.com/microsoft/semantic-kernel/blame/7a61c9caff11f73bed93687144de3afda85fd39d/dotnet/src/Connectors/Connectors.Amazon/Bedrock/Core/Models/Anthropic/AnthropicService.cs#L71C9-L82C10\n\nBut ClaudeToolUse.ClaudeTool is internal and it's not available outside the SK lib, so we can't use it.\n\n@markwallace-microsoft, is anyone working on it? If I have some guidance I could try creating a PR. \n\nMy idea would be doing something similar as what happens on the OpenAI Connector, removing the need to check the ExtesionData.\n\nhttps://github.com/microsoft/semantic-kernel/blob/7a61c9caff11f73bed93687144de3afda85fd39d/dotnet/src/Connectors/Connectors.OpenAI/Core/ClientCore.ChatCompletion.cs#L150"
      },
      {
        "user": "halllo",
        "body": "I also need function calling with bedrock (claude).\n\nIn my [experiments](https://github.com/halllo/DotnetAgentExperiments) I get an the following exception as soon as I add `PromptExecutionSettings` with `FunctionChoiceBehavior.Auto()`:\n\n```\nMicrosoft.SemanticKernel.Connectors.Amazon.Core.BedrockChatCompletionClient[0]\n      Can't converse with 'anthropic.claude-3-sonnet-20240229-v1:0'. Reason: 1 validation error detected: Value '0' at 'inferenceConfig.maxTokens' failed to satisfy constraint: Member must have value greater than or equal to 1\n      Amazon.BedrockRuntime.Model.ValidationException: 1 validation error detected: Value '0' at 'inferenceConfig.maxTokens' failed to satisfy constraint: Member must have value greater than or equal to 1\n```\n\nIf it is really just a matter of an inaccessible property, why not get it using reflection? The connector is experimental after all."
      },
      {
        "user": "Douuuglas",
        "body": "> I also need function calling with bedrock (claude).\n> \n> In my [experiments](https://github.com/halllo/DotnetAgentExperiments) I get an the following exception as soon as I add `PromptExecutionSettings` with `FunctionChoiceBehavior.Auto()`:\n> \n> ```\n> Microsoft.SemanticKernel.Connectors.Amazon.Core.BedrockChatCompletionClient[0]\n>       Can't converse with 'anthropic.claude-3-sonnet-20240229-v1:0'. Reason: 1 validation error detected: Value '0' at 'inferenceConfig.maxTokens' failed to satisfy constraint: Member must have value greater than or equal to 1\n>       Amazon.BedrockRuntime.Model.ValidationException: 1 validation error detected: Value '0' at 'inferenceConfig.maxTokens' failed to satisfy constraint: Member must have value greater than or equal to 1\n> ```\n> \n> If it is really just a matter of an inaccessible property, why not get it using reflection? The connector is experimental after all.\n\nHey @halllo,\n\nTo make it work I had to set the \"max_tokens_to_sample\" property manually as you can see below:\n\n```\nPromptExecutionSettings promptExecutionSettings = new()\n{\n    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto(),\n    ModelId = modelid,\n    ExtensionData = new Dictionary<string, object>() {\n        { \"max_tokens_to_sample\", 4096 }\n    },\n};\n```\n\nAnd then I pass the prompt execution settings while calling the chat completion service:\n\n```\nvar result = await chatcompletionservice.GetChatMessageContentAsync(\n    history,\n    executionSettings: promptExecutionSettings,\n    kernel: kernel);\n```\n\nSee if that works for you!"
      }
    ]
  },
  {
    "issue_number": 10530,
    "title": ".Net: New Feature: .Net - OpenRouter Connector",
    "author": "tntwist",
    "state": "closed",
    "created_at": "2025-02-13T17:46:38Z",
    "updated_at": "2025-06-17T12:31:24Z",
    "labels": [
      ".NET",
      "stale"
    ],
    "body": "---\nname: OpenRouter connector\nabout: Add a connector to open router\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\nHi there,\nit would be nice to have a connector for [OpenRouter](https://openrouter.ai/).\nOpenRouter makes it really easy to use multiple ai providers with a single unified api.\nFrom their docs their should be compatible with the OpenAI SDK (see here: https://openrouter.ai/docs/quickstart)\n\nI also played around using the exiting Connector for OpenAi and changed the Endpoint to \"https://openrouter.ai/api/v1\" like this:\n```\n  kernelBuilder.AddOpenAIChatCompletion(\n      modelId: model.ProviderIdentifier,\n      apiKey: settings.OpenRouterSettings.ApiKey!,\n      endpoint: new(\"https://openrouter.ai/api/v1\")\n  );\n```\n\nChat completion does work with this when the response of OpenRouter is successfull.\nIf there is an error providerwise that is not perfectly handled since open router returns a 200 status code with some error message like this:\n```\n{\n    \"error\": {\n        \"message\": \"Provider returned error\",\n        \"code\": 400,\n        \"metadata\": {\n            \"raw\": \"{\\n  \\\"error\\\": {\\n    \\\"code\\\": 400,\\n    \\\"message\\\": \\\"Invalid JSON payload received. Unknown name \\\\\\\"additionalProperties\\\\\\\" at 'generation_config.response_schema.properties[0].value.items': Cannot find field.\\\",\\n    \\\"status\\\": \\\"INVALID_ARGUMENT\\\",\\n    \\\"details\\\": [\\n      {\\n        \\\"@type\\\": \\\"type.googleapis.com/google.rpc.BadRequest\\\",\\n        \\\"fieldViolations\\\": [\\n          {\\n            \\\"field\\\": \\\"generation_config.response_schema.properties[0].value.items\\\",\\n            \\\"description\\\": \\\"Invalid JSON payload received. Unknown name \\\\\\\"additionalProperties\\\\\\\" at 'generation_config.response_schema.properties[0].value.items': Cannot find field.\\\"\\n          }\\n        ]\\n      }\\n    ]\\n  }\\n}\\n\",\n            \"provider_name\": \"Google AI Studio\"\n        }\n    },\n    \"user_id\": \"*******\"\n}\n```\nUsing the OpenAi connector we get a null pointer exception since there is no real response.\n\nSo best would be to have a real OpenRouter connector that handles this correctly rather than using the OpenAI connector.",
    "comments": [
      {
        "user": "samimejri",
        "body": "> ```\n>   kernelBuilder.AddOpenAIChatCompletion(\n>       modelId: model.ProviderIdentifier,\n>       apiKey: settings.OpenRouterSettings.ApiKey!,\n>       endpoint: new(\"https://openrouter.ai/api/v1\")\n>   );\n> ```\n\nThis does not work for me. I get \"401 Unauthorized\". My API key doesn't seem to be taken into consideration.\n\n![Image](https://github.com/user-attachments/assets/cff928a5-1b90-4a5f-bca7-6919e7229974)\n\n**Edit:**\nNever mind, I made it work like this:\n```\nOpenAIClient client = new OpenAIClient(new(apiKey), new() { Endpoint = new Uri(endpoint)});\n\nvar kernel = Kernel.CreateBuilder()\n    .AddOpenAIChatCompletion(\n      model,\n      client)\n    .Build();\n```\n\nSorry for polluting this discussion!"
      },
      {
        "user": "Lanayx",
        "body": "Dedicated openrouter support won't be required if SemanticKernel could allow passing arbitrary parameters to the request (see https://openrouter.ai/docs/api-reference/parameters). As far as I understand currently it's not supported"
      },
      {
        "user": "tntwist",
        "body": "We still need a dedicated connector since provider errors are returned as 200ers from openrouter with a custom error json object. the open ai connector cannot handle this. 😅\n\nthe open ai connector will simply fail returning the response with a null reference exception."
      }
    ]
  },
  {
    "issue_number": 10706,
    "title": ".Net: Agent declarative file format support",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-02-27T12:28:54Z",
    "updated_at": "2025-06-17T12:31:16Z",
    "labels": [
      ".NET",
      "sk team issue",
      "agents",
      "stale",
      "Build",
      "SK-H2-Planning"
    ],
    "body": null,
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10223,
    "title": ".Net: IChatClient + Add FunctionChoiceBehavior to Google Connector",
    "author": "PraveenVerma17",
    "state": "closed",
    "created_at": "2025-01-17T22:03:16Z",
    "updated_at": "2025-06-17T12:31:09Z",
    "labels": [
      "bug",
      ".NET",
      "blocked external"
    ],
    "body": "**Describe the bug**\nkernel function call is not working properly with gemini-1.5-flash  for below code \n\n```\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.ChatCompletion;\nusing Microsoft.SemanticKernel.Connectors.Google;\nusing SementicKernelDemo.Plugins.LightsPlugin;\n\n\n\nstring? modelID = Environment.GetEnvironmentVariable(\"MODEL_ID\");\nstring? apiKey = Environment.GetEnvironmentVariable(\"API_KEY\");\n\nif (string.IsNullOrEmpty(modelID) || string.IsNullOrEmpty(apiKey))\n    throw new Exception(\"Please set the MODEL_ID and API_KEY environment variables to your Google AI Gemini model ID and API key, respectively.\");\n\n// create kernel\n#pragma warning disable SKEXP0070 \nvar kernel = Kernel.CreateBuilder()\n                    .AddGoogleAIGeminiChatCompletion(modelID, apiKey, GoogleAIVersion.V1)\n                    .Build();\n\n\n\n// Add plugins\nkernel.Plugins.AddFromType<LightsPlugin>(\"Lights\");\n\n\n//Enable Prompt Execution Settings \nGeminiPromptExecutionSettings geminiPromptExecutionSettings = new() {\n    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto(),\n    ToolCallBehavior = GeminiToolCallBehavior.EnableKernelFunctions\n};\n\n\n\nvar chatCompletionService = kernel.GetRequiredService<IChatCompletionService>();\nvar history = new ChatHistory();\n\nstring? userInput;\ndo\n{\n    Console.WriteLine(\"User >\");\n    userInput = Console.ReadLine();\n    try\n    {\n        history.AddUserMessage(userInput);\n        var result = await chatCompletionService.GetChatMessageContentAsync(\n            history,\n            executionSettings: geminiPromptExecutionSettings,\n            kernel: kernel);\n\n        Console.WriteLine(\"Assistant > \" + result);\n        history.AddMessage(result.Role, result.Content ?? string.Empty);\n    }\n    catch (Exception ex)\n    {\n        Console.WriteLine(\"Error: \" + ex.Message);\n    }\n} while (userInput is not null);\n\n\n#pragma warning restore SKEXP0070\n```\n\n\n**To Reproduce**\nSteps to reproduce the behavior:\nExecute the above code its failing when ToolCallBehavior = GeminiToolCallBehavior.EnableKernelFunctions or ToolCallBehavior = GeminiToolCallBehavior.AutoInvokeKernelFunctions is set. if i don't set it does not recognize kernel functions.  \n\n**Expected behavior**\nIt should recognize kernel functions and return the result back from my plugin\n\n**Screenshots**\n\n\n**Platform**\n - OS: Windows\n - IDE: Visual Studio\n - Language: C#\n - Source: microsoft.sementic.kernel and connector.google\n\n**Additional context**\n",
    "comments": [
      {
        "user": "thevivekm",
        "body": "I am also facing the same issue with gemini-1.5-flash, it doesn't interact with the kernel function."
      },
      {
        "user": "thevivekm",
        "body": "change **GeminiPromptExecutionSettings** to this\n\n```\nGeminiPromptExecutionSettings geminiAIPromptExecutionSettings = new()\n{\n    ToolCallBehavior = GeminiToolCallBehavior.AutoInvokeKernelFunctions\n};\n```\n\n"
      },
      {
        "user": "PraveenVerma17",
        "body": "@thevivekm I changed to  gemini-2.0-flash-exp model and then adding message to history if content is available. Otherwise next time onward LLM is giving 400 bad request. \n\n`\nConsole.WriteLine(\"Assistant > \" + result);\n\nif (!string.IsNullOrEmpty(result?.Content))\n    history.AddMessage(result.Role, result.Content ?? string.Empty);\n`\n\nNow i am facing a different problem, prompts are calling native function but some how LLM is not recognizing them.  I don't want to create separate native function. I was hoping when LLM executed native function it will understand the output.\n\n\nhere is my skprompt.txt and config.json\n\nThis is a list of songs **{{Songs.get_list_of_songs}**} and add them into chat history if not already available.\nThis is a recently played songs **{{User.user_recently_played_songs}}** and add them into chat history if not already available.\n\nYou are an AI assistant. The assistant is helpful, creative, clever, and very friendly.\nSuggest a new song to user based on list of songs but not recently played.\nwhile responding to user, you can use the following format:\n\"Hey, I have a song suggestion for you. How about listening to {title} by {artist}?\"\nIf Suggested song must be from the available songs list. If no available list of song present, Please inform client.\nIf all songs are in recently played list then select random song from available song list.\n\n\n{\n  \"schema\": 1,\n  \"type\": \"completion\",\n  \"description\": \"Suggest a new song\",\n  \"execution_settings\": {\n    \"default\": {\n      \"max_tokens\": 200,\n      \"temperature\": 0.7\n    }\n  }\n}\n\nI am new in Microsoft Semantic kernel and learning concepts. \n\n"
      }
    ]
  },
  {
    "issue_number": 11317,
    "title": ".Net: Add more conformance tests for Agents response consistency",
    "author": "westey-m",
    "state": "closed",
    "created_at": "2025-04-01T17:16:37Z",
    "updated_at": "2025-06-17T12:31:00Z",
    "labels": [
      ".NET",
      "sk team issue",
      "agents"
    ],
    "body": "Add more conformance tests for Agents response consistency",
    "comments": []
  },
  {
    "issue_number": 12320,
    "title": ".Net: Plugin name collision resolution mechanism",
    "author": "SergeyMenshykh",
    "state": "closed",
    "created_at": "2025-05-30T10:06:49Z",
    "updated_at": "2025-06-17T12:30:53Z",
    "labels": [
      ".NET",
      "agents"
    ],
    "body": "**Context:** Today, SK agents add functions returned by AI context providers to a new \"Tools\" plugin and add that plugin to the kernel plugins. It works fine if kernel plugins don't have a registered plugin with the same name. However, it fails if the \"Tools\" plugin is already present in the kernel plugins.    \n\n**To Do:** Decide on a way to handle cases like that and apply necessary fixes if needed.",
    "comments": []
  },
  {
    "issue_number": 6993,
    "title": ".Net: Bug: Not able to get AzureOpenAIChat to work on Android using MAUI",
    "author": "cwbrandsdal",
    "state": "closed",
    "created_at": "2024-06-27T19:48:59Z",
    "updated_at": "2025-06-17T12:30:40Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\r\nI have been trying to get the semantic kernel azure openai to work in a .net MAUI project running on android without luck. The code runs just fine when running the windows app, but it always fails on Android both in emulator and on device.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nI have set up a boilerplate .net 8 MAUI project using xaml. The only thing I have done is added the semantic kernel NuGet and pasted the following code in the MainPage.xaml.cs:\r\n\r\n```\r\npublic partial class MainPage : ContentPage\r\n{\r\n    public MainPage()\r\n    {\r\n        InitializeComponent();\r\n    }\r\n\r\n    private async Task<string> SendStringToAI()\r\n    {\r\n        string openAIKey = \"REDACTED\";\r\n        string openAIEndpoint = \"https://REDACTED.openai.azure.com/\";\r\n        string engine = \"gpt-4o\";\r\n\r\n        try\r\n        {\r\n            AzureOpenAIChatCompletionService chatCompletionService = new (engine,openAIEndpoint, openAIKey);\r\n\r\n            var res = await chatCompletionService.GetChatMessageContentsAsync(\"This is just a test.\");\r\n\r\n            return res.ToString();\r\n\r\n        }\r\n        catch (Exception ex)\r\n        {\r\n            string err = ex.Message;\r\n        }\r\n\r\n        return \"No response\";\r\n    }\r\n\r\n    private async void OnSendClicked(object sender, EventArgs e)\r\n    {\r\n        var response = await SendStringToAI();\r\n        ResponseLabel.Text = response;\r\n    }\r\n}\r\n```\r\n\r\nThis works fine on Windows, but on android I always get the following error on the call to GetChatMessageContentsAsync:\r\n\r\n```\r\n{Microsoft.SemanticKernel.HttpOperationException: Invalid value for 'content': expected a string, got null.\r\nStatus: 400 (model_error)\r\n\r\nContent:\r\n{\r\n  \"error\": {\r\n    \"message\": \"Invalid value for 'content': expected a string, got null.\",\r\n    \"type\": \"invalid_request_error\",\r\n    \"param\": \"messages.[0].content\",\r\n    \"code\": null\r\n  }\r\n}\r\n\r\nHeaders:\r\nx-ms-region: REDACTED\r\napim-request-id: REDACTED\r\nx-ratelimit-remaining-requests: REDACTED\r\nx-ms-rai-invoked: REDACTED\r\nX-Request-ID: REDACTED\r\nms-azureml-model-error-reason: REDACTED\r\nms-azureml-model-error-statuscode: REDACTED\r\nStrict-Transport-Security: REDACTED\r\nazureml-model-session: REDACTED\r\nX-Content-Type-Options: REDACTED\r\nx-envoy-upstream-service-time: REDACTED\r\nx-ms-client-request-id: 4359a357-f2f7-469f-911d-7e7cc3d95efc\r\nx-ratelimit-remaining-tokens: REDACTED\r\nDate: Thu, 27 Jun 2024 19:44:02 GMT\r\nContent-Length: 187\r\nContent-Type: application/json\r\n\r\n ---> Azure.RequestFailedException: Invalid value for 'content': expected a string, got null.\r\nStatus: 400 (model_error)\r\n\r\n   at Azure.Core.HttpPipelineExtensions.ProcessMessageAsync(HttpPipeline pipeline, HttpMessage message, RequestContext requestContext, CancellationToken cancellationToken)\r\n   at Azure.AI.OpenAI.OpenAIClient.GetChatCompletionsAsync(ChatCompletionsOptions chatCompletionsOptions, CancellationToken cancellationToken)\r\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.<RunRequestAsync>d__55`1[[Azure.Response`1[[Azure.AI.OpenAI.ChatCompletions, Azure.AI.OpenAI, Version=1.0.0.0, Culture=neutral, PublicKeyToken=92742159e12e44c8]], Azure.Core, Version=1.39.0.0, Culture=neutral, PublicKeyToken=92742159e12e44c8]].MoveNext()\r\n   --- End of inner exception stack trace ---\r\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.<RunRequestAsync>d__55`1[[Azure.Response`1[[Azure.AI.OpenAI.ChatCompletions, Azure.AI.OpenAI, Version=1.0.0.0, Culture=neutral, PublicKeyToken=92742159e12e44c8]], Azure.Core, Version=1.39.0.0, Culture=neutral, PublicKeyToken=92742159e12e44c8]].MoveNext()\r\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetChatMessageContentsAsync(ChatHistory chat, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)\r\n   at Test.MainPage.SendStringToAI() in D:\\Temp\\Test\\MainPage.xaml.cs:line 29}\r\n\r\n{\r\n  \"error\": {\r\n    \"message\": \"Invalid value for 'content': expected a string, got null.\",\r\n    \"type\": \"invalid_request_error\",\r\n    \"param\": \"messages.[0].content\",\r\n    \"code\": null\r\n  }\r\n}\r\n```\r\n\r\nI have tried to set it up with DI and Kernel, but exactly the same result. Works on Windows, not on Android.\r\n\r\n**Expected behavior**\r\nI expect it to just give a simple return string.\r\n\r\n**Platform**\r\n - OS: Windows\r\n - IDE: Visual Studio\r\n - Language: C#\r\n - Source: NuGet package Microsoft.SemanticKernel 1.15.0",
    "comments": [
      {
        "user": "siddharthsingh89",
        "body": "facing the same issue. It seems like it is not able to send the payload in the request to openai on android and iOS. Any workarounds?"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 14 days since being marked as stale."
      }
    ]
  },
  {
    "issue_number": 11381,
    "title": ".Net: [ITextSearch] Consider making top a mandatory parameter",
    "author": "roji",
    "state": "closed",
    "created_at": "2025-04-04T14:43:44Z",
    "updated_at": "2025-06-17T12:30:31Z",
    "labels": [
      ".NET",
      "Build"
    ],
    "body": "MEVD made Top a mandatory parameter (#10193), given that there's no reasonable default that would make sense, and it's generally better to have users consider how many results they actually need here. The same logic probably applies to ITextSearch, so it may be a good idea to do the same there.",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "Also need to align on\n\n- [ ] Return values\n- [ ] Filtering"
      }
    ]
  },
  {
    "issue_number": 10782,
    "title": ".Net: Create MCP Sample",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-03-04T11:25:46Z",
    "updated_at": "2025-06-17T12:30:25Z",
    "labels": [
      ".NET",
      "samples",
      "stale",
      "needs_port_to_python"
    ],
    "body": "- [x] Create sample showing how to use MCP Tools for function calling with Semantic Kernel\n- [x] Create Blog Post to describe the sample",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 9892,
    "title": ".Net: Python: API for listing/filtering records without similarity search",
    "author": "tlecomte",
    "state": "closed",
    "created_at": "2024-12-05T18:52:12Z",
    "updated_at": "2025-06-17T12:30:18Z",
    "labels": [
      ".NET",
      "python",
      "memory connector",
      "needs discussion",
      "memory",
      "Build",
      "SK-H2-Planning"
    ],
    "body": "**Describe the bug**\n`IVectorStoreRecordCollection` does not have a method to list all the keys stored in the collection. So we are missing a way to compare the content of the collection with the source to find what keys are to be deleted (when they are no longer in the source data).\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to https://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/Connectors/VectorData.Abstractions/VectorStorage/IVectorStoreRecordCollection.cs\n2. Observe that there are methods to get by key, delete by key, upsert by key, but no method to list all keys\n\n**Expected behavior**\nWe would expect to have something like `IVectorStoreRecordCollection.ListAsync`. That would allow to find what keys need to be deleted.\n\n**Screenshots**\nN/A\n\n**Platform**\n - OS: all\n - IDE: N/A\n - Language: C#\n - Source: main branch of repository\n\n**Additional context**\nN/A",
    "comments": [
      {
        "user": "westey-m",
        "body": "We should also consider being able to control the sort order as part of this. To reliably page through an entire dataset, being able to control the sort order is valuable, so that sorting can be done on a field that will not change, causing record to be missed.\nThis may of course not be supported by all VectorDBs so some analysis is required to validate whether this is feasible."
      },
      {
        "user": "roji",
        "body": "Seems like we also have #10295 tracking the same thing - am proposing we use this to track the Python side of the feature, and #10295 to track the .NET side."
      },
      {
        "user": "eavanvalkenburg",
        "body": "we also have this already for python: #9911"
      }
    ]
  },
  {
    "issue_number": 10473,
    "title": ".Net: Process visualization POC - Step 1",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-02-10T18:02:19Z",
    "updated_at": "2025-06-17T12:30:10Z",
    "labels": [
      ".NET",
      "vscode",
      "processes"
    ],
    "body": "As a developer I can execute and preview a process which I have defined delcaratively\n\n- [x] Create declarative representations of the process getting started samples\n- [x] Extend SKConsoleApp to execute each sample\n- [x] Add support for a new file type to SK Tools e.g. `.process.yml`\n- [x] Add action to execute a `.process.yml` file and display results in output window\n- [x] Add action to preview a `.process.yml` file and display a graphical visualization",
    "comments": [
      {
        "user": "crickman",
        "body": "Might be worthwhile to be aware of this POC: https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/GettingStartedWithProcesses/Utilities/MermaidRenderer.cs"
      }
    ]
  },
  {
    "issue_number": 11273,
    "title": ".Net: New Feature: support default value in VectorDataProperty",
    "author": "dluc",
    "state": "open",
    "created_at": "2025-03-29T07:17:33Z",
    "updated_at": "2025-06-17T12:21:45Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "Differently from a normal data type, where properties can have a default value, when working with `VectorStoreGenericDataModel`, non nullable properties don't have the default .NET value (e.g. 0 for long) and cannot be defined with a default value either.\n\nExample class:\n\n```csharp\npublic class Custom\n{\n    [VectorStoreRecordKey(StoragePropertyName = \"id\")]\n    public string Id { get; set; }\n\n    [VectorStoreRecordVector(3)]\n    public ReadOnlyMemory<float> Vec { get; set; }\n\n    [VectorStoreRecordData(IsFilterable = true)]\n    public int Count { get; set; }      // This has an implicit default of 0\n\n    [VectorStoreRecordData(IsFilterable = true)]\n    public int Count2 { get; set; } = 5;      // Explicit default of 5\n}\n```\n\n**VectorStoreGenericDataModel** example code throwing an exception, because the `Count` value is not set:\n\n```csharp\nvar db = new PostgresVectorStore(dataSource);\n\nvar def = new VectorStoreRecordDefinition\n{\n    Properties = new List<VectorStoreRecordProperty>\n    {\n        new VectorStoreRecordKeyProperty(\"Id\", typeof(string)),\n        new VectorStoreRecordVectorProperty(\"Vec\", typeof(ReadOnlyMemory<float>)) { Dimensions = 3, DistanceFunction = DistanceFunction.CosineSimilarity },\n        new VectorStoreRecordDataProperty(\"Count\", typeof(long))\n    }\n};\n\nvar collection = db.GetCollection<string, VectorStoreGenericDataModel<string>>(\"tests\", def);\nawait collection.CreateCollectionAsync().ConfigureAwait(false);\n\nvar record = new VectorStoreGenericDataModel<string>(\"1\");\nrecord.Vectors[\"Vec\"] = new ReadOnlyMemory<float>([0.15f, 0.25f, 0.35f]);\nawait collection.UpsertAsync(record);\n```\n\nException:\n\n> Microsoft.Extensions.VectorData.VectorStoreOperationException: Call to vector store failed.\n> ---> Npgsql.PostgresException (0x80004005): 23502: null value in column \"Count\" of relation \"tests\" violates not-null constraint\n\n\nThe exception can be fixed changing `Count` to nullable `long?` or setting a value\n```csharp\nrecord.Data[\"Count\"] = 123;\n```\n\nHowever, it would be better if\n\n1. The property used .NET default, e.g. 0 in this case\n2. The property could be configured with a default value, for instance:\n\n```csharp\nvar def = new VectorStoreRecordDefinition\n{\n    Properties = new List<VectorStoreRecordProperty>\n    {\n        new VectorStoreRecordKeyProperty(\"Id\", typeof(string)),\n        new VectorStoreRecordVectorProperty(\"Vec\", typeof(ReadOnlyMemory<float>)) { Dimensions = 3, DistanceFunction = DistanceFunction.CosineSimilarity },\n        new VectorStoreRecordDataProperty(\"Count\", typeof(long))\n        {\n            DefaultValue = 0,\n        }\n    }\n};\n```",
    "comments": [
      {
        "user": "roji",
        "body": "Thanks @dluc, this is really useful. Here's how I see it:\n\nThe problem is that we create a non-nullable column in the database, and then, since the user dictionary contains no value, we don't send anything for the column when upserting, causing the exception. So this is bug is specific to dynamic mapping (with POCO mapping the property is always there and has the CLR default. Note also that non-relational databases probably don't error, but instead get null; this also isn't good, since trying to read the record back would likely throw when reading the null into e.g. int (TBC)>\n\nSo this seems like a bug with our handling of dynamic mapping. As I'll be doing lots of work in the area very shortly (it's the next item on my list), I'll make sure this is handled properly.\n\nAdding something like `DefaultValue` on VectorStoreRecordDataProperty is quite different though - that would presumably correspond to setting up the default value in the database; AFAIK that's supported only on SQL databases. I don't see us doing this in the short term - we could introduce it via #10359."
      },
      {
        "user": "dluc",
        "body": "\n> Adding something like `DefaultValue` on VectorStoreRecordDataProperty is quite different though - **that would presumably correspond to setting up the default value in the database**; AFAIK that's supported only on SQL databases. I don't see us doing this in the short term - we could introduce it via [#10359](https://github.com/microsoft/semantic-kernel/issues/10359).\n\nWhen the code is\n\n```csharp\npublic int Count2 { get; set; } = 5;   \n```\n\ndo we set `5` default in the table schema?\n- if Yes: I find this confusing, because I could edit the value, but the table schema would not change\n- if No: then `DefaultValue` on `VectorStoreRecordDataProperty` would work the same way, initializing the runtime value, without affecting the table schema -- plus, this would work across all storage types"
      },
      {
        "user": "roji",
        "body": "> do we set 5 default in the table schema?\n\nNo. There's no way for us to know that the property is initialized to 5 - that would mean decompiling the actual code for that property (and it could be an arbitrarily-complex piece of code).\n\n> then DefaultValue on VectorStoreRecordDataProperty would work the same way, initializing the runtime value, without affecting the table schema -- plus, this would work across all storage types\n\nI'm not following. How would we know when to use that DefaultValue setting, i.e. how do decide when the user set the property (and we should send what the .NET property has), and when the user didn't set the property (and we should send DefaultValue)?"
      }
    ]
  },
  {
    "issue_number": 10877,
    "title": ".Net: Collection typing with custom schemas",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-03-10T12:14:45Z",
    "updated_at": "2025-06-17T12:20:33Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata",
      "NextSemester"
    ],
    "body": "In my scenarios, schemas are always built programmatically based on configuration or user input. This means collections cannot be strongly typed to a specific class. \nCurrently, I use:\n\n→ var collection = vectorStore.GetCollection<ulong, object>(\"name\", definition);\n\nSetting TRecord to object seems to work but feels hacky. A class extension to abstract/hide this would improve usability, something like this:\n\n→ var collection = vectorStore.GetCollection<ulong>(\"name\", definition);",
    "comments": [
      {
        "user": "roji",
        "body": "@dluc how does this work? What type is actually returned when you call the search API? At the end fo the day, MEVD needs to instantiate a .NET type when reading results back from the database, what type would it be?\n\nAs @westey-m wrote offline, we have VectorStoreGenericDataModel for this purpose; I believe it should be possible to simplify that and simply use `Dictionary<string, object>` as the dynamic CLR type (rather than requiring the special VectorStoreGenericDataModel), but **some** concrete type is required."
      },
      {
        "user": "westey-m",
        "body": "Also see our documentation page here for the generic data model:\nhttps://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/generic-data-model"
      },
      {
        "user": "dluc",
        "body": "I'll check out `VectorStoreGenericDataModel` 👍\n\nI'd still like having a solution where the consumer app code is the same regardless of the engine selected in the configuration. Currently, the need for different `TKey`, `TRecord` for each engine is leaking in the app, requiring switch statements (something I would not expect from an abstraction layer).\n\nBtw, I'm writing an abstraction on top of the abstraction, so that I can switch from Qdrant to Azure AI Search to Postgres, etc, so it is definitely possible to do it. The question is whether we want users (like me) having to develop another abstraction layer or whether the library takes care of it."
      }
    ]
  },
  {
    "issue_number": 10875,
    "title": ".Net: VectorStoreRecordVectorProperty type restrictions",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-03-10T12:12:38Z",
    "updated_at": "2025-06-17T12:19:23Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata",
      "NextSemester"
    ],
    "body": "The following​ code causes an exception:\n\n→ new VectorStoreRecordVectorProperty(\"DescriptionEmbedding\", typeof(float)) { Dimensions = 4, DistanceFunction = DistanceFunction.CosineDistance, IndexKind = IndexKind.Hnsw },\n\nThe embedding is an array, but alternative types also fail:\ntypeof(float[]) → exception\ntypeof(List<float>) → exception\ntypeof(ReadOnlyMemory<float>) → works\nThe type constraint seems too strict. It should support float[], List<float>, IEnumerable<float>, etc.",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "@dluc our plan is to align with the abstractions being defined for M.E.AI. Take a look at this issue for the details: https://github.com/microsoft/semantic-kernel/issues/10492 "
      },
      {
        "user": "dluc",
        "body": "We had these exact pain points and discussions three years ago when prototyping SK :-)\n\nBack then, we created a powerful and flexible `Embedding` struct with implicit operators, enbling readable code and seamless integrations.  We even anticipated this concept becoming part of the official .NET libs.\n\nIt's unclear why we're discarding that work, it seems like a regression. I've commented on the other ticket; hopefully, the team can reach a compromise to reduce boilerplate. Otherwise, developers might resort again to extension methods or similar approaches to abstract away this complexity."
      },
      {
        "user": "roji",
        "body": "> The type constraint seems too strict. It should support float[], List, IEnumerable, etc.\n\nAs I wrote in https://github.com/microsoft/semantic-kernel/issues/10492#issuecomment-2721983344, it makes no sense to me to represent embeddings as List or IEnumerable, since they're opaque results of an embedding model that should never need to be mutated like collections; I'm happy to be convinced otherwise, but I'd like to see some proposals of concrete usages requiring List/IEnumerable. For float[] specifically I could possibly see it making sense to accept that for ease-of-use, I'd need to think a bit more about it.\n\n> Back then, we created a powerful and flexible Embedding struct with implicit operators, enbling readable code and seamless integrations. We even anticipated this concept becoming part of the official .NET libs.\n\nSimilar to the above, what kind of implicit operators would you expect from an Embedding type?"
      }
    ]
  },
  {
    "issue_number": 10872,
    "title": ".Net: Typed collections limit code reuse",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-03-10T12:08:29Z",
    "updated_at": "2025-06-17T12:17:44Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata",
      "NextSemester"
    ],
    "body": "The current approach requires different code for each storage type, making it hard to switch storage backends without modifying the code.\nExample inconsistencies:\n\n- Qdrant: GetCollection<Guid, object> or GetCollection<Guid, ulong>\n- Weaviate: GetCollection<Guid, object>\n- Azure AI Search: GetCollection<string, object>\n- Sqlite: GetCollection<Guid, object> or GetCollection<Guid, ulong>\n- Postgres: supports short, int, long, string, Guid, but not ulong\n- AzureCosmosDbNoSQL: allows composite keys\n\n**Suggestion:** introduce a more general abstraction that allows using different storage backends without requiring code changes. Don’t force .NET “generics” unless they are consistent across storage engines.",
    "comments": [
      {
        "user": "roji",
        "body": "@dluc I don’t see how this actually works: at the end of the day, some databases support only Guids, others only strings, and yet others only ints - how could an abstraction (or decorator) allow the same code to be written against all three? At the end of the day, the code using IVectorStoreRecordCollection must pass in a specific type (Guid or int) that comes from somewhere; so even if we make the collection non-generic, code interacting with it would still have to be specialized per vector store, since it would need to provide the correct type.\n\nAs written elsewhere, the expectation to be able to write a single piece of code that just works on all connectors is unfortunately not something that can be achieved IMHO, simply because the databases themselves differ. We can do our best, but it's always going to be a leaky abstraction.\n\nHaving said that, we do have #10802 tracking improving dynamic usage around keys (i.e. the fact that VectorStoreGenericDataModel is generic over the key type)."
      },
      {
        "user": "dluc",
        "body": "> the expectation to be able to write a single piece of code that just works on all connectors is unfortunately not something that can be achieved IMHO\n\nI don't think that's true, to me this is about choosing a good \"default\" in each implementation, rather than asking the user to \"always\" make a choice. Choosing the type of keys should be possible, but not mandatory.\n\nLet's say one registers `IVectorStore` in the the service provider, assuming the app can substitute concrete implementations, for example using Qdrant during developent on localhost and using Azure AI Search in prod on Azure (or anything, this is just an example). \n\nThe assumption won't work in a few scenarios, because the code is not portable (substitution not allowed). \n\nHere's an example (from https://github.com/microsoft/semantic-kernel/issues/10877) showing how the code will have to check against all the supported classes (aka leaky abstraction), requiring knowledge about each specific class:\n\n```csharp\npublic async Task<IResult> CreateCollectionAsync(IVectorStore vectorStore, CreateCollectionRequest req, CancellationToken cancellationToken)\n{\n    // Record definition\n    var properties = new List<VectorStoreRecordProperty>();\n    foreach (var f in req.Fields)\n    {\n        switch (f.Type)\n        {\n            case FieldTypes.PrimaryKey:\n                // More conditions needed here to support other storage engines (ulong keys, long, int, composite keys, etc)\n\n                properties.Add(vectorStore is QdrantVectorStore     // <--------\n                    ? new VectorStoreRecordKeyProperty(f.Name, typeof(Guid))\n                    : new VectorStoreRecordKeyProperty(f.Name, typeof(string)));\n\n                break;\n            case FieldTypes.Vector:\n                properties.Add(new VectorStoreRecordVectorProperty(f.Name, typeof(ReadOnlyMemory<float>))\n                {\n                    Dimensions = req.Dimensions,\n                    DistanceFunction = req.DistanceFunction,\n                    IndexKind = req.IndexKind\n                });\n                break;\n            // ...\n        }\n    }\n    var recordDefinition = new VectorStoreRecordDefinition { Properties = properties };\n\n    // Create collection\n    try\n    {\n        switch (vectorStore)                                // <--------\n        {\n            case QdrantVectorStore:                         // <--------\n            {\n                var collection = vectorStore.GetCollection<Guid, object>(req.CollectionName, recordDefinition);\n                await collection.CreateCollectionAsync(cancellationToken).ConfigureAwait(false);\n                break;\n            }\n            case AzureAISearchVectorStore:                  // <--------\n            {\n                var collection = vectorStore.GetCollection<string, object>(req.CollectionName, recordDefinition);\n                await collection.CreateCollectionAsync(cancellationToken).ConfigureAwait(false);\n                break;\n            }\n            case InMemoryVectorStore:                       // <--------\n            {\n                var collection = vectorStore.GetCollection<string, VectorStoreGenericDataModel<string>>(req.CollectionName, recordDefinition);\n                await collection.CreateCollectionAsync(cancellationToken).ConfigureAwait(false);\n                break;\n            }\n        }\n    }\n    catch (VectorStoreOperationException e)\n    {\n        // Qdrant + Azure AI Search                         // <--------\n        if (e.InnerException?.Message.Contains(\"already exists\", StringComparison.OrdinalIgnoreCase) == true)\n        {\n            this._log.LogError(\"Unable to create {StorageType} collection, it already exists\", req.StorageType);\n            return ...\n        }\n\n        // InMemory? (not throwing yet)\n                                                            // <--------\n\n        // More string matching for other storage engines\n                                                            // <--------\n\n        this._log.LogError(e, \"Unable to create {StorageType} collection, unexpected error: {Message}\", req.StorageType, e.Message);\n        return ...\n    }\n}\n```"
      },
      {
        "user": "roji",
        "body": "> I don't think that's true, to me this is about choosing a good \"default\" in each implementation, rather than asking the user to \"always\" make a choice. Choosing the type of keys should be possible, but not mandatory.\n\nI understand the desire here, but you still haven't explained how this is actually supposed to work - the concrete key values that your application code provides need to differ. In other words, how would you imagine **actual** code that interacts with two databases, one in which the key is an `int` and the other in which the key is a `Guid`?"
      }
    ]
  },
  {
    "issue_number": 10864,
    "title": "Bug: Stop using v1 of Chroma API",
    "author": "DmitryLukyanov",
    "state": "open",
    "created_at": "2025-03-09T13:48:25Z",
    "updated_at": "2025-06-17T12:00:37Z",
    "labels": [
      "bug",
      "memory connector",
      "memory",
      "msft.ext.vectordata"
    ],
    "body": "**Describe the bug**\nRecently Chroma 1.0 drops support for v1 API \n\n**To Reproduce**\n\nJust configure chroma db with latest chroma docker image. Observe `400` or `405` http response from chroma server.\n\n**Platform**\n - Language: [C#]\n - Source: [Microsoft.SemanticKernel.Connectors.Chroma, Version=\"1.40.0-alpha\"]\n - OS: [Windows]\n\n**Additional context**\nPlease see this issue in chroma repository: https://github.com/chroma-core/chroma/issues/3879 with related discussions",
    "comments": [
      {
        "user": "roji",
        "body": "Note that if/when we do more work on Chroma support, it'll like be in the form of creating an implementation of the new IVectorStore implementation, rather than updating the current older IMemoryStore implementation; this is tracked by #12510."
      }
    ]
  },
  {
    "issue_number": 12510,
    "title": ".Net: [MEVD] Chroma VectorStore provider",
    "author": "roji",
    "state": "open",
    "created_at": "2025-06-17T11:59:50Z",
    "updated_at": "2025-06-17T12:00:03Z",
    "labels": [
      ".NET",
      "triage",
      "msft.ext.vectordata"
    ],
    "body": "There's currently only an IMemoryStore implementation.\n\n",
    "comments": []
  },
  {
    "issue_number": 12509,
    "title": ".Net: [MEVD] Milvus VectorStore provider",
    "author": "roji",
    "state": "open",
    "created_at": "2025-06-17T11:57:25Z",
    "updated_at": "2025-06-17T11:59:29Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "There's currently only an IMemoryStore implementation.",
    "comments": []
  },
  {
    "issue_number": 12284,
    "title": "Bug: Chroma API v2 not supported in Microsoft.SemanticKernel.Connectors.Chroma",
    "author": "ramonduraes",
    "state": "closed",
    "created_at": "2025-05-27T16:41:29Z",
    "updated_at": "2025-06-17T11:58:21Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "The `Microsoft.SemanticKernel.Connectors.Chroma` connector is currently using the deprecated Chroma DB v1 API. When attempting to connect to the latest Chroma DB (v2), the following error is returned:\n\n{\"error\":\"Unimplemented\",\"message\":\"The v1 API is deprecated. Please use /v2 apis\"}\n\n**Suggestion:**\n\nPlease update the connector to use the new `/v2` API endpoints to ensure compatibility with the latest version of Chroma DB.\n\n**Reference:**\n\n- https://docs.trychroma.com/\n",
    "comments": []
  },
  {
    "issue_number": 12504,
    "title": ".Net: [MEVD] Ensure Contains compatibility with C# 14 first-class spans",
    "author": "roji",
    "state": "open",
    "created_at": "2025-06-17T11:13:12Z",
    "updated_at": "2025-06-17T11:53:10Z",
    "labels": [
      ".NET",
      "triage",
      "msft.ext.vectordata"
    ],
    "body": "C# 14 is bringing first-class spans, which changes the compiler resolution of method invocations, causes MemoryExtensions.Contains() (accepting spans) to be resolved instead of the traditional Enumerable.Contains. This can cause issues with LINQ providers, and likely would need reacting to in MEVD LINQ filters as well. We can integrate this logic into the FilterTranslationPreprocessor.\n\nOn the EF side, see https://github.com/dotnet/efcore/issues/35547 and https://github.com/dotnet/efcore/issues/35100",
    "comments": []
  },
  {
    "issue_number": 12508,
    "title": ".Net: [MEVD] Refactor embedding type resolution to ProviderServices (to avoid duplication in each connector)",
    "author": "roji",
    "state": "open",
    "created_at": "2025-06-17T11:51:04Z",
    "updated_at": "2025-06-17T11:51:15Z",
    "labels": [
      ".NET",
      "triage",
      "msft.ext.vectordata"
    ],
    "body": "Throughout all providers, we have code such as the following, which successively tries all the vector types supported by the provider:\n\n```c#\nif (vectorProperty.TryGenerateEmbedding<TRecord, Embedding<float>>(record, cancellationToken, out var floatTask))\n{\n\tgeneratedEmbeddings ??= new Dictionary<VectorPropertyModel, IReadOnlyList<Embedding>>(vectorPropertyCount);\n\tgeneratedEmbeddings[vectorProperty] = [await floatTask.ConfigureAwait(false)];\n}\n#if NET8_0_OR_GREATER\nelse if (vectorProperty.TryGenerateEmbedding<TRecord, Embedding<Half>>(record, cancellationToken, out var halfTask))\n{\n\tgeneratedEmbeddings ??= new Dictionary<VectorPropertyModel, IReadOnlyList<Embedding>>(vectorPropertyCount);\n\tgeneratedEmbeddings[vectorProperty] = [await halfTask.ConfigureAwait(false)];\n}\n#endif\nelse\n{\n\tthrow new InvalidOperationException(\n\t\t$\"The embedding generator configured on property '{vectorProperty.ModelName}' cannot produce an embedding of type '{typeof(Embedding<float>).Name}' for the given input type.\");\n}\n```\n\nThis should be factored out to generic logic, ideally in the model, so that providers can be made simpler. This may involve a merging of the current CollectionModel and CollectionModelBuilder types.",
    "comments": []
  },
  {
    "issue_number": 10194,
    "title": "[MEVD] Specification/conformance integration test suite for providers",
    "author": "roji",
    "state": "open",
    "created_at": "2025-01-15T12:52:12Z",
    "updated_at": "2025-06-17T11:46:53Z",
    "labels": [
      ".NET",
      "memory connector",
      "sk team issue",
      "memory",
      "msft.ext.vectordata"
    ],
    "body": "The current integration tests for NEVM providers are implemented separately for each provider, with no support from the abstraction. To promote better universal coverage and reduce the per-provider work needed, we could expoes a \"specification test suite\" with the abstraction, which providers implement, and which execute a set of standardized tests to ensure the provider works correctly. A good model here would be the Entity Framework Core specification test suite, which exercises an EF provider in a myriad of ways.\n\n/cc @westey-m ",
    "comments": [
      {
        "user": "dmytrostruk",
        "body": "Small note: I already refactored the integration tests for `IVectorStore` interface to have a single set of tests for each provider:\nhttps://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/IntegrationTests/Connectors/Memory/BaseVectorStoreTests.cs\n\nI tried to do the same with `IVectorStoreRecordCollection` interface and it appeared that it's more complicated than `IVectorStore`, since each provider has some limitations (e.g. the requirements for collection/table/field names, supported index kinds, dimensions etc), which makes it harder to unify into a single test suite to cover all methods in that abstraction and all available connectors. At the end, I noticed that I still need to override some test cases for specific connectors to ensure high coverage and this put the idea of a single test suite under the question.\n\nAs an alternative solution, we can keep current tests for specific providers to ensure high coverage but add separate set of test cases (maybe basic ones) using abstraction only and enable it for as many connectors as we can."
      },
      {
        "user": "roji",
        "body": "@dmytrostruk thanks for the context, yeah, that all makes sense. I plan to take a look at this, will share what i come up with. I agree that it's OK if an implementation needs to have its own tests in addition to the \"standard\" ones it can get for free - I do hope we'll be able to provide a meaningful set of standard ones, without being prevented from testing too much by differences between the database etc."
      },
      {
        "user": "roji",
        "body": "Note: the basic infrastructure for this and first test implementation are being done as part of #10273 (LINQ-based filtering).\n\n* Base test classes and fixtures to be extended by each provider (and published via a separate nuget). This is the main point of the exercise.\n* Use of [testcontainers](https://testcontainers.com) wherever possible; this simplifies setting up the Docker container, manages waiting for the service to actually be available (reduces flakiness), etc.\n* Where container-based testing isn't possible (since there's no Docker image), have the test suite check whether the required configuration for accessing the service is defined - and skip if not. This allows a dev to simply define e.g. an endpoint URL environment variable to activate testing a connector, while not failing tests where that config is missing (just skip).\n* Separate integration test project for each provider. This makes sense since we eventually would like connectors to be owned by their respective vector database vendors, and so ideally move out of the SK repo. This also separates the tests for different containers, allowing easier work on one without affecting others, etc.\n\nAssuming we're all OK with this direction (we should discuss), am keeping this issue to track porting over the other non-filter integration tests over into this new integration test structure."
      }
    ]
  },
  {
    "issue_number": 12505,
    "title": ".Net: [MEVD] Data type tests",
    "author": "roji",
    "state": "open",
    "created_at": "2025-06-17T11:32:13Z",
    "updated_at": "2025-06-17T11:35:07Z",
    "labels": [
      ".NET",
      "triage",
      "msft.ext.vectordata"
    ],
    "body": "We have integration test coverage for vector types, but not for data types. This should do full, end-to-end testing of all relevant features for supporting a type (serialization, deserialization, filtering in LINQ...).",
    "comments": []
  },
  {
    "issue_number": 12506,
    "title": ".Net: [MEVD] Merge old integration tests into the new ones",
    "author": "roji",
    "state": "open",
    "created_at": "2025-06-17T11:34:40Z",
    "updated_at": "2025-06-17T11:34:52Z",
    "labels": [
      ".NET",
      "triage",
      "msft.ext.vectordata"
    ],
    "body": "We currently have two integration test suites for MEVD: the older one (under src/IntegrationTests, mainly provider-by-provider), and the new one (under test/VectorData, with cross-provider conformance conformance tests). Go over the older one, merge anything that's missing from the new one, and remove the old one.",
    "comments": []
  },
  {
    "issue_number": 12220,
    "title": ".Net: [MEVD] [SQL Server] Support efficient binary encoding for vector (when available)",
    "author": "roji",
    "state": "open",
    "created_at": "2025-05-21T16:43:04Z",
    "updated_at": "2025-06-17T11:30:10Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": null,
    "comments": []
  },
  {
    "issue_number": 12164,
    "title": ".Net: [MEVD] Ensure providers have usable constructors as appropriate",
    "author": "roji",
    "state": "open",
    "created_at": "2025-05-19T13:18:14Z",
    "updated_at": "2025-06-17T11:30:05Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "Most of our providers currently accept a low-level client only in their constructors. For example, QdrantVectorStore requires a QdrantClient, so the getting started experience is as follows:\n\n```c#\nusing var client = new QdrantClient(\"localhost\", 6334);\nusing var store = new QdrantVectorStore(client, ownsClient: true);\n```\n\nAt the same time, we do have more usable DI registration methods which accept endpoint information directly:\n\n```c#\nservices.AddQdrantVectorStore(\"localhost\", 6334);\n```\n\nWe should align the constructors to ensure that non-DI users get the same usability as DI users:\n\n```c#\nusing var store = new QdrantVectorStore(\"localhost\", 6334);\n```\n\nWe can now do this since the abstraction types implement IDisposable; this means that QdrantVectorStore can now instantiate a QdrantClient internally, use it, and then dispose it when it's disposed.\n\nNote that low-level clients may have many combinations of initialization possibilities - we don't have to replicate all of these on our constructors (or DI methods). But at least doing the simple 90% probably makes sense.\n\nNote that we should do this on both the VectorStore and VectorStoreCollection implementation.",
    "comments": []
  },
  {
    "issue_number": 12161,
    "title": ".Net MEVD: Ensure proper escaping and quoting of storage names defined by the users",
    "author": "adamsitnik",
    "state": "open",
    "created_at": "2025-05-19T10:17:41Z",
    "updated_at": "2025-06-17T11:29:59Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "#12129 took care of potentially untrusted input like string values coming from the user input. It was important and will be included for BUILD\n\nWhat is left is proper escaping and quoting of storage names defined by the users (safe input).\n\nThere was already one attempt: #11676",
    "comments": []
  },
  {
    "issue_number": 12182,
    "title": ".Net: [MEVD] Support Guid across MEVD providers (as string where needed)",
    "author": "roji",
    "state": "open",
    "created_at": "2025-05-20T06:48:55Z",
    "updated_at": "2025-06-17T11:28:40Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "Our support for the Guid type is currently spotty. Some providers support it natively (e.g. PostgreSQL, SQL Server); for those that don't, some support it by storing it as a string, but others don't. We should probably allow Guids everywhere, as they're mappable to string without loss of information etc.\n\nNote: pay attention to uppercase vs. lowercase representation of Guids as strings - lowercase is probably the way to go (though EF generates uppercase by default, see https://github.com/dotnet/efcore/issues/19651).",
    "comments": []
  },
  {
    "issue_number": 11350,
    "title": ".Net: [MEVD] Support proper bulk operations on Cosmos NoSQL",
    "author": "roji",
    "state": "open",
    "created_at": "2025-04-03T06:55:39Z",
    "updated_at": "2025-06-17T11:28:31Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "For upsert and delete, we currently fire off N separate tasks (in parallel), but Cosmos has proper bulk operation support ([docs](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/bulk-executor-dotnet)), at the very least for upserting.",
    "comments": []
  },
  {
    "issue_number": 11352,
    "title": ".Net: [MEVD] Improve partition keys with Cosmos NoSQL",
    "author": "roji",
    "state": "open",
    "created_at": "2025-04-03T07:23:11Z",
    "updated_at": "2025-06-17T11:28:25Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "* We currently only support a single partition key property, but Cosmos supports hierarchical partition keys; see [docs](https://learn.microsoft.com/en-us/azure/cosmos-db/hierarchical-partition-keys?tabs=net-v3%2Cbicep).\n* We only support string partition keys, but Cosmos supports bool and double. We should just use the Cosmos SDK's [PartitionKey type](https://learn.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.partitionkey?view=azure-dotnet) to represent partition keys.",
    "comments": []
  },
  {
    "issue_number": 11272,
    "title": ".Net: Bug: Postgres upsert fails",
    "author": "dluc",
    "state": "closed",
    "created_at": "2025-03-29T03:36:26Z",
    "updated_at": "2025-06-17T11:24:04Z",
    "labels": [
      ".NET",
      "documentation",
      "msft.ext.vectordata"
    ],
    "body": "The code below fails on `UpsertAsync`. Manual insert with SQL works fine instead.\n\n**This works:**\n\n```SQL\nINSERT INTO public.\"tests\" (\"id\", \"Count\", \"Vec\")\nVALUES ('1', 5, '[0.15, 0.25, 0.35]')\nON CONFLICT (\"id\")\nDO UPDATE SET \"Count\"=5, \"Vec\"='[0.15, 0.25, 0.35]';\n```\n\n**This fails:**\n\n```csharp\npublic static async Task Run(string connString)\n{\n    var dataSource = NpgsqlDataSource.Create(connString);\n    var db = new PostgresVectorStore(dataSource);\n    var collection = db.GetCollection<string, Model>(\"tests\") as PostgresVectorStoreRecordCollection<string, Model>;\n    await collection.DeleteCollectionAsync().ConfigureAwait(false);\n    await collection.CreateCollectionAsync().ConfigureAwait(false);\n\n    // exception\n    await collection.UpsertAsync(\n        new Model { Id = \"1\", Count = 5, Vec = new ReadOnlyMemory<float>([0.15f, 0.25f, 0.35f]) });\n}\n\npublic class Model\n{\n    [VectorStoreRecordKey(StoragePropertyName = \"id\")]\n    public string Id { get; set; }\n\n    [VectorStoreRecordVector(3)]\n    public ReadOnlyMemory<float> Vec { get; set; }\n\n    [VectorStoreRecordData(IsFilterable = true)]\n    public int Count { get; set; }\n}\n```\n\nPostgres: docker.io/pgvector/pgvector:pg17\n\n**Error:**\n\n> Unhandled exception. Microsoft.Extensions.VectorData.VectorStoreOperationException: **Call to vector store failed**.\n>  ---> System.InvalidCastException: **Writing values of 'Pgvector.Vector' is not supported for parameters having no NpgsqlDbType or DataTypeName**. Try setting one of these values to the expected database type..\n>    at Npgsql.Internal.AdoSerializerHelpers.<GetTypeInfoForWriting>g__ThrowWritingNotSupported|1_0(Type type, PgSerializerOptions options, Nullable`1 pgTypeId, Nullable`1 npgsqlDbType, Exception inner)\n>    at Npgsql.Internal.AdoSerializerHelpers.GetTypeInfoForWriting(Type type, Nullable`1 pgTypeId, PgSerializerOptions options, Nullable`1 npgsqlDbType)\n>    at Npgsql.NpgsqlParameter.ResolveTypeInfo(PgSerializerOptions options)\n>    at Npgsql.NpgsqlParameterCollection.ProcessParameters(PgSerializerOptions options, Boolean validateValues, CommandType commandType)\n>    at Npgsql.NpgsqlCommand.ExecuteReader(Boolean async, CommandBehavior behavior, CancellationToken cancellationToken)\n>    at Npgsql.NpgsqlCommand.ExecuteReader(Boolean async, CommandBehavior behavior, CancellationToken cancellationToken)\n>    at Npgsql.NpgsqlCommand.ExecuteNonQuery(Boolean async, CancellationToken cancellationToken)\n>    at Microsoft.SemanticKernel.Connectors.Postgres.PostgresVectorStoreDbClient.ExecuteNonQueryAsync(PostgresSqlCommandInfo commandInfo, CancellationToken cancellationToken) in ~/dotnet/src/Connectors/Connectors.Memory.Postgres/PostgresVectorStoreDbClient.cs:line 266\n>    at Microsoft.SemanticKernel.Connectors.Postgres.PostgresVectorStoreDbClient.ExecuteNonQueryAsync(PostgresSqlCommandInfo commandInfo, CancellationToken cancellationToken) in ~/dotnet/src/Connectors/Connectors.Memory.Postgres/PostgresVectorStoreDbClient.cs:line 267\n>    at Microsoft.SemanticKernel.Connectors.Postgres.PostgresVectorStoreDbClient.UpsertAsync(String tableName, Dictionary`2 row, String keyColumn, CancellationToken cancellationToken) in ~/dotnet/src/Connectors/Connectors.Memory.Postgres/PostgresVectorStoreDbClient.cs:line 122\n>    at Microsoft.SemanticKernel.Connectors.Postgres.PostgresVectorStoreRecordCollection`2.<>c__DisplayClass14_0.<<UpsertAsync>b__1>d.MoveNext() in ~/dotnet/src/Connectors/Connectors.Memory.Postgres/PostgresVectorStoreRecordCollection.cs:line 162\n> --- End of stack trace from previous location ---\n>    at Microsoft.SemanticKernel.Connectors.Postgres.PostgresVectorStoreRecordCollection`2.RunOperationAsync[T](String operationName, Func`1 operation) in ~/dotnet/src/Connectors/Connectors.Memory.Postgres/PostgresVectorStoreRecordCollection.cs:line 346\n>    --- End of inner exception stack trace ---\n>    at Microsoft.SemanticKernel.Connectors.Postgres.PostgresVectorStoreRecordCollection`2.RunOperationAsync[T](String operationName, Func`1 operation) in ~/dotnet/src/Connectors/Connectors.Memory.Postgres/PostgresVectorStoreRecordCollection.cs:line 350\n",
    "comments": [
      {
        "user": "dluc",
        "body": "Workarounds:\n\nOption 1 (deprecated):\n\ncall `NpgsqlConnection.GlobalTypeMapper.UseVector();`\n\nOption 2:\n\n```csharp\nvar builder = new NpgsqlDataSourceBuilder(connString);\nbuilder.UseVector(); // <--\nvar db = new PostgresVectorStore(builder.Build());\n```"
      },
      {
        "user": "roji",
        "body": "@dluc I can't reproduce this - can you post a runnable, minimal console program please? And which exact version of the MEVD connector are you running against (1.44.0-preview nuget, feature-vector-data-prev2 branch?)"
      },
      {
        "user": "westey-m",
        "body": "@dluc, note that UseVector is required.  Since the connector is taking a `NpgsqlDataSource` as input, the caller needs to set this before passing it in.\nThis is set automatically when using the DI helpers that construct `NpgsqlDataSource`.\nSee https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/out-of-the-box-connectors/postgres-connector?pivots=programming-language-csharp#getting-started\n\nI think there are two actions here:\n\n1. See if there is a way to check if UseVector has been set, and throw if not on construction.\n2. Update the docs page further to call this out in a limitations section similar to some of the other connectors."
      }
    ]
  },
  {
    "issue_number": 11311,
    "title": ".Net: Move hybrid search compliance tests to new model to benefit from dataset availability checks",
    "author": "westey-m",
    "state": "closed",
    "created_at": "2025-04-01T14:40:16Z",
    "updated_at": "2025-06-17T11:23:30Z",
    "labels": [
      "docs and tests",
      ".NET",
      "memory connector",
      "sk team issue",
      "Build",
      "msft.ext.vectordata"
    ],
    "body": "Some of our hybrid search tests are failing intermittently due to indexing latency.\nWe should move these tests to the new test model to avail of the checks that ensure that the dataset is indexed before doing any further work.\nhttps://github.com/microsoft/semantic-kernel/actions/runs/14192207242/job/39759230084",
    "comments": []
  },
  {
    "issue_number": 11353,
    "title": ".Net: [MEVD] Support additional key types with Cosmos NoSQL",
    "author": "roji",
    "state": "open",
    "created_at": "2025-04-03T07:26:08Z",
    "updated_at": "2025-06-17T11:22:28Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "We currently only support string types, but users should be able to use a Guid or numeric value.",
    "comments": [
      {
        "user": "roji",
        "body": "Additional problem around keys... The recommended Cosmos NoSQL practice is for the user to choose a partition key that makes sense; we currently default to using the `id` as the partition key, which can be acceptable in some situations but it probably not the right default - I think we should be guiding users towards making a careful, informed decision about the partition key, rather than trying to hide that away from that.\n\nThat would mean, for example, registering Cosmos with a AzureCosmosDBNoSQLCompositeKey by default (that's a type that wraps an id and a partition key), rather than string.\n\nPutting for consideration for Build as changing this later would be breaking.\n\n/cc @Pilchie "
      }
    ]
  },
  {
    "issue_number": 11320,
    "title": ".Net: [MEVD] Review our vector validation",
    "author": "roji",
    "state": "open",
    "created_at": "2025-04-01T18:45:35Z",
    "updated_at": "2025-06-17T11:22:16Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "All our connectors do validation on the vector CLR types (e.g. `ReadOnlyMemory<float>` during model building. Some perform validation from the vector search methods, on the generic argument passed by the user - but not all. Add test coverage for passing bogus embedding types.",
    "comments": []
  },
  {
    "issue_number": 11665,
    "title": ".Net: [MEVD] Comparing to nullable value type in filter translators fails",
    "author": "TsengSR",
    "state": "closed",
    "created_at": "2025-04-21T21:54:46Z",
    "updated_at": "2025-06-17T11:20:49Z",
    "labels": [
      "bug",
      ".NET",
      "Build",
      "msft.ext.vectordata"
    ],
    "body": "\n**Describe the bug**\nWhen the Filter predicate on `VectorSearchOptions<T>` is a nullable, search fails with an exception. \n\n`activeOnly` is defined as `bool? activeOnly = null` in the methods parameters. Since the if block already ensures its never null, I don't see any issue why that operation should fail. \n\nBoth `activeOnly` and `activeOnly.Value` result in the exception. \n\nOnly by changing the method declaration to `bool activeOnly` or `bool activeOnly = true`, read non-nullable, the query executes successfully. \n\nImho this is not an expected behaviour and should be fixed, as its a very common way to define predicates that way. \n\n**To Reproduce**\n```csharp\n        Expression<Func<Project, bool>>? filter = null;\n        if (activeOnly is not null)\n        {\n            filter = project => project.IsActive == activeOnly;\n        }\n        VectorSearchOptions<Project> filterOptions = new()\n        {\n            Top = top,\n            IncludeTotalCount = true,\n            Filter = filter,\n        };\n```\n\nThrows:\n<details><summary>Exception Details</summary>\n<p>\n<pre>\nSystem.NotSupportedException\n  HResult=0x80131515\n  Message=Equality expression not supported by Qdrant\n  Source=Microsoft.SemanticKernel.Connectors.Qdrant\n  StackTrace:\n   at Microsoft.SemanticKernel.Connectors.Qdrant.QdrantFilterTranslator.TranslateEqual(Expression left, Expression right, Boolean negated)\n   at Microsoft.SemanticKernel.Connectors.Qdrant.QdrantFilterTranslator.Translate(Expression node)\n   at Microsoft.SemanticKernel.Connectors.Qdrant.QdrantFilterTranslator.Translate(LambdaExpression lambdaExpression, IReadOnlyDictionary`2 storagePropertyNames)\n   at Microsoft.SemanticKernel.Connectors.Qdrant.QdrantVectorStoreRecordCollection`1.<VectorizedSearchAsync>d__32`1.MoveNext()\n</pre>\n</p>\n</details> \n\n```csharp\n\n        Expression<Func<Project, bool>>? filter = null;\n        if (activeOnly is not null)\n        {\n            filter = project => project.IsActive == activeOnly.Value;\n        }\n        VectorSearchOptions<Project> filterOptions = new()\n        {\n            Top = top,\n            IncludeTotalCount = true,\n            Filter = filter,\n        };\n```\n\n**Expected behavior**\nOperation to succeed.\n\n\n**Platform**\n - Language: C#\n - Source:  Nuget\n   - Microsoft.SemanticKernel 1.47.0\n   - Microsoft.SemanticKernel.Connectors.Ollama 1.47.0-alpha\n   - Microsoft.SemanticKernel.Connectors.Qdrant 1.47.0-preview\n - AI model: not applicable\n - IDE: Visual Studio 2022\n - OS: Windows 11, 24H2\n",
    "comments": []
  },
  {
    "issue_number": 11673,
    "title": ".Net: Bug: MEVD: Member field access doesn't work in filter translators",
    "author": "westey-m",
    "state": "closed",
    "created_at": "2025-04-22T08:24:50Z",
    "updated_at": "2025-06-17T11:20:42Z",
    "labels": [
      "bug",
      ".NET",
      "memory connector",
      "Build",
      "msft.ext.vectordata"
    ],
    "body": "The following filter is failing with `System.NotSupportedException : Member access for 'mystring' is unsupported - only member access over the filter parameter are supported`\n\n```csharp\nstring mystring = \"foo\"\nr => r.str == mystring\n```\n",
    "comments": [
      {
        "user": "roji",
        "body": "Repro test:\n\n```c#\nprivate int _memberField = 8;\n\n[ConditionalFact]\npublic virtual Task Member_field()\n    => this.TestFilterAsync(\n        r => r.Int == this._memberField,\n        r => (int)r[\"Int\"] == this._memberField);\n```"
      }
    ]
  },
  {
    "issue_number": 11709,
    "title": ".Net: MEVD: Add compliance test for records with multiple vectors",
    "author": "westey-m",
    "state": "open",
    "created_at": "2025-04-24T09:39:51Z",
    "updated_at": "2025-06-17T11:20:20Z",
    "labels": [
      ".NET",
      "memory connector",
      "sk team issue",
      "msft.ext.vectordata"
    ],
    "body": "We should have some compliance tests that verify cases where there are multiple vectors on a single record, and run that against all dbs that support this.\n\nIt should include CRUD as well as search where we pick a vector to search against, plus validations for failure cases, e.g. where we don't specify a vector to search against, but should.",
    "comments": []
  },
  {
    "issue_number": 12482,
    "title": ".Net: New Feature: Allow DI of `SqliteConnection` to `SqliteVectorStore`",
    "author": "aaronpowell",
    "state": "open",
    "created_at": "2025-06-16T06:18:20Z",
    "updated_at": "2025-06-17T11:06:18Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "---\nname: Feature request\nabout: Suggest an idea for this project\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n\nI'm wanting to integrate the Sqlite Vector Store support and orchestrate it with the [Aspire SQLite hosting package](https://www.nuget.org/packages/CommunityToolkit.Aspire.Hosting.Sqlite). While this works fine and generates a connection string, there's no way in which I can leverage the client integration packages, as I can only provide a connection string to `AddSqliteCollection`.\n\nI'd like to be able to provide the `SqliteConnection` via DI, so that I don't have to unpack the connection string, but also leverage what the Aspire integration provides, such as health checks on the connection.",
    "comments": [
      {
        "user": "roji",
        "body": "@aaronpowell can you provide a bit more context on what accepting a SqliteConnection would unlock? Like how is it blocking you with Aspire that the provider only accepts a connection string?\n\nOne reason we'd rather avoid accepting SqliteConnection is that it isn't thread-safe, making the MEVD SqliteColllection also non-thread-safe. This means it would have to be Scoped rather than Singleton, which would be less good for performance, and generallly a bit confusing/problematic (the same type is thread-safe/Singleton if constructed from a connection string, but no thread-safe/Scoped if constructed from a connection)."
      }
    ]
  },
  {
    "issue_number": 12483,
    "title": "New Feature:  Use Web RTC https://learn.microsoft.com/en-in/azure/ai-services/openai/how-to/realtime-audio-webrtc in Semantic kernel.",
    "author": "kkdubey",
    "state": "open",
    "created_at": "2025-06-16T06:41:36Z",
    "updated_at": "2025-06-17T09:51:09Z",
    "labels": [
      "triage"
    ],
    "body": "---\nname: Feature request\nabout: How can i use [Web RTC](https://learn.microsoft.com/en-in/azure/ai-services/openai/how-to/realtime-audio-webrtc) in Semantic kernel.\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Hi @kkdubey, can you give some more context, please? Is there a docs bug? A better explanation needed for realtime audio? Something different? Thank you."
      },
      {
        "user": "kkdubey",
        "body": "@moonbox3 In my application, I'm using Azure CallAutomation service with AI Foundry's real-time model and semantic kernel for multiple agents. \n\nI want to incorporate WebRTC live voice into my service.\n"
      }
    ]
  },
  {
    "issue_number": 12484,
    "title": ".NET: Bug: Calls to PTU 4o-mini instance hanging / exceeding 10 seconds",
    "author": "dominic-codespoti",
    "state": "closed",
    "created_at": "2025-06-16T07:13:33Z",
    "updated_at": "2025-06-17T09:07:30Z",
    "labels": [
      "bug",
      ".NET",
      "java",
      "triage"
    ],
    "body": "**Describe the bug**  \nCalls to Azure OpenAI using a Provisioned Throughput Unit (PTU) for the `gpt-4o-mini` model intermittently hang for up to 10+ seconds before failing with a timeout. These are low-token requests (<100 total tokens) and do not consistently reproduce, but the latency spikes are severe enough to trigger client-side timeouts and significantly degrade reliability.\n\nThe issue persists despite:\n- Using `SocketsHttpHandler` with `PooledConnectionLifetime = 1 minute`\n- Applying Polly retry and timeout strategies\n- Rebuilding `SemanticKernel` and `HttpClient` per request\n- Using dedicated PTU resources (no quota errors)\n\n---\n\n**To Reproduce**  \nSteps to reproduce the behavior:\n1. Deploy Azure OpenAI with a provisioned GPT-4o Mini deployment (`gpt-4o-mini`)\n2. Use Semantic Kernel to call `GetChatMessageContentAsync(...)` on low-token prompts\n3. Wrap the call in a timeout of 3–5 seconds\n4. Observe intermittent timeouts or long-running requests (>10s)\n5. Logs show no content returned and cancelled sockets (`OperationCanceledException`, sometimes `SocketException`)\n\n---\n\n**Expected behavior**  \nConsistently low-latency completions from a provisioned GPT-4o Mini deployment.\n\n---\n\n**Platform**  \n- Language: C#  \n- Source: NuGet package `Microsoft.SemanticKernel` latest version\n- AI model: Azure OpenAI PTU — `gpt-4o-mini`  \n- IDE: Rider  \n- OS: Windows 11 / Linux (reproduced on both)  \n\n--- \n\n**Snippets**\n\n```\n    static IAsyncPolicy<HttpResponseMessage> GetRetryPolicy() =>\n        HttpPolicyExtensions\n            .HandleTransientHttpError()\n            .Or<TimeoutRejectedException>()\n            .WaitAndRetryAsync(3, _ => TimeSpan.Zero);\n\n    static IAsyncPolicy<HttpResponseMessage> GetTimeoutPolicy() =>\n        Policy.TimeoutAsync<HttpResponseMessage>(TimeSpan.FromSeconds(3));\n\n    public static IServiceCollection AddApplicationServices(this IServiceCollection services)\n    {\n        services.AddHttpClient();\n\n        services.AddSingleton<OpenAiRateLimitHandler>();\n        services.AddHttpClient(nameof(OpenAiClient))\n            .ConfigurePrimaryHttpMessageHandler(() => new SocketsHttpHandler\n            {\n                PooledConnectionLifetime = TimeSpan.FromMinutes(1),\n                ConnectTimeout = TimeSpan.FromSeconds(2),\n                AutomaticDecompression = DecompressionMethods.GZip | DecompressionMethods.Deflate,\n                MaxConnectionsPerServer = 50\n            })\n            .AddPolicyHandler(GetRetryPolicy())\n            .AddPolicyHandler(GetTimeoutPolicy())\n            .AddHttpMessageHandler<OpenAiRateLimitHandler>();\n```\n\n```\npublic sealed class OpenAiClient : IOpenAiClient\n{\n    private readonly Kernel _kernel;\n    private readonly ILogger<OpenAiClient> _logger;\n    private readonly List<IPlugin> _plugins;\n\n    private static readonly JsonSerializerOptions JsonOpts = new()\n    {\n        PropertyNameCaseInsensitive = true,\n        Encoder = System.Text.Encodings.Web.JavaScriptEncoder.UnsafeRelaxedJsonEscaping\n    };\n    \n    private static OpenAIPromptExecutionSettings Exec(string name, BinaryData schema) => new()\n    {\n        Temperature = 0f,\n        ResponseFormat = ChatResponseFormat.CreateJsonSchemaFormat(name, schema, jsonSchemaIsStrict: false),\n        FunctionChoiceBehavior = FunctionChoiceBehavior.None([]),\n    };\n\n    private static readonly OpenAIPromptExecutionSettings NonJsonExec = new()\n    {\n        Temperature = 0f,\n        FunctionChoiceBehavior = FunctionChoiceBehavior.None([]),\n    };\n    \n    private static readonly OpenAIPromptExecutionSettings ManualExec = new()\n    {\n        Temperature = 0f,\n        ResponseFormat = ChatResponseFormat.CreateJsonSchemaFormat(\"Tool\", BinaryData.FromString(UnhydratedToolCall.JsonSchema), jsonSchemaIsStrict: false),\n        FunctionChoiceBehavior = FunctionChoiceBehavior.None([]),\n    };\n\n    public OpenAiClient(IEnumerable<IPlugin> plugins,\n        IOptions<OpenAISettings> cfg,\n        ILogger<OpenAiClient> logger,\n        IHttpClientFactory httpFactory)\n    {\n        _logger = logger;\n        _kernel = BuildKernel(cfg.Value, httpFactory.CreateClient(nameof(OpenAiClient)));\n        _plugins = plugins.ToList();\n    }\n\n    public async Task<OpenAiResponse<string>> Generate(ChatHistory hist, CancellationToken ct)\n    {\n        var service = _kernel.GetRequiredService<IChatCompletionService>();\n        var response = await service.GetChatMessageContentAsync(hist, NonJsonExec, _kernel, ct);\n        var tokensUsed = GetTokensUsed(response);\n        var content = response.Content;\n        _logger.LogInformation(\"Chat usage: {Tokens}\", tokensUsed);\n\n        return new OpenAiResponse<string>(content, tokensUsed, []);\n    }\n\n    public async Task<OpenAiResponse<TOut>> Generate<TOut>(string convoId, ChatHistory hist, BinaryData schema, PluginInfo pluginInfo, CancellationToken ct)\n    {\n        // -----------------------------------------------------------------\n        // ①  Prepare kernel and register only requested plugins\n        // -----------------------------------------------------------------\n        var kernel = _kernel.Clone();\n        kernel.Data[\"conversationId\"] = convoId;\n\n        var plugins = pluginInfo.Plugins.Select(x => _plugins.FirstOrDefault(p => p.Name == x))\n            .Where(x => x is not null)\n            .Cast<IPlugin>()\n            .ToList();\n\n        var pluginKernel = kernel.Clone();\n        foreach (var p in plugins)\n        {\n            pluginKernel.Plugins.AddFromObject(p, p.GetType().Name);\n        }\n\n        // -----------------------------------------------------------------\n        // ②  ROUTING PASS  (autoInvoke:false)\n        //     Ask the LLM which of those plugins it actually wants to call\n        // -----------------------------------------------------------------\n        var systemPrompt = PluginRouter.CreateSystemPrompt(plugins);\n\n        var routerHist = new ChatHistory();\n        routerHist.AddSystemMessage(systemPrompt);\n        routerHist.AddUserMessage(pluginInfo.Prompt);\n\n        var svc = kernel.GetRequiredService<IChatCompletionService>();\n        var routingMsg = await svc.GetChatMessageContentAsync(routerHist, ManualExec, kernel, ct);\n        var callsToMake = JsonSerializer.Deserialize<UnhydratedToolCall>(routingMsg.Content.ExtractJson(), JsonOpts)?.Hydrated.ToList() ?? [];\n\n        var callContentList = callsToMake.Select(x => new FunctionCallContent(x.FunctionName, x.PluginName, x.Id)).ToList();\n        var callResultList = new List<ChatMessageContent>();\n         \n        foreach (var call in callsToMake)\n        {\n            var matchingFn = pluginKernel.Plugins.TryGetFunction(call.PluginName, call.FunctionName, out var fn) ? fn : null;\n            if (matchingFn is null)\n            {\n                _logger.LogWarning(\"No matching function found for call: {Call}\", call);\n                callContentList.RemoveAll(x => x.Id == call.Id);\n                continue;\n            }\n\n            FunctionResult result;\n            try \n            {\n                result = await matchingFn.InvokeAsync(kernel, new KernelArguments(call.Arguments), ct);\n            }\n            catch (Exception ex)\n            {\n                result = new FunctionResult(matchingFn, value: $\"Function call failed: {ex.Message}\");\n            }\n\n            var resultContent = new FunctionResultContent(call.FunctionName, call.PluginName, call.Id, result.GetValue<object?>());\n            callResultList.Add(new ChatMessageContent(AuthorRole.Tool, [resultContent])\n            {\n                Content = JsonSerializer.Serialize(result.GetValue<object>(), JsonOpts),\n            });\n        }\n\n        if (callsToMake.Count != 0)\n        {\n            var callContent = new ChatMessageContent\n            {\n                Items = [..callContentList],\n                Metadata = new Dictionary<string, object?>\n                {\n                    { \"tool_calls\", callContentList },\n                },\n                Role = AuthorRole.Assistant,\n                Content = string.Empty\n            };\n             \n            hist.Add(callContent);\n            hist.AddRange(callResultList);\n        }\n         \n        var routerTokensUsed = GetTokensUsed(routingMsg);\n        _logger.LogInformation(\"Plugin usage: {Usage}\", routerTokensUsed);\n         \n        // -----------------------------------------------------------------\n        // ④  MAIN PASS  (standard Exec settings)\n        // -----------------------------------------------------------------\n        var response = await svc.GetChatMessageContentAsync(hist, Exec(typeof(TOut).Name, schema), kernel, ct);\n        var json = response.Content.ExtractJson();\n        var tokensUsed = GetTokensUsed(response);\n        var toolCalls = hist.Where(x => x.Role == AuthorRole.Tool).Select(x => x.Content ?? string.Empty).Where(x => !string.IsNullOrWhiteSpace(x)).ToArray();\n        _logger.LogInformation(\"Chat usage: {Usage}\", tokensUsed);\n \n        return string.IsNullOrWhiteSpace(json) \n            ? new OpenAiResponse<TOut>(default, tokensUsed, [])\n            : new OpenAiResponse<TOut>(JsonSerializer.Deserialize<TOut>(json, JsonOpts), tokensUsed, toolCalls);\n    }\n\n    private static Kernel BuildKernel(OpenAISettings cfg, HttpClient http)\n    {\n        var b = Kernel.CreateBuilder();\n        b.Services.AddLogging(l => l.SetMinimumLevel(LogLevel.Information));\n\n        if (string.IsNullOrWhiteSpace(cfg.Key))\n        {\n            b.AddAzureOpenAIChatCompletion(cfg.ChatDeploymentId, cfg.Endpoint, new DefaultAzureCredential(), httpClient: http);\n        }\n        else\n        {\n            b.AddAzureOpenAIChatCompletion(cfg.ChatDeploymentId, cfg.Endpoint, cfg.Key, httpClient: http);\n        }\n\n        return b.Build();\n    }\n\n    private static int GetTokensUsed(ChatMessageContent content)\n    {\n        if (content.Metadata is not { } meta)\n        {\n            return 0;\n        }\n\n        if (meta.TryGetValue(\"Usage\", out var usage) && usage is ChatTokenUsage tokenUsage)\n        {\n            return tokenUsage.TotalTokenCount;\n        }\n\n        return 0;\n    }\n}\n```",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "@dominic-codespoti Can you create an issue for the Azure OpenAI service team\n\n- https://azure.microsoft.com/en-us/support/create-ticket/?msockid=3d14b60cac5765162868a328adbd6451\n- https://github.com/Azure/azure-sdk-for-net/issues\n"
      },
      {
        "user": "dominic-codespoti",
        "body": "Thanks for confirming it's not a Semantic Kernel issue. Thought it might be something strange internally, wanted to just check it off :) Anecdotally saw the issue far more when the \"store\" field was true on the Azure OpenAI settings object, but makes sense for it to be on their end."
      }
    ]
  },
  {
    "issue_number": 12471,
    "title": "Python: New Feature: Support Multiple Concurrent Sessions in Realtime Services [Python]",
    "author": "mateoperezrivera",
    "state": "open",
    "created_at": "2025-06-13T00:09:24Z",
    "updated_at": "2025-06-17T09:01:09Z",
    "labels": [
      "python",
      "ai connector"
    ],
    "body": "---\nname: Feature request\nabout: Suggest an idea for this project\n\n\n---\n\n## Feature Request: Support Multiple Concurrent Sessions in Realtime Services\n\n### Description\n\nCurrently, the Azure Realtime services (`AzureRealtimeWebsocket` and `AzureRealtimeWebRTC`) in Semantic Kernel are designed with a 1:1 relationship between service instance and session. This makes it challenging to build scalable backend applications that need to serve multiple concurrent users.\n\n### Current Behavior\n\n- Each `AzureRealtimeWebsocket` or `AzureRealtimeWebRTC` instance can only maintain one active session at a time\n- The `create_session()` method modifies the service instance's internal state directly\n- To serve multiple users, we need to create separate service instances for each user\n- The session state (connection, settings, etc.) is tightly coupled to the service instance\n\n### Proposed Solution\n\nRefactor the Realtime services to support multiple concurrent sessions by separating session management from the service instance:\n#Example\n```python\nclass AzureRealtimeWebsocket:\n    async def create_session(\n        self,\n        chat_history: ChatHistory | None = None,\n        settings: PromptExecutionSettings | None = None,\n        **kwargs: Any\n    ) -> RealtimeSession:\n        \"\"\"Create and return a new session instance\"\"\"\n        session = RealtimeSession(\n            service=self,\n            chat_history=chat_history,\n            settings=settings,\n            **kwargs\n        )\n        await session.connect()\n        return session\n\nclass RealtimeSession:\n    \"\"\"Represents an individual realtime session\"\"\"\n    async def send(self, event: RealtimeEvents) -> None: ...\n    async def receive(self) -> AsyncGenerator[RealtimeEvents, None]: ...\n    async def update_session(self, **kwargs) -> None: ...\n    async def close(self) -> None: ...\n```\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n",
    "comments": []
  },
  {
    "issue_number": 12485,
    "title": ".Net: Bug: Unexpected Accumulation of Previous Assistant Messages in Final Response Using InvokeStreamingAsync",
    "author": "nels-ai-source",
    "state": "open",
    "created_at": "2025-06-16T08:18:20Z",
    "updated_at": "2025-06-17T08:55:36Z",
    "labels": [
      "bug",
      ".NET",
      "agents"
    ],
    "body": "Describe the bug\nWhen using the InvokeStreamingAsync method of ChatCompletionAgent, and receiving multi-turn conversation responses in the OnIntermediateMessage, it was found that the final assistant message included previous assistant messages. It appears that the last message accumulates all previous assistant messages. I am not sure if this is a bug, but it has caused confusion during usage.\n\nTo Reproduce\nSteps to reproduce the behavior:\n\nDefine a ChatAgent with provided tools.\n\n```c#\nChatCompletionAgent agent = new()\n{\n    Instructions = processRequest.AgentRequest.Instructions,\n    Kernel = processRequest.Kernel,\n    HistoryReducer = new ChatHistoryTruncationReducer(6, 4),\n    Arguments = new KernelArguments(new OpenAIPromptExecutionSettings\n    {\n        Temperature = 0,\n        FunctionChoiceBehavior = FunctionChoiceBehavior.Auto(functions: processRequest.AgentRequest.Functions, autoInvoke: processRequest.AgentRequest.Functions?.Count > 0, options: new() { RetainArgumentTypes = true })\n    })\n};\n```\nReceive messages in OnIntermediateMessage and notice overlapping data.\nExpected behavior\nEach message should be independent and non-repetitive.\n\nPlatform\n\nLanguage: C#\nSource: 1.56.0\nIDE: Visual Studio\nOS: Windows",
    "comments": []
  },
  {
    "issue_number": 12488,
    "title": ".Net: New Feature: Remote Agent and Agent Orchestration (A2A)",
    "author": "ultrasimulation",
    "state": "open",
    "created_at": "2025-06-16T11:13:12Z",
    "updated_at": "2025-06-17T08:54:01Z",
    "labels": [
      ".NET",
      "agents"
    ],
    "body": "---\nname: Feature request\nabout: Suggest an idea for this project\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n\nHi, the growing merket interest in the A2A protocol makes it the standard candidate for exposing and consuming agents. \n\nIn my opinion, different needs arise to work with it in Semantic Kernel:\n1) The need to expose an Agent built with SK (e.g. Chat Completion Agent) with the A2A protocol\n2) The need to reference a remote Agent exposed with the A2A protocol\n3) The possibility of using a remote A2A Agent in a multi-agent orchestration\n\nDo you already have this in your roadmap?\n",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Hi @ultrasimulation, thanks for filing. Yes we have this on our roadmap. We're working on SK abstractions for an A2AClient / A2AServer for both Python and .NET. Now that there's an A2A SDK (both for Python and .NET), it makes it easier to build out these abstractions. "
      },
      {
        "user": "ultrasimulation",
        "body": "That is very good. I checked around the project and I see this activity:\nhttps://github.com/microsoft/semantic-kernel/pull/12050\n\nIs it the planned implementation? Any chance to have it shipped with SK soon?"
      },
      {
        "user": "markwallace-microsoft",
        "body": "@ultrasimulation Yes, this will be release as an experimental feature within the next couple of weeks."
      }
    ]
  },
  {
    "issue_number": 12491,
    "title": ".Net: New Feature: Ollama + Support of advanced properties on generate endpoint",
    "author": "kbeaugrand",
    "state": "open",
    "created_at": "2025-06-16T14:26:08Z",
    "updated_at": "2025-06-17T08:53:29Z",
    "labels": [
      ".NET",
      "ai connector"
    ],
    "body": "---\nname: Feature request\nabout: Ollama + Support of advanced properties on generate endpoint\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\nI found some issues when using some ollama models (e.g qwen3). Structured output are not rendered. \nI didn't really found the underlying reason but I saw that on activating the ``raw`` parameter while generating this works.\n\nBut I saw that those properties are not available through the SK or even set.\n\nDocumentation available at: [https://github.com/ollama/ollama/blob/main/docs/api.md#parameters](https://github.com/ollama/ollama/blob/main/docs/api.md#parameters)\n\nPlease give us a way to adapt that depending on our use cases.",
    "comments": []
  },
  {
    "issue_number": 12451,
    "title": ".Net: Bug: Microsoft.SemanticKernel.Connectors.SqlServer Method not found: 'Void Microsoft.Extensions.VectorData.ProviderServices.PropertyModel.SetValue(System.Object, !!0)'.",
    "author": "idlemind",
    "state": "closed",
    "created_at": "2025-06-11T12:51:43Z",
    "updated_at": "2025-06-17T08:41:29Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nException Method not found: 'Void Microsoft.Extensions.VectorData.ProviderServices.PropertyModel.SetValue(System.Object, !!0)'. when executing await foreach (TextSearchResult result in textResults.Results)\n\nMessage: \nMethod not found: 'Void Microsoft.Extensions.VectorData.ProviderServices.PropertyModel.SetValue(System.Object, !!0)'.\n\nSource: \nMicrosoft.SemanticKernel.Connectors.SqlServer\n\nStak Trace:\n   at Microsoft.SemanticKernel.Connectors.SqlServer.SqlServerMapper`1.<MapFromStorageToDataModel>g__PopulateValue|2_0(SqlDataReader reader, PropertyModel property, Object record)\n   at Microsoft.SemanticKernel.Connectors.SqlServer.SqlServerMapper`1.MapFromStorageToDataModel(SqlDataReader reader, Boolean includeVectors)\n   at Microsoft.SemanticKernel.Connectors.SqlServer.SqlServerCollection`2.<ReadVectorSearchResultsAsync>d__23.MoveNext()\n   at Microsoft.SemanticKernel.Connectors.SqlServer.SqlServerCollection`2.<ReadVectorSearchResultsAsync>d__23.System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult(Int16 token)\n   at Microsoft.SemanticKernel.Connectors.SqlServer.SqlServerCollection`2.<SearchAsync>d__21`1.MoveNext()\n   at Microsoft.SemanticKernel.Connectors.SqlServer.SqlServerCollection`2.<SearchAsync>d__21`1.MoveNext()\n   at Microsoft.SemanticKernel.Connectors.SqlServer.SqlServerCollection`2.<SearchAsync>d__21`1.System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult(Int16 token)\n   at Microsoft.SemanticKernel.Data.VectorStoreTextSearch`1.<ExecuteVectorSearchAsync>d__16.MoveNext()\n   at Microsoft.SemanticKernel.Data.VectorStoreTextSearch`1.<ExecuteVectorSearchAsync>d__16.MoveNext()\n   at Microsoft.SemanticKernel.Data.VectorStoreTextSearch`1.<ExecuteVectorSearchAsync>d__16.System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult(Int16 token)\n   at Microsoft.SemanticKernel.Data.VectorStoreTextSearch`1.<GetResultsAsTextSearchResultAsync>d__18.MoveNext()\n   at Microsoft.SemanticKernel.Data.VectorStoreTextSearch`1.<GetResultsAsTextSearchResultAsync>d__18.MoveNext()\n   at Microsoft.SemanticKernel.Data.VectorStoreTextSearch`1.<GetResultsAsTextSearchResultAsync>d__18.System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult(Int16 token)\n\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Builder kernel with IEmbeddingGenerator AsIEmbeddingGenerator()\n2. Add AddSqlServerCollection\n3. Build the collection\n4. Search the collection using new VectorStoreTextSearch<TextSnippet>(vectorStoreCollection);\n5. KernelSearchResults<TextSearchResult> textResults = await textSearch.GetTextSearchResultsAsync(query, new() { Top = 5, Skip = 0 });\n6. Exception caught here:\n\ntry\n{\n\tawait foreach (TextSearchResult result in textResults.Results)\n\t{\n\t\tmemoryBuilder.AppendLine(result.Value);\n\t}\n}\ncatch (Exception ex)\n{\n\tthrow;\n}\n\n\n**Expected behavior**\nTextSearchResult returns search results for the created collection\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Platform**\n - Language: C#\n - Source: Microsoft.Extensions.VectorData.Abstractions 9.5.0, Microsoft.SemanticKernel 1.56.0, Microsoft.SemanticKernel.Connectors.SqlServer 1.56.0-preview\n - AI model: text-embedding-ada-002\n - IDE: Visual Studio\n - OS: Windows\n\n**Additional context**\nI have created a console app here (https://github.com/idlemind/RAGTest). \n\nThe solution uses AzureOpenAIEmbeddings text-embedding-ada-002\n\n",
    "comments": [
      {
        "user": "rwjdk",
        "body": "I've hit this error as well. In 1.54.0 it works, but 1.55.0 and 1.56.0 fail with this error (using SQL Server as the Vector provider)"
      },
      {
        "user": "roji",
        "body": "You are almost certainly mixing incompatible versions of Microsoft.Extensions.VectorData (the general abstractions library) and Microsoft.SemanticKernel.Connectors.SqlServer (the SQL Server provider). I'd recommend using the latest version of both - these should be compatible; otherwise, check which exact version of ME.VectorData is referenced by your MSC.SqlServer version and use that.\n\nIf you can't get this to work, please post your csproj."
      },
      {
        "user": "rwjdk",
        "body": "@roji: I don't have the Microsoft.Extensions.VectorData nuget referenced (trasative via Microsoft.SemanticKernel.Connectors.SqlServer as 9.5.0)\n\nAdding it as Nuget specifically does not resolve the issue\n\nHere are the nuget of the failing packages:\n\n```\n<ItemGroup>\n\t\t<PackageReference Include=\"Azure.Security.KeyVault.Secrets\" Version=\"4.7.0\" />\n\t\t<PackageReference Include=\"Blazored.LocalStorage\" Version=\"4.5.0\" />\n\t\t<PackageReference Include=\"Coravel\" Version=\"6.0.2\" />\n\t\t<PackageReference Include=\"CronExpressionDescriptor\" Version=\"2.41.0\" />\n\t\t<PackageReference Include=\"JetBrains.Annotations\" Version=\"2024.3.0\" />\n\t\t<PackageReference Include=\"KristofferStrube.Blazor.Confetti\" Version=\"0.2.1\" />\n\t\t<PackageReference Include=\"Markdig\" Version=\"0.41.2\" />\n\t\t<PackageReference Include=\"Markdown.ColorCode\" Version=\"3.0.0\" />\n\t\t<PackageReference Include=\"Microsoft.AspNetCore.DataProtection.EntityFrameworkCore\" Version=\"9.0.5\" />\n\t\t<PackageReference Include=\"Microsoft.AspNetCore.Identity.EntityFrameworkCore\" Version=\"9.0.5\" />\n\t\t<PackageReference Include=\"Microsoft.AspNetCore.Identity.UI\" Version=\"9.0.5\" />\n\t\t<PackageReference Include=\"Microsoft.EntityFrameworkCore.SqlServer\" Version=\"9.0.5\" />\n\t\t<PackageReference Include=\"Microsoft.EntityFrameworkCore.Tools\" Version=\"9.0.5\">\n\t\t\t<PrivateAssets>all</PrivateAssets>\n\t\t\t<IncludeAssets>runtime; build; native; contentfiles; analyzers; buildtransitive</IncludeAssets>\n\t\t</PackageReference>\n\t\t<PackageReference Include=\"Microsoft.Extensions.Logging.Console\" Version=\"9.0.5\" />\n\t\t<PackageReference Include=\"Microsoft.Identity.Web\" Version=\"3.9.3\" />\n\t\t<PackageReference Include=\"Microsoft.Identity.Web.UI\" Version=\"3.9.3\" />\n\t\t<PackageReference Include=\"Microsoft.SemanticKernel\" Version=\"1,56.0\" />\n\t\t<PackageReference Include=\"Microsoft.SemanticKernel.Connectors.SqlServer\" Version=\"1.56.0-preview\" />\n\t\t<PackageReference Include=\"Microsoft.SemanticKernel.Agents.Core\" Version=\"1.56.0\" />\n\t\t<PackageReference Include=\"NCrontab.Signed\" Version=\"3.3.3\" />\n\t\t<PackageReference Include=\"Octokit.Webhooks.AspNetCore\" Version=\"2.4.1\" />\n\t\t<PackageReference Include=\"OpenTelemetry.Api\" Version=\"1.12.0\" />\n\t\t<PackageReference Include=\"Radzen.Blazor\" Version=\"7.0.8\" />\n\t\t<PackageReference Include=\"Relewise.Internals.SecretKeys\" Version=\"1.12.0\" />\n\t\t<PackageReference Include=\"MudBlazor\" Version=\"8.6.0\" />\n\t\t<PackageReference Include=\"Octokit\" Version=\"14.0.0\" />\n\t\t<PackageReference Include=\"ReverseMarkdown\" Version=\"4.7.0\" />\n\t\t<PackageReference Include=\"ScottPlot\" Version=\"5.0.55\" />\n\t\t<PackageReference Include=\"SendGrid\" Version=\"9.29.3\" />\n\t\t<PackageReference Include=\"SkiaSharp\" Version=\"3.119.0\" />\n\t\t<PackageReference Include=\"Slack.NetStandard\" Version=\"9.9.0\" />\n\t\t<PackageReference Include=\"TrelloDotNet\" Version=\"2.0.5\" />\n\t\t<PackageReference Include=\"CsvHelper\" Version=\"33.1.0\" />\n\t\t<PackageReference Include=\"WorkTimeCalculator\" Version=\"1.0.5\" />\n\t</ItemGroup> ```\n\n"
      }
    ]
  },
  {
    "issue_number": 12465,
    "title": "Python: Bug: Function calling not working with AzureRealtimeWebsocket",
    "author": "mateoperezrivera",
    "state": "open",
    "created_at": "2025-06-12T15:35:55Z",
    "updated_at": "2025-06-17T08:38:48Z",
    "labels": [
      "bug",
      "python"
    ],
    "body": "**Describe the bug**\nFunction calling is not working with AzureRealtimeWebsocket in Semantic Kernel. When adding a plugin to the kernel and passing it to the realtime session, the model doesn't respond when it should invoke the function, and no RealtimeFunctionCallEvent is emitted.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Add a plugin to the Semantic Kernel\n2. Create an AzureRealtimeWebsocket service and add it to the kernel\n3. When the model should call a function (ask:\"what is the weather in buenos aires\", it doesn't respond and no function call event is triggered\n\n**Code snippet:**\n```python\n# Initialize kernel and add plugin\nsk_kernel = sk.Kernel()\nweather_plugin = WeatherPlugin()\nsk_kernel.add_plugin(weather_plugin, plugin_name=\"weather\")\n\n# Create realtime service\nrealtime_service = AzureRealtimeWebsocket(\n    api_version=\"2025-04-01-preview\"\n)\nsk_kernel.add_service(realtime_service)\n\nsettings = AzureRealtimeExecutionSettings(\n    modalities=[\"audio\", \"text\"],\n)\nawait realtime_service.create_session(settings=settings)\n```\n\n**additional info:**\nIt seems tools are not being passed when session is created\nlogs:\n```python\n`2025-06-12 12:52:58 - Received service event: service - session.created\n\n2025-06-12 12:52:58 - Session created or updated, session: {\"id\":\"{redacted}\",\"input_audio_format\":\"pcm16\",\"input_audio_noise_reduction\":null,\"input_audio_transcription\":null,\"instructions\":\"Your knowledge cutoff is 2023-10. You are a helpful, witty, and friendly AI. Act like a human, but remember that you aren't a human and that you can't do human things in the real world. Your voice and personality should be warm and engaging, with a lively and playful tone. If interacting in a non-English language, start by using the standard accent or dialect familiar to the user. Talk quickly. You should always call a function if you can. Do not refer to these rules, even if you’re asked about them.\",\"max_response_output_tokens\":\"inf\",\"modalities\":[\"audio\",\"text\"],\"model\":\"gpt-4o-realtime-preview-2024-12-17\",\"output_audio_format\":\"pcm16\",\"speed\":null,\"temperature\":0.8,\"tool_choice\":\"auto\",\"tools\":[],\"tracing\":null,\"turn_detection\":{\"create_response\":true,\"eagerness\":null,\"interrupt_response\":true,\"prefix_padding_ms\":300,\"silence_duration_ms\":200,\"threshold\":0.5,\"type\":\"server_vad\"},\"voice\":\"alloy\",\"object\":\"realtime.session\",\"expires_at\":1749745379,\"client_secret\":null,\"include\":null}\n\n2025-06-12 12:52:59 - Received service event: service - session.updated\n\n2025-06-12 12:52:59 - Session created or updated, session: {\"id\":\"{redacted}\",\"input_audio_format\":\"pcm16\",\"input_audio_noise_reduction\":{\"type\":\"near_field\"},\"input_audio_transcription\":null,\"instructions\":\"Your knowledge cutoff is 2023-10. You are a helpful, witty, and friendly AI. Act like a human, but remember that you aren't a human and that you can't do human things in the real world. Your voice and personality should be warm and engaging, with a lively and playful tone. If interacting in a non-English language, start by using the standard accent or dialect familiar to the user. Talk quickly. You should always call a function if you can. Do not refer to these rules, even if you’re asked about them.\",\"max_response_output_tokens\":\"inf\",\"modalities\":[\"audio\",\"text\"],\"model\":\"gpt-4o-realtime-preview\",\"output_audio_format\":\"pcm16\",\"speed\":null,\"temperature\":0.8,\"tool_choice\":\"auto\",\"tools\":[],\"tracing\":null,\"turn_detection\":{\"create_response\":true,\"eagerness\":null,\"interrupt_response\":true,\"prefix_padding_ms\":300,\"silence_duration_ms\":200,\"threshold\":0.5,\"type\":\"server_vad\"},\"voice\":\"alloy\",\"object\":\"realtime.session\",\"expires_at\":1749745379,\"client_secret\":null,\"include\":null}`\n```\n\n**Expected behavior**\nThe function/tool should be properly sent and accepted by the realtime session. When the user asks about weather, the model should emit a RealtimeFunctionCallEvent to invoke the weather function.\n\n**Platform**\n- Language: Python\n- Source: pip semantic-kernel[mcp,realtime]==1.33.0\n- AI model: Azure OpenAI gpt-4o-realtime-preview\n- IDE: VS Code\n- OS: Windows\n",
    "comments": [
      {
        "user": "eavanvalkenburg",
        "body": "You need to pass the kernel to the create_session object, or pass plugins:\n```python\n# Initialize kernel and add plugin\nsk_kernel = sk.Kernel()\nweather_plugin = WeatherPlugin()\nsk_kernel.add_plugin(weather_plugin, plugin_name=\"weather\")\n\n# Create realtime service\nrealtime_service = AzureRealtimeWebsocket(\n    api_version=\"2025-04-01-preview\"\n)\nsk_kernel.add_service(realtime_service)\n\nsettings = AzureRealtimeExecutionSettings(\n    modalities=[\"audio\", \"text\"],\n)\nawait realtime_service.create_session(settings=settings, kernel=sk_kernel)\n```\nor \n```python\n# Initialize kernel and add plugin\nweather_plugin = WeatherPlugin()\n\n# Create realtime service\nrealtime_service = AzureRealtimeWebsocket(\n    api_version=\"2025-04-01-preview\"\n)\nsettings = AzureRealtimeExecutionSettings(\n    modalities=[\"audio\", \"text\"],\n)\nawait realtime_service.create_session(settings=settings, plugins=[WeatherPlugin()])\n```\n\nPlease let me know if that solves it for you!"
      }
    ]
  },
  {
    "issue_number": 12489,
    "title": "New Feature: Provide better documentation for the InProcessRuntime",
    "author": "ultrasimulation",
    "state": "open",
    "created_at": "2025-06-16T11:23:25Z",
    "updated_at": "2025-06-17T08:36:00Z",
    "labels": [
      "agents"
    ],
    "body": "---\nname: Feature request\nabout: Suggest an idea for this project\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\nHi, most sections of the documentation on agent orchestrations focus on the use of InProcessRuntime. However, aside from a few examples, there’s little to no documentation that explains this functionality and its methods in detail. It would be helpful to improve the documentation to provide more comprehensive coverage of this component. \n\nAnother things that could be documented is:\n- Will be available other type of runtime (e.g. for distributed agent)? \n- Can i create custom one? \n- If not, what is the purpose of expose this concept?\n\n",
    "comments": []
  },
  {
    "issue_number": 12492,
    "title": "Python: Bug: AzureAIAgent doesn't register custom functions (plugins) in Azure AI Foundry",
    "author": "karolzak",
    "state": "open",
    "created_at": "2025-06-16T18:15:54Z",
    "updated_at": "2025-06-17T08:30:46Z",
    "labels": [
      "bug",
      "python"
    ],
    "body": "**Describe the bug**\nWhen I try to create a new AzureAIAgent with some custom plugins using AgentRegistry.create_from_file it works correctly when invoked in the local context but when I go to Azure AI Foundry, I see it's created correctly but it's missing the custom plugins I created it with.\nI tried using different combinations of kernel, plugins and tools definition in yaml and nothing seems to solve this.\n\n**To Reproduce**\nMinimum code to reproduce the behavior:\n```python\nimport asyncio\n\nfrom azure.identity.aio import DefaultAzureCredential\n\nfrom semantic_kernel.agents import AgentRegistry, AzureAIAgent, AzureAIAgentSettings\nfrom semantic_kernel.kernel import Kernel\nfrom src.tools.calculator import CalculatorPlugin\n\n\"\"\"\nThe following sample demonstrates how to create an Azure AI agent that can serve user as a calculator\nusing a custom implementation of Semantic Kernel Plugin. The agent is created using a yaml declarative spec.\nThe agent can perform basic arithmetic operations like addition and multiplication.\n\"\"\"\n\n# Simulate a conversation with the agent\nUSER_INPUTS = [\n    \"Hello\",\n    \"I want to add two numbers, 453 and 789.\",\n    \"Now, multiply the result by 3\",\n    \"Thank you\",\n]\n\n\nasync def main() -> None:\n    settings = AzureAIAgentSettings()\n    \n    async with (\n        DefaultAzureCredential() as creds,\n        AzureAIAgent.create_client(credential=creds) as client,\n    ):\n        # 1. Create a Kernel instance\n        # For declarative agents, the kernel is required to resolve the plugin(s)\n        kernel = Kernel()\n        kernel.add_plugin(CalculatorPlugin())\n\n        # 2. Create a Semantic Kernel agent for the Azure AI agent\n        agent: AzureAIAgent = await AgentRegistry.create_from_file(\n            \"src/agents/declarative/calculator.yaml\",\n            kernel=kernel,\n            # plugins=[CalculatorPlugin()],\n            settings=settings,\n            client=client\n        )\n\n        # 3. Create a thread for the agent\n        # If no thread is provided, a new thread will be\n        # created and returned with the initial response\n        thread = None\n\n        try:\n            for user_input in USER_INPUTS:\n                print(f\"# User: {user_input}\")\n                # 4. Invoke the agent for the specified thread for response\n                async for response in agent.invoke(\n                    messages=user_input,\n                    thread=thread,\n                ):\n                    print(f\"# {response.name}: {response}\")\n                    thread = response.thread  \n        finally:\n            print(\"# Conversation ended.\")\n            # 5. Cleanup: Delete the thread and agent\n            # await thread.delete() if thread else None\n            # await client.agents.delete_agent(agent.id)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\nAgent definition in yaml:\n```yaml\ntype: foundry_agent\nname: CalculatorAgent\ndescription: This agent uses the provided tools to perform calculations based on user queries.\ninstructions: Use the provided tools to perform calculations based on user queries.\nmodel:\n  id: ${AzureAI:ChatModelId}\n  options:\n    temperature: 0.4\ntools:\n  - id: CalculatorPlugin.add_numbers\n    type: function\n  - id: CalculatorPlugin.subtract_numbers\n    type: function\n  - id: CalculatorPlugin.multiply_numbers\n    type: function\n  - id: CalculatorPlugin.divide_numbers\n    type: function\n  - id: CalculatorPlugin.calculate_percentage\n    type: function\n  - id: CalculatorPlugin.format_currency\n    type: function\n```\n\n\n**Expected behavior**\nI would expect AzureAIAgent to correctly register agent in Azure AI Foundry along with custom functions/tools.\n\n**Platform**\n - Language: Python\n - Source: pip package version 3.3.0 (also tried earlier 3.2.1 version)\n - AI model: Azure OpenAI: GPT-4o\n - IDE: VS Code\n - OS: Windows with WSL Ubuntu (+dev container)\n",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Hi @karolzak, thanks for filing this issue. We can have functions registered when the agent definition is created; however, even if they are registered with the definition, they must also exist in the kernel. If functions are present in the definition but not available on the kernel, an error will be raised at invocation time.\n\nFrom a use-case perspective, can you help me understand why you need to view the functions as part of the definition on the server? If you use the same YAML file for different local agents, they will share the same function definitions. Are you aiming to register an agent via YAML, then retrieve it later using an agent_id, without the original YAML, and still be able to invoke the same functions? Thank you."
      },
      {
        "user": "karolzak",
        "body": "> Hi [@karolzak](https://github.com/karolzak), thanks for filing this issue. We can have functions registered when the agent definition is created; however, even if they are registered with the definition, they must also exist in the kernel. If functions are present in the definition but not available on the kernel, an error will be raised at invocation time.\n> \n> From a use-case perspective, can you help me understand why you need to view the functions as part of the definition on the server? If you use the same YAML file for different local agents, they will share the same function definitions. Are you aiming to register an agent via YAML, then retrieve it later using an agent_id, without the original YAML, and still be able to invoke the same functions? Thank you.\n\nThanks for the response!\nI guess my expectation was that when I create a certain agent with SK using AzureAIAgent, I expect the same agent configuration to be then present in my Azure AI Foundry. In reality, it's somewhat there but it's missing some functionalities and when I use the playground there, it's not the same agent (missing tools).\n\n> Are you aiming to register an agent via YAML, then retrieve it later using an agent_id, without the original YAML, and still be able to invoke the same functions?\n\nYes, this was my understanding and one of my expectations when I considered using AzureAIAgent. I thought I configure my agent, add functions and anything else I need and register (deploy?) it in Azure AI Foundry so then I can retrieve it from anywhere by the ID.\nIs my understanding incorrect?"
      },
      {
        "user": "moonbox3",
        "body": "Thanks for your response @karolzak.\n\n> I guess my expectation was that when I create a certain agent with SK using AzureAIAgent, I expect the same agent configuration to be then present in my Azure AI Foundry. In reality, it's somewhat there but it's missing some functionalities and when I use the playground there, it's not the same agent (missing tools).\n\nWe can close the gap and make sure if tools are provided to SK, that they are also represented as part of the agent definition that lives on the server. The only thing that I don't understand with that is that it's a JSON schema representation - how would one actually run the function tool in the agent playground in Foundry? It requires native code to execute.\n\n> Yes, this was my understanding and one of my expectations when I considered using AzureAIAgent. I thought I configure my agent, add functions and anything else I need and register (deploy?) it in Azure AI Foundry so then I can retrieve it from anywhere by the ID.\n\nThis is a way to handle it, yes. Because the agent definition lives on the server, it can be pulled down to different machines and run by getting the agent definition instead of creating it from scratch. As I said in the previous response, the functions still need to exist in the kernel, otherwise if they don't, we have no native functions to actually invoke. This is how the SK abstraction works for the AzureAIAgent. Foundry lets you play with an agent in the playground and test it out, sure. But to actually orchestrate the agent, you'll use an SDK like Semantic Kernel.\n"
      }
    ]
  },
  {
    "issue_number": 12364,
    "title": ".Net: Redis Connector - Unable to successfully query results",
    "author": "aventus13",
    "state": "closed",
    "created_at": "2025-06-04T09:50:52Z",
    "updated_at": "2025-06-17T08:25:48Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "I'm trying to get the .Net [Redis Connector](https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/out-of-the-box-connectors/redis-connector?pivots=programming-language-csharp) to work to no avail so far. \n\nFirst, using the same approach that I've been using for in-memory as well as Azure AI Search connectors, I've been saving items to the collection in the following way:\n\n```csharp\nvar collectionId = // some uuid\n// IVectorStore\nvar collection = _vectorStore.GetCollection<string, DocumentEmbeddingRecord>(collectionId);\n\n// Bulk store chunks as vectors\nfor (var i = 0; i < chunks.Count; i++)\n{\n    var record = new DocumentEmbeddingRecord\n    {\n        Key = Guid.NewGuid().ToString(),\n        Text = chunks[i],\n        Description = \"PDF content chunk\",\n        ExternalSourceName = document.Filename,\n        Embedding = vectors[i]\n    };\n\n    vectorRecords.Add(record);\n}\n\nawait collection.UpsertAsync(vectorRecords);\n```\n\nHere's my `DocumentEmbeddingRecord`:\n```csharp\npublic class DocumentEmbeddingRecord\n{\n    [VectorStoreKey]\n    public required string Key { get; init; }\n    \n    [VectorStoreData]\n    public required string Text { get; init; }\n    \n    [VectorStoreData]\n    public required string Description { get; init; }\n    \n    [VectorStoreData]\n    public required string ExternalSourceName { get; init; }\n    \n    [VectorStoreVector(Dimensions: 1536, DistanceFunction = DistanceFunction.CosineSimilarity)]\n    public required ReadOnlyMemory<float> Embedding { get; init; }\n}\n```\n\nHowever, when doing so, I was getting the following error from Redis:\n`StackExchange.Redis.RedisServerException: CROSSSLOT Keys in request don't hash to the same slot (context='', command='json.mset', original-slot='14999', wrong-slot='10973'...`\n\nAfter some digging, I updated the Key to force the item to the same slot:\n```csharp\nKey = $\"{{{collectionId}}}:{Guid.NewGuid().ToString()}\"\n```\n\nThis allows to successfully save data to Redis. However, when querying, no results can be found:\n\n```csharp\nvar collection = _vectorStore.GetCollection<string, DocumentEmbeddingRecord>(collectionId);\nvar searchResults = collection.SearchAsync(queryEmbedding.Vector, top: 3);\nvar results = new List<VectorSearchResult<DocumentEmbeddingRecord>>();\n\nawait foreach (var result in searchResults)\n{\n    results.Add(result);\n}\n\n// results.Count == 0\n```\n\nWhat am I doing wrong? To reiterate, the above approach works for in-memory and AI Search connectors. The generated collection in Redis is structured like this:\n\n![Image](https://github.com/user-attachments/assets/251dc6cb-2e12-40fb-bb77-723ebde65f93)\n\nExample of a stored record:\n\n![Image](https://github.com/user-attachments/assets/f619aa3d-d548-4c31-8b05-df01f7aa1a25)\n\n~I also tried generating data as hash set. While such generated collections can be found, the query still fails, and the embedding vector doesn't look right:~ This was caused by invalid dimensions used as Redis connector seems to be strict in that regard. However, the issue with collection names still remains using JSON data.\n\n![Image](https://github.com/user-attachments/assets/f54b5707-414a-45aa-b61f-38bcbf10d54e)",
    "comments": [
      {
        "user": "westey-m",
        "body": "Hi @aventus13,\n\nWhile I am not entirely sure, I think the issue may be related to index prefixes.  Redis doesn't have the concept of a collection similar to most other vector dbs, but it does have indexes.  Documents are only indexed by an index if the start of the document key matches a prefix setting on the index.\n\nThe SK Redis implementation automatically uses a prefix of `$\"{this.Name}:\"` where name is the collection name.  In your example therefore the value of the collection name would be `collectionId`.\nE.g. `01e0afed-8453-468e-abeb-6d75ca8e09f8:`\nNote the colon at the end of the Guid.\n\nIt's therefore important to double check that the prefix configured for the index that you created, is matching the records in the database.\n\nE.g. if the record key is prefixed with `{01e0afed-8453-468e-abeb-6d75ca8e09f8}:`, this will mean that the record is not indexed by the index with the prefix of `01e0afed-8453-468e-abeb-6d75ca8e09f8:` since the one is surrounded by curly braces and the other is not.\n\nThe SK Redis implementation will prefix all record keys on Upsert or Get with the index prefix, unless this is switched off via the `PrefixCollectionNameToKeyNames` option passed to the collection constructor.\n\n```csharp\nvar collection = new RedisJsonCollection<string, DocumentEmbeddingRecord>(\n    ConnectionMultiplexer.Connect(\"localhost:6379\").GetDatabase(),\n    collectionId,\n    new() { PrefixCollectionNameToKeyNames = false });\n```\n\nFYI, it is possible to check the prefix configured for an existing index in the redis-stack browser.\n\n![Image](https://github.com/user-attachments/assets/192aaa3d-f5fb-48d8-8525-bd0cebc773e3)\n\n"
      },
      {
        "user": "westey-m",
        "body": "Closing due to lack of information to troubleshoot further."
      }
    ]
  },
  {
    "issue_number": 12503,
    "title": "Python: support Azure OpenAI Next generation API",
    "author": "moonbox3",
    "state": "open",
    "created_at": "2025-06-17T08:17:21Z",
    "updated_at": "2025-06-17T08:17:32Z",
    "labels": [
      "python",
      "ai connector",
      "triage"
    ],
    "body": "According to the following API lifecycle doc, there will be support for preview features without having to roll the api-version each month:\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n    base_url=\"https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/\",\n    default_query={\"api-version\": \"preview\"}, \n)\n\nresponse = client.responses.create(   \n  model=\"gpt-4.1-nano\", # Replace with your model deployment name \n  input=\"This is a test.\",\n)\n\nprint(response.model_dump_json(indent=2)) \n```\n\nhttps://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-lifecycle?tabs=key#api-evolution",
    "comments": []
  },
  {
    "issue_number": 12017,
    "title": ".Net: Bug: .Net: Handlebars Template execution_settings Ignored When Passing PromptExecutionSettings with ServiceId",
    "author": "sami-carret-vis",
    "state": "open",
    "created_at": "2025-05-12T13:22:09Z",
    "updated_at": "2025-06-17T08:00:30Z",
    "labels": [
      "bug",
      ".NET",
      "Build"
    ],
    "body": "**Context**\n\nI have an application using the Microsoft Semantic Kernel. The user is able able to create their own Semantic Kernel Functions using the handlebars format, defining the Temperature and Max Tokens of the output of their function themselves as part of the handlebars template. Additionally, the user is able to choose which LLM will be used to invoke the function through a field in the user interface, the id of the LLM to be used is passed to the KernelFunction through a PromptExecutionSettings object. The problem is that the Semantic Kernel Function will only take the Execution Settings from the handlebars template into account when the PromptExecutionSettings object is set to null.\n\n**Describe the bug**\n\nWhen creating a kernel function from a Handlebars prompt using kernel.CreateFunctionFromPrompt, if a PromptExecutionSettings object is provided in the executionSettings parameter, any execution settings defined within the Handlebars template's execution_settings block (e.g., max_tokens, temperature) are ignored. This happens even if the provided PromptExecutionSettings object only contains a ServiceId and does not explicitly override settings like max_tokens or temperature.\n\nIt appears that providing any PromptExecutionSettings object during function creation causes the entire execution_settings block from the template to be disregarded, rather than being merged with or selectively overridden by the provided PromptExecutionSettings.\n\n**To Reproduce**\n\nSteps to reproduce the behavior:\n\n-   Use Microsoft.SemanticKernel version 1.46.0 and ensure Microsoft.SemanticKernel.PromptTemplates.Handlebars is referenced.\n-   Configure a Kernel instance with at least one chat completion service (e.g., identified by ServiceId = \"gpt-4o-mini\").\n-   Use the following code to define and create a function: \n```csharp\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.PromptTemplates.Handlebars;\n\n// --- Setup ---\nvar kernel = new Kernel(); // Replace with your actual kernel setup\n// kernel.AddChatCompletionService(...) // Add your service configuration\n// --- Define Prompt with Execution Settings ---\n\nvar functionName = \"GenerateStory\";\nvar promptWithSettings = \"\"\"\n\tname: GenerateStory\n\tdescription: This function generates a long funny story.\n\ttemplate:\n\tGenerate a long funny story.\n\ttemplate_format: handlebars\n\texecution_settings:\n\t\tdefault:\n\t\t\tmax_tokens: 10\n\t\t\ttemperature: 1.0\n\"\"\";\n\n// --- Define Execution Settings containing ONLY the ServiceId ---\nvar dynamicExecutionSettings = new PromptExecutionSettings() { ServiceId = \"gpt-4o-mini\" }; // Replace \"gpt-4o\" if needed\n\n// --- Create Function ---\nvar storyFunction = kernel.CreateFunctionFromPrompt(\n\t\t\t\tpromptWithSettings,\n\t\t\t\tfunctionName: functionName,\n\t\t\t\texecutionSettings: dynamicExecutionSettings, // Pass settings object with only ServiceId\n\t\t\t\ttemplateFormat: \"handlebars\",\n\t\t\t\tpromptTemplateFactory: new HandlebarsPromptTemplateFactory()\n\t\t\t\t);\n// --- Invoke Function ---\nConsole.WriteLine($\"Invoking function '{functionName}' with ServiceId '{dynamicExecutionSettings.ServiceId}'...\");\nConsole.WriteLine($\"Expected max_tokens: 10, temperature: 1.0 (from template)\");\ntry\n{\n\t// Use a simple input\n\tvar result = await kernel.InvokeAsync(storyFunction);\n\tConsole.WriteLine(\"\\n--- Function Result ---\");\n\tConsole.WriteLine(result);\n\tConsole.WriteLine(\"----------------------\");\n\t// Observe the length and style of the output here.\n\t// If max_tokens=10 was ignored, the output will likely be much longer.\n\t// If temperature=1.0 was ignored, the output might be less creative than expected.\n}\ncatch (Exception ex)\n{\n\tConsole.WriteLine($\"Error invoking function: {ex}\");\n}\n```\n-   Invoke the function and observe the output.\n\n**Expected behavior**\n\nThe function execution should use max_tokens: 10 and temperature: 1.0 as defined in the Handlebars template's execution_settings block because these were not overridden in the dynamicExecutionSettings object passed to CreateFunctionFromPrompt.\n\nThe expected mechanism is that settings provided during function creation/invocation should selectively override or merge with the template defaults. Providing only a ServiceId should use that service but retain other default parameters (max_tokens, temperature, etc.) from the template.\n\n**Actual behaviour**\n\nThe max_tokens and temperature values defined in the template's execution_settings block are ignored when dynamicExecutionSettings (containing only ServiceId) is passed. The execution appears to use the AI service's default values for these parameters instead (e.g., the output is significantly longer than 10 tokens, suggesting max_tokens was ignored).\n\n-   Observation: If the executionSettings parameter is omitted entirely during the CreateFunctionFromPrompt call (i.e., executionSettings: null), the settings defined within the Handlebars template are correctly applied.\n\n**Platform**\n\n-   OS: Windows 11\n-   Language: C# (.NET)\n-   Source: Microsoft.SemanticKernel v 1.46.0\n-   AI model: Depends on the ServiceId set in the promptExecutionSettings\n\n**Additional context**\n\nThe primary goal is to define default execution parameters (like temperature, max_tokens, top_p) within the prompt template itself for maintainability and consistency across different uses of the prompt/different semantic kernel functions. However, we need the flexibility to dynamically select the specific AI model/service (via ServiceId) at runtime based on user choice or other logic.\n\nThis bug currently prevents this workflow, forcing us to either:\n\n-   Omit the executionSettings parameter during creation (losing the ability to set ServiceId dynamically)\n-   Pass all execution settings dynamically (losing the benefit of template-defined settings or forcing workarounds such as parsing the prompt).\n\nThis behavior most likely originates from this function from the ```KernelFunctionFromPrompt.cs``` code: \n```csharp\n/// <summary>\n/// Creates a <see  cref=\"KernelFunction\"/> instance for a prompt specified via a prompt template.\n/// </summary>\n/// <param  name=\"promptTemplate\">Prompt template for the function, defined using the <see  cref=\"PromptTemplateConfig.SemanticKernelTemplateFormat\"/> template format.</param>\n/// <param  name=\"executionSettings\">Default execution settings to use when invoking this prompt function.</param>\n/// <param  name=\"functionName\">A name for the given function. The name can be referenced in templates and used by the pipeline planner.</param>\n/// <param  name=\"description\">The description to use for the function.</param>\n/// <param  name=\"templateFormat\">Optional format of the template. Must be provided if a prompt template factory is provided</param>\n/// <param  name=\"promptTemplateFactory\">Optional: Prompt template factory</param>\n/// <param  name=\"loggerFactory\">Logger factory</param>\n/// <returns>A function ready to use</returns>\n[RequiresUnreferencedCode(\"Uses reflection to handle various aspects of the function creation and invocation, making it incompatible with AOT scenarios.\")]\n[RequiresDynamicCode(\"Uses reflection to handle various aspects of the function creation and invocation, making it incompatible with AOT scenarios.\")]\npublic  static  KernelFunction  Create(\n\t\t\tstring promptTemplate,\n\t\t\tDictionary<string, PromptExecutionSettings>? executionSettings = null,\n\t\t\tstring? functionName = null,\n\t\t\tstring? description = null,\n\t\t\tstring? templateFormat = null,\n\t\t\tIPromptTemplateFactory? promptTemplateFactory = null,\n\t\t\tILoggerFactory? loggerFactory = null)\n{\n\tVerify.NotNullOrWhiteSpace(promptTemplate);\n\n\tif (promptTemplateFactory is not null)\n\t{\n\t\tif (string.IsNullOrWhiteSpace(templateFormat))\n\t\t{\n\t\t\tthrow new ArgumentException($\"Template format is required when providing a {nameof(promptTemplateFactory)}\", nameof(templateFormat));\n\t\t}\n\t}\n\n\tvar promptConfig = new PromptTemplateConfig\n\t{\n\t\tTemplateFormat = templateFormat ??  PromptTemplateConfig.SemanticKernelTemplateFormat,\n\t\tName = functionName,\n\t\tDescription = description ?? \"Generic function, unknown purpose\",\n\t\tTemplate = promptTemplate\n\t};\n\n\tif (executionSettings is not null)\n\t{\n\t\tpromptConfig.ExecutionSettings = executionSettings;\n\t}\n\n\tvar factory = promptTemplateFactory ?? new KernelPromptTemplateFactory(loggerFactory);\n\t\n\treturn  Create(promptTemplate: factory.Create(promptConfig),\n\t\t\t\t\tpromptConfig: promptConfig,\n\t\t\t\t\tloggerFactory: loggerFactory);\n}\n```",
    "comments": [
      {
        "user": "sami-carret-vis",
        "body": "Hello, I am commenting on this issue again to ask for an update it's status."
      },
      {
        "user": "sami-carret-vis",
        "body": "Hi there,\n\nI'm writing to follow up on issue #12017 concerning the executions settings not being taken into account.\n\nI know you're incredibly busy, but I was hoping to get a quick status update. We're still encountering this issue, and any information on a potential timeline or workaround would be greatly appreciated.\n\nIs there anything I can do to help move this forward? I'd be happy to help test a fix or provide additional diagnostics.\n\nThank you !"
      }
    ]
  },
  {
    "issue_number": 12480,
    "title": ".Net: Microsoft.SemanticKernel.Diagnostics.ModelDiagnostics.StartCompletionActivity: Collection was modified; enumeration operation may not execute.",
    "author": "dimable",
    "state": "open",
    "created_at": "2025-06-15T09:05:07Z",
    "updated_at": "2025-06-17T05:53:53Z",
    "labels": [
      "bug",
      ".NET",
      "ai connector"
    ],
    "body": "> Hi everyone, joining this thread since encountered the same error🙁 using .Net 9, Microsoft.SemanticKernel 1.55.0, OpenTelemetry 1.12.0.\n> here is the stack trace:\n> Collection was modified; enumeration operation may not execute.\n>       System.InvalidOperationException: Collection was modified; enumeration operation may not execute.\n>          at Microsoft.SemanticKernel.Diagnostics.ModelDiagnostics.StartCompletionActivity[TPromptExecutionSettings](Uri endpoint, String modelName, String modelProvider, ChatHistory chatHistory, TPromptExecutionSettings executionSettings)\n>          at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetChatMessageContentsAsync(String targetModel, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)\n>          at Microsoft.SemanticKernel.ChatCompletion.ChatCompletionServiceExtensions.GetChatMessageContentAsync(IChatCompletionService chatCompletionService, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)\n>          at API.AssistantApi.GetUserChatResponse(Guid id, ChatPrompt prompt, Kernel kernel, UserChatHistoryCache chatCache, CancellationToken cancellationToken) in /src/API/AssistantApi.cs:line 47\n>          at Microsoft.AspNetCore.Http.RequestDelegateFactory.ExecuteTaskResult[T](Task`1 task, HttpContext httpContext)\n>          at Microsoft.AspNetCore.Http.RequestDelegateFactory.<>c__DisplayClass101_2.<<HandleRequestBodyAndCompileRequestDelegateForJson>b__2>d.MoveNext()\n>       --- End of stack trace from previous location ---\n>          at Microsoft.AspNetCore.Authorization.AuthorizationMiddleware.Invoke(HttpContext context)\n>          at Microsoft.AspNetCore.Authentication.AuthenticationMiddleware.Invoke(HttpContext context)\n>          at Program.<>c.<<<Main>$>b__0_9>d.MoveNext() in /src/API/Program.cs:line 131\n> \n> The error occurs when I run functional tests, just 2 consecutive requests which trigger this code in web api action:\n> ```\n> var chatHistory = await chatCache.Get(id);\n> chatHistory.AddMessage(AuthorRole.User, prompt.Query);\n> \n> var chat = kernel.GetRequiredService<IChatCompletionService>();\n> var content = await chat.GetChatMessageContentAsync(chatHistory, _promptSettings, kernel, cancellationToken);\n> var messageId = chatHistory.AddMessage(AuthorRole.Assistant, content.Content);\n> chatCache.Update(chatHistory);\n> \n> return Results.Ok(new ChatResponse { Message = content.Content, ChatId = $\"{chatHistory.Id}\", MessageId = messageId });\n> ```\n> When this code is executed by a call from my client app it works fine.  \n\n _Originally posted by @dimable in [#6058](https://github.com/microsoft/semantic-kernel/issues/6058#issuecomment-2973602007)_",
    "comments": []
  },
  {
    "issue_number": 12481,
    "title": ".Net: [MEVD] Support for composite keys",
    "author": "roji",
    "state": "open",
    "created_at": "2025-06-15T12:16:27Z",
    "updated_at": "2025-06-17T05:51:02Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": null,
    "comments": []
  },
  {
    "issue_number": 12493,
    "title": ".Net: MEVD: Add test to ensure that MEVD is released when needed",
    "author": "westey-m",
    "state": "open",
    "created_at": "2025-06-16T19:35:39Z",
    "updated_at": "2025-06-17T05:50:30Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "We need to ensure that MEVD Abstractions are released when updated so that connectors do not reference code that doesn't exist.",
    "comments": []
  },
  {
    "issue_number": 12500,
    "title": "Python: New Feature: gpt-image-1 support",
    "author": "ymuichiro",
    "state": "open",
    "created_at": "2025-06-17T03:53:43Z",
    "updated_at": "2025-06-17T04:10:00Z",
    "labels": [
      "python",
      "triage"
    ],
    "body": "name: Python: gpt-image-1 support\nabout: \n\nAzureTextToImage does not support gpt-image-1. The AzureTextToImage implementation in Semantic Kernel does not support the gpt-image-1 deployment. It currently assumes a DALL·E 3-compatible interface that returns a URL, which gpt-image-1 does not provide.\n\n```python\nBelow is a minimal code example:\n\nasync def image_sample() -> None:\n    kernel = sk.Kernel()\n    service_id: str = \"image\"\n    kernel.add_service(\n        AzureTextToImage(\n            service_id=service_id,\n            deployment_name=\"gpt-image-1\",\n            endpoint=AZURE_OPENAI_IMAGE_ENDPOINT,\n            api_key=AZURE_OPENAI_IMAGE_API_KEY,\n        )\n    )\n\n    service: AzureTextToImage = kernel.get_service(service_id=service_id)\n    settings = service.get_prompt_execution_settings_class()(service_id=service.service_id)\n\n    if isinstance(settings, OpenAITextToImageExecutionSettings):\n        settings.prompt = \"sky\"\n        settings.size = ImageSize(width=1024, height=1024)\n        settings.quality = \"low\"\n\n    if isinstance(service, AzureTextToImage):\n        r = await service.generate_image(settings=settings)\n\n        print(r)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(image_sample())\n```\n\n## Problems Identified\n1. Outdated openai-python versionThe SDK version is outdated and needs to be updated for gpt-image-1 compatibility. See: https://github.com/openai/openai-python/commit/cc2c1fc15fd0bf1a5bdfb7b28b4d8d34e1cccad2\n2. Assumption of URL-based responses. The current implementation assumes a response format that includes an image url, which is not the case for gpt-image-1. See: https://github.com/microsoft/semantic-kernel/blob/8d1b3fd55155bb65e9671383366d2672c4582fb0/python/semantic_kernel/connectors/ai/open_ai/services/open_ai_text_to_image_base.py#L69\n\n## Suggested Fix\n- Add response handling logic that distinguishes between url (DALL·E 3) and b64_json (gpt-image-1).\n- Consider explicitly supporting gpt-image-1 via a new class or branching logic within AzureTextToImage.\n- (OPTIONAL) Update openai-python dependency to the latest version.\n\nI have confirmed that even without updating the openai-python SDK, image generation with gpt-image-1 works correctly as long as you do not need to specify the latest properties#. Therefore, the only essential modification required is to handle the response format appropriately#.\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n",
    "comments": []
  },
  {
    "issue_number": 10959,
    "title": ".Net: [Process Framework] Handle process final result in code",
    "author": "tommasodotNET",
    "state": "open",
    "created_at": "2025-03-13T15:48:00Z",
    "updated_at": "2025-06-17T02:15:07Z",
    "labels": [
      ".NET",
      "stale",
      "processes"
    ],
    "body": "---\nname: [Process Framework] Handle steps returns in code\nabout: There should be a way (or more than one) to handle the final result of a process\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\nProcess Framework will trigger a serie of steps which will produce, in the end, some sort of result. That result should be retrievable via code. This has mentioned on Discord as well.\n\nOf course some long running process might not be able to be awaitable and might end up writing their final result in a DB, sending an email or use something like SignalR to actively return the result to the application that triggered the process.\n\nIn other scenarios, it might be useful to have something like this\n```csharp\nlastStep\n    .OnFunctionResult()\n    .StopProcess()\n    .InterceptEvent((emittedEvent) => { \n        return emittedEvent.Data;\n    });\n```\nwhere we can intercept the final event of the last step, read the Data contained in the event and do something with it.",
    "comments": [
      {
        "user": "alliscode",
        "body": "Thank you @tommasodotNET, we are working on adding this functionality now. We will have something out to support this exact scenario soon."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10995,
    "title": ".Net: Bug: ToMermaid() generates extra edge from AssistantResponseStep to End",
    "author": "cpAtor",
    "state": "open",
    "created_at": "2025-03-15T17:04:39Z",
    "updated_at": "2025-06-17T02:15:05Z",
    "labels": [
      "bug",
      ".NET",
      "stale",
      "processes"
    ],
    "body": "**Describe the bug**\nWhen generating a mermaid diagram using `ToMermaid()` method for my chat process, an unexpected edge is added from AssistantResponseStep to End, even though I only defined a flow from AssistantResponseStep to UserInputStep, and UserInputStep to End.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Create a process with IntroStep, UserInputStep, and AssistantResponseStep\n2. Set up event flows as shown in the code below\n3. Generate a mermaid diagram using `process.ToMermaid()`\n4. Notice that the diagram shows an extra edge from AssistantResponseStep directly to End\n\n```csharp\nProcessBuilder processBuilder = new (\"ChatProcess\");\n\nProcessStepBuilder introStep = processBuilder.AddStepFromType<IntroStep>();\nProcessStepBuilder userInputStep = processBuilder.AddStepFromType<UserInputStep>();\nProcessStepBuilder assistantResponseStep = processBuilder.AddStepFromType<AssistantResponseStep>();\n\nprocessBuilder\n    .OnInputEvent(ChatBotEvents.StartProcess)\n    .SendEventTo(new (introStep));\n\nintroStep\n    .OnFunctionResult()\n    .SendEventTo(new (userInputStep));\n\nuserInputStep\n    .OnEvent(CommonEvents.UserInputReceived)\n    .SendEventTo(new (assistantResponseStep, AssistantResponseStep.Functions.GenerateAssistantResponse, AssistantResponseStep.Input.userMessage));\n\nassistantResponseStep\n    .OnEvent(CommonEvents.AssistantResponseGenerated)\n    .SendEventTo(new (userInputStep, UserInputStep.Functions.GetUserInput));\n\nuserInputStep\n    .OnEvent(CommonEvents.Exit)\n    .StopProcess();\n\nKernelProcess process = processBuilder.Build();\nstring mermaidGraph = process.ToMermaid();\n```\n\n## Generated mermaid\n\n```mermaid\nflowchart TB\n    Start[\"Start\"]\n    End[\"End\"]\n    IntroStep[\"IntroStep\"]\n    IntroStep[\"IntroStep\"] --> UserInputStep[\"UserInputStep\"]\n    UserInputStep[\"UserInputStep\"]\n    UserInputStep[\"UserInputStep\"] --> AssistantResponseStep[\"AssistantResponseStep\"]\n    UserInputStep[\"UserInputStep\"] --> End[\"End\"]\n    AssistantResponseStep[\"AssistantResponseStep\"]\n    AssistantResponseStep[\"AssistantResponseStep\"] --> UserInputStep[\"UserInputStep\"]\n    Start --> IntroStep[\"IntroStep\"]\n    AssistantResponseStep[\"AssistantResponseStep\"] --> End\n```\n\n**Expected behavior**\nThe generated mermaid diagram should only show the edges that were explicitly defined in the process builder. The extra edge from AssistantResponseStep to End should not be present, as I only defined:\n- Start → IntroStep\n- IntroStep → UserInputStep\n- UserInputStep → AssistantResponseStep\n- AssistantResponseStep → UserInputStep\n- UserInputStep → End\n\n**Platform**\n - Language: C#\n - Source: Microsoft.SemanticKernel.Process library\n - OS: Windows\n\n**Additional context**\nThis appears to be an issue with the ToMermaid() method in the KernelProcess class which adds edges that don't reflect the actual process flow. This could lead to confusion when reading the diagram to understand the process flow.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10996,
    "title": ".Net: HuggingFace Connector - Update Support for TextGeneration for TGI",
    "author": "RogerBarreto",
    "state": "open",
    "created_at": "2025-03-15T18:39:55Z",
    "updated_at": "2025-06-17T02:15:04Z",
    "labels": [
      "bug",
      ".NET",
      "ai connector",
      "stale"
    ],
    "body": "I see that we don't have a fully functional Text Generation as the latest Text Generation public endpoints from HuggingFace shifted to the OpenAI pattern, but I do see that the TGI supports still `/generate` and `/generate_stream`, might worth a future PR fixing our `TextGEnerationService` to support natively those endpoints from TGI.\n\nHere: https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi#curl",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 12375,
    "title": ".Net: Python: `on_intermediate_message` discrepancy",
    "author": "TaoChenOSU",
    "state": "open",
    "created_at": "2025-06-04T20:43:25Z",
    "updated_at": "2025-06-16T17:44:38Z",
    "labels": [
      ".NET",
      "python",
      "agents"
    ],
    "body": "Currently, our agents' `invoke` and `invoke_stream` methods accept a parameter called `on_intermediate_message`, that is supposed to allow developers to capture messages between the user's request and the agent's final response, **excluding** the final response. These messages usually contain tool call related contents.\n\nThe behavior of `invoke` and `invoke_stream` is consistent in Python. However, it's not the case in .Net.\n\nIn .Net, the `on_intermediate_message` is also called for the final agent response in the stream API. \n\nTake the `ChatCompletionAgent` as an example. \n\nSee here for the implementation of the non-streaming API: https://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/Agents/Core/ChatCompletionAgent.cs#L342. Also note that the name of the parameter for the internal method indicates the purpose of the callback: https://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/Agents/Core/ChatCompletionAgent.cs#L314. \n\nSee here for the implementation of the streaming API: https://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/Agents/Core/ChatCompletionAgent.cs#L417. Clearly the final response is assembled, and the callback is invoked with the full message.\n\nPut aside which behavior creates a better developer experience, we should at least have a consistent implementation across the two languages.",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Hi @westey-m, question for you:\n\nThe goal of the `OnIntermediateMessage` callback is to be able pass back to the caller interim messages that are non-final (i.e., FunctionCallContent / FunctionResultContent items that are part of a ChatMessageContent type).\n\nIn the ChatCompletionAgent, during a streaming invoke, the final message from the model is passed back to via the `OnIntermediateMessage` func:\n\nhttps://github.com/microsoft/semantic-kernel/blob/3499c7a92c612a1a089dc38f7d474577e2a6c773/dotnet/src/Agents/Core/ChatCompletionAgent.cs#L187C13-L194C15\n\nHere's where the code actually passes back a full string formed from streaming chunks:\n\nhttps://github.com/microsoft/semantic-kernel/blob/3499c7a92c612a1a089dc38f7d474577e2a6c773/dotnet/src/Agents/Core/ChatCompletionAgent.cs#L415C9-L419C10\n\nAlso, we should remove the following code that shows:\n\n```\n[Obsolete(\"Use InvokeStreamingAsync with AgentThread instead. This method will be removed after May 1st 2025.\")]\n```\n"
      },
      {
        "user": "westey-m",
        "body": "Maybe the naming of `OnIntermediateMessage` isn't the best in hindsight, since it is really about returning all fully formed messages, including the last one.  It is mostly useful for streaming scenarios where the user may want to have both the streamed updates but also the complete messages, but function call messages may of course also be useful.  Since we always have to assemble the fully formed messages inside the agent for thread update and ai context maintenance scenarios, it doesn't cost us anything to also make this available to callers.\n\nThanks for the reminder about removing the obsoletes.  Definitely something we can do now."
      },
      {
        "user": "moonbox3",
        "body": "@westey-m, I think we should have done a better job at recording some meeting/convo notes from this - I remember in a standup before GA'ing the single agent abstractions (I think @TaoChenOSU can vouch for for this, too):\n\n\"The goal of an agent invocation is to return the final answer to the caller. This means that any interim messages that are generated - usually tool related, including item types like `FunctionCallContent` and/or `FunctionResultContent` - will be returned to the caller via the (optional) callback `on_intermediate_message` (Python snake-case naming) or `OnIntermediateMessage` (.NET naming).\"\n\nIn the scenario that caller wants to know the weather in Seattle (and a weather-type plugin is registered for use), when the caller invokes the agent, they see:\n\n```python\nuser_input = \"What is the weather in Seattle?\"\nprint(f\"{user_input}\")\nasync for response in agent.invoke(user_input):\n    print(f\"Agent: {response}\")\n```\n\nOutput looks like:\n\n```bash\nUser: What is the weather in Seattle?\nAgent: The weather in Seattle is sunny.\n```\n(For the streaming invocation, the user seeings \"The...weather...in...Seattle...is...sunny\" (streaming in chunks), which is the final answer.)\n\nIf the caller wants to be able to get the interim-messages (tool calls), they provide the callback:\n\n```python\nasync def print_intermediate_message(message: ChatMessageContent) -> None:\n    for item in message.items or []:\n        if isinstance(item, FunctionResultContent):\n            print(f\"Function Result:> {item.result} for function: {item.name}\")\n        elif isinstance(item, FunctionCallContent):\n            print(f\"Function Call:> {item.name} with arguments: {item.arguments}\")\n        else:\n            print(f\"{item}\")\n\n\nuser_input = \"What is the weather in Seattle?\"\nprint(f\"{user_input}\")\nasync for response in agent.invoke(user_input, on_intermediate_message=print_intermediate_message):\n    print(f\"Agent: {response}\")\n```\n\nOutput looks like:\n\n```bash\nUser: What is the weather in Seattle?\nFunction Call:> WeatherPlugin-get_weather with arguments: {'city': 'Seattle'}\nFunction Result:> The weather in Seattle is sunny.\nAgent: The weather in Seattle is sunny. If you'd like to know the weather for a different city, let me know!\n```\n\nThis is similar for the streaming case as well. My question is: why then, if one provides the `OnIntermediateMessage` callback, would we get the final response for the completed messages? The user already received the streaming message chunks. Streaming invocation is useful when you want the lowest latency during response generation and want to show the chunks as they're made available. If the caller really needs the fully formed messages, they can handle it on their own - or just get the messages from the thread, since the fully formed message is there."
      }
    ]
  },
  {
    "issue_number": 12453,
    "title": ".Net: Can't find \"Microsoft.SemanticKernel.Agents.Orchestration\"",
    "author": "aigent-mehdi",
    "state": "open",
    "created_at": "2025-06-11T14:42:08Z",
    "updated_at": "2025-06-16T17:17:43Z",
    "labels": [
      "question",
      ".NET",
      "agents"
    ],
    "body": "Hello I am trying to build a concurrent agent sample using a notebook (.NEt Interactive)\nI am unable to find : \n\nMicrosoft.SemanticKernel.Agents.Orchestration; \nand \nMicrosoft.SemanticKernel.Agents.Orchestration.Concurrent;\n\nWhat nugget package do I need?",
    "comments": [
      {
        "user": "marcusramos2024",
        "body": "Same"
      },
      {
        "user": "crickman",
        "body": "All of the orchestrations, except for _Magentic_ are in the same package:\n\nhttps://www.nuget.org/packages/Microsoft.SemanticKernel.Agents.Orchestration\nhttps://www.nuget.org/packages/Microsoft.SemanticKernel.Agents.Magentic\n\n> NOTE:  target the version that aligns with your other SK packages:\n\n```sh\ndotnet add package Microsoft.SemanticKernel.Agents.Orchestration --version 1.56.0-preview --prerelease\n```\n\nFor Magentic use:\n\n```sh\ndotnet add package Microsoft.SemanticKernel.Agents.Magentic --version 1.56.0-preview --prerelease\n```"
      }
    ]
  },
  {
    "issue_number": 5924,
    "title": "Implement semantic cache",
    "author": "Rtoribiog",
    "state": "closed",
    "created_at": "2024-04-18T20:13:30Z",
    "updated_at": "2025-06-16T12:30:12Z",
    "labels": [
      "python",
      "enhancement"
    ],
    "body": "<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\r\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\r\n<!-- Please search existing issues to avoid creating duplicates. -->\r\n\r\nThat's a feature that already exists in langchain and will be beneficial to save costs.  The idea will be to ported from phyton to c#\r\n\r\nhttps://github.com/zilliztech/GPTCache\r\n\r\nhttps://python.langchain.com/docs/integrations/llms/llm_caching/\r\n\r\nhttps://www.mongodb.com/developer/products/atlas/advanced-rag-langchain-mongodb/\r\n\r\n\r\n<!-- Describe the feature you'd like. -->\r\n\r\nBe able to save all the queries together with llm responses in some database and try to fetch it first from there,  if not then call the LLM,  and have some parameters to invalidate or update cache from time to time.\r\n",
    "comments": [
      {
        "user": "madsbolaris",
        "body": "@markwallace-microsoft, going to assign this to @dmytrostruk since this is one of the samples he's working on using filters."
      },
      {
        "user": "madsbolaris",
        "body": "Also assigning @eavanvalkenburg since he was working on filters for Python"
      },
      {
        "user": "dmytrostruk",
        "body": "Example for Semantic Caching in .NET is here: https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/Concepts/Caching/SemanticCachingWithFilters.cs"
      }
    ]
  },
  {
    "issue_number": 12291,
    "title": ".Net: New Feature: Integrate Voice Live API",
    "author": "mattiasu96",
    "state": "open",
    "created_at": "2025-05-28T08:37:11Z",
    "updated_at": "2025-06-16T06:39:27Z",
    "labels": [
      ".NET",
      "ai connector"
    ],
    "body": "---\nname: Integrate Voice Live API\nabout: Integrate Voice Live API\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\nThere is this [new API](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/voice-live) from Microsoft that is integrating noise suppression/ echo cancellation on top of the Realtime API. \n\nI am working myself on some Realtime voice chatbots and already used Semantic Kernel to connect to the openAI/Azure realtime API and it is working pretty well.\nHowever, noise suppression/echo cancellation are still a big problem. As an example, if run on speakers, the models hears itself and starts looping answering itself. Having the Voice Live API available in Semantic Kernel would be a really nice and useful feature for everyone that is developing voice chatbots using the Azure ecosystem. \n\nI have some idea on how to do it and I am willing to contribute if possible. We can discuss about this\n\nThank you! ",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "@RogerBarreto would this be a candidate service to support with the planned M.E.AI real-time API?"
      },
      {
        "user": "mattiasu96",
        "body": "Note: I can contribute on the Python side, .NET code im not really an expert"
      },
      {
        "user": "mattiasu96",
        "body": "Any news to this? @markwallace-microsoft @RogerBarreto "
      }
    ]
  },
  {
    "issue_number": 10992,
    "title": "Python: Update Learn Docs with Text Search Python Code Samples",
    "author": "moonbox3",
    "state": "open",
    "created_at": "2025-03-14T22:14:32Z",
    "updated_at": "2025-06-16T02:16:03Z",
    "labels": [
      "python",
      "documentation",
      "stale"
    ],
    "body": "In our learn docs, the C# code samples are present, but Python samples are missing:\n\nhttps://learn.microsoft.com/en-us/semantic-kernel/concepts/text-search/?pivots=programming-language-python",
    "comments": [
      {
        "user": "shethaadit",
        "body": "H @moonbox3 / @eavanvalkenburg / @markwallace-microsoft / @sophialagerkranspandey, I have raised PR for this\nhttps://github.com/MicrosoftDocs/semantic-kernel-docs/pull/229\n\ncould you please review?"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10061,
    "title": "New Feature: GoLang support",
    "author": "AaronSaikovski",
    "state": "open",
    "created_at": "2025-01-06T01:30:14Z",
    "updated_at": "2025-06-16T01:02:56Z",
    "labels": [],
    "body": "Please provide native support for the Go programming language \nThx \n---\nname: Feature request\nabout: Suggest an idea for this project\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n",
    "comments": [
      {
        "user": "sphenry",
        "body": "Consideration for backlog"
      },
      {
        "user": "AaronSaikovski",
        "body": "any updates on this?"
      },
      {
        "user": "AaronSaikovski",
        "body": "Any updates (if any) if there will be a native Golang version of SK?"
      }
    ]
  },
  {
    "issue_number": 9972,
    "title": ".Net: Make {Azure}OpenAI packages Native-AOT compatible",
    "author": "SergeyMenshykh",
    "state": "closed",
    "created_at": "2024-12-13T18:22:54Z",
    "updated_at": "2025-06-15T16:30:11Z",
    "labels": [
      ".NET",
      "ai connector",
      "sk team issue",
      "stale",
      "aot"
    ],
    "body": "**Context:**\n<img width=\"549\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/7a169bb0-5d25-4e73-aa13-4166c273c4fb\" />\n[Source](https://github.com/microsoft/semantic-kernel/blob/e8838437e93b68d665ee8acf84c1abcd4f5410e8/dotnet/samples/Demos/AotCompatibility/README.md)\n\n**ToDo:**\nMake both `Connectors.OpenAI` and `Connectors.AzureOpenAI` packages AOT compatible",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 14 days since being marked as stale."
      },
      {
        "user": "mikeparker",
        "body": "Has this ticket been prematurely closed? Right now it seems using Semantic Kernel in AWS Lambda (typically using AOT to stop slow cold start times) doesnt work when using the OpenAI connector, as per linked issue https://github.com/microsoft/semantic-kernel/issues/9941"
      }
    ]
  },
  {
    "issue_number": 6058,
    "title": ".Net: System.InvalidOperationException: Collection was modified; enumeration operation may not execute from Microsoft.SemanticKernel.Connectors.OpenAI",
    "author": "gleborgne",
    "state": "closed",
    "created_at": "2024-04-30T11:11:27Z",
    "updated_at": "2025-06-15T09:03:36Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\r\nI try upgrading from 1.7.1 but any version above that I tried are throwing exceptions when I am using GetChatMessageContentsAsync. If using GetStreamingChatMessageContentsAsync with the exact same parameters everything is working fine.\r\n\r\nI am using Azure Open AI with a few kernel plugins and ToolCallBehavior = ToolCallBehavior.AutoInvokeKernelFunctions,.\r\n\r\nSystem.InvalidOperationException: Collection was modified; enumeration operation may not execute.\r\n   at System.Collections.Generic.Dictionary`2.Enumerator.MoveNext()\r\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetChatMessage(ChatChoice chatChoice, ChatCompletions responseData)\r\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.GetChatMessageContentsAsync(ChatHistory chat, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)\r\n\r\nFrom what I can see from the logs, the call to Azure Open AI was done and the problem is when linking to the kernel functions.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nno error ?\r\n\r\n**Platform**\r\n - OS: Windows\r\n - IDE: Visual Studio\r\n - Language: C#\r\n - Source: NuGet SemanticKernel 1.8.0 and up\r\n",
    "comments": [
      {
        "user": "stephentoub",
        "body": "Can you share a simple repro?"
      },
      {
        "user": "gleborgne",
        "body": "[TryReproSK.zip](https://github.com/microsoft/semantic-kernel/files/15165279/TryReproSK.zip)\r\n\r\nI have the bug with this simple project"
      },
      {
        "user": "stephentoub",
        "body": "Thanks!\r\n\r\n@SergeyMenshykh, you added this code:\r\nhttps://github.com/microsoft/semantic-kernel/blame/f66a30bbd9bee1715062b07cfad537fc10f54aee/dotnet/src/Connectors/Connectors.OpenAI/AzureSdk/ClientCore.cs#L1224-L1227\r\n```C#\r\n                        foreach (var argument in arguments)\r\n                        {\r\n                            arguments[argument.Key] = argument.Value?.ToString();\r\n                        }\r\n```\r\nThat's generally invalid: you can't change the dictionary while you're enumerating it."
      }
    ]
  },
  {
    "issue_number": 12452,
    "title": ".Net: Bug: Llama3.2-vision and Ollama Connector not working anymore.",
    "author": "tennaito",
    "state": "closed",
    "created_at": "2025-06-11T14:11:29Z",
    "updated_at": "2025-06-14T14:28:54Z",
    "labels": [
      ".NET"
    ],
    "body": "**Describe the bug**\nThe code below worked fine with llama3.2-vision in 1.31 versions (and the Ollama version from that time), but when upgraded to latest version it is not working anymore. Any directions?\n\nI´ve have tested again on 1.31 and it did not worked now in the future... I am lost.\n\n```csharp\n\tpublic static async Task<string> AnalyzeImage(string text, ReadOnlyMemory<byte> image, string? mimeType, OllamaPromptExecutionSettings settings = null, CancellationToken cancellationToken = default)\n\t{\n\t\tsettings ??= new OllamaPromptExecutionSettings();\n\t\tvar kernel = CreateKernel(settings.ModelId ?? \"llama3.2-vision\");\n\t\tvar chatCompletionService = kernel.GetRequiredService<IChatCompletionService>();\n\t\tChatHistory input = new ChatHistory();\n\t\tChatMessageContent content = new ChatMessageContent();  \n\t\tChatMessageContentItemCollection items = new ChatMessageContentItemCollection();\n\t\titems.Add(new TextContent(text));\n\t\titems.Add(new ImageContent(image, mimeType));\n\t\tinput.AddMessage(AuthorRole.User, items);\n\t\tChatMessageContent result = await chatCompletionService.GetChatMessageContentAsync(input, settings, kernel);\n\t\treturn result.Items.OfType<TextContent>().First().Text;\n\t}\n\n\tpublic static Kernel CreateKernel(string model = \"llama3.1\")\n\t{\n\t\tvar builder = Kernel.CreateBuilder();\n\t\tbuilder.Services.AddTransient(s =>\n\t\t{\n\t\t\tvar client = new HttpClient(new HttpClientHandler());\n\t\t\tclient.Timeout = TimeSpan.FromMinutes(2);\n\t\t\treturn client;\n\t\t});\n\t\t\n\t\tbuilder.Services.AddOllamaChatCompletion(model, new Uri(\"http://localhost:11434/\"));\n\t\treturn builder.Build();\n\t}\n```\n\n**To Reproduce**\nExecute method AnalyzeImage.\n\n**Expected behavior**\nNo error on version upgrade.\n\nOn the 1.31 version it returned the analysis of the image with the text context from user input (nowdays same error).\nOn the 1.56 version it returns:\n\n```csharp\nResponse status code does not indicate success: 500 (Internal Server Error).\n\nat System.Net.Http.HttpResponseMessage.EnsureSuccessStatusCode()\nat OllamaSharp.OllamaApiClient.EnsureSuccessStatusCodeAsync(HttpResponseMessage response)\nat OllamaSharp.OllamaApiClient.SendToOllamaAsync(HttpRequestMessage requestMessage, OllamaRequest ollamaRequest, HttpCompletionOption completionOption, CancellationToken cancellationToken)\nat OllamaSharp.OllamaApiClient.ChatAsync(ChatRequest request, CancellationToken cancellationToken)+MoveNext()\nat OllamaSharp.OllamaApiClient.ChatAsync(ChatRequest request, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult()\nat OllamaSharp.IAsyncEnumerableExtensions.StreamToEndAsync[Tin,Tout](IAsyncEnumerable`1 stream, IAppender`2 appender, Action`1 itemCallback)\nat OllamaSharp.IAsyncEnumerableExtensions.StreamToEndAsync[Tin,Tout](IAsyncEnumerable`1 stream, IAppender`2 appender, Action`1 itemCallback)\nat OllamaSharp.OllamaApiClient.Microsoft.Extensions.AI.IChatClient.GetResponseAsync(IEnumerable`1 messages, ChatOptions options, CancellationToken cancellationToken)\nat Microsoft.Extensions.AI.FunctionInvokingChatClient.GetResponseAsync(IEnumerable`1 messages, ChatOptions options, CancellationToken cancellationToken)\nat Microsoft.SemanticKernel.ChatCompletion.ChatClientChatCompletionService.GetChatMessageContentsAsync(ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)\nat Microsoft.SemanticKernel.ChatCompletion.ChatCompletionServiceExtensions.GetChatMessageContentAsync(IChatCompletionService chatCompletionService, ChatHistory chatHistory, PromptExecutionSettings executionSettings, Kernel kernel, CancellationToken cancellationToken)\nat MyApplication.Util.OpenAIHelper.AnalyzeVision(String text, ReadOnlyMemory`1 image, String mimeType, OllamaPromptExecutionSettings settings, CancellationToken cancellationToken)\n```\n\n**Platform that works**\n - Language: C#\n - Source: \n\t\tMicrosoft.SemanticKernel\" Version=\"1.31.0\"\n\t\tMicrosoft.SemanticKernel.Connectors.Ollama\" Version=\"1.31.0-alpha\"\n - AI model: llama3.2-vision\n - IDE: Visual Studio\n - OS: Windows\n\n**Platform that does not work**\n - Language: C#\n - Source: \n\t\tMicrosoft.SemanticKernel\" Version=\"1.56.0\"\n\t\tMicrosoft.SemanticKernel.Connectors.Ollama\" Version=\"1.56.0-alpha\"\n - AI model: llama3.2-vision\n - IDE: Visual Studio\n - OS: Windows\n",
    "comments": [
      {
        "user": "rogeriorfp",
        "body": "I tested your code and it worked. Which packages are you referencing?\nDid you try cleaning the solution and rebuilding it?\n"
      },
      {
        "user": "tennaito",
        "body": "@rogeriorfp Did you test your code with version 1.56?\n\nI have already cleaned, rebuilt, and locally deleted cached libraries.\n"
      },
      {
        "user": "rogeriorfp",
        "body": "> [@rogeriorfp](https://github.com/rogeriorfp) Did you test your code with version 1.56?\n> \n> I have already cleaned, rebuilt, and locally deleted cached libraries.\n\nYes @tennaito , I tested it with version 1.56.\n\n```\n<PackageReference Include=\"Microsoft.SemanticKernel\" Version=\"1.56.0\" />\n<PackageReference Include=\"Microsoft.SemanticKernel.Connectors.Ollama\" Version=\"1.56.0-alpha\" />\n```"
      }
    ]
  },
  {
    "issue_number": 12470,
    "title": "Python: Process Framework - When will this be in GA?",
    "author": "harrydobbs",
    "state": "open",
    "created_at": "2025-06-12T23:36:00Z",
    "updated_at": "2025-06-14T08:38:24Z",
    "labels": [
      ".NET",
      "python",
      "processes"
    ],
    "body": "Hi there,\n\nI am just wondering when with the process framework will be available for GA?\nI see it's currently in preview.\n\nCheers,\nHarry",
    "comments": []
  },
  {
    "issue_number": 11037,
    "title": "Python: Bug: Pydantic error: Missing parameter for AzureAIAgentSettings constructor. Agent Retrieval Example",
    "author": "juan-trujillo",
    "state": "closed",
    "created_at": "2025-03-18T13:18:37Z",
    "updated_at": "2025-06-13T19:25:38Z",
    "labels": [
      "bug",
      "python",
      "agents"
    ],
    "body": "**Describe the bug**\nWhen trying to run the Azure AI Agent Retrieval example, a pydantic exception is thrown.\n\n_File \"c:\\code repos\\semantic-kernel\\python\\.venv\\Lib\\site-packages\\pydantic\\main.py\", line 214, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for AzureAIAgentSettings\nmodel_deployment_name\n  Field required [type=missing, input_value={'azure_openai_chat_deplo...ith.search.windows.net'}, input_type=dict]_\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to [Step 7: Azure AI Agent Retrieval Example](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/getting_started_with_agents/azure_ai_agent/step7_azure_ai_agent_retrieval.py)\n2. Provide an Agent ID and remaining .env parameters\n3. F5 step7_azure_ai_agent_retrieval.py\n4. See error\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Screenshots**\n.env\n\nAZURE_AI_AGENT_PROJECT_CONNECTION_STRING = \"eastus.api.azureml.ms;[tenantid];[rg name];[Foundry Project Name]\"\nAZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME = \"gpt-4o\"\n\n**Module Versions**\nsemantic-kernel                    1.24.1\npydantic                           2.10.6\npydantic_core                      2.27.2\npydantic-settings                  2.5.2\n\n**Platform**\n - Language: Python\n - Source: sk-repo\n - AI model: gpt-4o\n - IDE: VS Code\n - OS: Windows\n\n**Additional context**\nThere is an additional warning being thrown by the pydantic module (See below). \n\n_UserWarning: Field \"model_deployment_name\" in AzureAIAgentSettings has conflict with protected namespace \"model_\".\n\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n  warnings.warn(_\n \n\n```\n# Copyright (c) Microsoft. All rights reserved.\nimport asyncio\nimport os\nimport warnings\n\nfrom azure.identity.aio import DefaultAzureCredential\nfrom semantic_kernel.agents.azure_ai import AzureAIAgent, AzureAIAgentSettings\n\n# import inspect\n\n\"\"\"\nThe following sample demonstrates how to use an already existing\nAzure AI Agent within Semantic Kernel. This sample requires that you\nhave an existing agent created either previously in code or via the\nAzure Portal (or CLI).\n\"\"\"\n\n\n# Simulate a conversation with the agent\nUSER_INPUTS = [\n    \"Why is the sky blue?\",\n]\n\nwarnings.filterwarnings(\"ignore\", message=\"Field .* has conflict with protected namespace .*\")\n\nasync def main() -> None:\n    \n\n    connection_string = os.getenv(\"AZURE_AI_AGENT_PROJECT_CONNECTION_STRING\")\n    deployment_name = os.getenv(\"azure_openai_chat_deployment_name\")\n  \n    async with (\n        DefaultAzureCredential() as creds,\n        AzureAIAgent.create_client(credential=creds) as client,\n    ):\n        # 1. Retrieve the agent definition based on the `assistant_id`\n        agent_definition = await client.agents.get_agent(\n            assistant_id=\"asst_id\",\n        )\n\n        # # 2. Create a Semantic Kernel agent for the Azure AI agent\n        agent_settings = AzureAIAgentSettings(\n            model_config={'protected_namespaces': ('settings_',)},\n            # model_deployment_name=deployment_name  # Using the model name from .env file\n            azure_openai_chat_deployment_name=deployment_name,  # Using the model name from .env file\n        )\n        agent = AzureAIAgent(\n            client=client,\n            definition=agent_definition,\n            settings=agent_settings,\n        )\n\n        # 3. Create a new thread on the Azure AI agent service\n        thread = await client.agents.create_thread()\n\n        try:\n            for user_input in USER_INPUTS:\n                # 4. Add the user input as a chat message\n                await agent.add_chat_message(thread_id=thread.id, message=user_input)\n                print(f\"# User: '{user_input}'\")\n                # 5. Invoke the agent for the specified thread for response\n                response = await agent.get_response(thread_id=thread.id)\n                print(f\"# {response.name}: {response}\")\n        finally:\n            # 6. Cleanup: Delete the thread and agent\n            await client.agents.delete_thread(thread.id)\n            # Do not clean up the assistant so it can be used again\n\n        \"\"\"\n        Sample Output:\n        # User: 'Why is the sky blue?'\n        # Agent: The sky appears blue because molecules in the Earth's atmosphere scatter sunlight, \n        and blue light is scattered more than other colors due to its shorter wavelength.\n        \"\"\"\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Hi @juan-trujillo, thanks for filing the issue. Here are my thoughts:\n\n1. The current bug I see is that we haven't updated this sample to use the `agent_id` kwarg, instead of the old `assistant_id` kwarg in `agent_definition = await client.agents.get_agent(assistant_id=\"asst_id\")`. It should be `agent_definition = await client.agents.get_agent(agent_id=\"you-agent-id\")`. In SK Python 1.24.1 we updated the underlying `azure-ai-projects` dependency to use `1.0.0b7` which has the breaking change form `assistant_id` to `agent_id`. I'll get a fix out for that.\n2. There's no need to pass in an instance of the `AzureAIAgentSettings` to the `AzureAIAgent` constructor. The `AzureAIAgentSettings` are used to set up the resources for the `AzureAIAgent`, by grabbing the deployment name and the project connection string. You'll notice there's no model field on both the `AzureAIAgent` or the base `Agent` class for the settings - there could be some conflict going on as Pydantic tries to validate the model during construction. Apart from that, if you do want to override the `chat_deployment_name`, it should be done like:\n\n```python\nagent_settings = AzureAIAgentSettings(\n    model_deployment_name=deployment_name \n)\n```\n\nThe attribute on the `AzureAIAgentSettings` is `model_deployment_name` instead of `azure_openai_chat_deployment_name` (which is used for `AzureOpenAISettings`). The `AzureAIAgentSettings` class, using Pydantic settings, looks for an env var (either on the system or in the .env file) with the name `AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME`."
      },
      {
        "user": "juan-trujillo",
        "body": "Hi @moonbox3 , thanks for replying so quickly.\n\nHere is the error I am getting after making the suggested changes.\n\n_pydantic_core._pydantic_core.ValidationError: 1 validation error for AzureAIAgentSettings\nmodel_deployment_name\n  Field required [type=missing, input_value={'azure_openai_chat_deplo...ith.search.windows.net'}, input_type=dict]_\n\n\n.env file contains only:\n\nAZURE_AI_AGENT_PROJECT_CONNECTION_STRING = \"eastus.api.azureml.ms;[tenantid];[rg name];[Foundry Project Name]\"\nAZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME = \"gpt-4o\"\n\nHere is the updated code:\n\n```# Copyright (c) Microsoft. All rights reserved.\nimport asyncio\nimport warnings\n\nfrom azure.identity.aio import DefaultAzureCredential\nfrom semantic_kernel.agents.azure_ai import AzureAIAgent\n\n# Simulate a conversation with the agent\nUSER_INPUTS = [\n    \"Why is the sky blue?\",\n]\n\nwarnings.filterwarnings(\"ignore\", message=\"Field .* has conflict with protected namespace .*\")\n\nasync def main() -> None:\n      \n    async with (\n        DefaultAzureCredential() as creds,\n        AzureAIAgent.create_client(credential=creds) as client,\n    ):\n        # 1. Retrieve the agent definition based on the `assistant_id`\n        agent_definition = await client.agents.get_agent(\n            agent_id=\"[my_agent_id]\",\n        )\n        # 2. Create an Azure AI agent instance\n        agent = AzureAIAgent(\n            client=client,\n            definition=agent_definition\n        )\n\n        # 3. Create a new thread on the Azure AI agent service\n        thread = await client.agents.create_thread()\n\n        try:\n            for user_input in USER_INPUTS:\n                # 4. Add the user input as a chat message\n                await agent.add_chat_message(thread_id=thread.id, message=user_input)\n                print(f\"# User: '{user_input}'\")\n                # 5. Invoke the agent for the specified thread for response\n                response = await agent.get_response(thread_id=thread.id)\n                print(f\"# {response.name}: {response}\")\n        finally:\n            # 6. Cleanup: Delete the thread and agent\n            await client.agents.delete_thread(thread.id)\n            # Do not clean up the assistant so it can be used again\n\n        \"\"\"\n        Sample Output:\n        # User: 'Why is the sky blue?'\n        # Agent: The sky appears blue because molecules in the Earth's atmosphere scatter sunlight, \n        and blue light is scattered more than other colors due to its shorter wavelength.\n        \"\"\"\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```"
      },
      {
        "user": "moonbox3",
        "body": "@juan-trujillo, the error really points to Pydantic settings not being able to find that env var. \n\nAs a test, can you try to create the AzureAIAgentSettings like:\n\nsettings = AzureAIAgentSettings.create(model_deployment_name=“gpt-4o”)\n\nThis will override any env var to make sure it is set. If that works, then something is off with the env var. Also, do you have an exact stack trace? It’ll help me see what calls lead to the error. Based on your code, it should be in the create client. \n\nI’m running the exact same code, and can’t repro it. Thanks. "
      }
    ]
  },
  {
    "issue_number": 12468,
    "title": ".NET Bug: OpenAIResponseAgent does not expose function-calling content",
    "author": "crickman",
    "state": "open",
    "created_at": "2025-06-12T21:18:39Z",
    "updated_at": "2025-06-13T19:07:36Z",
    "labels": [
      "bug",
      ".NET",
      "agents"
    ],
    "body": "**Describe the bug**\nIntegrationt test does not confirm to expected result.\n\n**To Reproduce**\nRun integration tests:\n`OpenAIResponseAgentInvokeTests.InvokeWithPluginNotifiesForAllMessagesAsync`\n\n> Good: `OpenAIResponseAgentInvokeStreamingTests.InvokeStreamingWithPluginNotifiesForAllMessagesAsync`\n\n**Platform**\n - Language: C#\n - Source: `main`\n - OS: Windows\n",
    "comments": []
  },
  {
    "issue_number": 12464,
    "title": ".Net: Bug: Adding ChatHistory.SystemMessage when using Connectors.Google throws exception",
    "author": "ssathya",
    "state": "open",
    "created_at": "2025-06-12T14:46:52Z",
    "updated_at": "2025-06-13T17:04:57Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nIf I were to invoke the method ChatHistory.AddSystemMessage along with Microsoft.SemanticKernel.Connectors.Google \nchatCompletionService.GetChatMessageContentAsync throws an exception.\n\n**To Reproduce**\nThis is my sample code:\n#pragma warning disable SKEXP0070\n\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.ChatCompletion;\nusing Microsoft.SemanticKernel.Connectors.Google;\n\nIKernelBuilder kernelBuilder = Kernel.CreateBuilder();\nkernelBuilder.AddGoogleAIGeminiChatCompletion(\n    modelId: \"gemini-2.0-flash\",\n    apiKey: \"API_KEY\",\n    httpClient: new HttpClient() // Optional; for customizing HTTP client\n);\nKernel kernel = kernelBuilder.Build();\nvar chatCompletionService = kernel.GetRequiredService<IChatCompletionService>();\n\nChatHistory history = [];\n//history.AddSystemMessage(\"You are my tour guide at Yosemite National Park!\");\nhistory.AddUserMessage(\"Hello, how are you?\");\n\nstring[] questions = new[] {\n    \"What is the best time to visit Yosemite?\",\n    \"What are the main attractions in Yosemite?\",\n    \"How do I get to Yosemite from San Francisco?\",\n    \"What should I pack for a trip to Yosemite?\",\n    \"Are there any guided tours available in Yosemite?\"\n};\nforeach (string question in questions)\n{\n    history.AddUserMessage(question);\n    ChatMessageContent responseContent = await chatCompletionService.GetChatMessageContentAsync(history);\n    if (responseContent != null && responseContent.Content != null)\n    {\n        history.AddAssistantMessage(responseContent.Content);\n        Console.WriteLine($\"Q: {question}\\nA: {responseContent.Content}\\n\");\n    }\n    else\n    {\n        Console.WriteLine($\"Q: {question}\\nA: No response received.\\n\");\n    }\n}\n\nIf I were to uncomment the call to AddSystemMessage the application throws an exception. \n\n**Expected behavior**\nAddSystemMessage shouldn't affect this console application.\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n![Exception](http://i.imgur.comzZK3HZG.png)\n**Platform**\n - Language:  C#\n - Source: Microsoft.SemanticKernel.Connectors.Google -Version 1.56.0-alpha\n - AI model: gemini-2.0-flash\n - IDE: Visual Studio\n - OS: Windows 11\n\n**Additional context**\nn/a",
    "comments": [
      {
        "user": "ssathya",
        "body": "[Image link](https://imgur.com/a/8hcmtTM). \n\n"
      }
    ]
  },
  {
    "issue_number": 12469,
    "title": ".Net Bug: OpenAIResponseAgent does not provide messages to AIContext for streaming",
    "author": "crickman",
    "state": "open",
    "created_at": "2025-06-12T21:37:34Z",
    "updated_at": "2025-06-13T17:02:53Z",
    "labels": [
      "bug",
      ".NET",
      "agents"
    ],
    "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\n**To Reproduce**\nRun integration/conformance test:\n\n`OpenAIResponseAgentWithAIContextProviderTests.StatePartReceivesMessagesFromAgentWhenStreamingAsync`\n\n**Platform**\n - Language: C#\n - Source: `main`\n - OS: Windows\n",
    "comments": []
  },
  {
    "issue_number": 12473,
    "title": "Ensure storage of messages in a 3rd party service works with our agent abstraction",
    "author": "westey-m",
    "state": "open",
    "created_at": "2025-06-13T16:14:16Z",
    "updated_at": "2025-06-13T17:02:46Z",
    "labels": [],
    "body": null,
    "comments": []
  },
  {
    "issue_number": 12472,
    "title": ".Net: The message could not be added to the thread due to an error response from the service.",
    "author": "karthiksimplist",
    "state": "open",
    "created_at": "2025-06-13T00:54:21Z",
    "updated_at": "2025-06-13T17:01:01Z",
    "labels": [
      ".NET",
      "agents"
    ],
    "body": "Why we would get this error and how to handle this?\n\n```\nMessage: The message could not be added to the thread due to an error response from the service.\nStack: \n   at Microsoft.SemanticKernel.Agents.AzureAI.AzureAIAgentThread.OnNewMessageInternalAsync(ChatMessageContent newMessage, CancellationToken cancellationToken)\n   at Microsoft.SemanticKernel.Agents.AgentThread.OnNewMessageAsync(ChatMessageContent newMessage, CancellationToken cancellationToken)\n   at Microsoft.SemanticKernel.Agents.Agent.EnsureThreadExistsWithMessagesAsync[TThreadType](ICollection`1 messages, AgentThread thread, Func`1 constructThread, CancellationToken cancellationToken)\n   at Microsoft.SemanticKernel.Agents.AzureAI.AzureAIAgent.InvokeStreamingAsync(ICollection`1 messages, AgentThread thread, AzureAIAgentInvokeOptions options, CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Agents.AzureAI.AzureAIAgent.InvokeStreamingAsync(ICollection`1 messages, AgentThread thread, AzureAIAgentInvokeOptions options, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource.GetResult()\n```",
    "comments": []
  },
  {
    "issue_number": 12445,
    "title": ".Net: Bug: SessionsPythonPlugin does not correctly hhandle a numeric execution result returned by Dynamic Sessions endpoint",
    "author": "davidames",
    "state": "open",
    "created_at": "2025-06-10T22:38:54Z",
    "updated_at": "2025-06-13T17:00:28Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nThe Container Apps Dynamic Sessions endpoint will sometimes return an executionResult as an integer instead of a string, resulting in the following error\n\n\n```\nSystem.Text.Json.JsonException: The JSON value could not be converted to System.String. Path: $.result.executionResult | LineNumber: 0 | BytePositionInLine: 222.\n ---> System.InvalidOperationException: Cannot get the value of a token type 'Number' as a string.\n   at System.Text.Json.ThrowHelper.ThrowInvalidOperationException_ExpectedString(JsonTokenType tokenType)\n   at System.Text.Json.Utf8JsonReader.GetString()\n   at System.Text.Json.Serialization.Converters.StringConverter.Read(Utf8JsonReader& reader, Type typeToConvert, JsonSerializerOptions options)\n   at System.Text.Json.Serialization.Metadata.JsonPropertyInfo`1.ReadJsonAndSetMember(Object obj, ReadStack& state, Utf8JsonReader& reader)\n   at System.Text.Json.Serialization.Converters.ObjectDefaultConverter`1.OnTryRead(Utf8JsonReader& reader, Type typeToConvert, JsonSerializerOptions options, ReadStack& state, T& value)\n   at System.Text.Json.Serialization.JsonConverter`1.TryRead(Utf8JsonReader& reader, Type typeToConvert, JsonSerializerOptions options, ReadStack& state, T& value, Boolean& isPopulatedValue)\n   at System.Text.Json.Serialization.Metadata.JsonPropertyInfo`1.ReadJsonAndSetMember(Object obj, ReadStack& state, Utf8JsonReader& reader)\n   at System.Text.Json.Serialization.Converters.ObjectDefaultConverter`1.OnTryRead(Utf8JsonReader& reader, Type typeToConvert, JsonSerializerOptions options, ReadStack& state, T& value)\n   at System.Text.Json.Serialization.JsonConverter`1.TryRead(Utf8JsonReader& reader, Type typeToConvert, JsonSerializerOptions options, ReadStack& state, T& value, Boolean& isPopulatedValue)\n   at System.Text.Json.Serialization.JsonConverter`1.ReadCore(Utf8JsonReader& reader, T& value, JsonSerializerOptions options, ReadStack& state)\n   --- End of inner exception stack trace ---\n   at System.Text.Json.ThrowHelper.ReThrowWithPath(ReadStack& state, Utf8JsonReader& reader, Exception ex)\n   at System.Text.Json.Serialization.JsonConverter`1.ReadCore(Utf8JsonReader& reader, T& value, JsonSerializerOptions options, ReadStack& state)\n   at System.Text.Json.Serialization.Metadata.JsonTypeInfo`1.Deserialize(Utf8JsonReader& reader, ReadStack& state)\n   at System.Text.Json.JsonSerializer.ReadFromSpan[TValue](ReadOnlySpan`1 utf8Json, JsonTypeInfo`1 jsonTypeInfo, Nullable`1 actualByteCount)\n   at System.Text.Json.JsonSerializer.ReadFromSpan[TValue](ReadOnlySpan`1 json, JsonTypeInfo`1 jsonTypeInfo)\n   at System.Text.Json.JsonSerializer.Deserialize[TValue](String json, JsonSerializerOptions options)\n   at Microsoft.SemanticKernel.Plugins.Core.CodeInterpreter.SessionsPythonPlugin.ExecuteCodeAsync(String code, CancellationToken cancellationToken)\n   at Microsoft.SemanticKernel.KernelFunctionFromMethod.<>c__DisplayClass25_0.<<GetReturnValueMarshalerDelegate>b__11>d.MoveNext()\n--- End of stack trace from previous location ---\n   at Microsoft.SemanticKernel.KernelFunction.<>c__DisplayClass32_0.<<InvokeAsync>b__0>d.MoveNext()\n--- End of stack trace from previous location ---\n   at Microsoft.SemanticKernel.Kernel.InvokeFilterOrFunctionAsync(NonNullCollection`1 functionFilters, Func`2 functionCallback, FunctionInvocationContext context, Int32 index)\n   at SKSpikes.Experiments.AgentChat.LoggingFunctionInvocationFilter.OnFunctionInvocationAsync(FunctionInvocationContext context, Func`2 next) in C:\\_cfdev\\semantic-kernel-spikes\\Experiments\\AgentChat\\LoggingFunctionInvocationFilter.cs:line 15\n   at Microsoft.SemanticKernel.Kernel.InvokeFilterOrFunctionAsync(NonNullCollection`1 functionFilters, Func`2 functionCallback, FunctionInvocationContext context, Int32 index)\n   at Microsoft.SemanticKernel.Kernel.OnFunctionInvocationAsync(KernelFunction function, KernelArguments arguments, FunctionResult functionResult, Boolean isStreaming, Func`2 functionCallback, CancellationToken cancellationToken)\n   at Microsoft.SemanticKernel.KernelFunction.InvokeAsync(Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)\n\n```\n\n\n**To Reproduce**\nThe following code reproduces the issue 3/10 times.\n\n\n```\n[Fact]\n    [Experimental(\"SKEXP0110\")]\n    public async Task GenerateAnswerWithCsvFile_Flattened()\n    {\n        async Task<string> TokenProvider(CancellationToken ct)\n        {\n            if (_cachedToken is null)\n            {\n                var resource = \"https://acasessions.io/.default\";\n                var credential = new DefaultAzureCredential();\n                var accessToken = await credential.GetTokenAsync(new Azure.Core.TokenRequestContext([resource]), ct)\n                    .ConfigureAwait(false);\n                _cachedToken = accessToken.Token;\n            }\n\n            return _cachedToken;\n        }\n\n        var kernelBuilder = Kernel.CreateBuilder();\n        var openAiConfig = GetService<AzureOpenAiConfig>();\n\n        var settings = new SessionsPythonSettings(\n            Guid.NewGuid().ToString(),\n            new Uri(\n                \"https://eastus.dynamicsessions.io/subscriptions/REDACTED\"));\n\n        kernelBuilder.Services.AddHttpClient();\n        kernelBuilder.Services.AddSingleton(sp\n            => new SessionsPythonPlugin(\n                settings,\n                sp.GetRequiredService<IHttpClientFactory>(),\n                TokenProvider,\n                sp.GetRequiredService<ILoggerFactory>()));\n\n        kernelBuilder.AddAzureOpenAIChatCompletion(\n            openAiConfig.Gpt4oDeploymentName,\n            openAiConfig.Endpoint,\n            openAiConfig.ApiKey,\n            modelId: openAiConfig.Gpt4oDeploymentName\n        );\n\n\n        var kernel = kernelBuilder.Build();\n        kernel.Plugins.AddFromObject(kernel.GetRequiredService<SessionsPythonPlugin>());\n        ChatCompletionAgent agent =\n            new()\n            {\n                Name = \"MyAgent\",\n                Instructions = \"Act as a useful agent\",\n                Kernel = kernel,\n                Arguments =\n                    new KernelArguments(\n                        new OpenAIPromptExecutionSettings\n                        {\n                            FunctionChoiceBehavior =\n                                FunctionChoiceBehavior.Auto()\n                        })\n            };\n\n        var thread = new ChatHistoryAgentThread();\n        var plugin = agent.Kernel.GetRequiredService<SessionsPythonPlugin>();\n        var csvPath = Path.Combine(\"test-data\", \"Employee Data.csv\");\n\n        var uploadFileMetaData = await plugin.UploadFileAsync(\"EmployeeData.csv\", csvPath);\n        Logger.Information(\"File uploaded: {FileName} {FilePath}\", uploadFileMetaData.Name,\n            uploadFileMetaData.FullPath);\n\n        var question =\n            \"Use your code interpreter to answer the question: What is the average age of each employee in the file EmployeeData.csv?\";\n\n        var sb = new StringBuilder();\n        await foreach (var responseUpdate in agent.InvokeStreamingAsync(question, thread))\n            sb.Append(responseUpdate.Message);\n\n\n        Logger.Information(\"Response: {Response}\", sb.ToString());\n    }\n\n```\n\n \nHere is the response returned by the Dynamic Sessions endpoint that causes the above exception.\n\nI suspect the result from Dynamic Sessions is not really successful,  (even though it says success) but I have not investigated further yet\n\n```\n{\n  \"$id\" : \"1\",\n  \"id\" : \"5900409f-f3e7-4cf5-9b9e-7dd900e0f95c\",\n  \"identifier\" : \"afea20c6-f3cb-4d95-923c-8ee1e00a5530\",\n  \"executionType\" : \"Synchronous\",\n  \"status\" : \"Succeeded\",\n  \"result\" : {\n    \"$id\" : \"2\",\n    \"stdout\" : \"\",\n    \"stderr\" : \"\",\n    \"executionResult\" : 33,\n    \"executionTimeInMilliseconds\" : 52\n  }\n}\n\n```\n\n\n\n**Expected behavior**\nThe executionResult should be mapped into the ExecutionResult property.\n\n**Screenshots**\nAs \n\n**Platform**\n Language: C#\n - Source: SK Nuget 1.54\n - AI model: e.g. OpenAI:GPT-4o\n - IDE: Rider\n - OS: Windows\n\n",
    "comments": []
  },
  {
    "issue_number": 12347,
    "title": ".Net: Bug: Gemini Responses Don't Deserialize (calling functions)",
    "author": "DevEnable",
    "state": "open",
    "created_at": "2025-06-03T01:45:04Z",
    "updated_at": "2025-06-13T06:51:12Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nThis seems to be an issue which has only started to occur in the last 12-24 hours. Previously the connector was working fine. No changes have been made to the code since. (Apart from updating to the latest version of SK to try and resolve this issue)\n\nI'm guessing the issue is because Google has changed their API contract in some way\n\n**To Reproduce**\n- Create an application that uses the Google connector and either Flash or Gemini 2.5 Pro that calls a function (auto)\n- When SK / the LLM tries to call the function it will fail to deserialise. Running a debugger across it I can see the issue is when it is deserialising the `GeminiPart` and after deserialising the object it calls `IsValid`. This fails as ALL of the parts are null. Here is an obfusciated response I can see from Google using the exception's `Data` dictionary \"ResponseData\":\n\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\": [\n          {\n            \"thought\": true,\n            \"thoughtSignature\": \"xyz==\"\n          },\n          {\n            \"functionCall\": {\n              \"name\": \"MyPlugin_my_function\",\n              \"args\": {\n                \"requests\": [\n                  {\n                    \"MyArg\": \"MyValue\"\n                  }\n                ]\n              }\n            }\n          }\n        ],\n        \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\",\n      \"index\": 0\n    }\n  ],\n...\n}\n\nAs you can se the \"functionCall\" is present. I'm not sure what has changed as I've not retrieved a prior working version.\n\n**Expected behavior**\nThe response from Google should deserialise.\n\n**Platform**\n - Language: C#\n - Source: Microsoft.SemanticKernel.Connectors.Google 1.54.0-alpha (connector, other SK libraries are 1.54.0)\n - AI model: gemini-2.5-flash-preview-05-20, gemini-2.5-pro-preview-05-06. \n\nI just tried gemini-2.5-pro-preview-03-25 and it also has the same problem.\n\n - IDE: Jetbrains Rider\n - OS: Mac\n\n",
    "comments": [
      {
        "user": "DevEnable",
        "body": "Could the issue be related to this? https://github.com/googleapis/python-genai/releases. (See Adding thought_signature field to the Part to store the signature for thoughts. ([303f906](https://github.com/googleapis/python-genai/commit/303f9069f508e544fe2f9c680a700624057b6341)))"
      },
      {
        "user": "RogerBarreto",
        "body": "@DevEnable This issue seems related to C#, current under investigation. Thanks for the heads up!"
      },
      {
        "user": "DevEnable",
        "body": "> [@DevEnable](https://github.com/DevEnable) This issue seems related to C#, current under investigation. Thanks for the heads up!\n\n@RogerBarreto you are probably all across this but looking at the `GeminiPart` and the recent commit from Google's Python SDK it looks like they've introduced a new \"thought\" part and that is what is breaking the deserialisation / logic.\n\nIt hasn't impacted me too badly as I just switched to the Azure Open AI LLMs (yet another great benefit of SK) but each LLM treats prompts differently so it hasn't been a like for like. \n\nI'm not in production at the moment so it's not a big deal. But my concern is later down the line if Google were to do something similar again and introduce another part type. If I were to wait for a fix (which may take days) my platform would seriously impacted (unless I switched LLMs again or tried to figure out the code and created my own fork with a fix) as currently all prompts which rely on function calls (most of mine) would fail.\n\nWithout trying to over-engineer a solution, but to provide some future-proofing. Would it be possible to introduce some sort of configuration variable that instructed the deserialisation logic to ignore any parts it doesn't recognise? Turned off by default so we aren't silently ignoring potential issues, but with the option to turn it on should something like this happen until SK can catch up? It would be a lot less stressful for everyone involved 😃, and help to reduce one of the drawbacks of not directly using Google's SDK / APIs.\n\nIt could be a `bool` or a `string[]` (of part names to ignore).\n\nIf not and I am massively blowing out the scope, then I'll make sure to tag your PR so I can hopefully quickly fix it again in the future should Google make a similar set of changes. "
      }
    ]
  },
  {
    "issue_number": 10931,
    "title": ".Net: Difference in duration values between OpenTelemetry metrics and traces",
    "author": "merveozbay",
    "state": "open",
    "created_at": "2025-03-12T07:31:45Z",
    "updated_at": "2025-06-13T02:14:48Z",
    "labels": [
      "bug",
      ".NET",
      "stale",
      "telemetry"
    ],
    "body": "Hello,\n\nI instrumented my service as mentioned at the documentation [Observability in Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/concepts/enterprise-readiness/observability/?pivots=programming-language-csharp) to emit metrics and traces. \n\nWhen I checked the metrics and traces, I saw that 'semantic_kernel_function_invocation_duration_seconds_sum' metric was sometimes different from what came in trace. \n\nFor example, the duration of one of my kernel functions appears as 12.85 ms in the trace, but while in the Prometheus, it seems 0.00310 sec. Similarly, the one which appears as 28.04ms in the trace seems 0.00729 in the metrics. But, there is no problem with those which appear in seconds in the trace like 1.49 sec. I also see this as 1.49 sec in the metric. \n\n What could be the reason for this difference?\n\nThank you!\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10963,
    "title": ".Net: Enable multiple Data Sources in AzureOpenAIPromptExecutionSettings class",
    "author": "dmytrostruk",
    "state": "open",
    "created_at": "2025-03-13T17:56:01Z",
    "updated_at": "2025-06-13T02:14:47Z",
    "labels": [
      ".NET",
      "ai connector",
      "sk team issue",
      "stale"
    ],
    "body": "- [ ] Deprecate existing `AzureChatDataSource` property\n- [ ] Add new `ChatDataSources` property, which should be a collection of `Azure.AI.OpenAI.Chat.ChatDataSource` instances\n- [ ] Update the examples and show how to use multiple data sources",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 12466,
    "title": ".Net: Add previous messages support to RAG AIContexProvider",
    "author": "westey-m",
    "state": "open",
    "created_at": "2025-06-12T17:00:58Z",
    "updated_at": "2025-06-12T17:01:25Z",
    "labels": [
      ".NET",
      "agents",
      "memory"
    ],
    "body": "Add the ability for the AIContexProvider to keep previous messages in memory to increase the context for RAG search.",
    "comments": []
  },
  {
    "issue_number": 12443,
    "title": "Bug: OpenAPI plugin function name parsing results in invalid kernel function name",
    "author": "TaoChenOSU",
    "state": "open",
    "created_at": "2025-06-10T20:38:39Z",
    "updated_at": "2025-06-12T16:38:37Z",
    "labels": [
      "bug",
      "python"
    ],
    "body": "**Describe the bug**\nThe fallback mechanism in handling an OpenAPI operation that doesn't have an operation ID may result in invalid kernel function name: https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/connectors/openapi_plugin/openapi_parser.py#L239\n\n```\nTraceback (most recent call last):\n  File \"C:\\Users\\taochen\\Project\\semantic-kernel\\python\\semantic_kernel\\functions\\kernel_function_from_method.py\", line 69, in __init__\n    metadata = KernelFunctionMetadata(\n               ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\taochen\\Project\\semantic-kernel\\python\\.venv\\Lib\\site-packages\\pydantic\\main.py\", line 253, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for KernelFunctionMetadata\nname\n  String should match pattern '^[0-9A-Za-z_-]+$' [type=string_pattern_mismatch, input_value='/anything_patch', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/string_pattern_mismatch\n```\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to sample: https://github.com/microsoft/semantic-kernel/tree/main/python/samples/concepts/plugins/openapi\n2. Open [openapi.yaml](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/concepts/plugins/openapi/openapi.yaml)\n3. Comment out line 11\n4. Run [openapi_client.py](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/concepts/plugins/openapi/openapi_client.py)\n\n**Expected behavior**\nThe plugin is successfully added to the kernel.\n\n**Platform**\n - Language: Python\n\n**Additional context**\nWe should either require the `operationId` attribute or perform some more meaningful parsing and a more targeted error message.",
    "comments": [
      {
        "user": "sshandilya1991",
        "body": "hey @TaoChenOSU / @moonbox3 \nI surmise this is due to [this](https://github.com/microsoft/semantic-kernel/blob/ef912a9e66592893d5ec26a1c5d09e8fb99eafc3/python/semantic_kernel/connectors/openapi_plugin/openapi_parser.py#L259)\n\nI can raise a fix for this - but here is a question:\n- do we want to fail/raise if operationId is not present ?  If yes, then is it the right place to fail or we should fail a little before in parsing logic.\n- if we don't want to raise/throw (assuming operationId is not obtained from details object) - the variable `operationId` would be something like `/users_get` which is going to fail the regex validation.  So, question is to either to make the regex validation flexible or making sure `operationId` in details exists or maybe something else that I may be missing.\n\nthoughts?\n\nEdit:\nI see that anyway we are directly using operation.id in further functions assuming it's there anyway - so, IMO, we should simply have a validation for it & raise if it's not there or probably create one if not there in openapispec(wild thought) ?"
      }
    ]
  },
  {
    "issue_number": 12363,
    "title": ".Net: Bug: InMemory Connector is not StrongName signed and won't work for .Net Framework target SDKs",
    "author": "e1em3ntoDX",
    "state": "closed",
    "created_at": "2025-06-04T08:48:10Z",
    "updated_at": "2025-06-12T11:11:48Z",
    "labels": [
      "bug",
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "Hi,\n\nThis issue is pretty similar to the one I found here: https://github.com/microsoft/semantic-kernel/issues/8928. \n\nWe at DevExpress are now implementing the semantic search feature for our Grid Controls using the `Microsoft.Extensions.VectorData.Abstractions` nuget package/assembly functionality. We reached the stage of implementing a demonstration example and chose the [InMemory connector](https://www.nuget.org/packages/Microsoft.SemanticKernel.Connectors.InMemory/1.55.0-preview) for these purposes. As our product demos are now mostly compiled against the .NET FW 4.6.2, we have to use the assembly, but the application does not start due to the exception.\n\n<img width=\"660\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/157ba0f7-cab5-4e11-8310-6973a444ee84\" />\n\nI didn’t include the repro steps because the issue seems straightforward, but please let me know if you still need them and the example demonstrating it. \n\nPlease let us know if you have plans to sign the assembly, and if so, is there any ETA available? We have the major product release coming in a few weeks, so the ETA might affect our future plans in this regard.\n\nThank you\n",
    "comments": [
      {
        "user": "westey-m",
        "body": "Hi @e1em3ntoDX, thanks for raising this issue. We have a fix and are aiming to put out a new release by Tuesday next week with signed dlls for .net Framework."
      },
      {
        "user": "e1em3ntoDX",
        "body": "> Hi [@e1em3ntoDX](https://github.com/e1em3ntoDX), thanks for raising this issue. We have a fix and are aiming to put out a new release by Tuesday next week with signed dlls for .net Framework.\n\nHi @westey-m, thanks for the quick response. Looking forward to your next release!"
      },
      {
        "user": "westey-m",
        "body": "@e1em3ntoDX, we released yesterday, and this issue should be fixed now.  If you are still having issues, please let me know."
      }
    ]
  },
  {
    "issue_number": 12257,
    "title": "GeminiRequest are not full",
    "author": "VladyslavLishchyna",
    "state": "closed",
    "created_at": "2025-05-23T14:05:48Z",
    "updated_at": "2025-06-12T09:16:31Z",
    "labels": [
      "ai connector"
    ],
    "body": "---\nname: Feature request\nabout: GeminiRequest are not full, don`t contains \"labels\" field\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n\nAccording to this documentation https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference?_gl=1*1qhhhsi*_ga*MTcwMjMzNTkxOS4xNzE4OTY0MzU0*_ga_WH2QY8WWF5*czE3NDc5ODg4MzEkbzE1NyRnMSR0MTc0Nzk5MDE1MSRqNTAkbDAkaDAkZFIzUlVsTDBEVmRsUGdUc2xUbWI4NFpJZTU5WWNCSjVVYmc \nrequest to Gemini contains \"labels\" so please add addition property \"labels\" to GeminiRequest and some possibility to add labels to requests",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "@VladyslavLishchyna would you be interested in contributing this change to Semantic Kernel?"
      },
      {
        "user": "shethaadit",
        "body": "Hi @markwallace-microsoft, @RogerBarreto, I have raised PR for this. Please let me know if anything is missed. :)"
      },
      {
        "user": "VladyslavLishchyna",
        "body": "Hi thanks for PR but actually it`s not working (\n\nI add labels like i[n this PR on tests ](https://github.com/microsoft/semantic-kernel/pull/12300/commits/4fe3523e9b0b086df5b5b419197adcf10c7b6af4#diff-13e8a90119ba70138b9b1fd9021e82d7b566cfeec4da469b9d8071d8e5df551dR529)\n` \n   var executionSettings = new GeminiPromptExecutionSettings\n   {\n      Labels = \"Key1:Value1\"\n   };\n`\n\nThe exception is :\n```\n{\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid value at 'labels' (type.googleapis.com/google.cloud.aiplatform.v1.GenerateContentRequest.LabelsEntry), \\\"gpd-service-name:gpd-typeform\\\"\",\n    \"status\": \"INVALID_ARGUMENT\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n        \"fieldViolations\": [\n          {\n            \"field\": \"labels\",\n            \"description\": \"Invalid value at 'labels' (type.googleapis.com/google.cloud.aiplatform.v1.GenerateContentRequest.LabelsEntry), \\\"gpd-service-name:gpd-typeform\\\"\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\nif I do like this\n```\nprotected override async Task<HttpResponseMessage> SendAsync(HttpRequestMessage request, CancellationToken cancellationToken)\n{\n\tvar content = await request.Content?.ReadAsStringAsync(cancellationToken)!;\n\tvar jsonObject = JsonObject.Parse(content)!;\n\tjsonObject[\"labels\"] = new JsonObject { [\"gpd-service-name\"] = options.Value.ApplicationName };\n\trequest.Content = new StringContent(jsonObject.ToString());\n\n\treturn await base.SendAsync(request, cancellationToken);\n}\n```\nEverything works fine. So maybe possible to do Lables in GeminiPromptExecutionSettings as dictionary and serialize it"
      }
    ]
  },
  {
    "issue_number": 10976,
    "title": ".Net: New Feature: Implement Responses API with Web Search Tool and File Search Tool",
    "author": "crodriguezSofteng",
    "state": "open",
    "created_at": "2025-03-14T12:05:05Z",
    "updated_at": "2025-06-12T00:55:41Z",
    "labels": [
      ".NET",
      "ai connector"
    ],
    "body": "---\nname: Implement Responses API with Web Search Tool and File Search Tool\nabout: OpenAI recently released Responses API, a new api that is a superset of ChatCompletions API that allows Web Search Tool and File Search Tool\n\nhttps://platform.openai.com/docs/quickstart?api-mode=responses\nhttps://openai.com/index/new-tools-for-building-agents/\n\nSpecially the first one, automatic Web Search Tool it's very powersfull. Please implement this new API with Semantic Kernel (specially dotnet :P )\n\nThanks\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n",
    "comments": [
      {
        "user": "jwasserzug",
        "body": "Hey, is there an expected date for semantic kernel support for the Responses API?"
      },
      {
        "user": "readingdancer",
        "body": "As no one has replied to this thread yet, @markwallace-microsoft do you have any idea if/when this is going to be added to SK C# version, not Python...  or are we better off just talking directly to the API at this stage?"
      }
    ]
  },
  {
    "issue_number": 12454,
    "title": "Python: Bug: Python & .Net Response API based Agent doesn't have a Service to register it to AgentGroupChat / GroupChat",
    "author": "ian-t-adams",
    "state": "closed",
    "created_at": "2025-06-11T16:01:43Z",
    "updated_at": "2025-06-12T00:18:41Z",
    "labels": [
      "bug",
      ".NET",
      "python"
    ],
    "body": "**Describe the bug**\nThere is a responses agent in semantic_kernel.agents but I not an equivalent to AzureChatCompletion in semantic_kernel.connectors.ai.open_ai.  This means I can't create a service for the Kernel, so are response based agents are only possible as isolated agents. A corresponding connectors.ai.open_ai is required to register them to work within a GroupChat / AgentGroupChat.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. In Python or .Net create an AzureResponseAgent https://learn.microsoft.com/en-us/python/api/semantic-kernel/semantic_kernel.agents.open_ai.azure_responses_agent.azureresponsesagent?view=semantic-kernel-python\n2. Attempt to register the AzureResponseAgent with the corresponding Service but it's not available https://learn.microsoft.com/en-us/python/api/semantic-kernel/semantic_kernel.connectors.ai.open_ai.services?view=semantic-kernel-python\n3. Error: Cannot add the newly created AzureResponseAgent to an AgentGroupChat or GroupChat Kernel.\n\n**Expected behavior**\nThe ability to use an AzureResponseAgent in either the AgentGroupChat or GroupChat objects,\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Platform**\n - Language: C#, Python\n - Source: https://www.nuget.org/packages/Microsoft.SemanticKernel/, https://pypi.org/project/semantic-kernel/ pip version 1.33.0\n - AI model: GPT-4.1, GPT-4.1-mini\n - IDE: VS Code / All\n - OS: Windows and Mac\n\n**Additional context**\nResponses API handles truncation issues better and a AzureResponseAgent is required to utilize the responses api.",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "@ian-t-adams This omission is intentional. We have introduced new multi-agent orchestration patterns, see: https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-orchestration/?pivots=programming-language-csharp. This is one of the first steps in our convergence with AutoGen.\n\nOur plan is to phase out the current experimental agent orchestration support in favour of this new pattern."
      },
      {
        "user": "moonbox3",
        "body": "@ian-t-adams for Python, please have a look at the following orchestration patterns: https://github.com/microsoft/semantic-kernel/tree/main/python/samples/getting_started_with_agents/multi_agent_orchestration. This is what Mark called out.\n\nThere's no further work here on this issue. I will close it. If you experience an issue using the new orchestration patterns, please file a new issue. Thank you."
      }
    ]
  },
  {
    "issue_number": 12232,
    "title": "Memory Management and Plugin: MagenticOrchestration",
    "author": "mjnong",
    "state": "open",
    "created_at": "2025-05-22T06:14:13Z",
    "updated_at": "2025-06-11T20:08:48Z",
    "labels": [
      "agents"
    ],
    "body": "## Problem\n\nI’m experimenting with the new orchestration APIs in Semantic Kernel and I’ve run into a few limitations:\n\n1. **No history insertion/retrieval**  \n   There doesn’t seem to be any way to insert or fetch conversation history (e.g., by thread or session ID). Is this functionality available, or am I missing something?\n\n2. **No plugin support at the Orchestrator level**  \n   While agents can register plugins (e.g., `TimePlugin()`), I’d love to be able to attach plugins directly to the Orchestrator so they could be shared across all orchestrations.\n\n3. **No streaming output**  \n   I haven’t found a way to stream generated content back to the caller. Is streaming support on the roadmap?\n\n---\n\n## What I’m looking for\n- **Memory / Context management**  \n  - Ability to persist and retrieve arbitrarily-keyed context or history for an orchestration.\n\n- **Plugins inside the Orchestrator**  \n  - A mechanism to register and invoke plugins (e.g., logging, timers, data enrichment) directly on the orchestrator instance.\n\n- **Streaming responses**  \n  - Support for chunked or real-time streaming of content generated by the orchestration.\n---\nIf any of these are already supported or there are recommended workarounds, I’d appreciate pointers. Otherwise, are there plans to add these capabilities?\n\nThanks!\n— Marcus",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Thank you for the feedback, @mjnong. Adding @TaoChenOSU."
      },
      {
        "user": "mjnong",
        "body": "I'm also wondering whether or not this is working with AzureAIAgents because I didn't get it to work."
      },
      {
        "user": "seizeuniverse",
        "body": "I am also interested how to make it work with AzureAIAgents and + history context into this orch."
      }
    ]
  },
  {
    "issue_number": 10738,
    "title": ".Net: Bug: AWS Bedrock Connector - Cross-region inference Not Supported",
    "author": "HallianTech-ChiefEngineer",
    "state": "open",
    "created_at": "2025-02-28T15:02:52Z",
    "updated_at": "2025-06-11T17:52:17Z",
    "labels": [
      "bug",
      ".NET",
      "stale"
    ],
    "body": "**Describe the bug**\nwhen using the cross inference profileid, I receive this error: An error occurred while initializing the BedrockChatCompletionService: Unsupported model provider: us\n\nWhen using a non cross inference profileid, just the modelid, I get this message from AWS:  Invocation of model ID anthropic.claude-3-5-sonnet-20241022-v2:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.\n\n**To Reproduce**\nSteps to reproduce the behavior:\nkernelBuilder.AddBedrockChatCompletionService(\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\");\n\n**Expected behavior**\nI would expect either the cross region profile id to work with the semanic kernal when ondemand does not work.\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Platform**\n - Language: C# \n - Source: Microsoft.SemanticKernel.Connectors.Amazon 1.38.0-alpha\n - AI model: us.anthropic.claude-3-5-sonnet-20241022-v2:0\n - IDE: Visual Studio\n - OS: Windows\n\n**Additional context**\nUsing this with C#, dot net core 8, Blazor Server",
    "comments": [
      {
        "user": "lspinheiro",
        "body": "I'm currently facing the same issue for python. Here is my configuration and the error.\n\n```py\nbedrock_runtime_client = boto3.client(service_name='bedrock-runtime', config=my_config)\nbedrock_client = boto3.client(\"bedrock\", config=my_config)\n\nsk_client = BedrockChatCompletion(\n    model_id='us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n    runtime_client=bedrock_runtime_client,\n    client=bedrock_client,\n)\n\n# Configure execution settings\nsettings = BedrockChatPromptExecutionSettings(\n    temperature=0.7,\n    max_tokens=1000,\n)\n```\n\n```sh\n  File \"/Users/leonardopinheiro/git/autogen/python/packages/autogen-ext/src/autogen_ext/models/semantic_kernel/_sk_chat_completion_adapter.py\", line 565, in create_stream\n    async for streaming_messages in self._sk_client.get_streaming_chat_message_contents(\n  File \"/Users/leonardopinheiro/git/autogen/python/.venv/lib/python3.11/site-packages/semantic_kernel/connectors/ai/chat_completion_client_base.py\", line 261, in get_streaming_chat_message_contents\n    async for streaming_chat_message_contents in self._inner_get_streaming_chat_message_contents(\n  File \"/Users/leonardopinheiro/git/autogen/python/.venv/lib/python3.11/site-packages/semantic_kernel/utils/telemetry/model_diagnostics/decorators.py\", line 164, in wrapper_decorator\n    async for streaming_chat_message_contents in completion_func(*args, **kwargs):\n  File \"/Users/leonardopinheiro/git/autogen/python/.venv/lib/python3.11/site-packages/semantic_kernel/connectors/ai/bedrock/services/bedrock_chat_completion.py\", line 134, in _inner_get_streaming_chat_message_contents\n    model_info = await self.get_foundation_model_info(self.ai_model_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/leonardopinheiro/git/autogen/python/.venv/lib/python3.11/site-packages/semantic_kernel/connectors/ai/bedrock/services/bedrock_base.py\", line 24, in get_foundation_model_info\n    response = await run_in_executor(\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/leonardopinheiro/git/autogen/python/.venv/lib/python3.11/site-packages/semantic_kernel/connectors/ai/bedrock/services/model_provider/utils.py\", line 26, in run_in_executor\n    return await asyncio.get_event_loop().run_in_executor(executor, partial(func, *args, **kwargs))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/leonardopinheiro/.pyenv/versions/3.11.8/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/leonardopinheiro/git/autogen/python/.venv/lib/python3.11/site-packages/botocore/client.py\", line 569, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/leonardopinheiro/git/autogen/python/.venv/lib/python3.11/site-packages/botocore/client.py\", line 1023, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the GetFoundationModel operation: The provided model identifier is invalid.\n```"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      },
      {
        "user": "henriqueholtz",
        "body": "Similar here if using any model `{...}-v2:0`:\n\n```{...} Reason: The provided model identifier is invalid {...}```\n\nThe work around is use the \"same\" model but using  `{...}-v1:0`. E.g.:\n\n- `us.anthropic.claude-3-5-haiku-20241022-v1:0 ` => OK\n- `us.anthropic.claude-3-5-haiku-20241022-v2:0` => ERROR\n\nYet, not sure if it's a Semantic Kernel error or it's something on AWS's side."
      }
    ]
  },
  {
    "issue_number": 10339,
    "title": "Python: azure ai service model invoke fails with no_model_name error code in sk python",
    "author": "PurnaChandraPanda",
    "state": "open",
    "created_at": "2025-01-29T16:04:16Z",
    "updated_at": "2025-06-11T14:07:11Z",
    "labels": [
      "python"
    ],
    "body": "## Code snippet\n```\nimport os\nimport asyncio\nfrom semantic_kernel.contents.chat_history import ChatHistory\nfrom semantic_kernel.connectors.ai.prompt_execution_settings import PromptExecutionSettings\nfrom semantic_kernel.connectors.ai.azure_ai_inference import AzureAIInferenceChatCompletion\nfrom semantic_kernel.connectors.ai.azure_ai_inference import AzureAIInferenceChatPromptExecutionSettings\nfrom azure.ai.inference import ChatCompletionsClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom dotenv import load_dotenv\n\nload_dotenv(\"../.env\")\n\nllm = AzureAIInferenceChatCompletion(\n    endpoint=os.getenv(\"AZUREAI_INFERENCE_ENDPOINT\"), # \"https://{myresource}.services.ai.azure.com/models\"\n    api_key=os.getenv(\"AZUREAI_ENDPOINT_KEY\"),\n    ai_model_id=\"phi-4\",\n    service_id=\"phi-4\",\n)\n\n# execution_settings = AzureAIInferenceChatPromptExecutionSettings(\n#     max_tokens=100,\n#     temperature=0.5,\n#     top_p=0.9,\n#     service_id=\"phi-4\",\n#     model_id=\"phi-4\",\n#     # extra_parameters={...},    # model-specific parameters\n# )\n\nexecution_settings = PromptExecutionSettings(model=\"phi-4\")\n\nasync def main():\n    chat_history = ChatHistory(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\n    \n    response = await llm.get_chat_message_content(chat_history, \n                                                  execution_settings,\n                                                  headers={\"x-ms-model-mesh-model-name\": \"phi-4\"},\n                                                  )\n    print(response)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n## Error details\n```\nTraceback (most recent call last):\n  File \"/afh/projects/aiproj01-ea1be05d-2ca9-4751-94bd-64549ebf820f/shared/Users/pupanda/ai-foundation-models/ai-inference/phi/phi4-sk.py\", line 41, in <module>\n    asyncio.run(main())\n  File \"/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/afh/projects/aiproj01-ea1be05d-2ca9-4751-94bd-64549ebf820f/shared/Users/pupanda/ai-foundation-models/ai-inference/phi/phi4-sk.py\", line 34, in main\n    response = await llm.get_chat_message_content(chat_history,\n  File \"/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/semantic_kernel/connectors/ai/chat_completion_client_base.py\", line 197, in get_chat_message_content\n    results = await self.get_chat_message_contents(chat_history=chat_history, settings=settings, **kwargs)\n  File \"/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/semantic_kernel/connectors/ai/chat_completion_client_base.py\", line 142, in get_chat_message_contents\n    return await self._inner_get_chat_message_contents(chat_history, settings)\n  File \"/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/semantic_kernel/connectors/ai/azure_ai_inference/services/azure_ai_inference_chat_completion.py\", line 127, in _inner_get_chat_message_contents\n    response: ChatCompletions = await self.client.complete(\n  File \"/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/inference/aio/_patch.py\", line 670, in complete\n    raise HttpResponseError(response=response)\nazure.core.exceptions.HttpResponseError: (no_model_name) No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header.\nCode: no_model_name\nMessage: No model specified in request. Please provide a model name in the request body or as a x-ms-model-mesh-model-name header.\nUnclosed client session\nclient_session: <aiohttp.client.ClientSession object at 0x7fb32ae6c850>\nUnclosed connector\nconnections: ['deque([(<aiohttp.client_proto.ResponseHandler object at 0x7fb322838640>, 2558.922774265)])']\nconnector: <aiohttp.connector.TCPConnector object at 0x7fb32ae6c9a0>\n```\n\n\nThis error of no_model_name comes from azure-ai-inference in SK. SK wrapper is not able to pass model vaue to ChatcompletionClient. So, when client.complete() tried, it just says model name info is missing. \n\nIt looks like a bug. Does anyone have any insights if thing can be addressed from client side?",
    "comments": [
      {
        "user": "moonbox3",
        "body": "@TaoChenOSU any thoughts on this?"
      },
      {
        "user": "TaoChenOSU",
        "body": "Hi @PurnaChandraPanda,\n\nThe `ai_model_id` doesn't point the connector to the specified model. The `ai_model_id` is a parameter users can use for identifying what model the connector is created with in the case of `AzureAIInference`. \n\nFor Azure AI Inference, your endpoint should point directly to your model. It should look something like this: `https://Phi-4-xxxxx.southcentralus.models.ai.azure.com` if you're using serverless compute."
      },
      {
        "user": "PurnaChandraPanda",
        "body": "Hello @TaoChenOSU, @moonbox3 \n\nThank you for the response.\n\nI am using the \"azure ai model service endpoint\" in ai foundry - https://learn.microsoft.com/en-us/azure/ai-foundry/model-inference/how-to/inference?tabs=python#using-the-routing-capability-in-the-azure-ai-model-inference-endpoint. As per which, the base_url would appear as `https://{myresource}.services.ai.azure.com/models`. \n\n<img width=\"398\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6ce96067-fb03-4382-bfbc-8bb918d6fece\" />\n\nCould you please try the same in your end? You would notice the same error as me.\n\nOn ai-model-id, i just passed the model name here, which is phi-4 in my case. Shall i pass a different value here otherwise?"
      }
    ]
  },
  {
    "issue_number": 12449,
    "title": "Python: Bug: CopilotStudioAgent in Orchestration gets stuck, always timeout",
    "author": "angandin",
    "state": "open",
    "created_at": "2025-06-11T08:33:32Z",
    "updated_at": "2025-06-11T08:33:53Z",
    "labels": [
      "bug",
      "python",
      "triage"
    ],
    "body": "**Describe the bug**\nUsing a CopilotStudioAgent in any orchestration pattern does not get any response, even after waiting tens of minutes. Using the same exact Agent with _.get_response_, it works perfectly.\n\n**To Reproduce**\n1. Use CopilotStudioAgent class ([https://github.com/microsoft/semantic-kernel/blob/main/python/samples/getting_started_with_agents/copilot_studio/step1_copilot_studio_agent_simple.py](url)) to define the CopilotStudioAgent\n2. Use the ConcurrentOrchestration pattern ([https://github.com/microsoft/semantic-kernel/blob/main/python/samples/getting_started_with_agents/multi_agent_orchestration/step1_concurrent.py](url)), with just one agent defined in step 1\n3. Execute code: _value = await orchestration_result.get(timeout=20)_ will get stuck indefenitely\n\n**Expected behavior**\nGet result from orchestration_result.get() method, a text from the CopilotStudioAgent\n\n**Platform**\n - Language: Python\n - Source: pip semantic-kernel 1.32.2, pip microsoft-agents-copilotstudio-client 0.0.0a3\n - AI model: OpenAI gpt-4o\n - IDE: Visual Studio Code\n - OS: Windows",
    "comments": []
  },
  {
    "issue_number": 12398,
    "title": ".Net: Bug: Copilot Studio Documentation",
    "author": "devinleighsmith",
    "state": "closed",
    "created_at": "2025-06-06T04:40:12Z",
    "updated_at": "2025-06-11T01:52:39Z",
    "labels": [
      "bug",
      ".NET",
      "documentation"
    ],
    "body": "**Describe the bug**\nIt appears the documentation for:\nhttps://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/Agents/Copilot/README.md?plain=1\n\nmay be out-of-date\n\nThe nuget package listed does not appear to exist, even as a pre-release:\nhttps://www.nuget.org/packages?q=Microsoft.SemanticKernel.Agents.CopilotStudio\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Execute command listed in documentation: dotnet add package Microsoft.SemanticKernel.Agents.CopilotStudio --prerelease\n2. Get the following error from the Package Manager Console: error: There are no versions available for the package 'Microsoft.SemanticKernel.Agents.CopilotStudio'.\n3. Check nuget.org, also no package by that name.\n\n**Expected behavior**\nThe package should install, and exist in nuget.org \n\n**Platform**\nVisual Studio 2022 Community",
    "comments": [
      {
        "user": "crickman",
        "body": "Hi @devinleighsmith, we tend to publish new packages weekly.  I'd expect the current state of `main` to be available when we publish tomorrow.  The code needs to precede the package, so there's naturally a small gap."
      },
      {
        "user": "crickman",
        "body": "https://www.nuget.org/packages/Microsoft.SemanticKernel.Agents.CopilotStudio/1.56.0-alpha"
      },
      {
        "user": "devinleighsmith",
        "body": "Thanks very much! Sorry for the bother, didn't realize this was pending."
      }
    ]
  },
  {
    "issue_number": 11720,
    "title": ".Net: Bring Support for Azure OpenAI gpt-4o audio responses",
    "author": "Cobra86",
    "state": "open",
    "created_at": "2025-04-24T19:54:37Z",
    "updated_at": "2025-06-11T01:09:25Z",
    "labels": [
      ".NET",
      "ai connector"
    ],
    "body": "**Describe the bug**\nWhen using GPT-4o audio-preview with Semantic Kernel, the audio response isn't return  The code runs without errors, but no sound is returned from the AI response.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n\n1. Create a new .NET 9 project\n2. Install Microsoft.SemanticKernel v1.47.0\n3. Configure SDK with Azure OpenAI and the gpt-4o-audio-preview model\n4. Set up audio input from microphone using NAudio\n5. Request both text and audio responses with ChatResponseModalities.Text | ChatResponseModalities.Audio\n6. No Audio response and only text.\n\n**Expected behavior**\nThe AI should respond both with text (which works) and audio (which doesn't return).\n\n**Screenshots**\nnot applicable.\n\n**Platform**\n\n- Language: C#\n- Source: NuGet package Microsoft.SemanticKernel version 1.47.0\n- AI model: Azure OpenAI gpt-4o-audio-preview\n- IDE: Visual Studio\n- OS: Windows\n\n**Additional context**\nI followed the example from https://devblogs.microsoft.com/semantic-kernel/using-openais-audio-preview-model-with-semantic-kernel/ and implemented microphone input using NAudio. The text response works correctly.",
    "comments": [
      {
        "user": "Cobra86",
        "body": "I just wanted to add that it's working fine with OpenAI api"
      },
      {
        "user": "SamArmand",
        "body": "I can confirm this issue is still a problem. I'm using the same tutorial. I also tried gpt-4o-mini-audio-preview"
      }
    ]
  },
  {
    "issue_number": 12437,
    "title": "Python: Bug: ValueError: Unknown field for Schema: title",
    "author": "MadhanMohanReddy2301",
    "state": "open",
    "created_at": "2025-06-10T14:02:44Z",
    "updated_at": "2025-06-11T00:01:30Z",
    "labels": [
      "bug",
      "python",
      "triage"
    ],
    "body": "**Describe the bug**\n🔹 Unknown field for Schema: title occurs because MCP plugin schemas include a \"title\" attribute that google-generativeai's protobuf marshaller doesn't expect, causing to_proto() to fail—remove that field to fix it\n\n**To Reproduce**\nSteps to reproduce the behavior:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom semantic_kernel.functions import KernelArguments\nfrom semantic_kernel import Kernel\nfrom semantic_kernel.connectors.mcp import MCPSsePlugin\nfrom semantic_kernel.connectors.ai import FunctionChoiceBehavior\nfrom semantic_kernel.agents import ChatCompletionAgent,ChatHistoryAgentThread\nfrom semantic_kernel.connectors.ai.google.google_ai import GoogleAIChatCompletion\n\nfrom semantic_kernel.services.kernel_services_extension import DEFAULT_SERVICE_NAME\n\nload_dotenv()\nchat_completion = GoogleAIChatCompletion(gemini_model_id=\"gemini-1.5-flash\",api_key=os.getenv(\"GOOGLE_API_KEY\"),service_id=DEFAULT_SERVICE_NAME)\nAGENT_NAME =\"Sql_Agent\"\n\nclass SQL_Agent:\n    async def get_agent(self):\n        agent_kernal = Kernel()\n        agent_kernal.add_service(chat_completion)\n        settings = agent_kernal.get_prompt_execution_settings_from_service_id(chat_completion.service_id)\n        settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n        agent_prompt = \"\"\"you are a sql agent, have to answer the user query using the db \n        \"\"\"\n        plugin= MCPSsePlugin(name=\"sql\",\n            url=\"http://localhost:8000/sse\"\n        )\n        await plugin.connect()\n        agent = ChatCompletionAgent(\n            kernel=agent_kernal,\n            name=AGENT_NAME,\n            instructions=agent_prompt,\n            arguments=KernelArguments(settings=settings),\n            plugins=[plugin]\n        )\n        return agent\n    async def run(self):\n        sql_agent = await self.get_agent()\n        thread: ChatHistoryAgentThread | None = None\n        while True:\n            user_input =input(\"Enter something:\")\n            async for response in sql_agent.invoke(messages=user_input, thread=thread):\n                print(f\"{response.content}\")\n                thread= response.thread\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(SQL_Agent().run())\n```\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Platform**\n - Language: python\n - AI model: gemini-1.5-flash\n\n\n**Additional context**\n- below are the error details\n- C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Scripts\\python.exe C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\agents\\sql_agent\\agent.py \nEnter something:hi\nan error occurred during closing of asynchronous generator <async_generator object sse_client at 0x0000016D3B376440>\nasyncgen: <async_generator object sse_client at 0x0000016D3B376440>\n  + Exception Group Traceback (most recent call last):\n  |   File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\mcp\\client\\sse.py\", line 155, in sse_client\n    |     yield read_stream, write_stream\n    | GeneratorExit\n    | \n    | During handling of the above exception, another exception occurred:\n    | \n    | Traceback (most recent call last):\n    |   File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\mcp\\client\\sse.py\", line 58, in sse_client\n    |     async with aconnect_sse(\n    |                ^^^^^^^^^^^^^\n    |   File \"C:\\Users\\reddy\\anaconda3\\Lib\\contextlib.py\", line 267, in __aexit__\n    |     raise RuntimeError(\"generator didn't stop after athrow()\")\n    | RuntimeError: generator didn't stop after athrow()\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\mcp\\client\\sse.py\", line 54, in sse_client\n    async with anyio.create_task_group() as tg:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 778, in __aexit__\n    if self.cancel_scope.__exit__(type(exc), exc, exc.__traceback__):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 457, in __exit__\n    raise RuntimeError(\nRuntimeError: Attempted to exit cancel scope in a different task than it was entered in\nTraceback (most recent call last):\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\proto\\marshal\\rules\\message.py\", line 36, in to_proto\n    return self._descriptor(**value)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Parameter to CopyFrom() must be instance of same class: expected <class 'Schema'> got <class 'dict'>.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\proto\\marshal\\rules\\message.py\", line 36, in to_proto\n    return self._descriptor(**value)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: Protocol message Schema has no \"title\" field.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\agents\\sql_agent\\agent.py\", line 50, in <module>\n    asyncio.run(SQL_Agent().run())\n  File \"C:\\Users\\reddy\\anaconda3\\Lib\\asyncio\\runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\anaconda3\\Lib\\asyncio\\runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\anaconda3\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\agents\\sql_agent\\agent.py\", line 45, in run\n    async for response in sql_agent.invoke(messages=user_input, thread=thread):\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\agent_diagnostics\\decorators.py\", line 39, in wrapper_decorator\n    async for response in invoke_func(*args, **kwargs):\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\chat_completion\\chat_completion_agent.py\", line 328, in invoke\n    async for response in self._inner_invoke(\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\chat_completion\\chat_completion_agent.py\", line 487, in _inner_invoke\n    responses = await chat_completion_service.get_chat_message_contents(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py\", line 139, in get_chat_message_contents\n    completions = await self._inner_get_chat_message_contents(chat_history, settings)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\model_diagnostics\\decorators.py\", line 112, in wrapper_decorator\n    return await completion_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\google\\google_ai\\services\\google_ai_chat_completion.py\", line 140, in _inner_get_chat_message_contents\n    response: AsyncGenerateContentResponse = await model.generate_content_async(\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\google\\generativeai\\generative_models.py\", line 359, in generate_content_async\n    request = self._prepare_request(\n              ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\google\\generativeai\\generative_models.py\", line 145, in _prepare_request\n    tools_lib = self._get_tools_lib(tools)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\google\\generativeai\\generative_models.py\", line 182, in _get_tools_lib\n    return content_types.to_function_library(tools)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\google\\generativeai\\types\\content_types.py\", line 917, in to_function_library\n    return FunctionLibrary(tools=lib)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\google\\generativeai\\types\\content_types.py\", line 855, in __init__\n    tools = _make_tools(tools)\n            ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\google\\generativeai\\types\\content_types.py\", line 898, in _make_tools\n    tools = [_make_tool(t) for t in tools]\n             ^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\google\\generativeai\\types\\content_types.py\", line 820, in _make_tool\n    return Tool(**tool)\n           ^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\google\\generativeai\\types\\content_types.py\", line 731, in __init__\n    _make_function_declaration(f) for f in function_declarations\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\google\\generativeai\\types\\content_types.py\", line 664, in _make_function_declaration\n    return FunctionDeclaration(**fun)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\google\\generativeai\\types\\content_types.py\", line 572, in __init__\n    self._proto = protos.FunctionDeclaration(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\proto\\message.py\", line 728, in __init__\n    pb_value = marshal.to_proto(pb_type, value)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\proto\\marshal\\marshal.py\", line 235, in to_proto\n    pb_value = self.get_rule(proto_type=proto_type).to_proto(value)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\proto\\marshal\\rules\\message.py\", line 46, in to_proto\n    return self._wrapper(value)._pb\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\proto\\message.py\", line 728, in __init__\n    pb_value = marshal.to_proto(pb_type, value)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\proto\\marshal\\marshal.py\", line 233, in to_proto\n    return {k: self.to_proto(recursive_type, v) for k, v in value.items()}\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\proto\\marshal\\marshal.py\", line 235, in to_proto\n    pb_value = self.get_rule(proto_type=proto_type).to_proto(value)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\proto\\marshal\\rules\\message.py\", line 46, in to_proto\n    return self._wrapper(value)._pb\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\reddy\\PycharmProjects\\Unknown_BackEnd\\.venv\\Lib\\site-packages\\proto\\message.py\", line 724, in __init__\n    raise ValueError(\nValueError: Unknown field for Schema: title\n\nProcess finished with exit code 1\n",
    "comments": []
  },
  {
    "issue_number": 12153,
    "title": ".Net: Unable to reconstruct GeminiChatMessageContent ToolCalls for message history",
    "author": "ionite34",
    "state": "open",
    "created_at": "2025-05-19T03:43:49Z",
    "updated_at": "2025-06-10T19:05:45Z",
    "labels": [
      "bug",
      "PR: in progress",
      ".NET"
    ],
    "body": "Since the `IReadOnlyList<GeminiFunctionToolCall>? ToolCalls` property of `GeminiChatMessageContent` is get only (https://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/Connectors/Connectors.Google/Models/Gemini/GeminiChatMessageContent.cs#L96), and there is no public constructor accepting tool calls like there is for tool call results, it is not possible to recreate past history when loading context involving assistant messages with tool calls. \n\nFor manual tool invocation, (since automatic kernel tool execution is broken when Gemini returns non-first part results currently https://github.com/microsoft/semantic-kernel/issues/11651), it also makes it impossible to recreate the last turn message for responding to the tool use manually when receiving streaming responses (since we need to be able to convert the streaming response to a ChatMessageContent to add it to ChatHistory)\n\n**Likely solution is adding a public constructor to `GeminiChatMessageContent` accepting a `IEnumerable<GeminiFunctionToolCall>`**",
    "comments": []
  },
  {
    "issue_number": 12442,
    "title": "Python: Bug: ValueError: Unknown field for Schema: anyOf",
    "author": "TyagarajN",
    "state": "open",
    "created_at": "2025-06-10T19:05:08Z",
    "updated_at": "2025-06-10T19:05:27Z",
    "labels": [
      "bug",
      "python",
      "triage"
    ],
    "body": "Describe the bug\nWhen using ChatCompletionAgent instances as plugins with GoogleAIChatCompletion service, the application crashes with ValueError: Unknown field for Schema: anyOf during schema conversion from Semantic Kernel to Google AI's protobuf format.\nTo Reproduce\nSteps to reproduce the behavior:\n\nCreate multiple ChatCompletionAgent instances using GoogleAIChatCompletion service\nAdd these agents as plugins to another ChatCompletionAgent using the plugins parameter\nCall get_response() method on the parent agent with any message\nSee error: ValueError: Unknown field for Schema: anyOf\n\nMinimal reproduction code:\npythonimport asyncio\nfrom semantic_kernel.agents import ChatCompletionAgent\nfrom semantic_kernel.connectors.ai.google.google_ai import GoogleAIChatCompletion\n\n# Initialize Google AI chat completion service\nchat_completion = GoogleAIChatCompletion(\n    gemini_model_id=\"gemini-2.0-flash\",\n    api_key=\"YOUR_API_KEY\"\n)\n\n# Create child agents\nbilling_agent = ChatCompletionAgent(\n    service=chat_completion, \n    name=\"BillingAgent\", \n    instructions=\"You handle billing issues.\"\n)\n\nrefund_agent = ChatCompletionAgent(\n    service=chat_completion,\n    name=\"RefundAgent\",\n    instructions=\"Assist users with refund inquiries.\",\n)\n\n# Create parent agent with child agents as plugins\ntriage_agent = ChatCompletionAgent(\n    service=chat_completion,\n    name=\"TriageAgent\",\n    instructions=\"Evaluate user requests and forward them to other agents.\",\n    plugins=[billing_agent, refund_agent],  # This causes the error\n)\n\nasync def main():\n    # This call triggers the error\n    response = await triage_agent.get_response(\n        messages=\"test message\",\n        thread=None,\n    )\n\nasyncio.run(main())\nExpected behavior\nThe triage agent should successfully process the user input and coordinate with the plugin agents to provide a response without any schema-related errors.\nScreenshots\nFull error traceback:\nTraceback (most recent call last):\n  File \"C:\\gen-ai\\agents_poc\\semantic_kernel\\venv\\Lib\\site-packages\\proto\\marshal\\rules\\message.py\", line 36, in to_proto\n    return self._descriptor(**value)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^\nTypeError: Parameter to CopyFrom() must be instance of same class: expected <class 'Schema'> got <class 'dict'>.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\gen-ai\\agents_poc\\semantic_kernel\\venv\\Lib\\site-packages\\proto\\marshal\\rules\\message.py\", line 36, in to_proto\n    return self._descriptor(**value)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^\nValueError: Protocol message Schema has no \"anyOf\" field.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"google_check3.py\", line 69, in <module>\n    asyncio.run(main())\nValueError: Unknown field for Schema: anyOf\nPlatform\n\nLanguage: Python 3.13\nSource: pip package (please run pip show semantic-kernel to get exact version)\nAI model: Google AI Gemini 2.0 Flash\nIDE: VS Code\nOS: Windows\n\nAdditional context\nThis error occurs specifically when using the Google AI connector with agent plugins. The issue appears to be that Semantic Kernel generates JSON schemas containing \"anyOf\" fields, but Google AI's protobuf Schema definition doesn't support these fields.\nThe error occurs in the schema conversion process at:\n\nFile: google\\generativeai\\types\\content_types.py\nFunction: FunctionDeclaration.__init__()\nDuring: _rename_schema_fields() processing\n\nThis issue prevents using the agent plugin architecture with Google AI services, while other providers (OpenAI, Azure) may work correctly. A schema transformation layer or validation step may be needed to make Semantic Kernel compatible with Google AI's function declaration requirements.",
    "comments": []
  },
  {
    "issue_number": 12440,
    "title": "Python: Bug: fail to add plugin from file with \"utf-8\" encoding (UnicodeDecodeError: 'charmap' codec)",
    "author": "BinarywoodB",
    "state": "open",
    "created_at": "2025-06-10T18:51:51Z",
    "updated_at": "2025-06-10T18:53:08Z",
    "labels": [
      "bug",
      "python",
      "triage"
    ],
    "body": "**Describe the bug**\nWhen I add plugin to kernel via `kelnel.add_plugin()`, the loading of prompt files from `skprompt.txt` file will fail with `UnicodeDecodeError` error\n\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. in `plugins/attachment/asr_test_analysis/` folder, I have 2 files. One is the prompt file `skprompt.txt`, the other is the `config.json` file indicating the function configuration like input variables, etc.\n2. In my program, add this `attachment` plugin via:\n```\n        prompt_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"plugins\")\n        kernel.add_plugin(\n            parent_directory=prompt_dir,\n            plugin_name=\"attachment\",\n        )\n```\n3. See error:\n```\n----\n File \"C:\\Users\\dilin\\Documents\\workspace\\SPEECH\\car_agent\\speech_car\\agent_for_car\\venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function_from_prompt.py\", line 395, in from_directory\n    prompt_template_config.template = prompt_file.read()\n                                      ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\dilin\\AppData\\Local\\anaconda3\\Lib\\encodings\\cp1252.py\", line 23, in decode\n    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nUnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 1938: character maps to <undefined>\n```\n4. We use a workaround to set `PYTHONUTF8` to bypass this issue. But we are hoping that the issue can be resolved from the semantic kernel side.\n\n**Expected behavior**\nWhen semantic kernel loads the prompt file, it should open it with encoding setting `utf-8`. \n\n\n**Platform**\n - Language: Python\n - Source: pip package version 1.29.0\n - AI model: gpt-4o\n - IDE: VS Code\n - OS: Windows\n\n",
    "comments": []
  },
  {
    "issue_number": 12435,
    "title": "Python: Bug: AgentThreadActions get_messages",
    "author": "mjnong",
    "state": "open",
    "created_at": "2025-06-10T11:38:34Z",
    "updated_at": "2025-06-10T15:41:15Z",
    "labels": [
      "bug",
      "python",
      "triage"
    ],
    "body": "**Describe the bug**\nNot able to fetch messages for an `AzureAIAgentThread` with a thread where the agent no longer exists. Reason for this is that each message inside a thread seems to have the field `agent_id` associated to it. This in itself causes the issue when calling `get_messages` on a thread since the message object has an id of an agent which no longer exists causing this method to fail. The reason why this should not fail is because if a thread exists we should still be able to fetch the messages in order to either continue the conversation with an other agent (doable in the playground) or to just retrieve the messages in the thread for any arbitrary reason when the original assistant to those messages no longer exists.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Create `AzureAIAgent`\n2. Create `AzureAIAgentThread`\n3. Save the thread id\n4. Invoke chat with agent\n5. Delete agent\n6. Create `AzureAIAgentThread` with the original thread id saved from step (3)\n7. Try to retrieve the messages for the thread\n\n**Expected behavior**\nMessages from that thread to be returned even if `assistant_id` for those messages are no longer present. If thread id itself is non-existent then it should return error.\n\n\n**Platform**\n - Language: Python\n - AI model: OpenAI:GPT-4o-mini(2024-07-18)\n - IDE: Visual Studio, VS Code\n - OS: Mac",
    "comments": []
  },
  {
    "issue_number": 12438,
    "title": "New Feature: Use different models for Function calling and Summarization since some models doesn't support tool calling",
    "author": "yamuna83",
    "state": "closed",
    "created_at": "2025-06-10T14:35:06Z",
    "updated_at": "2025-06-10T15:40:27Z",
    "labels": [
      "triage"
    ],
    "body": "---\nname: Feature request\nabout: Suggest an idea for this project\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "@yamuna83 This is possible at the moment. Here is an example showing how you could implement this: https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/Concepts/Kernel/CustomAIServiceSelector.cs"
      }
    ]
  },
  {
    "issue_number": 12362,
    "title": ".Net Bug: Token usage and Content Safety information aren't returned in InvokeStreamingAsync of ChatCompletionAgent",
    "author": "stefanozanelliunitn",
    "state": "closed",
    "created_at": "2025-06-04T07:22:04Z",
    "updated_at": "2025-06-10T15:21:02Z",
    "labels": [
      "bug",
      ".NET",
      "ai connector"
    ],
    "body": "**Describe the bug**\nI am experiencing missing token Usage and ContentSafety information when InvokeStreamingAsync of a ChatCompletionAgent is called. Instead, InvokeAsync works correctly by returning Usage and Content Safety information in the generated response\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Create a ChatCompletionAgent\n2. InvokeStreamingAsync with ChatMessageContent and ChatHistoryAgentThread\n3. Inspect the last StreamingChatMessageContent object that is returned (OpenAI.Chat.StreamingChatCompletionUpdate)\n4. Usage property is null (also in the Metadata property), GetRequestContentFilterResult/GetResponseContentFilterResult returns null\n\n**Expected behavior**\nToken usage and Content Safety information should be returned like in non-streaming mode\n\n**Platform**\n - Language: C# (NET8)\n - Source: NuGet package version 1.52\n - AI model: Azure OpenAI GPT-4o-mini\n - IDE: Visual Studio 2022\n - OS: Windows 10",
    "comments": [
      {
        "user": "crickman",
        "body": "@stefanozanelliunitn - Content provided by`ChatCompletionAgent` should be identical to what is provided by the `IChatCompletionService` you've configured.  That is, I believe the behavior you've described is specific to the AI service connector a not the agent.\n\nI'll double check."
      },
      {
        "user": "crickman",
        "body": "I'm observing that `ChatCompletionAgent` exhibits identical behavior as when using the `OpenAIChatCompletionService` directly...which is:  Metdata is not provided on each chunk, but rather only the final one.  This appears to align with the spec: https://platform.openai.com/docs/api-reference/responses-streaming"
      },
      {
        "user": "stefanozanelliunitn",
        "body": "AddAzureOpenAIChatCompletion is registered in the Kernel.\nPlugins are also registered and are being invoked correctly.\n\nI may have misunderstood how streaming mode works. Here's what I observed while intercepting the full stream:\n- The first chunk contains ContentSafety information related to the request (GetRequestContentFilterResult) — Noted (OK).\n- If any plugin is invoked, the FinishReason is set to \"tool_calls\". The following chunk then includes both the request's ContentSafety and Usage information — Noted (OK).\n- I continue listening until FinishReason reaches \"stop\". At this point, Usage information is still missing —Noted (OK).\n- In the final chunk, after the \"stop\" FinishReason has been sent (and its value becomes null), I receive Usage information — Noted (OK).\n\nHowever, ContentSafety information related to the response (GetResponseContentFilterResult) is never set; it always has a null value — this is incorrect (KO).\n\nWhere am I going wrong?"
      }
    ]
  },
  {
    "issue_number": 12432,
    "title": ".Net: Obsolete the AgentChannel and associated usage",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-06-10T07:36:58Z",
    "updated_at": "2025-06-10T15:12:01Z",
    "labels": [
      ".NET",
      "agents"
    ],
    "body": null,
    "comments": []
  },
  {
    "issue_number": 12303,
    "title": "Python: Bug: Chat history summarizer is not working",
    "author": "pintuiitbhi",
    "state": "open",
    "created_at": "2025-05-29T02:56:44Z",
    "updated_at": "2025-06-10T09:55:08Z",
    "labels": [
      "bug",
      "python",
      "agents",
      "chat history"
    ],
    "body": "**Describe the bug**\nChat history summarizer is not working here. It still passes the complete chat history to llm api call.\nThis is one of the agent. Each agent has its own kernel instance.\nIts is used in GroupChatOrchestation as one of agent memeber.\n\n\n```python\nsummarization_reducer = ChatHistorySummarizationReducer(\n        service=kernel.get_service(),\n        target_count=3,\n        threshold_count=2,\n        auto_reduce=True,\n        include_function_content_in_summary=True,\n    )\n    system_message = \"\"\"\n    Summarize the chat history to reduce its size while retaining key information.\n    The summary should be concise, capturing the essence of the conversation without losing important details.\n    \"\"\"\n\n    summarization_reducer.add_system_message(system_message)\n    return ChatCompletionAgent(\n        kernel=kernel,\n        name='agent1',\n        prompt_template_config=prompt_config,\n        arguments=KernelArguments(\n            chat_history=summarization_reducer,\n            settings=PromptExecutionSettings(\n                temperature=temperature\n            ),\n        ),\n    )\n```\n\n**To Reproduce**\nSteps to reproduce the behavior:\nJust create a multi turn agent collaboration using 2 or 3 agents and observer in debug log message of semantic kernel when it is making llm chat completion api call using openai base client.\n\n**Expected behavior**\nIt should summarize then chat history should be passed to llm request body\n\n**Platform**\n - Language:  Python\n - Source: main branch of repository\n - AI model: gpt-4o Azure\n - IDE: VS Code\n - OS: Windows\n",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Hi @pintuiitbhi, when using the chat history reducer, there are two paths: \n1. the caller needs to explicitly handle reducing the history. \n2. the history will be reduced automatically if configured with the bool (auto_reduce), and if messages are added via `add_message_async()`. \n\nRight now, there is no handling in the new orchestration patterns, to allow one to pass in a chat history reducer. We will need to look at how we can allow on to incorporate this.\n\nTagging @TaoChenOSU for visibility. "
      },
      {
        "user": "pintuiitbhi",
        "body": "Got it. \n\nIn real-world multi-agent applications, passing the full chat history in every LLM call leads to token overflow issues and inefficiencies. Managing context size—via summarization, truncation, or memory—is essential and should be part of the initial design.\n\nCurrently, GroupChatOrchestration sends the entire history with each request, which can impact performance and scalability. Addressing this will be key to broader adoption.\n\n[Another issue of similar nature](https://github.com/microsoft/semantic-kernel/issues/12144)"
      },
      {
        "user": "TaoChenOSU",
        "body": "Hi @pintuiitbhi!\n\nYou raised a valuable feature request. Currently, the chat history reducer only works on the `ChatCompletionAgent`, whereas the orchestrations rely on a higher-level abstraction that is the `AgentThread`. We will see how to make it possible to specify a thread for an agent in an orchestration."
      }
    ]
  },
  {
    "issue_number": 12431,
    "title": ".Net: Eliminate use of HttpMessageHandlerStub",
    "author": "markwallace-microsoft",
    "state": "open",
    "created_at": "2025-06-10T07:12:54Z",
    "updated_at": "2025-06-10T07:13:36Z",
    "labels": [
      ".NET",
      "sk team issue"
    ],
    "body": "`MultipleHttpMessageHandlerStub`has expanded capabilities of `HttpMessageHandlerStub`, when using the multiple in new code I don't think we need to add the `HttpMessageHandlerStub` also.",
    "comments": []
  },
  {
    "issue_number": 12403,
    "title": "Python: Bug: Inconsistent behavior of function calling from agent class",
    "author": "NirajC-Microsoft",
    "state": "open",
    "created_at": "2025-06-06T13:42:20Z",
    "updated_at": "2025-06-10T02:42:08Z",
    "labels": [
      "bug",
      "python",
      "agents"
    ],
    "body": "**Describe the bug**\nI'm experiencing an issue where calling a new function from an agent class returns the output of a previously invoked function, leading to inconsistent agent responses.\n\n\n**Expected behavior**\nWhen a specific function is called from an agent class, the response should accurately reflect the output of that function, not a previously invoked function. Each function call should produce a fresh, contextually relevant response based on the current invocation\n\n**Screenshots**\nCorrect Scenario:\nHere assign_mentor gets called and giving correct response\n![Image](https://github.com/user-attachments/assets/90fbe49c-7b53-4d53-a85d-36732de2cfa2)\n\nBug:\nHere assign_mentor gets called but giving incorrect response, that response is from previous function called from same agent class.\n![Image](https://github.com/user-attachments/assets/7f26fc9f-23b6-4ea0-ade5-5a39b9431a5f)\n\n**Platform**\n - Language: Python\n - Source: pip 25.0.1 \n - AI model: OpenAI:GPT-4o\n - IDE: [e.g. Visual Studio, VS Code]\n - OS: Windows\n\n**Additional context**\nAdd any other context about the problem here.",
    "comments": [
      {
        "user": "Roopan-Microsoft",
        "body": "Hi @markwallace-microsoft - Any update on this?"
      },
      {
        "user": "moonbox3",
        "body": "@NirajC-Microsoft, this looks to be an issue related to specifically using an Azure AI Agent inside of the playground? If so, we'll need to file the issue in the [Azure SDK for Python repo](https://github.com/Azure/azure-sdk-for-python/issues). SK provides abstractions for the `AzureAIAgent` (think of it as a wrapper) but we don't have any control over the actual service calls."
      },
      {
        "user": "moonbox3",
        "body": "As a note: the AzureAIAgent may use previous function results from its thread/history/context if it thinks it can answer it without having to invoke the function again. You may need to experiment with certain instructions to the agent to make sure it invokes the function again each time, and tell it to not re-use existing function results in the context. "
      }
    ]
  },
  {
    "issue_number": 10889,
    "title": ".Net: Integrating with VertexAI Endpoint",
    "author": "sophialagerkranspandey",
    "state": "open",
    "created_at": "2025-03-10T15:32:05Z",
    "updated_at": "2025-06-10T02:15:32Z",
    "labels": [
      ".NET",
      "stale"
    ],
    "body": "\n### Discussed in https://github.com/microsoft/semantic-kernel/discussions/10866\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **weirdyang** March  9, 2025</sup>\nHow do I Use a vertex endpoint with Semantic Kernel? The AddVertexAIGeminiChatCompletion doesn't have parameter fore endpoint id.</div>",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10898,
    "title": ".Net: [OpenAPI] Use strongly-typed properties",
    "author": "SergeyMenshykh",
    "state": "open",
    "created_at": "2025-03-10T18:49:39Z",
    "updated_at": "2025-06-10T02:15:30Z",
    "labels": [
      ".NET",
      "openapi",
      "stale",
      "V2"
    ],
    "body": "**Context:** \nToday, `RestAPIOperationRunner` uses the `Exception.Data` collection to provide information about the HTTP request method, URL, payload, and options. There were a few PR comments: [Comment 1](https://github.com/microsoft/semantic-kernel/pull/6904#discussion_r1657451436), [Comment 2](https://github.com/microsoft/semantic-kernel/pull/7119#discussion_r1667234689) pointing out that it's better to use strongly typed properties than `Exception.Data`.\n\n**ToDo**\n1. Add optional strongly-typed properties to the `HttpOperationException`.\n2. Assign those properties in the `RestAPIOperationRunner`.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10910,
    "title": ".Net Bug: ContentBuffer is not cleared when using AutoInvoke in AddStreamingMessageAsync (OpenAI Connector)",
    "author": "MadLongTom",
    "state": "open",
    "created_at": "2025-03-11T09:12:44Z",
    "updated_at": "2025-06-10T02:15:28Z",
    "labels": [
      "bug",
      ".NET",
      "stale"
    ],
    "body": "**Describe the bug**\nContentBuffer is not cleared when using AutoInvoke in AddStreamingMessageAsync (OpenAI Connector). When the function is automatically invoked and chat completed for the second time, the second completion content in ChatHistory will include the duplicate content of the first completion.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Add a simple plugin and then let LLM call it using AddStreamingMessageAsync completion\n2. Check the ChatHistory and see duplicate content\n\n**Expected behavior**\nThe second assistant content should not include the first assistant content\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/a6626011-ac95-4a6c-8eca-48be475e8457)\n\n**Platform**\n - Language: C#\n - Source: NuGet package version 0.14.0-alpha\n - AI model: QwQ:32B with OneAPI\n - IDE: Visual Studio\n - OS: Windows\n\n**Additional context**",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 12397,
    "title": "Python: How to get process running state while the process is running",
    "author": "alfredzouang",
    "state": "open",
    "created_at": "2025-06-06T04:07:14Z",
    "updated_at": "2025-06-10T01:32:41Z",
    "labels": [
      "python",
      "processes"
    ],
    "body": "I'm using the semantic-kernel process and I started my process using the following code:\n```python\nasync with await start_local_process(\n            process=process,\n            kernel=kernel,\n            initial_event=KernelProcessEvent(\n                id=\"StartProcess\",\n                data=project,\n                visibility=KernelProcessEventVisibility.Public,\n            ),\n        ) as process_context:\n           while True:\n                process_state = await process_context.get_state()\n                steps = process_state.steps\n                logger.debug(f\"Current process state: {process_state}\")\n                # You can customize the chunk content as needed\n                for step in steps:\n                    yield f\"data: {step.model_dump_json(indent=2)}\\n\\n\"\n```\n\nThe problem is I can't get the steps or state object before the process is completed. Is there any way I can get the running state of the process? I need to return to UI and updated the running status. Thank you very much.\n\nsemantic-kernel version: 1.28.1",
    "comments": [
      {
        "user": "alfredzouang",
        "body": "If I want a real-time process state, must I extend the current process framework and expose internal state via new method? Or is there a way like filter/telemetry etc to accomplish this requirement? Thank you in advance."
      },
      {
        "user": "alfredzouang",
        "body": "For anyone who is interest in this issue, I got this running by adding a status callback in each step and manually report the status to frontend ui.\n\nTry the following code:\n```python\n@app.post(\"/project/start\")\nasync def start_project_documentation_process(project: Project):\n    \"\"\"\n    Start the project documentation process and stream status updates.\n    Uses a singleton kernel and pre-connected plugins for performance.\n    \"\"\"\n    logger.info(f\"Starting project documentation process for project: {project.name}\")\n    state_queue = asyncio.Queue()\n\n    def state_callback_to_ui(state):\n        # This can be called from sync code, so use asyncio.create_task to put into the queue\n        logger.debug(f\"State callback received: {state}\")\n        asyncio.create_task(state_queue.put(state))\n\n    async def run_process():\n        process = build_process(state_callback=state_callback_to_ui)\n        async with await start_local_process(\n            process=process,\n            kernel=kernel,\n            initial_event=KernelProcessEvent(\n                id=\"StartProcess\",\n                data=project,\n                visibility=KernelProcessEventVisibility.Public,\n            ),\n        ) as process_context:\n            process_state = await process_context.get_state()\n            process_state_metadata = process_state.to_process_state_metadata()\n            process_timestamp = date.today().strftime(\"%Y%m%d%H%M%S\")\n            PROCESS_STATE_JSON_FILE_NAME = f\"{process_timestamp}_{project.name}_state.json\"\n            dump_process_state_metadata_locally(process_state_metadata, PROCESS_STATE_JSON_FILE_NAME)  # The process runs and triggers state_callback as it goes\n\n    async def status_stream():\n        # Build the process (replace with documentation-specific process if available)\n        yield \"data: {\\\"status\\\": \\\"stream started\\\"}\\n\\n\"\n        # process = build_process(state_callback=state_callback_to_ui)\n        process_task = asyncio.create_task(run_process())\n        process_done = False\n        while True:\n            try:\n                # Wait for a state with a timeout to periodically check if the process is done\n                state = await asyncio.wait_for(state_queue.get(), timeout=0.5)\n                yield f\"data: {state}\\n\\n\"\n            except asyncio.TimeoutError:\n                # No state received in this interval, check if process is done\n                if process_task.done():\n                    # Drain any remaining states\n                    while not state_queue.empty():\n                        state = await state_queue.get()\n                        yield f\"data: {state}\\n\\n\"\n                    # Optionally, send a final message\n                    yield 'data: {\"status\": \"done\"}\\n\\n'\n                    break\n        await process_task  # Ensure process finishes\n\n    return StreamingResponse(status_stream(), media_type=\"text/event-stream\")\n```\n\n"
      },
      {
        "user": "moonbox3",
        "body": "Thanks for filing and following up with the sample, @alfredzouang. This is a viable way to hook into the current state of the process. The OOB callback mechanism with our process framework is a gap. As we move towards GA, we will investigate how to provide a native way to get updates while a process runs."
      }
    ]
  },
  {
    "issue_number": 12330,
    "title": "Python: Process Framework w/ agents can get in infinite loop if sharing the same Kernel in Python",
    "author": "jordanbean-msft",
    "state": "closed",
    "created_at": "2025-05-31T14:44:34Z",
    "updated_at": "2025-06-10T01:24:22Z",
    "labels": [
      "bug",
      "python",
      "agents",
      "processes"
    ],
    "body": "**Describe the bug**\nSharing the same Kernel when using the Process Framework can result in an infinite loop as the various steps may try and invoke each other over and over again.\n\n**To Reproduce**\nI set up a `Process` with a `Step` that can call out to an `AzureAIAgent` for processing. Both the `Process` & the `AzureAIAgent` share the same `Kernel`. It seems like there is an infinite loop problem where the Process Framework (inside a `Step`) calls one of my `@kernel_function` decorated `Step` functions (that was registered in the `Kernel`). Inside that function, a call to the `AzureAIAgent` is made, that `AzureAIAgent` decides to also call the same `@kernel_function` `Step` function, which then triggers another `AzureAIAgent` call, which decides to call the same `@kernel_function`, etc.\n \nTo fix this, I instantiated a new `Kernel` for the `AzureAIAgent` that doesn't know about the Process Framework `@kernel_function` decorated functions.\n \nIs this unintended behavior and is the correct solution to instantiate multiple Kernels? Should the docs make it clear that you may need multiple Kernels?\n\n**Expected behavior**\nEither add checks to ensure agents & tools don't call each other over and over again (which may be impossible) or explicitly call out that you may need to instantiate separate Kernels to prevent this. Maybe a different decorator for Process Framework `Steps` (instead of the `@kernel_function` that is used by plugins) to ensure other parts of SK don't try and use them.\n\n**Platform**\n - Language: Python\n - Source: 1.31.0\n - AI model: gpt-4o\n - IDE: VS Code\n - OS: Windows + WSL",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Great catch, @jordanbean-msft. This is an issue right now as process steps use the same decorator as functions/plugins. I'll update the docs to callout that one needs to use separate kernels as this is a larger change needed within the process framework to better handle this."
      },
      {
        "user": "moonbox3",
        "body": "Handling via docs update in a separate repo PR."
      }
    ]
  },
  {
    "issue_number": 12394,
    "title": "Python: Bug: OpenAI Agent sample not working",
    "author": "marinomoscoso1",
    "state": "closed",
    "created_at": "2025-06-05T22:41:38Z",
    "updated_at": "2025-06-10T01:17:07Z",
    "labels": [
      "bug",
      "python",
      "agents"
    ],
    "body": "Hi, i've a problem with the ai agents for beginers course, the introduction code sample doesnt work and i dont know because i have the openai endpoint and apikey from the azure ai foundry hub\n\n`ServiceResponseException: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.OpenAIChatCompletion'> service failed to complete the prompt\", AuthenticationError(\"Error code: 401 - {'error': {'code': 'unauthorized', 'message': 'The `models` permission is required to access this endpoint', 'details': 'The `models` permission is required to access this endpoint'}}\"))\n`",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "@marinomoscoso1 This could be an issue with your OpenAI API key"
      },
      {
        "user": "crickman",
        "body": "@marinomoscoso1, are you connecting via API key or a token?  If token, this may be related to the permissions assigned to the active account; otherwise, please double check the API key.  \n\nNote, if you are targeting chat-completion...try using the OpenAI endpoint instead of the Foundry endpoint.  (They are both provided on the Foundry Project _Overview_ page)"
      },
      {
        "user": "moonbox3",
        "body": "@marinomoscoso1, can you please refer to the setup guide to make sure you're using the correct PAT? https://github.com/microsoft/ai-agents-for-beginners/tree/main/00-course-setup#set-up-for-samples-using-github-models\n\nAdditionally, please post the issue in the following [repo](https://github.com/microsoft/ai-agents-for-beginners/issues) as it pertains to the course. You can tag me (@moonbox3) if you'd like."
      }
    ]
  },
  {
    "issue_number": 12382,
    "title": "Python: Bug: agent: AgentRegistry.create_from_yaml does not work with Ollama",
    "author": "xyang2013",
    "state": "closed",
    "created_at": "2025-06-05T01:27:18Z",
    "updated_at": "2025-06-10T01:13:37Z",
    "labels": [
      "bug",
      "python",
      "agents"
    ],
    "body": "code:\n```\nagent: ChatCompletionAgent = await AgentRegistry.create_from_yaml(\n        AGENT_YAML, kernel=kernel, service=local_chat_completion_model\n)\n```\n\nerror:\n```\nFile \"C:\\Users\\xiaoy\\repos\\semantic_kernel2\\step11_chat_completion_agent_declarative.py\", line 79, in main\n    agent: ChatCompletionAgent = await AgentRegistry.create_from_yaml(\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\xiaoy\\miniforge3\\envs\\semantic_kernel\\Lib\\site-packages\\semantic_kernel\\agents\\agent.py\", line 721, in create_from_yaml\n    _preload_builtin_agents()\n  File \"C:\\Users\\xiaoy\\miniforge3\\envs\\semantic_kernel\\Lib\\site-packages\\semantic_kernel\\agents\\agent.py\", line 669, in _preload_builtin_agents\n    raise RuntimeError(f\"Failed to preload the following built-in agent modules:\\n{error_msgs}\")\nRuntimeError: Failed to preload the following built-in agent modules:\n- semantic_kernel.agents.azure_ai.azure_ai_agent: No module named 'azure.ai'\n```",
    "comments": [
      {
        "user": "moonbox3",
        "body": "@xyang2013 what SK package version are you using?"
      },
      {
        "user": "xyang2013",
        "body": "Hi @moonbox3, here is the version info:\n```\n(semantic_kernel) C:\\Users\\xiaoy\\repos\\semantic_kernel2>pip show semantic-kernel\nName: semantic-kernel\nVersion: 1.31.0\nSummary: Semantic Kernel Python SDK\nHome-page: https://learn.microsoft.com/en-us/semantic-kernel/overview/\nAuthor:\nAuthor-email: Microsoft <SK-Support@microsoft.com>\nLicense:\nLocation: C:\\Users\\xiaoy\\miniforge3\\envs\\semantic_kernel\\Lib\\site-packages\nRequires: aiohttp, aiortc, azure-identity, cloudevents, defusedxml, jinja2, nest-asyncio, numpy, openai, openapi_core, opentelemetry-api, opentelemetry-sdk, prance, protobuf, pybars4, pydantic, pydantic-settings, scipy, typing-extensions, websockets\nRequired-by:\n``` "
      },
      {
        "user": "moonbox3",
        "body": "Thank you. Could you try and install the latest SK version? In previous versions, the AzureAIAgent package required one to do:\n\n```bash\npip install semantic-kernel[azure]\n```\n\nNow the AzureAIAgent packages are installed with the default dependencies:\n\n```bash\npip install semantic-kernel\n```\n\n I think what it is happening it it's trying to load the module, but the underlying dependencies are not there, so it's failing.\n\nIf you still hit this error with the latest SK version, please let me know and I will continue to investigate. "
      }
    ]
  },
  {
    "issue_number": 12411,
    "title": "Python: Bug: invoke_stream method in chat_completion_agent.py not returning token usage data",
    "author": "sdsmt05",
    "state": "closed",
    "created_at": "2025-06-06T18:32:07Z",
    "updated_at": "2025-06-09T23:19:35Z",
    "labels": [
      "bug",
      "python",
      "agents"
    ],
    "body": "**Describe the bug**\n\nWhen calling the `invoke_stream` method in `chat_completion_agent.py` the usage information isn't being sent back in the final chunk.\n\nIt appears the issue is with lines 453-460 in `chat_completion_agent.py`\n\n```python\n                if (\n                    role == AuthorRole.ASSISTANT\n                    and response.items\n                    and not any(\n                        isinstance(item, (FunctionCallContent, FunctionResultContent)) for item in response.items\n                    )\n                ):\n                    yield AgentResponseItem(message=response, thread=thread)\n```\n\nif `response.items` is empty, it skips yielding back the usage information.\n\n**To Reproduce**\n\nCall the `invoke_stream` method.\n\n**Expected behavior**\n\nThe streaming usage information should be returned back in the final chunk.\n\n**Screenshots**\n\nCurrent behavior (showing zero token usage):\n\n![Image](https://github.com/user-attachments/assets/2f6bf661-825a-485b-8707-6bc77caba83e)\n\nDesired behavior (this was achieved by commenting out line `455` in `chat_completion_agent.py`:\n\n```python\n                if (\n                    role == AuthorRole.ASSISTANT\n                    # and response.items\n                    and not any(\n                        isinstance(item, (FunctionCallContent, FunctionResultContent)) for item in response.items\n                    )\n                ):\n                    yield AgentResponseItem(message=response, thread=thread)\n```\n\n![Image](https://github.com/user-attachments/assets/1e608b7f-025b-4cc8-a203-17fb58bd1a8e)\n\n**Platform**\n\n - Language: Python\n - Source: Semantic Kernel v1.29.0\n - AI model: gpt-4o-mini-2024-07-18\n - IDE: VS Code\n - OS: Mac\n\n**Additional context**\n\nWhile commenting out line `455` in `chat_completion_agent.py` yielded the usage information, it did also include additional \"empty\" partial responses.",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Thanks for filing, @sdsmt05. We'll get this handled ASAP."
      }
    ]
  },
  {
    "issue_number": 12400,
    "title": "Python: Remove SK planners",
    "author": "SergeyMenshykh",
    "state": "closed",
    "created_at": "2025-06-06T08:15:23Z",
    "updated_at": "2025-06-09T21:43:09Z",
    "labels": [
      "python",
      "planner"
    ],
    "body": "SK .NET planners were first stopped from being published as NuGet packages and later removed from the codebase:\n- https://github.com/microsoft/semantic-kernel/pull/11741\n- https://github.com/microsoft/semantic-kernel/pull/12399\n\nSimilarly, SK Python planners should be treated in the same way.",
    "comments": []
  },
  {
    "issue_number": 12406,
    "title": "Python: Bug: MCP error calling Nasa MCP server",
    "author": "pietrobr",
    "state": "closed",
    "created_at": "2025-06-06T14:49:54Z",
    "updated_at": "2025-06-09T20:38:00Z",
    "labels": [
      "bug",
      "python",
      "core plugin"
    ],
    "body": "I'm trying to use this\n\n```python\n\nasync with MCPStdioPlugin(\n        name=\"nasa\",\n        description=\"nasa Plugin\",\n        command=\"docker\",\n        args=[\"run\",\"-i\",\"--rm\",\"ghcr.io/metorial/mcp-container--programcomputer--nasa-mcp-server--nasa-mcp-server\",\"npm run start\"]\n    ) as nasa_plugin:\n        agent = ChatCompletionAgent(\n            service=AzureChatCompletion(),\n            name=\"NasaAgent\",\n            instructions=\"Answer questions about Nasa.\",\n            plugins=[nasa_plugin],\n        )\n```\n\n\nbut I get this error:\n\nError from MCP server: 1 validation error for JSONRPCMessage\n  Invalid JSON: EOF while parsing a value at line 1 column 0 [type=json_invalid, input_value='', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\nError from MCP server: 1 validation error for JSONRPCMessage\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='> @programcomputer/nasa-mcp-server@1.0.12 start', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\nError from MCP server: 1 validation error for JSONRPCMessage\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='> node dist/index.js', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\nError from MCP server: 1 validation error for JSONRPCMessage\n  Invalid JSON: EOF while parsing a value at line 1 column 0 [type=json_invalid, input_value='', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Hi @pietrobr, curious if you're able to use this MCP tool outside of SK? Even if I allow `/` in the SK plugin/function name, the model then returns a 400 because it doesn't allow `/`. One of the tools is called:\n\n`nasa/browse-near-earth-objects`.\n\nIf I then replace any `/` or `\\` with `-` for the function's name, I still see the validation errors for JSONRPCMessage, but I get a response:\n\n```\nError from MCP server: 1 validation error for JSONRPCMessage\n  Invalid JSON: EOF while parsing a value at line 1 column 0 [type=json_invalid, input_value='', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\nError from MCP server: 1 validation error for JSONRPCMessage\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='> @programcomputer/nasa-mcp-server@1.0.12 start', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\nError from MCP server: 1 validation error for JSONRPCMessage\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='> node dist/index.js', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\nError from MCP server: 1 validation error for JSONRPCMessage\n  Invalid JSON: EOF while parsing a value at line 1 column 0 [type=json_invalid, input_value='', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\nUser: Are there any close approaches for asteroids or comets soon?\nThere are several upcoming close approaches of asteroids to Earth between October 13 to October 19, 2023. Here are some notable ones:\n\n1. **Asteroid 2023 TC1**\n   - Date: October 13, 2023, 02:57 UTC\n   - Distance: 0.027 AU (Astronomical Units)\n   - Velocity: 8.12 km/s\n   \n2. **Asteroid 2023 TB4**\n   - Date: October 13, 2023, 04:06 UTC\n   - Distance: 0.017 AU\n   - Velocity: 10.62 km/s\n   \n3. **Asteroid 2023 TK12**\n   - Date: October 13, 2023, 05:41 UTC\n   - Distance: 0.031 AU\n   - Velocity: 17.76 km/s\n   \n4. **Asteroid 2023 TM27 (Closest approach)**\n   - Date: October 13, 2023, 15:09 UTC\n   - Distance: 0.003 AU\n   - Velocity: 12.54 km/s\n\n5. **Asteroid 2023 TD7 (Closest approach)**\n   - Date: October 14, 2023, 11:11 UTC\n   - Distance: 0.0011 AU\n   - Velocity: 10.09 km/s\n\nPlease note that the closest approaches (e.g., those within 0.01 AU) are the most noteworthy in terms of proximity. Always keep in mind that these distances are still quite large when compared to everyday scales.\nnpm error path /app\nnpm error command failed\nnpm error signal SIGTERM\nnpm error command sh -c node dist/index.js\nnpm error A complete log of this run can be found in: /root/.npm/_logs/2025-06-09T05_54_05_236Z-debug-0.log\n```\n\nThose initial errors look to be unrelated to SK code - they occur somewhere in the MCP package. Let me work on getting out some better handling where we replaced invalid name chars for function calling with valid chars."
      },
      {
        "user": "pietrobr",
        "body": "Hi @moonbox3 you are right, I tried on cloude desktop and I get some validation errors at the startup of the app, but then some api are working. So it's not a SK specific  problem."
      }
    ]
  },
  {
    "issue_number": 12422,
    "title": "Python: Bug: AttributeError in azure_ai_agent when invoking agent with RAG (Azure AI Search) on v1.32.2",
    "author": "MarekFabinski",
    "state": "open",
    "created_at": "2025-06-09T09:40:54Z",
    "updated_at": "2025-06-09T15:53:13Z",
    "labels": [
      "bug",
      "python",
      "agents"
    ],
    "body": "**Description**\nAfter updating to the latest version of the semantic-kernel library, invoking an AzureAIAgent that is configured to use the built-in Azure AI Search tool for Retrieval-Augmented Generation (RAG) consistently fails. The process throws an AttributeError: 'RunStepDeltaToolCall' object has no attribute 'azure_ai_search' deep within the library's streaming logic.\n\nThis appears to be a breaking change or a bug in how the latest SDK version parses streaming tool call responses from the Azure AI service, as downgrading to an earlier version 1.32.1 resolves the issue.\n\n![Image](https://github.com/user-attachments/assets/54817582-bd5b-4841-a5ca-cacc11c0d855)\n\n**Expected behavior**\nThe invoke() call should complete successfully. The AzureAIAgent should use its RAG tool in the cloud, synthesize an answer, and stream the final ChatMessageContent back to the orchestrator without any errors.\n",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Thanks for filing, @MarekFabinski. Must be related to the Agent service's move to GA and not captured within SK. Will get this handled ASAP."
      },
      {
        "user": "sumansuhag",
        "body": "Hey MarekFabinski,\n\nThanks for reporting that important bug! It looks like you ran into an AttributeError that popped up after the update from version 1.32.1 to 1.32.2 in the semantic-kernel library, which is definitely a problem when dealing with RAG and Azure AI Search.\n\nYour guess about it being a breaking change or a bug in how the latest SDK version parses streaming tool call responses sounds spot on. The RunStepDeltaToolCall objects are part of the output from agents, and if there’s been a change in how their structure is expected or how the SDK reads them, that could cause this kind of error.\n\nThe fact that going back to 1.32.1 fixes the issue really points to a problem that came with 1.32.2.\n\nThis needs to be looked at quickly by the semantic-kernel team. They should compare the changes between these versions, especially anything related to agent streaming and tool call handling, as well as the azure_ai_search integration. It could be a regression or a breaking change that didn’t get documented well.\n\nFor now, it’s best to stick with 1.32.1. I’m looking forward to seeing the official fix, as having RAG work smoothly with Azure AI Search is really important for a lot of projects."
      }
    ]
  },
  {
    "issue_number": 12294,
    "title": "Python: Bug: In Handoff orchestration - when handling a request message from an agent in the handoff group, the persona adoption message is sent by the User which triggers openAI jailbreak guardrails",
    "author": "SaurabhNiket231",
    "state": "open",
    "created_at": "2025-05-28T14:43:26Z",
    "updated_at": "2025-06-09T14:34:07Z",
    "labels": [
      "bug",
      "python",
      "agents"
    ],
    "body": "**Describe the bug**\nwhen handling a request message from an agent in the handoff group, the persona adoption message is sent by the User which triggers openAI jailbreak guardrails\n\n![Image](https://github.com/user-attachments/assets/f780f3ed-ae69-4dc2-804e-72b825079e4f)\n\nif self._agent_thread is None:\n            self._chat_history.add_message(\n                ChatMessageContent(\n                    role=AuthorRole.USER,\n                    content=f\"Transferred to {self._agent.name}, adopt the persona immediately.\",\n                )\n            )\n            response_item = await self._agent.get_response(\n                messages=self._chat_history.messages,  # type: ignore[arg-type]\n                kernel=self._kernel,\n            )\n        else:\n            response_item = await self._agent.get_response(\n                messages=ChatMessageContent(\n                    role=AuthorRole.USER,\n                    content=f\"Transferred to {self._agent.name}, adopt the persona immediately.\",\n                ),\n                thread=self._agent_thread,\n                kernel=self._kernel,\n            )\n\nChanging the role to Assistant fixes the issue.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Create a handoff orchestration\n2. And make sure the open ai guardrails are deployed.\n3. Then run the orchestration.\n4. See error\n\n**Expected behavior**\nIt should handoff to other agents on runtime without Jailbreak issues\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n![Image](https://github.com/user-attachments/assets/f780f3ed-ae69-4dc2-804e-72b825079e4f)\n\n**Platform**\n - Language: Python\n - Source: pip package semantic-kernel 1.31.0\n - AI model: OpenAI:GPT-4o\n - IDE: VS Code\n - OS: Windows\n\n**Additional context**\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\repo\\CG_GenAI_Accelerator_repos\\sk-agentic-demo\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1549, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': True, 'detected': True}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"d:\\repo\\CG_GenAI_Accelerator_repos\\sk-agentic-demo\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\runtime\\in_process\\in_process_runtime.py\", line 470, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n    )\n    ^\n  File \"d:\\repo\\CG_GenAI_Accelerator_repos\\sk-agentic-demo\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\runtime\\core\\base_agent.py\", line 129, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\repo\\CG_GenAI_Accelerator_repos\\sk-agentic-demo\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\orchestration\\agent_actor_base.py\", line 35, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\repo\\CG_GenAI_Accelerator_repos\\sk-agentic-demo\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\runtime\\core\\routed_agent.py\", line 488, in on_message_impl\n    return await h(self, message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\repo\\CG_GenAI_Accelerator_repos\\sk-agentic-demo\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\runtime\\core\\routed_agent.py\", line 156, in wrapper\n    return_value = await func(self, message, ctx)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\repo\\CG_GenAI_Accelerator_repos\\sk-agentic-demo\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\orchestration\\handoffs.py\", line 285, in _handle_request_message\n    response_item = await self._agent.get_response(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n    )\n    ^\n",
    "comments": [
      {
        "user": "SaurabhNiket231",
        "body": "I pasted the solution image actually that changing the role to Assitant fixes it."
      },
      {
        "user": "TaoChenOSU",
        "body": "Hey @SaurabhNiket231, thanks for reporting the issue!\n\nPlease help me better understand the issue. Are you saying that if you have Azure content filtering enabled on your model endpoint then the persona adoption message would trigger a policy violation?"
      },
      {
        "user": "SaurabhNiket231",
        "body": "@TaoChenOSU That is correct. Because the handoff message is being created as USER role it raises the policy violation. But if I change it to ASSISTANT then it can’t raise this issue. It took me a while to understand this and I thought there is some prompt in my agents causing this. To verify that I created a planner agent with very simple prompt like one line prompt. Then also it did that. So I started debugging and found this. Please let me know if you need more details."
      }
    ]
  },
  {
    "issue_number": 12402,
    "title": "Python: GroupChatManager Lacks Chat History for Follow-up Questions",
    "author": "MarekFabinski",
    "state": "closed",
    "created_at": "2025-06-06T11:44:44Z",
    "updated_at": "2025-06-09T09:51:08Z",
    "labels": [
      "python",
      "triage"
    ],
    "body": "I am working with the GroupChatManager in Python, following the official documentation on [Agent Orchestration with Group Chat](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-orchestration/group-chat?pivots=programming-language-python).\n\nMy goal is to create a multi-turn conversation where agents can respond to follow-up questions by retaining the context of the preceding conversation.\n\n**Expected Behavior**\n- User asks an initial question: \"Give me a list of 5 products from Contoso.\"\n- The GroupChatManager selects the appropriate agent to answer.\n- The selected agent answers: \"Here are 5 products from Contoso: [Product A, Product B, Product C, Product D, Product E].\"\n- User asks a follow-up question: \"Provide me with 2 more.\"\n- The GroupChatManager understands the conversational context and re-engages the same agent.\n- The agent answers by recalling the previous turn: \"Here are two more products: [Product F, Product G].\"\n\n**Actual Behavior**\nThe GroupChatManager and the selected agent do not appear to retain the chat history. When the user asks a follow-up question (\"Provide me with 2 more\"), the agent fails to understand the context and asks for clarification, such as, \"What products are you referring to?\". The history of the conversation is lost.\n\n**What I've Tried**\nManaging the conversation in a while loop and attempting to pass the chat history back to the manager on each turn.\nInvestigating the use of Azure AI Foundry threads to maintain a persistent conversation state, but I could not find a clear implementation path for integrating this with the GroupChatManager.\n\n**Question**\nIs there a standard method for preserving and passing chat history to the GroupChatManager to enable contextual follow-up conversations? Could this be a potential bug, or am I missing a step in the implementation?\n\nAny guidance on how to properly manage conversation history in this scenario would be greatly appreciated.",
    "comments": []
  },
  {
    "issue_number": 12401,
    "title": "Java: Remove SK planners",
    "author": "SergeyMenshykh",
    "state": "closed",
    "created_at": "2025-06-06T08:17:18Z",
    "updated_at": "2025-06-09T08:18:17Z",
    "labels": [
      "planner",
      "java",
      "triage"
    ],
    "body": "SK .NET planners were first stopped from being published as NuGet packages and later removed from the codebase:\n- https://github.com/microsoft/semantic-kernel/pull/11741\n- https://github.com/microsoft/semantic-kernel/pull/12399\n\nSimilarly, SK Java planners should be treated in the same way.",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "Java issues belong here https://github.com/microsoft/semantic-kernel-java/issues/323"
      }
    ]
  },
  {
    "issue_number": 12353,
    "title": ".Net: Bug: RetainArgumentTypes breaks tools with enum arguments",
    "author": "AlesRuzickaEu",
    "state": "open",
    "created_at": "2025-06-03T11:49:12Z",
    "updated_at": "2025-06-09T08:16:27Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nWhen FunctionChoiceBehaviorOptions.RetainArgumentTypes is set to true, system can't call tools that accept enums as arguments, results in error: \n\n`Object of type 'System.Text.Json.JsonElement' cannot be converted to type 'Plugin+Sentiments`\n\n**To Reproduce**\nSteps to reproduce the behavior (code):\n\n```\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.ChatCompletion;\nusing Microsoft.SemanticKernel.Connectors.OpenAI;\n\n#region Hide\nvar apiKey = \"xxx\";\n#endregion\n\nvar builder = Kernel\n    .CreateBuilder();\n\nbuilder.AddOpenAIChatCompletion(\"gpt-4.1-mini\", apiKey);\n\nbuilder.Services.AddSingleton<IFunctionInvocationFilter, FunctionFilter>();\n\nvar kernel = builder.Build();\n\nkernel.Plugins.AddFromType<Plugin>();\n\nvar chatCompletionServices = kernel.GetRequiredService<IChatCompletionService>();\n\n#pragma warning disable SKEXP0001 // Type is for evaluation purposes only and is subject to change or removal in future updates. Suppress this diagnostic to proceed.\nvar promptExecutionSettings = new OpenAIPromptExecutionSettings\n{\n    FunctionChoiceBehavior = FunctionChoiceBehavior.Required(\n        options: new FunctionChoiceBehaviorOptions\n        {\n            AllowStrictSchemaAdherence = true,\n            RetainArgumentTypes = true\n        }),\n};\n#pragma warning restore SKEXP0001 // Type is for evaluation purposes only and is subject to change or removal in future updates. Suppress this diagnostic to proceed.\n\nvar history = new ChatHistory();\n\nhistory.AddUserMessage(\"Track sentiment of this message: Hi.\");\n\nvar result = await chatCompletionServices.GetChatMessageContentAsync(history, promptExecutionSettings, kernel);\n\nConsole.WriteLine(result.Content);\n\nConsole.ReadKey();\n\nclass Plugin\n{\n    public enum Sentiments\n    {\n        Positive,\n        Negative\n    }\n\n    [KernelFunction(\"log_sentiment\")]\n    public void GenerateTour(Sentiments sentiment)\n    {\n        Console.WriteLine(\"Sentiment logged: \" + sentiment);\n    }\n}\n\npublic class FunctionFilter : IFunctionInvocationFilter\n{\n    public async Task OnFunctionInvocationAsync(FunctionInvocationContext context, Func<FunctionInvocationContext, Task> next)\n    {\n        try\n        {\n            await next(context);\n        }\n        catch (Exception ex)\n        {\n            Console.WriteLine(\"Error in tool call: \" + ex.Message);\n            throw;\n        }\n    }\n}\n```\n\n**Expected behavior**\nTool with enum as argument should be called, not system crashed\n\n**Platform**\n - Language: C#\n - Source: Microsoft.SemanticKernel 1.54.0\n - AI model: gpt-4.1-mini, gpt-4.1, etc.\n - IDE: Visual Studio\n - OS: Windows",
    "comments": []
  },
  {
    "issue_number": 12336,
    "title": "Python: Redis 6.3.0 No module named 'redis.commands.search.indexDefinition'",
    "author": "moonbox3",
    "state": "open",
    "created_at": "2025-06-02T05:38:21Z",
    "updated_at": "2025-06-09T08:15:31Z",
    "labels": [
      "bug",
      "python",
      "memory connector"
    ],
    "body": "https://github.com/microsoft/semantic-kernel/actions/runs/15384172458/job/43279700423\n\nThere appears to be a breaking change in Redis 6.3.0 (or a similar version, perhaps 6.0.0?) that causes this error:\n\n```python\n___ ERROR collecting tests/unit/connectors/memory/redis/test_redis_store.py ____\nImportError while importing test module '/Users/runner/work/semantic-kernel/semantic-kernel/python/tests/unit/connectors/memory/redis/test_redis_store.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/opt/homebrew/Cellar/python@3.[12](https://github.com/microsoft/semantic-kernel/actions/runs/15384172458/job/43279700423#step:5:13)/3.12.10_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py:90: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\ntests/unit/connectors/memory/redis/test_redis_store.py:9: in <module>\n    from semantic_kernel.connectors.memory.redis.const import RedisCollectionTypes\nsemantic_kernel/connectors/memory/redis/__init__.py:3: in <module>\n    from semantic_kernel.connectors.memory.redis.const import RedisCollectionTypes\nsemantic_kernel/connectors/memory/redis/const.py:6: in <module>\n    from redis.commands.search.indexDefinition import IndexType\nE   ModuleNotFoundError: No module named 'redis.commands.search.indexDefinition'\n```",
    "comments": [
      {
        "user": "vjav77",
        "body": "your application relies on modules like [redis.commands.search](http://redis.commands.search/).indexdefinition, you may encounter compatibility issues with versions 5.0 and above due to structural changes in the package looks like .  To maintain compatibility with such modules, hence we can try with  redis-py version 4.5.5."
      }
    ]
  },
  {
    "issue_number": 12250,
    "title": ".Net: Can we make contributing more friendly to dotnet part?",
    "author": "dziedrius",
    "state": "closed",
    "created_at": "2025-05-23T07:27:26Z",
    "updated_at": "2025-06-09T08:14:45Z",
    "labels": [
      ".NET",
      "triage"
    ],
    "body": "Trying to make couple contributions and couple observations:\n1. I did not find instructions how to setup environment to run all needed tests. I probably can figure that out by reading code/debugging, but it would be nice to have all instructions in single place. \nAlso would be nice to understand how much of integration tests is a must - for example if I've changed something in one connector, do I still have to setup access to all other providers, most of which are not free?\n\n2. Feedback loop is very slow, not sure why. Using Rider as main IDE and even after slight changes affecting one-two projects, it takes forever to run the test, it might be related to next item.\n\n3. To run build.cmd took I think ~2 hours to run on my i7-13700H (64GB RAM, using dev drive, etc.). Building was very very slow, going all 20 cores at full speed. It seems that `DotnetFormatOnBuild` is taking very much time in that, tests also were running forever.\n\nWondering if there's something could be done to make this more smooth.",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "Hi @dziedrius \nwe have discussed this with the team and agree we will provide some additional set up instructions.\n\nOur team is using Visual Studio and build times are typically < 5 mins.\n\nA few questions for you:\n\n1. Are you performing a Release build by default?\n2. Have you tried building from the command-line for comparison? The first build may be slow as packages are downloaded but subsequent builds should be pretty fast."
      },
      {
        "user": "dziedrius",
        "body": "1. No, using Debug by default.\n2. Yes, attaching below what I've found.\n\nI've opened `build.cmd` and removed tests part, as that part is also slow, but can't block myself from work computer for a half day :)\nSo what left is:\n```\n@echo off\nsetlocal\ncd \"%~dp0\"\ndotnet build --configuration Release --interactive\n```\nRan it with time measuring:\n```\nPS D:\\src\\semantic-kernel\\dotnet> Measure-Command { .\\build.cmd }\n\n\nDays              : 0\nHours             : 0\nMinutes           : 22\nSeconds           : 5\nMilliseconds      : 813\nTicks             : 13258136118\nTotalDays         : 0.0153450649513889\nTotalHours        : 0.368281558833333\nTotalMinutes      : 22.09689353\nTotalSeconds      : 1325.8136118\nTotalMilliseconds : 1325813.6118\n\n```\n22 minutes at full throttle, to the point where even browser is not responsive, meaning that I can't do anything meaningful with computer while it builds. I had suspicion, that `dotnet format` is the evil neighbor, hence went ahead and commented out in `Directory.Build.Targets`:\n```\n  <Target Name=\"DotnetFormatOnBuild\" BeforeTargets=\"Build\"\n          Condition=\" '$(Configuration)' == 'Release' AND '$(GITHUB_ACTIONS)' == '' \">\n    <Message Text=\"Running dotnet format\" Importance=\"high\" />\n    <Exec Command=\"dotnet format --no-restore -v diag $(ProjectFileName)\" />\n  </Target>\n```\nRan again:\n```\nPS D:\\src\\semantic-kernel\\dotnet> Measure-Command { .\\build.cmd }\n\n\nDays              : 0\nHours             : 0\nMinutes           : 2\nSeconds           : 11\nMilliseconds      : 217\nTicks             : 1312170112\nTotalDays         : 0.00151871540740741\nTotalHours        : 0.0364491697777778\nTotalMinutes      : 2.18695018666667\nTotalSeconds      : 131.2170112\nTotalMilliseconds : 131217.0112\n```\n10x faster, 2 minutes I could wait :)\nIt is harder to measure in IDE changing for example single source file and running single unit test for it, but I guess story should be similar - `dotnet format` is slowing things down a lot.\n\nUpdate: for unit tests `dotnet format` is not the cause, I've noticed there's condition to do it for release config only. \n\nWithout any changes, rerunning 1 test took 14s in build step:\n```\nBuild with surface heuristics started at 18:15:19\nUse build tool: C:\\Program Files\\dotnet\\sdk\\9.0.204\\MSBuild.dll\nCONSOLE: MSBuild version 17.13.25+b1feb5ea6 for .NET\nCONSOLE: Build started 6/2/2025 6:15:24 PM.\n\n4>------- Finished building project: Connectors.Amazon.UnitTests. Succeeded: True. Errors: 0. Warnings: 0\nBuild completed in 00:00:14.214\n```"
      },
      {
        "user": "markwallace-microsoft",
        "body": "We would recommend not building for Release by default because that is much slower. Is there a reason you need to do this?"
      }
    ]
  },
  {
    "issue_number": 10836,
    "title": "Question about Semantic Kernel Process with Human-in-the-loop",
    "author": "sophialagerkranspandey",
    "state": "open",
    "created_at": "2025-03-06T16:37:14Z",
    "updated_at": "2025-06-09T02:16:23Z",
    "labels": [
      "stale",
      "processes"
    ],
    "body": "\n### Discussed in https://github.com/microsoft/semantic-kernel/discussions/10792\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **ledgarl** March  4, 2025</sup>\nHello,\r\n\r\nI have been trying to implement a process with human-in-the-loop in Semantic Kernel, following the documentation. However, I couldn't get it to work as expected. I would like to know if this feature is fully supported and if there is any clear example that demonstrates how to create a process with multiple steps that includes human intervention at specific points.\r\n\r\nCould someone provide guidance or share a working example?\r\n\r\nThanks in advance!</div>",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10867,
    "title": "Python: Implement new filters",
    "author": "eavanvalkenburg",
    "state": "open",
    "created_at": "2025-03-10T08:45:47Z",
    "updated_at": "2025-06-09T02:16:22Z",
    "labels": [
      "stale",
      "Build",
      "SK-H2-Planning"
    ],
    "body": "Implement new filter features for TextSearch\n\nBased on lambda expressions, parsed with AST\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10870,
    "title": "Python: Add telemetry for vector stores",
    "author": "markwallace-microsoft",
    "state": "open",
    "created_at": "2025-03-10T11:37:44Z",
    "updated_at": "2025-06-09T02:16:20Z",
    "labels": [
      "stale",
      "SK-H2-Planning"
    ],
    "body": null,
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10891,
    "title": ".Net: How to user Prompt Template with Vertex AI Chat Completion?",
    "author": "sophialagerkranspandey",
    "state": "open",
    "created_at": "2025-03-10T15:36:31Z",
    "updated_at": "2025-06-09T02:16:19Z",
    "labels": [
      ".NET",
      "stale"
    ],
    "body": "\n### Discussed in https://github.com/microsoft/semantic-kernel/discussions/10885\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **weirdyang** March 10, 2025</sup>\nI'm trying to learn how to use the prompt templates, but when I try this code, I get the error ` Required service of type Microsoft.SemanticKernel.TextGeneration.ITextGenerationService not registered.`\r\n\r\nI think I need to include a prompt execution settings, or pass in my service id somehow, but `InvokeAsync` does not see to have a parameter for it.\r\n\r\n```\r\n             var template = await File.ReadAllTextAsync(\"Assets/arc_ops_prompt_2.txt\");\r\n\r\n            var arguments = new KernelArguments()\r\n            {\r\n                { \"customer\", new\r\n                    {\r\n                        name = user.Name,\r\n                        role = user.Role,\r\n                        data = !string.IsNullOrWhiteSpace(user.Name) && !string.IsNullOrWhiteSpace(user.Role)\r\n                    }\r\n                },\r\n                { \"history\", new[]\r\n                    {\r\n                        new { role = \"user\", content = user.Query },\r\n                    }\r\n                },\r\n            };\r\n            GeminiPromptExecutionSettings settings = new()\r\n            {\r\n                ToolCallBehavior = GeminiToolCallBehavior.AutoInvokeKernelFunctions,\r\n                ServiceId = ServiceName\r\n            };\r\n            var templateFactory = new HandlebarsPromptTemplateFactory();\r\n            var promptTemplateConfig = new PromptTemplateConfig()\r\n            {\r\n                Template = template,\r\n                TemplateFormat = \"handlebars\",\r\n                Name = \"QueryPrompt\",\r\n                // pass in execution settings here?\r\n            };\r\n\r\n            var function = kernel.CreateFunctionFromPrompt(promptTemplateConfig, templateFactory);\r\n            var response = await kernel.InvokeAsync(function, arguments);\r\n```\r\n\r\nand when I check my rendered prompt, the history isn't rendered:\r\n\r\n```\r\nUser: ?!?!\r\nAssistant: Sorry, I can not help with that.\r\n</message>\r\n{% for item in history %}\r\n<message role=\"\">\r\n    \r\n</message>\r\n{% endfor %}\r\n```\r\n\r\nhere's how it looks in my template:\r\n\r\n```\r\n{% for item in history %}\r\n<message role=\"{{item.role}}\">\r\n    {{item.content}}\r\n</message>\r\n{% endfor %}\r\n```\r\n\r\nI tested it with the template from the sample https://learn.microsoft.com/en-us/semantic-kernel/concepts/prompts/handlebars-prompt-templates?pivots=programming-language-csharp, and it doesn't work either:\r\n![image](https://github.com/user-attachments/assets/dd7b5782-fc29-4b57-a2d4-716a65580ea0)\r\n\r\n```\r\n\r\n    Make sure to reference the customer by name response.\r\n</message>\r\n{% for item in history %}\r\n<message role=\"\">\r\n    \r\n</message>\r\n{% endfor %}\r\n``</div>",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10892,
    "title": ".Net: Missing guidance on GenAIOps (LLMOps) with Semantic Kernel / especially in the area of ​​.NET + experimentation and evaluation",
    "author": "sophialagerkranspandey",
    "state": "open",
    "created_at": "2025-03-10T15:40:40Z",
    "updated_at": "2025-06-09T02:16:17Z",
    "labels": [
      ".NET",
      "stale"
    ],
    "body": "\n### Discussed in https://github.com/microsoft/semantic-kernel/discussions/10022\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **RicardoNiepel** December 20, 2024</sup>\nThis discussion adds to #10007 with additional dimensions.\r\n\r\nLooking into [Microsoft's current strategy for GenAIOps (LLMOps)](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-llmops-maturity) Prompt Flow as part of AI Studio and Azure Machine Learning is always used as a solution ([1](https://github.com/microsoft/genaiops-promptflow-template) & [2](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-end-to-end-llmops-with-prompt-flow))\r\n\r\nBut concrete **guidance is missing**:\r\n- How to do **GenAIOps** with **Semantic Kernel Agent Framework**\r\n  - especially how to do **prompt experimentation and evaluation** when using **Agent Collaboration with AgentChat** is not available\r\n- How to do **GenAIOps** when using **DotNet with Semantic Kernel**, as Promptflow lags .NET support [#1317 ](https://github.com/microsoft/promptflow/issues/1317#issuecomment-2151313898)\r\n\r\n_Any PG insights, community input or references are welcome._</div>",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10893,
    "title": "Dotnet: Implement Process Runtime on Shared Runtime Abstractions",
    "author": "alliscode",
    "state": "open",
    "created_at": "2025-03-10T15:51:24Z",
    "updated_at": "2025-06-09T02:16:15Z",
    "labels": [
      "stale",
      "needs_port_to_python"
    ],
    "body": "Re-implement the Process runtime using the shared runtime abstractions. Use the message passing patterns in the current Dapr runtime when doing this.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10894,
    "title": "Python: Implement Process Runtime on Shared Runtime Abstractions",
    "author": "github-actions[bot]",
    "state": "open",
    "created_at": "2025-03-10T15:52:32Z",
    "updated_at": "2025-06-09T02:16:14Z",
    "labels": [
      "python",
      "stale"
    ],
    "body": "# Original issue\n  https://github.com/microsoft/semantic-kernel/issues/10893\n  ## Description\n  Re-implement the Process runtime using the shared runtime abstractions. Use the message passing patterns in the current Dapr runtime when doing this.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 12312,
    "title": "Python: AzureAIAgent - Log additional Agent tool calls",
    "author": "jordanbean-msft",
    "state": "closed",
    "created_at": "2025-05-29T13:50:52Z",
    "updated_at": "2025-06-09T01:20:06Z",
    "labels": [
      "python",
      "agents"
    ],
    "body": "---\nname: Log additional Agent tool calls\nabout: Log additional Agent tool calls\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\nThe current `AgentThreadActions._process_stream_events` method does not log intermediate messages from the other `AgentsNamedToolChoiceType` enums. This lack of logging makes it much harder to debug & trace what agents are doing.\n\nMissing types\n- `FUNCTION`\n- `FILE_SEARCH`\n- `MICROSOFT_FABRIC`\n- `SHAREPOINT`\n- `BING_CUSTOM_SEARCH`\n- `CONNECTED_AGENT`",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Part of 1.32.1."
      },
      {
        "user": "jordanbean-msft",
        "body": "@moonbox3, the PR doesn't cover all the different enumerations available. It fixed `FUNCTION` and `FILE_SEARCH`, but the others are still not being reported. I think this also means that the intermediate message callback isn't being called for these other enumerations."
      },
      {
        "user": "moonbox3",
        "body": "We have support for the following streaming tools right now:\n\n- Code Interpreter (code gets sent back to caller as streaming text content, not via the message callback - perhaps we should look at sending the code back via the callback to align with how other tools are handled.\n- Bing Grounding - provided to caller via optional callback\n- Azure AI search - provided to caller via optional callback\n- File Search - provided to caller via optional callback\n- OpenAPI - provided to caller via optional callback\n\nWe don't have handling for these yet:\n- Fabric\n- Connected Agent\n- Logic Apps\n\nThe Sharepoint knowledge source was removed when the Azure SDK went to GA.\n\nhttps://github.com/microsoft/semantic-kernel/blob/3499c7a92c612a1a089dc38f7d474577e2a6c773/python/semantic_kernel/agents/azure_ai/agent_thread_actions.py#L525"
      }
    ]
  },
  {
    "issue_number": 12414,
    "title": "Deleted as irrelevant (wrong repo)",
    "author": "lsoft",
    "state": "closed",
    "created_at": "2025-06-08T12:59:49Z",
    "updated_at": "2025-06-08T13:01:24Z",
    "labels": [
      "bug",
      ".NET",
      "triage"
    ],
    "body": "Deleted as irrelevant (wrong repo)",
    "comments": []
  },
  {
    "issue_number": 10962,
    "title": "Python: Add a code execution tool chat completion agent sample using the Azure python code interpreter",
    "author": "TaoChenOSU",
    "state": "closed",
    "created_at": "2025-03-13T17:09:45Z",
    "updated_at": "2025-06-06T19:28:56Z",
    "labels": [
      "python",
      "samples"
    ],
    "body": "We have the chat completion agent and other agents. Currently we don't have a sample to show how to do code interpretation with the chat completion agent while all the other agents have samples for code interpretation. \n\nWe need to add a sample for the chat completion agent and we will use the Azure Python code interpreter.",
    "comments": [
      {
        "user": "shshr",
        "body": "@TaoChenOSU when can we expect to see this sample?"
      },
      {
        "user": "TaoChenOSU",
        "body": "> [@TaoChenOSU](https://github.com/TaoChenOSU) when can we expect to see this sample?\n\nHi @shshr,\n\nPlease see here for a new sample: https://github.com/microsoft/semantic-kernel/blob/main/python/samples/getting_started_with_agents/chat_completion/step12_chat_completion_agent_code_interpreter.py\nPlease follow the steps listed here on how to configure the plugin: https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/core_plugins/sessions_python_tool/README.md"
      }
    ]
  },
  {
    "issue_number": 12378,
    "title": ".Net Bug: Filters ignored when `IChatClient` is used by `Kernel`",
    "author": "crickman",
    "state": "closed",
    "created_at": "2025-06-04T22:38:05Z",
    "updated_at": "2025-06-06T18:15:54Z",
    "labels": [
      "bug",
      ".NET",
      "ai connector",
      "core plugin",
      "samples",
      "function_calling",
      "msft.ext.ai"
    ],
    "body": "**Describe the bug**\nAI services based on `IChatClient` does not invoke filters.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n```c#\nIChatClient chatClient = ...; // However one creates a chat client\n\nIKernelBuilder builder = Kernel.CreateBuilder();\nbuilder.Services.AddSingleton(functionCallingChatClient);\nKernel kernel = builder.Build();\n\nkernel.Plugins.AddFromType<MyPlugin>(); // Any plugin\nkernel.FunctionInvocationFilters.Add(new MyFunctionFilter());\n\nkernel.InvokePromptAsync(\n    \"<some message that triggers function invocation>\", \n    new KernelArguments(new PromptExecutionSettings() { FunctionChoiceBehavior = FunctionChoiceBehavior.Auto() });\n\nprivate sealed class MyFunctionFilter : IFunctionInvocationFilter\n{\n    public async Task OnFunctionInvocationAsync(FunctionInvocationContext context, Func<FunctionInvocationContext, Task> next)\n    {\n        System.Console.WriteLine($\"INVOKING: {context.Function.Name}\");\n        await next.Invoke(context);\n        System.Console.WriteLine($\"RESULT: {context.Result}\");\n    }\n}\n```\n**Expected behavior**\nFilter invoked as part of function invocation.\n\n**Platform**\n - Language: C#\n - Source: `main`\n\n**Additional context**\nTransition to support MEAI `IChatClient` should be seamless.  Instead, there is a silent failure for an existing application or service that is using filters.  This is not at parity for SK developers and has poor discoverability (unable to realize what doesn't work ahead of time).\n\n> NOTE: Certain of our AI connectors primarily rely on `IChatClient` (i.e. Ollama). ",
    "comments": [
      {
        "user": "RogerBarreto",
        "body": "For invocation to be used with Kernel:\n\n- Use the `Add<Provider>ChatClient` extension methods in `IServiceCollection` or `KernelBuilder`.\nOR \n- Add kernel invocation capability to your already existing `IChatClient` using the `Builder` pattern: \n   i.e: `myChatClient().AsBuilder().UseKernelFunctionInvocation().Build()`"
      },
      {
        "user": "crickman",
        "body": "I've done the second since I'm using the logic you implemented in the samples.  When `useChatClient` is false, the filter is invoked.  When true, it is not."
      }
    ]
  },
  {
    "issue_number": 12144,
    "title": "Python: Bug: AgentGroupChat | ChatCompletionAgent | Complete chat history is getting passed to llm",
    "author": "pintuiitbhi",
    "state": "closed",
    "created_at": "2025-05-18T05:31:42Z",
    "updated_at": "2025-06-06T17:40:52Z",
    "labels": [
      "bug",
      "python",
      "agents"
    ],
    "body": "**Describe the bug**\nThere is two agent - Agent1 and Agent2. I am using AgentGroupChat using ChatCompletionAgent. It is multi turn conversation based. Then complete chat history is getting passed to llm. Each agent has its own `Kernel ` instance. \nThis is what currently I observerd from two DEBUG logs of Semantic-Kernel\n\n![Image](https://github.com/user-attachments/assets/2e87a785-4ec0-45cb-aab9-0ccdacfcddff)\n\nThis is the each item content in json_data for a llm api calls  request has - \n\nROLE, CONTENT\n1. system, Agent1 Prompt\n2. user, task\n3. assistant, tool_calls\n4. tool, tool_calls result\n5. assistant, Agent1 response\n6. assistant, Agent1 response\n7. assistant, Agent2 Prompt\n8. assistant, Agent1 response\n9. assistant, Agent1 response\n10. assistant, Agent2 response\n\nAs its clear for more iterations it will pass more such messages and we will get token limit exceeded error. \nCurrently for my use case I just need this for any llm api call\n\n1. system <agent1> prompt\n2, user task\n3. assistant tool_calls\n4. tool response\n5. assistant <agent2> response\n6. assistnat <agent1> response\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. This can be observed in Semantic Kernel DEBUG log for any multi-turn group chat for atleast 10 iterations\n\n**Expected behavior**\n6 message items should be enough.\n\nWhen its <agent1> turn then this should be the llm api call payload:\n1. system <agent1> prompt\n2, user task\n3. assistant tool_calls\n4. tool response\n5. assistant <agent2> response\n6. assistnat <agent1> response\n\nWhen its <agent2> turn then this should be the llm api call payload:\n1. system <agent2> prompt\n2, user task\n3. assistant tool_calls\n4. tool response\n5. assistant <agent1> response\n6. assistnat <agent2> response\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Platform**\n - Language: Python\n - Source: main branch , 1.30.0 Semantic kernel version\n - AI model: Currently I used gpt-4o using Azure\n - IDE: Vscode\n - OS: Windows\n\n**Additional context**\nAdd any other context about the problem here.",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Hello @pintuiitbhi, please have a look at our new group chat orchestration patterns (step3): https://github.com/microsoft/semantic-kernel/tree/main/python/samples/getting_started_with_agents/multi_agent_orchestration\n\nWe have done a soft-release of these patterns and plan to have a more structured blog post/migration guide coming soon. We won’t be putting any more dev effort into the old AgentGroupChat patterns. We recommend devs switching to the new patterns as soon as possible. We will be tracking feedback on these new patterns going forward. "
      },
      {
        "user": "moonbox3",
        "body": "As a quick note: the \"old/legacy style\" `AgentGroupChat` does pass the entire chat history to the model. That is by design. As I mentioned above, we're moving to the new orchestration patterns. Please have a look at those, and if you need help create a new issue for us to track. Closing this issue as there won't be more development on the old patterns."
      },
      {
        "user": "pintuiitbhi",
        "body": "I have migrated now to group chat orchestration and created a custom chat manager inheriting from group chat manager following one of sample example.\n\nI still see the same issue. SK is passing complete chat history to llm api chat completion api.\n"
      }
    ]
  },
  {
    "issue_number": 12080,
    "title": "Python: New Feature: More granular spans for AutoFunctionInvocationLoop",
    "author": "HuskyDanny",
    "state": "open",
    "created_at": "2025-05-15T09:03:13Z",
    "updated_at": "2025-06-06T16:46:19Z",
    "labels": [
      "python",
      "telemetry"
    ],
    "body": "We are debugging the performance issue for the agent calling which uses the functional calling that uses the AutoFunctionInvocationLoop.\n\nFrom the trace, we see the span at AutoFunctionInvocationLoop level which took a quite lot of time and below it are some tool calls.\n\n![Image](https://github.com/user-attachments/assets/5152f7c1-04ce-4b07-aa68-ccecc70aefea)\n\nCan we add more granular spans on the AutoFunctionInvocationLoop? Specifically, I want to identify which part is taking the majority of time, I know there is functional calling to decide which tool to call, can we have this span in as well?\n\nOverall, I feel this is important to diagnose agent performance issue.\n\nThe SDK I am using is semantic-kernel==1.28.0 in python",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "@TaoChenOSU what are your thoughts on this enhancement request?"
      },
      {
        "user": "HuskyDanny",
        "body": "@TaoChenOSU  Any thoughts on this?"
      },
      {
        "user": "TaoChenOSU",
        "body": "Hi @HuskyDanny,\n\nThank you for raising the question!\n\nIt looks like you are not capturing the model invocation spans which probably take up the most time. If you have the model invocation spans captured, you will see something like this:\n\n![Image](https://github.com/user-attachments/assets/e51ca004-012b-440b-a0f5-3b7e44946291)\n\nTo capture the model invocation spans, please refer to this [documentation](https://learn.microsoft.com/en-us/semantic-kernel/concepts/enterprise-readiness/observability/telemetry-with-console?tabs=Powershell-CreateFile%2CEnvironmentFile&pivots=programming-language-python#environment-variables)."
      }
    ]
  },
  {
    "issue_number": 11722,
    "title": "Bug: Telemetry results in error in Foundry tracing portal",
    "author": "TaoChenOSU",
    "state": "closed",
    "created_at": "2025-04-24T20:25:35Z",
    "updated_at": "2025-06-06T16:39:55Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nA clear and concise description of what the bug is.\nRun this sample: https://github.com/microsoft/semantic-kernel/tree/main/dotnet/samples/Demos/TelemetryWithAppInsights and expect the traces to show up in Application Insights. And if the application insights instance is attached to an AI Foundry project, the traces should also show up in the Foundry tracing UI. \n\nThe traces only show up in Application Insights, but in the Foundry tracing UI, it results in errors:\n\n![Image](https://github.com/user-attachments/assets/0173268a-e971-4eaf-8e45-ae82ed923042)\n\n![Image](https://github.com/user-attachments/assets/a22fccf0-003c-458c-9c45-e70e0f671f62)\n\n**To Reproduce**\nRun: https://github.com/microsoft/semantic-kernel/tree/main/dotnet/samples/Demos/TelemetryWithAppInsights\n\n**Platform**\n - Language: C#\n - Source: 1.47.0\n - OS: Windows\n\n**Additional context**\nThe Python equivalent is working as expected.",
    "comments": [
      {
        "user": "TaoChenOSU",
        "body": "Closed by #11912 "
      }
    ]
  },
  {
    "issue_number": 11065,
    "title": "Python: Bug: Ollama assistant message formatting not raising with empty data",
    "author": "rracanicci",
    "state": "closed",
    "created_at": "2025-03-19T12:41:34Z",
    "updated_at": "2025-06-06T16:38:17Z",
    "labels": [
      "bug",
      "python"
    ],
    "body": "**Describe the bug**\nCondition in `_format_assistant_message` in `semantic_kernel/connections/ai/ollama/services/utils.py` could be:\n\n```python\n    if image_items:\n        if any(not image_item.data for image_item in image_items):\n            raise ValueError(\"Image must be encoded as base64.\")\n```\n\ninstead of \n\n```python\n    if image_items:\n        if any(image_item.data is None for image_item in image_items):\n            raise ValueError(\"Image must be encoded as base64.\")\n```\n\nSo it also raises for empty data.\n\n**To Reproduce**\n\nAdd the following test to `tests/unit/connectors/ai/ollama/services/test_utils.py`:\n\n```python\ndef test_message_converters_assistant_with_image_missing_data() -> None:\n    \"\"\"Test assistant message image with no data should raise ValueError.\"\"\"\n    # Arrange\n    bad_image = ImageContent(data=None)\n    content = ChatMessageContent(role=AuthorRole.ASSISTANT, items=[bad_image])\n\n    converter = MESSAGE_CONVERTERS[AuthorRole.ASSISTANT]\n\n    # Act / Assert\n    with pytest.raises(ValueError) as exc_info:\n        converter(content)\n\n    assert \"Image must be encoded as base64.\" in str(exc_info.value), \"Expected ValueError due to missing image data.\"\n```\n\n**Expected behavior**\nEmpty image data could also throw the `ValueError`, which is already what happens with user messages (see `_format_user_message`).\n\n**Platform**\n - Language: Python\n - Source: latest main\n - OS: any",
    "comments": [
      {
        "user": "TaoChenOSU",
        "body": "Closed by #10849"
      }
    ]
  },
  {
    "issue_number": 11044,
    "title": "Expanding ChatPromptParser to handle other content types",
    "author": "sophialagerkranspandey",
    "state": "closed",
    "created_at": "2025-03-18T15:36:23Z",
    "updated_at": "2025-06-06T15:51:49Z",
    "labels": [
      "chat history"
    ],
    "body": "\n### Discussed in https://github.com/microsoft/semantic-kernel/discussions/11012\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **glorious-beard** March 17, 2025</sup>\nNow that OpenAI can handle file inputs (for PDFs) in addition to text and images, are there plans to add the ability to parse additional content tags in `ChatPromptParser` to handle additional content types, like `BinaryContent`, `AudioContent`, etc.? (Claude can handle PDFs too - [see here](https://docs.anthropic.com/en/docs/build-with-claude/pdf-support))\r\n\r\nAdditional tags could include:\r\n* '&lt;audio&gt; *(base64 audio stream)* &lt;/audio&gt;' - Parsed into an `AudioContent` instance\r\n* '&lt;binary mimeType=\"*(mime type)*\"&gt; *(base64 content)* &lt;/binary&gt;' - Parsed into a `BinaryContent` instance, with `mimeType` defaulting to \"application/octet-stream\" if not present\r\n* '&lt;pdf&gt; *(base64 content)* &lt;/pdf&gt;' - Parsed into a new `PdfContent` class derived from `BinaryContent`\r\n\r\nMy application makes heavy use of the YAML prompt templates so this would be very helpful in not having to manually build chat histories for any operation involving inputs beyond text and images.\r\n\r\nI volunteer to add the above if it's not already planned for a near term release.\r\n\r\n(Maybe this is an extension of [this discussion](https://github.com/microsoft/semantic-kernel/discussions/8487)?)</div>",
    "comments": []
  },
  {
    "issue_number": 12297,
    "title": ".Net: Bug: KernelParameterMetadata not compatible with C# 7.3 (netstandard 2.0 and Framework 4.7.2)",
    "author": "cabal95",
    "state": "closed",
    "created_at": "2025-05-28T17:24:16Z",
    "updated_at": "2025-06-06T15:47:06Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nThe KernelParameterMetadata class uses init-only properties, but some of these do not have a constructor that allows setting them. When trying to create an instance of this class in .NET Framework 4.7.2 the compiler gives an error because init-only properties are not supported on C# 7.3.\n\nThe properties affected by this are:\n* Description\n* DefaultValue\n* ParameterType\n* Schema\n\nAs noted [here](https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/language-versioning) NET Standard 2.0 and all versions of .NET Framework only support C# 7.3.\n\n**To Reproduce**\nCreate a .NET Framework console with a reference to the Microsoft.SemanticKernal.Abstractions package and then paste in this code:\n\n```cs\nvar parameter = new KernelParameterMetadata( \"promptAsJson\" )\n{\n    Description = \"A JSON object with the information to register for the event.\",\n    IsRequired = true,\n    Schema = ParseSchema( function.InputSchema )\n};\n```\n\n**Expected behavior**\nA constructor should be provided to allow initializing all init-only properties.\n\n**Workaround**\nIt seems like I can use reflection to set those properties, but that isn't an ideal long-term solution.\n\n**Platform**\n - Language: C#\n - Source: Microsoft.SemanticKernel.Abstractions 1.54.0\n - AI model: n/a\n - IDE: Visual Studio 17.13.7\n - OS: Windows\n\n**Additional context**\nn/a",
    "comments": []
  },
  {
    "issue_number": 11876,
    "title": "Bug: SK function naming is to strict for use with MCP",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-05-02T18:54:19Z",
    "updated_at": "2025-06-06T08:55:22Z",
    "labels": [
      "bug",
      "function_calling",
      "modelcontextprotocol"
    ],
    "body": "**Describe the bug**\nSee: https://github.com/Azure/azure-mcp/issues/42#issuecomment-2847814145\n",
    "comments": [
      {
        "user": "jhzhu89",
        "body": "Will this be addressed in next release?\n"
      }
    ]
  },
  {
    "issue_number": 10070,
    "title": ".Net: C# package cleanup",
    "author": "markwallace-microsoft",
    "state": "open",
    "created_at": "2025-01-06T14:26:02Z",
    "updated_at": "2025-06-06T08:19:15Z",
    "labels": [
      ".NET",
      "kernel",
      "sk team issue",
      "Build",
      "SK-H2-Planning"
    ],
    "body": "Review C# package a determine if meta-namespaces can be created to make developer experience similar (anything else)\nAll non GA packages should have a disposition i.e., graduate, obsolete/replace, redesign\n\n- Plugins.Core package\n   - [x] Revise experimental APIs and remove experimental attribute - https://github.com/microsoft/semantic-kernel/pull/11906\n   - [x] Revise public API surface of Graduate Code Interpreter\n   - [x] Graduate the package from alpha to preview - https://github.com/microsoft/semantic-kernel/pull/11950\n   - ~[ ] Enable package validation after the next SK release - 1.50~\n\n- OpenApi package\n   - [x] Revise experimental APIs and update as needed - https://github.com/microsoft/semantic-kernel/pull/11745\n\n- PromptTemplates.Liquid\n   - [x] Revise experimental APIs and update as needed\n   - [x] Graduate the package - https://github.com/microsoft/semantic-kernel/pull/11716\n   - [x] Enable package validation after the next SK release - 1.49\n\n- Functions.Markdown\n   - [x] Add migration guide\n   - [x] Remove from SK repo\n\n- Planners\n   - [x] [.Net] Stop publishing them - https://github.com/microsoft/semantic-kernel/pull/11741\n   - [x] [.Net] Remove from SK repo - https://github.com/microsoft/semantic-kernel/pull/12399\n   - [ ] [Java] https://github.com/microsoft/semantic-kernel/issues/12401\n   - [ ] [Python] https://github.com/microsoft/semantic-kernel/issues/12400\n   - [ ] Remove documentation after python and java are ready\n \n\n- [ ] Graduate Bing and Google TextSearch\n- [ ] Remove obsolete packages: Planners, Orchestration Flow, Document, gRPC, MS Graph, ...",
    "comments": []
  },
  {
    "issue_number": 6597,
    "title": ".Net: Correlated filters across entire agent lifecycle",
    "author": "madsbolaris",
    "state": "open",
    "created_at": "2024-06-07T11:24:46Z",
    "updated_at": "2025-06-05T21:18:33Z",
    "labels": [
      ".NET",
      "kernel",
      "sk team issue",
      "experimental",
      "agents"
    ],
    "body": "In Semantic Kernel, it should be possible to create filters that can be correlated across the entire agent lifecycle:\n\n1. Invoke agent\n2. Render agent prompt template\n3. Call functions within prompts\n4. Serialize prompt to Chat History object\n5. Choose model\n6. Prepare execution settings for LLM\n7. Send Chat History object to LLM\n8. Receive raw result from LLM\n  - Respond to new message\n  - Respond to new function call(s)\n  - Respond to termination signal\n9. Append new message(s) to chat history\n10. Repeat steps 1-9 until termination signal has been received\n\nThe following are scenarios where having a way to correlate filters across the entire lifecycle\n\n> [!CAUTION]\n> These scenarios _may_ be solvable without full lifecycle filters, making some of these requirements unnecessary.\n\n- **Telemetry with correlation IDs** – For customers that use correlation IDs (e.g., 1.2.1), to trace the entire process, they'll need a way to hook into each step. This includes steps that could be addressed with things like custom HTTP handlers, because these steps wouldn't have context of the previous correlation ID to increment.\n- **Detect and remove PII that hasn't been shared** – To determine which PII to remove, the developer needs to first check which PII has already been shared by the user. To achieve this, a filter needs to be able to analyze the initial prompt, user messages, and previous function calls to determine what doesn't need to be scrubbed. Afterwards, the developer needs to compare PII coming from other function calls or the LLM response to determine if they are safe or not. For example, an agent may have the two function 1) get_current_user_details and 2) get_notes_about_user. The second one could accidentally grab information about other customers, the developer should be able to compare it with data that's already been shared in the chat history to determine this.\n- **Change available plugins based on previous data** – Earlier in the lifecycle, the developer may have determined which state the AI is in (e.g., perfoming_work, responding_to_user, authoring_text, generating_media, etc.). The developer could then use this information to change downstream behaviors: which prompt template to use, the model, the available plugins, levels of content safety, how the chat history is updated, etc. For example, if the AI is a \"responding_to_user\" mode, the developer may choose to use a simpler prompt template, a local model, different execution settings, simpler (or no) plugins, and a quick validation by a more expensive model before adding it to the chat history.\n\nFull lifecycle filters will also simplify adding of out-of-the-box filters to a kernel. Instead of adding individual filters, devs could just add a single Lifecycle filter.",
    "comments": [
      {
        "user": "waaverecords",
        "body": "Filters for step 7 and 8 would be very usefull for what I'm trying to achieve.\n\nI want to save all token usage somewhere, (not just for functions)\nIt would be much better dx to do such a thing in a filter, rather than doing that every time an invocation is done."
      },
      {
        "user": "waaverecords",
        "body": "Is this something I can work on and make a PR for?"
      }
    ]
  },
  {
    "issue_number": 12103,
    "title": ".Net: Bug: When using ChatCompletionAgent and the locally deployed llama3.2:3b model, the user's Chinese question became garbled in the function call parameters.",
    "author": "yong-zhang-newtera",
    "state": "closed",
    "created_at": "2025-05-16T00:18:57Z",
    "updated_at": "2025-06-05T17:21:11Z",
    "labels": [
      "bug",
      ".NET",
      "needs more info"
    ],
    "body": "**Describe the bug**\nFramework: Microsoft Semantic Kernel 1.49.0\n\nI am testing ChatCompletionAgent with a locally deployed llama3.2:3b to query a knowledge base with data in Chinese via a text search plugin. When a user asks a question in Chinese, the agent can invoke the text search plugin, but with a garbled Chinese text, causing the search to fail. Please see the screenshot below:\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/36bdc23a-ceda-40c7-9483-32c59c6a4aa9)\n\nI attach part of my code below:\n\n```\n        kernelBuilder.Services.AddOllamaChatCompletion(\n                modelId: LLMConfig.Instance.ConfigModel.ModelId,\n                endpoint: new Uri(LLMConfig.Instance.ConfigModel.ApiEndpoint)\n            );\n```\n\n        var textEmbeddingGeneration = vectorStoreFixture.TextEmbeddingGenerationService;\n            var vectorSearch = vectorStoreFixture.VectorStoreRecordCollection;\n            var customVectorSearch = new CustomVectorSearch(vectorSearch, threshold);\n\n            // Create a text search instance using the InMemory vector store.\n            var textSearch = new VectorStoreTextSearch<VectorRecordModel>(\n                customVectorSearch,\n                textEmbeddingGeneration);\n\n            var searchPlugin = KernelPluginFactory.CreateFromFunctions(\n                pluginName, description,\n                [textSearch.CreateGetTextSearchResults(searchOptions: searchOptions)]);\n\n            kernel.Plugins.Add(searchPlugin);\n\n           var kernel = kernelBuilder.Build();\n           ChatCompletionAgent faqAgent =\n               new()\n               {\n                   Name = \"SearchFAQAgent\",\n                   Instructions = LLMConfig.Instance.ConfigModel.Instructions,\n                   Kernel = kernel,\n                   Arguments =\n                       new KernelArguments(new OllamaPromptExecutionSettings()\n                       {\n                           FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()\n                       })\n               };\n`\n\n\n**Platform**\n - Language: [C#]\n - AI model: [llama3.2:3b]\n - IDE: [Visual Studio]\n - OS: [Windows]\n",
    "comments": [
      {
        "user": "RogerBarreto",
        "body": "@yong-zhang-newtera You provided some content and code here but is not clear for me what is the failure.\n\nAs you showed in the screenshot I couldn't notice any garbled error or anything that I can reproduce.\n\nAppreciate if you can provide the Ollama version, Model used, what is the expected result and what is actually wrong in the message.\n\nThanks."
      },
      {
        "user": "yong-zhang-newtera",
        "body": "Hi @RogerBarreto, thanks for looking into this issue. This is an intermittent issue where the original user's question in Chinese gets converted to a search plugin query in garbled Chinese by invoking ChatCompletionAgent with the plugin. For example, the original user's question is:\n\n\"仪器日常维护要做哪些工作？\" => (What tasks are involved in the daily maintenance of the instrument?)\n\nThe converted query used to invoke the search plugin becomes:\n\n\"简喜帮浞室’s说\\u3040四公万\"\n\n (It is rather a meaningless Chinese sentence than a garbled Chinese sentence)\n\nAs shown in the log below:\n\ntrce: Microsoft.Extensions.AI.FunctionInvokingChatClient\n      Invoking SearchFAQPlugin_GetTextSearchResults({\n        \"count\": \"1\",\n        \"query\": \"简喜帮浞室’s说\\u3040四公万\",\n        \"skip\": \"0\"\n      }).\n\nThis behavior is intermittent because it occasionally produces queries as expected, \"仪器日常维护\" which is \"daily maintenance of the instrument\" in English.\n\nMy testing configurations are:\n\n.Net 8\nSemantic Kernel 1.54.0\nOllama version is 0.6.6\nllama version is llama3.2:3b\n\nPlease let me know if you need any other info.\n\nThanks,\n\n\n"
      },
      {
        "user": "RogerBarreto",
        "body": "@yong-zhang-newtera I see the problem here.\n\nBasically the parameter \"query\": \"简喜帮浞室’s说\\u3040四公万\" is what comes back from the model to call the functions, this is a model response limitation when it attempts to call your function back. I might suggest trying a bigger llama3.2 model and see if the problem goes away.\n\nTo ensure this is a model behavior I suggest attempting to do the same query against the Ollama API directly (capture your current query and try via http to check how the request `toolCall` comes back)\n\nPlan B: If you have very fast Tokens per second, add a chinese-english / english-chinese translation in the calling loop (possible with a model that natively supports chinese language).\n"
      }
    ]
  },
  {
    "issue_number": 11649,
    "title": "Bug: ModelContextProtocol.McpException: Transport is not connected",
    "author": "hemantkathuria",
    "state": "open",
    "created_at": "2025-04-19T06:15:23Z",
    "updated_at": "2025-06-05T15:44:15Z",
    "labels": [
      "bug"
    ],
    "body": "Add Mcp Tools and keep on getting the below error randomly\n\nfail: Microsoft.SemanticKernel.KernelFunction[0]\n      Function Tools-GetCropHealthParameters failed. Error: Transport is not connected\n      ModelContextProtocol.McpException: Transport is not connected\n         at ModelContextProtocol.Shared.McpSession.SendRequestAsync(JsonRpcRequest request, CancellationToken cancellationToken)\n         at ModelContextProtocol.McpEndpointExtensions.SendRequestAsync[TParameters,TResult](IMcpEndpoint endpoint, String method, TParameters parameters, JsonTypeInfo`1 parametersTypeInfo, JsonTypeInfo`1 resultTypeInfo, Nullable`1 requestId, CancellationToken cancellationToken)\n         at ModelContextProtocol.Client.McpClientTool.InvokeCoreAsync(AIFunctionArguments arguments, CancellationToken cancellationToken)\n         at Microsoft.SemanticKernel.ChatCompletion.AIFunctionKernelFunction.InvokeCoreAsync(Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)\n         at Microsoft.SemanticKernel.KernelFunction.<>c__DisplayClass31_0.<<InvokeAsync>b__0>d.MoveNext()\n      --- End of stack trace from previous location ---\n         at Microsoft.SemanticKernel.Kernel.InvokeFilterOrFunctionAsync(NonNullCollection`1 functionFilters, Func`2 functionCallback, FunctionInvocationContext context, Int32 index)\n         at Microsoft.SemanticKernel.Kernel.OnFunctionInvocationAsync(KernelFunction function, KernelArguments arguments, FunctionResult functionResult, Boolean isStreaming, Func`2 functionCallback, CancellationToken cancellationToken)\n         at Microsoft.SemanticKernel.KernelFunction.InvokeAsync(Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)",
    "comments": [
      {
        "user": "SergeyMenshykh",
        "body": "Hi @hemantkathuria, it's quite difficult to tell what is going on without having the code. Please create a console app that reproduces this behavior and share it with us so we can start investigating. The first thing to look at would be the disposed McpClient that your app cached/keeps a reference to for accessing the McpServer."
      },
      {
        "user": "SergeyMenshykh",
        "body": "Link to the same issue in MCP C# repo - https://github.com/modelcontextprotocol/csharp-sdk/issues/333"
      },
      {
        "user": "hemantkathuria",
        "body": "Hello @SergeyMenshykh \nIt is a web app where I am facing this.\nBelow is the code in startup. But let me also try upgrading the package as suggested in MCP repo\n\n`// Add Kernel\nIKernelBuilder kernelBuilder = builder.Services.AddKernel();\n\nstring mcpServerUrl = builder.Configuration.GetSection(\"MCPClient\").GetValue<string>(\"MCPSSEServerUrl\");\n\nawait AddTools(mcpServerUrl, kernelBuilder);\n\n//Add Azure Open AI\nbuilder.Services.AddAzureOpenAIChatCompletion(\n        deploymentName: builder.Configuration.GetSection(\"AIServices:AzureOpenAI\").GetValue<string>(\"DeploymentName\"),\n        endpoint: builder.Configuration.GetSection(\"AIServices:AzureOpenAI\").GetValue<string>(\"Endpoint\"),\n        //Use the Azure CLI (for local) or Managed Identity (for Azure running app) to authenticate to the Azure OpenAI service\n        credentials: new DefaultAzureCredential());\n\nbuilder.Services.AddTransient<ChatCompletionAgent>((sp) =>\n{\n    return new ChatCompletionAgent()\n    {\n        Name = AgentName,\n        Instructions = AgentInstructions,\n        Kernel = sp.GetRequiredService<Kernel>(),\n        Arguments = new KernelArguments(new OpenAIPromptExecutionSettings()\n        {\n            FunctionChoiceBehavior = FunctionChoiceBehavior.Auto(),\n            ResponseFormat = \"json_object\"\n        })\n        \n    };\n});\n\n\n// Register IStorage.  For development, MemoryStorage is suitable.\n// For production Agents, persisted storage should be used so\n// that state survives Agent restarts, and operate correctly\n// in a cluster of Agent instances.\nbuilder.Services.AddSingleton<IStorage, MemoryStorage>();\n\nvar app = builder.Build();\n\napp.MapGet(\"/\", () => \"Hello Agent!\");\n\napp.UseStaticFiles();\n\napp.UseDeveloperExceptionPage();\n\napp.MapControllers();//.AllowAnonymous();\n\napp.Run();\n\n\n/// <summary>\n/// Creates an MCP client and connects it to the MCPServer server.\n/// </summary>\n/// <returns>An instance of <see cref=\"IMcpClient\"/>.</returns>\nstatic Task<IMcpClient> CreateSSeMcpClientAsync(string mcpServerUrl)\n{\n    \n\n    return McpClientFactory.CreateAsync(new SseClientTransport(new()\n    {\n        Name = \"MCPServer\",\n\n        // Point the client to the MCPServer server executable\n        Endpoint = new Uri(mcpServerUrl),\n    }));\n}\n\nstatic async Task AddTools(string mcpServerUrl, IKernelBuilder kernelBuilder)\n{\n    IMcpClient mcpClient = await CreateSSeMcpClientAsync(mcpServerUrl);\n\n    IList<McpClientTool> tools = await mcpClient.ListToolsAsync();\n\n    kernelBuilder.Plugins.AddFromFunctions(\"Tools\", tools.Select(aiFunction => aiFunction.AsKernelFunction()));\n\n    //// Enable automatic function calling\n    //OpenAIPromptExecutionSettings executionSettings = new()\n    //{\n    //    Temperature = 0,\n    //    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto(options: new() { RetainArgumentTypes = true })\n    //};\n\n}`"
      }
    ]
  },
  {
    "issue_number": 10961,
    "title": "Python: Implement a user proxy agent",
    "author": "TaoChenOSU",
    "state": "closed",
    "created_at": "2025-03-13T17:03:55Z",
    "updated_at": "2025-06-05T15:35:05Z",
    "labels": [
      "python",
      "agents"
    ],
    "body": "---\nname: User proxy agent\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\nAdd and test the following to SK:\n```python\n# Copyright (c) Microsoft. All rights reserved.\n\nimport sys\n\nif sys.version_info >= (3, 12):\n    from typing import override  # pragma: no cover\nelse:\n    from typing_extensions import override  # pragma: no cover\n\nimport asyncio\nimport logging\nimport threading\nfrom collections.abc import AsyncIterable\nfrom typing import ClassVar\n\nfrom semantic_kernel.agents import AgentGroupChat, ChatCompletionAgent\nfrom semantic_kernel.agents.agent import Agent\nfrom semantic_kernel.agents.channels.agent_channel import AgentChannel\nfrom semantic_kernel.agents.strategies import TerminationStrategy\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\nfrom semantic_kernel.contents import AuthorRole, ChatMessageContent\nfrom semantic_kernel.exceptions import AgentChatException\n\nlogging.basicConfig(level=logging.WARNING)\n\n\n#####################################################################\n# 1. A minimal \"UserInputChannel\" that reads from the console.\n#####################################################################\n\n\nclass UserInputChannel(AgentChannel):\n    \"\"\"A simple AgentChannel that reads user input from the console.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.lock = threading.Lock()\n        self.closed = False\n        self.history: list[ChatMessageContent] = []\n\n    @override\n    async def receive(self, history: list[ChatMessageContent]) -> None:\n        \"\"\"Receive a list of ChatMessageContent and append to the local history.\"\"\"\n        for message in history:\n            self.history.append(message)\n\n    @override\n    async def invoke(self, agent: \"Agent\") -> AsyncIterable[tuple[bool, ChatMessageContent]]:\n        \"\"\"Async generator returning exactly one `(bool, ChatMessageContent)`.\"\"\"\n        if self.closed:\n            raise AgentChatException(\"UserInputChannel is closed.\")\n\n        # Use `asyncio.to_thread(...)` to avoid blocking the event loop with input().\n        user_input = await asyncio.to_thread(input, \"You (single-turn): \")\n\n        user_message = ChatMessageContent(role=AuthorRole.USER, content=user_input)\n\n        self.history.append(user_message)\n\n        yield True, user_message\n\n    @override\n    async def invoke_stream(self, agent: \"Agent\", history: \"list[ChatMessageContent]\"):\n        \"\"\"\n        Async generator of ChatMessageContent.\n        Each line typed by the user is emitted as a separate message.\n\n        # NOTE(evmattso): this method needs testing and may need adjustments.\n        \"\"\"\n        raise NotImplementedError(\"invoke_stream is not implemented.\")\n\n    @override\n    async def get_history(self) -> AsyncIterable[ChatMessageContent]:\n        \"\"\"Async generator of ChatMessageContent in reverse order.\"\"\"\n\n        for message in reversed(self.history):\n            yield message\n\n    @override\n    async def reset(self) -> None:\n        with self.lock:\n            self.closed = True\n            self.history.clear()\n\n\n#####################################################################\n# 2. Define a UserInputAgent that binds to UserInputChannel.\n#####################################################################\nclass UserInputAgent(Agent):\n    channel_type: ClassVar[type[AgentChannel]] = UserInputChannel\n\n    def __init__(self, name: str = \"UserInputAgent\"):\n        super().__init__(name=name)\n\n    async def create_channel(self) -> AgentChannel:\n        return UserInputChannel()\n\n    @override\n    async def get_response(self, *args, **kwargs) -> ChatMessageContent:\n        \"\"\"Get a response from the agent.\"\"\"\n        raise NotImplementedError(\"get_response is not implemented.\")\n\n    @override\n    async def invoke(self, *args, **kwargs) -> AsyncIterable[ChatMessageContent]:\n        \"\"\"Invoke the agent.\"\"\"\n        raise NotImplementedError(\"invoke is not implemented.\")\n\n    @override\n    async def invoke_stream(self, *args, **kwargs) -> AsyncIterable[ChatMessageContent]:\n        \"\"\"Invoke the agent as a stream.\"\"\"\n        raise NotImplementedError(\"invoke_stream is not implemented.\")\n\n\n#####################################################################\n# 3. Define a User Input Exit Termination Strategy to exit when the\n#    user types \"exit\".\n#####################################################################\nclass UserInputExitTerminationStrategy(TerminationStrategy):\n    async def should_terminate(self, chat: AgentGroupChat, message: ChatMessageContent):\n        # Check if the message content includes \"exit\"\n        return message[0].role == AuthorRole.USER and \"exit\" in message[-1].content.lower()\n\n\n#####################################################################\n# 4. Example usage with an AgentGroupChat\n#####################################################################\nasync def main():\n    user_agent = UserInputAgent(name=\"UserProxyAgent\")\n\n    assistant_agent = ChatCompletionAgent(\n        service=AzureChatCompletion(),\n        name=\"EchoAssistant\",\n        instructions=\"You are a friendly AI that echoes user messages with a playful spin.\",\n    )\n\n    group_chat = AgentGroupChat(\n        agents=[user_agent, assistant_agent],\n        termination_strategy=UserInputExitTerminationStrategy(),\n    )\n\n    while not group_chat.is_complete:\n        async for msg in group_chat.invoke():\n            if msg.role != AuthorRole.USER:\n                print(f\"{msg.role}({msg.name}): {msg.content}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "comments": [
      {
        "user": "TaoChenOSU",
        "body": "Closing because we are deprecating `AgentGroupChat`."
      }
    ]
  },
  {
    "issue_number": 10739,
    "title": "Python: New Feature:  Update samples in the ai-agents-for-beginners repo",
    "author": "TaoChenOSU",
    "state": "closed",
    "created_at": "2025-02-28T19:32:24Z",
    "updated_at": "2025-06-05T15:34:18Z",
    "labels": [
      "sk team issue",
      "enhancement",
      "documentation",
      "stale"
    ],
    "body": "---\nname: Update samples in the ai-agents-for-beginners repo\nabout: Agents\n---\nMicrosoft has this repository to help people get started with agents: https://github.com/microsoft/ai-agents-for-beginners\n\nSome of the samples in there are either incorrect or outdated. We should update them.\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      },
      {
        "user": "TaoChenOSU",
        "body": "Closing due to resource constraints"
      }
    ]
  },
  {
    "issue_number": 12354,
    "title": ".Net: New Feature: Support filtering deprecated endpoints when using OpenAPI",
    "author": "frederikrosenberg",
    "state": "open",
    "created_at": "2025-06-03T13:40:23Z",
    "updated_at": "2025-06-05T15:34:04Z",
    "labels": [
      ".NET",
      "openapi"
    ],
    "body": "---\nname: Support filtering deprecated endpoints when using OpenAPI\nabout: Support filtering deprecated endpoints when using OpenAPI\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\nWhen loading functions based on a OpenAPI specification it is not possible to filter out any endpoints which is deprecated due to the property not being exposed and not available in the Semantic Kernel representation of an OpenAPI operation.\n\nThe only workaround for this is to parse the OpenAPI specification, remove the deprecated endpoints and then serialize the specification again, and then parse it using Semantic Kernel, unless I have missed something.\n\nAnother question: is there any reason that the `OperationSelectionPredicate` does not expose all endpoint properties such as extensions? If any filtering on extensions is needed then the less efficient method of filtering is required. https://github.com/microsoft/semantic-kernel/blob/dda17e4ce8e4b123eafb09ec1996df6643e6a8b9/dotnet/samples/Concepts/Plugins/OpenApiPlugin_Filtering.cs#L149\n\n",
    "comments": []
  },
  {
    "issue_number": 11123,
    "title": ".Net: [MEVD] Stop serializing to intermediate type in relational connectors",
    "author": "roji",
    "state": "open",
    "created_at": "2025-03-22T09:05:14Z",
    "updated_at": "2025-06-05T15:14:44Z",
    "labels": [
      ".NET",
      "msft.ext.vectordata"
    ],
    "body": "The PostgreSQL connector (and others) doesn't directly deserialize from the DbDataReader; there's a temporary `Dictionary<string, object>` \"storage model\", which is read from the DbDataReader and then the CLR type is populated from that. This adds an additional unnecessary intermediate conversion, memory allocations for the temporary dictionary, and various dictionary lookups. We should change the relevant connectors to read and write directly instead. The dictionary also forces boxing of value types to occur (#11183).\n\nNote: this will prevent the user of custom mappers in their current form, since they're designed around the idea of converting one object to another; but when dealing with ADO.NET, database results aren't an object - they're exposed via DbDataReader which must be exposed. So a PostgreSQL custom mapper would need to accept a DbDataReader and implement the logic for reading properties out of it directly (and for the other side - writing - it would need to accept or return a DbCommand, on which it would populate DbParameters).",
    "comments": []
  },
  {
    "issue_number": 11651,
    "title": ".Net: Bug: Function calling fails when Gemini returns function call as non-first part",
    "author": "silmon27",
    "state": "closed",
    "created_at": "2025-04-19T20:26:09Z",
    "updated_at": "2025-06-05T10:44:09Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nWhen using the Google connector in .NET Semantic Kernel with Gemini models (e.g., gemini-2.0-flash), function calling does not always work as expected. If the model returns a function call as any part other than the first in its response, Semantic Kernel ignores it and defaults to using only the first part. This leads to auto function call behavior not working reliably with models that frequently return multiple parts (e.g., text + function call).\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Use Semantic Kernel with the Google connector and a recent Gemini model (e.g., gemini-2.0-flash).\n2. Prompt the model in a way that triggers both text and function call responses (very common).\n3. Observe that only the first returned part is handled; if it’s not a function call, invoking the function is skipped.\n4. Auto function call behavior does not work if the function call is in any part other than the first.\n\n**Expected behavior**\nSemantic Kernel should handle any function call returned by Gemini, regardless of which part of the response it appears in—not just the first part.\n\n**Platform**\n - Language: C#\n - Source: 1.47.0-alpha\n - AI model: gemini-2.0-flash\n\n**Additional context**\n[https://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/Connectors/Connectors.Google/Core/Gemini/Clients/GeminiChatCompletionClient.cs#L607](url)\n```\nprivate GeminiChatMessageContent GetChatMessageContentFromCandidate(GeminiResponse geminiResponse, GeminiResponseCandidate candidate)\n{\n    GeminiPart? part = candidate.Content?.Parts?[0];\n    GeminiPart.FunctionCallPart[]? toolCalls = part?.FunctionCall is { } function ? [function] : null;\n    return new GeminiChatMessageContent(\n        role: candidate.Content?.Role ?? AuthorRole.Assistant,\n        content: part?.Text ?? string.Empty,\n        modelId: this._modelId,\n        functionsToolCalls: toolCalls,\n        metadata: GetResponseMetadata(geminiResponse, candidate));\n}\n```\n\nThis considers only the first part of the response for function calling. However, Gemini models often return multiple parts (e.g., text + function call). If the function call is not the first part, it is ignored completely.",
    "comments": [
      {
        "user": "blackwire",
        "body": "@markwallace-microsoft - I need this feature so I added a proposed fix. Hopefully this is helpful.\nhttps://github.com/microsoft/semantic-kernel/pull/11664"
      }
    ]
  },
  {
    "issue_number": 10074,
    "title": ".Net: Plugins at scale",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-01-06T14:47:13Z",
    "updated_at": "2025-06-05T09:30:34Z",
    "labels": [
      ".NET",
      "sk team issue",
      "Build",
      "SK-H2-Planning"
    ],
    "body": "Provide samples and guidelines on how to use multiple plugins:\n\n- [x] Fixes for known reliability issues e.g., hallucinated function names\n- [x] Add the ability to change functions being advertised dynamically based on current context\n    - [x] Works with M.E.AI \n- [x] Learn site documentation and samples showing how to manage a large set of plugins/functions\n - https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-contextual-function-selection?pivots=programming-language-csharp\n - https://devblogs.microsoft.com/semantic-kernel/smarter-sk-agents-with-contextual-function-selection/ \n~~- [ ] Samples/tests showing how to evaluate how a new model behaves~~\n~~- [ ] Initial task is to create an ADR describing the approach~~",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "Consider changing this to be a multi-agent sample where each agent has a limited set of functions and the correct agent is selected."
      }
    ]
  },
  {
    "issue_number": 11629,
    "title": ".Net: Bug: Sample fails after switching to AddGoogleAIGeminiChatCompletion",
    "author": "bradyoo12",
    "state": "closed",
    "created_at": "2025-04-17T15:00:03Z",
    "updated_at": "2025-06-05T08:09:01Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nI was following https://devblogs.microsoft.com/semantic-kernel/integrating-model-context-protocol-tools-with-semantic-kernel-a-step-by-step-guide/. \nkernel.Plugins.AddFromFunctions(\"GitHub\", tools.Select(aiFunction => aiFunction.AsKernelFunction()));\nworked well until I change AddOpenAIChatCompletion to AddGoogleAIGeminiChatCompletion and its parameter values.\nWith AddGoogleAIGeminiChatCompletion, the response that should be about the latest commits on Github is about those commited about one year ago. It is not calling a function but just making up using the data in LLM.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Install the followings in Nuget package: ModelContextProtocol (0.1.0-preview.10), Microsoft.SemanticKernel (1.47.0), Microsoft.SemanticKernel.Connectors.Google (1.47.0-alpha)\n2. Run  the followings\n```\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.Connectors.Google;\nusing Microsoft.SemanticKernel.Connectors.OpenAI;\nusing ModelContextProtocol.Client;\nusing ModelContextProtocol.Protocol.Transport;\n\n#pragma warning disable SKEXP0070\n#pragma warning disable SKEXP0001\n\n// Prepare and build kernel\nvar builder = Kernel.CreateBuilder();\n#region AzureOpenAI\n/*\nbuilder.Services.AddAzureOpenAIChatCompletion(\n    deploymentName: Credential.AzureOpenAI.ModelId,\n    endpoint: Credential.AzureOpenAI.Endpoint,\n    apiKey: Credential.AzureOpenAI.ApiKey);\nOpenAIPromptExecutionSettings executionSettings = new()\n{\n    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto(options: new() { RetainArgumentTypes = true })\n};\n*/\n#endregion\n\n#region GoogleAIGemini\nbuilder.Services.AddGoogleAIGeminiChatCompletion(\n    Credential.GoogleAIGemini.ModelId,\n    Credential.GoogleAIGemini.ApiKey\n);\nGeminiPromptExecutionSettings executionSettings = new()\n{\n    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto(options: new() { RetainArgumentTypes = true }), // shows '2024 commits'\n    //ToolCallBehavior = GeminiToolCallBehavior.AutoInvokeKernelFunctions // 400 bad request\n};\n#endregion\n\nKernel kernel = builder.Build();\n\n\n// Create an MCPClient for the GitHub server\nawait using IMcpClient mcpClient = await McpClientFactory.CreateAsync(new StdioClientTransport(new()\n{\n    Name = \"GitHub\",\n    Command = \"npx\",\n    Arguments = [\"-y\", \"@modelcontextprotocol/server-github\"],\n}));\n\nvar tools = await mcpClient.ListToolsAsync().ConfigureAwait(false);\nkernel.Plugins.AddFromFunctions(\"GitHub\", tools.Select(aiFunction => aiFunction.AsKernelFunction()));\n\nvar prompt = \"Summarize the last four commits to the microsoft/semantic-kernel repository?\";\nvar result = await kernel.InvokePromptAsync(prompt, new(executionSettings)).ConfigureAwait(false);\nConsole.WriteLine($\"\\n\\n{prompt}\\n{result}\");\nConsole.ReadLine();\n```\n\n**Expected behavior**\nA response should include the latest 4 commits of microsoft/semantic-kernel repository.\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/7857d0a7-e253-4f94-985b-d25b0701b86c)\n\n**Platform**\n - Language: C#\n - Source: dotnet add package Microsoft.SemanticKernel.Connectors.Google, dotnet add package ModelContextProtocol --prerelease \n - AI model: gemini-2.5-pro-preview-03-25\n - IDE: Visual Studio 2022\n - OS: Windows\n",
    "comments": []
  },
  {
    "issue_number": 12338,
    "title": ".Net: Bug: AuthorName is always null in ResultTransform when using (Ex : GroupChatOrchestrator)",
    "author": "rvinothrajendran",
    "state": "closed",
    "created_at": "2025-06-02T13:38:59Z",
    "updated_at": "2025-06-05T07:29:20Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\n\nIn Agent Orchestration, within the ResultTransform, the ChatMessageContent contains an AuthorName property. However, this value is always null, making it impossible to determine which agent produced the final response.\n\n**To Reproduce**\nUse ResultTransform callback \n\n**Expected behavior**\nThe AuthorName should reflect the agent that generated the response.\n\n\n**Platform**\n - Language: C#\n - Source:  \n - AI model: \n - IDE: Visual Studio\n - OS: Windows\n",
    "comments": [
      {
        "user": "crickman",
        "body": "@rvinothrajendran - The group-chat result isn't directly produced by any single agent.  The result is synthesized via the `FilterResults` method of the `GroupChatManager`.   You can override this to perform any logic you like, but ultimately the \"result\" is _just_ outside of the agent chat.\n\n"
      }
    ]
  },
  {
    "issue_number": 12384,
    "title": "Python: AzureAIAgent support for Fabric, Connected Agent and Logic Apps",
    "author": "moonbox3",
    "state": "open",
    "created_at": "2025-06-05T04:20:08Z",
    "updated_at": "2025-06-05T04:20:31Z",
    "labels": [
      "python",
      "agents"
    ],
    "body": null,
    "comments": []
  },
  {
    "issue_number": 12370,
    "title": "Python: Bug: raise NotImplementedError",
    "author": "pietrobr",
    "state": "closed",
    "created_at": "2025-06-04T15:13:00Z",
    "updated_at": "2025-06-05T03:45:30Z",
    "labels": [
      "bug",
      "python",
      "triage"
    ],
    "body": "Running this code\n# Example using Semantic Kernel with AzureChatCompletion\nimport asyncio\nfrom semantic_kernel import Kernel\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureChatPromptExecutionSettings\nfrom semantic_kernel.contents import ChatHistory\n\n# Initialize the kernel\nkernel = Kernel()\n\n# Configure Azure OpenAI Chat Completion\nchat_completion_service = AzureChatCompletion(\n    endpoint=\"https://<my>-resource.openai.azure.com/\",\n    api_key=\"mykey\",\n    deployment_name=\"gpt-4.1-mini\"\n)\nkernel.add_service(chat_completion_service)\n\n# Define a system message and chat history\nsystem_message = \"\"\"\nYou are a chat bot. Your name is Mosscap and\nyou have one goal: figure out what people need.\nYour full name, should you need to know it, is\nSplendid Speckled Mosscap. You communicate\neffectively, but you tend to answer with long\nflowery prose.\n\"\"\"\nchat_history = []\nchat_history = ChatHistory(system_message=system_message)\n\nasync def chat() -> bool:\n    try:\n        user_input = input(\"User:> \")\n    except (KeyboardInterrupt, EOFError):\n        print(\"\\n\\nExiting chat...\")\n        return False\n\n    if user_input.lower().strip() == \"exit\":\n        print(\"\\n\\nExiting chat...\")\n        return False\n\n    chat_history.add_user_message(user_input)\n\n    settings = AzureChatPromptExecutionSettings(\n        max_tokens=2000,\n        temperature=0.7,\n        top_p=0.8\n    )\n\n    response = await chat_completion_service.get_chat_message_content(\n        chat_history=chat_history,\n        settings=settings\n    )\n\n    print(f\"Mosscap:> {response}\")\n    chat_history.add_assistant_message(response)\n\n    return True\n\nasync def main() -> None:\n    chatting = True\n    while chatting:\n        chatting = await chat()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n\non this line: chat_history.add_assistant_message(response)\n\nchat_history.py\", line 140, in add_assistant_message\n    raise NotImplementedError\nNotImplementedError\n\n-----\nName: semantic-kernel                                                          \nVersion: 1.32.1\nSummary: Semantic Kernel Python SDK\nHome-page: https://learn.microsoft.com/en-us/semantic-kernel/overview/\nAuthor: \nAuthor-email: Microsoft <SK-Support@microsoft.com>\nLicense: \nLocation: C:\\Python312\\Lib\\site-packages\nRequires: aiohttp, aiortc, azure-ai-agents, azure-ai-projects, azure-identity, cloudevents, defusedxml, jinja2, nest-asyncio, numpy, openai, openapi_core, opentelemetry-api, opentelemetry-sdk, prance, protobuf, pybars4, pydantic, pydantic-settings, scipy, typing-extensions, websockets",
    "comments": [
      {
        "user": "TaoChenOSU",
        "body": "Hi!\n\nIf you are adding a chat message content, you don't need to use the type-specific method to add messages of certain roles. You can simply use the more generic API `add_message`, i.e. `chat_history.add_message(response)`.\n\nThe type-specific methods are good for when you only have a string or a list of contents, i.e. `chat_history.add_assistant_message(\"How can I help you?\")`."
      },
      {
        "user": "moonbox3",
        "body": "Closing as the answer has been provided. "
      }
    ]
  },
  {
    "issue_number": 12383,
    "title": "Python: Ability to provide additional annotations/descriptions on the `AzureAISearchTool` to encourage the LLM to choose this tool",
    "author": "jordanbean-msft",
    "state": "closed",
    "created_at": "2025-06-05T02:36:29Z",
    "updated_at": "2025-06-05T03:42:48Z",
    "labels": [
      "python",
      "triage"
    ],
    "body": "---\nname: Ability to provide additional annotations/descriptions on the `AzureAISearchTool` to encourage the LLM to choose this tool\nabout: Ability to provide additional annotations/descriptions on the `AzureAISearchTool` to encourage the LLM to choose this tool\n---\n\nThe current `AzureAISearchTool` only takes the connection id & index name for an AI Search Index.\n\n```python\nai_search = AzureAISearchTool(\n        index_connection_id=ai_search_conn_id, \n        index_name=AZURE_AI_SEARCH_INDEX_NAME,\n    )\n```\n\nThis provides almost no information to the LLM to let it know when it should call this tool. How can the LLM know what data is inside this index or when it should call it when it only has the name of the index?\n\nI would suggest adding a `description` argument or some other way to indicate the data the index contains so the LLM knows when it should use this tool. \n\nSomething like this:\n\n```python\nai_search = AzureAISearchTool(\n        index_connection_id=ai_search_conn_id, \n        index_name=AZURE_AI_SEARCH_INDEX_NAME,\n        name=\"find-thingamajig-reference-data\",\n        description=\"This index contains reference data for building thingamajigs. Use this index when the user question references thingamajigs, but not gizmos.\"\n    )\n```\n\nA `name` argument would also be useful (both for helping the LLM and making it easier to write a prompt that encourages the LLM to choose this tool).\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Hi @jordanbean-msft, SK doesn't own this abstraction. \n\nWe import it like so:\n\n```python\nfrom azure.ai.agents.models import AzureAISearchTool\n```\n\nCan you please file this request in the `azure-idk-for-python` repo? Link: https://github.com/Azure/azure-sdk-for-python. If they do support this at some point, we'll pull in the new package while allow devs to take advantage of the added metadata. Thanks. "
      }
    ]
  },
  {
    "issue_number": 10812,
    "title": ".Net: New Feature: Support IKernelBuilderPlugins.AddFromPromptDirectory for YAML",
    "author": "grafanaKibana",
    "state": "open",
    "created_at": "2025-03-05T17:48:54Z",
    "updated_at": "2025-06-05T02:13:52Z",
    "labels": [
      ".NET",
      "stale"
    ],
    "body": "---\nname: \"Add support for YAML Prompt Directory in IKernelBuilderPlugins\"  \nabout: \"Allow users to add YAML prompt functions as a plugin with a single method call, similar to AddFromPromptDirectory for JSON/TXT, reducing boilerplate code.\"\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n### Feature Request: AddFromYamlPromptDirectory\n\n#### Motivation\nWhen working with JSON and TXT prompt files, developers can easily add them to the kernel’s plugin collection using `IKernelBuilderPlugins.AddFromPromptDirectory`. In contrast, YAML prompt files require extra steps: manually reading each file, creating functions via `CreateFunctionFromPromptYaml`, and then adding them with `AddFromFunctions`. This extra overhead makes using YAML less streamlined.\n\n#### Proposal\nIntroduce a new extension method, `AddFromYamlPromptDirectory`, that:\n- Accepts a directory containing YAML prompt files.\n- Reads and converts each YAML prompt into a kernel function using `CreateFunctionFromPromptYaml`.\n- Automatically groups these functions under a single plugin in the kernel.\n\nFor example, with the new method, developers could write:\n\n```csharp\nvar pluginsPath = Path.Combine(Directory.GetCurrentDirectory(), \"Plugins\");\nforeach (var directory in Directory.EnumerateDirectories(pluginsPath))\n{\n    kernelBuilder.Plugins.AddFromYamlPromptDirectory(directory, \"Plugin Description\", promptTemplateFactory: new KernelPromptTemplateFactory());\n}\n```\n\nWhile currently we have to do something like this (Pseudo Code):\n\n```csharp\nvar kernel = kernelBuilder.Build();\nvar pluginsPath = Path.Combine(Directory.GetCurrentDirectory(), \"Plugins\");\nforeach (var directory in Directory.GetDirectories(pluginsPath))\n{\n    var functions = new List<KernelFunction>();\n    foreach (var file in Directory.GetFiles(directory, \"*.yaml\"))\n    {\n        var yamlContent = await File.ReadAllTextAsync(file);\n        functions.Add(kernel.CreateFunctionFromPromptYaml(yamlContent));\n    }\n    var pluginName = Path.GetFileName(directory);\n    kernelBuilder.Plugins.AddFromFunctions(pluginName, functions: functions);\n}\n```\n\n\n#### Benefits\n- Simplified Code: Reduces boilerplate by allowing YAML prompt plugins to be added in one call.\n- Consistency: Aligns YAML processing with the established JSON/TXT workflow.\n- Improved Developer Experience: Less error-prone and more intuitive for users who prefer YAML.\n\n#### Considerations\n- The new method should accept optional parameters (such as plugin name and a custom IPromptTemplateFactory) similar to AddFromPromptDirectory.\n\n#### Additional Note\nIn additional to this feature request, it will be beneficial to have ability add YAML functions to the `KernelBuilder` too, not only to the `Kernel`\n\nLooking forward to your feedback and suggestions.",
    "comments": [
      {
        "user": "sophialagerkranspandey",
        "body": "Hi @grafanaKibana, thanks so much for sharing this. Would you be interesting to contribute to this? "
      },
      {
        "user": "grafanaKibana",
        "body": "Hi @sophialagerkranspandey, I'd love to! I'll do my best, though it might take some time to get familiar with the codebase. I'll share my results later, whether I succeed or not."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 12373,
    "title": "Python: Bug: add_plugin_from_openapi() has hardcoded 5 seconds timeout",
    "author": "GitAashishG",
    "state": "closed",
    "created_at": "2025-06-04T18:21:38Z",
    "updated_at": "2025-06-04T23:38:52Z",
    "labels": [
      "bug",
      "python",
      "openapi"
    ],
    "body": "When adding a plugin using add_plugin_from_openapi(), the timeout is [hardcoded to 5-seconds](https://github.com/microsoft/semantic-kernel/blob/2a78664add11baf68ef407d5baab528009511c5e/python/semantic_kernel/connectors/openapi_plugin/openapi_runner.py#L177).\n\nThis can be side-stepped by creating custom client for now:\n```\n            custom_client = httpx.AsyncClient(timeout=custom_timeout)\n\n            # Create execution settings with the custom HTTP client\n            execution_settings = OpenAPIFunctionExecutionParameters(\n                http_client=custom_client\n            )\n\n            self.kernel.add_plugin_from_openapi(\n                plugin_name=plugin_name,\n                openapi_document_path=openapi_url,\n                description=description,\n                execution_settings=execution_settings,\n     )\n```\n",
    "comments": []
  },
  {
    "issue_number": 12355,
    "title": "Python: New Feature: OllamaChatPromptExecutionSettings does not support structured outputs",
    "author": "xyang2013",
    "state": "open",
    "created_at": "2025-06-03T14:03:36Z",
    "updated_at": "2025-06-04T12:12:29Z",
    "labels": [
      "python",
      "triage"
    ],
    "body": "The following does not work:\n```\nsettings = kernel.get_prompt_execution_settings_from_service_id(service_id)\nsettings.format = InputScore.model_json_schema()\n```\n\nLinks:\n\nhttps://learn.microsoft.com/en-us/python/api/semantic-kernel/semantic_kernel.connectors.ai.ollama.ollama_prompt_execution_settings.ollamachatpromptexecutionsettings?view=semantic-kernel-python\n\nhttps://ollama.com/blog/structured-outputs",
    "comments": [
      {
        "user": "xyang2013",
        "body": "I resolved the issue by modifying `ollama_prompt_execution_settings.py`.\n\nOriginal:\n```\nclass OllamaPromptExecutionSettings(PromptExecutionSettings):\n    \"\"\"Settings for Ollama prompt execution.\"\"\"\n\n    format: Literal[\"json\"] | None = None\n    options: dict[str, Any] | None = None\n```\n\nMy fix:\n```\nclass OllamaPromptExecutionSettings(PromptExecutionSettings):\n    \"\"\"Settings for Ollama prompt execution.\"\"\"\n\n    format: dict[str, Any] | None = None\n    options: dict[str, Any] | None = None\n```\n"
      }
    ]
  },
  {
    "issue_number": 12060,
    "title": ".Net: Bug: MCP SSE Server and Semantic Kernel,  DI, Blazor",
    "author": "mhackermsft",
    "state": "open",
    "created_at": "2025-05-14T17:36:04Z",
    "updated_at": "2025-06-04T10:50:36Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nI have a Blazor Server app where I am registering SK as a singleton so it can be easily used within my application.  I configure it with AzureOpenAIChatCompletion and AzureOpenAITextEmbeddingGeneration.  Running the app, this works perfectly.\n\nI now want to add in MCPClient capabilities.  I read MCPServer configuration data from my appsettings.json file.  I do a foreach over all of the settings and then create an MCPClient for each and then add them to the SK Plugins collection.   All of this seems to work fine.\n\nThe issue:  If I have an SSE MCPServer configured, SK will only call it once properly and get results.  If a user asks a question that triggers the tool / MCPServer a second time the connection the MCPServer hangs and times out. \n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Clone https://github.com/mhackermsft/SKMCPDemo and deploy the MCPServerFunction to Azure Function (Linux)\n2. Clone https://github.com/mhackermsft/BlazorAIChat and configure the appsettings.json to use Azure OpenAI gpt-4o or gpt4.1 models.  Configure the MCPServer section in appsettings.json to reference the MCPServer deployed to Azure Functions.\n3. Run the BlazorAIChat application, start a new chat, ask a question about the weather in Detroit.  You should get a proper weather report.\n4. In the same chat session, ask about the weather in Boston.  This time the application will hang until the attempted call to the MCP Server times out.  The AI Assistant will mention that there was a technical issue.\n\n**Expected behavior**\nMultiple requests for weather information in one or more chat sessions should work without timing out.\n\n**Screenshots**\nn/a\n\n**Platform**\n - Language: C#\n - Source: Nuget 1.49.0\n - AI model: gpt-4.1\n - IDE: Visual Studio\n - OS: Windows\n\n**Additional context**\nSTDIO MCP Servers seem to work fine over and over.  Only multiple calls to SSE servers tend to cause a timeout.  The MCP Server appears to always be functional and that this is a client side problem.",
    "comments": [
      {
        "user": "mhackermsft",
        "body": "I did a bit more testing and digging into this...\n\nI refactored my code to ensure that a clean HttpClient object without any retry policies are being used for the MCP client.  I also switch to a transient service registration for Semantic Kernel.   This did not solve the problem I am seeing.\n\nWhat I noticed is that if I ask a question that calls the MCP SSE server hosted on Azure Functions it works the first time.  If a follow-up question is asked that triggers a call to the MCP Server it hangs.  If I let it timeout and the chat bot to respond that it failed, I can ask it to \"try again\" it works.   \n\nIn my logs I see that for every question, calls are being sent to the MCP Server.  \n\nIf I wait a bit between questions that make calls to the MCP Server, all of them work.  It looks like I have to wait about 2 minutes or so for it to work.  Here is a conversation with timestamps:\n\n![Image](https://github.com/user-attachments/assets/609da445-5372-4c09-99d6-2374923bcdbe)\n\nA thing to note:  I wrote a very basic command line app using SK and MCP and have it calling the same Azure Function MCP Server endpoint.  With that app, I can ask one question after another about the weather in different cities and it responds quickly with no errors.  It seems the issue is only happening in my Blazor app.  The demo command line app can be found here:  https://github.com/mhackermsft/SKMCPDemo/blob/master/MCPClient/Program.cs\n\n\n"
      },
      {
        "user": "SergeyMenshykh",
        "body": "Hi @mhackermsft, thank you for providing the source code and detailed steps to reproduce the issue.\n\nI've tried to reproduce the issue but was unable to. Here’s my setup:\n\n**MCPServerFunction**\n- Deployed to Azure or hosted locally (via F5 in Visual Studio) as an isolated worker process.\n\n**BlazorAIChat**\n- Configured to use MCP SSE.\n- The code for the AIService and ChatHistoryService classes was simplified and stripped down to speed up the setup:\n\n    ```csharp\n      public class AIService\n      {\n          private readonly AppSettings settings;\n          private readonly IChatCompletionService? chatCompletionService;\n          public ChatHistory history { get; private set; } = new ChatHistory();\n          private readonly Kernel kernel;\n  \n          public AIService(IOptions<AppSettings> appSettings, ChatHistoryService chatHistoryService, IHttpClientFactory httpClientFactory, AIChatDBContext dbContext, ILogger<AIService> logger, Kernel kernel)\n          {\n              this.kernel = kernel;\n              settings = appSettings.Value;\n              chatCompletionService = kernel.GetRequiredService<IChatCompletionService>();\n          }\n  \n          public async Task<IAsyncEnumerable<List<StreamingChatMessageContent>>> GetChatResponseAsync(string prompt, Message message, Session currentSession, User currentUser)\n          {\n              IAsyncEnumerable<StreamingChatMessageContent> streamingMessages;\n  \n              // Get the chat response as a stream of messages\n              AzureOpenAIPromptExecutionSettings executionSettings = new()\n              {\n                  Temperature = 0,\n                  FunctionChoiceBehavior = FunctionChoiceBehavior.Auto(options: new() { RetainArgumentTypes = true })\n              };\n  \n              history.AddUserMessage(prompt);\n  \n              streamingMessages = chatCompletionService.GetStreamingChatMessageContentsAsync(history, executionSettings, kernel);\n  \n              // Buffer and yield messages in chunks\n              return BufferMessagesInChunks(streamingMessages, settings.AzureOpenAIChatCompletion.ResponseChunkSize);\n          }\n  \n          private async IAsyncEnumerable<List<StreamingChatMessageContent>> BufferMessagesInChunks(IAsyncEnumerable<StreamingChatMessageContent> streamingMessages, int chunkSize)\n          {\n              List<StreamingChatMessageContent> buffer = new List<StreamingChatMessageContent>(chunkSize);\n  \n              await foreach (var message in streamingMessages)\n              {\n                  buffer.Add(message);\n  \n                  if (buffer.Count >= chunkSize)\n                  {\n                      yield return new List<StreamingChatMessageContent>(buffer);\n                      buffer.Clear();\n                  }\n              }\n  \n              // Yield any remaining messages\n              if (buffer.Count > 0)\n              {\n                  yield return new List<StreamingChatMessageContent>(buffer);\n              }\n          }\n  \n          public async Task<string> SummarizeChatSessionNameAsync(string? sessionId)\n          {\n              return await Task.FromResult(\"fake summary\");\n          }\n  \n          public async Task<bool> ProcessDocsWithKernelMemory(MemoryStream memoryStream, string filename, Session currentSession, User currentUser)\n          {\n              return true;\n          }\n  \n          public async Task<bool> DeleteUploadedDocs(string? sessionIdToDelete)\n          {\n              return true;\n          }\n          \n          public bool AddImageToChat(MemoryStream imageStream, string uploadImageType)\n          {\n              return true;\n          }\n      }\n    ```\n    ```csharp\n    public class ChatHistoryService\n    {\n        public ChatHistoryService(IOptions<AppSettings> settings, ILogger<ChatHistoryService> logger)\n        {\n        }\n\n        public async Task<Session> InsertSessionAsync(Session session)\n        {\n            return await Task.FromResult(session); // Placeholder for actual implementation\n        }\n\n        public async Task<Message> InsertMessageAsync(Message message)\n        {\n            return await Task.FromResult(message); // Placeholder for actual implementation\n        }\n\n        public async Task<List<Session>> GetSessionsAsync(string userId)\n        {\n            return await Task.FromResult(new List<Session>()); // Placeholder for actual implementation\n        }\n\n        public async Task<List<Message>> GetSessionMessagesAsync(string sessionId)\n        {\n            return await Task.FromResult(new List<Message>()); // Placeholder for actual implementation\n        }\n\n        public async Task<Session> UpdateSessionAsync(Session session)\n        {\n            return await Task.FromResult(session); // Placeholder for actual implementation\n        }\n\n        public async Task<Session> GetSessionAsync(string sessionId)\n        {\n            return await Task.FromResult(new Session { SessionId = sessionId }); // Placeholder for actual implementation\n        }\n\n        public async Task UpsertSessionBatchAsync(params dynamic[] messages)\n        {\n        }\n\n        public async Task DeleteSessionAndMessagesAsync(string sessionId)\n        {\n        }\n    }\n    ```\n  \n   <img width=\"937\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/da237246-71f3-4572-bad1-c7a852f0426e\" />\n\nMy recommendation for the next steps is to remove all code from the AIService and ChatHistoryService classes that seems irrelevant to the issue, and then check if the problem persists. If the issue is resolved, it indicates that the removed code is relevant. In this case, I suggest adding the code back feature by feature to identify what is causing the issue."
      }
    ]
  },
  {
    "issue_number": 12356,
    "title": "Python: .Net: New Feature: Connected agents using AzureAIAgent sdk",
    "author": "saralangaz",
    "state": "open",
    "created_at": "2025-06-03T14:13:30Z",
    "updated_at": "2025-06-04T05:22:12Z",
    "labels": [
      ".NET",
      "python",
      "agents"
    ],
    "body": "---\nname: Connected agents using AzureAIAgent sdk\nabout: \n\nHi team,\n\nI’m currently building a multi-agent application using the Azure AI Agents SDK via Semantic Kernel. One limitation I’ve encountered is the lack of a native way to link agents together — for example, having an “orchestrator” agent that can call or delegate tasks to “collaborator” agents.\n\nIt would be extremely useful to have first-class support in the SDK to define relationships or invocations between agents (e.g., agent A can invoke agent B as a tool or sub-agent). Right now, this requires workarounds or custom function tools.\n\nWould you consider adding support for inter-agent orchestration in a future release?\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Hi @saralangaz, thanks for filing the request. Yes, we'll want to provide the connected agent functionality for the SK AzureAIAgent.\n\nBefore that feature rolls out, there are two things to look at:\n\n1.  When using any SK agent, like the AzureAIAgent, one can add other agents as plugins. You can see I've defined two agents in this sample, and the triage agent can leverage those agents during function calling:\n\n```python\nimport asyncio\n\nfrom azure.identity.aio import DefaultAzureCredential\n\nfrom semantic_kernel.agents import AzureAIAgent, AzureAIAgentSettings, AzureAIAgentThread\n\n\n# Simulate a conversation with the agent\nUSER_INPUTS = [\n    \"How do I say 'tomorrow' in French?\",\n    \"How do I say 'tomorrow' in Spanish?\",\n]\n\n\nasync def main() -> None:\n    async with (\n        DefaultAzureCredential() as creds,\n        AzureAIAgent.create_client(credential=creds) as client,\n    ):\n        french_agent_definition = await client.agents.create_agent(\n            model=AzureAIAgentSettings().model_deployment_name,\n            name=\"FrenchAgent\",\n            instructions=\"Answer the user's questions and respond only in French.\",\n        )\n\n        french_agent = AzureAIAgent(\n            client=client,\n            definition=french_agent_definition,\n        )\n\n        spanish_agent_definition = await client.agents.create_agent(\n            model=AzureAIAgentSettings().model_deployment_name,\n            name=\"SpanishAgent\",\n            instructions=\"Answer the user's questions and respond only in Spanish.\",\n        )\n\n        spanish_agent = AzureAIAgent(\n            client=client,\n            definition=spanish_agent_definition,\n        )\n\n        triage_agent_definition = await client.agents.create_agent(\n            model=AzureAIAgentSettings().model_deployment_name,\n            name=\"TriageAgent\",\n            instructions=\"Direct the user's request to the appropriate agent and/or tool.\",\n        )\n\n        triage_agent = AzureAIAgent(\n            client=client,\n            definition=triage_agent_definition,\n            plugins=[french_agent, spanish_agent],\n        )\n\n        thread: AzureAIAgentThread = None\n\n        try:\n            for user_input in USER_INPUTS:\n                print(f\"# User: {user_input}\")\n                # 4. Invoke the agent with the specified message for response\n                response = await triage_agent.get_response(messages=user_input, thread=thread)\n                print(f\"# {response.name}: {response}\")\n                thread = response.thread\n        finally:\n            # 6. Cleanup: Delete the thread and agent\n            await thread.delete() if thread else None\n            await client.agents.delete_agent(french_agent.id)\n            await client.agents.delete_agent(spanish_agent.id)\n            await client.agents.delete_agent(triage_agent.id)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nThe output looks like:\n\n```\n# User: How do I say 'tomorrow' in French?\n# TriageAgent: En français, on dit « demain » pour dire « tomorrow ».\n# User: How do I say 'tomorrow' in Spanish?\n# TriageAgent: La palabra para \"tomorrow\" en español es \"mañana\".\n```\n\n2. Alternatively, have you looked at our new agent orchestration patterns, like the [group chat with manager pattern](https://github.com/microsoft/semantic-kernel/blob/main/python/samples/getting_started_with_agents/multi_agent_orchestration/step3b_group_chat_with_chat_completion_manager.py)?\nI know this may not exactly replicate the AzureAIAgent \"connected\" agent usage, but it is similar in the sense that it delegates the agent selection to a manager agent."
      }
    ]
  },
  {
    "issue_number": 10770,
    "title": "Python: [Python] factory_function limitations",
    "author": "sophialagerkranspandey",
    "state": "open",
    "created_at": "2025-03-03T17:12:17Z",
    "updated_at": "2025-06-04T02:14:29Z",
    "labels": [
      "bug",
      "python",
      "stale",
      "processes"
    ],
    "body": "Full issue here: https://teams.microsoft.com/l/message/19:W5nYJLrRvQatcJ04yDmCkzm-t7B6c7VTxS57yNtvSzU1@thread.tacv2/1740997230547?tenantId=72f988bf-86f1-41af-91ab-2d7cd011db47&groupId=a87ae912-78ce-4eb9-862a-e7edf1b2aaa6&parentMessageId=1740997230547&teamName=Semantic%20Kernel%20-%20MSFT%20Support&channelName=General&createdTime=1740997230547\n\n[Python] factory_function limitations\nFound couple of limitations in PF when using factory_function to create Step.\n\n1. [semantic-kernel/python/semantic_kernel/processes/process_step_builder.py at main · microsoft/semantic-kernel](https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/processes/process_step_builder.py#L214) - If the Step class constructor has required parameter then the add_step fails because the code above tries to instantiate the Step without using the factory. All parameters in constructor has to be optional for SK to work. Can this constraint be removed?\n\n2. [semantic-kernel/python/semantic_kernel/processes/process_builder.py at main · microsoft/semantic-kernel](https://github.com/microsoft/semantic-kernel/blob/main/python/semantic_kernel/processes/process_builder.py#L67) - process builder maintains the factory map using FQN of Step class. This prevents reusing the class and registering it as different steps having different dependencies. Example - we have AgentStep class with following constructor __init__(self, agent: Agent). I want to register two different steps using this class as:\nprocess.add_step(step_type=AgentStep, name=\"ReviewerAgent\", factory_fuction=lambda: AgentStep(agent=ReviewerAgent))\nprocess.add_step(step_type=AgentStep, name=\"WriterAgent\", factory_fuction=lambda: AgentStep(agent=WriterAgent))\nI am getting some weird issues with this as the second add_step overwrites the factory_function from first add_step.\nmicrosoft/semantic-kernel\n\n\n\n\n\n\n\n\n\n**Describe the bug**\nA clear and concise description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to '...'\n2. Click on '....'\n3. Scroll down to '....'\n4. See error\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Platform**\n - Language: [e.g. C#, Python]\n - Source: [e.g. NuGet package version 0.1.0, pip package version 0.1.0, main branch of repository]\n - AI model: [e.g. OpenAI:GPT-4o-mini(2024-07-18)]\n - IDE: [e.g. Visual Studio, VS Code]\n - OS: [e.g. Windows, Mac]\n\n**Additional context**\nAdd any other context about the problem here.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10820,
    "title": ".Net: Add HTTP header for usage of Process Framework",
    "author": "dmytrostruk",
    "state": "open",
    "created_at": "2025-03-05T22:25:29Z",
    "updated_at": "2025-06-04T02:14:27Z",
    "labels": [
      ".NET",
      "sk team issue",
      "stale"
    ],
    "body": "Based on: https://github.com/microsoft/semantic-kernel/issues/10447\nThis should be possible to implement when OpenAI .NET SDK will support updating headers per request for both streaming and non-streaming scenarios.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 12337,
    "title": "Python: Update `AzureAIAgent` learn docs to show the dependencies are installed by default",
    "author": "moonbox3",
    "state": "closed",
    "created_at": "2025-06-02T07:34:04Z",
    "updated_at": "2025-06-04T00:12:25Z",
    "labels": [
      "python",
      "agents",
      "documentation"
    ],
    "body": "Right now, our docs talk about needing to install the `azure` dependencies for the `AzureAIAgent` by installing the `azure` extra package:\n\n```\npip install semantic-kernel[azure]\n```\n\nIn #12335, the Azure AI Agent packages were moved to be installed with the default `semantic-kernel` packages. Remove any callout in the docs that this is required. Once can now do:\n\n```\npip install semantic-kernel\n```\n\nAnd they'll be able to interact with the `AzureAIAgent`.",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Complete and live."
      }
    ]
  },
  {
    "issue_number": 12262,
    "title": "New Feature: OTel GenAI Tool Execution span",
    "author": "TaoChenOSU",
    "state": "closed",
    "created_at": "2025-05-23T21:11:58Z",
    "updated_at": "2025-06-03T22:32:45Z",
    "labels": [
      "telemetry"
    ],
    "body": "---\nname: OTel GenAI Tool Execution span\nabout: OTel GenAI telemetry\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\nThe OTel GenAI semantic conventions have introduces a new span kind for tracing tool executions: https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/#execute-tool-span\n\nSK already creates spans that trace function executions but the feature was added prior to the availability of the OTel GenAI semantic conventions. \n\nWe should change the name of the current function span to the name format specified by OTel and add the required attributes to the span.",
    "comments": []
  },
  {
    "issue_number": 12357,
    "title": "Python: Bug: OrderStatus agent always hits Semantic Kernel Filter policy",
    "author": "Sandido",
    "state": "closed",
    "created_at": "2025-06-03T15:52:18Z",
    "updated_at": "2025-06-03T21:32:54Z",
    "labels": [
      "bug",
      "python",
      "triage"
    ],
    "body": "**Describe the bug**\nFollowing the example here, https://github.com/microsoft/semantic-kernel/blob/main/python/samples/getting_started_with_agents/multi_agent_orchestration/step4_handoff.py\nThe conversation is: \n\"I have a question about my customer order\"\n\"Hi There! I'm here to assist you with your order. Could you please tell me more about your question? Are you looking for information about the order status, returns, or refunds?\"\n\"status\"\nThe Semantic Kernel ResponsibleAI policy is tripped and throws an error.\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy.\n\nMy AOAI instance (gpt-4o, 2024-11-20) has no custom filters on it, so I wonder why the above code causes a content violation. You might want to revisit this. \n\n**To Reproduce**\nSteps to reproduce the behavior:\nFollowing the example here, https://github.com/microsoft/semantic-kernel/blob/main/python/samples/getting_started_with_agents/multi_agent_orchestration/step4_handoff.py\nThe conversation is: \n\"I have a question about my customer order\"\n\"Hi There! I'm here to assist you with your order. Could you please tell me more about your question? Are you looking for information about the order status, returns, or refunds?\"\n\"status\"\nThe Semantic Kernel ResponsibleAI policy is tripped and throws an error.\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy.\n\n**Expected behavior**\nA content violation should not occur. \n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Platform**\n - Language: Python\n - Source: Semantic Kernel \n - AI model: (gpt-4o, 2024-11-20)\n - IDE:  VS Code\n - OS: Windows\n - \n**Additional context**\nAdd any other context about the problem here.",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Thanks for filing, @Sandido. We're tracking this in #12294. Closing as this is a duplicate."
      }
    ]
  },
  {
    "issue_number": 11812,
    "title": ".Net: Bug: Unsupported content type. FunctionCallContent is not supported by Gemini.",
    "author": "donatas-xyz",
    "state": "closed",
    "created_at": "2025-04-30T06:54:50Z",
    "updated_at": "2025-06-03T19:07:36Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\n`Unsupported content type. FunctionCallContent is not supported by Gemini.` error when calling plugin functions after upgrading from `Microsoft.SemanticKernel.Connectors.Google` 1.47.0-alpha to 1.48.0-alpha.\n\n**To Reproduce**\n```csharp\n[KernelFunction]\n[Description(\"Returns the current date.\")]\npublic async Task<string> CurrentDate()\n{\n    return await Task.FromResult(DateTime.Now.ToLongDateString());\n}\n```\n```text\nGemini, what's today's date?\n```\n\n**Expected behavior**\nAfter reverting back to 1.47.0-alpha\n```text\nToday is 30 April 2025.\n```\n\n**Platform**\n - Language: C#, .NET 9\n - Source:`Microsoft.SemanticKernel.Connectors.Google 1.48.0-alpha`\n - AI model: gemini-2.0-flash\n - IDE: Visual Studio\n - OS: Windows 11\n - Semantic Kernel: 1.48.0\n\n**Additional context**\nJust quickly reporting. I haven't checked for breaking changes or done any other investigation other than a quick Google search. ",
    "comments": [
      {
        "user": "RogerBarreto",
        "body": "I'm unable to reproduce the bug reported.\n\nI created the below reproduction code to try simulate your problem but didn't have any issue and it worked as expected.\n\nPlease provide more detais on your reproduction code so I can invetigate further, for now I consider this as resolved.\n\n```csharp\n [RetryFact]\n public async Task GoogleAIChatCompletionWithStatelessPluginFunctionCalling()\n {\n     Console.WriteLine(\"============= Google AI - Gemini Chat Completion with function calling =============\");\n\n     Assert.NotNull(TestConfiguration.GoogleAI.ApiKey);\n     Assert.NotNull(TestConfiguration.GoogleAI.Gemini.ModelId);\n\n     Kernel kernel = Kernel.CreateBuilder()\n         .AddGoogleAIGeminiChatCompletion(\n             modelId: TestConfiguration.GoogleAI.Gemini.ModelId,\n             apiKey: TestConfiguration.GoogleAI.ApiKey)\n         .Build();\n\n     kernel.Plugins.AddFromType<TimePlugin>();\n\n     GeminiPromptExecutionSettings settings = new() { ToolCallBehavior = GeminiToolCallBehavior.AutoInvokeKernelFunctions };\n     Console.WriteLine(await kernel.InvokePromptAsync(\n         \"Check current date, please\", new(settings)));\n     Console.WriteLine();\n }\n\n private sealed class TimePlugin\n {\n     [KernelFunction]\n     [Description(\"Returns the current date.\")]\n     public async Task<string> CurrentDate()\n     {\n         return await Task.FromResult(DateTime.Now.ToLongDateString());\n     }\n }\n```"
      },
      {
        "user": "donatas-xyz",
        "body": "For what it's worth, @RogerBarreto, it seems to be working with version `1.54.0-alpha`. I've turned chat services into agents since then, so that may have helped as well. "
      }
    ]
  },
  {
    "issue_number": 12340,
    "title": ".Net: Update dependency on Fluid.Core in PromptTemplates.Liquid to the latest",
    "author": "vlad-kuznetsov-corvel",
    "state": "closed",
    "created_at": "2025-06-02T19:37:44Z",
    "updated_at": "2025-06-03T18:35:30Z",
    "labels": [
      ".NET"
    ],
    "body": "<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\nWhen using **PromptTemplates.Liquid** alongside other libraries that depend on Fluid.Core v2.x, we run into runtime conflicts such as MissingMethodException due to API differences between Fluid.Core 2.11.1 and >2.11.1\n\n\n```\nSystem.MissingMethodException\n  HResult=0x80131513\n  Message=Method not found: 'Void Fluid.TemplateContext..ctor(Fluid.TemplateOptions)'.\n  Source=Microsoft.SemanticKernel.PromptTemplates.Liquid\n  StackTrace:\n   at Microsoft.SemanticKernel.PromptTemplates.Liquid.LiquidPromptTemplate.GetTemplateContext(KernelArguments arguments)\n   at Microsoft.SemanticKernel.PromptTemplates.Liquid.LiquidPromptTemplate.<RenderAsync>d__11.MoveNext()\n   at Microsoft.SemanticKernel.KernelFunctionFromPrompt.<>c__DisplayClass22_0.<<RenderPromptAsync>b__1>d.MoveNext()\n   at Microsoft.SemanticKernel.Kernel.<InvokeFilterOrPromptRenderAsync>d__36.MoveNext()\n```\n\nThis makes it difficult to integrate the extension into projects where Fluid.Core >2.11.1 is already in use.\n\nIt would be great if the extension could upgrade its dependency on Fluid.Core to 2.24 assuming compatibility. ",
    "comments": [
      {
        "user": "shethaadit",
        "body": "Hi @markwallace-microsoft / @RogerBarreto, I have raised PR to fix this."
      }
    ]
  },
  {
    "issue_number": 11820,
    "title": ".Net: Bug: Microsoft.SemanticKernel.Connectors.OpenAI.OpenAIChatMessageContent - A class should either have a default constructor, one constructor with arguments or a constructor marked with the JsonConstructor attribute.",
    "author": "MavhengeleMpho",
    "state": "closed",
    "created_at": "2025-04-30T13:56:37Z",
    "updated_at": "2025-06-03T17:14:09Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "We are using Microsoft.SemanticKernel.Connectors.OpenAI.OpenAIChatMessageContent for our chatHistory, it works fine when saving history as strings, the challenge comes when we try to save messages from AuthorRole tool and when there is toolCalls involved, this happens when we use the plugin to get data from the apis/ database, we are able to save messages from other AuthorRoles its the Tool AuthorRole that introduces issues, and the tool authorRoles are added outside of our codebase, initially we where removing the messages in the chathistory with tool authorRole but now we must put them back since we starting to work towards building agents calling other agents to avoid loosing context we put the tool messages back in but we get the below error: \n\nSystem.AggregateException\n  HResult=0x80131500\n  Message=One or more errors occurred. (Something went wrong when loading state: Unable to find a constructor to use for type Microsoft.SemanticKernel.Connectors.OpenAI.OpenAIChatMessageContent. A class should either have a default constructor, one constructor with arguments or a constructor marked with the JsonConstructor attribute. Path '**************************.$values[1].ToolCalls'.)\n  Source=System.Private.CoreLib\n  StackTrace:\n   at System.Threading.Tasks.Task.ThrowIfExceptional(Boolean includeTaskCanceledExceptions)\n   at System.Threading.Tasks.Task`1.GetResultCore(Boolean waitCompletionNotification)\n   at Microsoft.Teams.AI.State.TurnState.<LoadStateAsync>d__26.MoveNext()\n   at Microsoft.Teams.AI.Application`1.<_OnTurnAsync>d__66.MoveNext()\n   at Microsoft.Teams.AI.Application`1.<OnTurnAsync>d__63.MoveNext()\n   at Microsoft.Bot.Builder.MiddlewareSet.<ReceiveActivityWithStatusAsync>d__3.MoveNext()\n   at Microsoft.Bot.Builder.BotAdapter.<RunPipelineAsync>d__23.MoveNext()\n   at Microsoft.Bot.Builder.BotAdapter.<RunPipelineAsync>d__23.MoveNext()\n   at Microsoft.Bot.Builder.CloudAdapterBase.<ProcessActivityAsync>d__22.MoveNext()\n   at Microsoft.Bot.Builder.CloudAdapterBase.<ProcessActivityAsync>d__21.MoveNext()\n   at Microsoft.Bot.Builder.Integration.AspNet.Core.CloudAdapter.<ProcessAsync>d__4.MoveNext()\n  .Controllers.BotController.<PostAsync>d__5.MoveNext() in D:\\SourceControl\\2025\\\\Controllers\\BotController.cs:line 41\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ActionMethodExecutor.TaskResultExecutor.<Execute>d__0.MoveNext()\n   at System.Runtime.CompilerServices.ValueTaskAwaiter`1.GetResult()\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.<<InvokeActionMethodAsync>g__Logged|12_1>d.MoveNext()\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.<<InvokeNextActionFilterAsync>g__Awaited|10_0>d.MoveNext()\n\n  This exception was originally thrown at this call stack:\n    [External Code]\n\nInner Exception 1:\nTeamsAIException: Something went wrong when loading state: Unable to find a constructor to use for type Microsoft.SemanticKernel.Connectors.OpenAI.OpenAIChatMessageContent. A class should either have a default constructor, one constructor with arguments or a constructor marked with the JsonConstructor attribute. Path 'adfd964c-955f-48dd-afcc-dddf578af169.$values[1].ToolCalls'.\n\nInner Exception 2:\nJsonSerializationException: Unable to find a constructor to use for type Microsoft.SemanticKernel.Connectors.OpenAI.OpenAIChatMessageContent. A class should either have a default constructor, one constructor with arguments or a constructor marked with the JsonConstructor attribute. Path 'adfd964c-955f-48dd-afcc-dddf578af169.$values[1].ToolCalls'.\n\n\n**To Reproduce**\nJust have toolCalls from Kernel Functions in your chatHistory\n\n**Expected behavior**\nIt must serialize and deserialize  the chatHistory, be able to keep it in AppState and not break\n\n![Image](https://github.com/user-attachments/assets/86a7b086-fdb9-405c-aa63-41196031badf)\n\n**Screenshots**\nScreenshot is attached.\n\n**Platform**\n - Language:  C#, API\n - Source: Microsoft.SemanticKernel.Abstractions\" Version=\"1.47.0\" and \n                Microsoft.SemanticKernel\" Version=\"1.47.0\"\n - AI model: OpenAI:GPT-4o\n - IDE:  Visual Studio\n - OS: . Windows\n\n**Additional context**\nAll we are trying to do is, save the messages objects and metadata from the AuthorRole tool to the chatHistory same as we also save user and assistant messages to the chatHistory and be able to load them back From AppState ->Conversation State->chatHistory and use them again when sending the messages back to the GPT fro context.",
    "comments": []
  },
  {
    "issue_number": 11101,
    "title": ".Net: Bug: .NET: Manual function calling when using Mistral-Small-24B-Instruct-2501",
    "author": "stiankraggerud",
    "state": "closed",
    "created_at": "2025-03-21T08:01:11Z",
    "updated_at": "2025-06-03T16:45:27Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "I am currently running the Mistral-Small-24B-Instruct-2501 model on an internal server using vLLM, and it operates as expected. To integrate function calling capabilities, I've developed a test application in C#. Understanding that Mistral does not support automatic function calling, I've implemented a straightforward method to extract and execute the necessary function calls.\n\nHowever, I've encountered an issue with the chat history structure generated by Semantic Kernel (SK). Specifically, the JSON data lacks the \"type\": \"function\" field that the Mistral model requires, leading to errors during execution. The expected structure, as outlined in the Mistral model documentation, includes this \"type\": \"function\" field.\n\nIn my attempts to resolve this, I've utilized the FunctionCallContent class in SK. Unfortunately, it doesn't seem to allow the addition of custom properties, such as \"type\": \"function\".\n\nHere is a snippet of the code I've been working with:\n```\nvar functionCallContent = new FunctionCallContent(\n    functionName: functionName,\n    pluginName: pluginName,\n    id: callId,\n    arguments: new KernelArguments(toolCall.Arguments.ToDictionary(kvp => kvp.Key, kvp => (object)kvp.Value))\n);\n\n// Attempt to add custom metadata\nfunctionCallContent.Metadata = new ReadOnlyDictionary<string, object?>(new Dictionary<string, object?>\n{\n    { \"type\", \"function\" }\n});\n```\nDespite these efforts, the \"type\": \"function\" property is not included in the serialized JSON sent to the Mistral model.\n\nHas anyone faced a similar issue or can provide guidance on how to include the \"type\": \"function\" field in the JSON structure generated by Semantic Kernel? Any insights or suggestions would be greatly appreciated.\n\nThis is the json data sent to the llm server:\n\n```\n{\n\t\"model\": \"mistralai/Mistral-Small-24B-Instruct-2501\",\n\t\"messages\": [\n\t\t{\n\t\t\t\"role\": \"system\",\n\t\t\t\"content\": \"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\r\\n\\t\\tYour knowledge base was last updated on 2023-10-01. The current date is 2025-01-30.\\r\\n\\t\\tWhen you\\u0027re not sure about some information, you say that you don\\u0027t have the information and don\\u0027t make up anything.\\r\\n\\t\\tIf the user\\u0027s question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request \\r\\n\\t\\t(e.g. \\u0022What are some good restaurants around me?\\u0022 =\\u003E \\u0022Where are you?\\u0022 or \\u0022When is the next flight to Tokyo\\u0022 =\\u003E \\u0022Where do you travel from?\\u0022). Answer in maximum 50 words. If you are asked a question, use functions as primary source\"\n\t\t},\n\t\t{\n\t\t\t\"role\": \"user\",\n\t\t\t\"content\": \"V\\u00E6ret oslo norge?\"\n\t\t},\n\t\t{\n\t\t\t\"role\": \"assistant\",\n\t\t\t\"content\": \"\",\n\t\t\t\"tool_calls\": [\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"chatcmpl-b814d48e7419490e980e2a4290fe8c3f\",\n\t\t\t\t\t\"function\": {\n\t\t\t\t\t\t\"name\": \"WeatherPlugin1-GetWeather\",\n\t\t\t\t\t\t\"arguments\": \"{\\u0022location\\u0022:\\u0022Oslo, 03\\u0022}\"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"role\": \"tool\",\n\t\t\t\"content\": \"17\\u00B0C\\nWind: 23 KMPH\\nHumidity: 59%\\nMostly cloudy\",\n\t\t\t\"name\": \"WeatherPlugin1-GetWeather\",\n\t\t\t\"tool_call_id\": \"chatcmpl-b814d48e7419490e980e2a4290fe8c3f\"\n\t\t}\n\t],\n\t\"temperature\": 0.7,\n\t\"top_p\": 1,\n\t\"stream\": false,\n\t\"safe_prompt\": false,\n\t\"tools\": [\n\t\t{\n\t\t\t\"type\": \"function\",\n\t\t\t\"function\": {\n\t\t\t\t\"name\": \"OrderPizza-get_pizza_menu\",\n\t\t\t\t\"description\": \"\",\n\t\t\t\t\"parameters\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {},\n\t\t\t\t\t\"required\": []\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t{\n\t\t\t\"type\": \"function\",\n\t\t\t\"function\": {\n\t\t\t\t\"name\": \"WeatherPlugin1-GetWeather\",\n\t\t\t\t\"description\": \"Get the current weather in a given location.\",\n\t\t\t\t\"parameters\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"location\": {\n\t\t\t\t\t\t\t\"description\": \"The city and department, e.g. Marseille, 13\",\n\t\t\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t\"required\": [ \"location\" ]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t],\n\t\"tool_choice\": \"auto\"\n}\n```\n\nThis gives the following error from mistral:\nField required [type=missing, input_value={'id': 'chatcmpl-68b1c676... '{\"location\":\"Oslo\"}'}}, input_type=dict]\n\nThis is how the json should be:\n```\n{\n\t\"model\": \"mistralai/Mistral-Small-24B-Instruct-2501\",\n\t\"messages\": [\n\t\t{\n\t\t\t\"role\": \"system\",\n\t\t\t\"content\": \"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\r\\n\\t\\tYour knowledge base was last updated on 2023-10-01. The current date is 2025-01-30.\\r\\n\\t\\tWhen you\\u0027re not sure about some information, you say that you don\\u0027t have the information and don\\u0027t make up anything.\\r\\n\\t\\tIf the user\\u0027s question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request \\r\\n\\t\\t(e.g. \\u0022What are some good restaurants around me?\\u0022 =\\u003E \\u0022Where are you?\\u0022 or \\u0022When is the next flight to Tokyo\\u0022 =\\u003E \\u0022Where do you travel from?\\u0022). Answer in maximum 50 words. If you are asked a question, use functions as primary source\"\n\t\t},\n\t\t{\n\t\t\t\"role\": \"user\",\n\t\t\t\"content\": \"V\\u00E6ret oslo norge?\"\n\t\t},\n\t\t{\n\t\t\t\"role\": \"assistant\",\n\t\t\t\"content\": \"\",\n\t\t\t\"type\":  \"function\", <--- Should be added\n\t\t\t\"tool_calls\": [\n\t\t\t\t{\n\t\t\t\t\t\"id\": \"chatcmpl-b814d48e7419490e980e2a4290fe8c3f\",\n\t\t\t\t\t\"function\": {\n\t\t\t\t\t\t\"name\": \"WeatherPlugin1-GetWeather\",\n\t\t\t\t\t\t\"arguments\": \"{\\u0022location\\u0022:\\u0022Oslo, 03\\u0022}\"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"role\": \"tool\",\n\t\t\t\"content\": \"17\\u00B0C\\nWind: 23 KMPH\\nHumidity: 59%\\nMostly cloudy\",\n\t\t\t\"name\": \"WeatherPlugin1-GetWeather\",\n\t\t\t\"tool_call_id\": \"chatcmpl-b814d48e7419490e980e2a4290fe8c3f\"\n\t\t}\n\t],\n\t\"temperature\": 0.7,\n\t\"top_p\": 1,\n\t\"stream\": false,\n\t\"safe_prompt\": false,\n\t\"tools\": [\n\t\t{\n\t\t\t\"type\": \"function\",\n\t\t\t\"function\": {\n\t\t\t\t\"name\": \"OrderPizza-get_pizza_menu\",\n\t\t\t\t\"description\": \"\",\n\t\t\t\t\"parameters\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {},\n\t\t\t\t\t\"required\": []\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t{\n\t\t\t\"type\": \"function\",\n\t\t\t\"function\": {\n\t\t\t\t\"name\": \"WeatherPlugin1-GetWeather\",\n\t\t\t\t\"description\": \"Get the current weather in a given location.\",\n\t\t\t\t\"parameters\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"location\": {\n\t\t\t\t\t\t\t\"description\": \"The city and department, e.g. Marseille, 13\",\n\t\t\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t\"required\": [ \"location\" ]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t],\n\t\"tool_choice\": \"auto\"\n}\n```\n\n** Nuget versions **\n```\n<PackageReference Include=\"Microsoft.SemanticKernel\" Version=\"1.41.0\" />\n<PackageReference Include=\"Microsoft.SemanticKernel.Connectors.MistralAI\" Version=\"1.41.0\" />\n```\n\n\n** Complete example **\n```\n#pragma warning disable SKEXP0070\n#pragma warning disable SKEXP0010\n#pragma warning disable SKEXP0060\n\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.ChatCompletion;\nusing System.Collections.ObjectModel;\nusing SemanticKernelStripped;\nusing Microsoft.SemanticKernel.Connectors.MistralAI;\nusing System.ComponentModel;\nusing System.Text.Json;\nusing System.Text.Json.Serialization;\nusing System.Text.RegularExpressions;\n\n\nclass Program\n{\n\n\n\tstatic async Task Main()\n\t{\n\t\tvar kernelBuilder = Kernel.CreateBuilder();\n\n\t\tstring modelId = \"mistralai/Mistral-Small-24B-Instruct-2501\";\n\t\tUri endpointUri = new Uri(\"http://xx.com:4040/v1\");\n\n\n\t\tkernelBuilder.AddMistralChatCompletion(modelId, \"fake\", endpointUri);\n\t\tvar settings = new MistralAIPromptExecutionSettings { ToolCallBehavior = MistralAIToolCallBehavior.EnableKernelFunctions };\n\t\t\n\n\t\tkernelBuilder.Plugins.AddFromType<OrderPizzaPlugin>(\"OrderPizza\");\n\t\tWeatherPlugin1 weatherPlugin = new WeatherPlugin1();\n\t\tkernelBuilder.Plugins.AddFromObject(weatherPlugin);\n\n\n\t\tvar kernel = kernelBuilder.Build();\n\t\tvar chatCompletionService = kernel.GetRequiredService<IChatCompletionService>();\n\n\n\t\tstring systemPrompt = @\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\n\t\tYour knowledge base was last updated on 2023-10-01. The current date is 2025-01-30.\n\t\tWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\n\t\tIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request \n\t\t(e.g. \"\"What are some good restaurants around me?\"\" => \"\"Where are you?\"\" or \"\"When is the next flight to Tokyo\"\" => \"\"Where do you travel from?\"\"). Answer in maximum 50 words. If you are asked a question, use functions as primary source\";\n\n\n\n\n\t\tvar chatHistory = new ChatHistory();\n\t\tchatHistory.AddSystemMessage(systemPrompt);\n\n\n\n\t\twhile (true)\n\t\t{\n\t\t\tConsole.WriteLine(\"Your question:\");\n\t\t\tvar question = Console.ReadLine();\n\t\t\tchatHistory.AddUserMessage(question);\n\n\t\n\t\t\tvar chatMessageContentsAsync = await chatCompletionService.GetChatMessageContentsAsync(chatHistory, settings, kernel);\n\t\t\tvar result = chatMessageContentsAsync.First();\n\n\n\t\t\tif (result.Content.Contains(\"[TOOL_CALLS]\"))\n\t\t\t{\n\t\t\t\tvar resultContent = result.Content ?? \"\";\n\t\t\t\tvar callId = result.Metadata.FirstOrDefault(x => x.Key == \"Id\").Value.ToString();\n\n\t\t\t\tvar match = Regex.Match(resultContent, @\"\\[TOOL_CALLS\\](.*)\");\n\n\t\t\t\tstring json = match.Success ? match.Groups[1].Value.Trim() : \"[]\"; // JSON-delen\n\t\t\t\tstring assistantToolCallMessageContent = Regex.Replace(resultContent, @\"\\[TOOL_CALLS\\].*\", \"\").Trim(); // Fjerner TOOL_CALLS fra teksten\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\tvar toolCalls = JsonSerializer.Deserialize<List<ToolCall>>(json);\n\n\t\t\t\t\tforeach (var toolCall in toolCalls)\n\t\t\t\t\t{\n\t\t\t\t\t\tvar splitName = toolCall.Name.Split('-');\n\t\t\t\t\t\tif (splitName.Length != 2) continue;\n\n\t\t\t\t\t\tstring pluginName = splitName[0];  // \"WeatherPlugin1\"\n\t\t\t\t\t\tstring functionName = splitName[1]; // \"GetWeather\"\n\n\n\t\t\t\t\t\tvar functionResult = await kernel.InvokeAsync(\n\t\t\t\t\t\t\tpluginName, functionName,\n\t\t\t\t\t\t\tnew KernelArguments { [\"location\"] = toolCall.Arguments[\"location\"] });\n\n\n\t\t\t\t\t\tvar functionValue = functionResult.GetValue<string>() ?? \"\";\n\n\n\n\t\t\t\t\t\tvar functionCallContent = new FunctionCallContent(\n\t\t\t\t\t\t\tfunctionName: functionName,\n\t\t\t\t\t\t\tpluginName: pluginName,\n\t\t\t\t\t\t\tid: callId,\n\t\t\t\t\t\t\targuments: new KernelArguments\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t[\"location\"] = toolCall.Arguments[\"location\"]\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t);\n\n\t\t\t\t\t\tfunctionCallContent.Metadata = new ReadOnlyDictionary<string, object?>(new Dictionary<string, object?>\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t{ \"type\", \"function\" }\n\t\t\t\t\t\t});\n\n\n\t\t\t\t\t\tchatHistory.Add(new()\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tRole = AuthorRole.Assistant,\n\t\t\t\t\t\t\tItems = [functionCallContent]\n\t\t\t\t\t\t});\n\n\n\n\t\t\t\t\t\t//Må nå bygge opp Tool-message:\n\t\t\t\t\t\tvar toolResponse = new FunctionResultContent(\n\t\t\t\t\t\t\tfunctionName: functionName,  // Navnet på funksjonen\n\t\t\t\t\t\t\tpluginName: pluginName,  // Navnet på pluginen\n\t\t\t\t\t\t\tresult: functionValue,  // Selve returverdien fra funksjonen\n\t\t\t\t\t\t\tcallId: callId\n\t\t\t\t\t\t);\n\n\n\t\t\t\t\t\tchatHistory.Add(new ChatMessageContent\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tRole = AuthorRole.Tool,\n\t\t\t\t\t\t\tItems = [toolResponse],\n\t\t\t\t\t\t});\n\t\t\t\t\t}\n\n\t\t\t\t\t\n\t\t\t\t\tchatMessageContentsAsync = await chatCompletionService.GetChatMessageContentsAsync(chatHistory, settings, kernel);\n\n\t\t\t\t\tvar content = chatMessageContentsAsync.First().Content;\n\n\t\t\t\t\tchatHistory.Add(new ChatMessageContent\n\t\t\t\t\t{\n\t\t\t\t\t\tRole = AuthorRole.Assistant,\n\t\t\t\t\t\tContent = content\n\t\t\t\t\t});\n\n\n\t\t\t\t\tConsole.WriteLine(content);\n\n\t\t\t\t}\n\t\t\t\tcatch (Exception ex)\n\t\t\t\t{\n\t\t\t\t\tConsole.WriteLine($\"Error parsing TOOL_CALLS: {ex.Message}\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\telse\n\t\t\t{\n\t\t\t\tchatHistory.AddAssistantMessage(result.Content ?? \"\");\n\t\t\t\tConsole.WriteLine(result);\n\t\t\t}\n\t\t}\n\t}\n}\n\npublic class ToolCall\n{\n\t[JsonPropertyName(\"name\")]\n\tpublic string Name { get; set; }\n\n\t[JsonPropertyName(\"arguments\")]\n\tpublic Dictionary<string, string> Arguments { get; set; }\n}\n\npublic sealed class WeatherPlugin1\n{\n\t[KernelFunction]\n\t[Description(\"Get the current weather in a given location.\")]\n\tpublic string GetWeather(\n\t\t[Description(\"The city and department, e.g. Marseille, 13\")] string location\n\t) => \"17°C\\nWind: 23 KMPH\\nHumidity: 59%\\nMostly cloudy\";\n}\n```",
    "comments": [
      {
        "user": "stiankraggerud",
        "body": "After cloning the SK project and digging into the Connectors.MistralAI repository, I found a way to fix this.\n\nTo fix this, I made two changes:\n\n**1: Added the missing type field in MistralToolCall.cs**\nI modified MistralToolCall.cs to explicitly define the type field:\n```\n[JsonPropertyName(\"type\")]\npublic string Type { get; set; } = \"function\";\n```\n\n**2: Updated AddFunctionCallContent in MistralClient.cs**\nI changed the InnerContent assignment in AddFunctionCallContent() to ensure \"type\": \"function\" is always included when serializing function calls:\n```\nprivate void AddFunctionCallContent(ChatMessageContent message, MistralToolCall toolCall)\n{\n    if (toolCall.Function is null)\n    {\n        return;\n    }\n\n    Exception? exception = null;\n    KernelArguments? arguments = null;\n    if (toolCall.Function.Arguments is not null)\n    {\n        try\n        {\n            arguments = JsonSerializer.Deserialize<KernelArguments>(toolCall.Function.Arguments);\n            if (arguments is not null)\n            {\n                var names = arguments.Names.ToArray();\n                foreach (var name in names)\n                {\n                    arguments[name] = arguments[name]?.ToString();\n                }\n            }\n        }\n        catch (JsonException ex)\n        {\n            exception = new KernelException(\"Error: Function call arguments were invalid JSON.\", ex);\n\n            if (this._logger.IsEnabled(LogLevel.Debug))\n            {\n                this._logger.LogDebug(ex, \"Failed to deserialize function arguments ({FunctionName}/{FunctionId}).\", toolCall.Function.Name, toolCall.Id);\n            }\n        }\n    }\n\n    var functionCallContent = new FunctionCallContent(\n        functionName: toolCall.Function.FunctionName,\n        pluginName: toolCall.Function.PluginName,\n        id: toolCall.Id,\n        arguments: arguments)\n    {\n        InnerContent = new\n        {\n            id = toolCall.Id,\n            type = toolCall.Type,  // Ensuring type is included\n            function = toolCall.Function\n        },\n        Exception = exception\n    };\n\n    message.Items.Add(functionCallContent);\n}\n```\n\nWith these changes, function calling now works correctly with Mistral. However, I haven't tested whether this affects other parts of Semantic Kernel.\n\nWould love to see this fixed officially so I don’t have to maintain a custom-built version of SK. Let me know if there's anything I can do to help get this reviewed! "
      },
      {
        "user": "RogerBarreto",
        "body": "@stiankraggerud Thanks for the further investigation and added context. \n\nPlease would you mind creating a PR with this fix? I will be happy to guide you thru the process as well as co-author your changes when needed. \n\nThanks!"
      }
    ]
  },
  {
    "issue_number": 10411,
    "title": "New Feature: AggregatorChannel Add custom mode",
    "author": "mickaelropars",
    "state": "open",
    "created_at": "2025-02-05T10:23:05Z",
    "updated_at": "2025-06-03T15:40:04Z",
    "labels": [
      "enhancement",
      "agents",
      "stale"
    ],
    "body": "---\nname: add a custom mode for Agregrator mode \nabout: when using a task and reviewer workflow, the flat mode allow to get all history in the parent chat , and the nested mode only the latest message. so in the the case \n- flat mode : I need to use the reducer to not take into account the reviewer comment in the parent chat\n- nested mode : I only have the the reviewer comment, and I prefer the task result when the agent is completed \n\nso my request is to have a custom mode , in order to select the message I want to display in the parent chat.\n\n---\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\nI would like to custom mode and and having a callback function in order to select the message visible in the parent chat . or to have the possibility to use chat reducer into the aggregator agent order to manage what agents will me part of the parent chat. but i prefer the custom selection  mode.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 12206,
    "title": ".Net: Add OnIntermediateMessage callback docs in Learn Site",
    "author": "moonbox3",
    "state": "open",
    "created_at": "2025-05-21T04:06:47Z",
    "updated_at": "2025-06-03T15:35:45Z",
    "labels": [
      ".NET",
      "agents",
      "documentation"
    ],
    "body": "Similar to what is included for Python, it would be beneficial to add the `OnIntermediateMessage` callback docs for .NET.\n\nHere's what's available in Python on the Learn Site:\n\n- [ChatCompletionAgent](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/chat-completion-agent?pivots=programming-language-python#handling-intermediate-messages-with-a-chatcompletionagent)\n- [OpenAIAssistantAgent](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/assistant-agent?pivots=programming-language-python#handling-intermediate-messages-with-an-openaiassistantagent)\n- [AzureAIAgent](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/azure-ai-agent?pivots=programming-language-python#handling-intermediate-messages-with-an-azureaiagent)\n- [OpenAIResponsesAgent](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/responses-agent?pivots=programming-language-python#handling-intermediate-messages-with-an-openairesponsesagent)",
    "comments": []
  },
  {
    "issue_number": 12339,
    "title": ".Net: Bug: Null reference exception at Azure.AI.Agents.Persistent.RunStepDetailsUpdate.get_FunctionName()",
    "author": "karthiksimplist",
    "state": "closed",
    "created_at": "2025-06-02T14:20:33Z",
    "updated_at": "2025-06-03T15:30:16Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nGetting this exception when SK tries to make the file search tool call. This is intermittent. \n\n```\nSystem.NullReferenceException\n  HResult=0x80004003\n  Message=Object reference not set to an instance of an object.\n  Source=Azure.AI.Agents.Persistent\n  StackTrace:\n   at Azure.AI.Agents.Persistent.RunStepDetailsUpdate.get_FunctionName()\n   at Microsoft.SemanticKernel.Agents.AzureAI.Internal.AgentThreadActions.<InvokeStreamingAsync>d__6.MoveNext()\n   at Microsoft.SemanticKernel.Agents.AzureAI.Internal.AgentThreadActions.<InvokeStreamingAsync>d__6.MoveNext()\n   at Microsoft.SemanticKernel.Agents.AzureAI.Internal.AgentThreadActions.<InvokeStreamingAsync>d__6.System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult(Int16 token)\n   at Microsoft.SemanticKernel.Diagnostics.ActivityExtensions.<RunWithActivityAsync>d__4`1.MoveNext()\n   at Microsoft.SemanticKernel.Diagnostics.ActivityExtensions.<RunWithActivityAsync>d__4`1.MoveNext()\n   at Microsoft.SemanticKernel.Diagnostics.ActivityExtensions.<RunWithActivityAsync>d__4`1.System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult(Int16 token)\n   at Microsoft.SemanticKernel.Agents.AzureAI.AzureAIAgent.<InvokeStreamingAsync>d__23.MoveNext()\n   at Microsoft.SemanticKernel.Agents.AzureAI.AzureAIAgent.<InvokeStreamingAsync>d__23.MoveNext()\n   at Microsoft.SemanticKernel.Agents.AzureAI.AzureAIAgent.<InvokeStreamingAsync>d__23.System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult(Int16 token)\n```\n\n\n**To Reproduce**\nSteps to reproduce the behavior:\nCreate a Vector Store\nCreate a Azure Agent Thread\nAssign the Vector Store to the Azure Agent Thread\nAdd a file to the Vector Store\nTry to access the file via user query using the following code.\nIf it works then try to delete the file from the vector store and ask to make the file search tool call again \n\n`await foreach (var streamingContent in azureAgent.InvokeStreamingAsync(streamContent, agentThread, cancellationToken: cts.Token))\n`\nwhere \n```\nazureAgent is {Microsoft.SemanticKernel.Agents.AzureAI.AzureAIAgent}\n\nstreamContent is a string\nagentThread is {Microsoft.SemanticKernel.Agents.AzureAI.AzureAIAgentThread}\ncts is {System.Threading.CancellationTokenSource}\n```\n\n**Expected behavior**\nSupposed to make the file search tool call and bring the results.\n\n**Screenshots**\n\n**Platform**\n - Language: C#\n - Source:  \n```\n    <PackageReference Include=\"Azure.AI.Agents.Persistent\" Version=\"1.1.0-beta.1\" />\n    <PackageReference Include=\"Azure.AI.Projects\" Version=\"1.0.0-beta.9\" />\n    <PackageReference Include=\"Microsoft.SemanticKernel\" Version=\"1.54.0\" />\n    <PackageReference Include=\"Microsoft.SemanticKernel.Core\" Version=\"1.54.0\" />\n    <PackageReference Include=\"Microsoft.SemanticKernel.Agents.Core\" Version=\"1.54.0\" />\n    <PackageReference Include=\"Microsoft.SemanticKernel.Agents.AzureAI\" Version=\"1.54.0-preview\" />\n```\n\n - AI model: [OpenAI:GPT-4o]\n - IDE: [e.g. Visual Studio]\n - OS: [e.g. Windows]\n\n**Additional context**\nAdd any other context about the problem here.",
    "comments": [
      {
        "user": "karthiksimplist",
        "body": "The actual calls on the thread works fine as I could see the results on the Azure AI Foundry UI threads. It's just the SK throwing this error when it tries to handle the request/repose.\n\nalso, getting the following error around the same line - not sure when we would get the error I reported on the bug vs this one.\n```\n\nSystem.NullReferenceException: Object reference not set to an instance of an object.\n   at Microsoft.SemanticKernel.Agents.AzureAI.Internal.AgentThreadActions.InvokeStreamingAsync(AzureAIAgent agent, PersistentAgentsClient client, String threadId, IList`1 messages, AzureAIInvocationOptions invocationOptions, ILogger logger, Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Agents.AzureAI.Internal.AgentThreadActions.InvokeStreamingAsync(AzureAIAgent agent, PersistentAgentsClient client, String threadId, IList`1 messages, AzureAIInvocationOptions invocationOptions, ILogger logger, Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Agents.AzureAI.Internal.AgentThreadActions.InvokeStreamingAsync(AzureAIAgent agent, PersistentAgentsClient client, String threadId, IList`1 messages, AzureAIInvocationOptions invocationOptions, ILogger logger, Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult()\n   at Microsoft.SemanticKernel.Diagnostics.ActivityExtensions.RunWithActivityAsync[TResult](Func`1 getActivity, Func`1 operation, CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Diagnostics.ActivityExtensions.RunWithActivityAsync[TResult](Func`1 getActivity, Func`1 operation, CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Diagnostics.ActivityExtensions.RunWithActivityAsync[TResult](Func`1 getActivity, Func`1 operation, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource<System.Boolean>.GetResult()\n   at Microsoft.SemanticKernel.Agents.AzureAI.AzureAIAgent.InvokeStreamingAsync(ICollection`1 messages, AgentThread thread, AzureAIAgentInvokeOptions options, CancellationToken cancellationToken)+MoveNext()\n```"
      },
      {
        "user": "markharrison",
        "body": "I have a similar problem - this code works for an AI Foundry Agent when I dont have Bing defined as Knowledge.  It gives an Exception when is Bing is set up as Knowledge.\n\n![Image](https://github.com/user-attachments/assets/4bb7e95e-a788-45f9-aa48-5d0ee753cf2d)\n\nversions:\n\n    <PackageReference Include=\"Azure.AI.Agents.Persistent\" Version=\"1.1.0-beta.1\" />\n    <PackageReference Include=\"Azure.AI.Projects\" Version=\"1.0.0-beta.9\" />\n    <PackageReference Include=\"Azure.Identity\" Version=\"1.14.0\" />\n    <PackageReference Include=\"DotNetEnv\" Version=\"3.1.1\" />\n    <PackageReference Include=\"Microsoft.Extensions.Hosting\" Version=\"10.0.0-preview.4.25258.110\" />\n    <PackageReference Include=\"Microsoft.SemanticKernel\" Version=\"1.54.0\" />\n    <PackageReference Include=\"Microsoft.SemanticKernel.Agents.AzureAI\" Version=\"1.54.0-preview\" />\n    <PackageReference Include=\"Microsoft.SemanticKernel.Agents.Core\" Version=\"1.54.0\" />\n    <PackageReference Include=\"Microsoft.SemanticKernel.Process.Abstractions\" Version=\"1.54.0-alpha\" />\n    <PackageReference Include=\"Microsoft.SemanticKernel.Process.Core\" Version=\"1.54.0-alpha\" />\n    <PackageReference Include=\"Microsoft.SemanticKernel.Process.LocalRuntime\" Version=\"1.54.0-alpha\" />\n    <PackageReference Include=\"Polly\" Version=\"8.5.2\" />\n\n--\n\nAfter thought - not sure its similar ... ill open a separate issue .  #12351 "
      }
    ]
  },
  {
    "issue_number": 12351,
    "title": ".Net Bug: Null reference exception, AI Foundry Agent Service using Bing knowledge",
    "author": "markharrison",
    "state": "closed",
    "created_at": "2025-06-03T08:17:27Z",
    "updated_at": "2025-06-03T15:30:15Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nI have SK code calling AI Foundry Agent. \nThe code fails with an Exception when I have Bing defined for Knowledge \n\n**To Reproduce**\nThe code works if the Agent has no Knowledge defined\nThe code fails if the Agent has Bing defined for Knowledge \n\n**Expected behavior**\n No Exception failure\n\n**Screenshots**\n\n![Image](https://github.com/user-attachments/assets/0dccf8ff-b1fc-4482-89e5-d3b570b740d1)\n\n![Image](https://github.com/user-attachments/assets/3d047079-7fe2-4714-80fc-5c72adb5f5d4)\n\n**Platform**\nC#\n```\n<PackageReference Include=\"Azure.AI.Agents.Persistent\" Version=\"1.1.0-beta.1\" />\n<PackageReference Include=\"Azure.AI.Projects\" Version=\"1.0.0-beta.9\" />\n<PackageReference Include=\"Azure.Identity\" Version=\"1.14.0\" />\n<PackageReference Include=\"DotNetEnv\" Version=\"3.1.1\" />\n<PackageReference Include=\"Microsoft.Extensions.Hosting\" Version=\"10.0.0-preview.4.25258.110\" />\n<PackageReference Include=\"Microsoft.SemanticKernel\" Version=\"1.54.0\" />\n<PackageReference Include=\"Microsoft.SemanticKernel.Agents.AzureAI\" Version=\"1.54.0-preview\" />\n<PackageReference Include=\"Microsoft.SemanticKernel.Agents.Core\" Version=\"1.54.0\" />\n<PackageReference Include=\"Microsoft.SemanticKernel.Process.Abstractions\" Version=\"1.54.0-alpha\" />\n<PackageReference Include=\"Microsoft.SemanticKernel.Process.Core\" Version=\"1.54.0-alpha\" />\n<PackageReference Include=\"Microsoft.SemanticKernel.Process.LocalRuntime\" Version=\"1.54.0-alpha\" />\n<PackageReference Include=\"Polly\" Version=\"8.5.2\" />\n```\n ",
    "comments": []
  },
  {
    "issue_number": 12288,
    "title": ".Net: Null reference error at Microsoft.SemanticKernel.Agents.OpenAI.Internal.AssistantThreadActions.ParseFunctionCall",
    "author": "karthiksimplist",
    "state": "closed",
    "created_at": "2025-05-28T01:58:33Z",
    "updated_at": "2025-06-03T15:30:15Z",
    "labels": [
      ".NET",
      "agents"
    ],
    "body": "Message: Object reference not set to an instance of an object.\n\nStack: \n   at Microsoft.SemanticKernel.Agents.OpenAI.Internal.AssistantThreadActions.ParseFunctionCall(String functionName, String functionArguments)\n   at Microsoft.SemanticKernel.Agents.OpenAI.Internal.AssistantThreadActions.ParseFunctionStep(OpenAIAssistantAgent agent, RunStep step)+MoveNext()\n   at System.Collections.Generic.LargeArrayBuilder`1.AddRange(IEnumerable`1 items)\n   at System.Collections.Generic.SparseArrayBuilder`1.ReserveOrAdd(IEnumerable`1 items)\n   at System.Linq.Enumerable.SelectManySingleSelectorIterator`2.ToArray()\n   at Microsoft.SemanticKernel.Agents.OpenAI.Internal.AssistantThreadActions.InvokeStreamingAsync(OpenAIAssistantAgent agent, AssistantClient client, String threadId, IList`1 messages, RunCreationOptions invocationOptions, ILogger logger, Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Agents.OpenAI.Internal.AssistantThreadActions.InvokeStreamingAsync(OpenAIAssistantAgent agent, AssistantClient client, String threadId, IList`1 messages, RunCreationOptions invocationOptions, ILogger logger, Kernel kernel, KernelArguments arguments, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource.GetResult()\n   at Microsoft.SemanticKernel.Diagnostics.ActivityExtensions.RunWithActivityAsync[TResult](Func`1 getActivity, Func`1 operation, CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Diagnostics.ActivityExtensions.RunWithActivityAsync[TResult](Func`1 getActivity, Func`1 operation, CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Diagnostics.ActivityExtensions.RunWithActivityAsync[TResult](Func`1 getActivity, Func`1 operation, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource.GetResult()\n   at Microsoft.SemanticKernel.Agents.AgentChat.InvokeStreamingAgentAsync(Agent agent, CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Agents.AgentChat.InvokeStreamingAgentAsync(Agent agent, CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Agents.AgentChat.InvokeStreamingAgentAsync(Agent agent, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource.GetResult()\n   at Microsoft.SemanticKernel.Agents.AgentGroupChat.InvokeStreamingAsync(Agent agent, CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Agents.AgentGroupChat.InvokeStreamingAsync(Agent agent, CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Agents.AgentGroupChat.InvokeStreamingAsync(Agent agent, CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource.GetResult()\n   at Microsoft.SemanticKernel.Agents.AgentGroupChat.InvokeStreamingAsync(CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Agents.AgentGroupChat.InvokeStreamingAsync(CancellationToken cancellationToken)+MoveNext()\n   at Microsoft.SemanticKernel.Agents.AgentGroupChat.InvokeStreamingAsync(CancellationToken cancellationToken)+System.Threading.Tasks.Sources.IValueTaskSource.GetResult()",
    "comments": [
      {
        "user": "crickman",
        "body": "Not much going on within `ParseFunctionCall`...which made it easy to spot the likely candidate:\n\nCurrent: `arguments[argumentKvp.Key] = argumentKvp.Value.ToString();`\n\nFixed: `arguments[argumentKvp.Key] = argumentKvp.Value?.ToString();`\n\nWas able to repro and verify fix with some sample hackery.\n\nHope to have fix merged by tomorrow.\n"
      }
    ]
  },
  {
    "issue_number": 11787,
    "title": ".Net Bug: Error value cannot be null. (Parameter 'content') when using OpenAI Chat Completion Audio model",
    "author": "Cobra86",
    "state": "closed",
    "created_at": "2025-04-28T23:14:36Z",
    "updated_at": "2025-06-03T13:25:53Z",
    "labels": [
      "bug",
      ".NET",
      "ai connector",
      "needs more info"
    ],
    "body": "**Describe the bug**\nI'm sorry, I don't have a lot of info for this bug but I'm testing the audio to audio, I'm getting this error below and it's happening randomly. \n\nError Message: Value cannot be null. (Parameter 'content')\n\nStackTrace:\n```\n   at OpenAI.Argument.AssertNotNull[T](T value, String name)\n   at OpenAI.Chat.AssistantChatMessage..ctor(String content)\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.CreateRequestMessages(ChatMessageContent message)\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.CreateChatCompletionMessages(OpenAIPromptExecutionSettings executionSettings, ChatHistory chatHistory)\n   at Microsoft.SemanticKernel.Connectors.OpenAI.ClientCore.<GetChatMessageContentsAsync>d__16.MoveNext()\n   at Microsoft.SemanticKernel.ChatCompletion.ChatCompletionServiceExtensions.<GetChatMessageContentAsync>d__2.MoveNext()\n```\n\n**Platform**\n - Language: C#\n - Source: 1.47\n - AI model: OpenAI : GPT-4o-audio-preview\n - IDE: Visual Studio\n - OS: Windows\n",
    "comments": [
      {
        "user": "RogerBarreto",
        "body": "Having a quick look in the stack trace, seems that you need to provide at least a message to go to the model together with your audio. Try sending some content. \nBy the looks of it, seems that sometimes you get an empty assistantMessage and you are creating your chatHistory without it.\n\nLet me know if that helps, and appreciate if you can share a bit more of your code use-case, where you do the loop audio in/audio out."
      },
      {
        "user": "Cobra86",
        "body": "Thank you @RogerBarreto. I tried your suggestion, but I'm still getting the error intermittently.\n\nI’m not sure exactly what’s going wrong in my code. Unfortunately, I can’t share the actual implementation as it’s for work. That said, I’ve replicated the flow separately and don’t get the error in that test setup.  [https://github.com/Cobra86/SKAudioDemo](https://github.com/Cobra86/SKAudioDemo) \n\nThe key difference I’ve noticed is that in the test version I’m using AddOpenAIChatCompletion, while in the work project I’m using Azure OpenAI setup with AddOpenAIChatCompletion. Could this difference be causing a stricter validation on message content in the Azure implementation?\n\nHappy to test further if you think there's something specific I should look at."
      },
      {
        "user": "Cobra86",
        "body": "Applgoise, so this is working for me now. \n\n```\n   var response = await completionService.GetChatMessageContentAsync(\n                 _messages,\n                 _aiPromptExecutionSettings,\n                 _kernel\n             );\n\n      var safeContent = response.Content ?? string.Empty;\n      _messages.AddAssistantMessage(safeContent);\n```"
      }
    ]
  },
  {
    "issue_number": 12343,
    "title": "Bug: ChatCompletionAgent arguments no longer resolving starting on .NET SK SDK version 1.39.0",
    "author": "ricrostest",
    "state": "open",
    "created_at": "2025-06-02T21:39:42Z",
    "updated_at": "2025-06-03T10:38:48Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce the behavior:\nMy arguments are getting resolved correctly from versions 1.22.0 - 1.38.0, but once I upgrade to 1.39.0 this no longer happens. I also tried using the latest version 1.54.0 in case it had been fixed already but no luck.\n \nI confirmed this behavior by replacing the placeholders directly on the instructions string (instead of passing them as arguments) and it works correctly when I do that workaround.\n \nHere is how I instantiate & invoke the completion agent:\n\n        var completionAgent = new ChatCompletionAgent()\n        {\n            Kernel = kernel,\n            Name = agent.Name,\n            Instructions = agent.BuildInstructions(),\n            Arguments = agent.BuildArguments()\n        };\n \n        var response = completionAgent .InvokeAsync(\n            history,\n            arguments: completionAgent.Arguments,\n            kernel,\n            cancellationToken\n        );\n \nThe agent.BuildArguments function you see referenced there returns the KernelArguments I need\n(the AddArgument function is a custom extension function I built on my end to enable chaining):\n\n    public KernelArguments BuildArguments()\n    {\n        return new KernelArguments(executionSettings)\n            .AddArgument(ARGUMENT1, BuildArgument1())\n            .AddArgument(ARGUMENT2, BuildArgument2());\n    }\n \nThe workaround I used to confirm the issue is as follows when setting the Instructions property:\n\nInstructions = agent.BuildInstructions()\n  .Replace(\"{{$ARGUMENT1}}\", BuildArgument1())\n  .Replace(\"{{$ARGUMENT2}}\", BuildArgument2())\n\n**Expected behavior**\nI should not need the workaround for my arguments to be resolved correctly.\n\n**Screenshots**\nN/A\n\n**Platform**\n - Language: C#\n - Source: NuGet package version 1.39.0\n - AI model: AzureOpenAI: gpt-4o-mini\n - IDE: Visual Studio\n - OS: Windows\n\n**Additional context**\nAll context was already included above.",
    "comments": []
  },
  {
    "issue_number": 12334,
    "title": ".Net: Empty completion response with gemini-2.5-flash-preview-05-20",
    "author": "drch-",
    "state": "open",
    "created_at": "2025-06-01T21:58:10Z",
    "updated_at": "2025-06-03T10:34:02Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nRunning the [GoogleAIUsingChatCompletion](https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/Concepts/ChatCompletion/Google_GeminiChatCompletion.cs#L15) test generates no output when using gemini-2.5-flash-preview-05-20.\n\nWhen no PromptExecutionSettings is passed (ie, is null), a default is created with a [MaxTokens of 256](https://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/Connectors/Connectors.Google/GeminiPromptExecutionSettings.cs#L361).  This _appears_ to be a mistake as in the implementation of [ClientBase](https://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/Connectors/Connectors.Google/Core/ClientBase.cs#L45) it says that null indicates that the model's own default value should be used.\n\nWith these thinking models it's possible to use up the MaxTokens in the thinking phase.  The model then returns an empty response as it does not include the thinking phase in the output.  The result from the test is no output and with a metadata \"FinishReason\" => \"MaxTokens\".\n\nThe fix is probably removing this 256 default minimum and allowing it to be null in order to use the model's own default max tokens.\n\n**Platform**\n - Language: C#\n - Source: main branch\n - AI model: \n - IDE: Visual Studio\n - OS: Windows\n\n",
    "comments": [
      {
        "user": "dmm-l-mediehus",
        "body": "This is a known Gemini issue.\n\nhttps://discuss.ai.google.dev/t/finishreason-max-tokens-but-text-is-empty/81874/3"
      },
      {
        "user": "drch-",
        "body": "@dmm-l-mediehus yes I understand the behaviour, but the point is that the hard-coded default of 256 in SK basically guarantees that the user will hit this.  I suggest the better experience would be to leave this as null, which will then use the model's own defaults."
      },
      {
        "user": "markwallace-microsoft",
        "body": "@drch- Thanks for creating the issue, would you be interested in contributing the fix?"
      }
    ]
  },
  {
    "issue_number": 12315,
    "title": ".Net: Ollama: How to send tools and image, string content at the same time, how to specify think mode",
    "author": "williamlzw",
    "state": "open",
    "created_at": "2025-05-30T04:35:45Z",
    "updated_at": "2025-06-03T10:32:31Z",
    "labels": [
      ".NET",
      "ai connector"
    ],
    "body": "\n1.ollama model: mistral-small3.1\nsupport tool and vision\n\n2.ollama model:deepseek-r1\nsupport think mode\nhttps://github.com/ollama/ollama/releases/tag/v0.9.0\n\nHow to send tools and image, string content at the same time, how to specify think mode\n\n```\n#pragma warning disable SKEXP0070 \nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.ChatCompletion;\nusing Microsoft.SemanticKernel.Connectors.Ollama;\nusing System.ComponentModel;\n\nvar builder = Kernel.CreateBuilder();\nvar modelId = \"mistral-small3.1:latest\";\nvar endpoint = new Uri(\"http://localhost:11434\");\nbuilder.Services.AddOllamaChatCompletion(modelId, endpoint);\nbuilder.Plugins\n    .AddFromType<WriteFilePlugin>()\n    .AddFromType<TimePlugin>();\n\nvar kernel = builder.Build();\nvar chatCompletionService = kernel.GetRequiredService<IChatCompletionService>();\n\nvar imgs = File.ReadAllBytes(\"d:/test.jpeg\");\nstring input = \"explain the image\";\n\nvar settings = new OllamaPromptExecutionSettings\n{\n    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto(),\n\n    ExtensionData = new Dictionary<string, object> {\n          {\"enable_thinking \",true } },\n    \n};\n\ntry\n{\n    await foreach (var response in chatCompletionService.GetStreamingChatMessageContentsAsync(input, settings, kernel: kernel))\n    {\n        if (string.IsNullOrEmpty(response.Content))\n        {\n            continue;\n        }\n        Console.Write(response.Content);\n    }\n}\ncatch (Exception ex)\n{\n    Console.WriteLine($\"Error: {ex.Message}\\n\\n> \");\n}\n\n\npublic class TimePlugin\n{\n    [KernelFunction, Description(\"This method is used to query the current system time and return it in the format yyyy-MM-dd HH:mm:ss\")]\n    public string QueryDateTime()\n    {\n        return DateTime.Now.ToString(\"yyyy-MM-dd HH:mm:ss\");\n    }\n}\n\npublic class WriteFilePlugin\n{\n    [KernelFunction, Description(\"The purpose of this method is to write the text content to the local computer's target path\")]\n    public void SaveFile(string filePath, string fileContent)\n    {\n        System.IO.File.WriteAllText(filePath, fileContent);\n    }\n}\n```",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "@williamlzw Thanks for creating the issue, would you be interested in contributing the fix?"
      }
    ]
  },
  {
    "issue_number": 12313,
    "title": ".Net: ResponseSchema not working with `ForJsonSchema` in `GeminiPromptExecutionSettings`",
    "author": "RogerBarreto",
    "state": "open",
    "created_at": "2025-05-29T16:06:58Z",
    "updated_at": "2025-06-03T10:31:00Z",
    "labels": [
      ".NET",
      "ai connector"
    ],
    "body": "\n### Discussed in https://github.com/microsoft/semantic-kernel/discussions/12234\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **VladyslavLishchyna** May 22, 2025</sup>\nHi community! I try to get response from Gemini as JSON object and use like this\r\n\r\n```\r\nGeminiPromptExecutionSettings executionSettings = new()\r\n{\r\n    FunctionChoiceBehavior = FunctionChoiceBehavior.None(),\r\n    ResponseMimeType = \"application/json\",\r\n    ResponseSchema = ChatResponseFormat.ForJsonSchema(jsonSchema)\r\n};\r\n```\r\n\r\nAnd have 400 status code\r\n```\r\nException while start application - Microsoft.SemanticKernel.HttpOperationException: Response status code does not indicate success: 400 (Bad Request).\r\n ---> System.Net.Http.HttpRequestException: Response status code does not indicate success: 400 (Bad Request).\r\n   at System.Net.Http.HttpResponseMessage.EnsureSuccessStatusCode()\r\n```\r\n\r\nSchema looks like:\r\n```\r\n{\r\n  \"$schema\": \"http://json-schema.org/draft-04/schema#\",\r\n  \"title\": \"Response\",\r\n  \"type\": \"object\",\r\n  \"additionalProperties\": false,\r\n  \"properties\": {\r\n    \"FirstProperty\": {\r\n      \"type\": \"integer\",\r\n      \"format\": \"int32\"\r\n    },\r\n    \"Summary\": {\r\n      \"type\": \"string\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nIf I run without ResponseSchema parameter I receive response successfully </div>",
    "comments": []
  },
  {
    "issue_number": 12311,
    "title": ".Net: Bug: Inconsistent Handling of ResponseFormat Type in AzureOpenAIPromptExecutionSettings Between IChatCompletionService and IChatClient",
    "author": "runceel",
    "state": "open",
    "created_at": "2025-05-29T13:48:24Z",
    "updated_at": "2025-06-03T09:59:51Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nWhen using AzureOpenAIPromptExecutionSettings.ResponseFormat with a System.Type (e.g., typeof(Person)), there is inconsistent behavior between IChatCompletionService and IChatClient.\n- IChatCompletionService returns structured output as expected.\n- IChatClient throws a serialization exception:\n```\nSerialization and deserialization of 'System.RuntimeType' instances is not supported. The unsupported member type is located on type 'System.Object'. Path: $.ResponseFormat.\nat System.Text.Json.ThrowHelper.ThrowNotSupportedException(WriteStack& state, Exception innerException)\nat System.Text.Json.Serialization.JsonConverter1.WriteCore(Utf8JsonWriter writer, T& value, JsonSerializerOptions options, WriteStack& state)   at System.Text.Json.Serialization.Metadata.JsonTypeInfo1.Serialize(Utf8JsonWriter writer, T& rootValue, Object rootValueBoxed)\nat System.Text.Json.Serialization.Metadata.JsonTypeInfo`1.SerializeAsObject(Utf8JsonWriter writer, Object rootValue)\nat System.Text.Json.JsonSerializer.WriteStringAsObject(Object value, JsonTypeInfo jsonTypeInfo)\nat Microsoft.SemanticKernel.PromptExecutionSettingsExtensions.ToChatOptions(PromptExecutionSettings settings, Kernel kernel)\nat Program.<<Main>$>d__0.MoveNext() (Program.cs:line 36)\n```\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Set up a Semantic Kernel project with both IChatCompletionService and IChatClient using Azure OpenAI.\n2. Set ResponseFormat = typeof(Person) in AzureOpenAIPromptExecutionSettings.\n3. Call both GetChatMessageContentAsync (via IChatCompletionService) and GetResponseAsync (via IChatClient).\n4. Observe that IChatClient throws a serialization exception, while IChatCompletionService works as expected.\n\n**Expected behavior**\nBoth IChatCompletionService and IChatClient should handle the ResponseFormat type consistently, enabling structured output and returning the result in JSON format.\n\n**Screenshots**\nN/A (see exception message above)\n\n**Platform**\n - Language: C#\n - Source: NuGet package Microsoft.SemanticKernel 1.54.0,\n - AI model: Azure OpenAI Service, gpt-4.1\n - IDE: Visual Studio 2022\n - OS: Windows 11\n\n**Additional context**\n\nThis difference in behavior was noticed when switching from AddAzureOpenAIChatCompletion to AddAzureOpenAIChatClient while using ChatCompletionAgent, which resulted in the exception.\n\nMinimal repro:\n\n```csharp\nusing Microsoft.Extensions.AI;\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.ChatCompletion;\nusing Microsoft.SemanticKernel.Connectors.AzureOpenAI;\n\nconst string Endpoint = \"<<your aoai endpoint>>\";\nconst string DeploymentName = \"<<your deployment model name>>\";\nconst string ApiKey = \"<<your api key>>\";\n\nvar builder = Kernel.CreateBuilder();\nbuilder.AddAzureOpenAIChatCompletion(\n    DeploymentName,\n    Endpoint,\n    ApiKey,\n    serviceId: \"ChatCompletion\");\nbuilder.AddAzureOpenAIChatClient(\n    DeploymentName,\n    Endpoint,\n    ApiKey,\n    serviceId: \"ChatClient\");\n\nvar kernel = builder.Build();\n\nIChatCompletionService chatCompletionService = kernel.GetRequiredService<IChatCompletionService>(\"ChatCompletion\");\nIChatClient chatClient = kernel.GetRequiredService<IChatClient>(\"ChatClient\");\n\nconst string input = \"My name is Jone Doe, I am 30 years old.\";\n\nvar chatCompletionResponse = await chatCompletionService.GetChatMessageContentAsync(input,\n    new AzureOpenAIPromptExecutionSettings\n    {\n        ResponseFormat = typeof(Person),\n    });\nConsole.WriteLine(chatCompletionResponse.Content);\n\nvar chatClientResponse = await chatClient.GetResponseAsync([new ChatMessage(ChatRole.User, input)],\n    options: new AzureOpenAIPromptExecutionSettings\n    {\n        ResponseFormat = typeof(Person),\n    }.ToChatOptions(kernel));\nConsole.WriteLine(chatClientResponse.Text);\n\nrecord Person(string Name, int Age);\n```",
    "comments": []
  },
  {
    "issue_number": 12350,
    "title": ".Net: Bug:",
    "author": "Transfersystem",
    "state": "closed",
    "created_at": "2025-06-03T06:31:08Z",
    "updated_at": "2025-06-03T07:54:44Z",
    "labels": [
      "bug",
      ".NET",
      "python",
      "triage"
    ],
    "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to '...'\n2. Click on '....'\n3. Scroll down to '....'\n4. See error\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Platform**\n - Language: [e.g. C#, Python]\n - Source: [e.g. NuGet package version 0.1.0, pip package version 0.1.0, main branch of repository]\n - AI model: [e.g. OpenAI:GPT-4o-mini(2024-07-18)]\n - IDE: [e.g. Visual Studio, VS Code]\n - OS: [e.g. Windows, Mac]\n\n**Additional context**\nAdd any other context about the problem here.",
    "comments": [
      {
        "user": "Transfersystem",
        "body": "i don't know "
      }
    ]
  },
  {
    "issue_number": 12070,
    "title": ".Net: Bug: C#, error in using Microsoft.SemanticKernel.Agents.AzureAI 1.48.0-preview and above",
    "author": "BenoityipMSFT",
    "state": "closed",
    "created_at": "2025-05-15T02:21:41Z",
    "updated_at": "2025-06-03T05:08:32Z",
    "labels": [
      "bug",
      ".NET"
    ],
    "body": "**Describe the bug**\nWhen using VS2022 C# .Net 9, and add Nuget package Microsoft.SemanticKernel.Agents.AzureAI 1.48.0-preview and above. Calling GetAgentsClient() method will throw a compile error\n\nusing 1.47.0-preview does not have this error\n\nReference to type 'ConnectionProvider' claims it is defined in 'System.ClientModel', but it could not be found\n\nThe code we follow is in https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/azure-ai-agent?pivots=programming-language-csharp#configuring-the-ai-project-client\n\n\n**Expected behavior**\nCompile the code successfully\n\n**Screenshots**\n<img width=\"745\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0ebcea46-d88f-4ac1-b0fb-1d0d28427e4c\" />",
    "comments": [
      {
        "user": "crickman",
        "body": "Hi @BenoityipMSFT, \n\n[ConnectionProvider](https://learn.microsoft.com/en-us/dotnet/api/system.clientmodel.primitives.connectionprovider?view=azure-dotnet-preview) is certainly defined in `System.ClientModel`; however, I cannot repro in my own project locally [1.47.0 - 1.50.0].  \n\nHere is the source:\n\n```xml\n<Project Sdk=\"Microsoft.NET.Sdk\">\n\n  <PropertyGroup>\n    <OutputType>Exe</OutputType>\n    <TargetFramework>net8.0</TargetFramework>\n    <ImplicitUsings>enable</ImplicitUsings>\n    <Nullable>enable</Nullable>\n    <NoWarn>$(NoWarn);SKEXP0110</NoWarn>\n  </PropertyGroup>\n\n  <ItemGroup>\n    <PackageReference Include=\"Microsoft.SemanticKernel.Agents.AzureAI\" Version=\"1.50.0-preview\" />\n  </ItemGroup>\n\n</Project>\n```\n\n```c#\nusing Azure.AI.Projects;\nusing Azure.Identity;\nusing Microsoft.SemanticKernel.Agents.AzureAI;\n\nnamespace SKFoundryAgentDependencyAnalysis;\n\ninternal class Program\n{\n    static async Task Main(string[] args)\n    {\n        AIProjectClient client = AzureAIAgent.CreateAzureAIClient(\"<conneciton string>\", new AzureCliCredential());\n        AgentsClient agentsClient = client.GetAgentsClient();\n        Agent agent = await agentsClient.GetAgentAsync(\"asst_ynGawRZhcWoDT6w78YMZUmUO\");\n        Console.WriteLine(agent.Name);\n    }\n}\n```\n\nI do wonder if this is a build error or intellisense.  Can you please switch your error filter to \"Build\" only:\n![Image](https://github.com/user-attachments/assets/e277854b-a698-49cb-aeb5-a7d27a142935)\n\nI'd also be interested to see the console message from the build failure.\n\nI've pinged you directly and am happy to take a look at what may be in play on your local environment.\n\n> Note: We are _just_ about to update the underlying SDK dependency (breaking change) as Foundry goes GA.  (So the dependency graph will be dramatically altered - FYI)"
      },
      {
        "user": "crickman",
        "body": "Working together on a call, we discover that this issue occurs when the `Azure.Identity` package is included as an explicit project dependncy.  As a work-around, Ben is unblocked by removing this reference and relying on the transitive dependency from SK package.\n\nWill continue to monitor and investigate as we updated our dependencies over the next couple days."
      },
      {
        "user": "BenoityipMSFT",
        "body": "Thanks for your effort for unblocking me. Happy to continue investigate with you together"
      }
    ]
  },
  {
    "issue_number": 10365,
    "title": "Python: Bug: Unsupported parameter: 'parallel_tool_calls' is not supported with this model error",
    "author": "kong156",
    "state": "closed",
    "created_at": "2025-02-01T12:45:08Z",
    "updated_at": "2025-06-03T02:25:21Z",
    "labels": [
      "bug",
      "python"
    ],
    "body": "I'm running the sample code bellow that worked since Azure OpenAI API was upgraded to 2024-12-01-preview. I get the error - Unsupported parameter: 'parallel_tool_calls' is not supported with this model.\n\n```python\n#.env:\nAZURE_OPENAI_COMPLETIONS_API_VERSION='2024-12-01-preview'\nAZURE_OPENAI_COMPLETIONS_MODEL='gpt-4o'\n-----\n#main.py\nfrom semantic_kernel import Kernel\nfrom semantic_kernel.agents import ChatCompletionAgent\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\nfrom semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\nfrom semantic_kernel.contents.chat_history import ChatHistory\nfrom semantic_kernel.contents import ChatMessageContent\nfrom semantic_kernel.contents.utils.author_role import AuthorRole\n\nasync def main():\n    load_dotenv()\n    kernel = Kernel()\n    service_id = os.getenv('SEMNATIC_KERNEL_SERVICE_ID')\n    chat_completion = AzureChatCompletion(\n        service_id=service_id,\n        base_url=os.getenv('AZURE_OPENAI_COMPLETIONS_URL'),\n        deployment_name=os.getenv('AZURE_OPENAI_COMPLETIONS_MODEL'),\n        api_version=os.getenv('AZURE_OPENAI_COMPLETIONS_API_VERSION'),\n        api_key=os.getenv('AZURE_OPENAI_API_KEY'))\n    \n    kernel.add_service(chat_completion)\n    kernel.add_plugin(\n        plugin=Retriever(),\n        plugin_name=\"Retriever\") \n    execution_settings = kernel.get_prompt_execution_settings_from_service_id(service_id=service_id)\n    execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n\n    agent = ChatCompletionAgent(\n        service_id=service_id,\n        kernel=kernel,\n        name=service_id,\n        execution_settings=execution_settings)\n    \n    history = ChatHistory()\n\n    while True:\n        user_input = input(\"User > \")\n\n        if user_input == \"exit\":\n            break\n\n        history.add_message(ChatMessageContent(role=AuthorRole.USER, content=user_input))\n\n        async for response in agent.invoke(history=history):\n            print(f\"{response.content}\")\n\n# Run the main function\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Hello @kong156, what version of SK Python are you running? One quick comment (although may not be related to the issue at hand): I'd recommend configuring the `AzureChatCompletion` `endpoint` instead of the `base_url`, where the `endpoint` env var has the form: `AZURE_OPENAI_ENDPOINT=\"https://<resouce-name>.openai.azure.com/\"`.\n\nAnother comment - once you do upgrade to 1.20.0 for the chat completion agent, you'll need to pass in `execution_settings` as part of the `arguments=KernelArguments(settings=execution_settings)`. \n\nI am currently running `AzureChatCompletion` with the `2024-12-01-preview` api version and I am not seeing any errors. My gpt-4o deployment is 2024-08-06. What do you have configured on your end?\n\n```\nDEBUG:openai._base_client:HTTP Request: POST https://<resource-name>.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \n\"200 OK\"\n```"
      },
      {
        "user": "kong156",
        "body": "Hi @moonbox3, pretty much the same settings, client and server side. I'm already at SK 1.20 with the mentioned changes.\nWhat is working for me at the moment is the following setup for AzureChatCompletion: \n- base_url='https://`<resource-name>`.openai.azure.com/openai/'\n- api_version='2024-12-01-preview'\nOn Azure OpenAI side: gpt-4o, model version 2024-08-06, endpoint: https://`<resource-name>`.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \nendpoint='https://`<resource-name>`.openai.azure.com/' returns 404 resource not found, so I have to stick with the above settings.\n\nThank you,"
      },
      {
        "user": "moonbox3",
        "body": "Hi @kong156, thanks for your response. I would still recommend using endpoint / chat_deployment_name (equivalent to passing in `deployment_name`). My underlying .env config looks like:\n\nAZURE_OPENAI_API_KEY=\"<optional if not using Entra auth>\"\nAZURE_OPENAI_CHAT_DEPLOYMENT_NAME=\"gpt-4o\"\nAZURE_OPENAI_ENDPOINT=\"https://<resource-name>.openai.azure.com/\"\nAZURE_OPENAI_API_VERSION=\"2024-12-01-preview\"\n\nThis is what we do in the config to set up the client: https://github.com/microsoft/semantic-kernel/blob/f459caf02d010c237d61dff31b39050a82306886/python/semantic_kernel/connectors/ai/open_ai/services/azure_config_base.py#L82C13-L93C14\n\nSince VSCode treats my .env file like env vars and loads them as such (my .env file is at the root of my sk/python project), I only need to do:\n\n```python\nkernel = Kernel()\nkernel.add_service(AzureChatCompletion())\n```\n\nand all of the settings are picked up, as we use Pydantic settings underneath. \n\n---\n\nTo diagnose the 404 you're receiving when using endpoint, I'd recommend sticking this logging config:\n\n```python\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n```\n\nin your script that creates the agent, and then double checking what API endpoint is being used to call Azure OpenAI. This is a pretty quick indication if something is malformed, like (two `openai` instead of one):\n\n```\nhttps://<resource-name>.openai.azure.com/openai/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview\n```\n\nIn any case you say \"**What is working for me at the moment is the following setup for AzureChatCompletion...**\" So you are able to make calls to the model and it is not throwing an error related to `parallel_tool_calls`?"
      }
    ]
  },
  {
    "issue_number": 10679,
    "title": ".Net: Pausing Tool Calls in C# Semantic Kernel to Request User Input",
    "author": "sophialagerkranspandey",
    "state": "open",
    "created_at": "2025-02-25T16:37:17Z",
    "updated_at": "2025-06-03T02:14:26Z",
    "labels": [
      ".NET",
      "sk team issue",
      "stale"
    ],
    "body": "\n### Discussed in https://github.com/microsoft/semantic-kernel/discussions/10673\n\n<div type='discussions-op-text'>\n\n<sup>Originally posted by **StefandewitSPS** February 25, 2025</sup>\nI am using C# Semantic Kernel (1.38.0-alpha). I am wondering how I can temporarily stop the model from making tool calls to, for example, ask the user an intermediate question.\r\n\r\nAn example of this:\r\n\r\nUser input: \"Give me the information about organization 'example'.\"\r\n\r\nNow, a kernel function is called, which returns three results. Before the model proceeds with calling other functions, I want the user to first choose one of the organizations.\r\n\r\nI tried solving this using an IAutoFunctionInvocationFilter, but it does not work when I set a schema on an agent using AssistantResponseFormat.CreateJsonSchemaFormat(). However, if I change this JSON schema to another option in the class, such as AssistantResponseFormat.JsonObject, it does work.\r\n\r\nCould it be that IAutoFunctionInvocationFilter does not work in combination with AssistantResponseFormat.CreateJsonSchemaFormat(), or should I approach this differently? If so, how?</div>",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10769,
    "title": ".Net: Extract Plugin Name and Arguments with agent.invoke",
    "author": "sophialagerkranspandey",
    "state": "open",
    "created_at": "2025-03-03T17:10:41Z",
    "updated_at": "2025-06-03T02:14:22Z",
    "labels": [
      "bug",
      ".NET",
      "python",
      "stale"
    ],
    "body": "**Describe the bug**\nA clear and concise description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to '...'\n2. Click on '....'\n3. Scroll down to '....'\n4. See error\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Platform**\n - Language: [e.g. C#, Python]\n - Source: [e.g. NuGet package version 0.1.0, pip package version 0.1.0, main branch of repository]\n - AI model: [e.g. OpenAI:GPT-4o-mini(2024-07-18)]\n - IDE: [e.g. Visual Studio, VS Code]\n - OS: [e.g. Windows, Mac]\n\n**Additional context**\nAdd any other context about the problem here.\n\n\nfrom: https://teams.microsoft.com/l/message/19:W5nYJLrRvQatcJ04yDmCkzm-t7B6c7VTxS57yNtvSzU1@thread.tacv2/1741016042436?tenantId=72f988bf-86f1-41af-91ab-2d7cd011db47&groupId=a87ae912-78ce-4eb9-862a-e7edf1b2aaa6&parentMessageId=1741016042436&teamName=Semantic%20Kernel%20-%20MSFT%20Support&channelName=General&createdTime=1741016042436\n\nExtract Plugin Name and Arguments with agent.invoke\nIs there a way to extract the plugin name and arguments with agent.invoke ?\n \nI need this for evaluation purposes while using ChatCompletionAgent with FunctionChoiceBehavior.Auto().\n \nUsing ChatCompletionAgent with agent.invoke and FunctionChoiceBehavior.Auto() (sample code attached). \n \nWith agent.invoke(), content.items only contains TextContents.\nBut with agent.invoke_stream(), content.items includes both FunctionResultContents and TextContents. \n \nHow can I retrieveFunctionResultContents (with function name, plugin name, and arguments) when using agent.invoke()?\n \n \n \n# Copyright (c) Microsoft. All rights reserved.\nimport asyncio\nfrom typing import Annotated\nfrom semantic_kernel import Kernel\nfrom semantic_kernel.agents import ChatCompletionAgent\nfrom semantic_kernel.connectors.ai import FunctionChoiceBehavior\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\nfrom semantic_kernel.contents import ChatHistory, FunctionCallContent, FunctionResultContent\nfrom semantic_kernel.functions import KernelArguments, kernel_function\n\n# Define a sample plugin for the sample\nclass MenuPlugin:\n    \"\"\"A sample Menu Plugin used for the concept sample.\"\"\"\n    @kernel_function(description=\"Provides a list of specials from the menu.\")\n    def get_specials(self) -> Annotated[str, \"Returns the specials from the menu.\"]:\n        return \"\"\"\n        Special Soup: Clam Chowder\n        Special Salad: Cobb Salad\n        Special Drink: Chai Tea\n        \"\"\"\n    @kernel_function(description=\"Provides the price of the requested menu item.\")\n    def get_item_price(\n        self, menu_item: Annotated[str, \"The name of the menu item.\"]\n    ) -> Annotated[str, \"Returns the price of the menu item.\"]:\n        return \"$9.99\"\n\n# Simulate a conversation with the agent\nUSER_INPUTS = [\n    \"Hello\",\n    \"What is the special soup?\",\n    \"What does that cost?\",\n    \"Thank you\",\n]\n\nasync def main():\n    # 1. Create the instance of the Kernel to register the plugin and service\n    service_id = \"agent\"\n    kernel = Kernel()\n    kernel.add_plugin(MenuPlugin(), plugin_name=\"menu\")\n    kernel.add_service(AzureChatCompletion(service_id=service_id))\n    # 2. Configure the function choice behavior to auto invoke kernel functions\n    # so that the agent can automatically execute the menu plugin functions when needed\n    settings = kernel.get_prompt_execution_settings_from_service_id(service_id=service_id)\n    settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n    # 3. Create the agent\n    agent = ChatCompletionAgent(\n        kernel=kernel,\n        name=\"Host\",\n        instructions=\"Answer questions about the menu.\",\n        arguments=KernelArguments(settings=settings),\n    )\n    # 4. Create a chat history to hold the conversation\n    chat_history = ChatHistory()\n    for user_input in USER_INPUTS:\n        # 5. Add the user input to the chat history\n        chat_history.add_user_message(user_input)\n        print(f\"# User: {user_input}\")\n        # 6. Invoke the agent for a response\n        async for content in agent.invoke(chat_history):\n            print(f\"# {content.name}: \", end=\"\")\n            if (\n                not any(isinstance(item, (FunctionCallContent, FunctionResultContent)) for item in content.items)\n                and content.content.strip()\n            ):\n                # We only want to print the content if it's not a function call or result\n                print(f\"{content.content}\", end=\"\", flush=True)\n        print(\"\")\n \n \n \n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 12324,
    "title": "Python: Bug: Python - AzureAIAgent - BingGrounding - Streaming - Annotation missing quote/text.",
    "author": "rutzsco",
    "state": "closed",
    "created_at": "2025-05-30T17:49:17Z",
    "updated_at": "2025-06-03T01:37:32Z",
    "labels": [
      "bug",
      "python",
      "agents"
    ],
    "body": "**Describe the bug**\nWhen using bing_grounding tool in AzureAIAgent, the source citation(Eg.【3:4†source】) is not getting returned in the quote field of the StreamingAnnotationContent. This makes it more difficult to map the annotations to the reference in the agent response.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Create an AzureAIAgent with bing grounding\n2. Implement in SK with streaming response\n3. Invoke Agent with \"What is the weather forecast for today in New York City?\"\n4. See error\n\n**Expected behavior**\nStreamingAnnotationContent should have quote populated contain the text value for the source citation(eg 【3:4†source】.\n\n**Platform**\n - Language: [Python]\n - Source: [1.32.0]\n - AI model: [GPT-41]\n - IDE: [VS Code]\n - OS: [Windows]\n\n**Additional context**\n\nThe issue appears to be in **agent_content_generation.py** generate_streaming_bing_grounding_content not setting the quote in condition starting at line 575. See potential fix:\n\n![Image](https://github.com/user-attachments/assets/8d80a5b1-2c3c-4242-84d0-f55d3366ffbb)",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Strange, I thought we had already fixed this. Thanks for filing."
      },
      {
        "user": "moonbox3",
        "body": "Part of 1.32.1."
      }
    ]
  },
  {
    "issue_number": 12331,
    "title": "Python: Add Bing search query URL to `StreamingAnnotationContent`",
    "author": "jordanbean-msft",
    "state": "closed",
    "created_at": "2025-05-31T15:12:19Z",
    "updated_at": "2025-06-03T01:37:25Z",
    "labels": [
      "python",
      "agents"
    ],
    "body": "---\nname: Add Bing search query URL to `StreamingAnnotationContent`\nabout: Add Bing search query URL to `StreamingAnnotationContent`\n\n---\n\nThe terms of use for Azure AI Agents Service Grounding with Bing Search (and Bing Custom Search) require the UI to display both the website URLs & the Bing search query URLs.\n\nhttps://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/bing-grounding#how-to-display-grounding-with-bing-search-results\n\nThe existing `StreamingAnnotationContent` object does provide the website URL, but it does not provide the Bing search query URL. This makes is much harder to correctly render the results of Grounding with Bing Search as it requires an additional call to get the Bing search query URL from the underlying Azure AI Agents SDK `runstep` details (I'm also not sure how I would get the `run_id` when invoking the agent through SK that is needed to get the `runstep` details).\n\n**Please include the Bing search query URL in the `StreamingAnnotationContent`** so it is easy to build a UI that conforms to the [Bing terms of use and display requirements](https://www.microsoft.com/en-us/bing/apis/grounding-legal#use-and-display-requirements).\n\n![Image](https://github.com/user-attachments/assets/24126630-f49e-4791-940c-0462c68989f5)\n\n<!-- ⚠️⚠️ Do Not Delete This! feature_request_template ⚠️⚠️ -->\n<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->\n<!-- Please search existing issues to avoid creating duplicates. -->\n\n<!-- Describe the feature you'd like. -->\n",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Hi @jordanbean-msft, for streaming invocations, the Azure SDK passes back the Bing search query URL as part of the `THREAD_RUN_STEP_DELTA` tool call. If I enable the `on_intermediate_message` callback, I see:\n\n```\n# User: 'Which team won the 2025 NCAA basketball championship?'\nFunction Call:> bing_grounding with arguments: {'requesturl': 'https://api.bing.microsoft.com/v7.0/search?q=search(query: 2025 NCAA basketball championship winner)'}\nFunction Call:> bing_grounding with arguments: {'response_metadata': \"{'market': 'en-US', 'num_docs_retrieved': 5, 'num_docs_actually_used': 5}\"}\n# BingGroundingAgent: The Florida Gators won the 2025 NCAA men's basketball championship. They defeated the Houston Cougars with a close score of 65-63 in the championship game held in San Antonio, Texas. This victory marked their third national title. Florida overcame a 12-point deficit during the game to claim the championship【3:0†source】\nAnnotation :> https://en.wikipedia.org/wiki/2025_NCAA_Division_I_men%27s_basketball_championship_game, source=None, with start_index=308 and end_index=320\n【3:1†source】\nAnnotation :> https://www.ncaa.com/history/basketball-men/d1, source=None, with start_index=320 and end_index=332\n【3:2†source】\nAnnotation :> https://sports.yahoo.com/article/florida-gators-win-2025-ncaa-034021303.html, source=None, with start_index=332 and end_index=344\n```"
      },
      {
        "user": "moonbox3",
        "body": "Part of 1.32.1."
      }
    ]
  },
  {
    "issue_number": 12328,
    "title": "Python: AzureAIAgent invoke_stream not running all tools",
    "author": "moonbox3",
    "state": "closed",
    "created_at": "2025-05-30T22:24:48Z",
    "updated_at": "2025-06-03T01:37:18Z",
    "labels": [
      "bug",
      "python",
      "agents"
    ],
    "body": "Investigate the issue where not all tools are running during a streaming invoke. ",
    "comments": [
      {
        "user": "moonbox3",
        "body": "Part of 1.32.1."
      }
    ]
  },
  {
    "issue_number": 11494,
    "title": ".Net: A2A Samples",
    "author": "markwallace-microsoft",
    "state": "closed",
    "created_at": "2025-04-10T16:26:03Z",
    "updated_at": "2025-06-02T08:35:02Z",
    "labels": [
      ".NET",
      "python",
      "triage"
    ],
    "body": "1. Sample showing how communicate between two SK Chat Completion agents\n    - [ ] Python\n    - [ ] .Net \n 1. Sample showing how communicate between SK Agents and agents that implement A2A\n    - [ ] Python\n    - [ ] .Net \n\nReference: https://github.com/moonbox3/semantic-kernel/blob/a2a-with-sk/docs/decisions/00XX-A2A-integration-within-SK.md",
    "comments": [
      {
        "user": "markwallace-microsoft",
        "body": "Duplicate of https://github.com/microsoft/semantic-kernel/issues/11493"
      },
      {
        "user": "hansmbakker",
        "body": "This is not a duplicate, because this issue is about the .NET version. The Python version might be done but the .NET version is still being worked on in https://github.com/microsoft/semantic-kernel/pull/12050.\n\n@markwallace-microsoft @moonbox3 is it an idea to reopen this, so that it can be tracked?"
      }
    ]
  },
  {
    "issue_number": 10694,
    "title": ".Net: Cannot change log level of \"Microsoft.SemanticKernel\"",
    "author": "grafanaKibana",
    "state": "open",
    "created_at": "2025-02-26T12:17:36Z",
    "updated_at": "2025-06-02T02:16:09Z",
    "labels": [
      "bug",
      ".NET",
      "stale"
    ],
    "body": "**Describe the bug**\nChanging the LogLevel of Semantic Kernel to Warning doesn't affect actual logging and I still have Info logging.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to 'appsettings.json'\n2. Set \n```\n\"Logging\": {\n    \"LogLevel\": {\n      \"Default\": \"Information\",\n      \"Microsoft.AspNetCore\": \"Warning\",\n      \"Microsoft.SemanticKernel\": \"Warning\"\n    }\n  }\n```\n3. Run KernelFunction.InvokeAsync\n4. See Info logs\n\n**Expected behavior**\nLog level should affect SK logging\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Platform**\n - Language: C#\n - Source: 1.39.0\n - AI model: GPT-4o\n - IDE: Rider\n - OS: Windows\n",
    "comments": [
      {
        "user": "feiyun0112",
        "body": "I tested \"Microsoft.SemanticKernel\": \"Warning\" can work \n\nplease check configuration is overridden, for example, in appsettings.development.json. \n\nor provide a reproducible minimal project."
      },
      {
        "user": "grafanaKibana",
        "body": "@feiyun0112 \nI've checked the `appsettings.json` and `appsettings.development.json` and both not affecting the logs in debug or release mode.\n\nAlso I didn't managed to reproduce it on sample project unfortunately. Maybe I can share details of specific part of problem project?\n\nEdit: Also just found out that we are using Serilog for logging, is it affected by settings in the appsettings?"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10709,
    "title": ".Net: Create sample showing how to use an LLM to present data from a REST API call",
    "author": "markwallace-microsoft",
    "state": "open",
    "created_at": "2025-02-27T13:49:51Z",
    "updated_at": "2025-06-02T02:16:05Z",
    "labels": [
      "sk team issue",
      "stale"
    ],
    "body": "This is a question that came up in the 19-Feb-2025 office hours.\n\nHere's the scenario:\n\n1. REST API will return structured data (Information about EV's registered in the US)\n2. Goal is to present the data in well structured tabular format\n\nProposed solution is:\n\n1. Generate an application (2 tier with .Net backend and React frontend)\n2. Shape of the data desired will determine the code that get's generated\n\nAlternate solution:\n\n1. Use function calling (with structured outputs) to allow the LLM to request the raw data\n2. Allow the LLM to reason over the data and present it in the desired format\n3. Optionally integrate code interpreter to allow the LLM to perform data manipulation",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10744,
    "title": "Use ONNX Runtime GenAI's IChatClient implementation",
    "author": "stephentoub",
    "state": "open",
    "created_at": "2025-03-01T03:07:34Z",
    "updated_at": "2025-06-02T02:16:03Z",
    "labels": [
      "sk team issue",
      "stale"
    ],
    "body": "The ONNX Runtime GenAI library being used in Connectors.Onnx now includes an IChatClient implementation. That can be used in OnnxRuntimeGenAIChatCompletionService rather than a bespoke implementation.",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10753,
    "title": ".Net: Concept Project -  Creation & Transformation Agent",
    "author": "RogerBarreto",
    "state": "open",
    "created_at": "2025-03-03T12:57:33Z",
    "updated_at": "2025-06-02T02:16:01Z",
    "labels": [
      ".NET",
      "sk team issue",
      "stale"
    ],
    "body": "Prepare an Agent/Logic to go over the Concepts and suggest conversions based in the Guidelines.\n\n- [ ] Identify files and suggest changes based in the approved Guidelines.\n- [ ] Simplify editing and submission of changes\n- [ ] Experimental - Convert existing samples into Program.cs ready\n- [ ] Experimental - convert existing samples into notebooks \n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  },
  {
    "issue_number": 10142,
    "title": ".Net: Bug: .NET - Structured Output ResponseFormat schema incorrect for arrays of repeated types  --- Invalid schema for response_format - reference can only point to definitions defined at the top level of the schema.",
    "author": "AaronMacF",
    "state": "open",
    "created_at": "2025-01-09T15:40:47Z",
    "updated_at": "2025-06-01T02:16:36Z",
    "labels": [
      "bug",
      ".NET",
      "stale"
    ],
    "body": "**Describe the bug**\nWhen using ResponseFormat = typeof(MyClass), the schema is incorrectly generated when two array properties use the same object type, leading to the following error:\n```\n{\n  \"error\": {\n    \"message\": \"Invalid schema for response_format 'MyClass': In context=('properties', 'PropertyB', 'items'), reference can only point to definitions defined at the top level of the schema.\",\n    \"type\": \"invalid_request_error\",\n    \"param\": \"response_format\",\n    \"code\": null\n  }\n}\n```\n\n\n**To Reproduce**\nSteps to reproduce the behavior:\n\nSee this MRE:\n\nProgram.cs\n```\nOpenAIPromptExecutionSettings executionSettings = new() { ResponseFormat = typeof(MyClass) };\n KernelArguments kernelArgs = new(executionSettings);\nawait kernel.InvokeAsync(\"PluginName\", \"FunctionName\", kernelArgs);\n```\n\nMyClass.cs\n```\npublic class MyClass\n{\n  public SubClass[] PropertyA { get; set; }\n  public SubClass[] PropertyB { get; set; }\n}\n```\n\nSubClass.cs\n```\npublic class SubClass\n{\n  public string Property1 { get; set; }\n  public int Property2 { get; set; }\n}\n```\n\n**Invalid** generated schema:\n```\n{\n  \"response_format\": {\n    \"json_schema\": {\n      \"name\": \"MyClass\",\n      \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"PropertyA\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"Property1\": { \"type\": \"string\" },\n                \"Property2\": { \"type\": \"integer\" }\n              },\n              \"additionalProperties\": false,\n              \"required\": [\"Property1\", \"Property2\"]\n            }\n          },\n          \"PropertyB\": {\n            \"type\": \"array\",\n            \"items\": { \"$ref\": \"#/properties/PropertyA/items\" }\n          }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\"PropertyA\", \"PropertyB\"]\n      },\n      \"strict\": true\n    },\n    \"type\": \"json_schema\"\n  }\n}\n\n```\n\n**Expected valid schema**\n```\n{\n  \"response_format\": {\n    \"json_schema\": {\n      \"name\": \"MyClass\",\n      \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"PropertyA\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"Property1\": { \"type\": \"string\" },\n                \"Property2\": { \"type\": \"integer\" }\n              },\n              \"additionalProperties\": false,\n              \"required\": [\"Property1\", \"Property2\"]\n            }\n          },\n          \"PropertyB\": {\n            \"type\": \"array\",\n            \"items\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"Property1\": { \"type\": \"string\" },\n                \"Property2\": { \"type\": \"integer\" }\n              },\n              \"additionalProperties\": false,\n              \"required\": [\"Property1\", \"Property2\"]\n            }\n          }\n        },\n        \"additionalProperties\": false,\n        \"required\": [\"PropertyA\", \"PropertyB\"]\n      },\n      \"strict\": true\n    },\n    \"type\": \"json_schema\"\n  }\n}\n```\n\n**Expected behavior**\nThe generated schema is valid, and PropertyB doesn't use a reference to PropertyA\n\n**Platform**\n\n - Language: C# \n - Source: Semantic Kernel version 1.33.0",
    "comments": [
      {
        "user": "jb-preston-jennings",
        "body": "I'm running into a similar problem. When`strict` mode is enabled, OpenAI rejects the tool definitions because one of my tool parameters uses a class with a recursive structure.\n\nE.g.\n```\npublic class Control\n{\n    public List<Control> Children { get; set; } = new();\n}\n``` \n\nGoing back to the example provided by the OP, it appears the generated JSON schema output is using a bad reference. The \"#\" ref isn't rooted correctly (i.e. it's pointing to the `schema` node, not the root of the entire JSON document):\n\n```\n\"PropertyB\": {\n    \"type\": \"array\",\n    \"items\": { \"$ref\": \"#/properties/PropertyA/items\" }\n}\n```\n"
      },
      {
        "user": "jb-preston-jennings",
        "body": "Hi @SergeyMenshykh - I see this is assigned to you. Just wondering if this item is on the short to medium term radar, or if we should be trying to work around it (e.g. generating custom schema without the bad reference). Thanks!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 90 days with no activity."
      }
    ]
  }
]