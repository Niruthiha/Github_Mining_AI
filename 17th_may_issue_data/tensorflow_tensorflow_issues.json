[
  {
    "issue_number": 48545,
    "title": "Introduce ability to clear GPU memory in Tensorflow 2",
    "author": "GatGit12",
    "state": "closed",
    "created_at": "2021-04-15T14:48:12Z",
    "updated_at": "2025-06-17T14:10:22Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:feature",
      "comp:gpu",
      "TF 2.4"
    ],
    "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.1, 2.4.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently there is no way to completely free the (once) allocated GPU RAM. \r\nFor example, i want to use tensorflow in the context of 3d visualization which is made next to impossible by this behavior. Standard solutions like `tf.config.experimental.set_memory_growth(gpus[0], True)` are unfortunately not sufficient, because the once allocated RAM cannot be released again.\r\n\r\nIn #36465 (https://github.com/tensorflow/tensorflow/issues/36465#issuecomment-818742876), it is mentioned that by using `GPUProcessState::TestOnlyReset` and `ProcessState::TestOnlyReset` the option to release GPU memory exists, but is just not exposed or for testing purposes only.\r\n\r\nIt would be very nice for applications using tensorflow to have proper access to gpu ram release functions.\r\n\r\n**Will this change the current api? How?**\r\nIntroduce a new (experimental) function to reset the current session/graph/device/... - state and thus free the GPU RAM completely.\r\n\r\n**Who will benefit with this feature?**\r\nPeople who use Tensorflow in their application in conjunction with other GPU-RAM critical operations such as 3D rendering.",
    "comments": [
      {
        "user": "amahendrakar",
        "body": "@GatGit12,\r\nSince similar issue is already being tracking in issue [#36465](https://github.com/tensorflow/tensorflow/issues/36465), to avoid duplicates can you please close this and subscribe/follow that issue? Thanks!"
      },
      {
        "user": "GatGit12",
        "body": "> @GatGit12,\r\n> Since similar issue is already being tracking in issue [#36465](https://github.com/tensorflow/tensorflow/issues/36465), to avoid duplicates can you please close this and subscribe/follow that issue? Thanks!\r\n\r\nBut this issue (#36465) is marked as a bug, with no attention and no explicit feature request, which is why i formally opened this post here as a feature request.\r\n\r\nAlso there are several issues with the gpu ram clearning which are simply ignored...\r\n\r\nHere is a selection (without guarantee of completeness): #39535, #19571, #15880, #20387"
      },
      {
        "user": "sanjoy",
        "body": "Hi,\r\n\r\nWe expect this to be a non-issue once we're using the CUDA malloc async allocator by default.  Can you give it a try?  You can enable it by adding `TF_GPU_ALLOCATOR=cuda_malloc_async` to the environment."
      }
    ]
  },
  {
    "issue_number": 95235,
    "title": "Some sorting related ops produce results inconsistent with NumPy when tensor contains NaN",
    "author": "rookieLiu2018",
    "state": "open",
    "created_at": "2025-06-12T06:51:28Z",
    "updated_at": "2025-06-17T14:08:44Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "2.17"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIn 2.17.0 several sorting and indexing ops yield outputs that diverge from NumPy’s behavior on a tensor containing NaN. \nWhen run in nightly version, CPU and GPU implementations often produce different results.\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\nprint(\"TensorFlow version:\", tf.__version__)\n\n# Prepare test tensor with NaN\nx = tf.constant([1.0, float('nan'), 3.0], dtype=tf.float32)\nx_n = np.array([1.0, np.nan, 3.0], dtype=np.float32)\nlookup = np.array([0.0, np.nan, 2.0], dtype=np.float32)\n\n# 1. tf.sort\nwith tf.device('/CPU:0'):\n    sorted_cpu = tf.sort(x)\nwith tf.device('/GPU:0'):\n    sorted_gpu = tf.sort(x)\nnumpy_sorted = np.sort(x_n)\nprint(\"NumPy sort:          \", numpy_sorted)\nprint(\"CPU sorted result:   \", sorted_cpu.numpy())\nprint(\"GPU sorted result:   \", sorted_gpu.numpy(), \"\\n\")\n\n# 2. tf.argsort\nwith tf.device('/CPU:0'):\n    argsort_cpu = tf.argsort(x)\nwith tf.device('/GPU:0'):\n    argsort_gpu = tf.argsort(x)\nnumpy_argsort = np.argsort(x_n)\nprint(\"NumPy argsort:       \", numpy_argsort)\nprint(\"CPU argsort indices: \", argsort_cpu.numpy())\nprint(\"GPU argsort indices: \", argsort_gpu.numpy(), \"\\n\")\n\n# 3. tf.math.top_k\nk = 3\nwith tf.device('/CPU:0'):\n    topk_cpu = tf.math.top_k(x, k=k, sorted=True)\nwith tf.device('/GPU:0'):\n    topk_gpu = tf.math.top_k(x, k=k, sorted=True)\n# NumPy equivalent for top-k: sort descending and take first k\nnumpy_topk_values = np.sort(x_n)[::-1][:k]\nnumpy_topk_indices = np.argsort(x_n)[::-1][:k]\nprint(\"NumPy top_k values:  \", numpy_topk_values)\nprint(\"NumPy top_k indices: \", numpy_topk_indices)\nprint(\"CPU top_k values:    \", topk_cpu.values.numpy())\nprint(\"CPU top_k indices:   \", topk_cpu.indices.numpy())\nprint(\"GPU top_k values:    \", topk_gpu.values.numpy())\nprint(\"GPU top_k indices:   \", topk_gpu.indices.numpy(), \"\\n\")\n\n# 4. tf.searchsorted\nwith tf.device('/CPU:0'):\n    ss_cpu = tf.searchsorted(sorted_cpu, lookup)\nwith tf.device('/GPU:0'):\n    ss_gpu = tf.searchsorted(sorted_gpu, lookup)\nnumpy_searchsorted = np.searchsorted(numpy_sorted, lookup)\nprint(\"NumPy searchsorted:  \", numpy_searchsorted)\nprint(\"CPU searchsorted:    \", ss_cpu.numpy())\nprint(\"GPU searchsorted:    \", ss_gpu.numpy())\n```\n\n### Relevant log output\n\n```shell\n2.17:\nTensorFlow version: 2.17.0\nNumPy sort:           [ 1.  3. nan]\nCPU sorted result:    [ 1. nan  3.]\nGPU sorted result:    [ 1. nan  3.] \n\nNumPy argsort:        [0 2 1]\nCPU argsort indices:  [0 1 2]\nGPU argsort indices:  [0 1 2] \n\nNumPy top_k values:   [nan  3.  1.]\nNumPy top_k indices:  [1 2 0]\nCPU top_k values:     [ 3.  1. nan]\nCPU top_k indices:    [2 0 1]\nGPU top_k values:     [ 3.  1. nan]\nGPU top_k indices:    [2 0 1] \n\nNumPy searchsorted:   [0 2 1]\nCPU searchsorted:     [0 0 1]\nGPU searchsorted:     [0 0 1]\n\n\nnighly:\nTensorFlow version: 2.20.0-dev20250604\nNumPy sort:           [ 1.  3. nan]\nCPU sorted result:    [ 1. nan  3.]\nGPU sorted result:    [nan  1.  3.] \n\nNumPy argsort:        [0 2 1]\nCPU argsort indices:  [0 1 2]\nGPU argsort indices:  [1 0 2] \n\nNumPy top_k values:   [nan  3.  1.]\nNumPy top_k indices:  [1 2 0]\nCPU top_k values:     [ 3.  1. nan]\nCPU top_k indices:    [2 0 1]\nGPU top_k values:     [nan  3.  1.]\nGPU top_k indices:    [1 2 0] \n\nNumPy searchsorted:   [0 2 1]\nCPU searchsorted:     [0 0 1]\nGPU searchsorted:     [0 0 2]\n```",
    "comments": [
      {
        "user": "sylvester-francis",
        "body": "Hi! I'd like to work on this issue as my first contribution. I’ll reproduce the bug, investigate the root cause, and propose a fix that aligns TensorFlow’s behavior with NumPy.\n"
      },
      {
        "user": "YashSachdeva",
        "body": "Hello,\n\nI have thoroughly analyzed the issue regarding inconsistent NaN handling in TensorFlow's sorting and indexing operations across CPU and GPU. I have implemented a comprehensive fix that ensures consistent NaN-aware comparisons in the core kernel implementations of top_k, sort, argsort, and searchsorted ops, aligning their behavior with NumPy.\n\nAdditionally, I have created extensive tests covering these operations with NaN values to verify correctness and consistency across devices.\n\nI would like to contribute this fix and the associated tests to the TensorFlow codebase. Please assign this contribution to me so I can submit a pull request for review.\n\nThank you for your consideration. I look forward to your response.\n\nBest regards,\n\n@YashSachdeva"
      },
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue on using tensorflow version 2.17.0 and 2.19.0 on both CPU and GPU. Here i am providing [gist](https://colab.sandbox.google.com/gist/Venkat6871/f7047f8bdb361fbc25b7d49516c2295e/95235_tf-_2-17-0-2-19-0-v.ipynb) here for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 95274,
    "title": "Deeplabcut issue",
    "author": "jk184-droid",
    "state": "closed",
    "created_at": "2025-06-12T15:24:26Z",
    "updated_at": "2025-06-17T14:06:37Z",
    "labels": [
      "stat:awaiting response",
      "type:build/install",
      "subtype:windows",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nWindows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n12.9\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nExpected Deeplabcut to load the Tensorflow-based project and use pytorch in version 3.0\n\n\n\n\n\nTraceback (most recent call last):\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\deeplabcut\\gui\\window.py\", line 531, in _open_project\n    self._update_project_state(\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\deeplabcut\\gui\\window.py\", line 502, in _update_project_state\n    self.add_tabs()\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\deeplabcut\\gui\\window.py\", line 589, in add_tabs\n    self.train_network = TrainNetwork(\n                         ^^^^^^^^^^^^^\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\deeplabcut\\gui\\tabs\\train_network.py\", line 53, in __init__\n    self._shuffle_display = SelectedShuffleDisplay(self.root)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\deeplabcut\\gui\\displays\\selected_shuffle_display.py\", line 45, in __init__\n    self._update_display(self.root.shuffle_value)\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\deeplabcut\\gui\\displays\\selected_shuffle_display.py\", line 67, in _update_display\n    pose_cfg_path = Path(self.root.pose_cfg_path)\n                         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\deeplabcut\\gui\\window.py\", line 235, in pose_cfg_path\n    compat.return_train_network_path(\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\deeplabcut\\compat.py\", line 300, in return_train_network_path\n    from deeplabcut.pose_estimation_tensorflow import return_train_network_path\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\__init__.py\", line 16, in <module>\n    import tensorflow as tf\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\tensorflow\\__init__.py\", line 40, in <module>\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 88, in <module>\n    raise ImportError(\nImportError: Traceback (most recent call last):\n  File \"C:\\Users\\pmarm\\anaconda3\\envs\\deeplabcut3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\n\n\n\n\n![Image](https://github.com/user-attachments/assets/98b34400-419b-4b4b-b253-7f931047bc41)\n\n### Standalone code to reproduce the issue\n\n```shell\n.\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "oren40",
        "body": "Had the same issue with TensorFlow 2.19 and CUDA 12.9.  \nResolved it by creating a clean conda environment with TensorFlow 2.14 and CUDA 11.8:  \n\n```bash\nconda create -n dlcfix python=3.10\nconda activate dlcfix\npip install tensorflow==2.14\n"
      },
      {
        "user": "Venkat6871",
        "body": "Hi @jk184-droid ,\nApologies for the delay, and thank you for raising your concern.\nAs per the official TensorFlow [documentation](https://www.tensorflow.org/install/source_windows#gpu_support):\n`Note: GPU support on native Windows is only available for TensorFlow 2.10 or earlier.\nStarting with TensorFlow 2.11, official CUDA builds are no longer supported on native Windows.`\n\nTo use TensorFlow with GPU on Windows, you have two options:\n\nUse WSL2 (Windows Subsystem for Linux) to install TensorFlow with GPU support.\n\nUse tensorflow-cpu along with the TensorFlow-DirectML-Plugin, which enables GPU acceleration through DirectML.\n\nAdditionally, version compatibility between TensorFlow, CUDA, and cuDNN is critical. Please verify that the versions you are using are officially supported.\nFor your reference, here is the [official compatibility guide](https://www.tensorflow.org/install/source#gpu).\n\nThank you!"
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F95274\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F95274\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 95413,
    "title": "i am storing jupyter notebook in e disk but have tensorflow in other disks",
    "author": "noir4201",
    "state": "closed",
    "created_at": "2025-06-15T08:21:07Z",
    "updated_at": "2025-06-17T14:06:16Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "subtype:windows",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMicrosoft\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nImportError                               Traceback (most recent call last)\nFile ~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\n     72 try:\n---> 73   from tensorflow.python._pywrap_tensorflow_internal import *\n     74 # This try catch logic is because there is no bazel equivalent for py_extension.\n     75 # Externally in opensource we must enable exceptions to load the shared object\n     76 # by exposing the PyInit symbols with pybind. This error will only be\n     77 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\n     78 \n     79 # This logic is used in other internal projects using py_extension.\n\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\nDuring handling of the above exception, another exception occurred:\n\nImportError                               Traceback (most recent call last)\nCell In[2], line 1\n----> 1 import tensorflow as tf\n      2 print(tf.__version__)\n      3 print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n\nFile ~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\__init__.py:40\n     37 _os.environ.setdefault(\"ENABLE_RUNTIME_UPTIME_TELEMETRY\", \"1\")\n     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\n---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\n     41 from tensorflow.python.tools import module_util as _module_util\n     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader\n\nFile ~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\n     86     sys.setdlopenflags(_default_dlopen_flags)\n     87 except ImportError:\n---> 88   raise ImportError(\n     89       f'{traceback.format_exc()}'\n     90       f'\\n\\nFailed to load the native TensorFlow runtime.\\n'\n     91       f'See https://www.tensorflow.org/install/errors '\n     92       f'for some common causes and solutions.\\n'\n     93       f'If you need help, create an issue '\n     94       f'at https://github.com/tensorflow/tensorflow/issues '\n     95       f'and include the entire stack trace above this error message.')\n\nImportError: Traceback (most recent call last):\n  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @noir4201 ,\nCould you please provide the TensorFlow version and the compatible packages you are trying to install? There are at least three possible causes for the issue:\nMSVC 2019 redistributable is not installed.\nYour CPU does not support AVX2 instructions.\nYour CPU or Python installation is 32-bit.\nA required library is either missing or located in a different path, and thus cannot be loaded.\nAdditionally, please share your environment details and the steps you followed to install TensorFlow.\nAlso, it appears this issue is a duplicate of https://github.com/tensorflow/tensorflow/issues/91656\nThank you!"
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F95413\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F95413\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 95417,
    "title": "building //tensorflow/lite/ios:TensorFlowLiteC_xcframework results in broken header files",
    "author": "jules15",
    "state": "open",
    "created_at": "2025-06-15T20:09:23Z",
    "updated_at": "2025-06-17T12:36:11Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:build/install",
      "comp:lite",
      "subtype:macOS"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\nhash 84dd28bbc29d75e6a6d917eb2998e4e8ea90ec56\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nmacos 15.5\n\n### Mobile device\n\niOS 18\n\n### Python version\n\n3.13.2\n\n### Bazel version\n\n7.4.1\n\n### GCC/compiler version\n\n?\n\n### CUDA/cuDNN version\n\nn/a\n\n### GPU model and memory\n\nApple M2 Pro\n\n### Current behavior?\n\nI build TensorFlowLiteC for iOS:\n\n`% bazel build -c opt --config=ios //tensorflow/lite/ios:TensorFlowLiteC_xcframework --define tflite_with_xnnpack=true --define tflite_with_xnnpack_qs8=true --define tflite_with_xnnpack_qu8=true`\n\nIt builds fine.\n\nI unzip the build artifact:\n\n```\n% cd bazel-bin/tensorflow/lite/ios\n% unzip TensorFlowLiteC_xcframework.zip\n```\n\nI navigate to the headers:\n\n```\n% cd TensorFlowLiteC.xcframework/ios-arm64_x86_64-simulator/TensorFlowLiteC.framework/Headers\n```\n\nI inspect the `c_api.h` header:\n\n```\n% cat -n c_api.h \n...\n    28  #include \"builtin_ops.h\"\n    29  #include \"types.h\"\n    30  #include \"c_api_types.h\"  // IWYU pragma: export\n    31  #include \"operator.h\"  // IWYU pragma: export\n...\n```\n\nSee line 29. There is no `types.h` header:\n\n```\n% pwd\n<snip>/tensorflow/bazel-bin/tensorflow/lite/ios/TensorFlowLiteC.xcframework/ios-arm64_x86_64-simulator/TensorFlowLiteC.framework/Headers\n% ls *h\nbuiltin_ops.h*        c_api_experimental.h* c_api_types.h*        c_api.h*              common.h*             profiler.h*           telemetry_setting.h*  TensorFlowLiteC.h*    xnnpack_delegate.h*\n```\n\n\n\n### Standalone code to reproduce the issue\n\n```shell\nThis is easy to reproduce:\n1. Build the framework as described above.\n2. Using XCode, create a Swift iOS app project.\n3. Add `TensorFlowLiteC.framework` as a dependency to your main target.\n4. Add a `foo.cpp` file your target to create a create a mixed Swift/C++ target. When XCode prompts you to add a bridging header, do so.\n5. In `foo.cpp`, add an `#include <TensorFlowLiteC/c_api.h>`.\n6. Build.\n7. You will see the error `/Users/<snip>/Library/Developer/Xcode/DerivedData/test-erdzaqurjgfrrbarneayzpeywmra/Build/Products/Debug-iphonesimulator/TensorFlowLiteC.framework/Headers/c_api.h:29:10: error: 'types.h' file not found`\n```\n\n### Relevant log output\n\n```shell\n<module-includes>:1:9: note: in file included from <module-includes>:1:\n#import \"Headers/TensorFlowLiteC.h\"\n        ^\n/Users/<snip>/Library/Developer/Xcode/DerivedData/test-erdzaqurjgfrrbarneayzpeywmra/Build/Products/Debug-iphonesimulator/TensorFlowLiteC.framework/Headers/TensorFlowLiteC.h:2:9: note: in file included from /Users/<snip>/Library/Developer/Xcode/DerivedData/test-erdzaqurjgfrrbarneayzpeywmra/Build/Products/Debug-iphonesimulator/TensorFlowLiteC.framework/Headers/TensorFlowLiteC.h:2:\n#import <TensorFlowLiteC/c_api.h>\n        ^\n/Users/<snip>/Library/Developer/Xcode/DerivedData/test-erdzaqurjgfrrbarneayzpeywmra/Build/Products/Debug-iphonesimulator/TensorFlowLiteC.framework/Headers/c_api.h:29:10: error: 'types.h' file not found\n#include \"types.h\"\n         ^\n/Users/<snip>/Library/Developer/Xcode/DerivedData/test-erdzaqurjgfrrbarneayzpeywmra/Build/Products/Debug-iphonesimulator/TensorFlowLite.framework/Modules/TensorFlowLite.swiftmodule/arm64.swiftinterface:8:8: error: could not build Objective-C module 'TensorFlowLiteC'\nimport TensorFlowLiteC\n       ^\n/Users/<snip>/Desktop/Development/testTarget/test/DependencyInjection.swift:10:8: error: failed to build module 'TensorFlowLite' for importation due to the errors above; the textual interface may be broken by project issues or a compiler bug\nimport TensorFlowLite\n```",
    "comments": [
      {
        "user": "AumPatel1",
        "body": "Hello guys , can i take any of the following issues or you both are working on all ?"
      },
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @jules15 \nThank you for bringing this issue to our attention, I am able to reproduce the same behavior from my end and this confirms `c_api.h` is still trying to include a file `types.h` that isn't present in the framework's headers which is causing the issue so we'll have to investigate this issue from our end so will update you soon \n\n**For reference here is output log :**\n```\ngaikwadrahul-macbookpro2:ios gaikwadrahul$ cd TensorFlowLiteC.xcframework/ios-arm64_x86_64-simulator/TensorFlowLiteC.framework/Headers\ngaikwadrahul-macbookpro2:Headers gaikwadrahul$ ls\nbuiltin_ops.h\t\tc_api_types.h\t\tcommon.h\t\ttelemetry_setting.h\txnnpack_delegate.h\nc_api_experimental.h\tc_api.h\t\t\tprofiler.h\t\tTensorFlowLiteC.h\ngaikwadrahul-macbookpro2:Headers gaikwadrahul$ ls *h\nbuiltin_ops.h\t\tc_api_types.h\t\tcommon.h\t\ttelemetry_setting.h\txnnpack_delegate.h\nc_api_experimental.h\tc_api.h\t\t\tprofiler.h\t\tTensorFlowLiteC.h\ngaikwadrahul-macbookpro2:Headers gaikwadrahul$ cat -n c_api.h | grep \"types.h\"\n    29\t#include \"types.h\"\n    30\t#include \"c_api_types.h\"  // IWYU pragma: export\n   106\t// Opaque types used by the C API.  (See also c_api_types.h.)\n   256\t/// `TfLiteOpaqueDelegate` in tensorflow/lite/core/c/c_api_types.h.)\ngaikwadrahul-macbookpro2:Headers gaikwadrahul$ \n```\n**EDIT :** As temporary workaround if you want you can copy that file manually from this location `/Users/gaikwadrahul/tensorflow/tensorflow/lite/core/async/c/types.h .` to `TensorFlowLiteC.xcframework/ios-arm64_x86_64-simulator/TensorFlowLiteC.framework/Headers` and it should work but yeah it's not ideal solution but we'll fix this issue soon it should happen automated way.\n\nThank you for your cooperation and understanding"
      }
    ]
  },
  {
    "issue_number": 95489,
    "title": "YoloX different Model Output for Python and Android",
    "author": "RNoahG",
    "state": "open",
    "created_at": "2025-06-17T00:16:42Z",
    "updated_at": "2025-06-17T04:06:03Z",
    "labels": [
      "type:bug",
      "comp:lite",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf=2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Mint 22\n\n### Mobile device\n\nSamsung Galaxy A51\n\n### Python version\n\n3.11.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nHello,\n\nI am currently trying to get YoloX working for object detection in a mobile app, while the model inference in Python produces the desired and expected output, Android on the other hand produces nonsense for the same model and test image. I asked for help on [Stackoverflow ](https://stackoverflow.com/questions/79650499/model-ouput-of-androidkotlin-tflite-model-not-matching-python-output-for-same) to no avail, and came to the conclusion that this discrepancy might be caused by a bug.  The input float array representation on Android is the same as the raveled Numpy array input of the Python version, while the outputs produced are different in every way. I have excluded the usual endianness problems, due to TensorImage being used which handles this automatically.  Also similar code does work for both RT-detr and Yolov11/v8, which indicates there might be something in the model behaving wrong for Android specifically.\n\nThanks in Advance\n\n\nThe entire code for both Android and Python can be found in this [repository](https://github.com/RNoahG/YoloXPythonAndroid).\nFor Android the relevant code is in the MainActivity.kt and YoloDetector.kt, there is some test code commented out.\n\n### Standalone code to reproduce the issue\n\n```shell\n\nHere is an abridged version of the Android input pipeline:\n\n\nval imgStream = assets.open(\"TestImages/$imagePath\")\nval decode = BitmapFactory.decodeStream(imgStream)\nval imgmat = Mat()\nUtils.bitmapToMat(decode,imgmat)\nval imgmat3 = Mat()\nImgproc.cvtColor(imgmat,imgmat3,Imgproc.COLOR_RGBA2BGR)\nval resizedmat =  Mat()\nval paddedmat = Mat()\n\nval size = Size((1920F*ratio).toDouble(),(1080F*ratio).toDouble())\nval scalar = Scalar(114.0,114.0,114.0)\nImgproc.resize(imgmat3,resizedmat,size, 0.0, 0.0,INTER_LINEAR)\nCore.copyMakeBorder(resizedmat,paddedmat,0,(imsize- (1080*ratio)).toInt(),0,0,Core.BORDER_CONSTANT,scalar)                           \nval bitmap = createBitmap(paddedmat.cols(),paddedmat.width(),Bitmap.Config.ARGB_8888)\nImgproc.cvtColor(paddedmat,argbmat,Imgproc.COLOR_RGB2RGBA)\nUtils.matToBitmap(argbmat,bitmap)\n\nval image = TensorImage(DataType.UINT8)\nimage.load(bitmap)\nval tensorproc = ImageProcessor.Builder().add(CastOp(INPUT_IMAGE_TYPE)).build()\nval proctensor = tensorproc.process(image)\nval imageBuffer = proctensor.buffer\nval output = TensorBuffer.createFixedSize(intArrayOf(numChannel, numElements), OUTPUT_IMAGE_TYPE)\ninterpreter.run(imageBuffer, output.buffer)\n \n\nAbriged Version of Python code:\n\n\nif len(img.shape) == 3:\n        padded_img = np.ones((input_size[0], input_size[1], 3), dtype=np.uint8) * 114\n    else:\n        padded_img = np.ones(input_size, dtype=np.uint8) * 114\n\nr = min(input_size[0] / img.shape[0], input_size[1] / img.shape[1])\nresized_img = cv2.resize(img,(int(img.shape[1] * r), int(img.shape[0] * r)),\n                              interpolation=cv2.INTER_LINEAR,).astype(np.uint8)\n\npadded_img[: int(img.shape[0] * r), : int(img.shape[1] * r)] = resized_img\npadded_img = np.ascontiguousarray(padded_img, dtype=np.float32)\n\ninterpreter = tf.lite.Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\ninterpreter.set_tensor(input_details[0]['index'], img[None, :, :, :])\ninterpreter.invoke()\noutput = interpreter.get_tensor(output_details[0]['index'])\noutput = np.squeeze(output)\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": []
  },
  {
    "issue_number": 60534,
    "title": "Pybind11 Exception",
    "author": "yahyanik",
    "state": "open",
    "created_at": "2023-05-08T15:21:53Z",
    "updated_at": "2025-06-17T02:14:51Z",
    "labels": [
      "stat:awaiting response",
      "type:build/install",
      "type:support",
      "stale",
      "subtype:windows",
      "TF 2.12"
    ],
    "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nSupport\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.9, 2.7, 2.6, 2.5, 2.4, 2.3\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nWindows 10\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.7, 3.8\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n10.1, 11.2 and coresponding CuDNN\r\n\r\n### GPU model and memory\r\n\r\n3090, 1650\r\n\r\n### Current Behaviour?\r\n\r\nRunning TensorFlow custom code or sample code provided the TensorFlow website creates exceptions when looking at the dump file from C++ side. \r\n\r\nI used procdump.exe to see the exceptions in Windows 10 as follows: \r\nOpen a separate CMD.exe and run: ``` procdump -e 2 -l -f \"\" <PID of the process python running tensorflow code>```\r\n\r\nsample code used:\r\n\r\n```\r\nfrom time import sleep\r\nimport tensorflow as tf\r\n\r\ndef fn_raw(inputs):\r\n      return inputs*2\r\n\r\nwhile True:\r\n  fn = tf.function(fn_raw)\r\n  r = fn(2)\r\n  print(r)\r\n  sleep(1)\r\n```\r\n\r\nsample code tested from the TensorFLow website is located at https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb\r\n\r\nexceptions are seen at inference time (after the training period)\r\n\r\nhundreds of exceptions, as follows, are dumped:\r\n\r\n``` Exception: [E06D7363.?AVerror_already_set@pybind11@@]```\r\n\r\nI tested TnesorFlow GPU and CPU with several versions of TensorFlow 2.x\r\nAccording to Pybind11 documentation page, https://pybind11.readthedocs.io/en/stable/advanced/exceptions.html, this shows an issue with the Python code which is captured on C++ side.\r\n\r\nPS: The code runs with no issues and completes the task. However, these exceptions are concerning.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nfrom time import sleep\r\nimport tensorflow as tf\r\n\r\ndef fn_raw(inputs):\r\n      return inputs*2\r\n\r\nwhile True:\r\n  fn = tf.function(fn_raw)\r\n  r = fn(2)\r\n  print(r)\r\n  sleep(1)\r\n```\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nException: [E06D7363.?AVerror_already_set@pybind11@@]```\r\n```\r\n</details>",
    "comments": [
      {
        "user": "sachinprasadhs",
        "body": "Hi, Could you please test it with the latest Tensorflow version and with the below configuration.\r\n<h4 id=\"gpu\" data-text=\"GPU\" role=\"presentation\" style=\"box-sizing: inherit; margin: 32px 0px 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-variant-numeric: ; font-variant-east-asian: ; font-variant-alternates: ; font-weight: ; font-stretch: ; font-size: 16px; line-height: ; font-family: Roboto, &quot;Noto Sans&quot;, &quot;Noto Sans JP&quot;, &quot;Noto Sans KR&quot;, &quot;Noto Naskh Arabic&quot;, &quot;Noto Sans Thai&quot;, &quot;Noto Sans Hebrew&quot;, &quot;Noto Sans Bengali&quot;, sans-serif; font-optical-sizing: ; font-kerning: ; font-feature-settings: ; font-variation-settings: ; letter-spacing: normal; overflow: hidden; text-overflow: ellipsis; margin-inline-end: -40px; padding-inline-end: 40px; color: rgb(32, 33, 36); orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\"><span class=\"devsite-heading\" role=\"heading\" aria-level=\"4\" style=\"box-sizing: inherit;\">GPU</span><button type=\"button\" class=\"devsite-heading-link button-flat material-icons\" aria-label=\"Copy link to this section: GPU\" data-title=\"Copy link to this section: GPU\" data-id=\"gpu\" style=\"box-sizing: border-box; appearance: none; background: 0px center; border: 0px; border-radius: var(--devsite-button-border-radius,2px); box-shadow: none; color: var(--devsite-icon-color,var(--devsite-secondary-text-color)); cursor: pointer; display: inline-block; font-style: normal; font-variant-ligatures: ; font-variant-caps: ; font-variant-numeric: ; font-variant-east-asian: ; font-variant-alternates: ; font-weight: normal; font-stretch: ; font-size: 24px; font-family: &quot;Material Icons&quot;; font-optical-sizing: ; font-kerning: ; font-feature-settings: &quot;liga&quot;; font-variation-settings: ; height: 24px; letter-spacing: normal; line-height: 1; margin: var(--devsite-button-margin,0); margin-inline-end: var(--devsite-button-margin-x-end); max-width: var(--devsite-button-max-width,none); min-width: 36px; outline: 0px; overflow: hidden; padding: 0px 8px; text-align: center; text-decoration: none; text-overflow: ellipsis; text-transform: none; transition: background-color 0.2s ease 0s, border 0.2s ease 0s, box-shadow 0.2s ease 0s; vertical-align: bottom; white-space: nowrap; width: var(--devsite-button-width,auto); overflow-wrap: normal; direction: ltr; -webkit-font-smoothing: antialiased; opacity: 0;\"></button></h4><div class=\"devsite-table-wrapper\" style=\"box-sizing: inherit; margin: var(--devsite-table-margin,16px 0); padding: 0px; overflow: auto; color: rgb(32, 33, 36); font-family: Roboto, &quot;Noto Sans&quot;, &quot;Noto Sans JP&quot;, &quot;Noto Sans KR&quot;, &quot;Noto Naskh Arabic&quot;, &quot;Noto Sans Thai&quot;, &quot;Noto Sans Hebrew&quot;, &quot;Noto Sans Bengali&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;\">\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow-2.12.0 | 3.8-3.11 | GCC 9.3.1 | Bazel 5.3.0 | 8.6 | 11.8\r\n\r\n</div>\r\n\r\nLatest Tensorflow supports c++ 17 version, make sure your test is inline with the supported versions.\r\nLet us know the outcome after your test with the latest version. Thanks!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "yahyanik",
        "body": "Hi, @sachinprasadhs. thanks for the reply. I have tried TensorFlow 2.12 with Python 3.8. and Python 3.9.\r\n\r\nAs I mentioned initially, there is no need for GPU processing (although the same can be seen there). I am not using C++ code and so there is no compiler or Build tools in my environment. In case Tensorflow requires packages it may have installed them. The command I used for installing TensorFlow 2.12 is ```pip install tensorflow```. After core dump with ```procdump -e 2 -l -f \"\" <PID of the process python running tensorflow code>``` I still see that pybind11 is generating many exceptions."
      }
    ]
  },
  {
    "issue_number": 94863,
    "title": "Segmentation fault in tf.sets.size",
    "author": "cx104906",
    "state": "open",
    "created_at": "2025-06-05T12:53:49Z",
    "updated_at": "2025-06-17T02:14:38Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 18.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n6.5.0\n\n### GCC/compiler version\n\nclang 18.1.8\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSegfault when fuzzing with tf.sets.size\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nprint(tf.__version__)\n\nmylist = [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[['']]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\nmydict = {}\n\nfor i in range(10):\n  print(f\"{i}\")\n  try:\n    tf.sets.size(*mylist,**mydict)\n  except Exception as e:\n    print(f\"{e}\")\nprint(\"done\")\n```\n\n### Relevant log output\n\n```shell\nI'm not very good at triaging whether this is a real security issue. When compiling with ASan, it gets stuck at the final step of linking libtensorflow.so and makes no progress.\n\nGDB says\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\n0x00007ffff44ee114 in tensorflow::TF_TensorToPyArray(std::unique_ptr<TF_Tensor, tensorflow::detail::TFTensorDeleter>, _object**) ()\n   from /root/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so\n(gdb) bt\n#0  0x00007ffff44ee114 in tensorflow::TF_TensorToPyArray(std::unique_ptr<TF_Tensor, tensorflow::detail::TFTensorDeleter>, _object**) ()\n   from /root/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so\n#1  0x00007ffff44eca89 in tensorflow::TF_TensorToMaybeAliasedPyArray(std::unique_ptr<TF_Tensor, tensorflow::detail::TFTensorDeleter>, _object**) ()\n   from /root/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so\n#2  0x00007ffff447d89a in tensorflow::TFE_TensorHandleToNumpy(TFE_TensorHandle*, TSL_Status*) ()\n   from /root/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so\n#3  0x00007ffff44847df in EagerTensor_numpy_internal(EagerTensor*) ()\n   from /root/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so\n#4  0x0000000000508f61 in cfunction_vectorcall_NOARGS (func=0x7fff7ca38bd0, args=<optimized out>, nargsf=<optimized out>, kwnames=<optimized out>)\n    at /usr/local/src/conda/python-3.11.11/Include/cpython/methodobject.h:52\n#5  0x000000000051ea31 in _PyObject_VectorcallTstate (kwnames=<optimized out>, \n    kwnames@entry=<error reading variable: dwarf2_find_location_expression: Corrupted DWARF expression.>, nargsf=<optimized out>, \n    nargsf@entry=<error reading variable: dwarf2_find_location_expression: Corrupted DWARF expression.>, args=<optimized out>, \n    args@entry=<error reading variable: dwarf2_find_location_expression: Corrupted DWARF expression.>, callable=0x7fff7ca38bd0, \n    callable@entry=<error reading variable: dwarf2_find_location_expression: Corrupted DWARF expression.>, tstate=0x8a7a38 <_PyRuntime+166328>, \n    tstate@entry=<error reading variable: dwarf2_find_location_expression: Corrupted DWARF expression.>)\n    at /usr/local/src/conda/python-3.11.11/Include/internal/pycore_call.h:77\n#6  PyObject_Vectorcall (callable=0x7fff7ca38bd0, args=<optimized out>, nargsf=<optimized out>, kwnames=<optimized out>)\n    at /usr/local/src/conda/python-3.11.11/Objects/call.c:299\n......\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @cx104906 ,\nApologies for the delayed response, and thank you for raising this issue.\nThe problem appears to stem from the use of a deeply nested input structure. Is there a specific reason for structuring the input this way? If this is for fuzzing purposes, as [mentioned](https://github.com/tensorflow/tensorflow/issues/93612#issuecomment-2902005057) by one of our engineers in a similar context, we kindly ask that fuzzing reports be triaged before filing them as bugs. In this particular case, it looks like the issue is due to an out-of-memory (OOM) condition rather than a security vulnerability.\n\nTo help us investigate further, please compile TensorFlow with AddressSanitizer (ASAN) enabled and share the full stack trace. A simple segmentation fault message is not sufficient for diagnosis an ASAN report and backtrace would provide more meaningful insights, especially to evaluate any potential security implications.\n\nI also tested a simplified version of your code with reduced nesting depth, and it executed without error. I am attaching a [Gist](https://colab.sandbox.google.com/gist/Venkat6871/c466220450bd6d02f86cf0fc8e9364e1/94863_tf_2-19-0-nightly-v.ipynb) for your reference. Please take a look and let us know if it aligns with your use case or if you are still facing issues.\n\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      }
    ]
  },
  {
    "issue_number": 91656,
    "title": "TensorFlow DLL failed to load with newer version of TF",
    "author": "Aditya2413",
    "state": "open",
    "created_at": "2025-04-17T09:53:10Z",
    "updated_at": "2025-06-17T00:46:26Z",
    "labels": [
      "type:build/install",
      "subtype:windows",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nWindows 11 64-bit\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nImportError: Traceback (most recent call last):\n  File \"C:\\Users\\adity\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n### Standalone code to reproduce the issue\n\n```shell\nNA\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @Aditya2413 ,\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\n\nYou need to install the MSVC 2019 redistributable\nYour CPU does not support AVX2 instructions\nYour CPU/Python is on 32 bits\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\nAlso kindly provide the environment details and the steps followed to install the tensorflow.\nhttps://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584\nThank you!"
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F91656\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F91656\">No</a>\n"
      },
      {
        "user": "Aditya2413",
        "body": "@Venkat6871 Thanks for replying! \nTensorflow working fine day before failing. My device has 64-bit architecture. I got error while executing code -\nImportError: Traceback (most recent call last):\n  File \"C:\\Users\\adity\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\nso I uninstall current tensorflow lib and install it again but error was still there, then I installed tensorflow version 2.16.2. it worked.\nwhen i install tensorfloe_hub it upgrade the tensrflow and got dependencies issue."
      }
    ]
  },
  {
    "issue_number": 93476,
    "title": "Depth anything V2 Tflite outputs constants on qualcomm gpus",
    "author": "at-brinc",
    "state": "open",
    "created_at": "2025-05-15T22:08:24Z",
    "updated_at": "2025-06-17T00:24:25Z",
    "labels": [
      "type:performance",
      "TFLiteConverter"
    ],
    "body": "### 1. System information\n\n- Ubuntu 20.04\n- bulit grom source tensorflow and tflite 2.17.1\n- v2.17.1 tensorflow 3c92ac03cab816044f7b18a86eb86aa01a294d95\n\n### 2. Code\n\nProvide code to help us reproduce your issues using one of the following options:\ntflite: https://huggingface.co/qualcomm/Depth-Anything-V2/tree/main\n\npip3 install qai-hub\nqai-hub configure --api_token INSERT_API_TOKEN\n\nimport qai_hub as hub\nimport numpy as np\n\n\n# TFlite model path\ntflite_model_id = \"tflite_path\"\n\n# Setup input data\ninput_tensor = np.arange(np.prod((1, 518, 518, 3)), dtype=np.float32).reshape((1, 518, 518, 3))\n\n# Submit inference job\njob = hub.submit_inference_job(\n    model=tflite_model_id,\n    device=hub.Device(\"QCS8550 (Proxy)\"),\n    name=\"depth_anything_v2\",\n    inputs=dict(image=[input_tensor]),\n    options=\"--compute_unit gpu\"\n)\n\n# Load the output data into a dictionary of numpy arrays\noutput_tensors = job.download_output_data()\n\n\n### 3. Failure after conversion\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\n\nModel produces wrong results on qualcomm gpus. In qai-hub I replaced the GELU layers in the huggingface config to gelu_new and the model seems to export just fine using qai-hub. When I run it on the QRB5165 it seems to run fine on the gpu but outputs a constant image with value 0.088684. This model works fine if I run it on the cpu though. Also other models are working on the gpu if I use them. Also it seems like this model works completely fine on gpu delegate using opencl on cuda hardware on my laptop. It seems like if the model works on cpu the tflite is correct and hence how the graph is executed depends on gpu delegates.\n",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @at-brinc \nThank you for your patience. I can confirm that I am able to reproduce the behavior you reported. I've included the output log below for your reference. We will investigate this issue further and provide an update as soon as possible. We appreciate you bringing this to our attention.\n\n```\nFinal job status: SUCCESS\nJob completed successfully. Downloading output data...\ntmpqb9llttz.h5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16.4k/16.4k [00:00<00:00, 17.8MB/s]\n\nOutput tensor shape: (1, 518, 518, 1), dtype: float32\nOutput tensor min: 0.08868408203125\nOutput tensor max: 0.08868408203125\nOutput tensor mean: 0.08868408203125\nOutput tensor std dev: 0.0\n\nWARNING: Output tensor appears to be constant or near-constant!\nConstant value observed: 0.08868408203125\n\n--- JOB LOGS ---\n```\n\nThank you for your cooperation and understanding."
      },
      {
        "user": "at-brinc",
        "body": "Great, thanks. Keep me updated"
      },
      {
        "user": "at-brinc",
        "body": "Any updates here?"
      }
    ]
  },
  {
    "issue_number": 32743,
    "title": "Failed to load delegate from libedgetpu.so.1.0 with tflite_runtime 1.14",
    "author": "Namburger",
    "state": "closed",
    "created_at": "2019-09-23T21:04:26Z",
    "updated_at": "2025-06-16T23:19:38Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:lite",
      "TF 1.14"
    ],
    "body": "**System information**\r\n- Have I written code (based on the docs):\r\n```\r\nfrom tflite_runtime.interpreter import Interpreter\r\nfrom tflite_runtime.interpreter import load_delegate\r\nmodel_path='my_compiled_model.tflite'\r\ninterpreter = Interpreter(model_path,\r\n  experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n```\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n```\r\n  Operating System: Ubuntu 18.04.3 LTS\r\n            Kernel: Linux 4.15.0-60-generic\r\n      Architecture: x86-64\r\n```\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: laptop\r\n- TensorFlow installed from (source or binary):\r\n`pip3 install tflite_runtime-1.14.0-cp36-cp36m-linux_x86_64.whl`\r\n- TensorFlow version (use command below): tflite_runtime 1.14\r\n- Python version: `Python 3.6.5 :: Anaconda, Inc.`\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nThis is the code that I ran:\r\n```\r\nfrom tflite_runtime.interpreter import Interpreter\r\nfrom tflite_runtime.interpreter import load_delegate\r\nmodel_path='my_compiled_model.tflite'\r\ninterpreter = Interpreter(model_path,\r\n  experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n```\r\nfollowing this tutorial:\r\nhttps://www.tensorflow.org/lite/guide/python\r\n\r\nThis was working before, but somehow broken with this error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/nam/anaconda3/lib/python3.6/site-packages/tflite_runtime/interpreter.py\", line 165, in load_delegate\r\n    delegate = Delegate(library, options)\r\n  File \"/home/nam/anaconda3/lib/python3.6/site-packages/tflite_runtime/interpreter.py\", line 119, in __init__\r\n    raise ValueError(capture.message)\r\nValueError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"evaluate_edgetpu_cifar10.py\", line 51, in <module>\r\n    interpreter = Interpreter(file_name,experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n  File \"/home/nam/anaconda3/lib/python3.6/site-packages/tflite_runtime/interpreter.py\", line 168, in load_delegate\r\n    library, str(e)))\r\nValueError: Failed to load delegate from libedgetpu.so.1.0\r\n```\r\nI have been messing around a lot with my machine since by installing different versions of tf. But for the purpose of using the tflite_runtime.interpreter's load_delegate function, shouldn't just the pip install works? \r\nVery weird behavior :/\r\nalso I do have `libedgetpu.so.1.0` installed here:\r\n```\r\n% ls /usr/lib/x86_64-linux-gnu/libedgetpu.so.1.0\r\n/usr/lib/x86_64-linux-gnu/libedgetpu.so.1.0\r\n```\r\n\r\nThanks in advance for the help!\r\n\r\n[EDIT]\r\nI guess I'll update the issue here with a solution so that any body else can reference:\r\n`ValueError: Failed to load delegate from libedgetpu.so.1.0` really is just due to the delegate library not being able to communicate with the edgetpu. This is a very standard linux problem and has nothing to do with the tensorflow library or libedgetpu. The failures most likely stems from some type of errno from the kernel which returns as failure to the user side.\r\n\r\nSo the easiest fix is to run with sudo:\r\n```\r\n$ sudo python your_script.py\r\n```\r\n\r\nBut the most permanent fix is to add your linux user to the `plugdev` group which will allows you to access devices without sudo (this will requires a reboot after):\r\n```\r\n$ sudo usermod -aG plugdev $USER\r\n```",
    "comments": [
      {
        "user": "EdjeElectronics",
        "body": "Hi @Namburger , I had the same issue!\r\n\r\nMake sure your Coral USB Accelerator is plugged in when you run your code. If the USB Accelerator isn't plugged in when you call the 'load_delegate' function, it will result in that error. If it IS plugged in, that error won't occur."
      },
      {
        "user": "programmer290399",
        "body": "I am facing the same issue , even when the USB accelerator is plugged in and the LED in it is shining bright ........ "
      },
      {
        "user": "Namburger",
        "body": "@EdjeElectronics wow... you were correct... thanks."
      }
    ]
  },
  {
    "issue_number": 64177,
    "title": "Layers.Flatten( ) fails under TF 2.16.1 but not under TF 2.15 ",
    "author": "JuanVargas",
    "state": "closed",
    "created_at": "2024-03-21T15:39:41Z",
    "updated_at": "2025-06-16T19:10:28Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:apis",
      "TF 2.16"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nTF 2.16.1\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.4 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\nCUDA 12.4; CUDNN 8  \n\n### GPU model and memory\n\nNVIDIA GeForce RTX 3060  12 GBs\n\n### Current behavior?\n\nCode from pp 289 of Cholllet's book second ed, that uses a Flatten layer, runs OK under TF 2.15 and Keras 2.15. The exact same code fails under TF 2.16.1 when it tries to execute the line to get the history obj. For reference the code is in the section for code of this form and the error msg is also in the box of this form\r\n\r\n\r\n\r\n2024-03-21 11:16:28.427720: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1711034188.704423   18284 service.cc:145] XLA service 0x7ad818006930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\nI0000 00:00:1711034188.704465   18284 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\r\n2024-03-21 11:16:28.713034: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:580 : INVALID_ARGUMENT: only one input size may be -1, not both 0 and 1\r\n\r\nStack trace for op definition: \r\n\r\n\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\ninputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\r\nx = keras.layers.Flatten()(inputs)\r\nx = keras.layers.Dense(16, activation=\"relu\")(x)\r\noutputs = keras.layers.Dense(1)(x)\r\nmodel = keras.Model(inputs, outputs)\r\n\r\ncallbacks = [\r\n   keras.callbacks.ModelCheckpoint(\"jena_dense.keras.x.keras\",\r\n                                    save_best_only=True)\r\n]\r\n\r\nmodel.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\r\n\r\nhistory = model.fit(train_dataset,\r\n                    epochs=10,\r\n                    validation_data=val_dataset,\r\n                    callbacks=callbacks)\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-21 11:16:28.427720: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1711034188.704423   18284 service.cc:145] XLA service 0x7ad818006930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\nI0000 00:00:1711034188.704465   18284 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\r\n2024-03-21 11:16:28.713034: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:580 : INVALID_ARGUMENT: only one input size may be -1, not both 0 and 1\r\n\r\nStack trace for op definition: \r\ntack trace for op definition: \r\nFile \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\nFile \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\r\nFile \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\r\nFile \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\r\nFile \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\r\nFile \"/tmp/ipykernel_18158/437540976.py\", line 29, in wrapper\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\r\nFile \"<ipython-input-1-6a9ff972cd60>\", line 250, in <module>\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 325, in fit\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 118, in one_step_on_iterator\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 106, in one_step_on_data\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 57, in train_step\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 814, in __call__\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 48, in __call__\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/models/functional.py\", line 194, in call\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/ops/function.py\", line 151, in _run_through_graph\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/models/functional.py\", line 578, in call\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 814, in __call__\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 48, in __call__\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py\", line 54, in call\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/ops/numpy.py\", line 4507, in reshape\r\nFile \"/drv3/hm3/code/python/tf.2.16.1/.tf.2.16.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py\", line 1545, in reshape\r\n\r\n\t [[{{node functional_1_1/flatten_1/Reshape}}]]\r\n\ttf2xla conversion failed while converting __inference_one_step_on_data_19335[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\r\n2024-03-21 11:16:28.713066: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: only one input size may be -1, not both 0 and 1\n```\n",
    "comments": [
      {
        "user": "sgkouzias",
        "body": "Set the `input_shape` argument of your Dense layer equal to `(sequence_length * raw_data.shape[-1],)`"
      },
      {
        "user": "Di-Is",
        "body": "Hi @JuanVargas\r\n\r\nI also encountered a similar error while training an image classification model.\r\nBelow is a list of measures that were effective in suppressing the error in my environment:\r\n\r\n1. Use tf.ensure_shape to explicitly define the shape of tensors in the training and validation dataset.\r\n    - Example: https://github.com/tensorflow/tensorflow/issues/57485#issuecomment-1249649166\r\n2. Switch from graph execution to eager execution.\r\n3. Use a reshape layer instead of a flatten layer.\r\n    - In your case `tf.keras.Reshape((sequence_length, raw_data.shape[-1]))` or `tf.keras.Reshape((None, sequence_length, raw_data.shape[-1]))`"
      },
      {
        "user": "sushreebarsa",
        "body": "@JuanVargas \r\nIn order to expedite the trouble-shooting process, please provide a complete code snippet to reproduce the issue reported here. Thank you!"
      }
    ]
  },
  {
    "issue_number": 95468,
    "title": "PERPRES",
    "author": "Himakh",
    "state": "closed",
    "created_at": "2025-06-16T16:29:44Z",
    "updated_at": "2025-06-16T16:30:04Z",
    "labels": [
      "type:bug"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n3.10\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nmicrosoft\n\n### Mobile device\n\nmicrosoft\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTraceback (most recent call last):\n  File \"D:\\PERPRES\\venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\PERPRES\\train_model.py\", line 2, in <module>\n    import tensorflow as tf\n  File \"D:\\PERPRES\\venv\\Lib\\site-packages\\tensorflow\\__init__.py\", line 40, in <module>\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import \n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\PERPRES\\venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 88, in <module>    \n    raise ImportError(\n    ...<6 lines>...\n        f'and include the entire stack trace above this error message.')\nImportError: Traceback (most recent call last):\n  File \"D:\\PERPRES\\venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>    \n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\n### Standalone code to reproduce the issue\n\n```shell\nTraceback (most recent call last):\n  File \"D:\\PERPRES\\venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\PERPRES\\train_model.py\", line 2, in <module>\n    import tensorflow as tf\n  File \"D:\\PERPRES\\venv\\Lib\\site-packages\\tensorflow\\__init__.py\", line 40, in <module>\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import \n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\PERPRES\\venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 88, in <module>    \n    raise ImportError(\n    ...<6 lines>...\n        f'and include the entire stack trace above this error message.')\nImportError: Traceback (most recent call last):\n  File \"D:\\PERPRES\\venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>    \n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F95468\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F95468\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 58424,
    "title": "Deleting legacy Java client from TensorFlow main repository",
    "author": "karllessard",
    "state": "open",
    "created_at": "2022-11-03T01:16:18Z",
    "updated_at": "2025-06-16T15:56:01Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:apis"
    ],
    "body": "TensorFlow main repository still contains the old [Java client](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java) based on TF1.x that has been replaced a few years ago by the [new version](https://github.com/tensorflow/java) maintained by SIG-JVM.\r\n\r\nThis is very misleading for users who wants to discover the capabilities of running TensorFlow models on Java (just this week a new example of such [question](https://discuss.tensorflow.org/t/what-does-it-mean-for-the-java-api-that-warning-this-api-is-deprecated-and-will-be-removed-in-a-future-version-of-tensorflow-after-the-replacement-is-stable/12757) appeared on the forum).\r\n\r\nThis issue is to start the process of deleting this client for good in TF main repo. We could start by replacing this [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md) for simply saying that this client is deprecated and provide links to the new repo. Then we can proceed to the folder deletion, making sure it won't break any code, CI jobs or external scripts (like the documentation one).\r\n\r\nIf need be, we at SIG-JVM can take care of pushing a series of pull requests to achieve this goal. \r\n\r\nCC\\ @bhack , @craigacp",
    "comments": [
      {
        "user": "bhack",
        "body": "/cc @theadactyl "
      },
      {
        "user": "bhack",
        "body": "P.s. I suggested to open an initial Issue (this) and PR sin the last SIG-JVM meeting. \r\n\r\nThe first one is at:\r\nhttps://github.com/tensorflow/tensorflow/pull/58427"
      },
      {
        "user": "bhack",
        "body": "https://github.com/tensorflow/tensorflow/pull/58427 was merged.\r\n\r\nDo we need something else?"
      }
    ]
  },
  {
    "issue_number": 79111,
    "title": "TensorFlow Stable Delegate Python API",
    "author": "KozaMateusz",
    "state": "open",
    "created_at": "2024-10-31T07:52:45Z",
    "updated_at": "2025-06-16T07:41:33Z",
    "labels": [
      "awaiting review",
      "type:feature",
      "type:support",
      "comp:lite",
      "TF 2.16"
    ],
    "body": "### Issue type\n\nSupport\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow Python API currently doesn't support running stable delegates. Are there any workarounds? If not, are there any plans to support this in the future?\n\n### Standalone code to reproduce the issue\n\n```shell\nself._model = tf.lite.Interpreter(\r\n    model_path=stryolov5.tflite,\r\n    experimental_delegates=[delegate.so]\r\n)\n```\n\n\n### Relevant log output\n\n_No response_",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @KozAAAAA \r\n\r\nI apologize for the delayed response, As far I know currently TensorFlow python API does not support running stable delegates and If I'm not wrong we do support C++ API please refer this [official documentation](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/utils/experimental/sample_stable_delegate),  regarding stable delegate python API I'll confirm with relevant team if there is any future plan to support that and will update you here if I got any update from that relevant team\r\n\r\nThank you for your cooperation and patience."
      },
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @pkgoogle\r\nPlease take a look into this issue. Thank you."
      },
      {
        "user": "pkgoogle",
        "body": "Hi @KozAAAAA, You are correct it does not seem supported. Hi @qukhan, perhaps you know if there are any workarounds or plans to support this. Thanks."
      }
    ]
  },
  {
    "issue_number": 95415,
    "title": "tf.nn.conv2d with invalid input dims crashes in TF ≤2.19 — now raises InvalidArgumentError in nightly",
    "author": "panda123dd",
    "state": "open",
    "created_at": "2025-06-15T14:16:38Z",
    "updated_at": "2025-06-16T07:38:20Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nlinux ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn Ubuntu 22.04 with TensorFlow 2.19 (stable branch), running the following code:\n\n```python\nimport tensorflow as tf\nx = tf.random.normal([4, 10], mean=1.75, stddev=0.5)\ninitializer = tf.initializers.truncated_normal(mean=0.0, stddev=0.05)\nw = tf.Variable(initializer([10, 5]), dtype=tf.float32)\nb = tf.Variable(tf.zeros([5]), dtype=tf.float32)\ny = tf.nn.softmax(tf.matmul(x, w) + b)\ny_conv = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='VALID')\n\nresults in a crash with abort/core dumped, showing:\nCheck failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3，0，C\nAborted (core dumped)\n\nHowever, in TensorFlow 2.20.0-dev20250604 (nightly), the same code raises a catchable InvalidArgumentError:\nconvolution input must be 4-dimensional: [4,10] [Op:Conv2D] name: \n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nx = tf.random.normal([4, 10], mean=1.75, stddev=0.5)\ninitializer = tf.initializers.truncated_normal(mean=0.0, stddev=0.05)\nw = tf.Variable(initializer([10, 5]), dtype=tf.float32)\nb = tf.Variable(tf.zeros([5]), dtype=tf.float32)\ny = tf.nn.softmax((tf.matmul(x, w) + b))\ny_conv = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='VALID')\n```\n\n### Relevant log output\n\n```shell\nCheck failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3，0，C\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @panda123dd ,\nHi, thanks for raising your concern!\nI tested your code on both TensorFlow 2.19.0 and a nightly version using Google Colab. In both versions, the code does not crash, but instead correctly raises an error due to an input rank mismatch for `tf.nn.conv2d`, which expects a 4D tensor as input.\nAs expected:\n`x` has shape `[4, 10]` (i.e., it is a 2D tensor),\nBut `tf.nn.conv2d` requires a 4D input of the form `[batch, height, width, channels]`.\n\nTo fix the issue, you can reshape x into a 4D tensor. I made this change and the code worked fine for me. Here is a [Gist](https://colab.sandbox.google.com/gist/Venkat6871/554482d6c1223feeab217dfc7ea71dde/95415_tf_2-19-0-nightly-v.ipynb) for your reference.\n\n**Note**: The nightly version of TensorFlow changes frequently, so for more stable and consistent behavior, it is recommended to stick with official stable releases.\n\nThank you!"
      }
    ]
  },
  {
    "issue_number": 63495,
    "title": "tensorflow-macos still required for version 2.16.1",
    "author": "gmyrianthous",
    "state": "closed",
    "created_at": "2024-03-12T10:38:11Z",
    "updated_at": "2025-06-15T10:02:24Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:build/install",
      "subtype:macOS"
    ],
    "body": "### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.16.1\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nmacOS Sonoma 14.1.1\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nAccording to the [release notes of Tensorflow 2.16.1](https://github.com/tensorflow/tensorflow/releases/tag/v2.16.1), \r\n\r\n> Apple Silicon users: If you previously installed TensorFlow using `pip install tensorflow-macos`, please update your installation method. Use` pip install tensorflow` from now on.\r\n\r\nHowever, without installing `tensorflow-macos` I get the following error:\r\n\r\n```\r\nFatal Python error: Illegal instruction\r\n```\r\n\r\nThis error will get away after installing `tensorflow-macos`, which means Tensorflow 2.16.1 still requires the macos package to run on Apple Silicon machines. \r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\npip install tensorflow==2.16.1\r\n\r\nimport tensorflow as tf\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nFatal Python error: Illegal instruction\r\n```\r\n",
    "comments": [
      {
        "user": "iamthebot",
        "body": "Bizarely I can't repro this on OSX 14.3.1 w/ Python 3.11.x on an M1 macbook pro. The `tensorflow` package works just fine."
      },
      {
        "user": "mihaimaruseac",
        "body": "Can you try within a clean environment? It is possible you have old packages."
      },
      {
        "user": "iamthebot",
        "body": "```\r\n# create new python 3.10 conda env\r\nconda create -n tf_osx_test python==3.10.13\r\n\r\n# activate\r\nconda activate tf_osx_test\r\n\r\n# verify python version\r\npython --version\r\n> Python 3.10.13\r\n\r\n# install tensorflow via pip\r\npip install tensorflow\r\n\r\nCollecting tensorflow\r\n  Downloading ...tensorflow-2.16.1-cp310-cp310-macosx_12_0_arm64.whl (227.0 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.0/227.0 MB 21.3 MB/s eta 0:00:00\r\n...\r\nSuccessfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 certifi-2024.2.2 charset-normalizer-3.3.2 dm-tree-0.1.8 flatbuffers-24.3.7 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.10.0 idna-3.6 keras-3.0.5 libclang-16.0.6 markdown-3.5.2 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.7 numpy-1.26.4 opt-einsum-3.3.0 packaging-24.0 protobuf-4.25.3 pygments-2.17.2 requests-2.31.0 rich-13.7.1 six-1.16.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 typing-extensions-4.10.0 urllib3-2.2.1 werkzeug-3.0.1 wrapt-1.16.0\r\n\r\ntry to import it:\r\n\r\n>>> import tensorflow as tf\r\n```\r\n\r\nNo issues whatsoever. Maybe your python installation is corrupted @mihaimaruseac ? This is on a 2021 M1 MBP w/ OSX Sonoma 14.3.1."
      }
    ]
  },
  {
    "issue_number": 36181,
    "title": "AttributeError: 'Tensor' object has no attribute 'log_prob'",
    "author": "nbro",
    "state": "open",
    "created_at": "2020-01-24T13:50:40Z",
    "updated_at": "2025-06-15T09:47:30Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:ops",
      "TF 2.11"
    ],
    "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina (Version: 10.15.2 (19C57))\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7.5\r\n- GPU model and memory: Intel Iris Pro 1536 MB\r\n\r\n**Describe the current behavior**\r\n\r\nI get the error\r\n\r\n> AttributeError: 'Tensor' object has no attribute 'log_prob'\r\n\r\nwith TensorFlow Probability 0.9 (and TF 2.1).\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe following code\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\nfrom tensorflow_probability import distributions as tfd\r\n\r\n\r\ndef get_mnist_data(normalize=True):\r\n    img_rows, img_cols = 28, 28\r\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\n    if tf.keras.backend.image_data_format() == 'channels_first':\r\n        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n        input_shape = (1, img_rows, img_cols)\r\n    else:\r\n        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n        input_shape = (img_rows, img_cols, 1)\r\n\r\n    x_train = x_train.astype('float32')\r\n    x_test = x_test.astype('float32')\r\n\r\n    if normalize:\r\n        x_train /= 255\r\n        x_test /= 255\r\n\r\n    return x_train, y_train, x_test, y_test, input_shape\r\n\r\n\r\ndef get_bayesian_cnn(input_shape, num_classes=10):\r\n    model_input = tf.keras.layers.Input(shape=input_shape)\r\n\r\n    # kernel_divergence_fn=None to solve a symbolic exception.\r\n    x = tfp.layers.Convolution2DFlipout(6, kernel_size=(5, 5), padding=\"SAME\", activation=tf.nn.relu,\r\n                                        kernel_divergence_fn=None)(model_input)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    x = tfp.layers.DenseFlipout(84, activation=tf.nn.relu)(x)\r\n    x = tfp.layers.DenseFlipout(num_classes)(x)\r\n\r\n    model_output = tfp.layers.DistributionLambda(lambda t: tfd.Categorical(logits=t, validate_args=True))(x)\r\n\r\n    model = tf.keras.Model(model_input, model_output)\r\n\r\n    return model\r\n\r\n\r\ndef neg_log_likelihood(y_true, y_pred):\r\n    return -tf.reduce_mean(y_pred.log_prob(tf.cast(tf.argmax(y_true, axis=-1), tf.int32)))\r\n\r\n\r\ndef train():\r\n    x_train, y_train, x_test, y_test, input_shape = get_mnist_data()\r\n\r\n    model = get_bayesian_cnn(input_shape=input_shape)\r\n\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=neg_log_likelihood,\r\n                  metrics=[neg_log_likelihood])\r\n\r\n    model.fit(x_train, y_train, batch_size=128, epochs=1, verbose=1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    train()\r\n```\r\n\r\n**Comments**\r\n\r\nThis error seems to be due to the fact that `y_pred` is a tensor when the loss is called, while it should be a distribution. Meanwhile, I found a [question on Stack Overflow related to the third issue I mentioned above](https://stackoverflow.com/q/59743872/3924118). \r\n\r\n(_This is a duplicate issue of https://github.com/tensorflow/probability/issues/742, but, for completeness, I decided to open it here too._)",
    "comments": [
      {
        "user": "gadagashwini-zz",
        "body": "Could able to reproduce the issue with Tf 2.1.\r\nPlease take a look at the gist [here](https://colab.research.google.com/gist/gadagashwini/a64c7d6f31212ac2b3b0653989d45b32/untitled359.ipynb). Thanks!"
      },
      {
        "user": "nbro",
        "body": "See https://github.com/keras-team/keras/issues/4506."
      },
      {
        "user": "jvishnuvardhan",
        "body": "@nbro I think the error is coming from the line `model_output` while creating the model. \r\n\r\n`model_output = tfp.layers.DistributionLambda(lambda t: tfd.Categorical(logits=t[0], validate_args=True))(x)\r\n`\r\n\r\nThe shape of output layer is ((), ()) while the metric tries to access the `index -1` and throws the following error \r\n\r\n`InvalidArgumentError: slice index -1 of dimension 0 out of bounds. for 'metrics_9/neg_log_likelihood/strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <-1>, input[2] = <0>, input[3] = <1>.`\r\n\r\nPlease check the model summary below.\r\n\r\n```\r\nModel: \"model_12\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_13 (InputLayer)        [(None, 28, 28, 1)]       0         \r\n_________________________________________________________________\r\nconv2d_flipout_12 (Conv2DFli (None, 28, 28, 6)         306       \r\n_________________________________________________________________\r\nflatten_12 (Flatten)         (None, 4704)              0         \r\n_________________________________________________________________\r\ndense_flipout_24 (DenseFlipo (None, 84)                790356    \r\n_________________________________________________________________\r\ndense_flipout_25 (DenseFlipo (None, 10)                1690      \r\n_________________________________________________________________\r\ndistribution_lambda_11 (Dist ((), ())                  0         \r\n=================================================================\r\nTotal params: 792,352\r\nTrainable params: 792,352\r\nNon-trainable params: 0\r\n```"
      }
    ]
  },
  {
    "issue_number": 252,
    "title": "Make TensorFlow compatible with PyPy",
    "author": "FabHan",
    "state": "open",
    "created_at": "2015-11-17T08:07:22Z",
    "updated_at": "2025-06-15T02:16:07Z",
    "labels": [
      "stat:contribution welcome",
      "type:feature",
      "stale"
    ],
    "body": "I know it's not a priority and will be a long way to get there; but making TF compatible with PyPy woud be super cool.\n\nThoughts?\n",
    "comments": [
      {
        "user": "mrry",
        "body": "As a team we don't use PyPy day-to-day, but we would welcome contributions if it's easy to do this without breaking CPython compatibility.\n\nMy guess is that the two stumbling blocks would be TensorFlow's reliance on NumPy in the Python front-end, and SWIG for interfacing with the C++ backend. Are you aware of any other issues that one would face?\n"
      },
      {
        "user": "girving",
        "body": "I'm not sure what the state of PyPy binding layers is these days, but there's a good chance this would require rewriting the whole swig interface.\n"
      },
      {
        "user": "lvella",
        "body": "PyPy only good interface with native code seems to be [CFFI](https://cffi.readthedocs.io/en/latest/), but it is a C <-> Python interface library. Besides being slower than CPython native interface (when running on CPython, of course), it would be too painful to interface with C++ (Python <-> C <-> C++).\n"
      }
    ]
  },
  {
    "issue_number": 12071,
    "title": "Numerical instability of gradient calculation of tf.norm (nan at 0, inf for small values) ",
    "author": "oduerr",
    "state": "open",
    "created_at": "2017-08-07T07:32:05Z",
    "updated_at": "2025-06-15T02:16:06Z",
    "labels": [
      "stat:contribution welcome",
      "stale"
    ],
    "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.11.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: #v1.2.0-5-g435cdfc    1.2.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: On CPU\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: tf.norm at [0,0] see below for code\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.GIT_VERSION, \"  \", tf.VERSION) #v1.2.0-5-g435cdfc    1.2.1\r\n\r\nX = tf.placeholder(tf.float32, shape=(4,None))\r\nZ = tf.norm(X, ord='euclidean', axis=1, name='logit')\r\nvar_grad = tf.gradients(Z, [X])\r\n\r\nwith tf.Session() as sess:\r\n    X_ = np.array([\r\n        [1],  # Grad OK\r\n        [0],  # Grad NaN\r\n        [1e-16],  # Grad OK\r\n        [1e-19] #Grad Inf\r\n    ], dtype=np.float32)\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run((Z, var_grad), feed_dict={X: X_}))\r\n    # Result:\r\n    #(array([9.99999940e-01, 0.00000000e+00, 9.99999951e-17,\r\n    #        0.00000000e+00], dtype=float32), [array([[1.00000012],\r\n    #                                                 [nan],\r\n    #                                                 [1.],\r\n    #                                                 [inf]], dtype=float32)])\r\n```\r\n\r\n### Describe the problem\r\n`nan` is calculated for the gradient of `tf.norm` at zero values. For extremely small values `inf` is calculated. Note that the exact result should be 1 in all cases above. \r\n\r\nAbove is a minimal example to reproduce it. The problem occurred in a real world scenario, when implementing a custom loss function (the entropy in https://arxiv.org/abs/1611.01449) and two embeddings where too close to each other (distance practically 0).\r\n\r\n### Source code / logs\r\nSee above \r\n\r\n#### Output of logfile\r\n```\r\n== cat /etc/issue ===============================================\r\nDarwin Olivers-MBP-5.fritz.box 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.11.6\r\n\r\n== are we in docker =========================================  echo == are we in docker ====================================num echo == are we in docker =========================================  ec==  echo == are we in docker =======================================c++ --version\r\n\r\n== uname -a =====================================================\r\nDarwin Olivers-MBP-5.fritz.box 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.1)\r\n\r\n== check for virtualenv ==============  echo == check for virtualenv =====on_b echo == check fo sys  echo == check for virtualenv ============== echo == check for virtualenv ============================================\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin Olivers-MBP-5.fritz.box 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.11.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 7.3.0 (clang-703.0.31)\r\nTarget: x86_64-apple-darwin15.6.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Olivers-MBP-5.fritz.box 15.6.0 Darwin Kernel Version 15.6.0: Tue Apr 11 16:00:51 PDT 2017; root:xnu-3248.60.11.5.3~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.1)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\r\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh.txt: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```",
    "comments": [
      {
        "user": "yaroslavvb",
        "body": "This is caused by square root in definition of tf.norm. IE you are taking gradient of sqrt(x^2). Gradient of sqrt approaches infinity, whereas gradient of x^2 approaches 0, so computing them separately then multiplying is a problem. @goodfeli -- do you remember if Theano uses some standard stabilizing transformation for this kind of case?"
      },
      {
        "user": "goodfeli",
        "body": "Theano gives NaN as the gradient of the norm of a vector with zero norm:\r\n\r\n    >>> x = theano.tensor.vector()\r\n    >>> y = theano.tensor.square(x)\r\n    >>> z = y.sum()\r\n    >>> norm = theano.tensor.sqrt(z)\r\n    >>> d = theano.tensor.grad(norm, x)\r\n    >>> d.eval({x: [0., 0.]})\r\n    array([ nan,  nan])\r\n\r\nTheano does give 0 as the gradient of the norm of a *scalar*. I think for this it is probably using a patternsub to turn sqrt(square(x)) into abs(x). Note that in Theano's conventions, the derivative of abs(x) at 0 is treated as 0 rather than undefined."
      },
      {
        "user": "yaroslavvb",
        "body": "Thinking philosophically, the general problem is that computational graph ends up with things like`a/a` where `a` is 0. Numerically it's undefined, but the limit exists. Similar issue exists with gradient of tf.select (https://github.com/tensorflow/tensorflow/issues/2540) and gradient of `tf.exp(-tf.exp(x))`\r\n\r\nYou have to do some algebraic massaging to get numerically defined result.\r\n\r\nIn your particular case, you could replace automatic gradient with a stable version:\r\n\r\n```\r\nfrom tensorflow.python.framework import function\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n@function.Defun(tf.float32, tf.float32)\r\ndef norm_grad(x, dy):\r\n    return dy*(x/tf.norm(x))\r\n\r\n@function.Defun(tf.float32, grad_func=norm_grad)\r\ndef norm(x):\r\n    return tf.norm(x)\r\n\r\nsess = tf.InteractiveSession()\r\nX = tf.placeholder(tf.float32, shape=(4,None))\r\nX_ = np.array([\r\n    [1],  # Grad OK\r\n    [0],  # Grad NaN\r\n    [1e-16],  # Grad OK\r\n    [1e-19] #Grad Inf\r\n], dtype=np.float32)\r\nZ = norm(X)\r\nvar_grad = tf.gradients(Z, [X])\r\nprint(sess.run((Z, var_grad), feed_dict={X: X_}))\r\n\r\n#1.0, [array([[  1.00000000e+00],\r\n#      [  0.00000000e+00],\r\n#     [  1.00000002e-16],\r\n#    [  9.99999968e-20]], dtype=float32)])\r\n\r\n\r\n```"
      }
    ]
  },
  {
    "issue_number": 41009,
    "title": "“Layer is not connected” issue while accessing intermediate layer from custom callback if model is built by sub-classing",
    "author": "MasterJEET",
    "state": "open",
    "created_at": "2020-07-01T23:15:59Z",
    "updated_at": "2025-06-15T01:51:08Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:keras",
      "TF 2.11"
    ],
    "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version:\r\nPython 3.7.3\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nCUDA 10.2\r\n- GPU model and memory:\r\nNVIDIA TITAN X (Pascal), ~12GB\r\n\r\n**Describe the current behavior**\r\nI've a simple model and need access of intermediate layers within a custom callback to get intermediate predictions. If I build the model by sub-classing, I get the error `AttributeError: Layer dense is not connected`.\r\n\r\n**Describe the expected behavior**\r\nIt shouldn't cause any error and be able to get predictions using intermediate layers.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nX = np.ones((8,16))\r\ny = np.sum(X, axis=1)\r\n\r\nclass CustomCallback(tf.keras.callbacks.Callback):\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        get_output = tf.keras.backend.function(\r\n            inputs = self.model.layers[0].input,\r\n            outputs = self.model.layers[1].output\r\n        )\r\n        print(\"\\nLayer output: \", get_output(X))\r\n\r\nclass Model(tf.keras.Model):\r\n    def build(self, input_shape):\r\n        self.dense1 = tf.keras.layers.Dense(units=32)\r\n        self.dense2 = tf.keras.layers.Dense(units=1)\r\n        \r\n    def call(self, input_tensor):\r\n        x = self.dense1(input_tensor)\r\n        x = self.dense2(x)\r\n        return x\r\n\r\nmodel = Model()\r\nmodel.compile(optimizer='adam',loss='mean_squared_error', metrics='accuracy')\r\nmodel.fit(X,y, epochs=2, callbacks=[CustomCallback()])\r\n```\r\n\r\n**Other info / logs** \r\nTraceback:\r\n```---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-dd6e118e08d6> in <module>\r\n     11 model = Model()\r\n     12 model.compile(optimizer='adam',loss='mean_squared_error', metrics='accuracy')\r\n---> 13 model.fit(X,y, epochs=2, callbacks=[CustomCallback()])\r\n\r\n/opt/anaconda3/envs/brats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/opt/anaconda3/envs/brats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    874           epoch_logs.update(val_logs)\r\n    875 \r\n--> 876         callbacks.on_epoch_end(epoch, epoch_logs)\r\n    877         if self.stop_training:\r\n    878           break\r\n\r\n/opt/anaconda3/envs/brats/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n    363     logs = self._process_logs(logs)\r\n    364     for callback in self.callbacks:\r\n--> 365       callback.on_epoch_end(epoch, logs)\r\n    366 \r\n    367   def on_train_batch_begin(self, batch, logs=None):\r\n\r\n<ipython-input-2-a1f33c1e2e52> in on_epoch_end(self, epoch, logs)\r\n      8     def on_epoch_end(self, epoch, logs=None):\r\n      9         get_output = tf.keras.backend.function(\r\n---> 10             inputs = self.model.layers[0].input,\r\n     11             outputs = self.model.layers[1].output\r\n     12         )\r\n\r\n/opt/anaconda3/envs/brats/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in input(self)\r\n   1806     if not self._inbound_nodes:\r\n   1807       raise AttributeError('Layer ' + self.name +\r\n-> 1808                            ' is not connected, no input to return.')\r\n   1809     return self._get_node_attribute_at_index(0, 'input_tensors', 'input')\r\n   1810 \r\n\r\nAttributeError: Layer dense is not connected, no input to return.\r\n```\r\nIf I build the model using functional API as shown below, it works fine:\r\n```\r\ninitial = tf.keras.layers.Input((16,))\r\nx = tf.keras.layers.Dense(units=32)(initial)\r\nfinal = tf.keras.layers.Dense(units=1)(x)\r\n\r\nmodel = tf.keras.Model(initial, final)\r\nmodel.compile(optimizer='adam',loss='mean_squared_error', metrics='accuracy')\r\nmodel.fit(X,y, epochs=2, callbacks=[CustomCallback()])\r\n```\r\n[Here's](https://stackoverflow.com/q/62668398/2679778) the stackoverflow question I created on the same issue.",
    "comments": [
      {
        "user": "ravikyram",
        "body": "I have tried in colab with TF versions 2.2, 2.3-rc0,nightly version(`2.4.0-dev20200701`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/fc81290bfd1c18d31a40e7c007da118e/untitled71.ipynb).Thanks!"
      },
      {
        "user": "ujjwal-ai",
        "body": "Is there any update on this ? This is an extremely important use case"
      },
      {
        "user": "cecabert",
        "body": "Run into similar issue trying to access intermediate layers in #46605, apparently you can't do it with subclassed model. Which is weird since it is just accessing subpart of the computational graph (build with submodel / functional api)."
      }
    ]
  },
  {
    "issue_number": 56423,
    "title": "Use of Keras `jit_compile` in a distribution strategy causes a `std::system_error`",
    "author": "TimCargan",
    "state": "open",
    "created_at": "2022-06-10T18:14:23Z",
    "updated_at": "2025-06-15T01:44:40Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:dist-strat",
      "TF 2.9"
    ],
    "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.9.1\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n11.2/8.1.1.33\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n\r\nThe following error is thrown during training after a number of steps / epochs \r\n```shell\r\nterminate called after throwing an instance of 'std::system_error'\r\nwhat():  Resource temporarily unavailable\r\n```\r\nI am able to reproduce this error in colab with my sample code\r\n\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport keras\r\nimport tensorflow as tf\r\n\r\ndef build_model_() -> keras.Model:\r\n    input = tf.keras.layers.Input(shape=(5,), name='input_a')\r\n    x = tf.keras.layers.Dense(512, activation = 'relu')(input)\r\n    x = tf.keras.layers.Dense(512, activation = 'relu')(x)\r\n    output = tf.keras.layers.Dense(1, name='output')(x)\r\n    model = tf.keras.models.Model(inputs=input, outputs=output)\r\n    return model\r\n\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint(f\"Can see {strategy.num_replicas_in_sync} gpus\")\r\nwith strategy.scope():\r\n  model = build_model_()\r\n  model.compile(loss = 'mse', jit_compile=True)\r\n\r\nBATCH_SIZE_PER_REPLICA = 1024\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\n\r\ndataset = tf.data.Dataset.from_tensors(\r\n    (tf.ones(5), 1)\r\n).repeat(10_000_000).batch(GLOBAL_BATCH_SIZE).with_options(options)\r\n\r\nhistory = model.fit(\r\n    x = dataset,\r\n    epochs=7,\r\n    verbose = 1,\r\n)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_</details>",
    "comments": [
      {
        "user": "sushreebarsa",
        "body": "@TimCargan \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you !"
      },
      {
        "user": "TimCargan",
        "body": "@sushreebarsa will do although I not sure if it is a keras issue. I was able to recrate it without using a keras compiled model and I think it has something to do with the `apply_gradients` step.\r\n\r\nA work around I have found that works is to move the update gradients step out of the XLA step:\r\n```python\r\n def train_step(self, data):\r\n        @tf.function(jit_compile=True, reduce_retracing=True)\r\n        def _jit_step(x, y, sample_weight):\r\n            # Run forward pass.\r\n            with tf.GradientTape() as tape:\r\n                y_pred = self(x, training=True)\r\n                loss = self.compute_loss(x, y, y_pred, sample_weight)\r\n            self._validate_target_and_loss(y, loss)\r\n            grads = tape.gradient(loss, self.trainable_variables)\r\n            return grads, y_pred\r\n\r\n        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\r\n        grads, y_pred = _jit_step(x, y, sample_weight)\r\n        # Run backwards pass outside jit\r\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\r\n        return self.compute_metrics(x, y, y_pred, sample_weight)\r\n```"
      },
      {
        "user": "chunduriv",
        "body": "@TimCargan,\r\n\r\nIt seems to be an issue with memory leak. Set if memory growth should be enabled for a physical device using `tf.config.experimental.set_memory_growth`. Thank you.\r\n"
      }
    ]
  },
  {
    "issue_number": 64250,
    "title": "Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object",
    "author": "hilmyas",
    "state": "closed",
    "created_at": "2024-03-22T03:34:09Z",
    "updated_at": "2025-06-14T20:03:59Z",
    "labels": [
      "type:docs-bug",
      "stat:awaiting response",
      "comp:keras"
    ],
    "body": "### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16.1\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSome days ago, I tried to run codes by using Google Colab from a tutorial of [transfer learning with movinet](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/video/transfer_learning_with_movinet.ipynb) without any error. Today, I tried to run codes by using Google Colab from the same tutorial and it produced error when I tried to compile the model.\r\n\r\n`num_epochs = 2\r\n\r\nloss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\r\n\r\nmodel.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])`\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/video/transfer_learning_with_movinet.ipynb#scrollTo=dVqBLrn1tBsd\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-12-0c295092a661> in <cell line: 7>()\r\n      5 optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\r\n      6 \r\n----> 7 model.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])\r\n\r\n/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)\r\n     68             # To get the full stack trace, call:\r\n     69             # `tf.debugging.disable_traceback_filtering()`\r\n---> 70             raise e.with_traceback(filtered_tb) from None\r\n     71         finally:\r\n     72             del filtered_tb\r\n\r\n/usr/local/lib/python3.10/dist-packages/tf_keras/src/optimizers/__init__.py in get(identifier, **kwargs)\r\n    333         )\r\n    334     else:\r\n--> 335         raise ValueError(\r\n    336             f\"Could not interpret optimizer identifier: {identifier}\"\r\n    337         )\r\n\r\nValueError: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x7af7a01536d0>\n```\n",
    "comments": [
      {
        "user": "tilakrayal",
        "body": "@hilmyas,\r\nBy default Tensorflow v2.16 uses keras v3.0 which might be the reason for the error you are facing. As the work-around, Could you please try to install keras 2 where the code was executed without any issue/error. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/4cd97fa3a806aa1315ba193773225755/transfer_learning_with_movinet.ipynb).\r\n```python\r\n!pip install -U tf_keras # Keras 2\r\nimport os\r\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\r\n```\r\n\r\nThank you!\r\n\r\n"
      },
      {
        "user": "hilmyas",
        "body": "It works. Thanks for your help."
      },
      {
        "user": "tilakrayal",
        "body": "@hilmyas,\r\nGlad the issue is resolved for you, Could you please feel free to move this issue to closed status. Thank you!"
      }
    ]
  },
  {
    "issue_number": 94709,
    "title": "Tensorflow is aborting with CompositeTensorVariant already registered",
    "author": "nishith-fujitsu",
    "state": "open",
    "created_at": "2025-06-03T05:40:58Z",
    "updated_at": "2025-06-14T02:11:45Z",
    "labels": [
      "stat:awaiting response",
      "type:build/install",
      "stale",
      "subtype: ubuntu/linux",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\nmaster (tf 2.20)\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux AArch64\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n8.2.1\n\n### GCC/compiler version\n\nclang version 17.0.6\n\n### CUDA/cuDNN version\n\nNA\n\n### GPU model and memory\n\nNA\n\n### Current behavior?\n\nI am trying to build the tensorflow-cpu from source with master branch on AArch64 machine (AWS graviton3 instance), build is successful, but I am not able to import tensorflow library. \n\nBazel command used to build tensorflow\n`taskset -c 6-31 bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --config=mkl_aarch64_threadpool --local_cpu_resources=24 --verbose_failures`\n\nThe error logs while importing tensorflow library:\n```\n2025-06-03 10:29:59.180194: W tensorflow/core/common_runtime/input_colocation_exemption_registry.cc:33] Input colocation exemption for op: IdentityN already registered\n2025-06-03 10:29:59.198646: F tensorflow/core/framework/variant_op_registry.cc:76] Check failed: existing == nullptr (0x4031a6f0 vs. nullptr)Unary VariantDecodeFn for type_name: CompositeTensorVariant already registered\nAborted\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\ntaskset -c 6-31 bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --config=mkl_aarch64_threadpool --local_cpu_resources=24 --verbose_failures\n```\n\n### Relevant log output\n\n```shell\nPython 3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow\n2025-06-03 11:10:07.006037: W tensorflow/core/common_runtime/input_colocation_exemption_registry.cc:33] Input colocation exemption for op: IdentityN already registered\n2025-06-03 11:10:07.008279: F tensorflow/core/framework/variant_op_registry.cc:76] Check failed: existing == nullptr (0x2ecdd560 vs. nullptr)Unary VariantDecodeFn for type_name: CompositeTensorVariant already registered\nAborted\n```",
    "comments": [
      {
        "user": "Gurupatil0003",
        "body": "❤️"
      },
      {
        "user": "Venkat6871",
        "body": "Hi @nishith-fujitsu ,\nApologies for the delay, and thank you for raising your concern here.\nThis issue might be related to a compatibility problem. I noticed a version mismatch between Bazel and TensorFlow. Could you please verify that you are using compatible versions to avoid errors?\nI am providing the official [documentation](https://www.tensorflow.org/install/source#cpu) for your reference.\n\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      }
    ]
  },
  {
    "issue_number": 60628,
    "title": "Numpy and tf experimental Numpy differ in vander matrix creation case for N=0",
    "author": "Ishticode",
    "state": "open",
    "created_at": "2023-05-18T15:11:22Z",
    "updated_at": "2025-06-13T12:04:51Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "comp:ops",
      "TF 2.11"
    ],
    "body": "<details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.11.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nUbuntu 22.04 jammy\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.10\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n11.6\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nThe behaviour of `tf.experimental.numpy.vander` is different than `np.vander` for `N=0` where both value and shape of the output differ.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np        # 1.23.5\r\nimport tensorflow as tf   # 2.11.0\r\n\r\nxn = np.array([1], dtype=np.int32)\r\nx = tf.constant([1], dtype=tf.int32)\r\nprint(np.vander(xn, 0))\r\nprint()\r\nprint(tf.experimental.numpy.vander(x, 0))\n```\n\n\n### Relevant log output\n\n```shell\n[]\r\n\r\ntf.Tensor([[1]], shape=(1, 1), dtype=int32)\n```\n</details>",
    "comments": [
      {
        "user": "tilakrayal",
        "body": "@Ishticode,\r\nI was able to reproduce the issue on tensorflow v2.11, v2.12 and tf-nightly. Kindly find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d8153b468f2dbc37a09b12626f25a0cf/untitled1159.ipynb)."
      },
      {
        "user": "Venkat6871",
        "body": "Hi @Ishticode ,\nApologies for the delay, and thank you for your patience.\nI tried running your code in Colab using the latest versions of TensorFlow, and it is giving the same results.\nIt seems the issue has been fixed in the latest versions. I am providing a [gist](https://colab.sandbox.google.com/gist/Venkat6871/e3ace27ee26a1c54f40a669b70bf15f8/60628_tf_2-19-0-nightly-v.ipynb) here for your reference.\n\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94853,
    "title": "How to run Android demo which uses NPU to inference?",
    "author": "YushengEdward",
    "state": "open",
    "created_at": "2025-06-05T05:22:17Z",
    "updated_at": "2025-06-13T11:25:29Z",
    "labels": [
      "type:support",
      "comp:lite"
    ],
    "body": "Hello!\nI'm wondering where can I find a demo to inference with NPU. It's better to detect objection.\nThank you!",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @YushengEdward ,\nApologies for the delay, and thank you for raising your concern here.\nIf you are looking to run object detection inference on a device with an NPU, we recommend using [TensorFlow Lite](https://ai.google.dev/edge/litert), which is designed for deploying models on mobile and edge devices with hardware acceleration support (such as NPUs, GPUs, and DSPs).\nFor your reference, here is an [Android object detection demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) that uses TensorFlow Lite and supports hardware acceleration via NNAPI.\n\nAdditionally, we noticed that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled out. Could you please update it with the following information?\nTensorFlow version\nSteps you followed before encountering the issue\nA standalone code snippet or a Colab link to reproduce the problem\nThis information will help us better analyze and assist you.\nThank you!\n"
      },
      {
        "user": "YushengEdward",
        "body": "Hi@Venkat6871，\nI have installed the demo [Android object detection demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android), it says that it can't support GPU.The demo use the NNAPI, but I'm not sure whether it use NPU or not when using NNAPI. Can you clear up my doubts?"
      },
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @YushengEdward \nThank you for bringing this issue to our attention, As far I know Yes, the object detection example demo does support NPU via NNAPI on Android. I see in [ObjectDetectorHelper.kt](https://github.com/tensorflow/examples/blob/3c7435733a9162481df350e57c53e58d2a705e4d/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/objectdetection/ObjectDetectorHelper.kt#L66), When `DELEGATE_NNAPI` is selected, the code calls `baseOptionsBuilder.useNnapi()` which enables TensorFlow Lite to use NNAPI. NNAPI will use the device's NPU (Neural Processing Unit) if available.\n\nThe Neural Networks API (NNAPI) is deprecated. It was [introduced in Android 8.1](https://developer.android.com/about/versions/oreo/android-8.1#nnapi) to provide a unified interface for hardware accelerated inference for on-device machine learning, and deprecated in Android 15 so if you're using Android 15 or later please give it try with prior version Android 15 please refer this official documentation of [NNAPI Migration Guide](https://developer.android.com/ndk/guides/neuralnetworks/migration-guide)\n\nIf I have missed something here please let me know. Thank you for your cooperation and understanding."
      }
    ]
  },
  {
    "issue_number": 62075,
    "title": "cuDNN, cuFFT, and cuBLAS Errors",
    "author": "joshuacuellar1",
    "state": "open",
    "created_at": "2023-10-09T18:57:10Z",
    "updated_at": "2025-06-13T11:12:22Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:build/install",
      "comp:gpu",
      "TF2.14"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nGIT_VERSION:v2.14.0-rc1-21-g4dacf3f368e VERSION:2.14.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nWSL2 Linux Ubuntu 22\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10, but I can try different versions\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\nCUDA version: 11.8, cuDNN version: 8.7\n\n### GPU model and memory\n\nNVIDIA Geforce GTX 1660 Ti, 8GB Memory\n\n### Current behavior?\n\nWhen I run the GPU test from the TensorFlow install instructions, I get several errors and warnings.\r\nI don't care about the NUMA stuff, but the first 3 errors are that TensorFlow was not able to load cuDNN. I would really like to be able to use it to speed up training some RNNs and FFNNs. I do get my GPU in the list of physical devices, so I can still train, but not as fast as with cuDNN.\n\n### Standalone code to reproduce the issue\n\n```shell\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n```\n\n\n### Relevant log output\n\n```shell\n2023-10-09 13:36:23.355516: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2023-10-09 13:36:23.355674: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2023-10-09 13:36:23.355933: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2023-10-09 13:36:23.413225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-10-09 13:36:25.872586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2023-10-09 13:36:25.916952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2023-10-09 13:36:25.917025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\nYour kernel may have been built without NUMA support.\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n```\n",
    "comments": [
      {
        "user": "SuryanarayanaY",
        "body": "Hi @Ke293-x2Ek-Qe-7-aE-B ,\r\n\r\nStarting from TF2.14 tensorflow provides CUDA package which can install all the cuDNN,cuFFT and cubLas libraries.\r\n\r\nYou can use `pip install tensorflow[and-cuda]` command for that.\r\n\r\nPlease try this command let us know if it helps. Thankyou!"
      },
      {
        "user": "joshuacuellar1",
        "body": "@SuryanarayanaY I did not know that it now came bundled with cuDNN. I installed tensorflow with the [and-cuda] part, though, but I also installed cuda toolkit and cuDNN separately. I will try just installing the cuda toolkit and then installing tensorflow[and-cuda].\r\nAlso, is there a way to install tensorflow for GPU without it coming with cuDNN? If I just `pip install tensorflow`, will that install with GPU support, just without cuDNN, so that I can manually install them? I don't really need to, but I am curious if it can be installed that way too."
      },
      {
        "user": "joshuacuellar1",
        "body": "@SuryanarayanaY I tried several times, reinstalling Ubuntu, but it still doesn't work."
      }
    ]
  },
  {
    "issue_number": 93917,
    "title": "Mismatch Between Quantized TFLite Layer Outputs and Expected Mathematical Values When Using get_tensor()",
    "author": "sradhamurali",
    "state": "open",
    "created_at": "2025-05-22T07:06:01Z",
    "updated_at": "2025-06-13T05:21:28Z",
    "labels": [
      "type:bug",
      "comp:lite",
      "2.6.0"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.6.2\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nWindows 10\n\n### Mobile device\n\nN/A\n\n### Python version\n\n3.6.8\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI am trying to extract intermediate outputs from a quantized TFLite model using the TFLite interpreter. The goal is to verify that the model's intermediate outputs match the mathematically expected values.\n\n**Steps I followed**:\n\n1) Loaded a quantized TFLite model using tf.lite.Interpreter.\n2) Set the model input using:    interpreter.set_tensor(input_index, quantized_input)\n3) Invoked the interpreter:    interpreter.invoke()\n4) Retrieved op details:\n                 op_details = interpreter._get_ops_details()\n5) For each layer, fetched the output tensor index:\n           out_tensor_index = op_details[i][\"outputs\"][0]\n6) Got the tensor values:\n           output = interpreter.get_tensor(out_tensor_index)\nSaved the output of each layer as .npy files for further analysis.\n\n**Issue:**\n\nThe output values obtained from interpreter.get_tensor() do not match the mathematically calculated (expected) values, even after considering quantization parameters (e.g., scale and zero-point).\n\n**Questions:**\n\n1)Is this the correct approach to extract intermediate layer outputs from a quantized TFLite model?\n2) Are there any limitations with using interpreter._get_ops_details() and get_tensor() on quantized models?\nAny help is welcome, let me know if more info is needed\n\n\n### Standalone code to reproduce the issue\n\n```shell\nSample code :\n\ninterpreter = tf.lite.Interpreter(model_path=\"quantized_model.tflite\")\ninterpreter.allocate_tensors()\n\n\ninput_details = interpreter.get_input_details()\ninput_index = input_details[0][\"index\"]\ninput_scale, input_zero_point = input_details[0]['quantization']\n\n# Prepare dummy input data (change as needed)\nfloat_input = np.random.rand(*input_details[0]['shape']).astype(np.float32)\nquantized_input = (float_input / input_scale + input_zero_point).astype(np.uint8)\n\ninterpreter.set_tensor(input_index, quantized_input)\n\ninterpreter.invoke()\n\nops_details = interpreter._get_ops_details()\ntensor_details = interpreter.get_tensor_details()\n\nfor i, op in enumerate(ops_details):\n    for out_idx in op[\"outputs\"]:\n        tensor_info = [t for t in tensor_details if t['index'] == out_idx]\n        if not tensor_info:\n            continue\n        tensor_info = tensor_info[0]\n\n        # Get raw output tensor\n        out_tensor = interpreter.get_tensor(out_idx)\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @sradhamurali, Let me address your questions about discrepancies between quantized TFLite Layer Outputs and expected mathematical values when using get_tensor()\n\nIs this the correct approach to extract intermediate layer outputs? Yes, using **interpreter._get_ops_details()** and **interpreter.get_tensor()** is a common debugging approach to inspect intermediate tensors. However, for correct numerical comparison the critical missing step was the explicit dequantization.\n\nAre there any limitations with using **interpreter._get_ops_details()** and **get_tensor()** on quantized models?\n**_get_ops_details()** is a private API means it's not part of the public stable API and can change without warning between TensorFlow versions. While it's widely used for debugging be aware it might break in future upgrades. As explained get_tensor() on quantized models yields raw integer values requiring manual dequantization for float comparison.\n\nThe names obtained from tensor_details['name'] might not always directly correspond to your original Keras layer names especially for complex operations or if the converter optimizes/fuses layers. This can make mapping intermediate outputs tricky. This method is primarily for debugging/verification not for efficient production inference where you'd rely on the model's defined inputs/outputs.\n\nThe core discrepancy you faced was indeed due to the need for explicit dequantization of intermediate get_tensor() outputs. While we can't access all intermediate tensors due to TFLite's internal optimizations for the ones that are accessible the dequantization method successfully brings them in line with float values considering the expected small precision loss.\n\nPlease refer official documentation of below methods of **tf.lite.Interpreter** and Also please refer TensorFlow Lite Quantization Guide [[Ref-1](https://ai.google.dev/edge/litert/models/post_training_quantization)] and [[Ref-2](https://ai.google.dev/edge/litert/models/quantization_spec)]:\n1. [get_tensor_details()](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter#get_tensor_details): Essential for getting tensor metadata, including quantization parameters (scale, zero_point).\n2. [get_input_details()](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter#get_input_details) and [get_output_details()](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter#get_output_details): For model input/output.\n3. [get_tensor(index)](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter#get_tensor): Retrieves the tensor's data. \n\nI hope this explanation helps clarify the behavior you're observing If I have missed something here please let me know. Thank you for your cooperation and understanding."
      },
      {
        "user": "sradhamurali",
        "body": "Hi, @gaikwadrahul8 , \nThank you for the response. \n**The names obtained from tensor_details['name'] might not always directly correspond to your original Keras layer names, especially for complex operations or if the converter optimizes/fuses layers. This can make mapping intermediate outputs tricky. This method is primarily for debugging/verification, not for efficient production inference where you'd rely on the model's defined inputs/outputs**  \nI am using the interpreter._get_ops_details() to obtain the ops_details, and loop through ops_details and get the output index, which is used for the .get_tensor() to get the values. Is there still a chance of misinterpretation?      \n\nfor i, op in enumerate(ops_details):\n    for out_idx in op[\"outputs\"]:\n        out_tensor = interpreter.get_tensor(out_idx) \n\n**The core discrepancy you faced was indeed due to the need for explicit dequantization of intermediate get_tensor() outputs**\nI'm using the quantization parameter obtained from the output_details of each layer, which is being mapped to the corresponding output_index. The corresponding quantization parameters are also used for the mathematical calculation, so the floating-point values are not matching. \n\nKindly let me know if I have missed anything. I look forward to hearing from you.\n"
      },
      {
        "user": "gaikwadrahul8",
        "body": "Hi @sradhamurali, your method of using `_get_ops_details()` and `get_tensor()` to inspect intermediate outputs is a common debugging technique. The primary root cause for the mismatch you're observing is likely the use of incorrect quantization parameters for these intermediate tensors. Each intermediate tensor possesses its own unique scale and zero-point distinct from the model's final output tensor parameters (which `get_output_details()` provides).\n\nTo resolve this make sure you retrieve the specific quantization parameters (scale, zero-point) for each intermediate tensor by its index from the list returned by `interpreter.get_tensor_details()` then apply the dequantization formula: `float_value = (raw_quantized_value.astype(np.float32) - zero_point) * scale`.  If you continue having issues, consider using the TensorFlow Lite Quantization Debugger [Inspecting Quantization Errors with Quantization Debugger | TensorFlow Lite](https://www.tensorflow.org/lite/performance/quantization_debugger) which is specifically designed for inspecting quantization errors and comparing intermediate outputs between float and quantized models.\n\nIf issue still persists could you please help us with Google Colab notebook along with`.tflite` model file to investigate this issue further from our end ? Thank you for your cooperation and understanding."
      }
    ]
  },
  {
    "issue_number": 93531,
    "title": "Segmentation fault in tf.queue.FIFOQueue",
    "author": "cx104906",
    "state": "closed",
    "created_at": "2025-05-16T11:18:23Z",
    "updated_at": "2025-06-13T02:56:46Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\nUbuntu 18.04\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n6.5.0\n\n### GCC/compiler version\n\nclang 18.1.8\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSegfault when using tf.queue.FIFOQueue\n\n### Standalone code to reproduce the issue\n\n```shell\nimport pickle\nimport tensorflow as tf\nimport time\nimport pprint\nimport numpy as np\n\nprint(tf.__version__)\n\nmylist = [False,\n 1822247737864973528224891031828,\n (128504154951878448978501253655603801238, False),\n 39615285460585582029130662038]\nmydict = {}\n\nfor i in range(100):\n  try:\n      print (f\"{i}\",flush=True)\n      tf.queue.FIFOQueue(*mylist,**mydict)\n  except Exception as e:\n      print(f\"{e}\")\n\nprint(\"done\")\n```\n\n### Relevant log output\n\n```shell\n2.19.0\n\ntest\n\n0\nSegmentation fault (core dumped)\n```",
    "comments": [
      {
        "user": "tilakrayal",
        "body": "@cx104906 \nThe issue here is in the usage of FIFOQueue.dequeue_many(). The function expects a scalar or a scalar Tensor n corresponding to the number of elements to dequeue. Both dequeue_many() and dequeue_up_to() also expect all the queue elements to have a specific shape, which is set when the queue is initialized."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 93525,
    "title": "java.lang.IllegalArgumentException: Internal error: Error applying delegate:",
    "author": "davidlad123",
    "state": "open",
    "created_at": "2025-05-16T10:21:22Z",
    "updated_at": "2025-06-13T02:14:11Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "type:support",
      "stale"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nLiteRT 1.2.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMac OS 15.4.1 \n\n### Mobile device\n\nPixel 7\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n12.6 cudnn 90501\n\n\n\n### GPU model and memory\nRTX 4500 ADA Generation 24GB\n\n\n### Current behavior?\n\nEvery tme I attempt to run inference with my object detection model, Its unable to use the GPU. The CPU operation works just fine and I have had to write CPU fallback code to initiate the model.\n\nProblem only seems to happen with efficientDet/EfficientNet family. Yolov8 was able to use the provided GPU\n\nThe code provided allows one to load any model and check to see if it will use the GPU on the android phone\n\n``` \n\nimport android.content.Context\nimport android.graphics.Bitmap\nimport android.graphics.RectF\nimport androidx.core.graphics.get\nimport androidx.core.graphics.scale\n\nimport org.tensorflow.lite.Interpreter\nimport org.tensorflow.lite.gpu.CompatibilityList\nimport org.tensorflow.lite.gpu.GpuDelegate\nimport org.tensorflow.lite.support.common.FileUtil\nimport timber.log.Timber\nimport java.io.File\nimport java.io.FileOutputStream\nimport java.nio.ByteBuffer\nimport java.nio.ByteOrder\nimport javax.inject.Inject\nimport javax.inject.Singleton\n\nclass CustomModelInferencer @Inject constructor(\n    private val context: Context,\n    private val customModelPath: String,\n)  {\n\n     val compatibilityList = CompatibilityList()\n    private val interpreter: Interpreter by lazy { setupInterpreter() }\n\nprivate fun setupInterpreter(): Interpreter {\n        val options = setupOptions()\n        val modelFile = createModelFile()\n\n        if (modelFile == null) {\n            throw IllegalStateException(\"Failed to create model file from asset\")\n        }\n\n        return try {\n            createInterpreter(modelFile, options)\n        } catch (e: IllegalArgumentException) {\n            Timber.d(\"Falling back to CPU interpreter options: delegates are: ${options.delegates}\")\n            val newOptions = Interpreter.Options()\n\n            newOptions.setNumThreads(6)\n            newOptions.setUseNNAPI(false)\n            createInterpreter(modelFile, newOptions)\n        }\n    }\n\n    private fun createInterpreter(\n        modelFile: File,\n        options: Interpreter.Options\n    ): Interpreter {\n        val interpreter = Interpreter(modelFile, options)\n        showTensorDetails(interpreter)\n        return interpreter\n    }\n\n    private fun showTensorDetails(interpreter: Interpreter) {\n        val inputTensor = interpreter.getInputTensor(0)\n        Timber.d(\"Input tensor shape: ${inputTensor.shape().contentToString()}\")\n        Timber.d(\"Input tensor type: ${inputTensor.dataType()}\")\n\n        for (i in 0 until interpreter.outputTensorCount) {\n            val outputTensor = interpreter.getOutputTensor(i)\n            Timber.d(\"Output tensor $i shape: ${outputTensor.shape().contentToString()}\")\n            Timber.d(\"Output tensor $i type: ${outputTensor.dataType()}\")\n        }\n    }\n\n\n    private fun setupOptions(): Interpreter.Options {\n        val options = Interpreter.Options().apply {\n            if (compatibilityList.isDelegateSupportedOnThisDevice) {\n                try {\n                    val delegateOptions = compatibilityList.bestOptionsForThisDevice\n                    delegateOptions.setInferencePreference(GpuDelegate.Options.INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER)\n                    this.addDelegate(GpuDelegate(delegateOptions))\n                    Timber.d(\"GPU delegate added to interpreter options\")\n                } catch (e: Exception) {\n                    Timber.e(\"Error adding GPU delegate from setup options\")\n                }\n            } else {\n                Timber.d(\"Compartibility list does not support GPU, falling back to CPU\")\n            }\n        }\n\n        return options\n    }\n\n    private fun createModelFile(): File? {\n        return try {\n            val assetPath = customModelPath\n            Timber.d(\"Loading model file from asset: $assetPath\")\n            val modelBuffer = FileUtil.loadMappedFile(context, assetPath)\n            File.createTempFile(\"tflite_model\", \".tflite\").apply {\n                FileOutputStream(this).use { fos ->\n                    val bytes = ByteArray(modelBuffer.remaining())\n                    modelBuffer.get(bytes)\n                    fos.write(bytes)\n                }\n                deleteOnExit()\n            }\n        } catch (e: Exception) {\n            Timber.e(e, \"Error creating file from model asset\")\n            null\n        }\n    }\n   fun checkInterpreter(){\n        Timber.d(\"The interpreter was loaded: $interpreter\")\n    }\n\n}\n```\n\nThe GPU Delegate is loaded but the error below is thrown when the code call ` private fun createInterpreter(\n        modelFile: File,\n        options: Interpreter.Options\n    )`\n\nmy libs.version.toml contains:\n\n`\nlitert = \"1.2.0\"\nlitert = { group = \"com.google.ai.edge.litert\", name = \"litert\", version.ref = \"litert\" }\nlitert-api = { group = \"com.google.ai.edge.litert\", name = \"litert-api\", version.ref = \"literti\" }\nlitert-gpu = { group = \"com.google.ai.edge.litert\", name = \"litert-gpu\", version.ref = \"litert\" }\nlitert-gpu-api = { group = \"com.google.ai.edge.litert\", name = \"litert-gpu-api\", version.ref = \"litert\" }\nlitert-metadata = { group = \"com.google.ai.edge.litert\", name = \"litert-metadata\", version.ref = \"litert\" }\n`\n\n### Standalone code to reproduce the issue\n\n```shell\ntest dependencies\n\ndependencies {\n    testImplementation 'junit:junit:4.13.2'\n    testImplementation 'org.mockito:mockito-core:3.12.4'\n    testImplementation 'org.mockito:mockito-inline:3.12.4'\n}\n\n\n\npackage com.detector.lens.core.ml\n\nimport android.content.Context\nimport org.junit.Before\nimport org.junit.Test\nimport org.junit.runner.RunWith\nimport org.mockito.Mock\nimport org.mockito.Mockito.*\nimport org.mockito.junit.MockitoJUnitRunner\nimport org.tensorflow.lite.Interpreter\nimport timber.log.Timber\n\n@RunWith(MockitoJUnitRunner::class)\nclass CustomModelInferencerTest {\n\n    @Mock\n    private lateinit var mockContext: Context\n\n    @Mock\n    private lateinit var mockInterpreter: Interpreter\n\n    private lateinit var customModelInferencer: CustomModelInferencer\n\n    @Before\n    fun setup() {\n        // Initialize Timber for logging\n        Timber.plant(object : Timber.Tree() {\n            override fun log(priority: Int, tag: String?, message: String, t: Throwable?) {\n                println(\"$tag: $message\")\n            }\n        })\n\n        // Create the CustomModelInferencer with mocked dependencies\n        customModelInferencer = spy(CustomModelInferencer(mockContext, \"test_model.tflite\"))\n\n        // Mock the interpreter creation\n        doReturn(mockInterpreter).`when`(customModelInferencer).setupInterpreter()\n    }\n\n    @Test\n    fun `checkInterpreter should log interpreter information`() {\n        // Call the method we want to test\n        customModelInferencer.checkInterpreter()\n\n        // Verify that the log message was called with the correct information\n        verify(customModelInferencer).checkInterpreter()\n    }\n}\n\n```\n\n### Relevant log output\n\n```shell\nGPU delegate added to interpreter options\nLoading model file from asset: fire_model_lite_quantized.effnet.v2.b1.tflite\nError applying delegate, falling back to CPU\njava.lang.IllegalArgumentException: Internal error: Error applying delegate: \n\tat org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\n\tat org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:110)\n\tat org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:58)\n\tat org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:32)\n\tat org.tensorflow.lite.Interpreter.<init>(Interpreter.java:200)\n\tat CustomModelInferencer.createInterpreter(CustomModelInferencer.kt:57)\n\tat CustomModelInferencer.setupInterpreter(CustomModelInferencer.kt:40)\n\tat CustomModelInferencer.interpreter_delegate$lambda$0(CustomModelInferencer.kt:29)\n\tat CustomModelInferencer.$r8$lambda$SVsjMeJxO6XQlg9B8_m_AI2SxJo(Unknown Source:0)\n\tat CustomModelInferencer$$ExternalSyntheticLambda0.invoke(D8$$SyntheticClass:0)\n\tat kotlin.SynchronizedLazyImpl.getValue(LazyJVM.kt:74)\n\tat CustomModelInferencer.getInterpreter(CustomModelInferencer.kt:29)\n\tat CustomModelInferencer.detectBitmap(CustomModelInferencer.kt:121)\n\tat com.detector.lens.features.common.domain.analyzers.TfliteObjectAnalyzer.runWithMutex(TfliteObjectAnalyzer.kt:118)\n\tat com.detector.lens.features.common.domain.analyzers.TfliteObjectAnalyzer.access$runWithMutex(TfliteObjectAnalyzer.kt:24)\n\tat com.detector.lens.features.common.domain.analyzers.TfliteObjectAnalyzer$processObjectImages$1$1.invokeSuspend(TfliteObjectAnalyzer.kt:96)\n\tat kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\n\tat kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:104)\n\tat kotlinx.coroutines.internal.LimitedDispatcher$Worker.run(LimitedDispatcher.kt:111)\n\tat kotlinx.coroutines.scheduling.TaskImpl.run(Tasks.kt:99)\n\tat kotlinx.coroutines.scheduling.CoroutineScheduler.runSafely(CoroutineScheduler.kt:584)\n\tat kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.executeTask(CoroutineScheduler.kt:811)\n\tat kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.runWorker(CoroutineScheduler.kt:715)\n\tat kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.run(CoroutineScheduler.kt:702)\nFalling back to CPU interpreter options: delegates are: [org.tensorflow.lite.gpu.GpuDelegate@ee312fe]\nInput tensor shape: [1, 224, 224, 3]\nInput tensor type: UINT8\nOutput tensor 0 shape: [1, 1]\nOutput tensor 0 type: UINT8\n```",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @davidlad123 \nI apologize for the delay in my response, Thank you for bringing this issue to our attention if possible could you please help us with minimal code with your Github repo along with complete steps to replicate same behavior from our end that will be very helpful to investigate this issue further from our end ?\n\nMeanwhile please try [tf.lite.experimental.Analyzer ](https://www.tensorflow.org/api_docs/python/tf/lite/experimental/Analyzer\n) something like below to check there are any unsupported Ops for GPU\n\n```\nmodel_path = \"/content/efficientdet.tflite\"\ntf.lite.experimental.Analyzer.analyze(model_path=model_path, gpu_compatibility=True)\n```\n\nThank you for your cooperation and patience."
      },
      {
        "user": "davidlad123",
        "body": "I'll add this to a github repo and let you know. I will also check the model for unsupported operations."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      }
    ]
  },
  {
    "issue_number": 94599,
    "title": "graph execution error bug with tfm.nlp.layers.MultiHeadRelativeAttention",
    "author": "Thorballer",
    "state": "open",
    "created_at": "2025-06-01T01:26:36Z",
    "updated_at": "2025-06-13T02:14:07Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:keras",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nP100\n\n### Current behavior?\n\nI'm creating a transformer encoder layer, and I was trying to add positional encoding, but I always encounter a graph execution error when using tfm.nlp.layers.MultiHeadRelativeAttention. Every time I use it, I get the posted error, and it may be an issue with how batches are being processed in the layer. The layer is really experimental, but I have tried many ways of getting around the error, but the error seems to persist.\n\n### Standalone code to reproduce the issue\n\n```shell\nclass TransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.key_dim = d_model // num_heads\n\n        # Attention layer\n        self.att = tfm.nlp.layers.MultiHeadRelativeAttention(\n            num_heads=num_heads,\n            key_dim=self.key_dim\n        )\n\n        # Trainable bias parameters with correct shape\n        self.content_bias = self.add_weight(\n            name='content_bias',\n            shape=[1, 1, num_heads, self.key_dim],  # [1, 1, H, Dk]\n            initializer='zeros',\n            trainable=True\n        )\n        self.position_bias = self.add_weight(\n            name='position_bias',\n            shape=[1, 1, num_heads, self.key_dim],  # [1, 1, H, Dk]\n            initializer='zeros',\n            trainable=True\n        )\n\n        # Rest of the network\n        self.ffn = tf.keras.Sequential([\n            tf.keras.layers.Dense(dff, activation='relu'),\n            tf.keras.layers.Dense(d_model)\n        ])\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.rel_pos_encode = tfm.nlp.layers.RelativePositionEmbedding(d_model)\n\n    def call(self, x, training=False, mask=None):\n        batch_size = tf.shape(x)[0]\n        seq_len = tf.shape(x)[1]\n        H = self.num_heads\n        Dk = self.key_dim\n\n        # 1) Prepare biases with correct shape [B, L, H, Dk]\n        content_attention_bias = tf.tile(self.content_bias, [batch_size, seq_len, 1, 1])\n        positional_attention_bias = tf.tile(self.position_bias, [batch_size, seq_len, 1, 1])\n\n        # 2) Generate relative position encoding [B*H, 2*L-1, Dk]\n        rel_len = 2 * seq_len - 1\n        rel_embedding = self.rel_pos_encode(inputs=None, length=rel_len)\n        rel_embedding = tf.reshape(rel_embedding, [rel_len, H, Dk])\n        rel_embedding = tf.transpose(rel_embedding, [1, 0, 2])  # [H, 2*L-1, Dk]\n        rel_embedding = tf.tile(rel_embedding, [batch_size, 1, 1])  # [B*H, 2*L-1, Dk]\n\n        # 3) Call attention with properly shaped biases\n        attn_output = self.att(\n            query=x,\n            value=x,\n            content_attention_bias=content_attention_bias,\n            positional_attention_bias=positional_attention_bias,\n            relative_position_encoding=rel_embedding,\n            attention_mask=mask\n        )\n\n        # 4) Standard transformer operations\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)\n        \n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n```\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n/tmp/ipykernel_35/1955170888.py in <cell line: 0>()\n----> 1 history = model.fit(\n      2     train,\n      3     validation_data=val,\n      4     epochs=50,\n      5 )\n\n/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)\n    120             # To get the full stack trace, call:\n    121             # `keras.config.disable_traceback_filtering()`\n--> 122             raise e.with_traceback(filtered_tb) from None\n    123         finally:\n    124             del filtered_tb\n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n     57       e.message += \" name: \" + name\n     58     raise core._status_to_exception(e) from None\n---> 59   except TypeError as e:\n     60     keras_symbolic_tensors = [x for x in inputs if _is_keras_symbolic_tensor(x)]\n     61     if keras_symbolic_tensors:\n\nInvalidArgumentError: Graph execution error:\n\nDetected at node gradient_tape/improved_transformer_23_1/transformer_block_45_1/multi_head_relative_attention_44/add_2/BroadcastGradientArgs defined at (most recent call last):\n<stack traces unavailable>\nIncompatible shapes: [8,4,512,512] vs. [32,4,512,512]\n\nStack trace for op definition: \nFile \"<frozen runpy>\", line 198, in _run_module_as_main\nFile \"<frozen runpy>\", line 88, in _run_code\nFile \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\nFile \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\nFile \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\nFile \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\nFile \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\nFile \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\nFile \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\nFile \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\nFile \"/tmp/ipykernel_35/1955170888.py\", line 1, in <cell line: 0>\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 113, in one_step_on_data\nFile \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 77, in train_step\n\n\t [[{{node gradient_tape/improved_transformer_23_1/transformer_block_45_1/multi_head_relative_attention_44/add_2/BroadcastGradientArgs}}]]\n\ttf2xla conversion failed while converting __inference_one_step_on_data_322878[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_multi_step_on_iterator_323180]\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @Thorballer ,\nApologies for the delay, and thank you for raising the issue here.\nI tried running your code on Colab using TensorFlow version 2.19.0 and the nightly build, but I did not encounter any issues.\nI am providing a [gist](https://colab.sandbox.google.com/gist/Venkat6871/59799bc9d878ea5594a99e95ab9d0893/94599_tf-2-19-0-nightly-v.ipynb) here for your reference.\nPlease let me know if I missed anything during execution.\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      }
    ]
  },
  {
    "issue_number": 66517,
    "title": "Cannot find any way to install tensorflow<=2.15.0",
    "author": "peter-fm",
    "state": "closed",
    "created_at": "2024-04-26T16:13:03Z",
    "updated_at": "2025-06-12T20:46:40Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "TF 2.15"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04.3\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI  want to install tensorflow in python and also then serve a model in a rust application. Since the only c++ bindings are for 2.15.0 (I cannot find any other compiled versions without building from source...which I tried and failed to do), I have to install tensorflow 2.15.0. I tried installing tensorflow 2.15.1 and training a model but the c++ bindings complain the model was trained using a version later than the c++ bindings (which is true)...so I assume I have to install tensorflow 2.15.0 or lower:\r\n\r\n```\r\npip install \"tensorflow==2.15.0\"\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.15.0 (from versions: 2.16.0rc0, 2.16.1)\r\nERROR: No matching distribution found for tensorflow==2.15.0\r\n```\r\nor\r\n```\r\npip install \"tensorflow<=2.15.0\"\r\nERROR: Could not find a version that satisfies the requirement tensorflow<=2.15.0 (from versions: 2.16.0rc0, 2.16.1)\r\nERROR: No matching distribution found for tensorflow<=2.15.0\r\n```\r\nI then tried to install 2.15 from source but since I don't have 3 phds I cannot decipher the reasons why it fails (after taking 4 hours to download, unzip and then compile).\r\n\r\nI'm at a total loss at what to do... if there was just a wheel available for some version below 2.15.1 or just more compiled c++ bindings available for something other than 2.15.0 then I could not be wasting my time.\r\n\r\nThe problem is even worse if I try and use `and-cuda` (but one thing at a time?). \n\n### Standalone code to reproduce the issue\n\n```shell\npip install \"tensorflow<=2.15.0\"\n```\n\n\n### Relevant log output\n\n```shell\nERROR: Could not find a version that satisfies the requirement tensorflow<=2.15.0 (from versions: 2.16.0rc0, 2.16.1)\r\nERROR: No matching distribution found for tensorflow<=2.15.0\n```\n",
    "comments": [
      {
        "user": "edwardyehuang",
        "body": "What is ur python version?"
      },
      {
        "user": "peter-fm",
        "body": "3.11. I think I tried 3.10 at some point but it didn't seem to help."
      },
      {
        "user": "winsweba",
        "body": "Please have you try to use python version **3.9** for the installing of the tensorflow "
      }
    ]
  },
  {
    "issue_number": 95222,
    "title": "how to build libtensorflowlite_c.so with Address Sanitizer",
    "author": "xin486946",
    "state": "open",
    "created_at": "2025-06-12T01:26:07Z",
    "updated_at": "2025-06-12T20:22:41Z",
    "labels": [
      "stat:awaiting response",
      "type:build/install",
      "comp:lite",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nlatest or 2.13\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nhow to build libtensorflowlite_c.so with Address Sanitizer?\n\n### Standalone code to reproduce the issue\n\n```shell\nhow to build libtensorflowlite_c.so with Address Sanitizer\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "HI, @xin486946 \nThank you for bringing this issue to our attention, Please follow this [official documentation](https://www.tensorflow.org/install/source) after this `./configure` (select by default option with Clang option) then use below bazel commad for address Sanitizer\n\n`bazel build --config=asan -c opt //tensorflow/lite/c:libtensorflowlite_c.so` if this command does not work please try below command for more information about AddressSanitizerFlags refer this [documentation](https://github.com/google/sanitizers/wiki/addresssanitizerflags)\n\n```\nbazel build \\\n    -c dbg \\\n    --copt=\"-fsanitize=address\" \\\n    --copt=\"-fno-omit-frame-pointer\" \\\n    --linkopt=\"-fsanitize=address\" \\\n    --strip=never \\\n    --jobs=$(nproc) \\\n    //tensorflow/lite/c:libtensorflowlite_c.so\n```\n\nPlease give it try and let us know is it working as expected or not ? If I have missed something here please let me know. Thank you for your cooperation and understanding."
      }
    ]
  },
  {
    "issue_number": 95106,
    "title": "TensorFlow disables SwiftUI Previews",
    "author": "muzamil8101",
    "state": "open",
    "created_at": "2025-06-10T05:29:19Z",
    "updated_at": "2025-06-12T18:38:01Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:lite",
      "2.17"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n'TensorFlowLiteSwift', '~> 2.17.0'\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\niOS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nAfter adding tensor flow library to iOS App, SwiftUI previews stop working. Though it seems fine when running on a device or a simulator but run time SwiftUI Previews stop working which they shouldn't.\n\n### Standalone code to reproduce the issue\n\n```shell\nJust create a simple SwiftUI app and then add TensorFlowLite Framework in the app. \nYou will see that swiftUI Previews will stop working\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @muzamil8101 \nThank you for bringing this issue to our attention, I'll try to reproduce the same behavior from my end and will update you soon. Thank you for your cooperation and understanding."
      }
    ]
  },
  {
    "issue_number": 60629,
    "title": "control_flow_ops_test unit test is flaky",
    "author": "elfringham",
    "state": "open",
    "created_at": "2023-05-18T15:22:49Z",
    "updated_at": "2025-06-12T08:15:04Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "type:build/install",
      "comp:ops",
      "subtype: ubuntu/linux"
    ],
    "body": "<details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n5.3.0\n\n### GCC/Compiler version\n\n10.2.1\n\n### CUDA/cuDNN version\n\nn/a\n\n### GPU model and memory\n\nn/a\n\n### Current Behaviour?\n\n//tensorflow/python/ops/parallel_for:control_flow_ops_test fails occasionally due to difference exceeding tolerance.\r\n\r\nSee https://github.com/tensorflow/tensorflow/actions/runs/5012758324/jobs/8985082872#step:5:29789\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel test --build_tests_only --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --local_test_jobs=64 --test_tag_filters=-nopip,-no_pip,-oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 -k -- //bazel_pip/tensorflow/... -//bazel_pip/tensorflow/compiler/tf2tensorrt/... -//bazel_pip/tensorflow/compiler/xrt/... -//bazel_pip/tensorflow/core/tpu/... -//bazel_pip/tensorflow/go/... -//bazel_pip/tensorflow/java/... -//bazel_pip/tensorflow/python/integration_testing/... -//bazel_pip/tensorflow/tools/toolchains/... -//bazel_pip/tensorflow/lite/... -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:conv_ops_test\n```\n\n\n### Relevant log output\n\n```shell\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=1e-05\r\nMismatched value: a is different from b. \r\nnot close where = (array([0]), array([0]), array([0]), array([1]), array([4]), array([0]))\r\nnot close lhs = [0.]\r\nnot close rhs = [0.77603436]\r\nnot close dif = [0.77603436]\r\nnot close tol = [8.760343e-05]\r\ndtype = float32, shape = (3, 3, 2, 12, 12, 3)\r\nMismatched elements: 1 / 7776 (0.0129%)\r\nMax absolute difference: 0.77603436\r\nMax relative difference: 1.\r\n x: array([[[[[[0.      , 0.      , 0.712515],\r\n           [0.      , 0.889897, 0.      ],\r\n           [0.      , 0.      , 0.      ],...\r\n y: array([[[[[[0.      , 0.      , 0.712515],\r\n           [0.      , 0.889897, 0.      ],\r\n           [0.      , 0.      , 0.      ],...\n```\n</details>",
    "comments": [
      {
        "user": "elfringham",
        "body": "Fixed by merge of #60638 "
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/60629\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/60629\">No</a>\n"
      },
      {
        "user": "elfringham",
        "body": "#60638 was undone along with removing flaky flag from other tests, so this issue still needs addressing."
      }
    ]
  },
  {
    "issue_number": 60599,
    "title": "Weird memory usage of shuffling in `tf.data.Dataset` ",
    "author": "massquantity",
    "state": "open",
    "created_at": "2023-05-15T14:54:49Z",
    "updated_at": "2025-06-12T07:43:08Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "comp:data",
      "type:performance",
      "TF 2.12"
    ],
    "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.12\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nUbuntu 22.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.8\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nHi, I've read the [tf.data doc](https://www.tensorflow.org/guide/data#randomly_shuffling_input_data), which says using a large `buffer size` in data shuffling is not recommended, but still the shuffling behavior costs way more memory than I would expect. For example, I expect the full shuffling of 50,000,000 `int` data may only use 1 GB of memory, but the following code after `print` essentially uses 10 GB.\r\n\r\nThis leads to some practical concerns. If I set `buffer_size=1024` in data shuffling, would the *actual* memory usage of the buffer size be 10 times that of 1024 elements?\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndata_size = 50000000\r\ntf_dataset = tf.data.Dataset.from_tensor_slices(np.arange(data_size))\r\ntf_dataset = iter(tf_dataset.shuffle(data_size))\r\nprint(next(tf_dataset))\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n<tf.Tensor: shape=(), dtype=int64, numpy=24774043>\r\n```\r\n</details>",
    "comments": [
      {
        "user": "nitya-khuntia",
        "body": "Hi massquantity,\r\n\r\nAfter reviewing your issue and the code you provided, I believe I have found a solution that can help reduce the memory usage during data shuffling in TensorFlow.\r\n\r\nThe key to reducing memory usage is to explicitly set the buffer size to a reasonable value. To resolve this issue, you can explicitly set the buffer_size to a smaller value that is appropriate for your use case. As a rule of thumb, the buffer size should be set to a value that is small enough to fit into memory but large enough to provide good shuffling performance. A buffer size of 1024, as you mentioned, may be too small and may result in poor shuffling performance. A buffer size of 10000 is generally a good default value.\r\n\r\nyou can use the prefetch() method in conjunction with shuffle(). This method allows you to overlap the shuffling process with other parts of your data pipeline, which can help reduce memory usage. \r\n\r\nHere's an updated version of your code that incorporates these changes:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndata_size = 50000000\r\n\r\ntf_dataset = tf.data.Dataset.from_tensor_slices(np.arange(data_size))\r\ntf_dataset = tf_dataset.shuffle(buffer_size=10000).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\ntf_dataset = iter(tf_dataset)\r\n\r\nprint(next(tf_dataset))\r\n```\r\n**Memory Usage**\r\n![Figure_3](https://github.com/tensorflow/tensorflow/assets/83278020/1bfa294d-0791-4f4d-9763-061f454e1d7a)\r\n\r\nI tested this code on my machine and found that it reduces memory usage to less than 500MiB.\r\nPlease let me know if this solution works for you. \r\n\r\nBest regards,\r\nNitya"
      },
      {
        "user": "massquantity",
        "body": "Thanks to Nitya, for your kind and detailed explanation! I'm also aware that the buffer_size should be set as large as possible for shuffling performance. `buffer_size=1024` is just an example, and what I'm seeking in this issue is some clarification on the memory usage of buffer_size. \r\n\r\nWhen a user set `buffer_size=1024`, she expects the underlying code will only use the memory of 1024 element rather than the memory of equivalent to 10240 elements, which is a \"surprise\" we generally want to avoid when using an open source library like TensorFlow. That's why I'm surprised when seeing the full shuffling of 50,000,000 `int` data cost 10 GB of memory.\r\n\r\nIf the code uses more memory than the specified buffer_size, that's totally OK to me, provided that the doc is clear about this. But the [shuffle doc](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) says \"This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements.\"  From my understanding, it only uses `buffer_size` elements of memory."
      },
      {
        "user": "grofte",
        "body": "@nitya-khuntia \r\n\r\nIf the `.shuffle(10_000)` operation increases memory usage by 500 megabyte then each number is using up around 50 kilobyte each. They should not. If we assume they are 64 bit numbers then they should be using 8 byte plus overhead. If the overhead is 50'000 byte minus 8 byte then that seems like a lot. It's about 6000 x the actual data."
      }
    ]
  },
  {
    "issue_number": 94534,
    "title": "Looking for the the reasoning speed comparison of different reaasoning frameworks",
    "author": "YushengEdward",
    "state": "closed",
    "created_at": "2025-05-30T09:16:04Z",
    "updated_at": "2025-06-12T07:08:26Z",
    "labels": [
      "comp:lite"
    ],
    "body": "Hello！\nI'm seachering for the reasoning speed comparison of the same model in different reasoning frameworks on mobile devices. I have looked for many places but didn't find what i wanted.\nLooking for your reply！\n",
    "comments": [
      {
        "user": "bandirevanth",
        "body": "While TFLite often provides superior inference speeds on Android devices, especially with GPU acceleration, Core ML excels on iOS due to its deep integration with Apple's hardware. PyTorch Mobile and ONNX Runtime Mobile offer flexibility and cross-platform capabilities but may require additional optimisation to match the performance of TFLite or Core ML. [Please refer: [Learn More](https://peerdh.com/blogs/programming-insights/comparing-tensorflow-lite-with-other-mobile-ml-frameworks-on-android)]\n\n| **Framework**       | **Model**       | **Device**           | **Inference Time (ms)**   | **Notes**                                                                 |\n|---------------------|-----------------|-----------------------|----------------------------|--------------------------------------------------------------------------|\n| **TensorFlow Lite** | MobileNetV3     | Samsung Galaxy S24    | 9.7 (FP32) / 5.6 (INT8)    | Quantization significantly reduces inference time.                      |\n| **PyTorch Mobile**  | YOLOv5          | iPhone                | 910                        | CoreML outperforms PyTorch Mobile on iOS for the same model.            |\n| **Core ML**         | YOLOv5          | iPhone                | 230                        | Leverages Apple's Neural Engine for high-speed inference.               |\n| **ONNX Runtime**    | MobileNet       | Mid-range Android     | <15                        | Demonstrates competitive performance with optimized hardware support.   |\n\nPhone-specific:\n| Model / Framework          | Device Example               | Reasoning/Inference Speed   | Notes                                                   |\n|---------------------------|-----------------------------|----------------------------|---------------------------------------------------------|\n| **MobileNetV2 (TFLite)**   | Google Pixel 5 (Snapdragon 765G) | ~20-40 ms per inference    | Lightweight CNN, TFLite optimized, uses NNAPI on Android |\n| **BERT Base (DistilBERT TFLite)** | Samsung Galaxy S21 (Exynos 2100) | ~150-300 ms per inference  | Transformer-based model, quantized for mobile            |\n| **ONNX Runtime (ResNet50)**| iPhone 13 (A15 Bionic)        | ~30-50 ms per inference     | Uses Core ML acceleration                                 |\n| **Prolog Interpreter (SWI-Prolog)** | Any mid-range phone            | Seconds to tens of seconds  | Pure symbolic reasoning, interpreted, not optimized for mobile |\n\n\nBecause of your use case, I recommend benchmarking your model on these frameworks on your target devices, considering the performance, model size, and development effort trade-offs.\n\nHope it helps :)"
      },
      {
        "user": "mihaimaruseac",
        "body": "Since this is not an issue related to the code itself, the question does not really belong here. Stack Overflow or reddit would be better places."
      },
      {
        "user": "YushengEdward",
        "body": "Did you have tried ncnn framework to reason?\nIn addition, do you use the NPU to reason when you use the above frameworks?"
      }
    ]
  },
  {
    "issue_number": 60570,
    "title": "[TFLite] flatbuffer64 support for TFlite",
    "author": "huanyingjun",
    "state": "open",
    "created_at": "2023-05-11T03:02:39Z",
    "updated_at": "2025-06-12T06:01:38Z",
    "labels": [
      "stat:awaiting response",
      "type:feature",
      "TFLiteConverter",
      "TF 2.12"
    ],
    "body": "Dear,\r\nflatbuffer has limit of 2G size.\r\nbut for now, many models like stable-diffusion, llama has the size larger than 2G, can not be convertted to tflite.\r\nIs there any plan TFlite update to flatbuffer64 ?\r\n\r\nThanks",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi,\n\nThank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base.\n\nThe Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow [version](https://pypi.org/project/tensorflow/) with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate.\n\nPlease follow the [release notes](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md) to stay up to date with the latest developments which are happening in the Tensorflow space."
      }
    ]
  },
  {
    "issue_number": 60583,
    "title": "rejection_resample loses track of ragged tensors",
    "author": "chris-remedy",
    "state": "open",
    "created_at": "2023-05-12T10:16:39Z",
    "updated_at": "2025-06-12T06:01:24Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "comp:data",
      "TF 2.12"
    ],
    "body": "<details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.14.0-dev20230512\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04.6\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.14\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nA `tf.data.Dataset` initialized from RaggedTensors normally will successfully batch into ragged batches. However after passing it through `rejection_resample`, it loses track of which input tensors were ragged, and so batching fails.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput = tf.ragged.constant([[1,2,3], [4,5], [7,8,9]])\r\nds = tf.data.Dataset.from_tensor_slices(input)\r\ntf.random.set_seed(0)\r\n# Removing this line makes everything work fine\r\nds = ds.rejection_resample(\r\n    class_func=lambda t: 1,\r\n    target_dist=(0.1, 0.9),\r\n)\r\nds = ds.batch(2)\r\nds.take(1).get_single_element()\n```\n\n\n### Relevant log output\n\n```shell\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DatasetToSingleElement_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 1. First element had shape [3] and element 1 had shape [2]. [Op:DatasetToSingleElement] name:\n```\n</details>",
    "comments": [
      {
        "user": "chris-remedy",
        "body": "It appears this can be worked around like this:\r\n```python\r\ninput = tf.ragged.constant([[1, 2, 3], [4, 5], [7, 8, 9]])\r\nds = tf.data.Dataset.from_tensor_slices(input)\r\noriginal_spec = ds.element_spec\r\ntf.random.set_seed(0)\r\nds = ds.rejection_resample(\r\n    class_func=lambda t: 1,\r\n    target_dist=(0.1, 0.9),\r\n).map(lambda class_func_result, data: data)\r\nds = ds.batch(1).map(lambda elem: tf.map_fn(\r\n      fn=lambda e: e,\r\n      elems=elem,\r\n      fn_output_signature=original_spec,\r\n  )).unbatch()\r\nds = ds.batch(2)\r\nds.take(1).get_single_element()\r\n```\r\n\r\nBut that's pretty annoying. IDK if there's a shorter way to express the same thing.\r\n\r\nAlso, it appears that the same problem applies generally to `Dataset.map`, i.e. this breaks in the same way:\r\n```python\r\ninput = tf.ragged.constant([[1, 2, 3], [4, 5], [7, 8, 9]])\r\nds = tf.data.Dataset.from_tensor_slices(input)\r\nds = ds.map(lambda e: e)\r\nds = ds.batch(2)\r\nds.take(1).get_single_element()\r\n```"
      },
      {
        "user": "SuryanarayanaY",
        "body": "Hi @chris-remedy ,\r\n\r\nI have replicated the reported behaviour and attached [gist](https://colab.research.google.com/gist/SuryanarayanaY/6e2138c261058cb31dfae033f5a7061b/60583.ipynb) for reference. This needs to dig into more to confirm the root cause. Thanks!"
      },
      {
        "user": "Venkat6871",
        "body": "Hi,\n\nThank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base.\n\nThe Tensorflow team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow [version](https://pypi.org/project/tensorflow/) with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate.\n\nPlease follow the [release notes](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md) to stay up to date with the latest developments which are happening in the Tensorflow space."
      }
    ]
  },
  {
    "issue_number": 94030,
    "title": "AttributeError with Protobuf >= 6.30",
    "author": "sevu",
    "state": "open",
    "created_at": "2025-05-23T17:02:30Z",
    "updated_at": "2025-06-12T04:00:04Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ncurrent master   v1.12.1-126604-g3d72b9f063c   2.20.0-dev0+selfbuilt\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 25.04 +venv\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.13\n\n### Bazel version\n\n7,4.1\n\n### GCC/compiler version\n\nclang 20.1\n\n### CUDA/cuDNN version\n\n-\n\n### GPU model and memory\n\n-\n\n### Current behavior?\n\nHave a version >= 6.30.0 of python-protobuf, such as 6.31.0\nWhen importing tensorflow the error message is displayed.\n\n\n### Standalone code to reproduce the issue\n\n```shell\npython -c \"import tensorflow\"\n```\n\n### Relevant log output\n\n```shell\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n```",
    "comments": [
      {
        "user": "ybaturina",
        "body": "FYI - the issue was introduced by https://github.com/tensorflow/tensorflow/pull/91651."
      }
    ]
  },
  {
    "issue_number": 94274,
    "title": "crash when two model parallel inference in two instances using libtensorflowlite_c.so and run delegate gpu opencl",
    "author": "xin486946",
    "state": "open",
    "created_at": "2025-05-27T10:39:05Z",
    "updated_at": "2025-06-12T02:13:33Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:lite",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nlatest and 2.13 all\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nandroid\n\n### Mobile device\n\nandroid phone arm mali gpu\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nno crash\n\n### Standalone code to reproduce the issue\n\n```shell\nF libc    : Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 20647 (seg_blur_tflite), pid 20207 (seg_blur_tflite)\nF DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\nF DEBUG   : Build fingerprint: 'UNISOC/ums9360_1h10_64only/ums9360_1h10:15/AP3A.240905.015.A2/20427:userdebug/test-keys'\nF DEBUG   : Revision: '0'\nF DEBUG   : ABI: 'arm64'\nF DEBUG   : Timestamp: 2025-05-17 10:37:56.673952136+0800\nF DEBUG   : Process uptime: 11s\nF DEBUG   : Cmdline: ./seg_blur_tflite.bin case\nF DEBUG   : pid: 20207, tid: 20647, name: seg_blur_tflite  >>> ./seg_blur_tflite.bin <<<\nF DEBUG   : uid: 0\nF DEBUG   : tagged_addr_ctrl: 0000000000000001 (PR_TAGGED_ADDR_ENABLE)\nF DEBUG   : signal 6 (SIGABRT), code -1 (SI_QUEUE), fault addr --------\nF DEBUG   : Abort message: 'Pointer tag for 0xffffffff was truncated, see 'https://source.android.com/devices/tech/debug/tagged-pointers'.'\nF DEBUG   :     x0  0000000000000000  x1  00000000000050a7  x2  0000000000000006  x3  000000753a256800\nF DEBUG   :     x4  731f7260761f6565  x5  731f7260761f6565  x6  731f7260761f6565  x7  7f7f7f7f7f7f7f7f\nF DEBUG   :     x8  00000000000000f0  x9  00000077dc23a468  x10 0000000000000001  x11 00000077dc28c070\nF DEBUG   :     x12 000000006827f684  x13 000000007fffffff  x14 0000000001cad9a6  x15 000004ae27a580d2\nF DEBUG   :     x16 00000077dc2f5fe8  x17 00000077dc2dfc00  x18 0000007525f6c000  x19 0000000000004eef\nF DEBUG   :     x20 00000000000050a7  x21 00000000ffffffff  x22 b4000076b1cb6640  x23 0000000000004eef\nF DEBUG   :     x24 000000753a256c80  x25 000000753a256c80  x26 000000753a256f90  x27 b4000076c16e6110\nF DEBUG   :     x28 0000000000104000  x29 000000753a256880\nF DEBUG   :     lr  00000077dc274998  sp  000000753a2567e0  pc  00000077dc2749c4  pst 0000000000001000\nF DEBUG   : 25 total frames\nF DEBUG   : backtrace:\nF DEBUG   :       #00 pc 000000000005d9c4  /apex/com.android.runtime/lib64/bionic/libc.so (abort+164) (BuildId: e4078f1fca32b8a35fbc313abda67be6)\nF DEBUG   :       #01 pc 00000000000454f8  /apex/com.android.runtime/lib64/bionic/libc.so (free+104) (BuildId: e4078f1fca32b8a35fbc313abda67be6)\nF DEBUG   :       #02 pc 0000000000194d9c  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #03 pc 0000000000194de8  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #04 pc 000000000009a324  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #05 pc 000000000009a238  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #06 pc 0000000000099f1c  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #07 pc 000000000009782c  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #08 pc 0000000000097864  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #09 pc 000000000003ab78  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #10 pc 000000000003eda4  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #11 pc 00000000004a7560  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #12 pc 00000000004a72d4  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #13 pc 00000000004a7588  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #14 pc 000000000048f998  /vendor/lib64/libtensorflowlite_c.so\nF DEBUG   :       #15 pc 0000000000211330  /vendor/lib64/libtensorflowlite_c.so (TfLiteInterpreterDelete+36)\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "bandirevanth",
        "body": "Potential fixes:\n- Serial Execution: Run model inferences sequentially to avoid concurrent access to shared GPU resources.\n- Separate Processes: Execute each inference in a separate process to isolate memory spaces.\n- CPU Delegate: Use the CPU delegate for inference, which may not exhibit the same concurrency issues.\n- Custom Delegates: Develop or utilize custom delegates that handle concurrency more gracefully\n\nHope it helps :)"
      },
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @xin486946 \nI apologize for the delay in my response, To help us investigate the issue you're encountering, could you please provide a minimal reproducible example? A GitHub repository containing the following would be ideal:\n\n- Minimal code that demonstrates the behavior you're observing.\n- Your .tflite model ( if possible)\n- Complete steps to reproduce the issue from our end.\n\nThis will allow us to quickly and accurately identify the root cause. We appreciate your patience and cooperation!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      }
    ]
  },
  {
    "issue_number": 60547,
    "title": "tf.linalg.matrix_rank  results has different results with or without @tf.function for numpy inputs under tensorflow-cpu",
    "author": "jiannanWang",
    "state": "open",
    "created_at": "2023-05-09T19:42:32Z",
    "updated_at": "2025-06-11T14:36:18Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "comp:apis",
      "comp:tf.function"
    ],
    "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.14.0-dev20230509\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\nN/A\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n`tf.linalg.matrix_rank` has different results with or without `@tf.function` when the input is a numpy tensor and **tensorFlow-cpu** is used. \r\n\r\nInterestingly, this issue does not occur when the numpy array is explicitly converted to a TensorFlow tensor before being passed as an argument to tf.linalg.matrix_rank. This explicit conversion shouldn't be necessary, as per the TensorFlow tutorial (https://www.tensorflow.org/tutorials/customization/basics#:~:text=TensorFlow%20operations%20automatically%20convert%20NumPy%20ndarrays%20to%20Tensors), which states that \"TensorFlow operations automatically convert NumPy ndarrays to Tensors\". This discrepancy seems to indicate a bug that prevents the utilization of this automatic conversion feature.\r\n\r\nThis issue was previously raised and discussed under issue (#57959), where the proposed solution was the explicit conversion of numpy arrays to TensorFlow tensors. While this solution works, it does not align with the functionality of TensorFlow's automatic conversion of numpy arrays to tensors, and it requires users to perform an additional step that should not be necessary.\r\n\r\nIn essence, this bug seems to affect the user's ability to leverage TensorFlow's automatic conversion of numpy arrays to tensors, particularly when using TensorFlow-CPU.\r\n\r\nI open this issue because the same behavior still exists in the latest nightly version and I believe it should not be a user issue according to the tutorial.\r\n\r\nThe reproduction colab link is here: https://colab.research.google.com/drive/1wEYxe5b-m7_3pqBP1iTrjSvydMd_jD_B?usp=sharing. \r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\ninput = {'name': 'matrix_rank', 'a': np.array([[-7.24721292e+307,  4.66389010e+307, -5.40181227e+307,\r\n         7.28793100e+307,  5.19885794e+307],\r\n       [-5.74381106e+307,  2.21923437e+307,  4.96898538e+307,\r\n         4.26402766e+307,  7.42174751e+307],\r\n       [-2.62810171e+307,  1.71425915e+307, -6.99349881e+307,\r\n        -8.11519519e+307,  4.04358640e+307],\r\n       [-8.52726304e+307,  1.44214314e+307, -4.53927548e+307,\r\n        -4.79571993e+307, -4.59672928e+307]])}\r\nprint(input['a'].dtype)\r\n\r\noutput1 = tf.linalg.matrix_rank(**input)\r\nprint(output1)\r\n\r\n@tf.function\r\ndef fun_wrapper(x):\r\n    return tf.linalg.matrix_rank(**x)\r\n\r\noutput2 = fun_wrapper(input)\r\nprint(output2)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2.14.0-dev20230509\r\nfloat64\r\ntf.Tensor(0, shape=(), dtype=int32)\r\ntf.Tensor(4, shape=(), dtype=int32)\r\n```\r\n</details>",
    "comments": [
      {
        "user": "synandi",
        "body": "In the first case, `tf.linalg.matrix_rank(**input)` is executed eagerly, meaning that the computation is immediately performed when the code is run. This is the default execution model in TensorFlow 2.x.\r\nIn the second case, `fun_wrapper(input)` is decorated with `tf.function`, which compiles the function into a TensorFlow graph. This graph is then executed using the TensorFlow runtime\r\nTo ensure consistency in the output, you can explicitly convert the numpy array to a TensorFlow tensor before passing it to `tf.linalg.matrix_rank`. For example:\r\n```\r\ninput = {'name': 'matrix_rank', 'a': tf.constant(np.array([[-7.24721292e+307,  4.66389010e+307, -5.40181227e+307,\r\n         7.28793100e+307,  5.19885794e+307],\r\n       [-5.74381106e+307,  2.21923437e+307,  4.96898538e+307,\r\n         4.26402766e+307,  7.42174751e+307],\r\n       [-2.62810171e+307,  1.71425915e+307, -6.99349881e+307,\r\n        -8.11519519e+307,  4.04358640e+307],\r\n       [-8.52726304e+307,  1.44214314e+307, -4.53927548e+307,\r\n        -4.79571993e+307, -4.59672928e+307]]))}\r\n```\r\nPlease refer to the gist with the same output in both the cases [here](https://colab.sandbox.google.com/gist/synandi/89f91d42bf0a7e2e5add961b67b88d2b/60547_nightly.ipynb). Thank you!"
      },
      {
        "user": "jiannanWang",
        "body": "Hi synandi,\r\n\r\nThank you for your response! \r\n\r\nAs stated in the issue, I'm wondering why the user needs to do the conversion explicitly. Because in the tutorial (https://www.tensorflow.org/tutorials/customization/basics#:~:text=TensorFlow%20operations%20automatically%20convert%20NumPy%20ndarrays%20to%20Tensors), it states that \"TensorFlow operations automatically convert NumPy ndarrays to Tensors\". I would assume TensorFlow would handle numpy inputs by automatically converting it to tensors, instead of requiring the users to explicitly do the conversion."
      },
      {
        "user": "Venkat6871",
        "body": "Hi @jiannanWang ,\nApologies for the delay, and thank you for your patience. The main cause of the difference in results seems to be the extremely large input values. This happens due to how TensorFlow handles NumPy inputs inside `@tf.function`. When a NumPy array is passed, it gets converted into a TensorFlow constant, and the internal representation might differ slightly, especially for large numbers.\nTo overcome this, you can explicitly convert the input to a TensorFlow tensor before passing it to the function. I tested the following code with more reasonable values, and it works fine on CPU.\nHere i am providing [gist](https://colab.sandbox.google.com/gist/Venkat6871/3703e5bc06dee882fc16e631c2152473/60547_tf_2-18-0-2-19-0.ipynb) for your reference. I hope this will helps.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 60539,
    "title": "Support/Feature Request: Pre-processing very large corpus text file as tokens to train GPT Models.",
    "author": "abhaskumarsinha",
    "state": "open",
    "created_at": "2023-05-08T18:34:35Z",
    "updated_at": "2025-06-11T13:53:10Z",
    "labels": [
      "stat:awaiting response",
      "type:feature",
      "type:support",
      "comp:data",
      "TF 2.12"
    ],
    "body": "<details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nFeature Request\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\nv2.9.0-18-gd8ce9f9c301 2.9.1\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nWindows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.5\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nSuppose I've a very simple Python code like this:\r\n\r\n\r\n```\r\ncorpus = file.read()\r\n            file_contents = corpus.split()[token_start : token_end]\r\n\r\n\r\ninput_tokens, output_tokens = [], []\r\n        for i in tqdm(range(len(file_contents) - gpt_input - 1)):\r\n            input_tokens += [file_contents[i : i + gpt_input]]\r\n            output_tokens += [file_contents[i + gpt_input]]\r\n               \r\n            \r\n        X = [' '.join(input_tokens[i]) for i in tqdm(range(len(input_tokens)))]\r\n        Y = output_tokens\r\n```\r\nThe code does three things:\r\n\r\n1. Load a file into RAM, split the contents of the file into words, i.e. - we have a list of words from the file in the order of the sentences.\r\n2. Next, use two variables - input_tokens, output_tokens as list and append list of first `gpt_input` words in input_token and `gpt-input`-th word in output_token. This ensures that we have all `i` to `i + gpt_input` words in input_tokens and `i + 1` tokens in output_tokens, for all i = 0 to i = `total_tokens - 1`.\r\n3. Now, we reconstruct sentences with words input_tokens, i.e. - we condensate gpt_input words back to the sentences.\r\n\r\nExample:\r\n\r\nIf the file has contents  like this:\r\n```\r\nHello World, I'm writing a new cool code in TensorFlow, please don't forget to check it!\r\n```\r\n\r\nThe end result:\r\ninput_tokens for gpt_input = 3:\r\n```\r\nHello World, I'm\r\nWorld, I'm writing\r\nI'm writing a\r\nwriting a new\r\na new cool\r\n...\r\n```\r\noutput_tokens for gpt_input = 3:\r\n```\r\nwriting\r\na\r\nnew\r\ncool\r\ncode\r\n...\r\n```\r\n\r\nSo, now the problem is - the file or the text corpus which is needed to train a GPT Model can be very large! like upto - 200-300 GB and can't be loaded into RAM/memory directly. So, TensorFlow offers - tf.data class, with the set of tools to help loading, caching and training from very large datasets. But the problem is that, I don't see any way to create and pre-process text file corpus using tf.data class from the documentation. To me, it seems pretty much impossible to do. If there is any way to load corpus fragments with a window size defined by words, kindly let me know.\r\n\r\nThank you in advance.\n\n### Standalone code to reproduce the issue\n\n```shell\ncorpus = file.read()\r\n            file_contents = corpus.split()[token_start : token_end]\r\n\r\ninput_tokens, output_tokens = [], []\r\n        for i in tqdm(range(len(file_contents) - gpt_input - 1)):\r\n            input_tokens += [file_contents[i : i + gpt_input]]\r\n            output_tokens += [file_contents[i + gpt_input]]\r\n               \r\n            \r\n        X = [' '.join(input_tokens[i]) for i in tqdm(range(len(input_tokens)))]\r\n        Y = output_tokens\n```\n\n\n### Relevant log output\n\n_No response_</details>",
    "comments": [
      {
        "user": "SuryanarayanaY",
        "body": "Hi @abhaskumarsinha ,\r\n\r\nIf the text data is in a directory then you can use the API `tf.keras.utils.text_dataset_from_directory` to generate a tf.data.Dataset object from text files in the directory. Please refer the API [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory) for more details.\r\n\r\nYou can also refer this documentation [guide](https://www.tensorflow.org/guide/data?_gl=1*1orjui2*_ga*MTM4Mzg1ODAxNi4xNjY1OTkwMTQ1*_ga_W0YLR4190T*MTY4MzYxMTczMi44ODMuMS4xNjgzNjEzMjc0LjAuMC4w#consuming_text_data) on how to consume text data.\r\n\r\nPlease refer to an end to end tutorial for handling text data [here](https://www.tensorflow.org/tutorials/load_data/text).\r\n\r\nPlease go through and let us know if this is helpful. Thanks!"
      },
      {
        "user": "abhaskumarsinha",
        "body": "Hello @SuryanarayanaY \r\n\r\nThank you for your response.\r\n\r\nI've already checked all of them before and I don't see how they would be helpful to me in this case.\r\n1.  `tf.keras.utils.text_dataset_from_directory` loads the text from different files. The output is an array of strings in the order of text. This is typically in contrast to my requirement - where I require output in the form of an array of tokens (or words, separated with a spacebar), not a sentence. For example: `[b'Hello', b'World!', b'I', b'love', b'TensorFlow']` and then pre-process them back to sentence and token (as in the example in my first post).\r\n2. The second guide link on consuming text data is similar. It processes the data in the form of sentences and not tokens.\r\n3. I've also checked the handling text data tutorial before. The goal of the preprocessing I require is fundamentally different from the one discussed in the tutorial.\r\n\r\nI also noticed `tf.strings.split` method to split the strings and map each sentence to one token, **this is again different from my requirement, where I actually require a flattened version of the tf.strings.split output** in the form of an array [1, m].\r\n\r\nOne way I can think to solve the issue is to write a generator function in Python that yields a pre-processed tokenized array of text tokens in the required array by reading a file in fragments each time and moving to the next iterator corresponding to the next fragment of the file, and use this generator with `tf.data`. But again, Python natively offers no such feature to **read files in the fragment of tokens** (but rather bytes, which I don't require)."
      },
      {
        "user": "abhaskumarsinha",
        "body": "Hello @SuryanarayanaY \r\n\r\nThe best I've got to do till now is this code:\r\n\r\n```\r\ndataset = './dataset/output_dataset.txt'\r\n\r\ndata = tf.data.TextLineDataset(dataset)\r\n\r\ndef split(string):\r\n    return tf.strings.split(string, sep=' ')\r\n\r\ndef filter_empty_string(string):\r\n    if tf.strings.length(tf.strings.reduce_join(string)) == 0:\r\n        return False\r\n    else:\r\n        return True\r\n\r\ndata.map(split).filter(filter_empty_string)\r\n\r\nfor i in data.map(split).filter(filter_empty_string).take(10):\r\n    print(i.numpy())\r\n```\r\n\r\n\r\nwhich produces an output like this:\r\n```\r\n[b'***' b'START' b'OF' b'THIS' b'PROJECT' b'GUTENBERG' b'EBOOK' b'COM@@'\r\n b'ING' b'AT@@' b'TRA@@' b'C@@' b'TION' b'***']\r\n[b'Produced' b'by' b'Greg' b'Weeks,' b'Mary' b'Me@@' b'ehan' b'and' b'the'\r\n b'Online']\r\n[b'Distributed' b'Proofreading' b'Team' b'at' b'http://www.pgdp.net']\r\n[b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\r\n b'' b'' b'' b'' b'' b'' b'' b'' b'' b'Coming' b'At@@' b'trac@@' b'tion']\r\n[b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\r\n b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'BY' b'F@@' b'R@@' b'IT@@' b'Z'\r\n b'LE@@' b'I@@' b'B@@' b'ER']\r\n[b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\r\n b'' b'' b'' b'' b'' b'Illustr@@' b'ated' b'by' b'Paul' b'C@@' b'alle']\r\n[b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'[T@@' b'ran@@' b\"scriber's\"\r\n b'Note:' b'This' b'et@@' b'ext' b'was' b'produced' b'from']\r\n[b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b'' b''\r\n b'Gal@@' b'ax@@' b'y' b'Sci@@' b'ence' b'F@@' b'ic@@' b'tion' b'November'\r\n b'19@@' b'50@@' b'.']\r\n[b'' b'' b'' b'' b'' b'' b'' b'' b'' b'Extensive' b'research' b'did'\r\n b'not' b'uncover' b'any' b'evidence' b'that']\r\n[b'' b'' b'' b'' b'' b'' b'' b'' b'' b'the' b'U.S.' b'copyright' b'on'\r\n b'this' b'publication' b'was' b'rene@@' b'we@@' b'd.]']\r\n```\r\n\r\nThis is far, far from what I need, that you can see in the example I've provided in the very first post. Somewhat similar to this:\r\n```\r\n[b'*** START OF' ], [b'THIS'] \r\n[b'START OF THIS'], [b'PROJECT']\r\n[b'OF THIS PROJECT'], [b'GUTENBERG'] \r\n...\r\n```\r\n\r\nfor `gpt_input = 3`"
      }
    ]
  },
  {
    "issue_number": 53341,
    "title": "failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error",
    "author": "NeilChangYanpeng",
    "state": "closed",
    "created_at": "2021-12-07T11:10:53Z",
    "updated_at": "2025-06-11T13:20:00Z",
    "labels": [
      "type:build/install",
      "subtype: ubuntu/linux",
      "TF 1.14",
      "TF 2.4"
    ],
    "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.6.13\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 10.0.130/7.4.2\r\n- GPU model and memory: GTX 2080Ti  12GB\r\n Output of nvidia-smi：\r\n`ubuntu@init:~$ nvidia-smi\r\nTue Dec  7 10:01:07 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 30%   29C    P8    16W / 250W |     41MiB / 11016MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n`\r\n\r\n**Describe the problem**\r\n\r\nAfter I installed tensorflow-gpu using pip，I tested it in terminal，but it seems gpu is not available\r\n\r\n`(tfgpu114) ubuntu@init:~$ python\r\nPython 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) \r\n[GCC 7.5.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> tensorflow.test.is_gpu_available()\r\n2021-12-07 09:01:22.849145: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2021-12-07 09:01:22.888771: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2021-12-07 09:01:22.949108: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n2021-12-07 09:01:22.949204: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: init\r\n2021-12-07 09:01:22.949225: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: init\r\n2021-12-07 09:01:22.949349: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.86.0\r\n2021-12-07 09:01:22.949407: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.86.0\r\n2021-12-07 09:01:22.949428: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.86.0\r\n2021-12-07 09:01:22.960938: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500025000 Hz\r\n2021-12-07 09:01:22.964071: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ed09dbc440 executing computations on platform Host. Devices:\r\n2021-12-07 09:01:22.964121: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nFalse\r\n>>> \r\n`\r\n\r\nDoes anyone meet this condition，please tell me a solution，thank you！",
    "comments": [
      {
        "user": "mohantym",
        "body": "Hi @NeilChangYanpeng ! \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Could you try again in latest version 2.7? Attaching relevant[ thread](https://stackoverflow.com/questions/47068709/your-cpu-supports-instructions-that-this-tensorflow-binary-was-not-compiled-to-u) for reference. Thanks!"
      },
      {
        "user": "NeilChangYanpeng",
        "body": "> \r\n\r\nthank you，I will have a try on latest version"
      },
      {
        "user": "NeilChangYanpeng",
        "body": "I created a new conda env with python 3.7 and installed tensorflow-gpu 2.4.1 using conda method, but still same error:\r\n![812e733f8b8369bc677f50e3072b5ee](https://user-images.githubusercontent.com/41663460/145041073-938f9302-841e-43ed-a767-db1cbe713f48.png)\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 94653,
    "title": "Enhance Memory Optimizer with Dynamic Cost Model for Operation Recomputation",
    "author": "aybakanawork",
    "state": "open",
    "created_at": "2025-06-02T12:05:20Z",
    "updated_at": "2025-06-11T06:26:17Z",
    "labels": [
      "stat:contribution welcome",
      "type:performance",
      "comp:core"
    ],
    "body": "## Background\n\nTensorFlow's memory optimizer currently uses a static list to determine which operations are \"cheap\" to recompute rather than keep in memory. This approach, while functional, leaves significant room for optimization that could improve both memory usage and performance across different models and hardware configurations.\n\nCurrent implementation in `tensorflow/core/grappler/optimizers/memory_optimizer.cc`:\n```cpp\nstd::unordered_set<string> GetCheapToRecomputeOps() {\n  std::unordered_set<string> cheap_ops = {\"Add\", \"AddN\", \"BiasAdd\", ...};\n  return cheap_ops;\n}\n```\n\n## Proposed Enhancement\n\nReplace the static list with a dynamic cost model that considers:\n\n1. **Operation Characteristics**:\n   - Actual computational complexity\n   - Memory usage patterns\n   - Input/output tensor sizes\n   - Operation type-specific metrics\n\n2. **Runtime Factors**:\n   - Available system memory\n   - Hardware capabilities (CPU/GPU/TPU)\n   - Current memory pressure\n   - Historical execution timing data\n\n3. **Adaptive Decision Making**:\n   - Cost-benefit ratio calculation for recomputation vs. storage\n   - Dynamic thresholds based on system state\n   - Learning from actual execution patterns\n\n## Expected Benefits\n\n1. **Improved Memory Efficiency**: Better decisions about when to recompute vs. store results\n2. **Enhanced Performance**: More optimal use of available computational resources\n3. **Hardware Adaptability**: Better adaptation to different hardware configurations\n4. **Dynamic Optimization**: Responsive to changing runtime conditions\n5. **Reduced Memory Pressure**: More intelligent memory management for large models\n\n## Testing Strategy\n\n1. Benchmark suite with various model architectures\n2. Memory usage comparisons\n3. Performance impact measurements\n4. Hardware-specific test cases\n5. Edge case validation\n\n## Success Metrics\n\n- Reduced peak memory usage in large models\n- Improved training speed for memory-constrained scenarios\n- Better resource utilization across different hardware configurations\n",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @aybakanawork ,\nApologies for the delay, and thank you for your contribution. Could you please raise a PR with the changes you are proposing? That will help us better understand and address your needs. For your reference, I am sharing the [contribution](https://www.tensorflow.org/community/contribute) guide below. Thank you!"
      }
    ]
  },
  {
    "issue_number": 94048,
    "title": "16KB pagination support for TF Lite Select Ops",
    "author": "adielwes",
    "state": "open",
    "created_at": "2025-05-23T20:59:10Z",
    "updated_at": "2025-06-11T02:14:14Z",
    "labels": [
      "stat:awaiting response",
      "stale",
      "comp:lite",
      "TF 2.16"
    ],
    "body": "1. System information\n**TF Lite Select Ops 2.16.1**\nMaven: https://central.sonatype.com/artifact/org.tensorflow/tensorflow-lite-select-tf-ops\n\n2. Code\nChecked ELF Alignment to app with lib and the following shared library is not aligned.\n**lib/arm64-v8a/libtensorflowlite_flex_jni.so** does not support 16KB alignement.\n\ngoogle guide :https://developer.android.com/guide/practices/page-sizes?hl=en\n\nFrom Nov. 1st, 2025, all the apps submitted to Google Play and targeting Android 15+ need to support 16 KB page sizes.\nSource: https://android-developers.googleblog.com/2024/12/get-your-apps-ready-for-16-kb-page-size-devices.html\n\nwhen is ETA for supporting 16KB so alignment?",
    "comments": [
      {
        "user": "Ofek-Haim",
        "body": "Hi @gaikwadrahul8,\n\nI'm interested in contributing to this issue as part of my initial involvement in TensorFlow Lite.  \nWould you be open to collaboration or sharing where the work currently stands?\n\nThanks,  \nOfek"
      },
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @adielwes and @Ofek-Haim Thank you for showing interest to contribute in this issue\nI have cross verified the select-tf-ops library (as of 2.16.1) does not support 16KB page alignment and I see there is one PR has been merged to take care of this support issue https://github.com/tensorflow/tensorflow/pull/70126 after checking Maven Central for `org.tensorflow:tensorflow-lite-select-tf-ops` shows that 2.16.1 is indeed the latest available stable version and 2.17.0 has not yet been released\n\n\n**Here is output log** of https://repo1.maven.org/maven2/org/tensorflow/tensorflow-lite-select-tf-ops/2.16.1/tensorflow-lite-select-tf-ops-2.16.1.aar for reference :\n\n```\ngaikwadrahul-macbookpro2:arm64-v8a gaikwadrahul$ readelf -l libtensorflowlite_flex_jni.so\n\nElf file type is DYN (Shared object file)\nEntry point 0x0\nThere are 9 program headers, starting at offset 64\n\nProgram Headers:\n  Type           Offset             VirtAddr           PhysAddr\n                 FileSiz            MemSiz              Flags  Align\n  PHDR           0x0000000000000040 0x0000000000000040 0x0000000000000040\n                 0x00000000000001f8 0x00000000000001f8  R      0x8\n  LOAD           0x0000000000000000 0x0000000000000000 0x0000000000000000\n                 0x0000000003f7f870 0x0000000003f7f870  R E    0x1000\n  LOAD           0x0000000003f80000 0x0000000003f80000 0x0000000003f80000\n                 0x000000000017abe8 0x000000000017abe8  RW     0x1000\n  LOAD           0x00000000040fabe8 0x00000000040fbbe8 0x00000000040fbbe8\n                 0x0000000000009960 0x0000000000013968  RW     0x1000\n  DYNAMIC        0x00000000040e7030 0x00000000040e7030 0x00000000040e7030\n                 0x00000000000001f0 0x00000000000001f0  RW     0x8\n  GNU_RELRO      0x0000000003f80000 0x0000000003f80000 0x0000000003f80000\n                 0x000000000017abe8 0x000000000017b000  R      0x1\n  GNU_EH_FRAME   0x0000000000fe91f4 0x0000000000fe91f4 0x0000000000fe91f4\n                 0x00000000000f2c64 0x00000000000f2c64  R      0x4\n  GNU_STACK      0x0000000000000000 0x0000000000000000 0x0000000000000000\n                 0x0000000000000000 0x0000000000000000  RW     0x0\n  NOTE           0x0000000000000238 0x0000000000000238 0x0000000000000238\n                 0x00000000000000b8 0x00000000000000b8  R      0x4\n\n Section to Segment mapping:\n  Segment Sections...\n   00     \n   01     .note.android.ident .note.gnu.build-id .dynsym .gnu.version .gnu.version_d .gnu.version_r .gnu.hash .hash .dynstr .rela.dyn .rela.plt .rodata protodesc_cold .gcc_except_table .eh_frame_hdr .eh_frame .text malloc_hook .plt \n   02     .data.rel.ro .fini_array .init_array .dynamic .got .got.plt \n   03     .data .bss \n   04     .dynamic \n   05     .data.rel.ro .fini_array .init_array .dynamic .got .got.plt \n   06     .eh_frame_hdr \n   07     \n   08     .note.android.ident .note.gnu.build-id \ngaikwadrahul-macbookpro2:arm64-v8a gaikwadrahul$ \n```\n\nMeanwhile you can build TensorFlow Lite (including the tensorflow-lite-select-tf-ops) from source with that specific commit (2d72742) merged. Please refer this [official documentation](https://ai.google.dev/edge/litert/android/lite_build) This would allow you to generate your own libtensorflowlite_flex_jni.so with 16KB page alignment before an official release till then I'll get back to you with the latest update on 16KB pagination support for TF Lite Select Ops if our internal relevant team working on this support issue\n\nThank you for your cooperation and understanding\n"
      },
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @adielwes \nI would recommend you to please go with LiteRT where it supports 16kb pagination support with `com.google.ai.edge.litert:litert:2.0.1-alpha` version please refer [LiteRT maven repositories](https://maven.google.com/web/index.html#com.google.ai.edge.litert:litert) Android ARM64 targets, the `liblitert_jni.so` library in `LiteRT 2.0.1-alpha` is indeed compiled with 16KB page alignment support! This is a confirmation of support for 16KB paging. Thank you for your cooperation and understanding.\n\n```\ngaikwadrahul-macbookpro2:litert_2_0_1_alpha_check gaikwadrahul$ greadelf -l litert_2_0_1_alpha_extracted/jni/arm64-v8a/liblitert_jni.so\n\nElf file type is DYN (Shared object file)\nEntry point 0x0\nThere are 9 program headers, starting at offset 64\n\nProgram Headers:\n  Type           Offset             VirtAddr           PhysAddr\n                 FileSiz            MemSiz              Flags  Align\n  PHDR           0x0000000000000040 0x0000000000000040 0x0000000000000040\n                 0x00000000000001f8 0x00000000000001f8  R      0x8\n  LOAD           0x0000000000000000 0x0000000000000000 0x0000000000000000\n                 0x0000000000063090 0x0000000000063090  R E    0x4000\n  LOAD           0x0000000000064000 0x0000000000064000 0x0000000000064000\n                 0x00000000000034a8 0x0000000000004000  RW     0x4000\n  LOAD           0x00000000000674a8 0x000000000006b4a8 0x000000000006b4a8\n                 0x00000000000002d8 0x0000000000004a70  RW     0x4000\n  DYNAMIC        0x00000000000667a0 0x00000000000667a0 0x00000000000667a0\n                 0x0000000000000200 0x0000000000000200  RW     0x8\n  GNU_RELRO      0x0000000000064000 0x0000000000064000 0x0000000000064000\n                 0x00000000000034a8 0x0000000000004000  R      0x1\n  GNU_EH_FRAME   0x00000000000158ac 0x00000000000158ac 0x00000000000158ac\n                 0x000000000000493c 0x000000000000493c  R      0x4\n  GNU_STACK      0x0000000000000000 0x0000000000000000 0x0000000000000000\n                 0x0000000000000000 0x0000000000000000  RW     0x0\n  NOTE           0x0000000000000238 0x0000000000000238 0x0000000000000238\n                 0x00000000000000b8 0x00000000000000b8  R      0x4\n\n Section to Segment mapping:\n  Segment Sections...\n   00     \n   01     .note.android.ident .note.gnu.build-id .dynsym .gnu.version .gnu.version_d .gnu.version_r .gnu.hash .dynstr .rela.dyn .rela.plt .rodata .gcc_except_table .eh_frame_hdr .eh_frame .text __lcxx_override malloc_hook google_malloc .plt \n   02     .data.rel.ro .fini_array .init_array .dynamic .got .got.plt .relro_padding \n   03     .data .bss \n   04     .dynamic \n   05     .data.rel.ro .fini_array .init_array .dynamic .got .got.plt .relro_padding \n   06     .eh_frame_hdr \n   07     \n   08     .note.android.ident .note.gnu.build-id \ngaikwadrahul-macbookpro2:litert_2_0_1_alpha_check gaikwadrahul$ \n\n```"
      }
    ]
  },
  {
    "issue_number": 94433,
    "title": "tf.transpose crashes with negative perm value: \"Check failed: d >= 0 (0 vs. -1)\"",
    "author": "panda123dd",
    "state": "open",
    "created_at": "2025-05-29T07:32:13Z",
    "updated_at": "2025-06-11T02:14:13Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nlinux ubuntu 18.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.21\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen calling tf.transpose with a perm argument containing negative indices (e.g., perm=(-1, 0)), TensorFlow crashes with the following fatal error：\n\nF tensorflow/core/framework/tensor_shape.cc:358] Check failed: d >= 0 (0 vs. -1)\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\ninput = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\ninput = tf.transpose(input, perm=((- 1), 0))\n```\n\n### Relevant log output\n\n```shell\nF tensorflow/core/framework/tensor_shape.cc:358] Check failed: d >= 0 (0 vs. -1)\n```",
    "comments": [
      {
        "user": "bandirevanth",
        "body": "**Why this happens**:\n- tf.transpose ultimately calls a kernel that expects every entry in perm to be a non-negative dimension number.  \n- When a negative value slips through, a low-level assertion (CHECK_GE(d, 0)) fires after tensors have been allocated, so the program aborts instead of throwing a recoverable exception.  \n- **This mirrors an older, already-closed report [(#65649)](https://github.com/tensorflow/tensorflow/issues/65649) for TensorFlow 2.15, _so the guard apparently never made it into the public build_.**\n\n### Workarounds for `tf.transpose` crash on negative `perm` indices\n\n| Approach | Effort | Drawback |\n|----------|--------|----------|\n| **Validate `perm` in Python**: `if any(p < 0 for p in perm): …` | trivial | extra boilerplate |\n| **Convert negatives manually**: `perm = [(p + rank) % rank for p in perm]` | low | potential silent logic errors |\n| **Wrap transpose in `tf.debugging.assert_non_negative(perm)`** | low | small runtime cost |\n| **Fallback to NumPy for exotic cases** | medium | loses graph-mode benefits |\n\n\n**Long-term remedies**:\n- Kernel-level contract – the operator should return an INVALID_ARGUMENT status rather than aborting.\n- Unit tests – add regression tests that feed negative perm and verify graceful failure.\n- Conversion layer – mimic NumPy semantics (auto-normalise negative indices) if the team wants stricter backward-compatibility.\n- Replace the CHECK with a TF_RETURN_IF_ERROR that propagates to Python. The performance impact is negligible because the loop over perm already exists."
      },
      {
        "user": "Venkat6871",
        "body": "Hi @panda123dd ,\nApologies for the delay, and thank you for raising your concern here.\nI tried running your code on Colab using TensorFlow version 2.19.0 and the nightly build. In TF 2.19.0, it crashes, but in the nightly build, it works fine. So, this issue might be fixed in the upcoming release.\nHere, I am attaching a [gist](https://colab.sandbox.google.com/gist/Venkat6871/9c964371841cd84a45cd554f69d19733/94433_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      }
    ]
  },
  {
    "issue_number": 92216,
    "title": "Muting Tensorflow Lite logs",
    "author": "cpappasILMX",
    "state": "open",
    "created_at": "2025-04-25T19:31:14Z",
    "updated_at": "2025-06-10T21:21:01Z",
    "labels": [
      "type:support",
      "comp:lite",
      "TF 2.16"
    ],
    "body": "### Issue type\n\nSupport\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\nAndroid and iOS\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n1.20\n\n### GCC/compiler version\n\nAppleClang 16\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWe would like to silence all logging from the TFLite binaries we compiled locally following the documentation found here https://ai.google.dev/edge/litert/build/android and here https://ai.google.dev/edge/litert/build/ios. For the Android build we extracted the .so binary from the .aar, and for the iOS build we built the TensorflowLiteC Static Framework. As it stands right now, we are unable to silence logs. \n\nI have tried implementing a definition in CMake for our TF module target\n`target_compile_definitions(my_tf_app PRIVATE TF_CPP_MIN_LOG_LEVEL=4)`\n\nI have tried setting the environment variable at runtime\n`setenv(\"TF_CPP_MIN_LOG_LEVEL\", \"4\", /*overwrite=*/1);`\n\nAnd I have tried calling the logging API\n`tflite::logging_internal::MinimalLogger::SetMinimumLogSeverity(\n      tflite::logging_internal::LogSeverity::TFLITE_LOG_SILENT);`\nIncluding the header\n`#include \"tensorflow/lite/minimal_logging.h\"`\nBut I am met with compilation issues that the symbols for `SetMinimumLogSeverity` are missing.\n\nEmploying all of these methods hasn't yielded silence from TFLite. How can I go about resolving this?\n\n\n### Standalone code to reproduce the issue\n\n```shell\nSee above.\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "cpappasILMX",
        "body": "Minor correction to my issue:\n\nCompiling for MacOS leads to the missing symbols issue. Compiling for iOS and Android doesn't lead to missing symbols. However, including `tensorflow/lite/logger.h` and calling `SetMinimumLogSeverity` with `TFLITE_LOG_SILENT` as the argument doesn't yield the expected result of no logging."
      },
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @cpappasILMX\nI apologize for the delay in my response, I see there is option to silence logging using `TFLITE_LOG_SILENT = 4` [[Ref](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/logger.h)] so please give it try with below header and API, Please make sure that place below call as early as possible in the application's startup sequence before any TFLite interpreters are created or models loaded.\n\n```\n#include \"tensorflow/lite/logger.h\"\n\n// Set this before any TFLite operations\ntflite::LoggerOptions::SetMinimumLogSeverity(tflite::TFLITE_LOG_SILENT);\n```\nIf issue still persists please give it try by upgrading or downgrading TensorFlow versions also try with tested build configuration versions of `clang` for **macOS** and see is it resolving your issue or not ?\n\n\nVersion | Python version | Compiler | Build tools\n-- | -- | -- | --\ntensorflow-2.16.1 | 3.9-3.12 | Clang from Xcode 13.6 | Bazel 6.5.0\ntensorflow-2.15.0 | 3.9-3.11 | Clang from xcode 10.15 | Bazel 6.1.0\n\nThank you for your cooperation and understanding.\n\n\n\n\n"
      },
      {
        "user": "cpappasILMX",
        "body": "I tried upgrading to 2.19, and calling `tflite::LoggerOptions::SetMinimumLogSeverity(tflite::TFLITE_LOG_SILENT);` prior to loading any models did not silence the logs at all. I am also unable to downgrade to 2.15 since some of our models are incompatible with that version."
      }
    ]
  },
  {
    "issue_number": 27537,
    "title": "Unexpected UnicodeDecodeError: invalid continuation byte when reading lines from a file",
    "author": "alter-bug-tracer",
    "state": "closed",
    "created_at": "2019-04-05T10:29:22Z",
    "updated_at": "2025-06-10T13:02:09Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:ops",
      "TF 2.11"
    ],
    "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nUnexpected and undocumented runtime exception/error when handling malformed data.\r\n\r\n**Describe the expected behavior**\r\nExpected a \"TypeError\" or an empty list as a result.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport csv\r\nimport sys\r\nimport tensorflow as tf\r\n\r\ninput_file_name = sys.argv[1]\r\n\r\nwith tf.gfile.Open(input_file_name, \"r\") as f:\r\n  reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\r\n  for line in reader:\r\n    print(line)\r\n```\r\nRun with the path to the attached file as a command line argument.\r\n\r\n**Other info / logs**\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow_bug.py\", line 9, in <module>\r\n    for line in reader:\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 220, in \\_\\_next\\_\\_\r\n    return self.next()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 214, in next\r\n    retval = self.readline()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 184, in readline\r\n    return self._prepare_value(self._read_buf.ReadLineAsString())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 100, in _prepare_value\r\n    return compat.as_str_any(val)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py\", line 107, in as_str_any\r\n    return as_str(value)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py\", line 80, in as_text\r\n    return bytes_or_text.decode(encoding)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte\r\n\r\n[corrupted_file1.zip](https://github.com/tensorflow/tensorflow/files/3047460/corrupted_file1.zip)\r\n",
    "comments": [
      {
        "user": "ymodak",
        "body": "In order to expedite the trouble-shooting process, please provide a complete code snippet to reproduce the issue reported here. Thanks!\r\n"
      },
      {
        "user": "alter-bug-tracer",
        "body": "This is the complete code snipped. Run it with the path to the attached file as its first argument.\r\n\r\n```\r\nimport csv\r\nimport sys\r\nimport tensorflow as tf\r\n\r\ninput_file_name = sys.argv[1]\r\n\r\nwith tf.gfile.Open(input_file_name, \"r\") as f:\r\n  reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\r\n  for line in reader:\r\n    print(line)\r\n```"
      },
      {
        "user": "yongtang",
        "body": "@alter-bug-tracer If `tf.gfile.Open` is replaced with `open`, a similar error show up as well:\r\n```\r\n  File \"/usr/lib/python3.5/encodings/ascii.py\", line 26, in decode\r\n    return codecs.ascii_decode(input, self.errors)[0]\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xed in position 0: ordinal not in range(128)\r\n```\r\n\r\nIt might be better to improve the error message in tf, though it seems the error message of `tf.gfile.Open` is inline with `open`?"
      }
    ]
  },
  {
    "issue_number": 94379,
    "title": "Crash in `tf.raw_ops.BiasAdd` when executing on GPU",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-28T14:04:05Z",
    "updated_at": "2025-06-10T10:37:50Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.20.0-dev20250526\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux GPU\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.13.2\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\nCUDA 12.5.1\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow crashes with a fatal error `Check failed: d < dims() (2 vs. 2)` when executing broadcasting addition by `BiasAdd` on GPU, but  runs fine on CPU.\n\nThe crash occurs in tensorflow/core/framework/tensor_shape.cc:359 and results in an aborted process with core dump.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nwith tf.device('/gpu:0'):\n    try:\n        value = tf.constant([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=tf.int32, shape=[2,6])\n        bias = tf.constant([1, 2, 3, 4, 5, 6], dtype=tf.int32, shape=[6])\n        data_format = \"NCHW\"\n        result = tf.raw_ops.BiasAdd(\n            value=value,\n            bias=bias,\n            data_format=data_format,\n            name=None\n        )\n        print(result)\n    except Exception as e:\n        print(e)\n```\n\n### Relevant log output\n\n```shell\nI0000 00:00:1748440976.276949 1213984 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2367 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n2025-05-28 22:02:56.315991: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (2 vs. 2)\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/a983c65e38acdea7b526ebb4492d1f77/94379_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      },
      {
        "user": "RMBhang",
        "body": "GPU 和 CPU都会有时崩溃是什么原因？\n"
      }
    ]
  },
  {
    "issue_number": 94593,
    "title": "TensorFlow Docker `tensorflow/tensorflow:latest-gpu` fails to detect GPU due to CUDA/cuDNN mismatch",
    "author": "ielenik",
    "state": "open",
    "created_at": "2025-05-31T09:33:33Z",
    "updated_at": "2025-06-10T04:46:46Z",
    "labels": [
      "type:bug",
      "comp:gpu",
      "TF 2.19"
    ],
    "body": "**Title:** TensorFlow Docker `tensorflow/tensorflow:latest-gpu` fails to detect GPU due to CUDA/cuDNN mismatch\n\n**Description:**\n\nThe latest TensorFlow GPU Docker image (`tensorflow/tensorflow:latest-gpu`, pulled as of May 31, 2025) is broken out-of-the-box for GPU usage.\n\nRunning:\n```bash\ndocker run --gpus all -it tensorflow/tensorflow:latest-gpu bash\npython3 -c 'import tensorflow as tf; print(tf.config.list_physical_devices(\"GPU\"))'\n```\n\n**Expected Behavior:**  \nTensorFlow should detect and list available GPUs.\n\n**Actual Behavior:**  \nTensorFlow returns an empty list: `[]`.\n\n**Error Logs:**\n\n```\nE tensorflow/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE tensorflow/stream_executor/cuda/cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW tensorflow/core/common_runtime/gpu/gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU.\nSkipping registering GPU devices...\n```\n\ntf.sysconfig.get_build_info() reports:\n```\nOrderedDict([('cpu_compiler', '/usr/lib/llvm-18/bin/clang'), ('cuda_compute_capabilities', ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90']), ('cuda_version', '12.5.1'), ('cudnn_version', '9'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', False)])\n```\nHowever, the container only includes CUDA 12.3 and cuDNN 8.9.6, causing a runtime mismatch and failure to initialize GPU.\nfind / -name 'libcu*' 2>/dev/null\n```\n/var/lib/dpkg/info/libcurl4:amd64.symbols\n/var/lib/dpkg/info/libcufft-12-3.md5sums\n/var/lib/dpkg/info/libcufile-12-3.list\n/var/lib/dpkg/info/libcurl4:amd64.list\n/var/lib/dpkg/info/libcufile-12-3.postinst\n/var/lib/dpkg/info/libcusolver-12-3.md5sums\n/var/lib/dpkg/info/libcurand-12-3.list\n/var/lib/dpkg/info/libcusparse-12-3.md5sums\n/var/lib/dpkg/info/libcusolver-12-3.list\n/var/lib/dpkg/info/libcublas-12-3.list\n/var/lib/dpkg/info/libcufile-12-3.md5sums\n/var/lib/dpkg/info/libcufft-12-3.list\n/var/lib/dpkg/info/libcufile-12-3.prerm\n/var/lib/dpkg/info/libcurl4:amd64.triggers\n/var/lib/dpkg/info/libcufile-12-3.conffiles\n/var/lib/dpkg/info/libcudnn8.md5sums\n/var/lib/dpkg/info/libcudnn8.list\n/var/lib/dpkg/info/libcurl4:amd64.shlibs\n/var/lib/dpkg/info/libcurand-12-3.md5sums\n/var/lib/dpkg/info/libcublas-12-3.md5sums\n/var/lib/dpkg/info/libcurl4:amd64.md5sums\n/var/lib/dpkg/info/libcusparse-12-3.list\n/var/lib/dpkg/info/libcurl3-gnutls:amd64.triggers\n/var/lib/dpkg/info/libcurl3-gnutls:amd64.list\n/var/lib/dpkg/info/libcurl3-gnutls:amd64.symbols\n/var/lib/dpkg/info/libcurl3-gnutls:amd64.shlibs\n/var/lib/dpkg/info/libcurl3-gnutls:amd64.md5sums\n/usr/local/cuda-12.3/compat/libcuda.so.545.23.06\n/usr/local/cuda-12.3/compat/libcudadebugger.so.545.23.06\n/usr/local/cuda-12.3/compat/libcudadebugger.so.1\n/usr/local/cuda-12.3/compat/libcuda.so\n/usr/local/cuda-12.3/compat/libcuda.so.1\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so.12\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so.1\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcuinj64.so\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so.12\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusolverMg.so.11\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcuinj64.so.12.3.101\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcupti.so.2023.3.1\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufft.so.11.0.12.1\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusparse.so.12.2.0.103\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcuinj64.so.12.3\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufile.so.1.8.1\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusolver.so.11.5.4.101\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufft.so.11\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufile_rdma.so.1.8.1\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufile.so.0\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libculibos.a\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusolverMg.so.11.5.4.101\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcurand.so.10.3.4.107\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcupti.so\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufile_rdma.so.1\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufftw.so.11\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcurand.so.10\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcupti.so.12\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so.12.3.4.1\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart_static.a\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/cmake/libcudacxx\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/cmake/libcudacxx/libcudacxx-config.cmake\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/cmake/libcudacxx/libcudacxx-header-search.cmake\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/cmake/libcudacxx/libcudacxx-config-version.cmake\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufftw.so.11.0.12.1\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so.12.3.101\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusolver.so.11\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so.12.3.4.1\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so.12\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudadevrt.a\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusparse.so.12\n/usr/local/cuda-12.3/extras/Debugger/lib64/libcudacore.a\n/usr/local/cuda-12.3/extras/Debugger/include/libcudacore.h\n/usr/lib/x86_64-linux-gnu/libcudadebugger.so.1\n/usr/lib/x86_64-linux-gnu/libcuda.so\n/usr/lib/x86_64-linux-gnu/libcuda.so.1\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8\n/usr/lib/x86_64-linux-gnu/libcurl.so.4.7.0\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcurl.so.4\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcurl-gnutls.so.4\n/usr/lib/x86_64-linux-gnu/libcurl-gnutls.so.4.7.0\n/usr/lib/x86_64-linux-gnu/libcurl-gnutls.so.3\n/usr/lib/wsl/drivers/nv_dispig.inf_amd64_0afec3f2050014a0/libcuda_loader.so\n/usr/lib/wsl/drivers/nv_dispig.inf_amd64_0afec3f2050014a0/libcuda.so.1.1\n/usr/share/lintian/overrides/libcudnn8\n/usr/share/lintian/overrides/libcurl3-gnutls\n/usr/share/doc/libcudnn8\n/usr/share/doc/libcublas-12-3\n/usr/share/doc/libcurand-12-3\n/usr/share/doc/libcufft-12-3\n/usr/share/doc/libcusolver-12-3\n/usr/share/doc/libcufile-12-3\n/usr/share/doc/libcusparse-12-3\n/usr/share/doc/libcurl4\n/usr/share/doc/libcurl3-gnutls\n```\nExpected behavior:\nThe latest-gpu image should include libraries matching the TensorFlow build (CUDA 12.5.1, cuDNN 9) or fall back to a compatible combination.\n",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce this issue on my end using a VM. I am attaching a [screenshot](https://screenshot.googleplex.com/6mb4BuPuw29anbx) here for your reference.\nThank you!"
      },
      {
        "user": "Anime-Run-Kingdom-Code",
        "body": "> **Title:** TensorFlow Docker `tensorflow/tensorflow:latest-gpu` fails to detect GPU due to CUDA/cuDNN mismatch\n> \n> **Description:**\n> \n> The latest TensorFlow GPU Docker image (`tensorflow/tensorflow:latest-gpu`, pulled as of May 31, 2025) is broken out-of-the-box for GPU usage.\n> \n> Running:\n> \n> docker run --gpus all -it tensorflow/tensorflow:latest-gpu bash\n> python3 -c 'import tensorflow as tf; print(tf.config.list_physical_devices(\"GPU\"))'\n> **Expected Behavior:** TensorFlow should detect and list available GPUs.\n> \n> **Actual Behavior:** TensorFlow returns an empty list: `[]`.\n> \n> **Error Logs:**\n> \n> ```\n> E tensorflow/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n> E tensorflow/stream_executor/cuda/cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n> E tensorflow/stream_executor/cuda/cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n> W tensorflow/core/common_runtime/gpu/gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU.\n> Skipping registering GPU devices...\n> ```\n> \n> tf.sysconfig.get_build_info() reports:\n> \n> ```\n> OrderedDict([('cpu_compiler', '/usr/lib/llvm-18/bin/clang'), ('cuda_compute_capabilities', ['sm_60', 'sm_70', 'sm_80', 'sm_89', 'compute_90']), ('cuda_version', '12.5.1'), ('cudnn_version', '9'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', False)])\n> ```\n> \n> However, the container only includes CUDA 12.3 and cuDNN 8.9.6, causing a runtime mismatch and failure to initialize GPU. find / -name 'libcu*' 2>/dev/null\n> \n> ```\n> /var/lib/dpkg/info/libcurl4:amd64.symbols\n> /var/lib/dpkg/info/libcufft-12-3.md5sums\n> /var/lib/dpkg/info/libcufile-12-3.list\n> /var/lib/dpkg/info/libcurl4:amd64.list\n> /var/lib/dpkg/info/libcufile-12-3.postinst\n> /var/lib/dpkg/info/libcusolver-12-3.md5sums\n> /var/lib/dpkg/info/libcurand-12-3.list\n> /var/lib/dpkg/info/libcusparse-12-3.md5sums\n> /var/lib/dpkg/info/libcusolver-12-3.list\n> /var/lib/dpkg/info/libcublas-12-3.list\n> /var/lib/dpkg/info/libcufile-12-3.md5sums\n> /var/lib/dpkg/info/libcufft-12-3.list\n> /var/lib/dpkg/info/libcufile-12-3.prerm\n> /var/lib/dpkg/info/libcurl4:amd64.triggers\n> /var/lib/dpkg/info/libcufile-12-3.conffiles\n> /var/lib/dpkg/info/libcudnn8.md5sums\n> /var/lib/dpkg/info/libcudnn8.list\n> /var/lib/dpkg/info/libcurl4:amd64.shlibs\n> /var/lib/dpkg/info/libcurand-12-3.md5sums\n> /var/lib/dpkg/info/libcublas-12-3.md5sums\n> /var/lib/dpkg/info/libcurl4:amd64.md5sums\n> /var/lib/dpkg/info/libcusparse-12-3.list\n> /var/lib/dpkg/info/libcurl3-gnutls:amd64.triggers\n> /var/lib/dpkg/info/libcurl3-gnutls:amd64.list\n> /var/lib/dpkg/info/libcurl3-gnutls:amd64.symbols\n> /var/lib/dpkg/info/libcurl3-gnutls:amd64.shlibs\n> /var/lib/dpkg/info/libcurl3-gnutls:amd64.md5sums\n> /usr/local/cuda-12.3/compat/libcuda.so.545.23.06\n> /usr/local/cuda-12.3/compat/libcudadebugger.so.545.23.06\n> /usr/local/cuda-12.3/compat/libcudadebugger.so.1\n> /usr/local/cuda-12.3/compat/libcuda.so\n> /usr/local/cuda-12.3/compat/libcuda.so.1\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so.12\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so.1\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcuinj64.so\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so.12\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusolverMg.so.11\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcuinj64.so.12.3.101\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcupti.so.2023.3.1\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufft.so.11.0.12.1\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusparse.so.12.2.0.103\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcuinj64.so.12.3\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufile.so.1.8.1\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusolver.so.11.5.4.101\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufft.so.11\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufile_rdma.so.1.8.1\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufile.so.0\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libculibos.a\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusolverMg.so.11.5.4.101\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcurand.so.10.3.4.107\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcupti.so\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufile_rdma.so.1\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufftw.so.11\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcurand.so.10\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcupti.so.12\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so.12.3.4.1\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart_static.a\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/cmake/libcudacxx\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/cmake/libcudacxx/libcudacxx-config.cmake\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/cmake/libcudacxx/libcudacxx-header-search.cmake\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/cmake/libcudacxx/libcudacxx-config-version.cmake\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcufftw.so.11.0.12.1\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so.12.3.101\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusolver.so.11\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so.12.3.4.1\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so.12\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudadevrt.a\n> /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcusparse.so.12\n> /usr/local/cuda-12.3/extras/Debugger/lib64/libcudacore.a\n> /usr/local/cuda-12.3/extras/Debugger/include/libcudacore.h\n> /usr/lib/x86_64-linux-gnu/libcudadebugger.so.1\n> /usr/lib/x86_64-linux-gnu/libcuda.so\n> /usr/lib/x86_64-linux-gnu/libcuda.so.1\n> /usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8\n> /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8\n> /usr/lib/x86_64-linux-gnu/libcurl.so.4.7.0\n> /usr/lib/x86_64-linux-gnu/libcudnn.so.8\n> /usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n> /usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n> /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n> /usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8\n> /usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n> /usr/lib/x86_64-linux-gnu/libcurl.so.4\n> /usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8\n> /usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\n> /usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8\n> /usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n> /usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n> /usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n> /usr/lib/x86_64-linux-gnu/libcurl-gnutls.so.4\n> /usr/lib/x86_64-linux-gnu/libcurl-gnutls.so.4.7.0\n> /usr/lib/x86_64-linux-gnu/libcurl-gnutls.so.3\n> /usr/lib/wsl/drivers/nv_dispig.inf_amd64_0afec3f2050014a0/libcuda_loader.so\n> /usr/lib/wsl/drivers/nv_dispig.inf_amd64_0afec3f2050014a0/libcuda.so.1.1\n> /usr/share/lintian/overrides/libcudnn8\n> /usr/share/lintian/overrides/libcurl3-gnutls\n> /usr/share/doc/libcudnn8\n> /usr/share/doc/libcublas-12-3\n> /usr/share/doc/libcurand-12-3\n> /usr/share/doc/libcufft-12-3\n> /usr/share/doc/libcusolver-12-3\n> /usr/share/doc/libcufile-12-3\n> /usr/share/doc/libcusparse-12-3\n> /usr/share/doc/libcurl4\n> /usr/share/doc/libcurl3-gnutls\n> ```\n> \n> Expected behavior: The latest-gpu image should include libraries matching the TensorFlow build (CUDA 12.5.1, cuDNN 9) or fall back to a compatible combination.\n\nSame issue "
      }
    ]
  },
  {
    "issue_number": 93798,
    "title": "Are there any training acceleration solutions for operations like embedding in a search and recommendation model trained on a CPU using TensorFlow?",
    "author": "Lenan22",
    "state": "closed",
    "created_at": "2025-05-21T03:52:30Z",
    "updated_at": "2025-06-10T02:14:50Z",
    "labels": [
      "stat:awaiting response",
      "type:support",
      "stale"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf any version\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI’m looking forward to recommendations from experts.\n\n### Standalone code to reproduce the issue\n\n```shell\nno\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @Lenan22 ,\nApologies for the delay, and thank you for raising your concern. To help us investigate further, could you please provide more details such as:\nYour system configuration (CPU, OS, etc.)\nModel architecture or code snippet (especially how embeddings are used)\nTensorFlow version and setup (e.g., installed via pip, built from source)\nIn the meantime, here are some resources that may help optimize embedding operations on CPU:\n[Optimizing TensorFlow for 4th Gen Intel Xeon Processors](https://blog.tensorflow.org/2023/01/optimizing-tensorflow-for-4th-gen-intel-xeon-processors.html)\n[Intel Extension for TensorFlow (PyPI)](https://pypi.org/project/intel-extension-for-tensorflow/)\n\nThese may provide performance improvements specifically for CPU-based training workloads, especially in search and recommendation systems.\nLet us know once you have shared the additional information we will be happy to assist you further!\n\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 93739,
    "title": "tensorflow.python.framework.errors_impl.FailedPreconditionError: Could not find variable bn_conv1/moving_mean",
    "author": "RexNihilis",
    "state": "closed",
    "created_at": "2025-05-20T12:09:26Z",
    "updated_at": "2025-06-10T02:14:49Z",
    "labels": [
      "stat:awaiting response",
      "stale",
      "type:others",
      "subtype:windows",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nOthers\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nWindows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\npython 3.11\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nC:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\\.venv\\Scripts\\python.exe C:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\\Train.py \nC:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\n2025-05-20 20:02:22.710100: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-05-20 20:02:23.470694: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\nWARNING:tensorflow:From C:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\\mrcnn\\model.py:31: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n\n2025-05-20 20:02:25.915848: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1747742545.919063   18172 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n2025-05-20 20:02:25.927669: W tensorflow/c/c_api.cc:305] Operation '{name:'conv1/bias/Assign' id:40 op device:{requested: '', assigned: ''} def:{{{node conv1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv1/bias, conv1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n2025-05-20 20:02:25.939725: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable bn_conv1/moving_mean. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/bn_conv1/moving_mean/class tensorflow::Var does not exist.\n\t [[{{node bn_conv1_1/Cast/ReadVariableOp}}]]\nTraceback (most recent call last):\n  File \"C:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\\Train.py\", line 175, in <module>\n    model = modellib.MaskRCNN(mode='training', config=config, model_dir=MODEL_DIR)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\\mrcnn\\model.py\", line 1834, in __init__\n    self.keras_model = self.build(mode=mode, config=config)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\\mrcnn\\model.py\", line 1895, in build\n    _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE,\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\\mrcnn\\model.py\", line 182, in resnet_graph\n    x = KL.Activation('relu')(x)\n        ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\", line 752, in __call__\n    base_layer_utils.create_keras_history(inputs)\n  File \"C:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_utils.py\", line 185, in create_keras_history\n    _, created_layers = _create_keras_history_helper(tensors, set(), [])\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_utils.py\", line 259, in _create_keras_history_helper\n    constants[i] = backend.function([], op_input)([])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3955, in __call__\n    fetched = self._callable_fn(*array_vals,\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Jasper\\Documents\\GitHub\\Mask_RCNN\\.venv\\Lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1510, in __call__\n    ret = tf_session.TF_SessionRunCallable(self._session._session,\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Could not find variable bn_conv1/moving_mean. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/bn_conv1/moving_mean/class tensorflow::Var does not exist.\n\t [[{{node bn_conv1_1/Cast/ReadVariableOp}}]]\n\nProcess finished with exit code 1\n\n### Standalone code to reproduce the issue\n\n```shell\nimport os\nimport sys\nimport json\nimport numpy as np\nimport time\nfrom PIL import Image, ImageDraw\n# Set the ROOT_DIR variable to the root directory of the Mask_RCNN git repo\nROOT_DIR = 'C:\\\\Users\\\\Jasper\\\\Documents\\\\GitHub\\\\Mask_RCNN'\nprint(ROOT_DIR)\nassert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist. Did you forget to read the instructions above? ;)'\n\n# Import mrcnn libraries\nsys.path.append(ROOT_DIR)\nfrom mrcnn.config import Config\nimport mrcnn.utils as utils\nfrom mrcnn import visualize\nimport mrcnn.model as modellib\n\n# Directory to save logs and trained model\nMODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n\n# Local path to trained weights file\nCOCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n\n# Download COCO trained weights from Releases if needed\nif not os.path.exists(COCO_MODEL_PATH):\n    utils.download_trained_weights(COCO_MODEL_PATH)\n\n\nclass HelmetConfig(Config):\n    # Give the configuration a recognizable name\n    NAME = \"hard_hats\"\n\n    # Train on 1 GPU and 1 image per GPU. Batch size is 1 (GPUs * images/GPU).\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\n    # Number of classes (including background)\n    NUM_CLASSES = 1 + 1  # background + 1 (hard_hats)\n\n    # All of our training images are 640x640\n    IMAGE_MIN_DIM = 640\n    IMAGE_MAX_DIM = 640\n\n    # You can experiment with this number to see if it improves training\n    STEPS_PER_EPOCH = 500\n\n    # This is how often validation is run. If you are using too much hard drive space\n    # on saved models (in the MODEL_DIR), try making this value larger.\n    VALIDATION_STEPS = 5\n\n    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n    BACKBONE = 'resnet101'\n\n    # To be honest, I haven't taken the time to figure out what these do\n    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n    TRAIN_ROIS_PER_IMAGE = 32\n    MAX_GT_INSTANCES = 50\n    POST_NMS_ROIS_INFERENCE = 500\n    POST_NMS_ROIS_TRAINING = 1000\n\nconfig = HelmetConfig()\n#config.display()\n\nclass CocoLikeDataset(utils.Dataset):\n    \"\"\" Generates a COCO-like dataset, i.e. an image dataset annotated in the style of the COCO dataset.\n        See http://cocodataset.org/#home for more information.\n    \"\"\"\n\n    def load_data(self, annotation_json, images_dir):\n        \"\"\" Load the coco-like dataset from json\n        Args:\n            annotation_json: The path to the coco annotations json file\n            images_dir: The directory holding the images referred to by the json file\n        \"\"\"\n        # Load json from file\n        json_file = open(annotation_json)\n        coco_json = json.load(json_file)\n        json_file.close()\n\n        # Add the class names using the base method from utils.Dataset\n        source_name = \"coco_like\"\n        for category in coco_json['categories']:\n            class_id = category['id']\n            class_name = category['name']\n            if class_id < 1:\n                print('Error: Class id for \"{}\" cannot be less than one. (0 is reserved for the background)'.format(\n                    class_name))\n                return\n\n            self.add_class(source_name, class_id, class_name)\n\n        # Get all annotations\n        annotations = {}\n        for annotation in coco_json['annotations']:\n            image_id = annotation['image_id']\n            if image_id not in annotations:\n                annotations[image_id] = []\n            annotations[image_id].append(annotation)\n\n        # Get all images and add them to the dataset\n        seen_images = {}\n        for image in coco_json['images']:\n            image_id = image['id']\n            if image_id in seen_images:\n                print(\"Warning: Skipping duplicate image id: {}\".format(image))\n            else:\n                seen_images[image_id] = image\n                try:\n                    image_file_name = image['file_name']\n                    image_width = image['width']\n                    image_height = image['height']\n                except KeyError as key:\n                    print(\"Warning: Skipping image (id: {}) with missing key: {}\".format(image_id, key))\n\n                image_path = os.path.abspath(os.path.join(images_dir, image_file_name))\n                image_annotations = annotations[image_id]\n\n                # Add the image using the base method from utils.Dataset\n                self.add_image(\n                    source=source_name,\n                    image_id=image_id,\n                    path=image_path,\n                    width=image_width,\n                    height=image_height,\n                    annotations=image_annotations\n                )\n\n    def load_mask(self, image_id):\n        \"\"\" Load instance masks for the given image.\n        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n        Args:\n            image_id: The id of the image to load masks for\n        Returns:\n            masks: A bool array of shape [height, width, instance count] with\n                one mask per instance.\n            class_ids: a 1D array of class IDs of the instance masks.\n        \"\"\"\n        image_info = self.image_info[image_id]\n        annotations = image_info['annotations']\n        instance_masks = []\n        class_ids = []\n\n        for annotation in annotations:\n            class_id = annotation['category_id']\n            mask = Image.new('1', (image_info['width'], image_info['height']))\n            mask_draw = ImageDraw.ImageDraw(mask, '1')\n            for segmentation in annotation['segmentation']:\n                mask_draw.polygon(segmentation, fill=1)\n                bool_array = np.array(mask) > 0\n                instance_masks.append(bool_array)\n                class_ids.append(class_id)\n\n        mask = np.dstack(instance_masks)\n        class_ids = np.array(class_ids, dtype=np.int32)\n\n        return mask, class_ids\n\ndataset_train = CocoLikeDataset()\ndataset_train.load_data('C:\\\\Users\\\\Jasper\\\\Documents\\\\GitHub\\\\Mask_RCNN\\\\datasets\\\\train\\\\_annotations.coco.json', 'C:\\\\Users\\\\Jasper\\\\Documents\\\\GitHub\\\\Mask_RCNN\\\\datasets\\\\train\\\\images')\ndataset_train.prepare()\n\ndataset_val = CocoLikeDataset()\ndataset_val.load_data('C:\\\\Users\\\\Jasper\\\\Documents\\\\GitHub\\\\Mask_RCNN\\\\datasets\\\\valid\\\\_annotations.coco.json', 'C:\\\\Users\\\\Jasper\\\\Documents\\\\GitHub\\\\Mask_RCNN\\\\datasets\\\\valid\\\\images')\ndataset_val.prepare()\n\ndataset = dataset_train\n#image_ids = np.random.choice(dataset.image_ids, 4)\n#for image_id in image_ids:\n    #image = dataset.load_image(image_id)\n    #mask, class_ids = dataset.load_mask(image_id)\n    #visualize.display_top_masks(image, mask, class_ids, dataset.class_names)\n\n# Create model in training mode\nmodel = modellib.MaskRCNN(mode='training', config=config, model_dir=MODEL_DIR)\n\n# Which weights to start with?\ninit_with = \"coco\"  # imagenet, coco, or last\n\nif init_with == \"imagenet\":\n    model.load_weights(model.get_imagenet_weights(), by_name=True)\nelif init_with == \"coco\":\n    # Load weights trained on MS COCO, but skip layers that\n    # are different due to the different number of classes\n    # See README for instructions to download the COCO weights\n    model.load_weights(COCO_MODEL_PATH, by_name=True,\n                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n                                \"mrcnn_bbox\", \"mrcnn_mask\"])\nelif init_with == \"last\":\n    # Load the last model you trained and continue training\n    model.load_weights(model.find_last(), by_name=True)\n\n# Train the head branches\n# Passing layers=\"heads\" freezes all layers except the head\n# layers. You can also pass a regular expression to select\n# which layers to train by name pattern.\nstart_train = time.time()\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE,\n            epochs=4,\n            layers='all')\nend_train = time.time()\nminutes = round((end_train - start_train) / 60, 2)\nprint(f'Training took {minutes} minutes')\n\n# Fine tune all layers\n# Passing layers=\"all\" trains all layers. You can also\n# pass a regular expression to select which layers to\n# train by name pattern.\nstart_train = time.time()\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE / 10,\n            epochs=8,\n            layers=\"all\")\nend_train = time.time()\nminutes = round((end_train - start_train) / 60, 2)\nprint(f'Training took {minutes} minutes')\n\n\nclass InferenceConfig(HelmetConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    IMAGE_MIN_DIM = 640\n    IMAGE_MAX_DIM = 640\n    DETECTION_MIN_CONFIDENCE = 0.85\n\n\ninference_config = InferenceConfig()\n\nmodel = modellib.MaskRCNN(mode=\"inference\",\n                          config=inference_config,\n                          model_dir=MODEL_DIR)\n\nmodel_path = model.find_last()\n\n# Load trained weights (fill in path to trained weights here)\nassert model_path != \"\", \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)\n\nimport skimage\nreal_test_dir = 'C:\\\\Users\\\\Jasper\\\\Documents\\\\GitHub\\\\Mask_RCNN\\\\datasets\\\\test\\\\images'\nimage_paths = []\nfor filename in os.listdir(real_test_dir):\n    if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:\n        image_paths.append(os.path.join(real_test_dir, filename))\n\nfor image_path in image_paths:\n    img = skimage.io.imread(image_path)\n    img_arr = np.array(img)\n    results = model.detect([img_arr], verbose=1)\n    r = results[0]\n    visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'],\n                                dataset_val.class_names, r['scores'], figsize=(5,5))\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "RexNihilis",
        "body": "I don't understand what caused the error and how it can be solved.\nMask r-cnn model taken from here: https://github.com/akTwelve/Mask_RCNN"
      },
      {
        "user": "Venkat6871",
        "body": "Hi @RexNihilis ,\nApologies for the delay, and thank you for raising your concern here. It appears that your code is based on TensorFlow 1.x, which is no longer actively supported. We recommend upgrading to TensorFlow 2.x to ensure compatibility with the latest Python versions and system libraries. TensorFlow 2.x provides improved performance, a more intuitive API, and better support for modern hardware.\nFor your convenience, here are some useful resources:\n[TensorFlow 1.x to 2.x Migration Guide](https://www.tensorflow.org/guide/migrate).\n[TensorFlow API Compatibility Checker](https://www.tensorflow.org/install/source_windows).\nIf you need any help during the migration process, feel free to reach out  we are happy to assist further.\n\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      }
    ]
  },
  {
    "issue_number": 93830,
    "title": "Bug: Chained HashedCrossing in TF 2.16.2 results in (None, D) vs (batch_size, D) input shapes",
    "author": "gongkang",
    "state": "closed",
    "created_at": "2025-05-21T12:31:56Z",
    "updated_at": "2025-06-10T02:14:44Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:apis",
      "TF 2.16"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16.2\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nmacOS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen defining a Keras Functional API model that uses chained `tf.keras.layers.HashedCrossing` for three or more features, a shape inconsistency error occurs during `model.fit()`. Specifically, when the output of a first `HashedCrossing` layer (crossing features A and B, resulting in a symbolic shape like `(None, 1)`) is used as an input to a second `HashedCrossing` layer along with a third feature C (derived from a tf.keras.layers.Input via `tf.keras.layers.StringLookup`, also initially `(None, 1)`), the model fails to build.\n\nThe error message indicates that the third feature C's tensor has its batch dimension concretized to the actual batch size from `model.fit()` (e.g., `(batch_size, 1)`), while the intermediate tensor from the first cross retains its symbolic `(None, 1)` batch dimension. This leads to a `ValueError: All HashedCrossing inputs should have equal shape.`\n\nThis issue occurs even without `tf.distribute.MirroredStrategy` and when all `tf.keras.layers.Input` layers are correctly defined with a symbolic `None` batch dimension. Crossing only two features with a single `HashedCrossing` layer works correctly.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nprint(f\"TensorFlow Version: {tf.__version__}\")\n\n# Optional: Test with eager execution\n# tf.config.run_functions_eagerly(True)\n\n# 1. Define Model Inputs (all initially with symbolic batch_size=None)\nvocab_list = [\"1\", \"2\", \"3\", \"cat\", \"dog\", \"mouse\"] # Example vocabulary\ninput_a = keras.Input(shape=(1,), dtype=tf.string, name='feature_a')\ninput_b = keras.Input(shape=(1,), dtype=tf.string, name='feature_b')\ninput_c = keras.Input(shape=(1,), dtype=tf.string, name='feature_c')\n\n# 2. StringLookup for each feature\nlookup_a = keras.layers.StringLookup(vocabulary=vocab_list, mask_token=None, num_oov_indices=0, name='lookup_a')(input_a)\nlookup_b = keras.layers.StringLookup(vocabulary=vocab_list, mask_token=None, num_oov_indices=0, name='lookup_b')(input_b)\nlookup_c = keras.layers.StringLookup(vocabulary=vocab_list, mask_token=None, num_oov_indices=0, name='lookup_c')(input_c)\n\nprint(f\"Initial shape of lookup_a: {lookup_a.shape}\")\nprint(f\"Initial shape of lookup_b: {lookup_b.shape}\")\nprint(f\"Initial shape of lookup_c: {lookup_c.shape}\")\n\n# 3. Chained HashedCrossing\nnum_bins_crossing = 1000\n# First cross (A x B)\ncross_ab = keras.layers.HashedCrossing(num_bins=num_bins_crossing, name='cross_ab')([lookup_a, lookup_b])\nprint(f\"Shape of cross_ab: {cross_ab.shape}\")\n\n# Second cross ((A x B) x C) - This is where the error is expected during model.fit()\n# due to shape mismatch between cross_ab and lookup_c\ncross_abc = keras.layers.HashedCrossing(num_bins=num_bins_crossing, name='cross_abc')([cross_ab, lookup_c])\nprint(f\"Shape of cross_abc: {cross_abc.shape}\")\n\n\n# 4. Embedding and Output layers\nembedding_dim = 8\nembedding = keras.layers.Embedding(input_dim=num_bins_crossing, output_dim=embedding_dim, name='embedding')(cross_abc)\nflatten = keras.layers.Flatten(name='flatten')(embedding)\noutput = keras.layers.Dense(1, activation='sigmoid', name='output')(flatten)\n\n# 5. Create and Compile Model\nmodel = keras.Model(inputs=[input_a, input_b, input_c], outputs=output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# 6. Create Dummy Data\nnum_samples = 200\nbatch_s = 10 # Using a batch size that will appear in the error\n\ndummy_a_np = np.random.choice(vocab_list, size=(num_samples, 1))\ndummy_b_np = np.random.choice(vocab_list, size=(num_samples, 1))\ndummy_c_np = np.random.choice(vocab_list, size=(num_samples, 1))\n\n# Ensure input arrays are of dtype=object for robust tf.string conversion\ndummy_a = dummy_a_np.astype(object)\ndummy_b = dummy_b_np.astype(object)\ndummy_c = dummy_c_np.astype(object)\n\ndummy_labels = np.random.randint(0, 2, size=(num_samples, 1)).astype(np.float32)\n\nprint(f\"\\nFeeding data with batch_size = {batch_s}\")\n# 7. Fit the model - Error is expected here\ntry:\n    model.fit(\n        [dummy_a, dummy_b, dummy_c],\n        dummy_labels,\n        batch_size=batch_s,\n        epochs=1,\n        verbose=2\n    )\nexcept ValueError as e:\n    print(f\"\\nCaught expected ValueError: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\nScript finished.\")\n```\n\n### Relevant log output\n\n```shell\nValueError: Exception encountered when calling HashedCrossing.call().\n\nAll `HashedCrossing` inputs should have equal shape. Received: inputs=[<tf.Tensor 'functional_1/hashed_crossing_1/Reshape:0' shape=(None, 1) dtype=int64>, <tf.Tensor 'functional_1/string_lookup_2_1/Identity:0' shape=(10, 1) dtype=int64>]\n\nArguments received by HashedCrossing.call():\n  • inputs=['tf.Tensor(shape=(None, 1), dtype=int64)', 'tf.Tensor(shape=(10, 1), dtype=int64)']ValueError: Exception encountered when calling HashedCrossing.call().\n\nAll `HashedCrossing` inputs should have equal shape. Received: inputs=[<tf.Tensor 'functional_1/hashed_crossing_1/Reshape:0' shape=(None, 1) dtype=int64>, <tf.Tensor 'functional_1/string_lookup_2_1/Identity:0' shape=(10, 1) dtype=int64>]\n\nArguments received by HashedCrossing.call():\n  • inputs=['tf.Tensor(shape=(None, 1), dtype=int64)', 'tf.Tensor(shape=(10, 1), dtype=int64)']\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @gongkang ,\nApologies for the delay, and thank you for raising your concern here.\nI tried running your code on Colab using TensorFlow version 2.19.0, and although I encountered a different issue initially, it worked correctly after making a few modifications. I have attached a [gist](https://colab.sandbox.google.com/gist/Venkat6871/296c7044fbccf7cbcf8773d3c913fc1d/93830_tf_2-19-0-v.ipynb) for your reference.\nIt looks like you are using an older version of TensorFlow. I recommend upgrading to the latest version to ensure compatibility with newer APIs.\nPlease feel free to let me know if I missed anything or if you observe different behavior on your end.\n\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 47905,
    "title": "tflite runtime API + NNAPI delegate refusing layers from a MobileNetV2SSD trained with Object Detection API, with high inference time",
    "author": "FSet89",
    "state": "closed",
    "created_at": "2021-03-19T08:02:39Z",
    "updated_at": "2025-06-09T16:50:16Z",
    "labels": [
      "stat:awaiting response",
      "stale",
      "comp:lite",
      "type:performance",
      "TFLiteNNAPIDelegate"
    ],
    "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): training on Ubuntu 18.04, inference on Yocto\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15 (tflite runtime 2.3)\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11\r\n- GPU model and memory: Titan-v 12GB\r\n\r\n\r\n**Describe the current behavior**\r\nI trained a Object detection model (MobileNetV2SSD) from scratch using the Object Detection API. I used TF version 1.15 as I needed to perform quantization-aware training. My pipeline config file includes the following block:\r\n\r\n```\r\ngraph_rewriter {\r\n  quantization { \r\n  delay: 48000\r\n  weight_bits: 8\r\n  activation_bits: 8\r\n  } \r\n}\r\n```\r\nThe model was then exported using the `object_detection/export_tflite_ssd_graph.py` script and converted to tflite format as described [here](https://neuralet.com/article/quantization-of-tensorflow-object-detection-api-models/) (full integer quantization). I then tested the model with the tflite runtime API + NNAPI on a board running YOCTO and equipped with a NPU. The inference time was very high (0.5 s) and I had many warnings from the NNAPI delegate which refused quantization operators (see below). Did I do something wrong?\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nInference code:\r\n```\r\nimport tflite_runtime.interpreter as tflite\r\nimport sys\r\nimport cv2\r\nimport numpy as np\r\nimport time\r\nimport os\r\n\r\nmodel_path =\"model_path\"\r\nimg_path = \"img_path\"\r\n\r\ninterpreter = tflite.Interpreter(model_path=model_path)\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nheight = input_details[0]['shape'][1]\r\nwidth = input_details[0]['shape'][2]\r\n\r\nimg = cv2.imread(img_path)\r\nimg = cv2.resize(img, (300,300))\r\n\r\nfor  i in range(100):\r\n       \r\n        input_data = np.expand_dims(img, axis=0)\r\n        input_data = (input_data/255.0).astype(np.float32)\r\n\r\n        interpreter.set_tensor(input_details[0]['index'], input_data)\r\n        t1 = time.time()\r\n        interpreter.invoke()\r\n        t2 = time.time()\r\n\r\n        output_data = interpreter.get_tensor(output_details[0]['index'])\r\n        print(t2-t1)\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n**INFO: Created TensorFlow Lite delegate for NNAPI.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator CUSTOM (v1) refused by NNAPI delegate: Unsupported operation type.\r\nApplied NNAPI delegate.**\r\n\r\n",
    "comments": [
      {
        "user": "abattery",
        "body": "@miaowang14 could you take a look?"
      },
      {
        "user": "miaowang14",
        "body": "Most of the errors here are related to the quantize / dequantize nodes. And I am not expecting FAKE_QUANT nodes in the exported model, which seems indicating that there seems something wrong with exporting script.\r\n\r\n@lev-prol , have you seen similar issues before?"
      },
      {
        "user": "pjpratik",
        "body": "Hi,\r\n\r\nThank you for opening this issue. Since this issue has been open for a long time, the code/debug information for this issue may not be relevant with the current state of the code base.\r\n\r\nThe TFLite team is constantly improving the framework by fixing bugs and adding new features. We suggest you try the latest TensorFlow [version](https://pypi.org/project/tensorflow/) with the latest compatible hardware configuration which could potentially resolve the issue. If you are still facing the issue, please create a new GitHub issue with your latest findings, with all the debugging information which could help us investigate.\r\n\r\nPlease follow the [release notes](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md) to stay up to date with the latest developments which are happening in the TFLite space.\r\n\r\nThanks."
      }
    ]
  },
  {
    "issue_number": 70730,
    "title": "Build error in tensorflow lite minimal example",
    "author": "bossebandowski",
    "state": "closed",
    "created_at": "2024-07-02T09:03:56Z",
    "updated_at": "2025-06-09T16:47:22Z",
    "labels": [
      "stat:contribution welcome",
      "awaiting review",
      "type:build/install",
      "stale",
      "comp:lite",
      "TF 2.16"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16.2\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n11.4.0\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nFollowing guide at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal\r\n\r\nDid not modify any files. The build step (5) fails with an undefined reference error (see log outputs).\r\nBuilding standalone tensorflow lite with cmake following https://www.tensorflow.org/lite/guide/build_cmake works just fine.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nsudo apt-get install cmake\r\nwget http://es.archive.ubuntu.com/ubuntu/pool/main/libf/libffi/libffi7_3.3-4_amd64.deb\r\nsudo dpkg -i libffi7_3.3-4_amd64.deb\r\ngit clone https://github.com/tensorflow/tensorflow.git tensorflow_src\r\nmkdir minimal_build\r\ncd minimal_build\r\ncmake ../tensorflow_src/tensorflow/lite/examples/minimal\r\ncmake --build . -j\n```\n\n\n### Relevant log output\n\n```shell\n[100%] Built target tensorflow-lite\r\n[100%] Building CXX object CMakeFiles/minimal.dir/minimal.cc.o\r\n[100%] Linking CXX executable minimal\r\n/usr/bin/ld: tensorflow-lite/libtensorflow-lite.a(fully_connected.cc.o): in function `tflite::ops::builtin::fully_connected::EvalHybridDense4Bit(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':\r\nfully_connected.cc:(.text+0x3d02): undefined reference to `tflite::optimized_4bit::ReferenceBatchQuantizeFloats4Bit(float const*, int, int, signed char*, float*, int, int, int*)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0x3d50): undefined reference to `tflite::optimized_4bit::ReferenceAssignBiasAndComputeOffsets(int const*, float const*, float const*, float const*, float*, int, int)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0x3db9): undefined reference to `void tflite::optimized_4bit::ReferenceRunKernel<4, 1, 32>(unsigned char const*, signed char const*, int*, int, int, int, int, int, int)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0x3ded): undefined reference to `void tflite::optimized_4bit::ReferenceUnpack<4, 1>(float*, int const*, int, int, float const*, float const*, int, int)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0x3ff6): undefined reference to `tflite::optimized_4bit::ReferencePrepack(unsigned char*, signed char const*, int, int, int, int, int, int)'\r\ncollect2: error: ld returned 1 exit status\r\ngmake[2]: *** [CMakeFiles/minimal.dir/build.make:186: minimal] Error 1\r\ngmake[1]: *** [CMakeFiles/Makefile2:1372: CMakeFiles/minimal.dir/all] Error 2\r\ngmake: *** [Makefile:136: all] Error 2\n```\n",
    "comments": [
      {
        "user": "sawantkumar",
        "body": "Hi @pkgoogle ,\r\n\r\nWhile replicating this issue I keep getting compilation error like below and the vm freezes, can you please take a look below\r\n\r\n`[100%] Building CXX object tensorflow-lite/CMakeFiles/tensorflow-lite.dir/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/compiler/mlir/lite/experimental/remat/metadata_util.cc.o\r\nIn file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h: In constructor ‘tflite::delegates::NnapiPlugin::NnapiPlugin(const tflite::TFLiteSettings&)’:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:74:52: warning: ‘SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int]’ is deprecated: use size() instead [-Wdeprecated-declarations]\r\n   74 |         nnapi_settings->accelerator_name()->Length() != 0) {\r\n      |                                                    ^\r\nIn file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,\r\n                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here\r\n  168 |   SizeT Length() const { return size(); }\r\n      |         ^~~~~~\r\nIn file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h: In member function ‘void tflite::delegates::NnapiPlugin::SetCompilationCacheDir(const tflite::TFLiteSettings&)’:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:103:77: warning: ‘SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int]’ is deprecated: use size() instead [-Wdeprecated-declarations]\r\n  103 |         tflite_settings.compilation_caching_settings()->cache_dir()->Length() !=\r\n      |                                                                             ^\r\nIn file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,\r\n                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here\r\n  168 |   SizeT Length() const { return size(); }\r\n      |         ^~~~~~\r\nIn file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:110:76: warning: ‘SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int]’ is deprecated: use size() instead [-Wdeprecated-declarations]\r\n  110 |                tflite_settings.nnapi_settings()->cache_directory()->Length() !=\r\n      |                                                                            ^\r\nIn file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,\r\n                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here\r\n  168 |   SizeT Length() const { return size(); }\r\n      |         ^~~~~~\r\nIn file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h: In member function ‘void tflite::delegates::NnapiPlugin::SetModelToken(const tflite::TFLiteSettings&)’:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:122:26: warning: ‘SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int]’ is deprecated: use size() instead [-Wdeprecated-declarations]\r\n  122 |                 ->Length() != 0) {\r\n      |                          ^\r\nIn file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,\r\n                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here\r\n  168 |   SizeT Length() const { return size(); }\r\n      |         ^~~~~~\r\nIn file included from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:127:72: warning: ‘SizeT flatbuffers::Vector<T, SizeT>::Length() const [with T = char; SizeT = unsigned int]’ is deprecated: use size() instead [-Wdeprecated-declarations]\r\n  127 |                tflite_settings.nnapi_settings()->model_token()->Length() != 0) {\r\n      |                                                                        ^\r\nIn file included from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/array.h:25,\r\n                 from /home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/flatbuffers.h:24,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/acceleration/configuration/configuration_generated.h:21,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.h:32,\r\n                 from /home/sawantkumar/work/minimal_build/tensorflow_src/tensorflow/lite/core/acceleration/configuration/nnapi_plugin.cc:18:\r\n/home/sawantkumar/work/minimal_build/minimal_build/flatbuffers/include/flatbuffers/vector.h:168:9: note: declared here\r\n  168 |   SizeT Length() const { return size(); }` "
      },
      {
        "user": "pkgoogle",
        "body": "Hi @bossebandowski, I was able to build with these steps on the nightly branch, will that work for you?\r\n\r\n\r\nMy OS and gcc:\r\n```sh\r\n$ gcc --version\r\ngcc (Debian 13.2.0-13) 13.2.0\r\nCopyright (C) 2023 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n\r\n```sh\r\n$ python --version\r\nPython 3.10.12\r\n```\r\n\r\nI already have cmake installed and I don't need the optional steps so here are my steps:\r\n```sh\r\ngit clone https://github.com/tensorflow/tensorflow.git tensorflow_src\r\ncd tensorflow_src\r\ngit switch nightly\r\ncd ..\r\nmkdir minimal_build\r\ncd minimal_build\r\ncmake ../tensorflow_src/tensorflow/lite/examples/minimal\r\ncmake --build . -j\r\n```"
      },
      {
        "user": "bossebandowski",
        "body": "Hey,\r\n\r\nthanks for the reply. I updated to gcc-13 but I am still getting the same error.\r\n\r\n```\r\ngcc --version\r\ngcc (Ubuntu 13.1.0-8ubuntu1~22.04) 13.1.0\r\nCopyright (C) 2023 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n```\r\n/usr/bin/ld: tensorflow-lite/libtensorflow-lite.a(fully_connected.cc.o): in function `tflite::ops::builtin::fully_connected::EvalHybridDense4Bit(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':\r\nfully_connected.cc:(.text+0xc919): undefined reference to `tflite::optimized_4bit::ReferenceBatchQuantizeFloats4Bit(float const*, int, int, signed char*, float*, int, int, int*)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0xc967): undefined reference to `tflite::optimized_4bit::ReferenceAssignBiasAndComputeOffsets(int const*, float const*, float const*, float const*, float*, int, int)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0xc9cf): undefined reference to `void tflite::optimized_4bit::ReferenceRunKernel<4, 1, 32>(unsigned char const*, signed char const*, int*, int, int, int, int, int, int)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0xca02): undefined reference to `void tflite::optimized_4bit::ReferenceUnpack<4, 1>(float*, int const*, int, int, float const*, float const*, int, int)'\r\n/usr/bin/ld: fully_connected.cc:(.text+0xcc2d): undefined reference to `tflite::optimized_4bit::ReferencePrepack(unsigned char*, signed char const*, int, int, int, int, int, int)'\r\ncollect2: error: ld returned 1 exit status\r\ngmake[2]: *** [CMakeFiles/minimal.dir/build.make:186: minimal] Error 1\r\ngmake[1]: *** [CMakeFiles/Makefile2:1307: CMakeFiles/minimal.dir/all] Error 2\r\ngmake: *** [Makefile:136: all] Error 2\r\n```\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 36697,
    "title": "Training suddenly freezes",
    "author": "muellesi",
    "state": "closed",
    "created_at": "2020-02-12T14:42:05Z",
    "updated_at": "2025-06-09T15:53:35Z",
    "labels": [
      "stat:awaiting response",
      "type:support",
      "comp:apis",
      "TF 2.0"
    ],
    "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): unknown 2.0.0 (see below)\r\n\r\ntensorboard=2.0.0=pyhb38c66f_1\r\ntensorflow=2.0.0=gpu_py37h57d29ca_0\r\ntensorflow-base=2.0.0=gpu_py37h390e234_0\r\ntensorflow-estimator=2.0.0=pyh2649769_0\r\ntensorflow-gpu=2.0.0=h0d30ee6_0\r\ntensorflow-probability=0.8.0=py_0\r\n\r\n\r\n- Python version: Python 3.7.6 (Anaconda)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cudatoolkit=10.0.130=0 / cudnn=7.6.4=cuda10.0_0\r\n- GPU model and memory: Nvidia GeForce RTX 2070 8GB\r\n\r\n\r\n**Describe the current behavior**\r\nTraining a CNN using `tf.keras.Model.fit()` and tensorflow's data pipeline with tfrecord files randomly seems to freeze/stop/hang. Whenever this happens, the console process stays open but the CPU/GPU will return to 0% utilization. Using verbose=1 as well as tensorboard also shows that no progress is being made anymore.\r\n\r\nWaiting for a while does not help. However, if I restart the training without rebooting my PC, the hang is much more likely to occur again.\r\n\r\nLooking into Process explorer, I can see that there is only one really active thread:\r\n![grafik](https://user-images.githubusercontent.com/6770131/74343696-c0f13100-4dab-11ea-9858-e7bb70908107.png)\r\n\r\nA Callstack for the thread is attached below. The upper few frames (nvcuda.dll!cuProfilerStop) change when refreshing but the rest of the frame stays constant.\r\n\r\n\r\n**Describe the expected behavior**\r\nThe training should continue normally.\r\n\r\n\r\n**Code to reproduce the issue**\r\nSince I was not able to reproduce the problem reliably, I have no idea which part of my code might actually be important, The data pipeline uses `dataset.map()` with `num_parallel_calls = tf.data.experimental.AUTOTUNE` for multiprocessing and contains two `tf.np_function()`s (after which I have to use tf.ensure_shape to recover the correct shape for the data).\r\n\r\n**Other info / logs**\r\n\r\n```\r\nnvcuda.dll!cuProfilerStop+0x226617\r\nnvcuda.dll!cuProfilerStop+0x17243c\r\nnvcuda.dll+0x4bbbe\r\nnvcuda.dll+0x10c29d\r\nnvcuda.dll!cuProfilerStop+0x5cc42\r\nnvcuda.dll!cuProfilerStop+0x5da20\r\nnvcuda.dll+0x10bbf0\r\nnvcuda.dll+0x10be3c\r\nnvcuda.dll+0xeac0\r\nnvcuda.dll!cuCtxSynchronize+0x1c2\r\n_pywrap_tensorflow_internal.pyd!std::unique_ptr<tensorflow::Status::State,std::default_delete<tensorflow::Status::State> >::~unique_ptr<tensorflow::Status::State,std::default_delete<tensorflow::Status::State> >+0x10b4f\r\n_pywrap_tensorflow_internal.pyd!tensorflow::Env::NowSeconds+0xd16\r\n_pywrap_tensorflow_internal.pyd!tensorflow::AllocationRecord::Clear+0x550c\r\n_pywrap_tensorflow_internal.pyd!google::protobuf::RepeatedPtrField<tensorflow::InterconnectLink>::Add+0x8f0c\r\n_pywrap_tensorflow_internal.pyd!std::vector<tensorflow::DtypeAndPartialTensorShape,std::allocator<tensorflow::DtypeAndPartialTensorShape> >::operator=+0x623\r\n_pywrap_tensorflow_internal.pyd!TFE_TensorHandleResolve+0x227\r\n_pywrap_tensorflow_internal.pyd!tensorflow::DataTypeSet::Contains+0x2350\r\n_pywrap_tensorflow_internal.pyd!std::vector<tensorflow::monitoring::Point::Label,std::allocator<tensorflow::monitoring::Point::Label> >::reserve+0x85a\r\npython37.dll!PyMethodDef_RawFastCallKeywords+0x387\r\npython37.dll!PyMethodDef_RawFastCallKeywords+0xa5c\r\npython37.dll!PyEval_EvalFrameDefault+0x403\r\npython37.dll!PyFunction_FastCallDict+0xdd\r\npython37.dll!PyObject_FastCall_Prepend+0x6c\r\npython37.dll!PySet_Contains+0x50d\r\npython37.dll!PyErr_NoMemory+0x24eaf\r\npython37.dll!PyEval_SliceIndex+0x42\r\npython37.dll!PySlice_Unpack+0x9d\r\n_multiarray_umath.cp37-win_amd64.pyd+0xc117e\r\n_multiarray_umath.cp37-win_amd64.pyd+0xc0093\r\npython37.dll!PyEval_EvalFrameDefault+0x7e4\r\npython37.dll!PyEval_EvalCodeWithName+0x1a6\r\npython37.dll!PyFunction_FastCallDict+0x1ba\r\npython37.dll!PySlice_New+0x23d\r\npython37.dll!PyEval_EvalFrameDefault+0x1174\r\npython37.dll!PyEval_EvalCodeWithName+0x1a6\r\npython37.dll!PyFunction_FastCallDict+0x1ba\r\npython37.dll!PyObject_Call_Prepend+0x6c\r\npython37.dll!PyType_GetDocFromInternalDoc+0x22d\r\npython37.dll!PyObject_Call+0x75\r\n_pywrap_tensorflow_internal.pyd!std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar::PtrOpKernelFactory>::operator()+0x97d\r\n_pywrap_tensorflow_internal.pyd!std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar::PtrOpKernelFactory>::operator()+0x420\r\n_pywrap_tensorflow_internal.pyd!tensorflow::NodeDef::mutable_experimental_debug_info+0xef56\r\n_pywrap_tensorflow_internal.pyd!tensorflow::NodeDef::mutable_experimental_debug_info+0x11f78\r\n_pywrap_tensorflow_internal.pyd!tensorflow::data::DatasetBaseIterator::RecordElement+0x6f\r\n_pywrap_tensorflow_internal.pyd!Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop+0x3f6\r\n_pywrap_tensorflow_internal.pyd!Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop+0x701\r\n_pywrap_tensorflow_internal.pyd!tensorflow::WindowsFileSystem::TranslateName+0x255\r\n_pywrap_tensorflow_internal.pyd!tensorflow::SavedAsset::GetCachedSize+0x2c19\r\nucrtbase.dll!beginthreadex+0x142\r\nKERNEL32.DLL!BaseThreadInitThunk+0x14\r\nntdll.dll!RtlUserThreadStart+0x21\r\n```\r\n",
    "comments": [
      {
        "user": "gadagashwini-zz",
        "body": "@muellesi, Could you provide the standalone code to replicate the issue. "
      },
      {
        "user": "gadagashwini-zz",
        "body": "@muellesi, Is this still an issue?"
      },
      {
        "user": "muellesi",
        "body": "Unfortunately yes, but it only happens very sporadically and I was not yet able to reproduce the problem reliably. Therefore I also don't have any piece of standalone code that reliably reproduces the issue. However, I had the problem on three different PCs now. If I find a repro, I will report back."
      }
    ]
  },
  {
    "issue_number": 94735,
    "title": "tf.data.experimental.prefetch_to_device has no effect inside tf.distribute.Strategy.distribute_datasets_from_function.",
    "author": "API92",
    "state": "open",
    "created_at": "2025-06-03T16:12:51Z",
    "updated_at": "2025-06-09T09:57:28Z",
    "labels": [
      "comp:data",
      "type:performance",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nPerformance\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.19.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nRHEL 9.4\n\n\n### Python version\n\n3.11\n\n\n### CUDA/cuDNN version\n\n12.5\n\n\n### Current behavior?\n\nMemcpyH2D does not overlap with model computation when using tf.data.experimental.prefetch_to_device inside tf.distribute.MirroredStrategy.distribute_datasets_from_function. I would expect this operations to overlap.\n\n![Image](https://github.com/user-attachments/assets/ae8e9e94-eb9b-49e8-b335-c22d30a0aec0)\n\n### Standalone code to reproduce the issue\n\n```python\nimport tensorflow as tf\n\nclass Model(tf.keras.Model):\n    def call(self, x):\n        y = x / 1000\n        for i in range(3):\n            y = tf.matmul(y, x / 1000)\n        return tf.reduce_sum(y, axis=[1, 2])\n\ndef get_dataset(ictx):\n    ds = tf.data.Dataset.range(1, 1001, output_type=tf.float32)\n    ds = ds.map(lambda i: (tf.ones((1024 * 5, 1024 * 5)) / i, 0.0))\n    ds = ds.batch(8)\n    ds = ds.apply(tf.data.experimental.prefetch_to_device('gpu'))\n    return ds\n\nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n    ds = strategy.distribute_datasets_from_function(get_dataset)\n    model = Model()\n    model.compile(loss='mse')\n    model.fit(\n        ds,\n        epochs=1,\n        steps_per_epoch=30,\n        callbacks=tf.keras.callbacks.TensorBoard(profile_batch=(15, 25)))\n```\n\n[gist](https://colab.research.google.com/drive/1LmGKFEtveVC5-3KgIcsjB7ZTE0yYdBEb?usp=sharing)",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @API92 ,\nApologies for the delay, and thank you for raising your concern here.\nI tried to replicate your issue using TensorFlow 2.19.0 with GPU on my setup. However, I encountered a different issue related to dataset compatibility with `model.fit()` when using `tf.distribute.MirroredStrategy` in combination with `tf.data.experimental.prefetch_to_device()`. Here i am providing [gist](https://colab.sandbox.google.com/gist/Venkat6871/8bb7b151c63dffd2ff934f4021a3fc4d/94735_tf_2-19-0-v.ipynb) for your reference.\nCould you please take a look and let me know if I missed anything in reproducing your setup?\n\nThank you!"
      },
      {
        "user": "API92",
        "body": "Hi @Venkat6871.\nYou've encountered a bug in keras 3.8. You have to use keras 3.9+. Here is my [gist](https://colab.research.google.com/drive/1LmGKFEtveVC5-3KgIcsjB7ZTE0yYdBEb?usp=sharing). Also I can reproduce my issue with tf 2.11 and keras 2.11."
      }
    ]
  },
  {
    "issue_number": 53572,
    "title": "tf.data.Dataset .map().batch() pattern is not matched to use fused implementation.",
    "author": "mcourteaux",
    "state": "open",
    "created_at": "2021-12-29T12:25:16Z",
    "updated_at": "2025-06-09T06:17:14Z",
    "labels": [
      "comp:tensorboard",
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:data",
      "TF 2.7"
    ],
    "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.1-69264-g0cdf35562dc 2.9.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.5 / 8.3\r\n- GPU model and memory: GTX1660 Ti\r\n\r\n**Describe the current behavior**\r\ncombining `tf.data.Dataset.map()` with `.batch()` does not use the fused BatchAndMap implementation.\r\n\r\n**Describe the expected behavior**\r\nIt does use the fused implementation. Currently, it's only possible to use the fused implementation when using the deprecated `experimental.map_and_batch()` transformation.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```py\r\nimport os\r\nimport datetime\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nprint('TF version', tf.__version__)\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        # Currently, memory growth needs to be the same across GPUs\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), 'Physical GPUs,', len(logical_gpus), 'Logical GPUs')\r\n    except RuntimeError as e:\r\n        # Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\n\r\n\r\n@tf.function\r\ndef do_stuff(wmat, tf_var):\r\n    with tf.device(\"/gpu:0\"):\r\n        S = tf.constant(0.0)\r\n        for i in tf.range(4):\r\n            fi = tf.cast(i, dtype=tf.float32)\r\n            A = tf.math.lgamma(tf.tanh(tf.matmul(wmat + fi, tf.transpose(wmat - fi, [0, 2, 1]))))\r\n            S += tf.reduce_sum(A)\r\n        error = tf.reduce_mean(tf_var)\r\n        return error, S\r\n\r\nexp_uuid = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n\r\nn_batches = 512\r\n\r\n\r\ndef gen():\r\n    for i in range(n_batches):\r\n        with tf.device(\"/cpu:0\"): # Make sure it comes from CPU\r\n            r = tf.ones((400,800))\r\n        yield r\r\n\r\noption_names = ['map().batch()', 'map_and_batch()']\r\nfor option in range(2):\r\n\r\n    with tf.device(\"/cpu:0\"):\r\n        dataset = tf.data.Dataset.from_generator(gen, output_types=tf.float32)\r\n\r\n        def my_identity(x):\r\n            with tf.device(\"/cpu:0\"):\r\n                print(\"my_identity input:\", x, x.device)\r\n                y = tf.identity(x)\r\n                print(\"my_identity output:\", y, y.device)\r\n                return y\r\n\r\n        if option == 0:\r\n            ## Option 0: map().batch()\r\n            dataset = dataset.map(my_identity).batch(16)\r\n\r\n        elif option == 1:\r\n            ## Option 1: deprecated map_and_batch()\r\n            dataset = dataset.apply(tf.data.experimental.map_and_batch(my_identity, 16))\r\n\r\n    gpu_transform = tf.data.experimental.prefetch_to_device('/gpu:0', buffer_size=4)\r\n    dataset = dataset.apply(gpu_transform)\r\n\r\n\r\n    tf_var = tf.Variable(tf.zeros(3))\r\n    adam = tf.keras.optimizers.Adam(1e-4)\r\n    logpath = os.path.join('data', 'logs', 'pa_' + exp_uuid + '_' + option_names[option])\r\n\r\n    tf.profiler.experimental.start(logpath)\r\n    start = datetime.datetime.now()\r\n    for b, wmat in tqdm(enumerate(dataset)):\r\n        with tf.GradientTape() as tape:\r\n\r\n            if b == 0:\r\n                print('\\n\\n dataset element device', wmat.device)\r\n                print('\\n')\r\n\r\n            # Do some calculations\r\n            result = do_stuff(wmat, tf_var)\r\n\r\n        grads = tape.gradient(result[0], [tf_var])\r\n        adam.apply_gradients(zip(grads, [tf_var]))\r\n    stop = datetime.datetime.now()\r\n    tf.profiler.experimental.stop()\r\n\r\n    print(f'\\n\\nOption {option_names[option]}\\n===========================\\n')\r\n    print(logpath)\r\n    print('Time lapsed=', stop - start)\r\n    print(\"\\n\\n\")\r\n```\r\n\r\n**Other info / logs**\r\n\r\n**Option 1**:\r\n![image](https://user-images.githubusercontent.com/845012/147661299-a7f72017-00ff-47b6-bb71-8812bd5163d3.png)\r\nSymptoms:\r\n - See the blocks `Iterator::FlapMap` and `Iterator::BatchV2` stacked on top of each other.\r\n - The MemcpyH2D (selected, see the details panel) is comping from pagable memory, instead of pinned memory (which is what MapAndBatch does). Because of the source being pagable memory, it can't overlap with kernel computations.\r\n \r\n**Option 2**:\r\n![image](https://user-images.githubusercontent.com/845012/147661558-9f316201-a4e5-4df1-a77b-032272537321.png)\r\nEvidence:\r\n - The MapAndBatch block is used.\r\n - The MemcopyH2D comes from pinned memory (see details pane) and overlaps with kernel computations.\r\n\r\nThe whole deal about pinned memory is to allow parallel data upload and kernel computations. So the dataset needs to be produced into pinned host memory, which then can be uploaded asynchronously by the driver without an extra copy. See https://github.com/tensorflow/tensorflow/issues/43905#issuecomment-823675760 and https://github.com/tensorflow/tensorflow/issues/43905#issuecomment-824145184 and:\r\nhttps://github.com/tensorflow/tensorflow/blob/40e9b534962989af7486bc6567ca472d71eb5049/tensorflow/core/kernels/data/experimental/map_and_batch_dataset_op.cc#L522\r\n\r\nThis is a follow up on https://github.com/tensorflow/tensorflow/issues/43905.\r\n",
    "comments": [
      {
        "user": "tilakrayal",
        "body": "@mcourteaux ,\r\nWe see that you are using tf version 1.12, 1.x is not actively supported, please update to latest stable tf v2.7 and let us know if you are using same issue."
      },
      {
        "user": "mcourteaux",
        "body": "No, I'm most definitely not. This was a fresh build from master branch from yesterday. Idk why the script that gives the TF version gives 1.12. It's most definitely wrong. I moved to TF 2 years ago. Note that that is the GIT_VERSION. Instead, `tf.version.VERSION` gives 2.9.0."
      },
      {
        "user": "tilakrayal",
        "body": "@mcourteaux ,\r\n I was able to execute the code without any issues.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d0b954e177f03ab8e85b6fa087612ba6/untitled171.ipynb).Please provide the error log and also confirm if anything is missing here.Thanks!"
      }
    ]
  },
  {
    "issue_number": 93130,
    "title": "`../tensorflow/third_party/xla/third_party/tsl/tsl/platform/ml_dtypes.h:19:10: error: 'ml_dtypes/include/float8.h' file not found [clang-diagnostic-error]`",
    "author": "SwuduSusuwu",
    "state": "open",
    "created_at": "2025-05-11T09:04:05Z",
    "updated_at": "2025-06-09T04:53:48Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:build/install",
      "subtype: ubuntu/linux"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\n- https://github.com/SwuduSusuwu/SusuLib/blob/304e6ecb8a339e4a74cd5845cd54649552f9b6ba/build.sh#L32-L93\n- https://github.com/SwuduSusuwu/SusuLib/blob/304e6ecb8a339e4a74cd5845cd54649552f9b6ba/cxx/ClassTensorFlowCns.hxx\n\n### TensorFlow version\n\ncommit d92ed6014f19e396a16b980c4f8969a74e8ada3a (grafted, HEAD -> master, origin/master, origin/HEAD)\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLocal [_Termux_](https://github.com/termux-play-store) on stock [_AOSP_](https://github.com/aosp-mirror) _Arm64_.\n- Reproduced on [GitHub Workflow (Ubuntu _x86-64_))](https://github.com/tensorflow/tensorflow/issues/93130#issuecomment-2876987576):\n\n### Mobile device\n\n_Google Pixel 6_\n\n### Python version\n\nNot applicable (_C++_)\n\n### Bazel version\n\nNot applicable (not linker test, just compiler test)\n\n### GCC/compiler version\n\n`clang version 20.1.3`\n\n### CUDA/cuDNN version\n\nNot applicable (_Arm64_)\n\n### GPU model and memory\n\nNot applicable (_Arm64_)\n\n### Current behavior?\n\n```\nIn file included from ./cxx/ClassTensorFlowCns.hxx:16:\nIn file included from ../tensorflow/tensorflow/core/framework/tensor.h:27:\nIn file included from ../tensorflow/tensorflow/core/framework/allocator.h:26:\nIn file included from ../tensorflow/third_party/xla/xla/tsl/framework/allocator.h:26:\nIn file included from ../tensorflow/third_party/xla/xla/tsl/framework/numeric_types.h:22:\nIn file included from ../tensorflow/third_party/xla/xla/tsl/platform/types.h:22:\n../tensorflow/third_party/xla/third_party/tsl/tsl/platform/ml_dtypes.h:19:10: fatal error: 'ml_dtypes/include/float8.h' file not found\n   19 | #include \"ml_dtypes/include/float8.h\"  // from @ml_dtypes_py\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n1 error generated.\n```\n\n### Standalone code to reproduce the issue\n\n```shell\ngit clone https://github.com/tensorflow/tensorflow.git; cd tensorflow && `clang++ -I../tensorflow/ -I../tensorflow/third_party/xla/ -I../tensorflow/third_party/xla/third_party/tsl/ -I../tensorflow/third_party/xla/third_party/py/ -I/data/data/com.termux/files/home/../usr/include/eigen3 ../tensorflow/tensorflow/core/framework/tensor.h\nfatal: destination path 'tensorflow' already exists and is not an empty directory.\nclang++: warning: treating 'c-header' input as 'c++-header' when in C++ mode, this behavior is deprecated [-Wdeprecated]\nIn file included from ../tensorflow/tensorflow/core/framework/tensor.h:27:\nIn file included from ../tensorflow/tensorflow/core/framework/allocator.h:26:\nIn file included from ../tensorflow/third_party/xla/xla/tsl/framework/allocator.h:26:\nIn file included from ../tensorflow/third_party/xla/xla/tsl/framework/numeric_types.h:22:\nIn file included from ../tensorflow/third_party/xla/xla/tsl/platform/types.h:22:\n../tensorflow/third_party/xla/third_party/tsl/tsl/platform/ml_dtypes.h:19:10: fatal error: 'ml_dtypes/include/float8.h' file not found\n   19 | #include \"ml_dtypes/include/float8.h\"  // from @ml_dtypes_py\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n1 error generated.\n```\n\n### Relevant log output\n**Local (_Termux_)**:\n```shell\n~ $ git clone https://github.com/SwuduSusuwu/SusuLib.git\nfatal: destination path 'SusuLib' already exists and is not an empty directory.\n~ $ cd SusuLib\n~/SusuLib $ git switch preview\nD       cxx/ClassTensorFlowCns.hxx\nAlready on 'preview'\nYour branch is up to date with 'origin/preview'.\n~/SusuLib $ ./build.sh\n~/SusuLib $ ./build.sh\n[Notice: (C) 2024 Swudu Susuwu, dual licenses: choose [GPLv2](./LICENSE_GPLv2) or [Apache 2](./LICENSE), allows all uses.]\n[Warning: `git branch` is \"experimental\" (which is unstable & sets `-DSUSUWU_EXPERIMENTAL`); for production use, execute `git switch trunk`.]\n[Warning: SUSUWU_DEPENDENCY_INCLUDE(): Package `libtensorflow` not found. To install, use `sudo apt install libtensorflow`]\n[Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `tensorflow`, FLAGS_USER will use `-F../tensorflow/`.]\n[Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `xla`, FLAGS_USER will use `-F../tensorflow/third_party/xla/`.]\n[Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `tsl`, FLAGS_USER will use `-F../tensorflow/third_party/xla/third_party/tsl/`.]\n[Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `ml_dtypes`, FLAGS_USER will use `-F../tensorflow/third_party/xla/third_party/py/`.]\n[Notice: SUSUWU_DEPENDENCY_INCLUDE(): Found package `eigen`, FLAGS_USER will use `-F/data/data/com.termux/files/home/../usr/include/eigen3`.]\n[Warning: SUSUWU_PRODUCTION_USE(): `git branch` is \"preview\"; for production use, use `git switch trunk`.]\n[Notice: SUSUWU_PROCESS_RELEASE_DEBUG(): `./build.sh ` defaults to `./build.sh  --debug`.]\n[Notice: SUSUWU_PROCESS_RELEASE_DEBUG(): `./build.sh  --debug` is slow (use `./build.sh  --release` to improve how fast `./${BINDIR}/${OUTPUT}` executes).]\n[Notice: SUSUWU_SETUP_OBJDIR(): To redirect `clang++ -c ... -o \"${OBJDIR}${OBJ}.o\"` (which has `OBJDIR=\"./obj/\"`), use `OBJDIR=\"<new-path>\"` (where \"<new-path>\" is a directory which you choose).]\n[Notice: SUSUWU_SETUP_BINDIR(): To redirect `clang++ ... -o \"${BINDIR}${OUTPUT}\"` (which has `BINDIR=\"./bin/\"`), use `BINDIR=\"<new-path>\"` (where \"<new-path>\" is a directory which you choose).]\nIn file included from ./cxx/ClassTensorFlowCns.hxx:16:\nIn file included from ../tensorflow/tensorflow/core/framework/tensor.h:27:\nIn file included from ../tensorflow/tensorflow/core/framework/allocator.h:26:\nIn file included from ../tensorflow/third_party/xla/xla/tsl/framework/allocator.h:26:\nIn file included from ../tensorflow/third_party/xla/xla/tsl/framework/numeric_types.h:22:\nIn file included from ../tensorflow/third_party/xla/xla/tsl/platform/types.h:22:\n../tensorflow/third_party/xla/third_party/tsl/tsl/platform/ml_dtypes.h:19:10: fatal error: 'ml_dtypes/include/float8.h' file not found\n   19 | #include \"ml_dtypes/include/float8.h\"  // from @ml_dtypes_py\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n1 error generated.\n[Notice: `clang++  -g -Og -fno-omit-frame-pointer -std=c++11 -fsanitize=address -fno-sanitize-recover=all -fsanitize=float-divide-by-zero -fsanitize=float-cast-overflow -fno-sanitize=null -fno-sanitize=alignment -DSUSUWU_EXPERIMENTAL -DSUSUWU_DEFAULT_BRANCH=\"trunk\" -I../tensorflow/ -I../tensorflow/third_party/xla/ -I../tensorflow/third_party/xla/third_party/tsl/ -I../tensorflow/third_party/xla/third_party/py/ -I/data/data/com.termux/files/home/../usr/include/eigen3 -Wall -Wno-unused-function -Wno-unused-function -Wextra -Wno-unused-parameter -Wno-ignored-qualifiers -Wpedantic ./cxx/ClassTensorFlowCns.hxx` failed, will not enable `CXXFLAGS=\"${CXXFLAGS} -std=c++17 -DUSE_TENSORFLOW_CNS\"` (skipped). If `libtensorflow` is installed, insert `-std=c++17 -DUSE_TENSORFLOW_CNS` into `./build.sh:FLAGS_USER`. To troubleshoot, use `cd ../tensorflow/ && ./configure`]\n[Notice: SUSUWU_BUILD_CTAGS(): Was called with less than 2 params; will default to `SUSUWU_BUILD_CTAGS -R --exclude=.git/ --exclude=*.html --exclude=compile_commands.json .`.]\n[Success: SUSUWU_BUILD_EXECUTABLE(): Reused \"./bin/Susuwu.out\" (5964408 bytes).]\n```\n\n",
    "comments": [
      {
        "user": "SwuduSusuwu",
        "body": "### Notice\n- have set \"Custom code\" to \"yes\" since this was found from custom code (and since `Current behavior?` is with custom code),\n  - But `Standalone code to reproduce the issue` reproduces this with just the stock <https://github.com/tensorflow/tensorflow> code from [`./tensorflow/core/framework/tensor.h`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/framework/tensor.h)\n  - There was no \"both\" option, so did not choose \"no\".\n- `./configure` has a question if you want to compile for _Android_, but <https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/android> says that _Android_ support is deprecated unless you switch to _TensorFlow lite_, and the project is supposed to support standard _TensorFlow_\n  -  If this can build + execute on smartphones (such as to use _Google Tensor_'s 200 **teraFLOP**s to compute), that is cool, but more important is to have full _TensorFlow_ use on desktops, laptops and servers.\n  - The test is just to ensure that the code will compile (into static libs); there is no linker test for the _TensorFlow_ code, since all of the development is on this smartphone.\n- the ` [clang-diagnostic-error]` suffix is specific to `clang-tidy`, but the rest of the error message shows with `clang++`."
      },
      {
        "user": "SwuduSusuwu",
        "body": "Is not similar to https://github.com/tensorflow/tensorflow/issues/62866 (where the workaround was to insert new include path), since in that issue the include path was found, but now is not found:\n```\n~/tensorflow $ find ./ | grep \"ml_dtypes/include\"\n~/tensorflow $\n```"
      },
      {
        "user": "SwuduSusuwu",
        "body": "_Assistant_ says that for platforms without `libtensorflow`, that as opposed to build from source, you can use:\n```\n#!/bin/bash\n\n# Install TensorFlow C++ headers and library\napt-get update && apt-get install -y \\\n    libsm6 \\\n    libxext6 \\\n    libxrender-dev \\\n    libgl1-mesa-glx \\\n    libgl1-mesa-dev \\\n    libgtk2.0-dev \\\n    libavcodec-dev \\\n    libavformat-dev \\\n    libswscale-dev \\\n    libv4l-dev \\\n    libjpeg-dev \\\n    libpng-dev \\\n    libatlas-base-dev \\\n    gfortran \\\n    wget \\\n    unzip \\\n    git \\\n    cmake \\\n    libssl-dev \\\n    libcurl4-openssl-dev\n\n# Download and extract TensorFlow C++ library\nwget https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.11.0.tar.gz\ntar xzf libtensorflow-cpu-linux-x86_64-2.11.0.tar.gz\nmv lib/* /usr/lib/\nmv include/* /usr/include/\n```\nBut [\"https://storage.googleapis.com/tensorflow/libtensorflow/\"](https://duckduckgo.com/?q=%22https%3A%2F%2Fstorage.googleapis.com%2Ftensorflow%2Flibtensorflow%2F%22&t=fpas&ia=web) has no search results."
      }
    ]
  },
  {
    "issue_number": 63849,
    "title": "Adding TensorFlow Hub KerasLayer to Sequential Model Raises ValueError",
    "author": "ruddyscent",
    "state": "open",
    "created_at": "2024-03-17T11:08:55Z",
    "updated_at": "2025-06-08T10:07:53Z",
    "labels": [
      "type:bug",
      "comp:keras",
      "awaiting PR merge",
      "TF 2.16"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16.1\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 22.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython 3.11.0rc1\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n12.3.0\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI can execute the following code without any issues in TensorFlow 2.15.0 and TensorFlow Hub 1.16.1. However, when I upgrade the TensorFlow version to 2.16.0 or above, I encounter an error stating that `KerasLayer` cannot be added to the Sequential model.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\nimage_size = 224\r\nURL = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\r\n\r\nmodel = tf.keras.Sequential([\r\n        hub.KerasLayer(URL, input_shape=(image_size, image_size, 3))\r\n])\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[29], line 1\r\n----> 1 model = tf.keras.Sequential([\r\n      2         feature_extractor,\r\n      3         tf.keras.layers.Dense(2, activation = 'softmax')\r\n      4 ])\r\n      6 model.build([None, image_size, image_size, 3])\r\n      7 model.summary()\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py:70, in Sequential.__init__(self, layers, trainable, name)\r\n     68 if layers:\r\n     69     for layer in layers:\r\n---> 70         self.add(layer, rebuild=False)\r\n     71     self._maybe_rebuild()\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py:92, in Sequential.add(self, layer, rebuild)\r\n     90         layer = origin_layer\r\n     91 if not isinstance(layer, Layer):\r\n---> 92     raise ValueError(\r\n     93         \"Only instances of `keras.Layer` can be \"\r\n     94         f\"added to a Sequential model. Received: {layer} \"\r\n     95         f\"(of type {type(layer)})\"\r\n     96     )\r\n     97 if not self._is_layer_name_unique(layer):\r\n...\r\n    101         \"the name of a layer in this model. Update the `name` argument \"\r\n    102         \"to pass a unique name.\"\r\n    103     )\r\n\r\nValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x7a4ac7e30f40> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)\n```\n",
    "comments": [
      {
        "user": "Aloqeely",
        "body": "After investigating the code, I found a potential cause of the issue in `tensorflow_hub/keras_layer.py` lines 26-31:\r\n```python\r\n# Use Keras 2.\r\nversion_fn = getattr(tf.keras, \"version\", None)\r\nif version_fn and version_fn().startswith(\"3.\"):\r\n  import tf_keras as keras\r\nelse:\r\n  keras = tf.keras\r\n```\r\nDepending on the Keras version, the module might either import `tf_keras` or directly use `tf.keras`, the former causes the `isinstance(layer, Layer)` check in `Sequential.add` to return `False` for `hub.KerasLayer`, even though it inherits from `keras.layers.Layer`"
      },
      {
        "user": "SuryanarayanaY",
        "body": "Hi @ruddyscent ,\r\n\r\nTFHub has dependency on `tf_keras` package (i.e Keras2) as per the setup.py of TFHub.\r\n\r\nhttps://github.com/tensorflow/hub/blob/ff72e25fef44bd67d5c14fb5328aa44e303e3404/tensorflow_hub/pip_package/setup.py#L31\r\n\r\nSince TF2.16 comes with Keras3 the problem arises. As a workaround, you can install tf_keras package and set environment variable TF_USE_LEGACY_KERAS=1 to ensure Keras2 will be used with tf.keras. "
      },
      {
        "user": "SuryanarayanaY",
        "body": "> After investigating the code, I found a potential cause of the issue in `tensorflow_hub/keras_layer.py` lines 26-31:\r\n> \r\n> ```python\r\n> # Use Keras 2.\r\n> version_fn = getattr(tf.keras, \"version\", None)\r\n> if version_fn and version_fn().startswith(\"3.\"):\r\n>   import tf_keras as keras\r\n> else:\r\n>   keras = tf.keras\r\n> ```\r\n> \r\n> Depending on the Keras version, the module might either import `tf_keras` or directly use `tf.keras`, the latter causes the `isinstance(layer, Layer)` check in `Sequential.add` to return `False` for `hub.KerasLayer`, even though it inherits from `keras.layers.Layer`\r\n\r\nHi, When I tried with TF2.16v on Colab environment the error stack seems generated from the _ensure_keras_2_importable() function from `tensorflow_hub/__init__.py` as per attached [gist](https://colab.sandbox.google.com/gist/SuryanarayanaY/5585bf7974198ca45bacdb9e84db91d4/63849_tf2-16.ipynb)."
      }
    ]
  },
  {
    "issue_number": 93692,
    "title": "Segmentation fault in tf.ragged.segment_ids_to_row_splits",
    "author": "cx104906",
    "state": "closed",
    "created_at": "2025-05-20T03:13:10Z",
    "updated_at": "2025-06-07T02:12:42Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 18.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n6.5.0\n\n### GCC/compiler version\n\nclang 18.1.8\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSegfault when using tf.ragged.segment_ids_to_row_splits\n\n### Standalone code to reproduce the issue\n\n```shell\nimport pickle\nimport tensorflow as tf\nimport time\nimport pprint\nimport numpy as np\n\nprint(tf.__version__)\n\narray1 = xxx\n# array1 with huge shape. shape=(9, 6, 7, 6, 4, 9). There are too many lines to list completely\n# so refer to colab : https://colab.research.google.com/drive/10_5keIFUKzP494zIUHAkKlZhuEg25cAJ?usp=sharing\n# refer to https://github.com/cx104906/poc/blob/main/tensorflow/py1.py\nts1 = tf.constant(array1,dtype=tf.uint64) \nmylist = [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[\"'gaussian'\"]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]],\n [[None],\n  [ts1,\n   7.253174223754603e+200,\n   True,\n   [[None],\n    '#W${Q|k31~8',\n    2.567431912835933e-289,\n    (1734665218, 282165145, 1124802197416811, None, True)],\n   (2370920871,),\n   True]]]\nmydict = {}\n\nfor i in range(10):\n  try:\n      print (f\"{i}\",flush=True)\n      tf.ragged.segment_ids_to_row_splits(*mylist,**mydict)\n  except Exception as e:\n      print(f\"{e}\")\n\nprint(\"done\")\n```\n\n### Relevant log output\n\n```shell\n2.19.0\n\ntest\n\n0\nSegmentation fault (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @cx104906 ,\nApologies for the delayed response, and thank you for raising your concern.\nThe issue appears to be caused by the use of deeply nested tuples in the input. Is there a specific reason for structuring the input this way?\nI tested a simplified version of your code with reduced nesting, and it ran successfully without errors. I am attaching [gist](https://colab.sandbox.google.com/gist/Venkat6871/7bd883fc94c2251a00535cdf86911c04/93692_tf_2-19-0-nightly-v.ipynb) here for you reference. Please let me know if this helps, or if you are still encountering issues.\n\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 92660,
    "title": "Problem when using tf-to-tosa-pipeline",
    "author": "GiuseppeSorrentino99",
    "state": "closed",
    "created_at": "2025-05-03T13:43:50Z",
    "updated_at": "2025-06-07T02:12:40Z",
    "labels": [
      "stat:awaiting response",
      "stale",
      "comp:lite",
      "TFLiteConverter",
      "TF 2.13"
    ],
    "body": "OS: Ubuntu 20-04\nTensorflow: 2.13\n\n0.\tProgram arguments: tf-opt --tf-executor-to-functional-conversion --tf-region-control-flow-to-functional --tf-shape-inference --tf-to-tosa-pipeline output/tf.mlir -o output/tosa.mlir\n1.\tProgram arguments: tf-opt --tf-executor-to-functional-conversion --tf-region-control-flow-to-functional --tf-shape-inference --tf-to-tosa-pipeline output/tf.mlir -o output/tosa.mlir\n #0 0x00007ff557ec5a2c llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/opt/tensorflow/bin/libtensorflow_framework.so.2+0x1096a2c)\n #1 0x00007ff557ec3325 llvm::sys::RunSignalHandlers() (/opt/tensorflow/bin/libtensorflow_framework.so.2+0x1094325)\n #2 0x00007ff557ec4215 SignalHandler(int) Signals.cpp:0:0\n #3 0x00007ff556cb7420 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x14420)\n #4 0x0000558a898ce60b mlir::ElementsAttr::isSplat() const (/opt/tensorflow/bin/tf-opt+0xf00360b)\n #5 0x0000558a84b971ee mlir::tosa::SliceOp::fold(llvm::ArrayRef<mlir::Attribute>) (/opt/tensorflow/bin/tf-opt+0xa2cc1ee)\n #6 0x0000558a84b16708 mlir::LogicalResult llvm::detail::UniqueFunctionBase<mlir::LogicalResult, mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&>::CallImpl<mlir::Op<mlir::tosa::SliceOp, mlir::OpTrait::ZeroRegions, mlir::OpTrait::OneResult, mlir::OpTrait::OneTypedResult<mlir::Type>::Impl, mlir::OpTrait::ZeroSuccessors, mlir::OpTrait::OneOperand, mlir::OpTrait::OpInvariants, mlir::InferShapedTypeOpInterface::Trait, mlir::ConditionallySpeculatable::Trait, mlir::OpTrait::AlwaysSpeculatableImplTrait, mlir::MemoryEffectOpInterface::Trait, mlir::tosa::TosaOp::Trait>::getFoldHookFn()::'lambda'(mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) const>(void*, mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) (/opt/tensorflow/bin/tf-opt+0xa24b708)\n #7 0x0000558a8983c499 mlir::Operation::fold(llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) (/opt/tensorflow/bin/tf-opt+0xef71499)\n #8 0x0000558a895aaf43 mlir::OperationFolder::tryToFold(mlir::OpBuilder&, mlir::Operation*, llvm::SmallVectorImpl<mlir::Value>&, llvm::function_ref<void (mlir::Operation*)>) (/opt/tensorflow/bin/tf-opt+0xecdff43)\n #9 0x0000558a895ab7bf mlir::OperationFolder::tryToFold(mlir::Operation*, llvm::function_ref<void (mlir::Operation*)>, llvm::function_ref<void (mlir::Operation*)>, bool*) (/opt/tensorflow/bin/tf-opt+0xece07bf)\n#10 0x0000558a8959fb24 mlir::applyPatternsAndFoldGreedily(llvm::MutableArrayRef<mlir::Region>, mlir::FrozenRewritePatternSet const&, mlir::GreedyRewriteConfig) (/opt/tensorflow/bin/tf-opt+0xecd4b24)\n#11 0x0000558a83edeb3b mlir::tosa::ApplyPatternsWithShapeResolution(mlir::func::FuncOp, mlir::FrozenRewritePatternSet const&) (/opt/tensorflow/bin/tf-opt+0x9613b3b)\n#12 0x0000558a83e29cb1 mlir::tosa::(anonymous namespace)::LegalizeTF::runOnOperation() legalize_tf.cc:0:0\n#13 0x0000558a897b61ca mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/opt/tensorflow/bin/tf-opt+0xeeeb1ca)\n#14 0x0000558a897b6652 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/opt/tensorflow/bin/tf-opt+0xeeeb652)\n#15 0x0000558a897b54c5 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/opt/tensorflow/bin/tf-opt+0xeeea4c5)\n#16 0x0000558a897b5dd9 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/opt/tensorflow/bin/tf-opt+0xeeeadd9)\n#17 0x0000558a897b6652 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/opt/tensorflow/bin/tf-opt+0xeeeb652)\n#18 0x0000558a897b74a5 mlir::PassManager::run(mlir::Operation*) (/opt/tensorflow/bin/tf-opt+0xeeec4a5)\n#19 0x0000558a855d6fbb performActions(llvm::raw_ostream&, bool, bool, std::shared_ptr<llvm::SourceMgr> const&, mlir::MLIRContext*, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, bool, bool) (.constprop.0) MlirOptMain.cpp:0:0\n#20 0x0000558a855d76aa processBuffer(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, bool, bool, bool, bool, bool, bool, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, llvm::ThreadPool*) MlirOptMain.cpp:0:0\n#21 0x0000558a855d78da mlir::LogicalResult llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>::callback_fn<mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, bool, bool, bool, bool, bool, bool, bool)::'lambda'(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>(long, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&) MlirOptMain.cpp:0:0\n#22 0x0000558a898ddd23 mlir::splitAndProcessBuffer(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>, llvm::raw_ostream&, bool, bool) (/opt/tensorflow/bin/tf-opt+0xf012d23)\n#23 0x0000558a855d66d3 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, bool, bool, bool, bool, bool, bool, bool) (/opt/tensorflow/bin/tf-opt+0xad0b6d3)\n#24 0x0000558a855d679b mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&, bool, bool, bool, bool, bool, bool, bool, bool) (/opt/tensorflow/bin/tf-opt+0xad0b79b)\n#25 0x0000558a855d7dfe mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&, bool) (/opt/tensorflow/bin/tf-opt+0xad0cdfe)\n#26 0x0000558a7bd43984 main (/opt/tensorflow/bin/tf-opt+0x1478984)\n#27 0x00007ff5568d6083 __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24083)\n#28 0x0000558a7be724fe _start (/opt/tensorflow/bin/tf-opt+0x15a74fe)\n #0 0x00007ff557ec5a2c llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/opt/tensorflow/bin/libtensorflow_framework.so.2+0x1096a2c)\n #1 0x00007ff557ec3325 llvm::sys::RunSignalHandlers() (/opt/tensorflow/bin/libtensorflow_framework.so.2+0x1094325)\n #2 0x00007ff557ec4215 SignalHandler(int) Signals.cpp:0:0\n #3 0x00007ff556cb7420 __restore_rt (/lib/x86_64-linux-gnu/libpthread.so.0+0x14420)\n #4 0x0000558a898ce60b mlir::ElementsAttr::isSplat() const (/opt/tensorflow/bin/tf-opt+0xf00360b)\n #5 0x0000558a84b971ee mlir::tosa::SliceOp::fold(llvm::ArrayRef<mlir::Attribute>) (/opt/tensorflow/bin/tf-opt+0xa2cc1ee)\n #6 0x0000558a84b16708 mlir::LogicalResult llvm::detail::UniqueFunctionBase<mlir::LogicalResult, mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&>::CallImpl<mlir::Op<mlir::tosa::SliceOp, mlir::OpTrait::ZeroRegions, mlir::OpTrait::OneResult, mlir::OpTrait::OneTypedResult<mlir::Type>::Impl, mlir::OpTrait::ZeroSuccessors, mlir::OpTrait::OneOperand, mlir::OpTrait::OpInvariants, mlir::InferShapedTypeOpInterface::Trait, mlir::ConditionallySpeculatable::Trait, mlir::OpTrait::AlwaysSpeculatableImplTrait, mlir::MemoryEffectOpInterface::Trait, mlir::tosa::TosaOp::Trait>::getFoldHookFn()::'lambda'(mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) const>(void*, mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) (/opt/tensorflow/bin/tf-opt+0xa24b708)\n #7 0x0000558a8983c499 mlir::Operation::fold(llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) (/opt/tensorflow/bin/tf-opt+0xef71499)\n #8 0x0000558a895aaf43 mlir::OperationFolder::tryToFold(mlir::OpBuilder&, mlir::Operation*, llvm::SmallVectorImpl<mlir::Value>&, llvm::function_ref<void (mlir::Operation*)>) (/opt/tensorflow/bin/tf-opt+0xecdff43)\n #9 0x0000558a895ab7bf mlir::OperationFolder::tryToFold(mlir::Operation*, llvm::function_ref<void (mlir::Operation*)>, llvm::function_ref<void (mlir::Operation*)>, bool*) (/opt/tensorflow/bin/tf-opt+0xece07bf)\n#10 0x0000558a8959fb24 mlir::applyPatternsAndFoldGreedily(llvm::MutableArrayRef<mlir::Region>, mlir::FrozenRewritePatternSet const&, mlir::GreedyRewriteConfig) (/opt/tensorflow/bin/tf-opt+0xecd4b24)\n#11 0x0000558a83edeb3b mlir::tosa::ApplyPatternsWithShapeResolution(mlir::func::FuncOp, mlir::FrozenRewritePatternSet const&) (/opt/tensorflow/bin/tf-opt+0x9613b3b)\n#12 0x0000558a83e29cb1 mlir::tosa::(anonymous namespace)::LegalizeTF::runOnOperation() legalize_tf.cc:0:0\n#13 0x0000558a897b61ca mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/opt/tensorflow/bin/tf-opt+0xeeeb1ca)\n#14 0x0000558a897b6652 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/opt/tensorflow/bin/tf-opt+0xeeeb652)\n#15 0x0000558a897b54c5 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/opt/tensorflow/bin/tf-opt+0xeeea4c5)\n#16 0x0000558a897b5dd9 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/opt/tensorflow/bin/tf-opt+0xeeeadd9)\n#17 0x0000558a897b6652 mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/opt/tensorflow/bin/tf-opt+0xeeeb652)\n#18 0x0000558a897b74a5 mlir::PassManager::run(mlir::Operation*) (/opt/tensorflow/bin/tf-opt+0xeeec4a5)\n#19 0x0000558a855d6fbb performActions(llvm::raw_ostream&, bool, bool, std::shared_ptr<llvm::SourceMgr> const&, mlir::MLIRContext*, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, bool, bool) (.constprop.0) MlirOptMain.cpp:0:0\n#20 0x0000558a855d76aa processBuffer(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, bool, bool, bool, bool, bool, bool, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, llvm::ThreadPool*) MlirOptMain.cpp:0:0\n#21 0x0000558a855d78da mlir::LogicalResult llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>::callback_fn<mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, bool, bool, bool, bool, bool, bool, bool)::'lambda'(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>(long, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&) MlirOptMain.cpp:0:0\n#22 0x0000558a898ddd23 mlir::splitAndProcessBuffer(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>, llvm::raw_ostream&, bool, bool) (/opt/tensorflow/bin/tf-opt+0xf012d23)\n#23 0x0000558a855d66d3 mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (mlir::PassManager&)>, mlir::DialectRegistry&, bool, bool, bool, bool, bool, bool, bool) (/opt/tensorflow/bin/tf-opt+0xad0b6d3)\n#24 0x0000558a855d679b mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::PassPipelineCLParser const&, mlir::DialectRegistry&, bool, bool, bool, bool, bool, bool, bool, bool) (/opt/tensorflow/bin/tf-opt+0xad0b79b)\n#25 0x0000558a855d7dfe mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&, bool) (/opt/tensorflow/bin/tf-opt+0xad0cdfe)\n#26 0x0000558a7bd43984 main (/opt/tensorflow/bin/tf-opt+0x1478984)\n#27 0x00007ff5568d6083 __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24083)\n#28 0x0000558a7be724fe _start (/opt/tensorflow/bin/tf-opt+0x15a74fe)",
    "comments": [
      {
        "user": "Muwinuddin",
        "body": "Hi @GiuseppeSorrentino99 and @Venkat6871,\n\nI would like to contribute to resolving this issue. Here’s how I plan to approach it:\n\n1. Analyze the provided stack trace to identify the root cause of the error during the `tf-to-tosa-pipeline` operation, particularly in the `LegalizeTF` pass.\n2. Investigate the `output/tf.mlir` file (if accessible) to check for specific TensorFlow operations causing the problem.\n3. Explore and review the relevant code in the `legalize_tf.cc` file and related MLIR passes to identify potential bugs or improvements.\n4. Test the pipeline on a similar setup (Ubuntu 20.04, TensorFlow 2.13) to reproduce the issue and validate potential fixes.\n\nIf you have any additional insights, pointers, or specific areas to focus on, please let me know. I'd be happy to collaborate on this!\n\nLooking forward to your response.❤️\n\nThanks!"
      },
      {
        "user": "GiuseppeSorrentino99",
        "body": "Hi @Muwinuddin \n\nThanks for your help. I can leave here the code for you to replicate everything. (The model is also opensource in Voxelmorph. I just took it to solve other \"intermediate\" issues, but you can find the opensource model as well [Voxelmorh](https://github.com/voxelmorph/voxelmorph)\n\nHowever, here is the code: \n\n```\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport tensorflow_addons as tfa\nimport os\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n\nclass SpatialTransformer(layers.Layer):\n    \"\"\"3D Spatial Transformer using batched 2D warps and static shape enforcement.\"\"\"\n    def call(self, inputs):\n        vol, flow = inputs  # vol: [B,D,H,W,C], flow: [B,D,H,W,3]\n\n        # 1. Enforce static (non-zero) shapes to satisfy TOSA requirements\n        #    (TOSA dialect expects all dims ≥ 1 and statically known) \n        vol  = tf.ensure_shape(vol,  [None, vol.shape[1], vol.shape[2], vol.shape[3], vol.shape[4]])\n        flow = tf.ensure_shape(flow, [None, flow.shape[1], flow.shape[2], flow.shape[3], 3])          \n\n        # 2. Flatten depth dimension into batch: [B,D,H,W,C] → [B*D,H,W,C]\n        shape = tf.shape(vol)\n        B, D, H, W, C = shape[0], shape[1], shape[2], shape[3], vol.shape[4]\n        vol_flat  = tf.reshape(vol,  tf.stack([B * D, H, W, C]))                                    \n        flow_flat = tf.reshape(flow, tf.stack([B * D, H, W, 3]))                                     \n\n        # 3. Perform a single batched 2D warp via dense_image_warp,\n        #    avoiding tf.map_fn loops entirely :contentReference[oaicite:7]{index=7}\n        moved_flat = tfa.image.dense_image_warp(vol_flat, flow_flat[..., :2])\n\n        # 4. Restore original shape: [B*D,H,W,C] → [B,D,H,W,C]\n        moved = tf.reshape(moved_flat, tf.stack([B, D, H, W, C]))                                     \n        return moved\n\ndef conv_block(x, filters, convs=2, kernel_size=3, activation='relu'):\n    for _ in range(convs):\n        x = layers.Conv3D(filters, kernel_size, padding='same',\n                          kernel_initializer='he_normal')(x)\n        x = layers.Activation(activation)(x)\n    return x\n\ndef build_minimal_voxelmorph(inshape,\n                             enc_features=(16, 32, 32, 32),\n                             dec_features=(32, 32, 32, 32, 32, 16, 16)):\n    moving = layers.Input(shape=(*inshape, 1), name='moving')\n    fixed  = layers.Input(shape=(*inshape, 1), name='fixed')\n    x = layers.Concatenate(axis=-1)([moving, fixed])\n\n    skips = []\n    for f in enc_features:\n        x = conv_block(x, f)\n        skips.append(x)\n        x = layers.MaxPool3D(2)(x)\n\n    x = conv_block(x, enc_features[-1] * 2)\n\n    for f, skip in zip(dec_features, reversed(skips)):\n        x = layers.UpSampling3D(2)(x)\n        x = layers.Concatenate(axis=-1)([x, skip])\n        x = conv_block(x, f)\n\n    flow  = layers.Conv3D(3, 3, padding='same', name='flow')(x)\n    moved = SpatialTransformer(name='moved')([moving, flow])\n\n    return models.Model(inputs=[moving, fixed],\n                        outputs=[moved, flow],\n                        name='VoxelmorphMinimalFlatten')\n\n# Instantiate model for a 128³ volume                        \nmodel = build_minimal_voxelmorph((128, 128, 128))\nmodel.summary()\n\nsave_path = os.path.join(os.getcwd(), \"model/simple/\")\ntf.saved_model.save(model, save_path) \n\n@tf.function\ndef infer(moving, fixed):\n    return model([moving, fixed])\n\ninp0, inp1 = model.inputs\n\nconcrete_func = infer.get_concrete_function(\n    moving=tf.TensorSpec(shape=inp0.shape, dtype=inp0.dtype, name=inp0.name.split(':')[0]),\n    fixed =tf.TensorSpec(shape=inp1.shape, dtype=inp1.dtype, name=inp1.name.split(':')[0])\n)\n\nfrozen_func = convert_variables_to_constants_v2(concrete_func)\ntf.io.write_graph(\n    graph_or_graph_def=frozen_func.graph,\n    logdir=os.getcwd(),\n    name=\"output/frozen_graph.pbtxt\",\n    as_text=True\n)\n\nprint(\"Frozen graph:\")\nwith tf.io.gfile.GFile(\"output/frozen_graph.pbtxt\", \"r\") as f:\n    frozen_graph = f.read()\n    print(frozen_graph)\n```\n\nThen, running in docker the following commands: \n\n```\ndocker run -u $(id -u):$(id -g) -v $(pwd):/working_dir --rm agostini01/soda \\\n  tf-mlir-translate \\\n    --graphdef-to-mlir \\\n    --tf-input-arrays=fixed,moving \\\n    --tf-input-data-types=DT_FLOAT,DT_FLOAT \\\n    --tf-input-shapes=128,128,128,1:128,128,128,1 \\\n    --tf-output-arrays=Identity,Identity_1 \\\n    output/frozen_graph.pbtxt \\\n    -o output/tf.mlir\n\ndocker run -u $(id -u):$(id -g) -v $(pwd):/working_dir --rm agostini01/soda \\\ntf-opt \\\n  --tf-executor-to-functional-conversion \\\n  --tf-region-control-flow-to-functional \\\n  --tf-shape-inference \\\n  --tf-to-tosa-pipeline \\\n  output/tf.mlir \\\n  -o output/tosa.mlir\n\n```\n\nThis Last command is what fails for me, with the error above. \nIf you need to replicate the docker setup, it is available here: [DockerTutorial](https://github.com/pnnl/soda-opt/tree/main/docs/tutorials/tensorflow)\n\nThanks to both of you @Venkat6871 @Muwinuddin for any help. \n\nRegards\nGiuseppe\n\n\n"
      },
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @GiuseppeSorrentino99 and Hi, @Muwinuddin, Thank you for showing your interest to resolve this issue \nI apologize for the delay in my response, I think I'm able to reproduce this issue from my end for reference I'm adding output log for investigation purpose and we will have to investigate this issue further from our end and will update you, Thank you for bringing this issue to our attention\n\n**Here is output log for reference :**\n\n```\n(tflite-issue) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22:~/TFlite-Issue-#92660/tflite_tosa_issue$ docker run -u $(id -u):$(id -g) -v $(pwd):/working_dir -w /working_dir --rm agostini01/soda \\\ntf-opt \\\n  --tf-executor-to-functional-conversion \\\n  --tf-region-control-flow-to-functional \\\n  --tf-shape-inference \\\n  --tf-to-tosa-pipeline \\\n  output/tf.mlir \\\n  -o output/tosa.mlir\n2025-05-22 14:31:25.767400: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nTensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.\nStack dump:\n0.      Program arguments: tf-opt --tf-executor-to-functional-conversion --tf-region-control-flow-to-functional --tf-shape-inference --tf-to-tosa-pipeline output/tf.mlir -o output/tosa.mlir\n #0 0x0000751fa537b1ae llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) (/opt/tensorflow/bin/libtensorflow_framework.so.2+0x1c371ae)\n #1 0x0000751fa537892d llvm::sys::RunSignalHandlers() (/opt/tensorflow/bin/libtensorflow_framework.so.2+0x1c3492d)\n #2 0x0000751fa5378d34 SignalHandler(int) Signals.cpp:0:0\n #3 0x0000751fa3279050 (/lib/x86_64-linux-gnu/libc.so.6+0x3c050)\n #4 0x0000615b74567077 mlir::ElementsAttr::isSplat() const (/opt/tensorflow/bin/tf-opt+0xfbaa077)\n #5 0x0000615b71e6745f mlir::tosa::SliceOp::fold(mlir::tosa::SliceOpGenericAdaptor<llvm::ArrayRef<mlir::Attribute>>) (/opt/tensorflow/bin/tf-opt+0xd4aa45f)\n #6 0x0000615b71e1a0f8 mlir::LogicalResult llvm::detail::UniqueFunctionBase<mlir::LogicalResult, mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&>::CallImpl<mlir::Op<mlir::tosa::SliceOp, mlir::OpTrait::ZeroRegions, mlir::OpTrait::OneResult, mlir::OpTrait::OneTypedResult<mlir::TensorType>::Impl, mlir::OpTrait::ZeroSuccessors, mlir::OpTrait::OneOperand, mlir::OpTrait::OpInvariants, mlir::BytecodeOpInterface::Trait, mlir::InferShapedTypeOpInterface::Trait, mlir::OpTrait::InferShapedTypeOpAdaptor, mlir::ConditionallySpeculatable::Trait, mlir::OpTrait::AlwaysSpeculatableImplTrait, mlir::MemoryEffectOpInterface::Trait, mlir::tosa::TosaOp::Trait>::getFoldHookFn()::'lambda'(mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) const>(void*, mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) (/opt/tensorflow/bin/tf-opt+0xd45d0f8)\n #7 0x0000615b71e4cb73 mlir::RegisteredOperationName::Model<mlir::tosa::SliceOp>::foldHook(mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) (/opt/tensorflow/bin/tf-opt+0xd48fb73)\n #8 0x0000615b744cbe23 mlir::Operation::fold(llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) (/opt/tensorflow/bin/tf-opt+0xfb0ee23)\n #9 0x0000615b744cc215 mlir::Operation::fold(llvm::SmallVectorImpl<mlir::OpFoldResult>&) (/opt/tensorflow/bin/tf-opt+0xfb0f215)\n#10 0x0000615b73ac16fd (anonymous namespace)::GreedyPatternRewriteDriver::processWorklist() GreedyPatternRewriteDriver.cpp:0:0\n#11 0x0000615b73ac2590 mlir::applyPatternsAndFoldGreedily(mlir::Region&, mlir::FrozenRewritePatternSet const&, mlir::GreedyRewriteConfig, bool*) (/opt/tensorflow/bin/tf-opt+0xf105590)\n#12 0x0000615b7024835c mlir::tosa::ApplyPatternsWithShapeResolution(mlir::func::FuncOp, mlir::FrozenRewritePatternSet const&) (/opt/tensorflow/bin/tf-opt+0xb88b35c)\n#13 0x0000615b701808bc mlir::tosa::(anonymous namespace)::LegalizeTF::runOnOperation() legalize_tf.cc:0:0\n#14 0x0000615b7445db90 mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/opt/tensorflow/bin/tf-opt+0xfaa0b90)\n#15 0x0000615b7445dd9c mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/opt/tensorflow/bin/tf-opt+0xfaa0d9c)\n#16 0x0000615b7445e32a mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::'lambda'(mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo&)::operator()(mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool)::OpPMInfo&) const Pass.cpp:0:0\n#17 0x0000615b7445cf53 mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) (/opt/tensorflow/bin/tf-opt+0xfa9ff53)\n#18 0x0000615b7445d90d mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) (/opt/tensorflow/bin/tf-opt+0xfaa090d)\n#19 0x0000615b7445dd9c mlir::detail::OpToOpPassAdaptor::runPipeline(mlir::OpPassManager&, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) (/opt/tensorflow/bin/tf-opt+0xfaa0d9c)\n#20 0x0000615b7445ebef mlir::PassManager::run(mlir::Operation*) (/opt/tensorflow/bin/tf-opt+0xfaa1bef)\n#21 0x0000615b720fa135 performActions(llvm::raw_ostream&, std::shared_ptr<llvm::SourceMgr> const&, mlir::MLIRContext*, mlir::MlirOptMainConfig const&) MlirOptMain.cpp:0:0\n#22 0x0000615b720fa95e processBuffer(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::MlirOptMainConfig const&, mlir::DialectRegistry&, llvm::ThreadPoolInterface*) MlirOptMain.cpp:0:0\n#23 0x0000615b720faa51 mlir::LogicalResult llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>::callback_fn<mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::DialectRegistry&, mlir::MlirOptMainConfig const&)::'lambda'(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>(long, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&) MlirOptMain.cpp:0:0\n#24 0x0000615b7457910a mlir::splitAndProcessBuffer(std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::function_ref<mlir::LogicalResult (std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, llvm::raw_ostream&)>, llvm::raw_ostream&, llvm::StringRef, llvm::StringRef) (/opt/tensorflow/bin/tf-opt+0xfbbc10a)\n#25 0x0000615b720f3e9d mlir::MlirOptMain(llvm::raw_ostream&, std::unique_ptr<llvm::MemoryBuffer, std::default_delete<llvm::MemoryBuffer>>, mlir::DialectRegistry&, mlir::MlirOptMainConfig const&) (/opt/tensorflow/bin/tf-opt+0xd736e9d)\n#26 0x0000615b720fac07 mlir::MlirOptMain(int, char**, llvm::StringRef, llvm::StringRef, mlir::DialectRegistry&) (/opt/tensorflow/bin/tf-opt+0xd73dc07)\n#27 0x0000615b720fb11f mlir::MlirOptMain(int, char**, llvm::StringRef, mlir::DialectRegistry&) (/opt/tensorflow/bin/tf-opt+0xd73e11f)\n#28 0x0000615b660c9295 main (/opt/tensorflow/bin/tf-opt+0x170c295)\n#29 0x0000751fa326424a (/lib/x86_64-linux-gnu/libc.so.6+0x2724a)\n#30 0x0000751fa3264305 __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x27305)\n#31 0x0000615b663223f1 _start (/opt/tensorflow/bin/tf-opt+0x19653f1)\n(tflite-issue) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22:~/TFlite-Issue-#92660/tflite_tosa_issue$ \n```\nThis issue is happening with `Tensorflow: 2.13` version so could you please give it try with latest stable version of TensorFlow and see is it resolving your issue ? Is there any specific reason to use `Tensorflow: 2.13` version due to package dependancies issues or specific project requirement ?\n\nIf I have missed something here please let me know. Thank you for your cooperation and patience."
      }
    ]
  },
  {
    "issue_number": 93697,
    "title": "TensorFlow Lite in Play Services issue",
    "author": "manaseer-55",
    "state": "closed",
    "created_at": "2025-05-20T03:59:46Z",
    "updated_at": "2025-06-07T02:12:39Z",
    "labels": [
      "stat:awaiting response",
      "type:support",
      "stale",
      "comp:lite"
    ],
    "body": "**System information**\n- Android Device information (use `adb shell getprop ro.build.fingerprint`\n  if possible):\n- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):\n- Google Play Services version\n  (`Settings` > `Apps` > `Google Play Services` > `App details`):\n\n**Standalone code to reproduce the issue**\nProvide a reproducible test case that is the bare minimum necessary to generate\nthe problem. If possible, please share a link to or attach code demonstrating\nthe problem.\n\n**Any other info / logs**\nInclude any logs or source code that would be helpful to diagnose the problem.\nIf including tracebacks, please include the full traceback. Large logs and files\nshould be attached.\n",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @manaseer-55 \nThank you for bringing this issue to our attention, If possible could you please help us with your minimal code Github repo along with complete steps to reproduce the same behavior from our end that will be very helpful to investigate this issue further from our end ?\n\nThank you for your cooperation and understanding."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 93813,
    "title": "App Crashes During High-Volume TensorFlow Lite Inference (500-1000+ Calls)",
    "author": "Animesh081005",
    "state": "closed",
    "created_at": "2025-05-21T09:14:23Z",
    "updated_at": "2025-06-07T02:12:37Z",
    "labels": [
      "stat:awaiting response",
      "type:support",
      "stale",
      "comp:lite"
    ],
    "body": "**System information**\n- Android Device information (use `adb shell getprop ro.build.fingerprint`\n  if possible): NA\n- TensorFlow Lite in Play Services SDK version (found in `build.gradle`): \n- Google Play Services version\n  (`Settings` > `Apps` > `Google Play Services` > `App details`): 25.18.33\n\n**Steps to reproduce the issue**\nLoad the ML model in the app.\nEnsure the model is called in a loop or frequent real-world usage scenario.\nTrigger Inferences:\nRun inference in a loop (e.g., 1000 times) with sample input data.\n\n**Any other info / logs**\n\n05-17 16:46:25.666 10090  1083  1083 F DEBUG   :       #06 pc 000000000031ff48 libtensorflowlite_jni.so (offset 0x1624000)\n\n05-17 16:46:25.666 10090  1083  1083 F DEBUG   :       #07 pc 00000000001b6b5c libtensorflowlite_jni.so (offset 0x1624000)\n\n05-17 16:46:25.666 10090  1083  1083 F DEBUG   :       #08 pc 00000000001c0c64 libtensorflowlite_jni.so (offset 0x1624000)\n\n05-17 16:46:25.666 10090  1083  1083 F DEBUG   :       #09 pc 0000000000324514 libtensorflowlite_jni.so (offset 0x1624000)\n\n05-17 16:46:25.666 10090  1083  1083 F DEBUG   :       #10 pc 0000000000323f50 libtensorflowlite_jni.so (offset 0x1624000)\n\n05-17 16:46:25.666 10090  1083  1083 F DEBUG   :       #11 pc 0000000000318794  libtensorflowlite_jni.so (offset 0x1624000)\n\n05-17 16:46:25.666 10090  1083  1083 F DEBUG   :       #12 pc 0000000000085468 libtensorflowlite_jni.so (offset 0x1624000) (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+88)\n",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @Animesh081005 \nThank you for bringing this issue to our attention, If possible could you please help us with your minimal code Github repo along with complete steps to reproduce the same behavior from our end that will be very helpful to investigate this issue further from our end ?\n\nThank you for your cooperation and understanding."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 93612,
    "title": "Segmentation fault in tf.repeat",
    "author": "cx104906",
    "state": "closed",
    "created_at": "2025-05-18T04:39:47Z",
    "updated_at": "2025-06-06T02:13:00Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 18.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n6.5.0\n\n### GCC/compiler version\n\nclang 18.1.8\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSegfault when using tf.repeat\n\n### Standalone code to reproduce the issue\n\n```shell\nimport pickle\nimport tensorflow as tf\nimport time\nimport pprint\nimport numpy as np\n\nprint(tf.__version__)\n\nmylist = [((((((((((((((((((((((((9.30228233591653e-196,),),),),),),),),),),),),),),),),),),),),),),),),\n ((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((('68S^',),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),),)]\nmydict = {}\n\nfor i in range(10):\n  try:\n      print (f\"{i}\",flush=True)\n      tf.repeat(*mylist,**mydict)\n  except Exception as e:\n      print(f\"{e}\")\n\nprint(\"done\")\n```\n\n### Relevant log output\n\n```shell\n2.19.0\nSegmentation fault (core dumped)\n```",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "Looks like OOM issue."
      },
      {
        "user": "Venkat6871",
        "body": "Hi @cx104906 ,\nApologies for the delayed response, and thank you for raising your concern.\nThe issue appears to be caused by the use of deeply nested tuples in the input. Is there a specific reason for structuring the input this way?\nI tested a simplified version of your code with reduced nesting, and it ran successfully without errors. I am attaching [gist](https://colab.sandbox.google.com/gist/Venkat6871/e4add4ed978fcfb64ee3b606c4e49ab5/93612_tf_2-18-0-2-19-0-v.ipynb) here for you reference. Please let me know if this helps, or if you are still encountering issues.\n\nThank you!"
      },
      {
        "user": "cx104906",
        "body": "@Venkat6871 Thanks for the reply. I'm just using fuzzing."
      }
    ]
  },
  {
    "issue_number": 93617,
    "title": "Question about fuzzing TensorFlow with Atheris(oss-fuzz)",
    "author": "cx104906",
    "state": "closed",
    "created_at": "2025-05-18T15:49:07Z",
    "updated_at": "2025-06-06T02:12:57Z",
    "labels": [
      "stat:awaiting response",
      "type:feature",
      "stale",
      "comp:ops"
    ],
    "body": "Hello, I’ve been reading about TensorFlow’s fuzzing efforts (https://blog.tensorflow.org/2021/04/how-to-write-python-fuzzer-for-tensorflow.html?m=1) in the OSS-Fuzz tensorflow-py project (https://github.com/google/oss-fuzz/tree/master/projects/tensorflow-py).\nFrom my understanding, Atheris can use -fsanitize=fuzzer-no-link to collect C++ coverage during fuzzing, which improves the effectivenes(https://github.com/google/atheris/blob/master/native_extension_fuzzing.md).\nSo can I compile the entire tensorflow project with -fsanitize=fuzzer-no-link and then use atheris to collect c++ coverage to improve efficiency? I see that tensorflow-py in oss-fuzz doesn't seem to do this. If it doesn't work, can anyone tell me the reason?\nThank you very much for solving my question.\n",
    "comments": [
      {
        "user": "tilakrayal",
        "body": "@cx104906,\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thank you!"
      },
      {
        "user": "mihaimaruseac",
        "body": "CC @fcoUnda @pak-laura \n\nFrom my understanding, it should use that approach, but the issue is that TF library is too large and compiling might with the flag might time out / run into OOM."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      }
    ]
  },
  {
    "issue_number": 63548,
    "title": " TypeError: this __dict__ descriptor does not support '_DictWrapper' objects during trivial model save",
    "author": "iamthebot",
    "state": "open",
    "created_at": "2024-03-12T19:52:24Z",
    "updated_at": "2025-06-06T02:09:51Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:model",
      "TF 2.16"
    ],
    "body": "### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo (bug doesn't exist in tf-nightly 2.17.0.dev20240312)\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\nv2.16.1\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nOSX\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.12\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen calling ` tf.saved_model.save(model, saved_model_path)`\r\n\r\nwe see:\r\n\r\n```\r\n../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py:190: in list_children\r\n    for name, child in super(_AugmentedGraphView, self).list_children(\r\n../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py:75: in list_children\r\n    for name, ref in super(ObjectGraphView,\r\n../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/trackable_view.py:85: in children\r\n    ref = converter.convert_to_trackable(ref, parent=obj)\r\n../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/trackable/converter.py:31: in convert_to_trackable\r\n    if (tensor_util.is_tf_type(obj) and\r\n../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/framework/tensor_util.py:1156: in is_tf_type\r\n    return isinstance(x, tf_type_classes)\r\n../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/typing.py:1871: in __instancecheck__\r\n    val = getattr_static(instance, attr)\r\n../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/inspect.py:1839: in getattr_static\r\n    instance_result = _check_instance(obj, attr)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nobj = DictWrapper({'input_shape': [(None, 16), (None, 32)]}), attr = 'is_tensor_like'\r\n\r\n    def _check_instance(obj, attr):\r\n        instance_dict = {}\r\n        try:\r\n>           instance_dict = object.__getattribute__(obj, \"__dict__\")\r\nE           TypeError: this __dict__ descriptor does not support '_DictWrapper' objects\r\n\r\n../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/inspect.py:1793: TypeError\r\n```\r\n\r\nI suspect this is related to #59869 which was supposedly fixed. However, in 2.16.1 TF removes the pin on wrapt and the issue indeed persists. I've even tried downgrading wrapt to 1.14.1 and the issue remains.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nimport tempfile\r\n\r\ndef get_two_tower_models():\r\n    online_features = [layers.Input(shape=(32,)), layers.Input(shape=(16,))]\r\n    offline_features = [layers.Input(shape=(16,)), layers.Input(shape=(32,))]\r\n    all_features = []\r\n    all_features.extend(online_features)\r\n    all_features.extend(offline_features)\r\n\r\n    def get_offline_tower(offline_features):\r\n        offline_inputs = layers.concatenate(offline_features, name=\"offline_concatenated\")\r\n        offline_hidden = layers.Dense(32, activation=\"tanh\", name=\"offline_hidden_1\")(offline_inputs)\r\n        offline_hidden = layers.Dense(16, activation=\"tanh\", name=\"offline_hidden_2\")(offline_hidden)\r\n        offline_final_embed = layers.Dense(8, name=\"offline_hidden_3\")(offline_hidden)\r\n\r\n        return offline_final_embed\r\n\r\n    def get_online_tower(online_features):\r\n        online_inputs = layers.concatenate(online_features, name=\"online_concatenated\")\r\n        online_hidden = layers.Dense(32, activation=\"tanh\", name=\"online_hidden_1\")(online_inputs)\r\n        online_hidden = layers.Dense(16, activation=\"tanh\", name=\"online_hidden_2\")(online_hidden)\r\n        online_final_embed = layers.Dense(8, name=\"online_hidden_3\")(online_hidden)\r\n\r\n        return online_final_embed\r\n\r\n    offline_tower_embed = get_offline_tower(offline_features)\r\n    online_tower_embed = get_online_tower(online_features)\r\n\r\n    # We normalize vectors with L2 norm to make sure we get the cosine similarity\r\n    offline_online_dot = layers.Dot(axes=1, normalize=True)([offline_tower_embed, online_tower_embed])\r\n\r\n    offline_tower_model = tf.keras.Model(inputs=offline_features, outputs=offline_tower_embed)\r\n    online_tower_model = tf.keras.Model(inputs=online_features, outputs=online_tower_embed)\r\n\r\n    full_model = tf.keras.Model(inputs=all_features, outputs=offline_online_dot)\r\n\r\n    return (full_model, offline_tower_model, online_tower_model)\r\n\r\nfull_model, offline_tower_model, online_tower_model = get_two_tower_models()\r\n\r\nwith tempfile.TemporaryDirectory() as tmpdirname:\r\n    tf.saved_model.save(online_tower_model, tmpdirname)\r\n```\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"/Users/alfredo_luque/repos/git.musta.ch/airbnb/bighead-service/packages/ml-frameworks/tensorflow/tests/minimal_repro.py\", line 44, in <module>\r\n    tf.saved_model.save(online_tower_model, tmpdirname)\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py\", line 1392, in save\r\n    save_and_return_nodes(obj, export_dir, signatures, options)\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py\", line 1427, in save_and_return_nodes\r\n    _build_meta_graph(obj, signatures, options, meta_graph_def))\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py\", line 1642, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py\", line 1564, in _build_meta_graph_impl\r\n    saveable_view = _SaveableView(augmented_graph_view, options)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py\", line 285, in __init__\r\n    checkpoint_util.objects_ids_and_slot_variables_and_paths(\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/util.py\", line 160, in objects_ids_and_slot_variables_and_paths\r\n    trackable_objects, node_paths = graph_view.breadth_first_traversal()\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py\", line 124, in breadth_first_traversal\r\n    return self._breadth_first_traversal()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py\", line 156, in _breadth_first_traversal\r\n    super(_AugmentedGraphView, self)._breadth_first_traversal())\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py\", line 128, in _breadth_first_traversal\r\n    return super(ObjectGraphView, self)._descendants_with_paths()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/trackable_view.py\", line 111, in _descendants_with_paths\r\n    for name, dependency in self.children(current_trackable).items():\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py\", line 97, in children\r\n    for name, ref in self.list_children(obj, **kwargs):\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py\", line 190, in list_children\r\n    for name, child in super(_AugmentedGraphView, self).list_children(\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py\", line 75, in list_children\r\n    for name, ref in super(ObjectGraphView,\r\n                     ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/trackable_view.py\", line 85, in children\r\n    ref = converter.convert_to_trackable(ref, parent=obj)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/trackable/converter.py\", line 31, in convert_to_trackable\r\n    if (tensor_util.is_tf_type(obj) and\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/framework/tensor_util.py\", line 1156, in is_tf_type\r\n    return isinstance(x, tf_type_classes)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/typing.py\", line 1871, in __instancecheck__\r\n    val = getattr_static(instance, attr)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/inspect.py\", line 1839, in getattr_static\r\n    instance_result = _check_instance(obj, attr)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/inspect.py\", line 1793, in _check_instance\r\n    instance_dict = object.__getattribute__(obj, \"__dict__\")\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: this __dict__ descriptor does not support '_DictWrapper' objects\r\n```\r\n",
    "comments": [
      {
        "user": "iamthebot",
        "body": "I can verify this bug is not present in 2.15.1. Nor is it present in `tf-nightly==2.17.0.dev20240312` with `keras-nightly==3.1.0.dev2024031203`"
      },
      {
        "user": "darrina",
        "body": "Same issue, even simpler example:\r\n\r\n- Python: 3.12.2\r\n- TensorFlow: 2.16.0rc0, 2.16.1, and 2.17.0-dev20240312\r\n\r\n```\r\nimport tensorflow as tf\r\npretrained_model = tf.keras.applications.MobileNet()\r\ntf.saved_model.save(pretrained_model, \"mobilenet\")\r\n```\r\n\r\n```\r\nTypeError: this __dict__ descriptor does not support '_DictWrapper' objects\r\n```\r\n\r\nTensorFlow 2.16.0rc0 and 2.16.1 are the only `tensorflow` versions currently available using pip install."
      },
      {
        "user": "txchen",
        "body": "I am having the exact same issue. "
      }
    ]
  },
  {
    "issue_number": 94650,
    "title": "MirroredStrategy does not log anything / does not generate any epoch / batch *.keras with multiple cores of NVIDIA A16 (single core of the same graphics works)",
    "author": "ldrahnik",
    "state": "closed",
    "created_at": "2025-06-02T11:29:41Z",
    "updated_at": "2025-06-05T09:32:00Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nServer 24.04.2 LTS (Noble Numbat)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\ngcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\n\n### CUDA/cuDNN version\n\n12.8\n\n### GPU model and memory\n\nNVIDIA A16\n\n### Current behavior?\n\nUsing arg `--no_mirroring` the same code works on 1 GPU core fine, but when I try mirroring using `tf.distribute.MirroredStrategy()` (no more differences in the code) like this:\n\n```\n# --- Model definition ---\ndef build_model(image_size, labels_count):\n    input_shape = (image_size, image_size, 3)\n    model = Sequential()\n    ...\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nif args.no_mirroring:\n    print(\"⚙️ Training without MirroredStrategy (single GPU mode)\")\n    model = build_model(IMAGE_SIZE, len(CLASS_NAMES))\nelse:\n    strategy = tf.distribute.MirroredStrategy()\n    print(f\"✅ Using {strategy.num_replicas_in_sync} GPUs\")\n\n    with strategy.scope():\n        model = build_model(IMAGE_SIZE, len(CLASS_NAMES))\n```\n\nthen are not displayed logs about progress as previously, it ends as below and it writes to the log nothing more (but all 4 cores GPU are fully utilized and each uses a lot of VRAM but not epoch checkpoint .keras file / batch file is generated:\n\n```\n2025-06-02 13:05:42.146276: I tensorflow/core/common_runtime/eager/execute.cc:1754] Executing op __inference_multi_step_on_iterator_13010 in device /job:localhost/replica:0/task:0/device:GPU:0\nI0000 00:00:1748862342.350336 1523332 cuda_dnn.cc:529] Loaded cuDNN version 91001\nI0000 00:00:1748862342.455305 1523335 cuda_dnn.cc:529] Loaded cuDNN version 91001\nI0000 00:00:1748862342.467518 1523323 cuda_dnn.cc:529] Loaded cuDNN version 91001\nI0000 00:00:1748862342.485236 1523330 cuda_dnn.cc:529] Loaded cuDNN version 91001\nvolta:1523113:1524251 [0] NCCL INFO Bootstrap : Using bond0:192.168.22.46<0>\nvolta:1523113:1524254 [0] NCCL INFO cudaDriverVersion 12080\nvolta:1523113:1524254 [0] NCCL INFO NCCL version 2.23.4+cudaCUDA_MAJOR.CUDA_MINOR\nvolta:1523113:1524263 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal network plugin.\nvolta:1523113:1524263 [1] NCCL INFO NET/IB : No device found.\nvolta:1523113:1524263 [1] NCCL INFO NET/Socket : Using [0]bond0:192.168.22.46<0>\nvolta:1523113:1524263 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\nvolta:1523113:1524263 [1] NCCL INFO Using network Socket\nvolta:1523113:1524265 [3] NCCL INFO Using network Socket\nvolta:1523113:1524262 [0] NCCL INFO Using network Socket\nvolta:1523113:1524264 [2] NCCL INFO Using network Socket\nvolta:1523113:1524265 [3] NCCL INFO ncclCommInitRank comm 0x71e06c0c66f0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 10000 commId 0x38d39a8a877ff0a5 - Init START\nvolta:1523113:1524263 [1] NCCL INFO ncclCommInitRank comm 0x71e06c047830 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId c000 commId 0x38d39a8a877ff0a5 - Init START\nvolta:1523113:1524264 [2] NCCL INFO ncclCommInitRank comm 0x71e06c086f90 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId e000 commId 0x38d39a8a877ff0a5 - Init START\nvolta:1523113:1524262 [0] NCCL INFO ncclCommInitRank comm 0x71e06c0080d0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId a000 commId 0x38d39a8a877ff0a5 - Init START\nvolta:1523113:1524262 [0] NCCL INFO Bootstrap timings total 0.001143 (create 0.000039, send 0.000150, recv 0.000395, ring 0.000372, delay 0.000000)\nvolta:1523113:1524263 [1] NCCL INFO Bootstrap timings total 0.001241 (create 0.000069, send 0.000225, recv 0.000503, ring 0.000151, delay 0.000000)\nvolta:1523113:1524265 [3] NCCL INFO Bootstrap timings total 0.001272 (create 0.000090, send 0.000232, recv 0.000297, ring 0.000108, delay 0.000001)\nvolta:1523113:1524264 [2] NCCL INFO Bootstrap timings total 0.001162 (create 0.000055, send 0.000184, recv 0.000540, ring 0.000095, delay 0.000000)\nvolta:1523113:1524263 [1] NCCL INFO Setting affinity for GPU 1 to 0f,ff000fff\nvolta:1523113:1524263 [1] NCCL INFO NVLS multicast support is not available on dev 1\nvolta:1523113:1524262 [0] NCCL INFO Setting affinity for GPU 0 to 0f,ff000fff\nvolta:1523113:1524262 [0] NCCL INFO NVLS multicast support is not available on dev 0\nvolta:1523113:1524264 [2] NCCL INFO Setting affinity for GPU 2 to 0f,ff000fff\nvolta:1523113:1524264 [2] NCCL INFO NVLS multicast support is not available on dev 2\nvolta:1523113:1524265 [3] NCCL INFO Setting affinity for GPU 3 to 0f,ff000fff\nvolta:1523113:1524265 [3] NCCL INFO NVLS multicast support is not available on dev 3\nvolta:1523113:1524263 [1] NCCL INFO comm 0x71e06c047830 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0\nvolta:1523113:1524262 [0] NCCL INFO comm 0x71e06c0080d0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0\nvolta:1523113:1524264 [2] NCCL INFO comm 0x71e06c086f90 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0\nvolta:1523113:1524263 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\nvolta:1523113:1524263 [1] NCCL INFO P2P Chunksize set to 131072\nvolta:1523113:1524262 [0] NCCL INFO Channel 00/02 : 0 1 2 3\nvolta:1523113:1524264 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\nvolta:1523113:1524265 [3] NCCL INFO comm 0x71e06c0c66f0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0\nvolta:1523113:1524262 [0] NCCL INFO Channel 01/02 : 0 1 2 3\nvolta:1523113:1524262 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\nvolta:1523113:1524262 [0] NCCL INFO P2P Chunksize set to 131072\nvolta:1523113:1524264 [2] NCCL INFO P2P Chunksize set to 131072\nvolta:1523113:1524265 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2\nvolta:1523113:1524265 [3] NCCL INFO P2P Chunksize set to 131072\nvolta:1523113:1524266 [1] NCCL INFO [Proxy Service] Device 1 CPU core 27\nvolta:1523113:1524267 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5\nvolta:1523113:1524271 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7\nvolta:1523113:1524270 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 5\nvolta:1523113:1524268 [3] NCCL INFO [Proxy Service] Device 3 CPU core 30\nvolta:1523113:1524269 [2] NCCL INFO [Proxy Service] Device 2 CPU core 7\nvolta:1523113:1524272 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 7\nvolta:1523113:1524273 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 8\nvolta:1523113:1524262 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\nvolta:1523113:1524262 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\nvolta:1523113:1524263 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\nvolta:1523113:1524263 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\nvolta:1523113:1524265 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\nvolta:1523113:1524265 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\nvolta:1523113:1524262 [0] NCCL INFO CC Off, Multi-GPU CC Off, workFifoBytes 1048576\nvolta:1523113:1524264 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\nvolta:1523113:1524264 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\nvolta:1523113:1524263 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so libnccl-net.so. Using internal tuner plugin.\nvolta:1523113:1524263 [1] NCCL INFO ncclCommInitRank comm 0x71e06c047830 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId c000 commId 0x38d39a8a877ff0a5 - Init COMPLETE\nvolta:1523113:1524263 [1] NCCL INFO Init timings - ncclCommInitRank: rank 1 nranks 4 total 0.50 (kernels 0.40, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.03, graphs 0.00, connections 0.01, rest 0.00)\nvolta:1523113:1524265 [3] NCCL INFO ncclCommInitRank comm 0x71e06c0c66f0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 10000 commId 0x38d39a8a877ff0a5 - Init COMPLETE\nvolta:1523113:1524262 [0] NCCL INFO ncclCommInitRank comm 0x71e06c0080d0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId a000 commId 0x38d39a8a877ff0a5 - Init COMPLETE\nvolta:1523113:1524265 [3] NCCL INFO Init timings - ncclCommInitRank: rank 3 nranks 4 total 0.50 (kernels 0.41, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.03, graphs 0.00, connections 0.01, rest 0.00)\nvolta:1523113:1524262 [0] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 4 total 0.50 (kernels 0.41, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.03, graphs 0.00, connections 0.01, rest 0.00)\nvolta:1523113:1524264 [2] NCCL INFO ncclCommInitRank comm 0x71e06c086f90 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId e000 commId 0x38d39a8a877ff0a5 - Init COMPLETE\nvolta:1523113:1524264 [2] NCCL INFO Init timings - ncclCommInitRank: rank 2 nranks 4 total 0.50 (kernels 0.41, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.03, graphs 0.00, connections 0.01, rest 0.00)\nvolta:1523113:1524277 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/direct pointer\nvolta:1523113:1524275 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/direct pointer\nvolta:1523113:1524276 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/direct pointer\nvolta:1523113:1524277 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/direct pointer\nvolta:1523113:1524275 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/direct pointer\nvolta:1523113:1524274 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/direct pointer\nvolta:1523113:1524276 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/direct pointer\nvolta:1523113:1524274 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/direct pointer\nvolta:1523113:1524275 [3] NCCL INFO Connected all rings\nvolta:1523113:1524276 [2] NCCL INFO Connected all rings\nvolta:1523113:1524277 [0] NCCL INFO Connected all rings\nvolta:1523113:1524274 [1] NCCL INFO Connected all rings\n```\n\nHow I start the code:\n\n```\nnohup env GRPC_TRACE=all GRPC_VERBOSITY=DEBUG GRPC_GO_LOG_SEVERITY_LEVEL=info GRPC_GO_LOG_VERBOSITY_LEVEL=2 CGO_ENABLED=1 NCCL_DEBUG=INFO sh -c 'python3 -u ./train-clustering-itself.py --batch-size 256 && mail -s \"train-output\" xz@xz. < train-output.log' > train-output.log 2>&1 &\n```\n\n```\nnvidia-smi \nMon Jun  2 13:12:23 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A16                     Off |   00000000:0A:00.0 Off |                  Off |\n|  0%   50C    P0             28W /   62W |    4342MiB /  16380MiB |    100%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA A16                     Off |   00000000:0C:00.0 Off |                  Off |\n|  0%   53C    P0             28W /   62W |    4334MiB /  16380MiB |    100%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA A16                     Off |   00000000:0E:00.0 Off |                  Off |\n|  0%   41C    P0             27W /   62W |    4334MiB /  16380MiB |    100%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA A16                     Off |   00000000:10:00.0 Off |                  Off |\n|  0%   37C    P0             27W /   62W |    4334MiB /  16380MiB |    100%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A         1523113      C   python3                                4334MiB |\n|    1   N/A  N/A         1523113      C   python3                                4326MiB |\n|    2   N/A  N/A         1523113      C   python3                                4326MiB |\n|    3   N/A  N/A         1523113      C   python3                                4326MiB |\n```\nWhat I do wrong? How I could find out more?\n\n\n### Relevant log output\n\n```shell\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\n2025-06-02 13:24:54.690328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748863494.709822 1526982 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748863494.715725 1526982 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1748863494.731933 1526982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1748863494.731955 1526982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1748863494.731958 1526982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1748863494.731960 1526982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-06-02 13:24:54.736761: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nv2.19.0-rc0-6-ge36baa30292 2.19.0\n```",
    "comments": [
      {
        "user": "ldrahnik",
        "body": "Minimal reproduce example: \n\n```\nimport tensorflow as tf\nimport numpy as np\nimport time\nimport os\n\n# 🧠 Callback pro měření času trénování\nclass TimingCallback(tf.keras.callbacks.Callback):\n    def on_train_begin(self, logs=None):\n        self.epoch_times = []\n        self.train_start = time.time()\n\n    def on_epoch_begin(self, epoch, logs=None):\n        self.epoch_start = time.time()\n        print(f\"\\n🚀 Začíná epocha {epoch + 1}\", flush=True)\n\n    def on_epoch_end(self, epoch, logs=None):\n        duration = time.time() - self.epoch_start\n        self.epoch_times.append(duration)\n        print(f\"⏱️ Epocha {epoch + 1} dokončena za {duration:.2f} s\", flush=True)\n\n    def on_train_end(self, logs=None):\n        total = time.time() - self.train_start\n        print(f\"\\n✅ Celkový čas trénování: {total:.2f} s\", flush=True)\n\n# 📁 Výstupní adresář pro checkpointy\nos.makedirs(\"checkpoints\", exist_ok=True)\n\n# 💾 Callback pro ukládání po každém batchi\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"checkpoints/model_batch_{batch:06d}.keras\",\n    save_freq='batch',\n    save_weights_only=False,\n    verbose=1\n)\n\n# ⚙️ Strategie pro více GPU\nstrategy = tf.distribute.MirroredStrategy()\nprint(f\"🔧 Počet GPU zařízení: {strategy.num_replicas_in_sync}\", flush=True)\n\n# 📌 Nastavení\nepochs = 5\nbatch_size_per_replica = 16\nglobal_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\nlearning_rate = 0.001\n\n# 📦 Umělý dataset\nnum_samples = 1000\nx_train = np.random.rand(num_samples, 224, 224, 3).astype(np.float32)\ny_train = np.random.randint(0, 2, size=(num_samples,)).astype(np.int32)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_dataset = train_dataset.shuffle(1000).batch(global_batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n\n# 🏗️ Model ve scope strategii\nwith strategy.scope():\n    model = tf.keras.applications.MobileNetV2(\n        input_shape=(224, 224, 3),\n        weights=None,\n        classes=2\n    )\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        metrics=['accuracy']\n    )\n\n# 🧪 Trénování s verbose výstupem\nprint(\"\\n🚀 Spouštím model.fit() ...\", flush=True)\nmodel.fit(\n    train_dataset,\n    epochs=epochs,\n    callbacks=[TimingCallback(), checkpoint_cb],\n    verbose=2  # ← vypíše každou batch\n)\n\n```\n\n```$ nohup env GRPC_TRACE=all GRPC_VERBOSITY=DEBUG GRPC_GO_LOG_SEVERITY_LEVEL=info GRPC_GO_LOG_VERBOSITY_LEVEL=2 CGO_ENABLED=1 NCCL_DEBUG=INFO sh -c 'python3 -u ./test-mirroring.py --train-if-missing-only --batch-size 120 && mail -s \"output-text\" xz@xz.xz < output-text.log' > output-text.log 2>&1 &```\n\nThe log:\n\n```\n$ cat output-text.log.log \nnohup: ignoring input\n2025-06-02 13:54:44.361907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748865284.379331 1533219 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748865284.384869 1533219 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1748865284.400156 1533219 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1748865284.400175 1533219 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1748865284.400178 1533219 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1748865284.400180 1533219 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-06-02 13:54:44.404544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nI0000 00:00:1748865293.307776 1533219 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14395 MB memory:  -> device: 0, name: NVIDIA A16, pci bus id: 0000:0a:00.0, compute capability: 8.6\nI0000 00:00:1748865293.309871 1533219 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14395 MB memory:  -> device: 1, name: NVIDIA A16, pci bus id: 0000:0c:00.0, compute capability: 8.6\nI0000 00:00:1748865293.311851 1533219 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14395 MB memory:  -> device: 2, name: NVIDIA A16, pci bus id: 0000:0e:00.0, compute capability: 8.6\nI0000 00:00:1748865293.313823 1533219 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14395 MB memory:  -> device: 3, name: NVIDIA A16, pci bus id: 0000:10:00.0, compute capability: 8.6\n🔧 Počet GPU zařízení: 4\n\n🚀 Spouštím model.fit() ...\n\n🚀 Začíná epocha 1\nEpoch 1/5\nI0000 00:00:1748865423.083862 1533436 cuda_dnn.cc:529] Loaded cuDNN version 91001\nI0000 00:00:1748865423.234299 1533437 cuda_dnn.cc:529] Loaded cuDNN version 91001\nI0000 00:00:1748865423.432656 1533432 cuda_dnn.cc:529] Loaded cuDNN version 91001\nI0000 00:00:1748865423.910540 1533429 cuda_dnn.cc:529] Loaded cuDNN version 91001\nvolta:1533219:1535118 [0] NCCL INFO Bootstrap : Using bond0:192.168.22.46<0>\nvolta:1533219:1535120 [0] NCCL INFO cudaDriverVersion 12080\nvolta:1533219:1535120 [0] NCCL INFO NCCL version 2.23.4+cudaCUDA_MAJOR.CUDA_MINOR\nvolta:1533219:1535132 [3] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal network plugin.\nvolta:1533219:1535132 [3] NCCL INFO NET/IB : No device found.\nvolta:1533219:1535132 [3] NCCL INFO NET/Socket : Using [0]bond0:192.168.22.46<0>\nvolta:1533219:1535132 [3] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\nvolta:1533219:1535132 [3] NCCL INFO Using network Socket\nvolta:1533219:1535129 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\nvolta:1533219:1535129 [0] NCCL INFO Using network Socket\nvolta:1533219:1535131 [2] NCCL INFO Using network Socket\nvolta:1533219:1535130 [1] NCCL INFO Using network Socket\nvolta:1533219:1535130 [1] NCCL INFO ncclCommInitRank comm 0x7d10f405e900 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId c000 commId 0x5d7760b8864df153 - Init START\nvolta:1533219:1535129 [0] NCCL INFO ncclCommInitRank comm 0x7d10f4020c10 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId a000 commId 0x5d7760b8864df153 - Init START\nvolta:1533219:1535131 [2] NCCL INFO ncclCommInitRank comm 0x7d10f409c5f0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId e000 commId 0x5d7760b8864df153 - Init START\nvolta:1533219:1535132 [3] NCCL INFO ncclCommInitRank comm 0x7d10f40da2e0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 10000 commId 0x5d7760b8864df153 - Init START\nvolta:1533219:1535129 [0] NCCL INFO Bootstrap timings total 0.017000 (create 0.000059, send 0.000192, recv 0.000439, ring 0.000118, delay 0.000000)\nvolta:1533219:1535131 [2] NCCL INFO Bootstrap timings total 0.016949 (create 0.000044, send 0.000137, recv 0.000812, ring 0.000118, delay 0.000000)\nvolta:1533219:1535130 [1] NCCL INFO Bootstrap timings total 0.017062 (create 0.000073, send 0.000236, recv 0.000627, ring 0.015833, delay 0.000001)\nvolta:1533219:1535132 [3] NCCL INFO Bootstrap timings total 0.016872 (create 0.000054, send 0.000184, recv 0.000822, ring 0.000121, delay 0.000000)\nvolta:1533219:1535130 [1] NCCL INFO Setting affinity for GPU 1 to 0f,ff000fff\nvolta:1533219:1535129 [0] NCCL INFO Setting affinity for GPU 0 to 0f,ff000fff\nvolta:1533219:1535131 [2] NCCL INFO Setting affinity for GPU 2 to 0f,ff000fff\nvolta:1533219:1535130 [1] NCCL INFO NVLS multicast support is not available on dev 1\nvolta:1533219:1535129 [0] NCCL INFO NVLS multicast support is not available on dev 0\nvolta:1533219:1535132 [3] NCCL INFO Setting affinity for GPU 3 to 0f,ff000fff\nvolta:1533219:1535132 [3] NCCL INFO NVLS multicast support is not available on dev 3\nvolta:1533219:1535131 [2] NCCL INFO NVLS multicast support is not available on dev 2\nvolta:1533219:1535131 [2] NCCL INFO comm 0x7d10f409c5f0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0\nvolta:1533219:1535132 [3] NCCL INFO comm 0x7d10f40da2e0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0\nvolta:1533219:1535130 [1] NCCL INFO comm 0x7d10f405e900 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0\nvolta:1533219:1535132 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2\nvolta:1533219:1535129 [0] NCCL INFO comm 0x7d10f4020c10 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0\nvolta:1533219:1535132 [3] NCCL INFO P2P Chunksize set to 131072\nvolta:1533219:1535129 [0] NCCL INFO Channel 00/02 : 0 1 2 3\nvolta:1533219:1535130 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\nvolta:1533219:1535129 [0] NCCL INFO Channel 01/02 : 0 1 2 3\nvolta:1533219:1535130 [1] NCCL INFO P2P Chunksize set to 131072\nvolta:1533219:1535129 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\nvolta:1533219:1535129 [0] NCCL INFO P2P Chunksize set to 131072\nvolta:1533219:1535131 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\nvolta:1533219:1535131 [2] NCCL INFO P2P Chunksize set to 131072\nvolta:1533219:1535133 [3] NCCL INFO [Proxy Service] Device 3 CPU core 27\nvolta:1533219:1535135 [1] NCCL INFO [Proxy Service] Device 1 CPU core 29\nvolta:1533219:1535136 [0] NCCL INFO [Proxy Service] Device 0 CPU core 30\nvolta:1533219:1535138 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 31\nvolta:1533219:1535137 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 4\nvolta:1533219:1535134 [2] NCCL INFO [Proxy Service] Device 2 CPU core 27\nvolta:1533219:1535139 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 32\nvolta:1533219:1535140 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 33\nvolta:1533219:1535130 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\nvolta:1533219:1535130 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\nvolta:1533219:1535132 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\nvolta:1533219:1535132 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\nvolta:1533219:1535131 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\nvolta:1533219:1535131 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\nvolta:1533219:1535129 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\nvolta:1533219:1535129 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\nvolta:1533219:1535129 [0] NCCL INFO CC Off, Multi-GPU CC Off, workFifoBytes 1048576\nvolta:1533219:1535130 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so libnccl-net.so. Using internal tuner plugin.\nvolta:1533219:1535130 [1] NCCL INFO ncclCommInitRank comm 0x7d10f405e900 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId c000 commId 0x5d7760b8864df153 - Init COMPLETE\nvolta:1533219:1535130 [1] NCCL INFO Init timings - ncclCommInitRank: rank 1 nranks 4 total 0.56 (kernels 0.44, alloc 0.05, bootstrap 0.02, allgathers 0.00, topo 0.03, graphs 0.00, connections 0.01, rest 0.01)\nvolta:1533219:1535132 [3] NCCL INFO ncclCommInitRank comm 0x7d10f40da2e0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 10000 commId 0x5d7760b8864df153 - Init COMPLETE\nvolta:1533219:1535131 [2] NCCL INFO ncclCommInitRank comm 0x7d10f409c5f0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId e000 commId 0x5d7760b8864df153 - Init COMPLETE\nvolta:1533219:1535132 [3] NCCL INFO Init timings - ncclCommInitRank: rank 3 nranks 4 total 0.56 (kernels 0.42, alloc 0.07, bootstrap 0.02, allgathers 0.00, topo 0.03, graphs 0.00, connections 0.01, rest 0.01)\nvolta:1533219:1535131 [2] NCCL INFO Init timings - ncclCommInitRank: rank 2 nranks 4 total 0.56 (kernels 0.43, alloc 0.06, bootstrap 0.02, allgathers 0.00, topo 0.03, graphs 0.00, connections 0.01, rest 0.01)\nvolta:1533219:1535129 [0] NCCL INFO ncclCommInitRank comm 0x7d10f4020c10 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId a000 commId 0x5d7760b8864df153 - Init COMPLETE\nvolta:1533219:1535129 [0] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 4 total 0.56 (kernels 0.43, alloc 0.06, bootstrap 0.02, allgathers 0.00, topo 0.03, graphs 0.00, connections 0.01, rest 0.00)\nvolta:1533219:1535142 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/direct pointer\nvolta:1533219:1535144 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/direct pointer\nvolta:1533219:1535143 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/direct pointer\nvolta:1533219:1535142 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/direct pointer\nvolta:1533219:1535141 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/direct pointer\nvolta:1533219:1535144 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/direct pointer\nvolta:1533219:1535143 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/direct pointer\nvolta:1533219:1535141 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/direct pointer\nvolta:1533219:1535144 [2] NCCL INFO Connected all rings\nvolta:1533219:1535143 [1] NCCL INFO Connected all rings\nvolta:1533219:1535142 [3] NCCL INFO Connected all rings\nvolta:1533219:1535141 [0] NCCL INFO Connected all rings\n```\n\n\nthe same code (also used scope mirroring), the same device but only 1 core (`CUDA_VISIBLE_DEVICES=2`):\n\n```\ntail -f output-text.log\n2025-06-02 15:27:29.495160: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748870849.513136 1550850 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748870849.521424 1550850 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1748870849.542132 1550850 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1748870849.542160 1550850 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1748870849.542164 1550850 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1748870849.542167 1550850 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-06-02 15:27:29.547943: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nI0000 00:00:1748870858.813298 1550850 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14395 MB memory:  -> device: 0, name: NVIDIA A16, pci bus id: 0000:0a:00.0, compute capability: 8.6\n🔧 Počet GPU zařízení: 1\n\n🚀 Spouštím model.fit() ...\n\n🚀 Začíná epocha 1\nEpoch 1/5\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1748870883.053445 1551056 service.cc:152] XLA service 0x7febfc002390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1748870883.053515 1551056 service.cc:160]   StreamExecutor device (0): NVIDIA A16, Compute Capability 8.6\n2025-06-02 15:28:03.712105: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\nI0000 00:00:1748870886.289025 1551056 cuda_dnn.cc:529] Loaded cuDNN version 91001\n2025-06-02 15:28:09.950996: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7895', 948 bytes spill stores, 948 bytes spill loads\n\n2025-06-02 15:28:21.765461: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-06-02 15:28:21.893219: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nI0000 00:00:1748870924.144948 1551056 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n\nEpoch 1: saving model to checkpoints/model_batch_000001.keras\n\nEpoch 1: saving model to checkpoints/model_batch_000002.keras\n\nEpoch 1: saving model to checkpoints/model_batch_000003.keras\n\nEpoch 1: saving model to checkpoints/model_batch_000004.keras\n\n...\n\nEpoch 1: saving model to checkpoints/model_batch_000062.keras\n⏱️ Epocha 1 dokončena za 113.73 s\n62/62 - 114s - 2s/step - accuracy: 0.4768 - loss: 0.8501\n\n🚀 Začíná epocha 2\nEpoch 2/5\n\n...\n```"
      },
      {
        "user": "ldrahnik",
        "body": "Nightly I tried like this with no difference: \n```\n$ pip uninstall tensorflow\nFound existing installation: tensorflow 2.19.0\nUninstalling tensorflow-2.19.0:\n  Would remove:\n    /mnt/magnus/ccp/drahnikl/projects/ecg-cnn/.penv/bin/import_pb_to_tensorboard\n    /mnt/magnus/ccp/drahnikl/projects/ecg-cnn/.penv/bin/saved_model_cli\n    /mnt/magnus/ccp/drahnikl/projects/ecg-cnn/.penv/bin/tensorboard\n    /mnt/magnus/ccp/drahnikl/projects/ecg-cnn/.penv/bin/tf_upgrade_v2\n    /mnt/magnus/ccp/drahnikl/projects/ecg-cnn/.penv/bin/tflite_convert\n    /mnt/magnus/ccp/drahnikl/projects/ecg-cnn/.penv/bin/toco\n    /mnt/magnus/ccp/drahnikl/projects/ecg-cnn/.penv/lib/python3.11/site-packages/tensorflow-2.19.0.dist-info/*\n    /mnt/magnus/ccp/drahnikl/projects/ecg-cnn/.penv/lib/python3.11/site-packages/tensorflow/*\nProceed (Y/n)? y\n  Successfully uninstalled tensorflow-2.19.0\n\n\n$ pip install tf-nightly\nCollecting tf-nightly\n  Downloading tf_nightly-2.20.0.dev20250602-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nRequirement already satisfied: absl-py>=1.0.0 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (2.3.0)\nRequirement already satisfied: astunparse>=1.6.0 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (0.4.0)\nRequirement already satisfied: google_pasta>=0.1.1 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (18.1.1)\nRequirement already satisfied: opt_einsum>=2.3.2 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (3.4.0)\nRequirement already satisfied: packaging in ./.penv/lib/python3.11/site-packages (from tf-nightly) (25.0)\nRequirement already satisfied: protobuf>=4.21.6 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (4.25.8)\nRequirement already satisfied: requests<3,>=2.21.0 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (2.32.3)\nRequirement already satisfied: setuptools in ./.penv/lib/python3.11/site-packages (from tf-nightly) (80.9.0)\nRequirement already satisfied: six>=1.12.0 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (3.1.0)\nRequirement already satisfied: typing_extensions>=3.6.6 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (4.13.2)\nRequirement already satisfied: wrapt>=1.11.0 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (1.14.1)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (1.71.0)\nCollecting tb-nightly~=2.19.0.a (from tf-nightly)\n  Downloading tb_nightly-2.19.0a20250218-py3-none-any.whl.metadata (1.8 kB)\nCollecting keras-nightly>=3.6.0.dev (from tf-nightly)\n  Downloading keras_nightly-3.10.0.dev2025060203-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: numpy<2.2.0,>=1.26.0 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (3.12.1)\nRequirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in ./.penv/lib/python3.11/site-packages (from tf-nightly) (0.5.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in ./.penv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tf-nightly) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in ./.penv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tf-nightly) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in ./.penv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tf-nightly) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in ./.penv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tf-nightly) (2025.4.26)\nRequirement already satisfied: markdown>=2.6.8 in ./.penv/lib/python3.11/site-packages (from tb-nightly~=2.19.0.a->tf-nightly) (3.8)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.penv/lib/python3.11/site-packages (from tb-nightly~=2.19.0.a->tf-nightly) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in ./.penv/lib/python3.11/site-packages (from tb-nightly~=2.19.0.a->tf-nightly) (3.1.3)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in ./.penv/lib/python3.11/site-packages (from astunparse>=1.6.0->tf-nightly) (0.42.0)\nRequirement already satisfied: rich in ./.penv/lib/python3.11/site-packages (from keras-nightly>=3.6.0.dev->tf-nightly) (14.0.0)\nRequirement already satisfied: namex in ./.penv/lib/python3.11/site-packages (from keras-nightly>=3.6.0.dev->tf-nightly) (0.1.0)\nRequirement already satisfied: optree in ./.penv/lib/python3.11/site-packages (from keras-nightly>=3.6.0.dev->tf-nightly) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in ./.penv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tb-nightly~=2.19.0.a->tf-nightly) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in ./.penv/lib/python3.11/site-packages (from rich->keras-nightly>=3.6.0.dev->tf-nightly) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.penv/lib/python3.11/site-packages (from rich->keras-nightly>=3.6.0.dev->tf-nightly) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in ./.penv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras-nightly>=3.6.0.dev->tf-nightly) (0.1.2)\nDownloading tf_nightly-2.20.0.dev20250602-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (604.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 604.1/604.1 MB 10.1 MB/s eta 0:00:00\nDownloading tb_nightly-2.19.0a20250218-py3-none-any.whl (5.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 40.8 MB/s eta 0:00:00\nDownloading keras_nightly-3.10.0.dev2025060203-py3-none-any.whl (1.4 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 19.0 MB/s eta 0:00:00\nInstalling collected packages: tb-nightly, keras-nightly, tf-nightly\nSuccessfully installed keras-nightly-3.10.0.dev2025060203 tb-nightly-2.19.0a20250218 tf-nightly-2.20.0.dev20250602\n\n\n$ cat ../output.log \nnohup: ignoring input\n2025-06-03 09:18:41.957407: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1748935132.836424 1732596 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14395 MB memory:  -> device: 0, name: NVIDIA A16, pci bus id: 0000:0a:00.0, compute capability: 8.6\nI0000 00:00:1748935132.838271 1732596 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14395 MB memory:  -> device: 1, name: NVIDIA A16, pci bus id: 0000:0c:00.0, compute capability: 8.6\n🔧 Počet GPU zařízení: 2\n\n🚀 Spouštím model.fit() ...\n\n🚀 Začíná epocha 1\nEpoch 1/5\nI0000 00:00:1748935199.869409 1732811 cuda_dnn.cc:529] Loaded cuDNN version 91001\nI0000 00:00:1748935200.089952 1732806 cuda_dnn.cc:529] Loaded cuDNN version 91001\nvolta:1732596:1733503 [0] NCCL INFO Bootstrap: Using bond0:192.168.22.46<0>\nvolta:1732596:1733503 [0] NCCL INFO cudaDriverVersion 12080\nvolta:1732596:1733503 [0] NCCL INFO NCCL version 2.25.1+cudaCUDA_MAJOR.CUDA_MINOR\nvolta:1732596:1733510 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal network plugin.\nvolta:1732596:1733510 [0] NCCL INFO NET/IB : No device found.\nvolta:1732596:1733510 [0] NCCL INFO NET/IB : Using [RO]; OOB bond0:192.168.22.46<0>\nvolta:1732596:1733510 [0] NCCL INFO NET/Socket : Using [0]bond0:192.168.22.46<0>\nvolta:1732596:1733510 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\nvolta:1732596:1733510 [0] NCCL INFO Using network Socket\nvolta:1732596:1733511 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.\nvolta:1732596:1733511 [1] NCCL INFO Using network Socket\nvolta:1732596:1733511 [1] NCCL INFO ncclCommInitRank comm 0x7b79200970f0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId c000 commId 0x3a43a27697915d59 - Init START\nvolta:1732596:1733510 [0] NCCL INFO ncclCommInitRank comm 0x7b792001cb90 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId a000 commId 0x3a43a27697915d59 - Init START\nvolta:1732596:1733510 [0] NCCL INFO RAS client listening socket at 127.0.0.1<28028>\nvolta:1732596:1733510 [0] NCCL INFO Bootstrap timings total 0.000977 (create 0.000032, send 0.000102, recv 0.000411, ring 0.000043, delay 0.000000)\nvolta:1732596:1733511 [1] NCCL INFO Bootstrap timings total 0.001047 (create 0.000044, send 0.000132, recv 0.000307, ring 0.000036, delay 0.000000)\nvolta:1732596:1733510 [0] NCCL INFO Setting affinity for GPU 0 to 0f,ff000fff\nvolta:1732596:1733511 [1] NCCL INFO Setting affinity for GPU 1 to 0f,ff000fff\nvolta:1732596:1733510 [0] NCCL INFO comm 0x7b792001cb90 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0\nvolta:1732596:1733511 [1] NCCL INFO comm 0x7b79200970f0 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0\nvolta:1732596:1733510 [0] NCCL INFO Channel 00/02 : 0 1\nvolta:1732596:1733510 [0] NCCL INFO Channel 01/02 : 0 1\nvolta:1732596:1733510 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\nvolta:1732596:1733511 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0\nvolta:1732596:1733511 [1] NCCL INFO P2P Chunksize set to 131072\nvolta:1732596:1733510 [0] NCCL INFO P2P Chunksize set to 131072\nvolta:1732596:1733510 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 1 directMode 1\nvolta:1732596:1733514 [1] NCCL INFO [Proxy Service] Device 1 CPU core 31\nvolta:1732596:1733513 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10\nvolta:1732596:1733515 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 25\nvolta:1732596:1733516 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 29\nvolta:1732596:1733510 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\nvolta:1732596:1733510 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\nvolta:1732596:1733511 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\nvolta:1732596:1733511 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\nvolta:1732596:1733510 [0] NCCL INFO CC Off, workFifoBytes 1048576\nvolta:1732596:1733510 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so libnccl-net.so. Using internal tuner plugin.\nvolta:1732596:1733510 [0] NCCL INFO ncclCommInitRank comm 0x7b792001cb90 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId a000 commId 0x3a43a27697915d59 - Init COMPLETE\nvolta:1732596:1733510 [0] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 2 total 0.32 (kernels 0.23, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.00, rest 0.00)\nvolta:1732596:1733511 [1] NCCL INFO ncclCommInitRank comm 0x7b79200970f0 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId c000 commId 0x3a43a27697915d59 - Init COMPLETE\nvolta:1732596:1733511 [1] NCCL INFO Init timings - ncclCommInitRank: rank 1 nranks 2 total 0.32 (kernels 0.23, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.03, graphs 0.00, connections 0.00, rest 0.00)\nvolta:1732596:1733518 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/direct pointer\nvolta:1732596:1733517 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/direct pointer\nvolta:1732596:1733518 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/direct pointer\nvolta:1732596:1733517 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/direct pointer\nvolta:1732596:1733518 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\nvolta:1732596:1733517 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1\n```"
      },
      {
        "user": "ldrahnik",
        "body": "@Venkat6871 Any idea how to debug what is wrong? After 24 hours of running the previous script with no progress I killed it. Thank you in advance!"
      }
    ]
  },
  {
    "issue_number": 94657,
    "title": "`tf.linalg.solve` behaves inconsistently on GPU for singular matrices depending on shape",
    "author": "rookieLiu2018",
    "state": "open",
    "created_at": "2025-06-02T13:22:38Z",
    "updated_at": "2025-06-05T08:28:18Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "comp:gpu",
      "2.17"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIn a Linux + GPU environment, calling tf.linalg.solve on singular matrices yields different results on GPU depending on the matrix shape:\nFor a 3×3 singular matrix, the CPU backend raises InvalidArgumentError, but the GPU backend returns an identity matrix without error.\nFor a 2×2 singular matrix, both CPU and GPU backends correctly raise InvalidArgumentError.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nfor name, matrix in [\n    (\"3×3 float64 singular matrix\", tf.constant([\n        [1.0, 2.0, 3.0],\n        [2.0, 5.0, 6.0],\n        [3.0, 6.0, 9.0]\n    ], dtype=tf.float64)),\n    (\"2×2 float64 singular matrix\", tf.constant([\n        [1.0, 2.0],\n        [2.0, 4.0]\n    ], dtype=tf.float64))\n]:\n    print(f\"\\n=== {name} ===\")\n\n    with tf.device('/CPU:0'):\n        try:\n            x_cpu = tf.linalg.solve(matrix, matrix)\n            print(\"CPU solve ->\\n\", x_cpu.numpy())\n        except Exception as e:\n            print(\"CPU solve raised:\", type(e).__name__, e)\n\n    with tf.device('/GPU:0'):\n        try:\n            x_gpu = tf.linalg.solve(matrix, matrix)\n            print(\"GPU solve ->\\n\", x_gpu.numpy())\n        except Exception as e:\n            print(\"GPU solve raised:\", type(e).__name__, e)\n```\n\n### Relevant log output\n\n```shell\n=== 3×3 float64 singular matrix ===\nCPU solve raised: InvalidArgumentError {{function_node __wrapped__MatrixSolve_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input matrix is not invertible. [Op:MatrixSolve] name: \nGPU solve ->\n [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n=== 2×2 float64 singular matrix ===\nCPU solve raised: InvalidArgumentError {{function_node __wrapped__MatrixSolve_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input matrix is not invertible. [Op:MatrixSolve] name: \nGPU solve raised: InvalidArgumentError {{function_node __wrapped__MatrixSolve_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input matrix is not invertible. [Op:MatrixSolve] name:\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the issue using TensorFlow version 2.19.0 and the nightly build. I have attached a [Gist](https://colab.sandbox.google.com/gist/Venkat6871/b950246eb60a701604197bd7d6134d09/94657_tf_2-19-0-nightly-v.ipynb) for your reference.\n\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94655,
    "title": "tf.linalg.slogdet returns incorrect values on GPU for singular matrix",
    "author": "rookieLiu2018",
    "state": "open",
    "created_at": "2025-06-02T13:05:59Z",
    "updated_at": "2025-06-05T07:55:34Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "comp:gpu",
      "2.17"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen calling `tf.linalg.slogdet` on a singular matrix, the CPU backend returns the mathematically correct result (sign=0, logabsdet=-∞). However, on GPU the same input produces, which contradicts the expected behavior for a singular matrix. According to documentation, for any singular matrix, logabsdet should be negative infinity and sign should be zero.\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\nsingular_np = np.array([\n    [1.0, 2.0, 3.0],\n    [2.0, 5.0, 6.0],\n    [3.0, 6.0, 9.0]\n], dtype=np.float32)\nmatrix = tf.constant(singular_np)\n\n# 1. On CPU:\nwith tf.device('/CPU:0'):\n    sign_cpu, logabs_cpu = tf.linalg.slogdet(matrix)\n    print(\"CPU slogdet -> sign =\", sign_cpu.numpy(), \", logabsdet =\", logabs_cpu.numpy())\n\n# 2. On GPU:\nwith tf.device('/GPU:0'):\n    sign_gpu, logabs_gpu = tf.linalg.slogdet(matrix)\n    print(\"GPU slogdet -> sign =\", sign_gpu.numpy(), \", logabsdet =\", logabs_gpu.numpy())\n```\n\n### Relevant log output\n\n```shell\nCPU slogdet -> sign = tf.Tensor(-0.0, shape=(), dtype=float32) , logabsdet = tf.Tensor(-inf, shape=(), dtype=float32)\nGPU slogdet -> sign = tf.Tensor(1.0, shape=(), dtype=float32) , logabsdet = tf.Tensor(-15.131454, shape=(), dtype=float32)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the issue using TensorFlow version 2.19.0 and the nightly build. I have attached a [Gist](https://colab.sandbox.google.com/gist/Venkat6871/744cab92d189637b7ca239a16ed39b65/94655_tf_2-19-0-nightly-v.ipynb) for your reference.\n\nThank you!"
      }
    ]
  },
  {
    "issue_number": 49858,
    "title": "tflite model produces different results if using the CPU on a Ubuntu PC or a NPU+NNAPI on an embedded system",
    "author": "FSet89",
    "state": "closed",
    "created_at": "2021-05-28T11:34:29Z",
    "updated_at": "2025-06-05T05:37:19Z",
    "labels": [
      "stat:awaiting response",
      "stale",
      "comp:lite",
      "TF 2.4"
    ],
    "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: no\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04 / Linux Yocto\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: 2.4.1\r\n-   **Python version**: 3.6\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI trained a classification network and I performed quantization-aware training using Tensorflow Model Optimization. Then I converted the trained and quantized model to tflite format.\r\nThe model is supposed to be used in an embedded system running Yocto. The system is provided with a NPU and the inference is performed with the [tflite_runtime](https://www.tensorflow.org/lite/guide/python) package and the NNAPI.\r\nThe problem is that the same tflite model produces different results if tested on my PC with TF2.4.1 and on the Yocto board. The result are often very similar but there are some cases where they are completely different, resulting in an accuracy drop on the board. I don't know why this happens. Is there a way to prevent it during training? \r\n\r\n\r\n",
    "comments": [
      {
        "user": "tilakrayal",
        "body": "@FSet89 ,\r\n\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled. \r\nAlso for analysing the issue can you please provide the stand alone code or share a colab gist with the issue reported.\r\n\r\nThanks!"
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Closing as stale. Please reopen if you'd like to work on this further.\n"
      }
    ]
  },
  {
    "issue_number": 73029,
    "title": "Proposal: Conditionally substitute boringSSL with system OpenSSL",
    "author": "npanpaliya",
    "state": "open",
    "created_at": "2024-08-02T15:16:19Z",
    "updated_at": "2025-06-05T02:42:10Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:feature",
      "type:build/install"
    ],
    "body": "Hello All,\r\n\r\nAs we know boringSSL doesn't support IBM Power architecture anymore, we are finding it difficult to build Tensorflow on Power with proper and updated boringSSL. Even if we just patch boringSSL to have Power support (by using older boringSSL or applying patch from previous commits that had Power support), it is difficult to maintain if boringSSL version gets updated with no equivalent changes for Power. \r\n\r\nOur Proposal: Use system installed OpenSSL instead of boringSSL based on a flag **only if** set. For e.g. USE_SYSTEM_OPENSSL\r\n\r\nTF brings in boringSSL dependency directly and indirectly (through curl and grpc) during bazel build process. So, we have tried some changes which does following -\r\n1. Use boringSSL and curl from the system (using TF_SYSTEM_LIBS)\r\n2. Patched grpc to fallback to system openSSL include and library paths during its build. \r\nNote: grpc already provides similar mechanism when grpcio's wheel is being built through [setup.py](https://github.com/grpc/grpc/blob/master/setup.py#L301). But this mechanism doesn't work when we build grpc through bazel.\r\n\r\nThese changes have enabled us to build TF on Power, and our testing also didn't give any issues so far (some of the bazel tests and tensorflow/models). \r\n\r\nWe want your opinion on this approach. Also, kindly let us know what all tests we should do to have better coverage of this replacement.\r\n\r\nThanks in advance. ",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "@belitskiy , @learning-to-play ."
      },
      {
        "user": "npanpaliya",
        "body": "Hello @Venkat6871 - Could you please let us know your opinion on this? Also, kindly help me tag the main Community members and get their views too on this?"
      },
      {
        "user": "dvarrazzo",
        "body": "Related issue: #93969 "
      }
    ]
  },
  {
    "issue_number": 93969,
    "title": "Psycopg crashes using OpenSSL if tensorflow is imported beforehand",
    "author": "dvarrazzo",
    "state": "open",
    "created_at": "2025-05-22T23:03:56Z",
    "updated_at": "2025-06-05T02:27:32Z",
    "labels": [
      "type:bug",
      "subtype: ubuntu/linux",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIssue reported to the Psycopg project in https://github.com/psycopg/psycopg/issues/1097 and https://github.com/psycopg/psycopg2/issues/1784 by @OlivierVerhaegen. Below the OP description and repro.\n\nAfter a log search and trial and error I found the following bug regarding the usage of psycopg in combination with Tensorflow.\nEvery time the application would have a segmentation error when starting up the SSL connection.\n\n![Image](https://github.com/user-attachments/assets/f01f6169-a881-4ede-9f57-6f771127b480)\n\nWorkaround was to import psycopg before tensorflow.\n**FYI: I tried all packages of psycopg (binary, c)**\n\n![Image](https://github.com/user-attachments/assets/faf26415-5ddc-45a6-b6d2-c5892f078942)\n\nResult afterwards:\n\n![Image](https://github.com/user-attachments/assets/d4349c1b-d15e-495a-bd20-13ac4bbc1425)\n\nHopefully this post helps other people to avoid this \n\n### Standalone code to reproduce the issue\n\n**Dockerfile**\n```dockerfile\nFROM python:3.12-slim\n\nRUN apt update && \\\n    apt install -y libpq-dev gcc python3-dev\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY main.py .\n\nENTRYPOINT [ \"python\", \"-u\", \"main.py\" ]\n```\n\n**requirements.txt**\n```txt\npandas==2.2.3\nnumpy==2.0.2\nscikit-learn==1.5.2\ntensorflow-cpu==2.19.0\npsycopg[binary]\nconfluent-kafka==2.10.0\n```\n\n**main.py**\n```python\n#Importing libraries\nimport psycopg\nimport tensorflow as tf\nimport logging\nimport json\nimport joblib\nimport os\nimport sys\nimport time\nlogging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s %(levelname)s %(message)s\")\nlogging.getLogger(\"psycopg\").setLevel(logging.DEBUG)\nprint(\"importing libraries done\")\n\n#Getting environment variables for database connection\ndbname=os.getenv(\"DB_NAME\")\nuser=os.getenv(\"DB_USER\")\npassword=os.getenv(\"DB_PWD\")\nhost=os.getenv(\"DB_HOST\")\nport=os.getenv(\"DB_PORT\")\nenv_vars = {\n    \"dbname\": dbname,\n    \"user\": user,\n    \"password\": password,\n    \"host\": host,\n    \"port\": port,\n    \"sslmode\": \"require\",\n}\n#Checking if all environment variables were given\nprint(\"Checking environment variables...\")\nempty = 0\nfor name, value in env_vars.items():\n    if value == None:\n        print(f'No {name} was given for database connection')\n        empty = empty +1\n\nif not empty == 0:\n    time.sleep(5)\n    sys.exit(\"Exiting program...\")\n\ntry:\n    print(\"Connecting to database...\")\n    # Establishing a connection to the database\n    connection = psycopg.connect(**env_vars)\n    print(\"Connected to database\")\n    # Creating a cursor object to interact with the database\n    cursor = connection.cursor()\n    # Performing database operations here...\nexcept (Exception, psycopg.Error) as error:\n    error_str = f\"Error connecting to the database: {error}\"\n    print(error_str)\n    time.sleep(5)\n    sys.exit(error_str)\n```",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "Since both libraries use C++ objects, this is likely caused by some ODR violation."
      },
      {
        "user": "dvarrazzo",
        "body": "Psycopg and libpq code doesn't use C++ objects, only C."
      },
      {
        "user": "mihaimaruseac",
        "body": "Yeah, but both depend on OpenSSL, which is a culprit here. Quoting from the other bug:\n\n> What comes to mind is that tensorflow comes with an incompatible openssl version.\n\nTF should update this version, but this require some work."
      }
    ]
  },
  {
    "issue_number": 93452,
    "title": "ExportArchive can not deal with StaticHashTable",
    "author": "PowerToThePeople111",
    "state": "closed",
    "created_at": "2025-05-15T14:21:03Z",
    "updated_at": "2025-06-05T02:13:29Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:keras",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17 (for my env) and 2.18 (colab minimal example)\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI try to apply thresholds on model outputs by using a StaticHashTable. Since I also use multiple custom layers and I want to package them for tf-serve i decided to go with keras.export.ExportArchive() as described in the **docs** [here](https://keras.io/api/models/model_saving_apis/export/). \nSince ExportArchive use saved_model format, it allows me to completely package all custom code with the model plus having a way to pre-process inputs and post-process outputs of the model (applying the thresholds).\n\nThis is code that minimally extends the example (so it can run actually) from the **docs** which I referenced above:\n```\nimport tensorflow as tf\nimport keras\n\nthreshold_keys = tf.constant([\"a\", \"b\"], dtype=tf.string, name=\"threshold_keys\")\nthreshold_values = tf.constant([0.5, 0.7], dtype=tf.float32, name=\"threshold_values\")\ninitializer = tf.lookup.KeyValueTensorInitializer(threshold_keys, threshold_values)\nlookup_table = tf.lookup.StaticHashTable(initializer, default_value=0.0)\n\nmodel = keras.layers.Dense(1)\nmodel(tf.constant([[0.5]]))\n\nexport_archive = keras.export.ExportArchive()\nmodel_fn = export_archive.track_and_add_endpoint(\n    \"model_fn\",\n    model, \n    input_signature=[tf.TensorSpec(shape=(None, 1), dtype=tf.float32)]\n)\n\nexport_archive.track(lookup_table)\n\n@tf.function()\ndef serving_fn(x):\n    x = lookup_table.lookup(x)\n    return model_fn(x)\n\nx = tf.constant([[\"a\"]])\nserving_fn(x)\nexport_archive.add_endpoint(name=\"serve\", fn=serving_fn)\n\nexport_archive.write_out(\"larifari\") #<--- here i get the exception\n\n```\n\nYou can also find that code at the bottom of the shared [colab notebook](https://colab.research.google.com/drive/1EpYpn6EhvY4PyKX65xrPd_QZVo3Dw2uO#scrollTo=rxp3GhfJRULi).\nThe model I built can successfully do predictions but when I try to save it, i get the error that some tensor can not be properly tracked:\n\n```\nAssertionError: Tried to export a function which references an 'untracked' resource. TensorFlow objects (e.g. tf.Variable) captured by functions must be 'tracked' by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly. See the information below:\nFunction name = b'__inference_signature_wrapper_294'\nCaptured Tensor = <ResourceHandle(name=\"101\", device=\"/job:localhost/replica:0/task:0/device:CPU:0\", container=\"localhost\", type=\"tensorflow::lookup::LookupInterface\", dtype and shapes : \"[  ]\")>\nTrackable referencing this tensor = <tensorflow.python.ops.lookup_ops.StaticHashTable object at 0x79e857dbcf10>\nInternal Tensor = Tensor(\"284:0\", shape=(), dtype=resource)\n```\nIt seems that the tensor which is not tracked is referenced by the StaticHashTable and probably is the \"resource_handle\" of that class. What i also tried:\n\n- having the StaticHashTable being constructed in the init method of the wrapper model and setting it as a class property\n- also exposing the resource_handle of the StaticHashTable and tracking it explicitly and setting it as a class property\n- try to use self._track_trackable to force tracking\n\nCan you tell me how to export this model for tf-serve usage with custom code and a function for pre- and post-processing that uses a StaticHashTable? \n\nI tried to solve this for 2 days and I am stuck.\n\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps://colab.research.google.com/drive/1EpYpn6EhvY4PyKX65xrPd_QZVo3Dw2uO#scrollTo=rxp3GhfJRULi\n```\n\n### Relevant log output\n\n```shell\nAssertionError: Tried to export a function which references an 'untracked' resource. TensorFlow objects (e.g. tf.Variable) captured by functions must be 'tracked' by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly. See the information below:\nFunction name = b'__inference_signature_wrapper_294'\nCaptured Tensor = <ResourceHandle(name=\"101\", device=\"/job:localhost/replica:0/task:0/device:CPU:0\", container=\"localhost\", type=\"tensorflow::lookup::LookupInterface\", dtype and shapes : \"[  ]\")>\nTrackable referencing this tensor = <tensorflow.python.ops.lookup_ops.StaticHashTable object at 0x79e857dbcf10>\nInternal Tensor = Tensor(\"284:0\", shape=(), dtype=resource)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @PowerToThePeople111 ,\nApologies for the delay, and thank you for raising your concern. I tested your code on Colab using TensorFlow versions 2.18 and 2.19, and I was able to reproduce the same issue.\nAdditionally, I noticed that the Colab links you shared currently do not grant access. Could you please update the sharing settings to allow access? This will help in debugging the issue more effectively.\n\nIn the meantime, I explored an alternative approach that worked for me. I am sharing the [Gist](https://colab.sandbox.google.com/gist/Venkat6871/da3463509000d857ef3ad9f00d070441/93452_tf_2-18-0-2-19-0-v.ipynb) here for your reference. I hope it proves helpful.\nAs the issue appears to be more related to Keras, I recommend posting it on the [keras-team/keras repo.](https://github.com/keras-team/keras/issues) for more targeted assistance.\nAlso, I found a similar issue reported elsewhere. I am attaching that [link](https://github.com/tensorflow/tensorflow/issues/61347) as well, it might offer some insights or potential solutions.\n\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 93487,
    "title": "Segmentation fault in tf.queue.RandomShuffleQueue",
    "author": "cx104906",
    "state": "closed",
    "created_at": "2025-05-16T01:57:43Z",
    "updated_at": "2025-06-05T02:13:27Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:core",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 18.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n6.5.0\n\n### GCC/compiler version\n\nclang 18.1.8\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSegfault when using tf.queue.RandomShuffleQueue\n\n### Standalone code to reproduce the issue\n\n```shell\nimport pickle\nimport tensorflow as tf\nimport time\nimport pprint\nimport numpy as np\n\nprint(tf.__version__)\n\nmylist = [2395835828703514.5,3746101199,True,(124102980877048145498407209769266638155,1262486112454788584176297869777513545)]\nmydict = {}\npprint.pprint(mylist)\npprint.pprint(mydict)\n\nfor i in range(1000):\n  try:\n      print (f\"{i}\",flush=True)\n      tf.queue.RandomShuffleQueue(*mylist,**mydict)\n  except Exception as e:\n      print(f\"{e}\")\n\nprint(\"done\")\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @cx104906 ,\nApologies for the delay, and thank you for raising this concern. The issue appears to be caused by passing extremely large or invalid values to `tf.queue.RandomShuffleQueue`, which may result in out-of-range behavior and even lead to a segmentation fault. I tested the same code with smaller, valid values and it worked as expected. For your reference, I have attached a working example in this [gist](https://colab.sandbox.google.com/gist/Venkat6871/e03e85ea25ab4130b0352bb0f5aff602/93487_tf_2-18-0-2-19-0-nightly-v.ipynb).\nPlease let me know if you have further questions. \nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 89272,
    "title": "TensorFlow on RTX 5090",
    "author": "maludwig",
    "state": "open",
    "created_at": "2025-03-15T03:40:16Z",
    "updated_at": "2025-06-04T21:41:07Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "wsl2",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0.dev20250314\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nWindows 11 - WSL2 - Ubuntu 22.04.5 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12\n\n### Bazel version\n\n7.4.1\n\n### GCC/compiler version\n\ngcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n\n### CUDA/cuDNN version\n\nCUDA Version: 12.8\n\n### GPU model and memory\n\nRTX 5090 32GB\n\n### Current behavior?\n\nI had hoped that tensorflow would work on the RTX 5090 at all. It does not, sadly. I tried building from source but that didn't work either. I tried running the environment script but that didn't work either. At least bash is my primary programming language, so I was able to tidy that one up here:\n\nhttps://github.com/tensorflow/tensorflow/pull/89271\n\nBut I wasn't able to get tensorflow running. I had a similar issue with PyTorch, which needed to use CUDA 12.8.* to work on the Blackwell cards, but no dice with the nightly build of tensorflow. Below is my test and the output, and under that is the `tf_env.txt` from my patched script.\n\nIt may be helpful to know that nvidia themselves seem to have it running here:\n\nhttps://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel-25-02.html\n\nBut I get the same errors that this other guy does when I try it out:\n\nhttps://www.reddit.com/r/tensorflow/comments/1iutjoj/tensorflow_2501_cuda_128_rtx_5090_on_wsl2_cuda/\n\nThis conversation was another one I found that may be helpful, according to these guys, you need to support CUDA 12.8.1 to support Blackwell (aka the RTX 50## series cards):\n\nhttps://discuss.ai.google.dev/t/building-tensorflow-from-source-for-rtx5000-gpu-series/65171/15\n\n\n```\n\n(tfnightie) mitch@win11ml:~/stable_diff\n$ cat tfnightie/test_2.py\nimport tensorflow as tf\nimport time\n\n# Check if TensorFlow sees the GPU\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Available GPUs:\", tf.config.experimental.list_physical_devices('GPU'))\n\n# Matrix multiplication test\nshape = (5000, 5000)\na = tf.random.normal(shape)\nb = tf.random.normal(shape)\n\n# Time execution on GPU\nwith tf.device('/GPU:0'):\n    print(\"Running on GPU...\")\n    start_time = time.time()\n    c = tf.matmul(a, b)\n    tf.print(\"Matrix multiplication (GPU) done.\")\n    print(\"Execution time (GPU):\", time.time() - start_time, \"seconds\")\n\n# Time execution on CPU for comparison\nwith tf.device('/CPU:0'):\n    print(\"Running on CPU...\")\n    start_time = time.time()\n    c = tf.matmul(a, b)\n    tf.print(\"Matrix multiplication (CPU) done.\")\n    print(\"Execution time (CPU):\", time.time() - start_time, \"seconds\")\n\n\n\n\n(tfnightie) mitch@win11ml:~/stable_diff\n$ python tfnightie/test_2.py\n2025-03-14 21:35:33.400099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nTensorFlow version: 2.20.0-dev20250314\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nW0000 00:00:1742009735.413544  326199 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\nAvailable GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nW0000 00:00:1742009735.417720  326199 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\nI0000 00:00:1742009735.572153  326199 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29043 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:09:00.0, compute capability: 12.0\n2025-03-14 21:35:36.969440: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX'\n\n2025-03-14 21:35:36.969480: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n\n2025-03-14 21:35:36.969505: W tensorflow/core/framework/op_kernel.cc:1843] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n2025-03-14 21:35:36.969533: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\nTraceback (most recent call last):\n  File \"/home/mitch/stable_diff/tfnightie/test_2.py\", line 10, in <module>\n    a = tf.random.normal(shape)\n  File \"/home/mitch/.virtualenvs/tfnightie/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/mitch/.virtualenvs/tfnightie/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 6027, in raise_from_not_ok_status\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\ntensorflow.python.framework.errors_impl.InternalError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Mul] name:\n```\n\nAlso, while nvidia's site says that the Compute Capability of the RTX5090 is \"10.0\", the card itself seems to report \"12.0\". I am not so sure that info will be helpful, but it spun me for a loop:\n\n```\n\n$ cat <<EOF > card_details.cu\n> #include <cuda_runtime.h>\n#include <iostream>\n\nint main() {\n    cudaDeviceProp prop;\n    int device;\n\n    cudaGetDevice(&device); // Get the current device ID\n    cudaGetDeviceProperties(&prop, device); // Get device properties\n\n    size_t free_mem, total_mem;\n    cudaMemGetInfo(&free_mem, &total_mem); // Get VRAM usage\n\n    std::cout << \"GPU Name: \" << prop.name << std::endl;\n    std::cout << \"Compute Capability: \" << prop.major << \".\" << prop.minor << std::endl;\n    std::cout << \"VRAM Usage: \" << (total_mem - free_mem) / (1024 * 1024) << \" MB / \" << total_mem / (1024 * 1024) << \" MB\" << std::endl;\n\n    return 0;\n}\nEOF\n\n\n\n$ nvcc card_details.cu -o card_details && ./card_details\nnvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\nGPU Name: NVIDIA GeForce RTX 5090\nCompute Capability: 12.0\nVRAM Usage: 1763 MB / 32606 MB\n```\n\n\n# tf_env.txt\n```\n\n== check python ====================================================\npython version: 3.10.12\npython branch:\npython build version: ('main', 'Feb  4 2025 14:57:36')\npython compiler version: GCC 11.4.0\npython implementation: CPython\n\n\n== check os platform ===============================================\nos: Linux\nos kernel version: #1 SMP Tue Nov 5 00:21:55 UTC 2024\nos release version: 5.15.167.4-microsoft-standard-WSL2\nos platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\nfreedesktop os release: {'NAME': 'Ubuntu', 'ID': 'ubuntu', 'PRETTY_NAME': 'Ubuntu 22.04.5 LTS', 'VERSION_ID': '22.04', 'VERSION': '22.04.5 LTS (Jammy Jellyfish)', 'VERSION_CODENAME': 'jammy', 'ID_LIKE': 'debian', 'HOME_URL': 'https://www.ubuntu.com/', 'SUPPORT_URL': 'https://help.ubuntu.com/', 'BUG_REPORT_URL': 'https://bugs.launchpad.net/ubuntu/', 'PRIVACY_POLICY_URL': 'https://www.ubuntu.com/legal/terms-and-policies/privacy-policy', 'UBUNTU_CODENAME': 'jammy'}\nmac version: ('', ('', '', ''), '')\nuname: uname_result(system='Linux', node='win11ml', release='5.15.167.4-microsoft-standard-WSL2', version='#1 SMP Tue Nov 5 00:21:55 UTC 2024', machine='x86_64')\narchitecture: ('64bit', 'ELF')\nmachine: x86_64\n\n== are we in docker ================================================\nNo\n\n== c++ compiler ====================================================\n/usr/bin/c++\nc++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nCopyright (C) 2021 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== check pips ======================================================\nnumpy                   2.1.3\nprotobuf                5.29.3\ntf_nightly              2.20.0.dev20250314\n\n== check for virtualenv ============================================\nRunning inside a virtual environment.\n\n== tensorflow import ===============================================\n2025-03-14 21:02:48.002965: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nW0000 00:00:1742007769.198398  317963 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\nW0000 00:00:1742007769.202246  317963 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\nI0000 00:00:1742007769.355021  317963 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29043 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:09:00.0, compute capability: 12.0\n\ntf.version.VERSION = 2.20.0-dev20250314\ntf.version.GIT_VERSION = v1.12.1-123444-g07ff428d432\ntf.version.COMPILER_VERSION = Ubuntu Clang 18.1.8 (++20240731024944+3b5b5c1ec4a3-1~exp1~20240731145000.144)\n\nSanity check: <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>\nlibcudnn not found\n\n== env =============================================================\nLD_LIBRARY_PATH /usr/local/cuda-12.8/lib64:\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ======================================================\nFri Mar 14 21:02:52 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.124.06             Driver Version: 572.70         CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 5090        On  |   00000000:09:00.0 Off |                  N/A |\n|  0%   43C    P1             78W /  600W |    2115MiB /  32607MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A              31      G   /Xwayland                             N/A      |\n|    0   N/A  N/A              35      G   /Xwayland                             N/A      |\n+-----------------------------------------------------------------------------------------+\n\n== cuda libs =======================================================\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart_static.a\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart.so.11.8.89\n/usr/local/cuda-12.8/targets/x86_64-linux/lib/libcudart_static.a\n/usr/local/cuda-12.8/targets/x86_64-linux/lib/libcudart.so.12.8.90\n\n== tensorflow installation =========================================\ntensorflow not found\n\n== tf_nightly installation =========================================\nName: tf_nightly\nVersion: 2.20.0.dev20250314\nSummary: TensorFlow is an open source machine learning framework for everyone.\nHome-page: https://www.tensorflow.org/\nAuthor-email: packages@tensorflow.org\nLicense: Apache 2.0\nLocation: /home/mitch/.virtualenvs/tfnightie/lib/python3.10/site-packages\nRequired-by:\n\n== python version ==================================================\n(major, minor, micro, releaselevel, serial)\n(3, 10, 12, 'final', 0)\n\n== bazel version ===================================================\nBazelisk version: v1.25.0\nBuild label: 7.4.1\nBuild time: Mon Nov 11 21:24:53 2024 (1731360293)\nBuild timestamp: 1731360293\nBuild timestamp as int: 1731360293\n```\n\n### Standalone code to reproduce the issue\n\n```shell\nTry running anything with an RTX 5090. My test script is above.\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "huiyijiangling",
        "body": "same problem"
      },
      {
        "user": "maludwig",
        "body": "I should mention that I'm a Senior AI Developer by trade and I'm more than happy to invest my personal time in helping to fix this, I'm just not sure where to start."
      },
      {
        "user": "maludwig",
        "body": "I should also mention that the latest clang release here supports building for compute_100/sm_100+\n\nhttps://github.com/llvm/llvm-project/releases/tag/llvmorg-20.1.0\n\nIt's not supported in LLVM 18. But it compiles this on my GPU just fine (extra logs attached just in case they help someone else).\n\n```\n\nmitch@win11ml:~/stable_diff/build_tf/hello/hello_nvcc\n$ clang++ --version\nclang version 20.1.0 (https://github.com/llvm/llvm-project 24a30daaa559829ad079f2ff7f73eb4e18095f88)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /home/mitch/stable_diff/fix_tf/llvm/LLVM-20.1.0-Linux-X64/bin\n\n\n\n\nmitch@win11ml:~/stable_diff/build_tf/hello/hello_nvcc\n$ cat card_details.cu\n#include <cuda_runtime.h>\n#include <iostream>\n\nint main() {\n    cudaDeviceProp prop;\n    int device;\n\n    cudaGetDevice(&device); // Get the current device ID\n    cudaGetDeviceProperties(&prop, device); // Get device properties\n\n    size_t free_mem, total_mem;\n    cudaMemGetInfo(&free_mem, &total_mem); // Get VRAM usage\n\n    std::cout << \"GPU Name: \" << prop.name << std::endl;\n    std::cout << \"Compute Capability: \" << prop.major << \".\" << prop.minor << std::endl;\n    std::cout << \"VRAM Usage: \" << (total_mem - free_mem) / (1024 * 1024) << \" MB / \" << total_mem / (1024 * 1024) << \" MB\" << std::endl;\n\n    return 0;\n}\n\n\n\n\nmitch@win11ml:~/stable_diff/build_tf/hello/hello_nvcc\n$ clang++ -std=c++17 --cuda-gpu-arch=sm_120 -x cuda --cuda-path=\"$CUDA_HOME\" -I\"$CUDA_HOME/include\" -L\"$CUDA_HOME/lib64\"  -lcudart card_details.cu -o card_details\nclang++: warning: CUDA version 12.8 is only partially supported [-Wunknown-cuda-version]\n\n\n\n\nmitch@win11ml:~/stable_diff/build_tf/hello/hello_nvcc\n$ ./card_details\nGPU Name: NVIDIA GeForce RTX 5090\nCompute Capability: 12.0\nVRAM Usage: 1763 MB / 32606 MB\n\n\n\n\nmitch@win11ml:~/stable_diff/build_tf/hello/hello_nvcc\n$ echo \"$CUDA_HOME\"\n/usr/local/cuda-12.8\n\n\n\n\nmitch@win11ml:~/stable_diff/build_tf/hello/hello_nvcc\n$ ls \"$CUDA_HOME\"\nDOCS  EULA.txt  README  bin  compute-sanitizer  doc  extras  gds  include  lib64  libnvvp  nsightee_plugins  nvml  nvvm  share  src  targets  tools  version.json\n\n\n\n\nmitch@win11ml:~/stable_diff/build_tf/hello/hello_nvcc\n$ cat /usr/local/cuda-12.8/version.json | head -n5\n{\n   \"cuda\" : {\n      \"name\" : \"CUDA SDK\",\n      \"version\" : \"12.8.1\"\n   },\n```"
      }
    ]
  },
  {
    "issue_number": 94333,
    "title": "Scale/zero-point restrictions on the input and output of quantized operators",
    "author": "wangw-1991",
    "state": "open",
    "created_at": "2025-05-28T01:35:18Z",
    "updated_at": "2025-06-04T15:23:48Z",
    "labels": [
      "stat:contribution welcome",
      "type:support",
      "comp:lite"
    ],
    "body": "Some operators (like transpose and resize_bilinear) have the restriction \"Input and outputs must all have same scale/zero_point\" in  [int8 quantized operator specifications](https://ai.google.dev/edge/litert/models/quantization_spec#int8_quantized_operator_specifications). However, this is not checked in [TFLite Kernel codes](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/transpose.cc;l=71;drc=70343319512a4883095fdb100796ea70fba34b9e).\n\nI suppose this is the requirement for a valid quantized operator to get right results and TFLite should check it. Can anyone clarify this? Thanks.",
    "comments": [
      {
        "user": "bandirevanth",
        "body": "The scale and zero_point equality requirement for certain quantized operators is a valid and critical constraint to ensure correct results. While TFLite operator kernels may not currently enforce this at runtime, it is best practice for model creators to guarantee compliance. Adding explicit checks within TFLite kernels would improve safety but might affect performance."
      },
      {
        "user": "mihaimaruseac",
        "body": "Happy to review PRs adding the check, assuming performance would not be severely impacted."
      }
    ]
  },
  {
    "issue_number": 94442,
    "title": "`tf.math.cumprod` on `complex128` with `Inf` produces incorrect result on both CPU and GPU",
    "author": "rookieLiu2018",
    "state": "open",
    "created_at": "2025-05-29T13:08:31Z",
    "updated_at": "2025-06-04T08:51:11Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "2.17"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen computing a cumulative product of a `complex128` tensor containing `Inf`, TensorFlow’s `tf.math.cumprod` produces the wrong first element—returning `NaN+infj` instead of `1+infj`—on both CPU and GPU. NumPy’s `np.cumprod` yields the mathematically correct result.\n\n\n### Standalone code to reproduce the issue\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\nvals = [complex(1, np.inf), complex(2, 3)]\nout_np = np.cumprod(vals)\nprint(\"NumPy cumprod result:\", out_np)\nwith tf.device('/CPU:0'):\n  x_cpu = tf.constant(vals, dtype=tf.complex128)\n  out_cpu = tf.math.cumprod(x_cpu, axis=0)\n  print(out_cpu)\n\nwith tf.device('/GPU:0'):\n  vals = [complex(1, np.inf), complex(2, 3)]\n  x_gpu = tf.constant(vals, dtype=tf.complex128)\n  out_gpu = tf.math.cumprod(x_gpu, axis=0)\n  print(out_gpu)\n```\n\n### Relevant log output\n\n```shell\nNumPy cumprod result: [  1.+infj -inf+infj]\ntf.Tensor([ nan+infj -inf+infj], shape=(2,), dtype=complex128)\ntf.Tensor([ nan+infj -inf+infj], shape=(2,), dtype=complex128)\n```",
    "comments": [
      {
        "user": "bandirevanth",
        "body": "### Workaround\n\n1. **Scan the tensor** for values where either the real or imaginary part is `Inf`.\n2. **Replace `Inf` values** with very large, but finite, numbers (e.g., `1e10 + 1e10j`).\n3. **Run `tf.math.cumprod`** on the modified tensor.\n4. **(Optional)** Re-map the results if your use case requires symbolic accuracy with `Inf`.\n\nThis avoids `NaN` propagation while yielding results that are more consistent with expected mathematical behavior.\n\n---\n\n### ⚠️ Caveat\n\nThis is a **numerical approximation**. If your application requires precise handling of infinities (e.g., in physical simulations or symbolic math), treat this workaround as a patch—not a solution. Accuracy may slightly deviate from theoretical values due to the substitution of finite values."
      },
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow version 2.19.0 and the nightly build on both CPU and GPU.\nI am providing a [gist](https://colab.sandbox.google.com/gist/Venkat6871/f2da589772096300904f06d1081a31b6/94442_tf_2-19-0-nightly-v.ipynb) here for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 90291,
    "title": "TensorFlow with CUDA: RTX 5xxx series isn't supported (CUDA_ERROR_INVALID_HANDLE)",
    "author": "weyn9q",
    "state": "open",
    "created_at": "2025-03-31T05:26:22Z",
    "updated_at": "2025-06-04T06:51:54Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:feature",
      "type:build/install",
      "wsl2",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nWSL2\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n12.8\n\n### GPU model and memory\n\n5070TI\n\n### Current behavior?\n\nhi guys. i upgraded my GPU to 5070ti and found that tensorflow is still not working with new nvidia cards\n\nhonestly i knew that can be problems. but its been 3 month since nvidia released new gpu, i thought now should be no prob already.\n\nany idea how long it will take until at least there will be a nightly version which will fix the problem?\nlike 1-2 weeks or half a year ? :)\n\nor maybe someone found an easy way to fix it?\n\n\n\n\n\n\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential([\n    tf.keras.Input(shape=(20,)),\n    tf.keras.layers.Dense(10),\n    tf.keras.layers.Dense(1)\n])\n```\n\n### Relevant log output\n\n```shell\n2025-03-31 14:23:13.091967: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-03-31 14:23:13.734399: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nW0000 00:00:1743398598.268271   13190 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\nW0000 00:00:1743398598.270049   13190 gpu_device.cc:2429] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\nI0000 00:00:1743398598.408238   13190 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13123 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5070 Ti, pci bus id: 0000:01:00.0, compute capability: 12.0\n2025-03-31 14:23:18.851409: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX'\n\n2025-03-31 14:23:18.851466: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n\n2025-03-31 14:23:18.851502: W tensorflow/core/framework/op_kernel.cc:1843] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n2025-03-31 14:23:18.851526: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\nTraceback (most recent call last):\n  File \"/mnt/c/Users/alex/desktop/test111.py\", line 3, in <module>\n    model = tf.keras.Sequential([\n  File \"/home/alexflame/.local/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 76, in __init__\n    self._maybe_rebuild()\n  File \"/home/alexflame/.local/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 149, in _maybe_rebuild\n    self.build(input_shape)\n  File \"/home/alexflame/.local/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 229, in build_wrapper\n    original_build_method(*args, **kwargs)\n  File \"/home/alexflame/.local/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 195, in build\n    x = layer(x)\n  File \"/home/alexflame/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/alexflame/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py\", line 152, in convert_to_tensor\n    return tf.cast(x, dtype)\ntensorflow.python.framework.errors_impl.InternalError: {{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:Cast] name:\n```",
    "comments": [
      {
        "user": "weyn9q",
        "body": "spend whole day to build custom tensorflow build but didnt succeed.\n\nnow finally found how to solve this problem. not perfect solution as it could be, but still better then nothing :)\n\nhttps://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel-25-02.html#\n\n\n\n"
      },
      {
        "user": "weyn9q",
        "body": "ohh seems its impossible. thinking about move to pytorch. i dont want but :(\n\nbut i really stuck here with this problems and i dont think will be any updates here soon. \ni cant run any of my codes with new GPU.\nnvidia container works but its soo complicated. docker still not fully optimizated with many mistakes. cant work good.\n\nexample - i have my own pretrained model which i am using to train RL agent. lets say i have 2 code.\nfirst one is for train model ( later - pretrained)\nsecond is use pretrained get embeddings train RL agent.\n\nwithout nvidia docker i just cant run any of them coz of CUDA errors.\nwith docker first code can run if will fix some mistakes which happens only if use docker. but.\nsecond code i cant deserialize custom layers from pretrained idk why. coz of docker i think. probably because they were builded with another drivers.\n\ni decided to check if i will train model in docker again and will try to use it in second code will it work or no. it is not working.\ni still cant deserialize custom layers even with newly trained model. i am giving up.\n\nbefore on 4xxx videocard everything was perfect. now idk what to do"
      },
      {
        "user": "AndrewTG5",
        "body": "Same issue same scenario here. Tried updating every driver I could think of. Hoping for at least a nightly version soon so I can work again."
      }
    ]
  },
  {
    "issue_number": 72282,
    "title": "CVE-2021-35958 vulnerability in the latest v2.17",
    "author": "OlgasAcc",
    "state": "closed",
    "created_at": "2024-07-22T08:49:34Z",
    "updated_at": "2025-06-04T02:14:08Z",
    "labels": [
      "stat:awaiting response",
      "stale",
      "type:others",
      "comp:core",
      "2.17"
    ],
    "body": "### Issue type\n\nOthers\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nlinux amd64, Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOur security scans are complaining on this Tensorflow vulnerability since we started use it (from v2.13): https://www.cvedetails.com/cve/CVE-2021-35958/\r\nIs there any ETA of fixing it?\r\nThanks\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps://www.cvedetails.com/cve/CVE-2021-35958/\n```\n\n\n### Relevant log output\n\n_No response_",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "> NOTE: the vendor's position is that tf.keras.utils.get_file is not intended for untrusted archives\r\n\r\nThis was a vulnerability submitted by a researcher after the team disagreed with the assessment. Please don't just go for zero scan results, instead analyze the reports.\r\n\r\nSince the TF team does not consider this a vulnerability, this could get closed."
      },
      {
        "user": "WilliamRoyNelson",
        "body": "Old issue, but I fixed the security vulnerability for Python 3.14, but because of the release schedules, I think it'll be another year before Tensorflow supports 3.14\nhttps://github.com/python/cpython/pull/122002"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 92924,
    "title": "[iOS 18] EXC_BAD_ACCESS when initializing TFLite model on device running iOS 18.4.1",
    "author": "Vouchnim",
    "state": "closed",
    "created_at": "2025-05-08T02:55:54Z",
    "updated_at": "2025-06-04T02:14:02Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:lite",
      "TF 2.15"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\niOS version: 18.4.1 (crashes)\n\n### Mobile device\n\n[e.g., iPhone 13 Pro]\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n### System Information\n\n- TensorFlow Lite version: [e.g., 2.15.0]\n- iOS version: 18.4.1 (crashes)\n- Works on iOS version: 17.4.1 (no crash)\n- Device: [e.g., iPhone 13 Pro]\n- Xcode version: [e.g., 15.3]\n- Swift version: [e.g., 5.9]\n- Model type: .tflite (converted from [e.g., Keras model])\n\n### Describe the current behavior\nWhen running my app on iOS 18.4.1, it crashes with EXC_BAD_ACCESS at the point where I initialize the TFLite interpreter. The same model and code work on iOS 17.x.\n\n### Crash Log\nThread 1: EXC_BAD_ACCESS (code=2, address=0x...)\n\n### Code Snippet\n\n```swift\nguard let model = Model(filePath: modelPath) else {\n    throw InterpreterError.failedToLoadModel\n}\ntry self.init(model: model, options: options, delegates: delegates)\n\n\n### Standalone code to reproduce the issue\n\n```shell\n### System Information\n\n- TensorFlow Lite version: [e.g., 2.15.0]\n- iOS version: 18.4.1 (crashes)\n- Works on iOS version: 17.4.1 (no crash)\n- Device: [e.g., iPhone 13 Pro]\n- Xcode version: [e.g., 15.3]\n- Swift version: [e.g., 5.9]\n- Model type: .tflite (converted from [e.g., Keras model])\n\n### Describe the current behavior\nWhen running my app on iOS 18.4.1, it crashes with EXC_BAD_ACCESS at the point where I initialize the TFLite interpreter. The same model and code work on iOS 17.x.\n\n### Crash Log\nThread 1: EXC_BAD_ACCESS (code=2, address=0x...)\n\n### Code Snippet\n\n\nguard let model = Model(filePath: modelPath) else {\n    throw InterpreterError.failedToLoadModel\n}\ntry self.init(model: model, options: options, delegates: delegates)\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Vouchnim",
        "body": "Please let me know if there's an incompatibility with iOS 18 or if a fix is in development.\n\n\n"
      },
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @Vouchnim \nI apologize for the delay in my response, I see you're using Xcode version 15.3, the version of Xcode intended for developing for iOS 18.4.1 is Xcode 16.3. Xcode 16.3 includes the iOS 18.4 SDK which supports iOS 18.4 and its subsequent updates including 18.4.1 so if possible could you please give it try with Xcode 16.3 version and see is it resolving your issue or not ?\n\nI would request you to enable memory diagnostics tools mentioned below if Xcode 16.3 version does not solve your issue\n\n1. Address Sanitizer (ASan): In your Xcode scheme's \"Diagnostics\" settings, enable \"Address Sanitizer.\" Rerun the app until it crashes. ASan can detect various memory errors (use-after-free, buffer overflows) and provide more detailed information about the cause of the EXC_BAD_ACCESS .\n\n2. Thread Sanitizer (TSan): If your model initialization involves or is preceded by multi-threaded operations enable the Thread Sanitizer to detect data races which can also lead to memory corruption .\n\n3. Zombie Objects: Enable Zombie Objects in the scheme's diagnostics. This tool helps identify if the crash is due to messaging a deallocated object. It keeps deallocated objects as zombies and logs an error if they are accessed\n\nOne more thing if possible could you please give it try with latest version of TensorFlow or nightly version I see currently you're using **2.15.0** version ?\n\nIf issue still persists please help us with complete debugging log to investigate this issue further from our end. Thank you for your cooperation and patience.\n\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      }
    ]
  },
  {
    "issue_number": 39565,
    "title": "Recursive support for tf.io.gfile.glob",
    "author": "Conchylicultor",
    "state": "open",
    "created_at": "2020-05-14T23:18:36Z",
    "updated_at": "2025-06-03T17:43:18Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:feature"
    ],
    "body": "Currently, the `tf.io.gfile.glob` API do not support `recursive=True` kwargs, which is inconsistent with Python [glob.glob](https://docs.python.org/3/library/glob.html).\r\n\r\nExample to demonstrate the issue:\r\n\r\n```py\r\nimport glob\r\nimport tensorflow as tf\r\n\r\ntf.io.gfile.makedirs('/tmp/a/b/c')\r\n\r\ntf.io.gfile.glob('/tmp/a/**')  # ['/tmp/a/b']\r\nglob.glob('/tmp/a/**', recursive=True)  # ['/tmp/a/', '/tmp/a/b', '/tmp/a/b/c']\r\n```\r\nIt would be nice to support `recursive=True` for the Gfile API:\r\n\r\n```py\r\ntf.io.gfile.glob('/tmp/a/**', recursive=True)  # ['/tmp/a/', '/tmp/a/b', '/tmp/a/b/c']\r\n```",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "We will implement this functionality after conversion to modular filesystems."
      },
      {
        "user": "Conchylicultor",
        "body": "@michaelbanfield Nice, by curiosity, do you know if there will also be a `pathlib` like interface ?  https://docs.python.org/3/library/pathlib.html\r\n\r\n```py\r\npath = tf.io.gpath.GPath('gs://my_bucket')\r\nfor file in path.glob('**'):\r\n  ...\r\n```\r\nThat would be awesome"
      },
      {
        "user": "mihaimaruseac",
        "body": "Yes, we plan to follow existing APIs for convenience."
      }
    ]
  },
  {
    "issue_number": 88122,
    "title": "tensorflow cannot detect GPU with cuda 12.8, torch can",
    "author": "ywangwxd",
    "state": "open",
    "created_at": "2025-02-26T06:33:16Z",
    "updated_at": "2025-06-03T17:41:56Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:support",
      "comp:gpu",
      "TF 2.13"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.13\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux CentOS 7\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\ncuda 12.8 cudnn 9.7.1\n\n### GPU model and memory\n\nv100\n\n### Current behavior?\n\nI have tried python 3.8, 3.11, none of the can detect GPU, but pytorch and tf-nightly is working well.\nWhenever I ask tensorflow to list GPU available, it gave me this message:\n\n> [GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\n \", len(tf.config.list_physical_devices('GPU')))2025-02-26 14:28:49.474758: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-02-26 14:28:49.528867: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "MrAnsif",
        "body": "I'm having trouble getting TensorFlow 2.13 to detect my GPU, even though PyTorch and tf-nightly work just fine.\n\nThings I’ve Tried\nChecked CUDA installation – nvidia-smi confirms the GPU is fine.\nVerified TensorFlow compatibility – Seems like TF 2.13 only supports CUDA 11.8, but I have CUDA 12.8.\nReinstalled TensorFlow with GPU support – No change.\nSet environment variables correctly – Still not working.\n\nWorkarounds\ntf-nightly works, so it looks like TensorFlow 2.13 doesn’t support CUDA 12.8.\n\nPotential Fixes:\nUse tf-nightly:\npip install tf-nightly\nDowngrade to CUDA 11.8 (since TF 2.13 officially supports that).\nWould be great to know if stable TensorFlow will support CUDA 12.8 soon. Thanks!"
      },
      {
        "user": "clickbaron",
        "body": "> ### Issue type\n> \n> Bug\n> \n> ### Have you reproduced the bug with TensorFlow Nightly?\n> \n> No\n> \n> ### Source\n> \n> binary\n> \n> ### TensorFlow version\n> \n> tf 2.13\n> \n> ### Custom code\n> \n> No\n> \n> ### OS platform and distribution\n> \n> Linux CentOS 7\n> \n> ### Mobile device\n> \n> _No response_\n> \n> ### Python version\n> \n> 3.8\n> \n> ### Bazel version\n> \n> _No response_\n> \n> ### GCC/compiler version\n> \n> _No response_\n> \n> ### CUDA/cuDNN version\n> \n> cuda 12.8 cudnn 9.7.1\n> \n> ### GPU model and memory\n> \n> v100\n> \n> ### Current behavior?\n> \n> I have tried python 3.8, 3.11, none of the can detect GPU, but pytorch and tf-nightly is working well.\n> Whenever I ask tensorflow to list GPU available, it gave me this message:\n> \n> > [GCC 11.2.0] :: Anaconda, Inc. on linux\n> Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n> >>> import tensorflow as tf\n>  \", len(tf.config.list_physical_devices('GPU')))2025-02-26 14:28:49.474758: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n> 2025-02-26 14:28:49.528867: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n> \n> ### Standalone code to reproduce the issue\n> \n> ```shell\n> import tensorflow as tf\n> print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n> ```\n> \n> ### Relevant log output\n> \n> ```shell\n> \n> ```\n\nOn it"
      },
      {
        "user": "Anuj-shishodia",
        "body": "Check GPU Availability – Ensure your system detects the GPU using (nvidia-smi). If not, install/update NVIDIA drivers.\n\nVerify TensorFlow's CUDA Requirements – Different TensorFlow versions require specific CUDA and cuDNN versions. Install the correct versions.\n\nInstall/Update NVIDIA Drivers – If no GPU is detected, install the latest NVIDIA driver.\nInstall CUDA & cuDNN – Download and install the correct versions required by TensorFlow.\n\nSet Environment Variables – Ensure CUDA and cuDNN paths are correctly set in the system’s PATH.\nRestart the System – After installation, reboot to apply changes.\n\nTest TensorFlow GPU Support – Run->( tf.config.list_physical_devices('GPU')) to check if TensorFlow detects the GPU."
      }
    ]
  },
  {
    "issue_number": 40614,
    "title": "If TMP is not set, then the build system defaults to the wrong path",
    "author": "sbrl",
    "state": "open",
    "created_at": "2020-06-19T13:51:48Z",
    "updated_at": "2025-06-03T17:29:33Z",
    "labels": [
      "type:build/install",
      "subtype: ubuntu/linux",
      "TF 2.10"
    ],
    "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.3.1611 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.2.0\r\n- Python version: 3.5.1\r\n- Installed using virtualenv? pip? conda?: no, compiling from source\r\n- Bazel version (if compiling from source): 2.0.0 (it claimed that Tensorflow did not support anything higher)\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla P100, 16280MiB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nWhen building Tensorflow from source when the `TMP` environment variable is not set, then following warning is shown:\r\n\r\n```\r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\n```\r\n\r\nNaturally, I would expect it to default to something. However, I'm on a _Linux_ computer, not a _Windows_ computer. On Linux, it should default to something sensible (e.g. `/tmp`, or the current directory), instead of `C:\\Windows\\Temp` - which clearly isn't going to work - as on Linux this is not a valid file/directory path.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\n```",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "What is the commit hash you are building from?\r\n\r\nThis seems weird, it looks as if Windows defines are present. Can you share the exact sequence of commands you are running from cloning the repo until the error? With full output, please."
      },
      {
        "user": "Saduf2019",
        "body": "@sbrl \r\nPlease update as per above comment."
      },
      {
        "user": "sbrl",
        "body": "Hello! Sorry for not updating sooner, I've been a bit busy. Here's what I did:\r\n\r\n```bash\r\n# Download bazel\r\ncurl -o bazel -L https://github.com/bazelbuild/bazel/releases/download/2.0.0/bazel-2.0.0-linux-x86_64\r\nchmod +x bazel\r\n# Set the PATH\r\nexport PATH=\"${PWD}:${PATH}\";\r\n\r\n# Load the appropriate modules (I am on an academic HPC)\r\nmodule load gcc/8.2.0;\r\nmodule load python/3.5.1;\r\nmodule load utilities/multi;\r\nmodule load cuda/10.1.168;\r\n\r\n# Clone tensorflow\r\ngit clone \"https://github.com/tensorflow/tensorflow.git\" \"tensorflow\";\r\ncd tensorflow;\r\ngit checkout v2.2.0;\r\n\r\n# Configure the build\r\n./configure\r\n\r\n# Interactive answers:\r\n# python location: /trinity/clustervision/CentOS/7/apps/python/3.5.1/bin/python3\r\n# Please input the desired Python library path to use\t(left as default)\r\n# OpenCL SYCL support?\tn\r\n# ROCm support? n (unless we're on a machine with an AMD GPU)\r\n# CUDA support? y\r\n# TensorRT support? n\r\n\r\n\r\n# Then it fails to locate the CUDA header files:\r\n# \"Could not find any cudnn.h, cudnn_version.h matching version '' in any subdirectory\"\r\n\r\n# Do this to locate them:\r\n# module show cuda/10.1.168\r\n# \r\n# CUDA SDK version: 10.1\r\n# cuDNN version: (left as default)\r\n# NCCL version: (left as default - seems like it'll download & build that too)\r\n# comma-separated list of base paths to look for CUDA libraries and headers:\r\n#\t/home/ViperAppsFiles/cuda/10.1.168/,/home/ViperAppsFiles/cuda/10.1.168/bin,/home/ViperAppsFiles/cuda/10.1.168/lib,/home/ViperAppsFiles/cuda/10.1.168/lib64,/home/ViperAppsFiles/cuda/10.1.168/extras/Debugger/include,/home/ViperAppsFiles/cuda/10.1.168/extras/Debugger/lib64,/home/ViperAppsFiles/cuda/10.1.168/include,/home/ViperAppsFiles/cuda/10.1.168/sdk/10.1.168/common/inc,/home/ViperAppsFiles/cuda/10.1.168/open64/bin,/home/ViperAppsFiles/cuda/10.1.168/open64/lib,/home/ViperAppsFiles/cuda/10.1.168/extras/CUPTI/include,/home/ViperAppsFiles/cuda/10.1.168/extras/CUPTI/lib64,/home/ViperAppsFiles/cuda/10.1.168/extras/Debugger/include,/home/ViperAppsFiles/cuda/10.1.168/targets/x86_64-linux/include\r\n# \r\n\r\n# Actually do the build:\r\nbazel build --config=opt --config=monolithic //tensorflow/tools/lib_package:libtensorflow\r\n# I also tried for an unrelated (much nastier and problematic) issue:\r\nbazel build --config=opt --config=cuda --config=monolithic //tensorflow/tools/lib_package:libtensorflow\r\n```\r\n\r\n/cc @mihaimaruseac, @Saduf2019 "
      }
    ]
  },
  {
    "issue_number": 93613,
    "title": "Inconsistencies in the result of `tf.signal.mdct` on cpu and gpu",
    "author": "Redempt1onzzZZ",
    "state": "closed",
    "created_at": "2025-05-18T07:01:41Z",
    "updated_at": "2025-06-03T02:14:08Z",
    "labels": [
      "stat:awaiting response",
      "type:support",
      "stale",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.19.0-rc0-6-ge36baa30292 2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nNVIDIA H100 - 80GB\n\n### Current behavior?\n\n`tf.signal.mdct` performs differently on cpu and gpu, this only happens when the dtype is float32, when dtype is float64, the result is expected.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nsignal = tf.constant([1.0, 0.0, -1.0, 0.0, 1.0, 0.0, -1.0, 0.0,\n                      1.0, 0.0, -1.0, 0.0, 1.0, 0.0, -1.0, 0.0],dtype=tf.float32)\n\nframe_length = 8 \nwith tf.device('/cpu:0'):\n    mdct_result = tf.signal.mdct(\n        signals=signal,\n        frame_length=frame_length,\n    )\n\n    print(\"cpu:\",mdct_result.numpy())\n    print(mdct_result.dtype)\n\nwith tf.device('/gpu:0'):\n    mdct_result = tf.signal.mdct(\n        signals=signal,\n        frame_length=frame_length,\n    )\n\n    print(\"gpu:\",mdct_result.numpy())\n    print(mdct_result.dtype)\n```\n\n### Relevant log output\n\n```shell\ncpu: [[-0.33414447  0.06423938  3.970529   -0.34513482]\n [-0.33414447  0.06423938  3.970529   -0.34513482]\n [-0.33414447  0.06423938  3.970529   -0.34513482]]\n<dtype: 'float32'>\ngpu: [[-0.33414438  0.06423952  3.9705293  -0.34513518]\n [-0.33414438  0.06423952  3.9705293  -0.34513518]\n [-0.33414438  0.06423952  3.9705293  -0.34513518]]\n<dtype: 'float32'>\n```",
    "comments": [
      {
        "user": "Redempt1onzzZZ",
        "body": "this also happens on `tf.signal.dct`\n```\nimport tensorflow as tf\n\nsignal = tf.constant([1.0, 0.0, -1.0, 0.0, 1.0, 0.0, -1.0, 0.0,\n                      1.0, 0.0, -1.0, 0.0, 1.0, 0.0, -1.0, 0.0],dtype=tf.float32)\n\ntype = 4\nwith tf.device('/cpu:0'):\n    dct_result = tf.signal.dct(\n        input=signal,\n        type=type,\n    )\n\n    print(\"cpu:\",dct_result.numpy())\n    print(dct_result.dtype)\n\nwith tf.device('/gpu:0'):\n    dct_result = tf.signal.dct(\n        input=signal,\n        type=type,\n    )\n\n    print(\"gpu:\",dct_result.numpy())\n    print(dct_result.dtype)\n```\n\n```\ncpu: [ 0.9543233   1.1870197   0.82439464  1.6538379   0.751008    2.910142\n  0.71484387 14.410847    0.7079587  -4.8190866   0.7289522  -2.0989256\n  0.7822068  -1.3754193   0.8803538  -1.0529331 ]\n<dtype: 'float32'>\ngpu: [ 0.9543233   1.1870196   0.8243948   1.6538382   0.75100785  2.9101415\n  0.7148444  14.410846    0.70795846 -4.819085    0.7289522  -2.0989244\n  0.782207   -1.3754193   0.8803537  -1.0529326 ]\n<dtype: 'float32'>\n```"
      },
      {
        "user": "Redempt1onzzZZ",
        "body": "this also repros on `tf.signal.idct`\n```\nimport tensorflow as tf\n\nsignal = tf.constant([1.0, 0.0, -1.0, 0.0, 1.0, 0.0, -1.0, 0.0,\n                      1.0, 0.0, -1.0, 0.0, 1.0, 0.0, -1.0, 0.0],dtype=tf.float32)\n\ntype = 4\nwith tf.device('/cpu:0'):\n    idct_result = tf.signal.idct(\n        input=signal,\n        type=type,\n    )\n\n    print(\"cpu:\",idct_result.numpy())\n    print(idct_result.dtype)\n\nwith tf.device('/gpu:0'):\n    idct_result = tf.signal.idct(\n        input=signal,\n        type=type,\n    )\n\n    print(\"gpu:\",idct_result.numpy())\n    print(idct_result.dtype)\n```\n\n```\ncpu: [ 0.9543233   1.1870197   0.82439464  1.6538379   0.751008    2.910142\n  0.71484387 14.410847    0.7079587  -4.8190866   0.7289522  -2.0989256\n  0.7822068  -1.3754193   0.8803538  -1.0529331 ]\n<dtype: 'float32'>\ngpu: [ 0.9543233   1.1870196   0.8243948   1.6538382   0.75100785  2.9101415\n  0.7148444  14.410846    0.70795846 -4.819085    0.7289522  -2.0989244\n  0.782207   -1.3754193   0.8803537  -1.0529326 ]\n<dtype: 'float32'>\n```"
      },
      {
        "user": "mihaimaruseac",
        "body": "Numerical inaccuracies are expected."
      }
    ]
  },
  {
    "issue_number": 52195,
    "title": "Support torch.nn.fold in tensorflow?",
    "author": "hwangdeyu",
    "state": "open",
    "created_at": "2021-09-30T09:43:16Z",
    "updated_at": "2025-06-03T00:40:09Z",
    "labels": [
      "stat:contribution welcome",
      "type:feature"
    ],
    "body": "This function is popular in Vision Models. \r\n\r\nI don't find any api function in tf that is equivalent with [torch.nn.fold](https://pytorch.org/docs/stable/generated/torch.nn.Fold.html?highlight=fold#).\r\n\r\nSo does Tensowflow already have such a function or is there a plan to do so?\r\n",
    "comments": [
      {
        "user": "sachinprasadhs",
        "body": "You can refer [this](https://github.com/tensorflow/fold/blob/master/tensorflow_fold/g3doc/index.md) document on Tensorflow Fold."
      },
      {
        "user": "hwangdeyu",
        "body": "hi @sachinprasadhs, thanks for your reply.\r\nAccording to the official documentation,\r\n> The *Tensorflow Fold* is  a library for creating TensorFlow models that consume structured data\r\n\r\nSo it's a library used for Dynamic Computation Graphs, which is different with the Pytorch function [torch.nn.fold](https://pytorch.org/docs/stable/generated/torch.nn.Fold.html?highlight=fold#)."
      },
      {
        "user": "kkimdev",
        "body": "Thanks for the feature request, we don't have a plan at this moment but contributions welcome. (probably https://www.tensorflow.org/addons is a better place initially)"
      }
    ]
  },
  {
    "issue_number": 94624,
    "title": "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
    "author": "dingyiren",
    "state": "closed",
    "created_at": "2025-06-01T20:02:32Z",
    "updated_at": "2025-06-02T17:11:22Z",
    "labels": [
      "type:bug"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n# TensorFlow Import Error: DLL Load Failed\n\n## Summary\nTensorFlow fails to import with `ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.`\n\n## Environment Information\n- **Operating System:** Windows (detected from file paths)\n- **Python Distribution:** Anaconda3\n- **Python Version:** [Please specify - check with `python --version`]\n- **TensorFlow Version:** [Please specify - check with `pip show tensorflow`]\n- **Installation Method:** pip/conda [Please specify which you used]\n\n## Steps to Reproduce\n1. Install TensorFlow in Anaconda environment\n2. Attempt to import TensorFlow or a package that depends on it (deeplogit in this case)\n3. Error occurs during import\n\n## Expected Behavior\nTensorFlow should import successfully without DLL errors.\n\n## Actual Behavior\nImport fails with DLL initialization error in `_pywrap_tensorflow_internal`.\n\n## Full Error Traceback\n```\nImportError                               Traceback (most recent call last)\nFile ~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\n     72 try:\n---> 73   from tensorflow.python._pywrap_tensorflow_internal import *\n\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\nDuring handling of the above exception, another exception occurred:\n\nImportError                               Traceback (most recent call last)\nCell In[1], line 4\n      2 import numpy as np\n      3 import pandas as pd\n----> 4 from deeplogit import DeepLogit\n\nFile ~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\deeplogit\\__init__.py:9\n      1 \"\"\"\n      2 deeplogit.\n      3 \n      4 Mixed Logit Estimation with Text and Image Embeddings Extracted Using Deep Learning Models\n      5 \"\"\"\n      7 __version__ = \"0.1.0\"\n----> 9 from .deeplogit import *\n\nFile ~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\deeplogit\\deeplogit.py:9\n      7 import numpy as np\n      8 import pandas as pd\n----> 9 from tensorflow.keras.preprocessing import image\n\nFile ~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:40\n---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n\nFile ~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\n---> 88   raise ImportError(\n     89       f'{traceback.format_exc()}'\n     90       f'\\n\\nFailed to load the native TensorFlow runtime.\\n'\n     91       f'See https://www.tensorflow.org/install/errors '\n     92       f'for some common causes and solutions.\\n'\n     93       f'If you need help, create an issue '\n     94       f'at https://github.com/tensorflow/tensorflow/issues '\n     95       f'and include the entire stack trace above this error message.')\n\nImportError: Failed to load the native TensorFlow runtime.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom deeplogit import DeepLogit\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94624\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94624\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 62928,
    "title": "tf-nightly: wrong packaging metadata prevents installation using poetry on Linux",
    "author": "thorbenk",
    "state": "open",
    "created_at": "2024-02-09T11:14:10Z",
    "updated_at": "2025-06-01T20:49:09Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:build/install",
      "subtype: ubuntu/linux",
      "TF 2.15"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.16.0.dev20240209\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.7\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI cannot install `tf-nightly` using `poetry` on Linux.\r\n\r\nI'm getting the following error:\r\n\r\n```\r\n$> poetry add tf-nightly\r\nUsing version ^2.16.0.dev20240209 for tf-nightly\r\n\r\nUpdating dependencies\r\nResolving dependencies... (0.0s)\r\n\r\nPackage 'tf-nightly' is listed as a dependency of itself.\r\n```\r\n\r\nI'm using the following versions:\r\n- Poetry: 1.7.1\r\n- Pip: 24.0\r\n- Python: 3.11.7\r\n\r\nIt looks to me like the package metadata is wrong:\r\n\r\nQuoting from https://pypi.org/pypi/tf-nightly/2.16.0.dev20240209/json ,\r\nthe `requires_dist` field contains `tf-nightly ==2.16.0-dev20240209 ; platform_system == \"Darwin\" and platform_machine == \"arm64\"'`.\r\n\r\nIf I understand `poetry`s output correctly, this is the self-dependency that is not allowed.\r\n\r\nTo reproduce this, just create an empty directory, paste the following as your initial `pyproject.toml`\r\n\r\n```toml\r\n[tool.poetry]\r\nname = \"tf-package-bug\"\r\nversion = \"0.1.0\"\r\ndescription = \"\"\r\nauthors = [\"Your Name <you@example.com>\"]\r\nreadme = \"README.md\"\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.11\"\r\n\r\n\r\n[build-system]\r\nrequires = [\"poetry-core\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n```\r\n\r\nAnd then run\r\n- `pyenv local 3.11.7` (this is how I'm activating a Python 3.11 installation)\r\n- `python -m pip install poetry` (make sure Poetry is installed for your Python 3.11)\r\n- `poetry install --no-root` (Creates the virtual env using Python 3.11)\r\n- `poetry add tf-nightly` (Throws the error above)\n\n### Standalone code to reproduce the issue\n\n```shell\nThe steps to reproduce are noted above.\n```\n\n\n### Relevant log output\n\n_No response_",
    "comments": [
      {
        "user": "frazane",
        "body": "Same issue on MacOS 14.2 and python 3.11.7. "
      },
      {
        "user": "johnthagen",
        "body": "This also happens for `tensorflow-macos` 2.16.1, though that package is supposed to be not used as of 2.16 in favor of the base`tensorflow` package."
      }
    ]
  },
  {
    "issue_number": 79421,
    "title": "Current LiteRT Android dependencies in documentation look broken",
    "author": "vykintazo",
    "state": "open",
    "created_at": "2024-11-05T11:29:22Z",
    "updated_at": "2025-05-31T10:19:15Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:support",
      "comp:lite",
      "TFLiteConverter",
      "TFLiteGpuDelegate"
    ],
    "body": "I think after the recent TensorflowLite rename to LiteRT some pages in documentation where renamed incorrectly and are currently very confusing.\r\nFor a particular example, see this: https://ai.google.dev/edge/litert/android/gpu\r\n- The docs say to add `com.google.ai.edge.litert:litert-gpu` and `com.google.ai.edge.litert:litert-gpu-api` with versions `2.X.Y`, which do not exist (current latest version is `1.0.1`), to a toml version catalog. \r\n- In the next paragraph it also switches into using gradle files instead of toml to declare other dependencies, which I found somewhat confusing.\r\n- Later, in [standalone setup](https://ai.google.dev/edge/litert/android/gpu#add_project_dependencies_2), it says to include `com.google.ai.edge.litert:litert-gpu-delegate-plugin` dependency, which does not exist and also follows with a code snipped supposedly showing how to include it, but it shows other dependencies.\r\n- [Other places](https://ai.google.dev/edge/litert/android/metadata/codegen#acceleration) too include dependencies with incorrect versions, like `com.google.ai.edge.litert:litert-gpu:2.3.0`\r\n\r\nI just happend to start working with LiteRT for Android right now and found it very difficult to distinguish which parts of documentation are outdated and which aren't.",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @vykintazo \r\n\r\nI apologize for the delayed response, thank you for bringing this issue to our attention I'll have look into this issue and will update you. \r\n\r\nThank you for your cooperation and patience."
      },
      {
        "user": "davidlad123",
        "body": "Was an update ever published for this issue? I'm attemping to ue liteRT and this missing library issue is cropping up"
      },
      {
        "user": "MohitGuptaC",
        "body": "Please fix the documentation. This whole situation is a mess. We have TensorFlow Lite, LiteRT and now the new LiteRT Next.\n\nI think you are just waiting for LiteRT Next to be stable to remove the current LiteRT which is fine but please just update the docs of the current LiteRT as we can't wait until LiteRT Next becomes stable."
      }
    ]
  },
  {
    "issue_number": 94494,
    "title": "Unable to build tensorflow master",
    "author": "V-R-S",
    "state": "closed",
    "created_at": "2025-05-29T22:59:59Z",
    "updated_at": "2025-05-31T07:12:52Z",
    "labels": [
      "type:build/install"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nmaster\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nWindows x64\n\n### Mobile device\n\nNone\n\n### Python version\n\n3.12\n\n### Bazel version\n\n7.4.1\n\n### GCC/compiler version\n\nclang 20.1.5\n\n### CUDA/cuDNN version\n\nNot using\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnable to build master branch with the default build command.\nWas able to build branch r2.19.\n\n### Standalone code to reproduce the issue\n\n```shell\nbazelisk build --config=win_clang --repo_env=TF_PYTHON_VERSION=3.12 //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu\n```\n\n### Relevant log output\n\n```shell\nC:\\Users\\ASUS\\tensorflow>bazelisk build --config=win_clang --repo_env=TF_PYTHON_VERSION=3.12 //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu\nStarting local Bazel server and connecting to it...\nINFO: Reading 'startup' options from c:\\users\\asus\\tensorflow\\.bazelrc: --windows_enable_symlinks\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\nINFO: Reading rc options for 'build' from c:\\users\\asus\\tensorflow\\.bazelrc:\n  Inherited 'common' options: --announce_rc --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility --noenable_bzlmod --noincompatible_enable_cc_toolchain_resolution --noincompatible_enable_android_toolchain_resolution --experimental_repo_remote_exec --java_runtime_version=remotejdk_21\nINFO: Options provided by the client:\n  'build' options: --python_path=C:/Program Files/Python312/python.exe\nINFO: Reading rc options for 'build' from c:\\users\\asus\\tensorflow\\.bazelrc:\n  'build' options: --repo_env=ML_WHEEL_TYPE=snapshot --repo_env=ML_WHEEL_BUILD_DATE= --repo_env=ML_WHEEL_VERSION_SUFFIX= --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --host_features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --@rules_python//python/config_settings:precompile=force_disabled\nINFO: Reading rc options for 'build' from c:\\users\\asus\\tensorflow\\.tf_configure.bazelrc:\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Program Files/Python312/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files/Python312/Lib/site-packages --python_path=C:/Program Files/Python312/python.exe --action_env CLANG_COMPILER_PATH=C:Program FilesLLVMbinclang.exe --repo_env=CC=C:Program FilesLLVMbinclang.exe --repo_env=BAZEL_COMPILER=C:Program FilesLLVMbinclang.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\nINFO: Found applicable config definition build:short_logs in file c:\\users\\asus\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\nINFO: Found applicable config definition build:v2 in file c:\\users\\asus\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\nINFO: Found applicable config definition build:win_clang in file c:\\users\\asus\\tensorflow\\.bazelrc: --config=win_clang_base --extra_toolchains=@local_config_cc//:cc-toolchain-x64_windows-clang-cl --extra_execution_platforms=//tensorflow/tools/toolchains/win:x64_windows-clang-cl --host_platform=//tensorflow/tools/toolchains/win:x64_windows-clang-cl\nINFO: Found applicable config definition build:win_clang_base in file c:\\users\\asus\\tensorflow\\.bazelrc: --@com_google_protobuf//:use_dlls=True --@com_google_absl//absl:use_dlls --linkopt=/demangle:no --host_linkopt=/demangle:no --linkopt=/errorlimit:0 --host_linkopt=/errorlimit:0 --copt=/clang:-Weverything --host_copt=/clang:-Weverything --compiler=clang-cl --linkopt=/FORCE:MULTIPLE --host_linkopt=/FORCE:MULTIPLE --action_env=PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYW\nINFO: Found applicable config definition build:windows in file c:\\users\\asus\\tensorflow\\.bazelrc: --copt=/W0 --host_copt=/W0 --copt=/Zc:__cplusplus --host_copt=/Zc:__cplusplus --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --features=compiler_param_file --features=archive_param_file --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --copt=-D_ENABLE_EXTENDED_ALIGNED_STORAGE --host_copt=-D_ENABLE_EXTENDED_ALIGNED_STORAGE --enable_runfiles --nobuild_python_zip --dynamic_mode=off --cxxopt=/std:c++17 --host_cxxopt=/std:c++17 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/Zc:preprocessor --host_copt=/Zc:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --config=no_tfrt\nINFO: Found applicable config definition build:monolithic in file c:\\users\\asus\\tensorflow\\.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false\nINFO: Found applicable config definition build:no_tfrt in file c:\\users\\asus\\tensorflow\\.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/ir,tensorflow/compiler/mlir/tfrt/ir/mlrt,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ifrt,tensorflow/compiler/mlir/tfrt/tests/mlrt,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/compiler/mlir/tfrt/transforms/mlrt,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/runtime_fallback/test,tensorflow/core/runtime_fallback/test/gpu,tensorflow/core/runtime_fallback/test/saved_model,tensorflow/core/runtime_fallback/test/testdata,tensorflow/core/tfrt/stubs,tensorflow/core/tfrt/tfrt_session,tensorflow/core/tfrt/mlrt,tensorflow/core/tfrt/mlrt/attribute,tensorflow/core/tfrt/mlrt/kernel,tensorflow/core/tfrt/mlrt/bytecode,tensorflow/core/tfrt/mlrt/interpreter,tensorflow/compiler/mlir/tfrt/translate/mlrt,tensorflow/compiler/mlir/tfrt/translate/mlrt/testdata,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug,tensorflow/core/tfrt/saved_model/python,tensorflow/core/tfrt/graph_executor/python,tensorflow/core/tfrt/saved_model/utils\nDEBUG: C:/users/asus/_bazel_asus/fxyngur5/external/local_xla/third_party/py/python_repo.bzl:87:10:\n=============================\nHermetic Python configuration:\nVersion: \"3.12\"\nKind: \"\"\nInterpreter: \"default\" (provided by rules_python)\nRequirements_lock label: \"@python_version_repo//:requirements_lock_3_12.txt\"\n=====================================\nINFO: Analyzed target //tensorflow/tools/pip_package:wheel (775 packages loaded, 47764 targets configured).\nERROR: C:/users/asus/tensorflow/tensorflow/python/BUILD:1209:27: Linking tensorflow/python/_pywrap_tfe.so failed: (Exit 1): lld-link.exe failed: error executing CppLink command (from target //tensorflow/python:_pywrap_tfe.so)\n  cd /d C:/users/asus/_bazel_asus/fxyngur5/execroot/org_tensorflow\n  SET CLANG_COMPILER_PATH=C:Program FilesLLVMbinclang.exe\n    SET LIB=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.44.35207\\ATLMFC\\lib\\x64;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.44.35207\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.26100.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\\\lib\\10.0.26100.0\\\\um\\x64;C:\\Program Files\\LLVM\\lib\\clang\\20\\lib\\windows\n    SET PATH=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.44.35207\\bin\\HostX64\\x64;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Team Tools\\DiagnosticsHub\\Collector;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.26100.0\\\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\\\x64;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\\\MSBuild\\Current\\Bin\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe;C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\vcpkg\n    SET PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYW\n    SET PWD=/proc/self/cwd\n    SET PYTHON_BIN_PATH=C:/Program Files/Python312/python.exe\n    SET PYTHON_LIB_PATH=C:/Program Files/Python312/Lib/site-packages\n    SET TEMP=C:\\Users\\ASUS\\AppData\\Local\\Temp\n    SET TF2_BEHAVIOR=1\n    SET TMP=C:\\Users\\ASUS\\AppData\\Local\\Temp\n  C:\\Program Files\\LLVM\\bin\\lld-link.exe @bazel-out/x64_windows-opt/bin/tensorflow/python/_pywrap_tfe.so-2.params\n# Configuration: bd88bd9d038d7a1a810484196146944dc9e334378bc6959e6e0467755220fa1e\n# Execution platform: //tensorflow/tools/toolchains/win:x64_windows-clang-cl\nlld-link: error: undefined symbol: ?InvalidArgumentError@lts_20230802@absl@@YA?AVStatus@12@V?$basic_string_view@DU?$char_traits@D@std@@@std@@@Z\n>>> referenced by _pywrap_tfe.so_cclib.lib(tfe_wrapper.obj):(??$InvalidArgument@PEBD@errors@tsl@@YA?AVStatus@lts_20230802@absl@@PEBD@Z)\n>>> referenced by _pywrap_tfe.so_cclib.lib(tfe_wrapper.obj):(??$InvalidArgument@PEBDV?$basic_string_view@DU?$char_traits@D@std@@@std@@@errors@tsl@@YA?AVStatus@lts_20230802@absl@@PEBDV?$basic_string_view@DU?$char_traits@D@std@@@std@@@Z)\n\nlld-link: error: undefined symbol: ?InternalError@lts_20230802@absl@@YA?AVStatus@12@V?$basic_string_view@DU?$char_traits@D@std@@@std@@@Z\n>>> referenced by _pywrap_tfe.so_cclib.lib(tfe_wrapper.obj):(??$Internal@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@errors@tsl@@YA?AVStatus@lts_20230802@absl@@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\nTarget //tensorflow/tools/pip_package:wheel failed to build\nINFO: Elapsed time: 6117.561s, Critical Path: 252.55s\nINFO: 23488 processes: 5176 internal, 18312 local.\nERROR: Build did NOT complete successfully\n```",
    "comments": [
      {
        "user": "bandirevanth",
        "body": "### 🧩The Issue\n\nThe error arises when attempting to build TensorFlow from the master branch using Bazel on Windows with Clang:\n\n```bash\nbazelisk build --config=win_clang --repo_env=TF_PYTHON_VERSION=3.12 //...  \n```\n\nThis command fails, whereas building the r2.19 branch succeeds.\n\n---\n\n🔧 Recommended Solutions\n- Use a Stable Release Branch:\nSince building the r2.19 branch works, consider using this stable release for your project. This approach ensures reliability and reduces the likelihood of encountering build issues.\n- Update Dependencies: Ensure all dependencies are up-to-date and compatible with the latest TensorFlow code.\n- Check Clang Version: Verify that the Clang version used is compatible with the TensorFlow master branch requirements.\n- Consult TensorFlow's Build Documentation.\n- Build with Python 3.11"
      },
      {
        "user": "V-R-S",
        "body": "Thanks for the recommendation.\nI tried to look for the master branch dependecies.\nBut all I could was find.\n\n![Image](https://github.com/user-attachments/assets/62b94649-17f4-4bb1-9664-9f0663d3615a)\nWhich some of them I could not match.\nIf you have a configuration that works please let me know.\n\nAs for the error.\nI tried looking into whether the function mentioned in the error \n```\n?InternalError@lts_20230802@absl@@YA?AVStatus@12@V?$basic_string_view@DU?$char_traits@D@std@@@std@@@Z\n?InvalidArgumentError@lts_20230802@absl@@YA?AVStatus@12@V?$basic_string_view@DU?$char_traits@D@std@@@std@@@Z\n```\nare actaully present in the **_pywrap_tensorflow.def** which they are.\n\nAlso the library status.lib seemed to have the symbols.\n\nSo, I am not sure whats going on here.\n"
      },
      {
        "user": "bandirevanth",
        "body": "## Possible root causes\n⭐ **Your build is missing appropriate linking to Abseil (Absl) libraries, particularly those that provide error handling symbols like `InvalidArgumentError` and `InternalError`. The relevant object (`tfe_wrapper.obj`) is referencing functions that weren't resolved at link time.**\n\n2. *ABI Mismatch (MSVC vs Clang)*:\n`status.lib` was likely built with MSVC, but you're linking with Clang. Their C++ ABIs differ—especially with types like `std::string_view`. Even minor toolchain mismatches can break symbol resolution.\n\n3. *Static vs Dynamic Linking Conflic*t:\nLinking a `static .lib` into a `.dll` requires proper exports (`__declspec(dllexport)` or `.def` file). Missing exports = unresolved symbols. Use `dumpbin /EXPORTS` to verify.\n\n4. *Clang on Windows (TensorFlow Limitation)*:\nTensorFlow supports MSVC on Windows, not Clang. ABI differences and STL type incompatibilities make Clang linking fragile.\n\n5. *Ignored .DEF File*:\nEven if `_pywrap_tensorflow.def` declares exports, Bazel may skip it without proper `linkopts` in the `BUILD` file. Ensure it's passed in the link step.\n\n6. *Python 3.12 _Broken_ Compatibility*\n- Python 3.12 has broken ABI compatibility for many native libraries.\n- TensorFlow may not yet support 3.12 fully on Windows + Clang.\n\n## Workarounds\n1. **Check Abseil Library Linking**:\n- Ensure Abseil C++ is correctly pulled and linked in your build.\n- Make sure `absl::Status` related libraries are added to deps.\n\n2. **Verify the Mangled Symbols**\nUse this to confirm matching symbols:\n`dumpbin /SYMBOLS status.lib | findstr \"InternalError\"`\n\nEnsure the exact mangled name in the error exists in `status.lib`.\n\n3. **Inspect the Linker Command**\nAdd Bazel flags to get full build diagnostics: `--sandbox_debug --verbose_failures`\nThis helps verify whether .def and .lib files are being passed to the linker.\n\n4. **Explicitly Export Symbols**\nIf you control the C++ source, wrap affected functions with:\n`__declspec(dllexport) absl::Status InternalError(absl::string_view msg);`\n\nOr ensure they are present in the .def file and not stripped during linking.\n\n---\n\n## **Final Note**\nYou're in an *unstable and untested matrix*:\n(TensorFlow master) + (Clang on Windows) + (Python 3.12).\n\nThis combination is likely to break due to ABI mismatches and incomplete support."
      }
    ]
  },
  {
    "issue_number": 91989,
    "title": "Problem in comparison function",
    "author": "zszZSZzzz",
    "state": "closed",
    "created_at": "2025-04-22T12:47:10Z",
    "updated_at": "2025-05-30T13:20:30Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:feature",
      "awaiting PR merge"
    ],
    "body": "### Issue type\n\nFeature Request\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.20.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nQsort callback WeightedDeltaCompare may return false result when input structs have all compared fields equal. This in turn may causes inconsistent order or even crashes in some qsort implementations.\nThe location of the error function WeightedDeltaCompare is tensorflow/tensorflow/tools/android/test/jni/object_tracking/frame_pair.cc.\nAccording to the following part of the code, when the two input structures are exactly the same, the function returns 1 instead of 0.\n`\ninline int WeightedDeltaCompare(const void* const a, const void* const b) {\n  return (reinterpret_cast<const WeightedDelta*>(a)->delta -\n          reinterpret_cast<const WeightedDelta*>(b)->delta) <= 0 ? 1 : -1;\n}\n`\n\n\n### Standalone code to reproduce the issue\n\n```shell\nWhen the two input structures are exactly the same, the function WeightedDeltaCompare returns 1 instead of 0.\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @zszZSZzzz ,\nApologies for the delay, and thank you for raising your issue here. Could you please provide a code snippet? It would help us troubleshoot the issue more effectively.\nThank you!"
      },
      {
        "user": "zszZSZzzz",
        "body": "The location of the error function WeightedDeltaCompare is tensorflow/tensorflow/tools/android/test/jni/object_tracking/frame_pair.cc.\nThe function starts from line 196.\n```\ninline int WeightedDeltaCompare(const void* const a, const void* const b) {\n  return (reinterpret_cast<const WeightedDelta*>(a)->delta -\n          reinterpret_cast<const WeightedDelta*>(b)->delta) <= 0 ? 1 : -1;\n}\n```\nThere is another function KeypointCompare with a similar error type. The location of the error function KeypointCompare is tensorflow/tensorflow/tools/android/test/jni/object_tracking/keypoint_detector.cc.\nThe function starts from line 70.\n```\ninline int KeypointCompare(const void* const a, const void* const b) {\n  return (reinterpret_cast<const Keypoint*>(a)->score_ -\n          reinterpret_cast<const Keypoint*>(b)->score_) <= 0 ? 1 : -1;\n}\n```"
      },
      {
        "user": "gabriele-ciccotelli",
        "body": "Hello!\nI've implemented a solution that should fix the issue.\nThe changes ensure that the functions work correctly, even when used in combination with qsort.\nI've also created some tests to verify the correct behavior of both functions."
      }
    ]
  },
  {
    "issue_number": 66025,
    "title": "Inference time using Interpreter API on Android inconsistent and 10–50 times slower than same tflite model on iOS",
    "author": "jakubdolejs",
    "state": "closed",
    "created_at": "2024-04-18T20:40:06Z",
    "updated_at": "2025-05-30T08:54:55Z",
    "labels": [
      "stat:awaiting tensorflower",
      "comp:lite",
      "type:performance",
      "Android"
    ],
    "body": "### Issue type\n\nPerformance\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.15.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\nGoogle Pixel 4a running Android 13\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI'm running inference on Yolov8-based tflite model on Android using the Interpreter API. I noticed that the first 30 or so calls to the `Interpreter.run()` function take much longer than the subsequent calls. The difference is quite marked, starting at about 3500ms per run and ending at about 500ms.\r\n\r\nI thought perhaps it was something about the input data so I tried a test with running the same call with the same input 100 times in a loop. Same behaviour, the first handful of inference runs take around 3 seconds, slowly speeding up to about 500–700ms by the 100th iteration.\r\n\r\nI wanted to find out whether there is a specific combination of the interpreter options causing this behaviour so I wrote a test matrix initialising interpreters with different options:\r\n- Using GPU delegate\r\n  - Using Google Play Services runtime\r\n    - Using model with precision reduced from float32 to float16\r\n  - Using bundled runtime\r\n    - Using model with precision reduced from float32 to float16\r\n- Using NNAPI delegate\r\n  - Using Google Play Services runtime\r\n    - Using model with precision reduced from float32 to float16\r\n  - Using bundled runtime\r\n    - Using model with precision reduced from float32 to float16\r\n- Using CPU with XNNPACK\r\n  - Using Google Play Services runtime\r\n    - Using model with precision reduced from float32 to float16\r\n  - Using bundled runtime\r\n    - Using model with precision reduced from float32 to float16\r\n- Using CPU without XNNPACK\r\n  - Using Google Play Services runtime\r\n    - Using model with precision reduced from float32 to float16\r\n  - Using bundled runtime\r\n    - Using model with precision reduced from float32 to float16\r\n\r\nThere doesn't seem to be any difference whichever combination runs first takes suspicious amount of time for the first handful of inference runs. Sometimes the time never decreases and all the inference runs for the given configuration take a very long time (~3 seconds).\r\n\r\nI'm including the code using the bundled runtime. The Play Services runtime times were in line with the bundled runtime.\r\n\r\nThe device (Google Pixel 4a) is used only for development. There are no other apps installed aside from the test app and whatever was pre-installed on the phone. The device wasn't connected to the internet while running the test.\r\n\r\n## iOS comparison\r\n\r\nIn comparison, version 2.14.0 of TfLite for Swift (latest available on CocoaPods) using the CoreML delegate runs inference on the **same input** using the **same model** in 70ms on iPhone 12.\n\n### Standalone code to reproduce the issue\n\n```shell\nfun testInferenceSpeed() {\r\n    val context = InstrumentationRegistry.getInstrumentation().context\r\n    val assetManager = context.assets\r\n    // Input serialized as a float array in JSON\r\n    val jsonFile = \"face_on_iPad_001.jpg-flat.json\"\r\n    assetManager.open(jsonFile).use { inputStream ->\r\n        val json = inputStream.bufferedReader().use { it.readText() }\r\n        val floatArray = Json.decodeFromString<FloatArray>(json)\r\n        // Models – float32 and float16\r\n        val models = arrayOf(\"ARC_PSD-001_1.1.122_bst_yl80201_float32.tflite\", \"ARC_PSD-001_1.1.122_bst_yl80201_float16.tflite\")\r\n        val options = arrayOf(\"gpu\", \"nnapi\", \"cpu\", \"xnnpack\")\r\n        for (model in models) {\r\n            assetManager.open(model).use { modelInputStream ->\r\n                // Copy the model from assets to the cache directory\r\n                val modelFile = File(context.cacheDir, model)\r\n                modelFile.outputStream().use { outputStream ->\r\n                    modelInputStream.copyTo(outputStream)\r\n                }\r\n                for (option in options) {\r\n                    val interpreterOptions = InterpreterApi.Options()\r\n                    val compatibilityList = CompatibilityList()\r\n                    when (option) {\r\n                        \"gpu\" -> {\r\n                            compatibilityList.use {\r\n                                if (it.isDelegateSupportedOnThisDevice) {\r\n                                    interpreterOptions.addDelegate(\r\n                                        GpuDelegate(\r\n                                            it.bestOptionsForThisDevice\r\n                                        )\r\n                                    )\r\n                                }\r\n                            }\r\n                        }\r\n                        \"nnapi\" -> {\r\n                            if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.P) {\r\n                                interpreterOptions.addDelegate(NnApiDelegate())\r\n                                interpreterOptions.useNNAPI = true\r\n                            }\r\n                        }\r\n                        \"cpu\" -> {\r\n                            interpreterOptions.numThreads =\r\n                                Runtime.getRuntime().availableProcessors()\r\n                            interpreterOptions.useXNNPACK = false\r\n                        }\r\n\r\n                        \"xnnpack\" -> {\r\n                            interpreterOptions.numThreads =\r\n                                Runtime.getRuntime().availableProcessors()\r\n                            interpreterOptions.useXNNPACK = true\r\n                        }\r\n                        else -> throw IllegalArgumentException(\"Unknown option: $option\")\r\n                    }\r\n                    InterpreterApi.create(modelFile, interpreterOptions)\r\n                        .use { interpreterApi ->\r\n                            val times = mutableListOf<Long>()\r\n                            for (i in 0 until 100) {\r\n                                interpreterApi.allocateTensors()\r\n                                val input = FloatBuffer.wrap(floatArray)\r\n                                val output =\r\n                                    FloatBuffer.allocate(5 * 8400).also { it.rewind() }\r\n                                val time = measureTimeMillis {\r\n                                    interpreterApi.run(input, output)\r\n                                }\r\n                                times.add(time)\r\n                            }\r\n                            Log.d(\r\n                                TAG,\r\n                                \"Model: $model, Option: $option, Inference times (ms): [${times.map { it.toString()+\"ms\" }.joinToString()}], Average inference time: ${times.average()} ms\"\r\n                            )\r\n                        }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\n```\n\n\n### Relevant log output\n\n```shell\nModel: ARC_PSD-001_1.1.122_bst_yl80201_float32.tflite, Option: gpu, Inference times (ms): [2502ms, 3011ms, 2987ms, 2723ms, 3529ms, 4245ms, 3387ms, 4510ms, 4133ms, 4034ms, 4015ms, 3307ms, 3207ms, 3240ms, 2718ms, 2978ms, 2985ms, 3357ms, 2751ms, 2969ms, 2942ms, 3028ms, 2916ms, 3029ms, 4428ms, 2727ms, 4982ms, 4320ms, 3211ms, 2980ms, 4010ms, 3239ms, 2712ms, 3974ms, 3994ms, 3999ms, 3997ms, 3047ms, 3687ms, 3744ms, 2972ms, 2944ms, 3709ms, 3936ms, 3971ms, 3998ms, 3315ms, 4495ms, 3285ms, 4655ms, 2758ms, 3307ms, 4880ms, 4912ms, 3599ms, 2750ms, 2004ms, 2643ms, 3383ms, 3372ms, 1664ms, 3297ms, 2969ms, 1714ms, 2834ms, 3381ms, 1764ms, 2303ms, 1715ms, 3314ms, 3379ms, 1434ms, 3221ms, 2842ms, 1783ms, 1784ms, 1418ms, 1618ms, 1400ms, 1777ms, 1960ms, 1962ms, 1471ms, 2355ms, 2883ms, 1494ms, 2806ms, 2281ms, 2482ms, 2915ms, 1504ms, 2772ms, 3376ms, 1753ms, 3300ms, 1748ms, 2584ms, 3377ms, 3384ms, 1648ms], Average inference time: 3021.08 ms\r\nModel: ARC_PSD-001_1.1.122_bst_yl80201_float32.tflite, Option: nnapi, Inference times (ms): [2288ms, 2105ms, 1637ms, 2280ms, 2085ms, 1695ms, 1634ms, 1759ms, 1637ms, 2006ms, 2210ms, 2018ms, 2050ms, 1979ms, 1698ms, 2201ms, 2105ms, 1989ms, 2040ms, 1966ms, 2034ms, 1970ms, 2031ms, 1970ms, 2033ms, 1968ms, 2034ms, 1966ms, 1763ms, 2160ms, 2077ms, 1987ms, 2040ms, 1966ms, 2033ms, 1859ms, 2106ms, 1993ms, 2041ms, 1965ms, 1826ms, 2117ms, 2073ms, 1979ms, 2041ms, 1969ms, 1632ms, 2109ms, 2212ms, 2024ms, 1362ms, 1284ms, 1970ms, 1806ms, 1212ms, 1800ms, 1231ms, 1452ms, 1465ms, 1128ms, 1185ms, 1519ms, 1246ms, 1824ms, 1224ms, 1719ms, 1234ms, 1964ms, 1133ms, 1973ms, 1689ms, 1241ms, 1890ms, 1194ms, 1187ms, 1108ms, 1089ms, 1091ms, 1086ms, 1084ms, 958ms, 1021ms, 1009ms, 999ms, 964ms, 1025ms, 1041ms, 980ms, 850ms, 1082ms, 1091ms, 976ms, 960ms, 1021ms, 1019ms, 991ms, 958ms, 850ms, 1008ms, 873ms], Average inference time: 1614.26 ms\r\nModel: ARC_PSD-001_1.1.122_bst_yl80201_float32.tflite, Option: cpu, Inference times (ms): [1445ms, 1504ms, 1364ms, 1337ms, 1383ms, 1350ms, 1364ms, 1365ms, 1354ms, 1413ms, 1403ms, 1310ms, 1336ms, 1823ms, 1355ms, 1728ms, 1450ms, 1492ms, 1383ms, 1274ms, 1370ms, 1251ms, 1719ms, 1800ms, 1539ms, 1546ms, 1722ms, 1390ms, 1394ms, 1330ms, 1338ms, 1373ms, 1362ms, 1424ms, 1604ms, 1316ms, 1431ms, 1313ms, 1381ms, 1265ms, 1449ms, 1663ms, 1354ms, 1372ms, 1358ms, 1419ms, 1356ms, 1355ms, 1310ms, 1430ms, 1346ms, 1304ms, 1405ms, 1315ms, 1816ms, 1320ms, 1397ms, 1311ms, 1393ms, 1345ms, 1416ms, 1375ms, 1370ms, 1373ms, 1274ms, 1365ms, 1433ms, 1362ms, 1352ms, 1304ms, 1351ms, 1337ms, 1438ms, 1401ms, 1369ms, 1365ms, 1633ms, 1670ms, 1396ms, 1657ms, 1367ms, 1404ms, 1373ms, 1439ms, 1387ms, 1371ms, 1339ms, 1411ms, 1416ms, 1370ms, 1483ms, 1389ms, 1341ms, 1402ms, 1320ms, 1370ms, 1424ms, 1479ms, 1520ms, 1308ms], Average inference time: 1414.73 ms\r\nModel: ARC_PSD-001_1.1.122_bst_yl80201_float32.tflite, Option: xnnpack, Inference times (ms): [1159ms, 1131ms, 1130ms, 1130ms, 1130ms, 1131ms, 1130ms, 1122ms, 1130ms, 1130ms, 1130ms, 1131ms, 1130ms, 1130ms, 1130ms, 1130ms, 1130ms, 1129ms, 1131ms, 1130ms, 1130ms, 1131ms, 1131ms, 1130ms, 1130ms, 1130ms, 1130ms, 1132ms, 1130ms, 1130ms, 1130ms, 1130ms, 1131ms, 1130ms, 1130ms, 1130ms, 1130ms, 1131ms, 1130ms, 1130ms, 1130ms, 1130ms, 1130ms, 1130ms, 1130ms, 1131ms, 1129ms, 1130ms, 1131ms, 1130ms, 1129ms, 1129ms, 1131ms, 1130ms, 1130ms, 1129ms, 1131ms, 1130ms, 1130ms, 1129ms, 1130ms, 1130ms, 1131ms, 1130ms, 1129ms, 1129ms, 1130ms, 1130ms, 1130ms, 1129ms, 1130ms, 1134ms, 1129ms, 1131ms, 1130ms, 1129ms, 1130ms, 1130ms, 1130ms, 1131ms, 1129ms, 1131ms, 1130ms, 1129ms, 1130ms, 1130ms, 1130ms, 1130ms, 1130ms, 1131ms, 1129ms, 1130ms, 1130ms, 1130ms, 1130ms, 1130ms, 1131ms, 1129ms, 1131ms, 1129ms], Average inference time: 1130.3 ms\r\nModel: ARC_PSD-001_1.1.122_bst_yl80201_float16.tflite, Option: gpu, Inference times (ms): [418ms, 714ms, 771ms, 622ms, 817ms, 814ms, 785ms, 813ms, 810ms, 812ms, 591ms, 812ms, 812ms, 812ms, 815ms, 662ms, 811ms, 812ms, 815ms, 624ms, 810ms, 807ms, 809ms, 811ms, 813ms, 814ms, 810ms, 813ms, 809ms, 809ms, 784ms, 810ms, 810ms, 809ms, 809ms, 770ms, 775ms, 812ms, 811ms, 804ms, 787ms, 809ms, 811ms, 810ms, 663ms, 816ms, 809ms, 812ms, 601ms, 809ms, 811ms, 808ms, 810ms, 809ms, 810ms, 816ms, 811ms, 810ms, 675ms, 809ms, 811ms, 810ms, 624ms, 808ms, 808ms, 813ms, 812ms, 811ms, 810ms, 816ms, 810ms, 809ms, 810ms, 812ms, 809ms, 660ms, 811ms, 806ms, 810ms, 808ms, 808ms, 812ms, 811ms, 820ms, 809ms, 809ms, 814ms, 813ms, 812ms, 811ms, 812ms, 817ms, 809ms, 810ms, 809ms, 811ms, 810ms, 589ms, 812ms, 812ms], Average inference time: 786.15 ms\r\nModel: ARC_PSD-001_1.1.122_bst_yl80201_float16.tflite, Option: nnapi, Inference times (ms): [1156ms, 1128ms, 1127ms, 1127ms, 1127ms, 1127ms, 1127ms, 1128ms, 1127ms, 1127ms, 1127ms, 1127ms, 1128ms, 1127ms, 1126ms, 1127ms, 1129ms, 1128ms, 1128ms, 1128ms, 1128ms, 1129ms, 1128ms, 1127ms, 1128ms, 1127ms, 1128ms, 1127ms, 1127ms, 1128ms, 1127ms, 1127ms, 1128ms, 1127ms, 1128ms, 1128ms, 1127ms, 1128ms, 1128ms, 1127ms, 1127ms, 1128ms, 1128ms, 1128ms, 1127ms, 1129ms, 1128ms, 1127ms, 1129ms, 1127ms, 1128ms, 1127ms, 1127ms, 1128ms, 1130ms, 1126ms, 1127ms, 1127ms, 1127ms, 1127ms, 1128ms, 1127ms, 1127ms, 1127ms, 1127ms, 1130ms, 1128ms, 1127ms, 1127ms, 1129ms, 1127ms, 1127ms, 1128ms, 1127ms, 1127ms, 1127ms, 1127ms, 1128ms, 1128ms, 1127ms, 1128ms, 1127ms, 1127ms, 1127ms, 1127ms, 1127ms, 1127ms, 1129ms, 1127ms, 1127ms, 1127ms, 1123ms, 1127ms, 1128ms, 1127ms, 1127ms, 1127ms, 1128ms, 1126ms, 1128ms], Average inference time: 1127.71 ms\r\nModel: ARC_PSD-001_1.1.122_bst_yl80201_float16.tflite, Option: cpu, Inference times (ms): [1293ms, 1412ms, 1377ms, 1389ms, 1452ms, 1516ms, 1465ms, 1520ms, 1476ms, 1383ms, 1373ms, 1440ms, 1557ms, 1592ms, 1405ms, 1328ms, 1385ms, 1342ms, 1356ms, 1348ms, 1743ms, 1693ms, 1603ms, 1329ms, 1391ms, 1356ms, 1441ms, 1439ms, 1316ms, 1309ms, 1305ms, 1556ms, 1467ms, 1641ms, 1385ms, 1420ms, 1352ms, 1342ms, 1584ms, 1272ms, 1332ms, 1388ms, 1327ms, 1311ms, 1446ms, 1699ms, 1380ms, 1692ms, 1779ms, 1335ms, 1389ms, 1598ms, 1441ms, 1441ms, 1340ms, 1363ms, 1435ms, 1360ms, 1407ms, 1321ms, 1447ms, 1422ms, 1362ms, 1474ms, 1366ms, 1390ms, 1622ms, 1723ms, 1386ms, 1438ms, 1412ms, 1352ms, 1650ms, 1679ms, 1432ms, 1742ms, 1469ms, 1291ms, 1403ms, 1446ms, 1419ms, 1416ms, 1395ms, 1280ms, 1491ms, 1644ms, 1297ms, 1314ms, 1391ms, 1429ms, 1379ms, 1755ms, 1505ms, 1551ms, 1662ms, 1396ms, 1317ms, 1409ms, 1366ms, 1360ms], Average inference time: 1444.19 ms\r\nModel: ARC_PSD-001_1.1.122_bst_yl80201_float16.tflite, Option: xnnpack, Inference times (ms): [1158ms, 1127ms, 1128ms, 1127ms, 1128ms, 1128ms, 1128ms, 1127ms, 1127ms, 1127ms, 1128ms, 1127ms, 1131ms, 1127ms, 1129ms, 1128ms, 1126ms, 1127ms, 1127ms, 1126ms, 1127ms, 1128ms, 1128ms, 1127ms, 1127ms, 1130ms, 1128ms, 1128ms, 1128ms, 1127ms, 1127ms, 1127ms, 1127ms, 1127ms, 1128ms, 1127ms, 1129ms, 1126ms, 1128ms, 1129ms, 1127ms, 1128ms, 1128ms, 1128ms, 1129ms, 1127ms, 1128ms, 1128ms, 1129ms, 1128ms, 1127ms, 1128ms, 1127ms, 1128ms, 1127ms, 1127ms, 1127ms, 1127ms, 1128ms, 1127ms, 1127ms, 1128ms, 1127ms, 1127ms, 1128ms, 1127ms, 1128ms, 1127ms, 1128ms, 1128ms, 1127ms, 1128ms, 1127ms, 1128ms, 1126ms, 1127ms, 1128ms, 1127ms, 1127ms, 1127ms, 1128ms, 1130ms, 1127ms, 1127ms, 1128ms, 1128ms, 1127ms, 1128ms, 1127ms, 1128ms, 1127ms, 1127ms, 1127ms, 1128ms, 1127ms, 1125ms, 1128ms, 1128ms, 1127ms, 1128ms], Average inference time: 1127.84 ms\n```\n",
    "comments": [
      {
        "user": "sawantkumar",
        "body": "Hi @jakubdolejs \r\n\r\nThere could be a bunch of reasons behind performance issues on pixel 4a compared to the iPhone 12. When you use Core ML delegate on the iPhone, it is using NPU which is much faster compared to the gpu on pixel 4a . Can you also benchmark your model on the pixel 4a using [tensorflow profiler](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#parameters) which will give you detailed information regarding your model execution like how many partitions of the model are created before execution and how many layers are falling back to the cpu in case of gpu delegate. Also pixel 4a's GPU is not optimised for fp32 calculations , it is only optimised for fp16 operations , so that could be the culprit behind poor gpu performance while using fp32.  Can you share the tensorflow lite profiler results once you benchmark your tflite model on the pixel using profiler."
      },
      {
        "user": "jakubdolejs",
        "body": "Thank you @sawantkumar. I'll try the profiler and upload the results here."
      },
      {
        "user": "jakubdolejs",
        "body": "Hello @sawantkumar,\r\n\r\nI ran the benchmark tool with different options on the float32 and float16 models. Please see the attached results. The file names ending with `gpu` are from runs that had the `--use_gpu` flag set to `true`. The ones ending with `nnapi` had the `--use_nnapi` flag set to `true`. The commands used to invoke the tests are included in the txt files.\r\n\r\nPlease let me know if you see anything unexpected in the results.\r\n\r\n[fp16_gpu.txt](https://github.com/tensorflow/tensorflow/files/15038893/fp16_gpu.txt)\r\n[fp16_nnapi.txt](https://github.com/tensorflow/tensorflow/files/15038894/fp16_nnapi.txt)\r\n[fp16.txt](https://github.com/tensorflow/tensorflow/files/15038895/fp16.txt)\r\n[fp32_gpu.txt](https://github.com/tensorflow/tensorflow/files/15038897/fp32_gpu.txt)\r\n[fp32_nnapi.txt](https://github.com/tensorflow/tensorflow/files/15038898/fp32_nnapi.txt)\r\n[fp32.txt](https://github.com/tensorflow/tensorflow/files/15038899/fp32.txt)\r\n"
      }
    ]
  },
  {
    "issue_number": 91733,
    "title": "warning in official",
    "author": "lykamspam",
    "state": "closed",
    "created_at": "2025-04-18T12:38:50Z",
    "updated_at": "2025-05-29T18:09:33Z",
    "labels": [
      "type:docs-bug",
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:lite",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nlast\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nCMake Warning (dev) at /home/user/.local/lib/python3.12/site-packages/cmake/data/share/cmake-3.30/Modules/FetchContent.cmake:1953 (message):\n  Calling FetchContent_Populate(abseil-cpp) is deprecated, call\n  FetchContent_MakeAvailable(abseil-cpp) instead.  Policy CMP0169 can be set\n  to OLD to allow FetchContent_Populate(abseil-cpp) to be called directly for\n  now, but the ability to call it with declared details will be removed\n  completely in a future version.\nCall Stack (most recent call first):\n  /home/user/tf/tensorflow/tensorflow/lite/tools/cmake/modules/OverridableFetchContent.cmake:537 (FetchContent_Populate)\n  /home/user/tf/tensorflow/tensorflow/lite/tools/cmake/modules/abseil-cpp.cmake:35 (OverridableFetchContent_Populate)\n  /home/user/tf/tensorflow/tensorflow/lite/tools/cmake/modules/Findabsl.cmake:18 (include)\n  /home/user/tf/tensorflow/tensorflow/lite/CMakeLists.txt:166 (find_package)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n\nCMake Warning at /home/user/tf/tensorflow/my/abseil-cpp/CMakeLists.txt:77 (message):\n  A future Abseil release will default ABSL_PROPAGATE_CXX_STD to ON for CMake\n  3.8 and up.  We recommend enabling this option to ensure your project still\n  builds correctly.\n\n\n### Standalone code to reproduce the issue\n\n```shell\noficial\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @lykamspam ,\nApologies for the delay, and thank you for raising your concern here. You can safely ignore these warnings for now, as they do not impact the functionality or the build output. We appreciate your feedback.\nThank you!"
      },
      {
        "user": "lykamspam",
        "body": "tutorial https://www.tensorflow.org/install/source?hl=pl\n```\ngit clone https://github.com/tensorflow/tensorflow.git\ncd tensorflow\n```\n./configure\nYou have bazel 7.4.1 installed.\nPlease specify the location of python. [Default is /usr/bin/python3]: \n\n\nFound possible Python library paths:\n  /usr/lib/python3/dist-packages\n  /usr/local/lib/python3.12/dist-packages\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\n\nDo you wish to build TensorFlow with ROCm support? [y/N]: \nNo ROCm support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\nCUDA support will be enabled for TensorFlow.\n\nPlease specify the hermetic CUDA version you want to use or leave empty to use the default version. \n\n\nPlease specify the hermetic cuDNN version you want to use or leave empty to use the default version. \n\n\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: \n\n\nPlease specify the local CUDA path you want to use or leave empty to use the default version. \n\n\nPlease specify the local CUDNN path you want to use or leave empty to use the default version. \n\n\nPlease specify the local NCCL path you want to use or leave empty to use the default version. \n\n\nDo you want to use clang as CUDA compiler? [Y/n]: y\nClang will be used as CUDA compiler.\n\nPlease specify clang path that to be used as host compiler. [Default is /usr/bin/clang]: \n\n\nYou have Clang 19.1.1 installed.\n\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: \n\n\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \nNot configuring the WORKSPACE for Android builds.\n\n```\nbazel build //tensorflow/tools/pip_package:wheel \\\n--repo_env=USE_PYWRAP_RULES=1 --repo_env=WHEEL_NAME=tensorflow --config=cuda \\\n--config=cuda_wheel --config=opt\n```\n[...]\nd5ce4b558cdc683acf86f0c4c8e6f4d9/external/upb/BUILD:57:11: Compiling upb/upb.c failed: (Exit 1): clang failed: error executing CppCompile command (from target @@upb//:upb) /usr/lib/llvm-19/bin/clang -MD -MF bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/upb/_objs/upb/upb.pic.o' -iquote external/upb -iquote ... (remaining 43 arguments skipped)\nexternal/upb/upb/upb.c:192:10: error: defining a type within 'offsetof' is a C23 extension [-Werror,-Wc23-extensions]\n  192 |   n &= ~(upb_alignof(upb_arena) - 1);\n      |          ^~~~~~~~~~~~~~~~~~~~~~\nexternal/upb/upb/upb.c:183:37: note: expanded from macro 'upb_alignof'\n  183 | #define upb_alignof(type) offsetof (struct { char c; type member; }, member)\n      |                                     ^~~~~~\n/usr/lib/llvm-19/lib/clang/19/include/__stddef_offsetof.h:16:43: note: expanded from macro 'offsetof'\n   16 | #define offsetof(t, d) __builtin_offsetof(t, d)\n      |                                           ^\n1 error generated.\nTarget //tensorflow/tools/pip_package:wheel failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 40.683s, Critical Path: 8.74s\nINFO: 6214 processes: 5106 internal, 1108 local.\nERROR: Build did NOT complete successfully\n\n\n\nubuntu \nuname -a\nLinux 6.11.0-24-generic #24-Ubuntu SMP PREEMPT_DYNAMIC Fri Mar 14 18:13:56 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n"
      },
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @lykamspam \n\nThank you for bringing this to our attention, the first warning occurs because `CMake 3.30` has deprecated direct calls to `FetchContent_Populate() `with just a dependency name as an argument:\n\nIn `CMake 3.11-3.13` projects had to use the pattern:\n\n```\nFetchContent_GetProperties(depname)\nif(NOT depname_POPULATED)\n    FetchContent_Populate(depname)\n    add_subdirectory(${depname_SOURCE_DIR} ${depname_BINARY_DIR})\nendif()\n```\n\nSince `CMake 3.14` this approach has been replaced by `FetchContent_MakeAvailable()` please refer this [official announcement](https://discourse.cmake.org/t/rfc-deprecating-direct-calls-to-fetchcontent-populate/8161) . The TensorFlow Lite CMake files are still using the older pattern in their `OverridableFetchContent.cmake` module so that warning message showing\n\nThe second warning is from the `Abseil` library itself notifying you that a future version will change the default value of `ABSL_PROPAGATE_CXX_STD` to `ON`. it's purely informational about a future change and doesn't affect the current build the simplest approach is to just ignore this specific warning.\n\nI see from your above mentioned log currently you're using the `Clang 19` version the most reliable solution is to use a version of Clang that is known to be well-supported by the current TensorFlow versions (e.g.`Clang 17` or `Clang 18`) so please give it try with below [TensorFlow's tested build configuration versions](https://www.tensorflow.org/install/source).\n\n\nVersion | Python version | Compiler | Build tools\n-- | -- | -- | --\ntensorflow-2.19.0 | 3.9-3.12 | Clang 18.1.8 | Bazel 6.5.0\ntensorflow-2.18.0 | 3.9-3.12 | Clang 17.0.6 | Bazel 6.5.0\n\nThank you for your cooperation and understanding.\n\n"
      }
    ]
  },
  {
    "issue_number": 94120,
    "title": "FPE in oneDNN `MaxPool3D` with Invalid Dimensions",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-24T18:18:45Z",
    "updated_at": "2025-05-29T15:47:39Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.3\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow crashes with Floating Point Exception (FPE) when using `tf.raw_ops.MaxPool3D` with oneDNN optimizations enabled (`TF_ENABLE_ONEDNN_OPTS=1`). The issue is specific to oneDNN-optimized code paths and does not occur with oneDNN disabled.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\nimport os\n\n# CRITICAL: Crash only occurs with oneDNN enabled\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n\n# Create input tensor with problematic dimensions and extreme bfloat16 values\ninput_data = np.full((9, 6, 7, 4, 4), 5.00741786e-32, dtype=np.float32)\ninput_tensor = tf.constant(input_data, dtype=tf.bfloat16)\n\n# Pooling parameters that create invalid output dimensions\nksize = [1, 4, 5, 5, 1]      # [batch, depth, height, width, channels]\nstrides = [1, 3, 2, 1, 1]    # [batch, depth, height, width, channels]\npadding = \"VALID\"\ndata_format = \"NDHWC\"\n\n# This call triggers division by zero in oneDNN\nresult = tf.raw_ops.MaxPool3D(\n    input=input_tensor,\n    ksize=ksize,\n    strides=strides,\n    padding=padding,\n    data_format=data_format\n)\n```\n\n### Relevant log output\n\n```shell\n2025-05-24 18:14:23.759566: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-05-24 18:14:23.807014: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-05-24 18:14:24.775612: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\nTensorFlow version: 2.20.0-dev20250516\n2025-05-24 18:14:25.002516: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\nInput parameters:\nInput shape: (9, 6, 7, 4, 4)\nInput dtype: <dtype: 'bfloat16'>\nKsize: [1, 4, 5, 5, 1]\nStrides: [1, 3, 2, 1, 1]\nPadding: VALID\nData format: NDHWC\n\nInput values (first few): [5.00742e-32 5.00742e-32 5.00742e-32 5.00742e-32 5.00742e-32 5.00742e-32\n 5.00742e-32 5.00742e-32 5.00742e-32 5.00742e-32]\nFloating point exception (core dumped)\n\n\nFull Stack Trace:\n\n#0 dnnl::impl::utils::div_up<int, int>(int, int) /external/onednn/src/common/utils.hpp:313:72\n#1 dnnl::impl::cpu::x64::jit_uni_pool_kernel<...>::generate()::lambda(int, bool) /external/onednn/src/cpu/x64/jit_uni_pool_kernel.cpp:1543:37\n#2 dnnl::impl::cpu::x64::jit_uni_pool_kernel<...>::generate() /external/onednn/src/cpu/x64/jit_uni_pool_kernel.cpp:1605:5\n#3 dnnl::impl::cpu::x64::jit_generator::create_kernel() /external/onednn/src/cpu/x64/jit_generator.hpp:2723:9\n#4 dnnl::impl::cpu::x64::jit_uni_pooling_fwd_t<...>::init(dnnl_engine*) /external/onednn/src/cpu/x64/jit_uni_pooling.cpp:501:21\n...\n#22 tensorflow::MklMaxPoolingOp<Eigen::ThreadPoolDevice, Eigen::bfloat16, true>::Compute(tensorflow::OpKernelContext*) /tensorflow/core/kernels/mkl/mkl_maxpooling_op.cc:152:21\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @SilentTester73 ,\nApologies for the delay, and thank you for raising your concern. I tried running your code on Colab using TensorFlow version 2.19.0 as well as the latest nightly build, and I was not able to reproduce the issue it worked fine in both cases.\nPlease keep in mind that nightly versions of TensorFlow are updated frequently and may sometimes introduce unexpected behavior. For better stability and reproducibility, I recommend using the latest stable release, such as TensorFlow 2.19.0.\nI am sharing a [gist](https://colab.sandbox.google.com/gist/Venkat6871/d12fdf408bc512fc3b296226e1285707/94120_tf_2-19-0-nightly-v.ipynb) here for your reference hope it helps!\n\nThank you!"
      },
      {
        "user": "SilentTester73",
        "body": "Hi @Venkat6871,\nThanks for looking into this! I understand that Colab couldn't reproduce the issue. I've tested on my local machines and Colab. Colab seems to have OneDNN disabled, while it's enabled by default on physical computers when installing TensorFlow via pip (if NVIDIA GPU is not presence).\n\nWould you mind trying to reproduce this in an environment with ONEDNN enabled? I'm confident you'll encounter the same issue, even with the nightly build."
      }
    ]
  },
  {
    "issue_number": 91782,
    "title": "Compiling TFLite introductory example fails due to missing headers",
    "author": "lykamspam",
    "state": "closed",
    "created_at": "2025-04-19T08:23:58Z",
    "updated_at": "2025-05-29T13:40:59Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:lite",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nlast\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nhttps://www.tensorflow.org/install/lang_c\n\n### Current behavior?\n\n gcc -I. hello_tf.c -ltensorflow -o hello_tf\nIn file included from ./tensorflow/c/c_api.h:26,\n                 from hello_tf.c:3:\n./tensorflow/c/tf_status.h:20:10: fatal error: xla/tsl/c/tsl_status.h: Nie ma takiego pliku ani katalogu\n   20 | #include \"xla/tsl/c/tsl_status.h\"\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps://www.tensorflow.org/install/lang_c\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @lykamspam ,\nApologies for the delay, and thank you for raising your concern here. I tried running the provided documentation using TensorFlow version 2.18.0, and I did not encounter any issues on my end. Could you please check which version you are using and ensure that all components are compatible with your operating system? Here, I am providing a [gist](https://colab.sandbox.google.com/gist/Venkat6871/4fd8c98074a0e108425d04e52781dd9c/lang_c.ipynb) for your reference.\nThank you!"
      },
      {
        "user": "lykamspam",
        "body": "What is official example/tutorial for making simple xor in C (lite c etc.)\n\n\n---------------\nclone tensorflow \n\nhttps://ai.google.dev/edge/litert/inference?hl=pl&authuser=0\n\nbazel build\n\n```\n$gcc -o prog prog.c -I./tensorflow -L./bazel-bin/tensorflow/lite -ltensorflowlite\nIn file included from ./tensorflow/tensorflow/lite/interpreter.h:21,\n                 from prog.c:2:\n./tensorflow/tensorflow/lite/core/interpreter.h:31:10: fatal error: atomic: Nie ma takiego pliku ani katalogu\n   31 | #include <atomic>\n      |          ^~~~~~~~\ncompilation terminated.\nmake: ***  Błąd 1\n```\n\n#include <stdio.h>\n#include <tensorflow/lite/interpreter.h>\n#include <tensorflow/lite/model.h>\n#include <tensorflow/lite/kernels/register.h>\n#include <tensorflow/lite/optional_debug_tools.h>\n\nint main()\n{\n // Wczytanie modelu\n const char *model_file = \"xor_model.tflite\";\n TfLiteModel *model = TfLiteModelCreateFromFile(model_file);\n if (model == NULL)\n  { fprintf(stderr, \"Failed to load the model\\n\"); return 1; }\n\n // Utworzenie interpreter\n TfLiteInterpreterOptions *options = TfLiteInterpreterOptionsCreate();\n TfLiteInterpreter *interpreter = TfLiteInterpreterCreate(model, options);\n if (interpreter == NULL)\n  {\n        fprintf(stderr, \"Failed to create the interpreter\\n\");\n        return 1;\n    }\n\n    // Alokacja pamięci\n    if (TfLiteInterpreterAllocateTensors(interpreter) != kTfLiteOk) {\n        fprintf(stderr, \"Failed to allocate tensors\\n\");\n        return 1;\n    }\n\n    // Wejście i wyjście\n    float input_data[2] = {0, 1}; // Przykładowe dane wejściowe\n    float *output_data;\n\n    // Ustawienie danych wejściowych\n    memcpy(TfLiteInterpreterGetInputTensor(interpreter, 0)->data.f, input_data, sizeof(input_data));\n\n    // Wykonanie inferencji\n    if (TfLiteInterpreterInvoke(interpreter) != kTfLiteOk) {\n        fprintf(stderr, \"Failed to invoke the interpreter\\n\");\n        return 1;\n    }\n\n    // Odczytanie wyniku\n    output_data = TfLiteInterpreterGetOutputTensor(interpreter, 0)->data.f;\n\n    printf(\"Wynik XOR: %f\\n\", output_data[0]);\n\n // Sprzątanie\n TfLiteInterpreterDelete(interpreter);\n TfLiteModelDelete(model);\n TfLiteInterpreterOptionsDelete(options);\n\nreturn 0;\n}\n"
      },
      {
        "user": "lykamspam",
        "body": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/README.md\n\nminimal this same\n------------\n....\n\n/home/user/tf/minimal_build/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:223:16:   required from ‘void EigenForTFLite::internal::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, RhsMapper>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = float; RhsScalar = float; StorageIndex = long int; OutputMapper = EigenForTFLite::internal::blas_data_mapper<float, long int, 0, 0, 1>; LhsMapper = EigenForTFLite::internal::TensorContractionInputMapper<float, long int, 1, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, true, false, 0, EigenForTFLite::MakePointer>; RhsMapper = EigenForTFLite::internal::TensorContractionInputMapper<float, long int, 0, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> > >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, false, false, 0, EigenForTFLite::MakePointer>; RhsBlock = float*; typename RhsMapper::SubMapper = EigenForTFLite::internal::TensorContractionSubMapper<float, long int, 0, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> > >, EigenForTFLite::ThreadPoolDevice>, std::array<long int, 1>, std::array<long int, 1>, 4, false, false, 0, EigenForTFLite::MakePointer>]’\n  223 |     RhsPacker()(*rhsBlock, data_mapper, depth, cols);\n      |     ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/home/user/tf/minimal_build/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h:857:24:   required from ‘void EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, EigenForTFLite::ThreadPoolDevice>::EvalParallelContext<DoneCallback, lhs_inner_dim_contiguous, rhs_inner_dim_contiguous, rhs_inner_dim_reordered, Alignment>::pack_rhs(EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, EigenForTFLite::ThreadPoolDevice>::Index, EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, EigenForTFLite::ThreadPoolDevice>::Index) [with DoneCallback = EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel>, EigenForTFLite::ThreadPoolDevice>::NoCallback; bool lhs_inner_dim_contiguous = true; bool rhs_inner_dim_contiguous = false; bool rhs_inner_dim_reordered = false; int Alignment = 0; Indices = const std::array<EigenForTFLite::IndexPair<long int>, 1>; LeftArgType = const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> > >; RightArgType = const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> >; OutputKernelType = const EigenForTFLite::NoOpOutputKernel; EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorContractionOp<Dimensions, LhsXprType, RhsXprType, OutputKernelType>, EigenForTFLite::ThreadPoolDevice>::Index = long int]’\n  857 |         kernel_.packRhs(&packed_rhs(n, k, n1, use_thread_local), rhs_.getSubMapper(k * bk_, n1 * bn_), bk(k), bn(n1));\n      |         ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/home/user/tf/minimal_build/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h:979:11:   [ skipping 8 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\n/home/user/tf/minimal_build/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:145:112:   required from ‘bool EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorReshapingOp<NewDimensions, XprType>, Device>::evalSubExprsIfNeeded(EvaluatorPointerType) [with NewDimensions = const EigenForTFLite::DSizes<long int, 4>; ArgType = const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel>; Device = EigenForTFLite::ThreadPoolDevice; EvaluatorPointerType = float*]’\n  145 |   EIGEN_STRONG_INLINE bool evalSubExprsIfNeeded(EvaluatorPointerType data) { return m_impl.evalSubExprsIfNeeded(data); }\n      |                                                                                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~\n/home/user/tf/minimal_build/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:138:44:   required from ‘bool EigenForTFLite::TensorEvaluator<const EigenForTFLite::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalSubExprsIfNeeded(EvaluatorPointerType) [with LeftArgType = EigenForTFLite::TensorMap<EigenForTFLite::Tensor<float, 4, 1, long int>, 16>; RightArgType = const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 4>, const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel> >; Device = EigenForTFLite::ThreadPoolDevice; EvaluatorPointerType = float*]’\n  138 |     return m_rightImpl.evalSubExprsIfNeeded(m_leftImpl.data());\n      |            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~\n/home/user/tf/minimal_build/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:303:61:   required from ‘static void EigenForTFLite::internal::TensorExecutor<Expression, EigenForTFLite::ThreadPoolDevice, Vectorizable, Tiling>::run(const Expression&, const EigenForTFLite::ThreadPoolDevice&) [with Expression = const EigenForTFLite::TensorAssignOp<EigenForTFLite::TensorMap<EigenForTFLite::Tensor<float, 4, 1, long int>, 16>, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 4>, const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel> > >; bool Vectorizable = true; EigenForTFLite::internal::TiledEvaluation Tiling = EigenForTFLite::internal::Off]’\n  303 |     const bool needs_assign = evaluator.evalSubExprsIfNeeded(nullptr);\n      |                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~\n/home/user/tf/minimal_build/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:40:60:   required from ‘EigenForTFLite::TensorDevice<ExpressionType, DeviceType>& EigenForTFLite::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived&) [with OtherDerived = EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 4>, const EigenForTFLite::TensorContractionOp<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const EigenForTFLite::TensorMap<const EigenForTFLite::Tensor<float, 4, 1, long int>, 16> >, const EigenForTFLite::NoOpOutputKernel> >; ExpressionType = EigenForTFLite::TensorMap<EigenForTFLite::Tensor<float, 4, 1, long int>, 16>; DeviceType = EigenForTFLite::ThreadPoolDevice]’\n   40 |     internal::TensorExecutor<const Assign, DeviceType>::run(assign, m_device);\n      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~\n/home/user/tf/tensorflow/tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h:129:29:   required from ‘void tflite::multithreaded_ops::EigenTensorConvFunctor<T>::operator()(const EigenForTFLite::ThreadPoolDevice&, const T*, int, int, int, int, const T*, int, int, int, int, int, int, int, tflite::PaddingType, T*, int, int) [with T = float]’\n  129 |       output.device(device) =\n      |       ~~~~~~~~~~~~~~~~~~~~~~^\n  130 |           Eigen::SpatialConvolution(input, filter, stride_cols, stride_rows,\n      |           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  131 |                                     RuntimePadding2EigenPadding(padding));\n      |                                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/home/user/tf/tensorflow/tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h:171:15:   required from here\n  171 |   conv_functor(device, input_data, batches, input_height, input_width,\n      |   ~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  172 |                input_depth, filter_data, filter_height, filter_width,\n      |                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  173 |                output_depth, stride_height, stride_width, pad_height, pad_width,\n      |                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  174 |                padding, output_data, output_height, output_width);\n      |                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/home/user/tf/tensorflow/third_party/xla/xla/tsl/framework/convolution/eigen_spatial_convolutions-inl.h:603:63: warning: ignoring attributes on template argument ‘EigenForTFLite::internal::packet_traits<float>::type’ {aka ‘__m128’} [-Wignored-attributes]\n  603 |     const int packetSize = internal::unpacket_traits<Packet>::size;\n      |                                                               ^~~~\ngmake[1]: *** [CMakeFiles/Makefile2:1515: tensorflow-lite/CMakeFiles/tensorflow-lite.dir/all] Błąd 2\ngmake: *** [Makefile:136: all] Błąd 2\n"
      }
    ]
  },
  {
    "issue_number": 94382,
    "title": "Crash in `tf.raw_ops.MaxPoolGradGradWithArgmax` when executing on GPU",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-28T14:40:21Z",
    "updated_at": "2025-05-29T13:12:48Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.20.0-dev20250526\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux CPU & GPU\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.13.2\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\nCUDA 12.5.1\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow crashes when executing `tf.raw_ops.MaxPoolGradGradWithArgmax` on GPU, but runs fine on CPU.\n\nThe issue can be reproduced in this Colab notebook: https://colab.research.google.com/drive/1oXq25c_qxoT2QD1zNt_IBPM6bdslAtB6?usp=sharing\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nwith tf.device('/cpu:0'):\n    try:\n        input_tensor = tf.constant([0, 0, 0, 0, 0], dtype=tf.float32, shape=[5,1,1,1])\n        grad = tf.constant([0, 0, 0, 0, 0], dtype=tf.float32, shape=[5,1,1,1])\n        argmax = tf.constant([0, 25288767438848, -1099511627776, -1, 4294967295], dtype=tf.int64, shape=[5,1,1,1])\n        ksize = [1, 1, 1, 1]\n        strides = [1, 1, 1, 1]\n        padding = 'VALID'\n        include_batch_in_index = True\n        \n        result = tf.raw_ops.MaxPoolGradGradWithArgmax(\n            input=input_tensor,\n            grad=grad,\n            argmax=argmax,\n            ksize=ksize,\n            strides=strides,\n            padding=padding,\n            include_batch_in_index=include_batch_in_index,\n            name=None\n        )\n        print(\"MaxPoolGradGradWithArgmax executed successfully on CPU.\")\n    except Exception as e:\n        print(f\"Exception on CPU: {e}\")\n\nwith tf.device('/gpu:0'):\n    try:\n        input_tensor = tf.constant([0, 0, 0, 0, 0], dtype=tf.float32, shape=[5,1,1,1])\n        grad = tf.constant([0, 0, 0, 0, 0], dtype=tf.float32, shape=[5,1,1,1])\n        argmax = tf.constant([0, 25288767438848, -1099511627776, -1, 4294967295], dtype=tf.int64, shape=[5,1,1,1])\n        ksize = [1, 1, 1, 1]\n        strides = [1, 1, 1, 1]\n        padding = 'VALID'\n        include_batch_in_index = True\n        \n        result = tf.raw_ops.MaxPoolGradGradWithArgmax(\n            input=input_tensor,\n            grad=grad,\n            argmax=argmax,\n            ksize=ksize,\n            strides=strides,\n            padding=padding,\n            include_batch_in_index=include_batch_in_index,\n            name=None\n        )\n        print(\"MaxPoolGradGradWithArgmax executed successfully on GPU.\")\n    except Exception as e:\n        print(f\"Exception on GPU: {e}\")\n```\n\n### Relevant log output\n\n```shell\nOutput:\n\n\nI0000 00:00:1748442135.660934 1229154 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2622 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\nMaxPoolGradGradWithArgmax executed successfully on CPU.\nF0000 00:00:1748442136.223198 1229230 device_event_mgr.cc:226] Unexpected Event status: 1\n*** Check failure stack trace: ***\n    @     0x7f8949a43424  absl::lts_20230802::log_internal::LogMessage::SendToLog()\n    @     0x7f8949a42dc4  absl::lts_20230802::log_internal::LogMessage::Flush()\n    @     0x7f8949a438b9  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()\n    @     0x7f8947cf97f3  tensorflow::EventMgr::PollEvents()::$_0::operator()<>()\n    @     0x7f8947cf8edb  tensorflow::EventMgr::PollEvents()\n    @     0x7f8947cf8b89  tensorflow::EventMgr::PollLoop()\n    @     0x7f8949766314  Eigen::ThreadPoolTempl<>::WorkerLoop()\n    @     0x7f8949766191  std::__invoke_impl<>()\n    @     0x7f894975228f  tsl::(anonymous namespace)::PThread::ThreadFn()\n    @     0x7f894b2417eb  (unknown)\n    @     0x7f894b2c518c  (unknown)\nAborted (core dumped)\n\n\nError Logs on Colab:\n\n\nI0000 00:00:1748442857.184649     177 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nF0000 00:00:1748442857.444401    1608 device_event_mgr.cc:226] Unexpected Event status: 1\n*** Check failure stack trace: ***\n    @     0x7eee0fb53424  absl::lts_20230802::log_internal::LogMessage::SendToLog()\n    @     0x7eee0fb52dc4  absl::lts_20230802::log_internal::LogMessage::Flush()\n    @     0x7eee0fb538b9  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()\n    @     0x7eee0de097f3  tensorflow::EventMgr::PollEvents()::$_0::operator()<>()\n    @     0x7eee0de08edb  tensorflow::EventMgr::PollEvents()\n    @     0x7eee0de08b89  tensorflow::EventMgr::PollLoop()\n    @     0x7eee0f876314  Eigen::ThreadPoolTempl<>::WorkerLoop()\n    @     0x7eee0f876191  std::__invoke_impl<>()\n    @     0x7eee0f86228f  tsl::(anonymous namespace)::PThread::ThreadFn()\n    @     0x7eee5d1a4ac3  (unknown)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/b3263a5f0f3431ea4ac2fdf3495a3959/94382_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94378,
    "title": "Inconsistent Error Handling in `tf.raw_ops.SparseToDense` Between CPU and GPU Implementations",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-28T13:45:21Z",
    "updated_at": "2025-05-29T13:01:41Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.20.0-dev20250526\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux CPU & GPU\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.13.2\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\nCUDA 12.5.1\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe `tf.raw_ops.SparseToDense` function exhibits inconsistent behavior between CPU and GPU implementations.\n\nWhen provided with out-of-range index `0`, the CPU version raise an exception normally while the GPU version acquiesces to negative indexing or causes an abort.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nwith tf.device('/cpu:0'):\n    try:\n        sparse_indices = tf.constant([1], dtype=tf.int32, shape=[1])\n        output_shape = tf.constant([0], dtype=tf.int32, shape=[1])\n        sparse_values = tf.constant([0], dtype=tf.uint16, shape=[1])\n        default_value = tf.constant([0], dtype=tf.uint16, shape=[])\n        validate_indices = False\n        result = tf.raw_ops.SparseToDense(\n            sparse_indices=sparse_indices,\n            output_shape=output_shape,\n            sparse_values=sparse_values,\n            default_value=default_value,\n            validate_indices=validate_indices,\n            name=None\n        )\n        print(\"SparseToDense executed successfully on CPU\")\n    except Exception as e:\n        print(\"Exception on CPU:\", e)\n\nwith tf.device('/gpu:0'):\n    try:\n        sparse_indices = tf.constant([1], dtype=tf.int32, shape=[1])\n        output_shape = tf.constant([0], dtype=tf.int32, shape=[1])\n        sparse_values = tf.constant([0], dtype=tf.uint16, shape=[1])\n        default_value = tf.constant([0], dtype=tf.uint16, shape=[])\n        validate_indices = False\n        result = tf.raw_ops.SparseToDense(\n            sparse_indices=sparse_indices,\n            output_shape=output_shape,\n            sparse_values=sparse_values,\n            default_value=default_value,\n            validate_indices=validate_indices,\n            name=None\n        )\n        print(\"SparseToDense executed successfully on GPU\")\n    except Exception as e:\n        print(\"Exception on GPU:\", e)\n```\n\n### Relevant log output\n\n```shell\nOutput:\n\n\nI0000 00:00:1748439889.789792 1199715 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1796 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n2025-05-28 21:44:49.808211: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Indices are not valid (out of bounds).  Shape: [0]\nException on CPU: {{function_node __wrapped__SparseToDense_device_/job:localhost/replica:0/task:0/device:CPU:0}} Indices are not valid (out of bounds).  Shape: [0] [Op:SparseToDense] name: \nSparseToDense executed successfully on GPU\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/926be6b5c03509dc1add7d0fc742e942/94378_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94376,
    "title": "Inconsistent Error Handling in `tf.raw_ops.SparseSegmentSqrtNGradV2` Between CPU and GPU Implementations",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-28T13:33:44Z",
    "updated_at": "2025-05-29T12:52:55Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.20.0-dev20250526\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux CPU & GPU\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.13.2\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\nCUDA 12.5.1\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nBug similar to tensorflow/tensorflow#94151\n\nThe `tf.raw_ops.SparseSegmentSqrtNGradV2` function exhibits inconsistent behavior between CPU and GPU implementations.\n\nWhen provided with out-of-range negative index, the CPU version raise an exception normally while the GPU version acquiesces to negative indexing or causes an abort.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nwith tf.device('/cpu:0'):\n    try:\n        grad = tf.constant([0, 0, 0, 0], dtype=tf.float64, shape=[4])\n        indices = tf.constant([-1], dtype=tf.int64, shape=[1])\n        segment_ids = tf.constant([-1], dtype=tf.int64, shape=[1])\n        dense_output_dim0 = tf.constant([1], dtype=tf.int32, shape=[])\n\n        tf.raw_ops.SparseSegmentSqrtNGradV2(\n            grad=grad,\n            indices=indices,\n            segment_ids=segment_ids,\n            dense_output_dim0=dense_output_dim0,\n            name=None\n        )\n        print(\"SparseSegmentSqrtNGradV2 executed successfully on CPU\")\n    except Exception as e:\n        print(f\"Exception on CPU: {e}\")\n        \nwith tf.device('/gpu:0'):\n    try:\n        grad = tf.constant([0, 0, 0, 0], dtype=tf.float64, shape=[4])\n        indices = tf.constant([-1], dtype=tf.int64, shape=[1])\n        segment_ids = tf.constant([-1], dtype=tf.int64, shape=[1])\n        dense_output_dim0 = tf.constant([1], dtype=tf.int32, shape=[])\n\n        tf.raw_ops.SparseSegmentSqrtNGradV2(\n            grad=grad,\n            indices=indices,\n            segment_ids=segment_ids,\n            dense_output_dim0=dense_output_dim0,\n            name=None\n        )\n        print(\"SparseSegmenSqrtNGradV2 executed successfully on GPU\")\n    except Exception as e:\n        print(f\"Exception on GPU: {e}\")\n```\n\n### Relevant log output\n\n```shell\nOutput:\n\n\nI0000 00:00:1748438686.325530  285106 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:c1:00.0, compute capability: 7.0\nI0000 00:00:1748438686.326316  285106 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31029 MB memory:  -> device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:e1:00.0, compute capability: 7.0\n2025-05-28 13:24:46.358801: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Index -1 out of range [0, 1).\nException on CPU: {{function_node __wrapped__SparseSegmentSqrtNGradV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index -1 out of range [0, 1). [Op:SparseSegmentSqrtNGradV2] name: \nSparseSegmenSqrtNGradV2 executed successfully on GPU\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/3fe4c4ea421f9596f55d72528e322fc0/94376_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94151,
    "title": "Inconsistent Error Handling in `tf.raw_ops.SparseSegmentSumGradV2` Between CPU and GPU Implementations",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-26T16:54:29Z",
    "updated_at": "2025-05-29T12:47:58Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.20.0-dev20250526\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux CPU & GPU\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.13.2\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\nCUDA 12.5.1\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe `tf.raw_ops.SparseSegmentSumGradV2` function exhibits inconsistent behavior between CPU and GPU implementations.\n\nWhen provided with  out-of-range negative index, the CPU version raise an exception normally while the GPU version acquiesces to negative indexing or causes an abort.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nwith tf.device('/cpu:0'):\n    try:\n        grad = tf.constant([0, 0, 0, 0], dtype=tf.float64, shape=[4])\n        indices = tf.constant([-1], dtype=tf.int64, shape=[1])\n        segment_ids = tf.constant([-1], dtype=tf.int64, shape=[1])\n        dense_output_dim0 = tf.constant([1], dtype=tf.int32, shape=[])\n\n        tf.raw_ops.SparseSegmentSumGradV2(\n            grad=grad,\n            indices=indices,\n            segment_ids=segment_ids,\n            dense_output_dim0=dense_output_dim0,\n            name=None\n        )\n        print(\"SparseSegmentSumGradV2 executed successfully on CPU\")\n    except Exception as e:\n        print(f\"Exception on CPU: {e}\")\n        \nwith tf.device('/gpu:0'):\n    try:\n        grad = tf.constant([0, 0, 0, 0], dtype=tf.float64, shape=[4])\n        indices = tf.constant([-1], dtype=tf.int64, shape=[1])\n        segment_ids = tf.constant([-1], dtype=tf.int64, shape=[1])\n        dense_output_dim0 = tf.constant([1], dtype=tf.int32, shape=[])\n\n        tf.raw_ops.SparseSegmentSumGradV2(\n            grad=grad,\n            indices=indices,\n            segment_ids=segment_ids,\n            dense_output_dim0=dense_output_dim0,\n            name=None\n        )\n        print(\"SparseSegmentSumGradV2 executed successfully on GPU\")\n    except Exception as e:\n        print(f\"Exception on GPU: {e}\")\n```\n\n### Relevant log output\n\n```shell\nOutput:\n\n\nI0000 00:00:1748278378.253343  624834 gpu_device.cc:2018] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2033 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n2025-05-27 00:52:58.280430: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Index -1 out of range [0, 1).\nException on CPU: {{function_node __wrapped__SparseSegmentSumGradV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index -1 out of range [0, 1). [Op:SparseSegmentSumGradV2] name: \nSparseSegmentSumGradV2 executed successfully on GPU\n```",
    "comments": [
      {
        "user": "sharktide",
        "body": "We could probably include a check for negaive indicies before doing continuing with the job. To me, this seems like a we-overlooked-this kinda issue."
      },
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/76fec8ecb99ec713182cbe9054a8fab1/94151_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94134,
    "title": "Fatal crash in `tf.raw_ops.TensorScatterMax` with malformed indices",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-25T20:00:40Z",
    "updated_at": "2025-05-29T12:08:16Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux CPU\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.3\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.TensorScatterMax` causes a fatal crash with Aborted (core dumped) when provided with malformed indices tensor. \n\nThe issue can be reproduced in this Google Colab notebook: https://colab.research.google.com/drive/19n9h_phK6X1usawpwCdp1S9hFnQR5Q9N?usp=sharing\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\nprint(\"TensorFlow version:\", tf.__version__)\n\n# Create tensors with problematic shapes\ninput_tensor = tf.constant(np.random.random((1, 10, 4)), dtype=tf.float64)\nindices_tensor = tf.constant([[[0]]], dtype=tf.int64)  # Malformed: shape (1,1,1) instead of (1,3)\nupdates_tensor = tf.constant([0.0], dtype=tf.float64)\n\nprint(\"Input Tensor shape:\", input_tensor.shape)\nprint(\"Indices Tensor shape:\", indices_tensor.shape) \nprint(\"Updates Tensor shape:\", updates_tensor.shape)\n\nprint(\"--- Testing TensorScatterMax ---\")\n# This causes a fatal crash\nresult = tf.raw_ops.TensorScatterMax(\n    tensor=input_tensor,\n    indices=indices_tensor,\n    updates=updates_tensor\n)\n```\n\n### Relevant log output\n\n```shell\n2025-05-25 19:56:43.286749: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/5ad677ca7f59122b40b0834007cddfde/94134_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94132,
    "title": "Fatal crash in `tf.raw_ops.TensorScatterMin` with malformed indices",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-25T19:54:54Z",
    "updated_at": "2025-05-29T12:03:10Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.TensorScatterMin` causes a fatal crash with Aborted (core dumped) when provided with malformed indices tensor. \n\nThe issue can be reproduced in this Google Colab notebook: https://colab.research.google.com/drive/1XZ33kWSTCkKA_INqY4QOvgbvyVyeND2l?usp=sharing\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\nprint(\"TensorFlow version:\", tf.__version__)\n\n# Create tensors with problematic shapes\ninput_tensor = tf.constant(np.random.random((1, 10, 4)), dtype=tf.float64)\nindices_tensor = tf.constant([[[0]]], dtype=tf.int64)  # Malformed: shape (1,1,1) instead of (1,3)\nupdates_tensor = tf.constant([0.0], dtype=tf.float64)\n\nprint(\"Input Tensor shape:\", input_tensor.shape)\nprint(\"Indices Tensor shape:\", indices_tensor.shape) \nprint(\"Updates Tensor shape:\", updates_tensor.shape)\n\n# This causes a fatal crash\nresult = tf.raw_ops.TensorScatterMin(\n    tensor=input_tensor,\n    indices=indices_tensor,\n    updates=updates_tensor\n)\n```\n\n### Relevant log output\n\n```shell\n2025-05-25 19:52:28.822273: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/76e8894c8ad1521d4b860c71a2a5451e/94132_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94131,
    "title": "Segmentation Fault in `tf.raw_ops.ResourceSparseApplyMomentum`",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-25T19:45:00Z",
    "updated_at": "2025-05-29T11:58:54Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.ResourceSparseApplyMomentum` causes a segmentation fault when called with mismatched tensor shapes, specifically when the gradient tensor shape doesn't align with the variable dimensions and indices.\n\nFatal Error: `Check failed: d < dims() (1 vs. 1) in tensorflow/core/framework/tensor_shape.cc:359`\n\nStack Trace Location:\n\nPrimary failure point: `tensorflow::SparseApplyMomentumOp<Eigen::bfloat16, int>::Compute`\nShape validation failure: `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(int) const`\n\ncolab: https://colab.research.google.com/drive/1OZrZ-tWh6m3ojrLX_2vsGIVsuzj1DQEg?usp=sharing\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\nprint(\"TensorFlow version:\", tf.__version__)\n\n# Create resource variables\nvar = tf.Variable(np.random.random((6, 6)).astype(np.float32), dtype=tf.bfloat16)\naccum = tf.Variable(np.zeros((6, 6)).astype(np.float32), dtype=tf.bfloat16)\n\n# Create tensors with problematic configurations\nlr_tensor = tf.constant(0.0, dtype=tf.bfloat16)\ngrad_tensor = tf.constant(np.zeros(10), dtype=tf.bfloat16)  # Shape [10] - mismatch\nindices_tensor = tf.constant([0, 0, 0, 1, 1, 2, 3, 4], dtype=tf.int32)  # 8 indices\nmomentum_tensor = tf.constant(0.9, dtype=tf.bfloat16)\n\n# This crashes the process\nresult = tf.raw_ops.ResourceSparseApplyMomentum(\n    var=var.handle,\n    accum=accum.handle,\n    lr=lr_tensor,\n    grad=grad_tensor,\n    indices=indices_tensor,\n    momentum=momentum_tensor,\n    use_locking=True,\n    use_nesterov=True\n)\n```\n\n### Relevant log output\n\n```shell\n2025-05-25 19:36:24.885970: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/901ab547b5fdd787569aca26151f1382/94131_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94130,
    "title": "Crash in `tf.raw_ops.ResourceSparseApplyAdagradDA`",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-25T19:27:01Z",
    "updated_at": "2025-05-29T11:53:30Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.3\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe `tf.raw_ops.ResourceSparseApplyAdagradDA` operation causes a fatal crash with the error `Check failed: d < dims() (1 vs. 0)` when called with a scalar gradient tensor and multi-dimensional indices tensor.\n\n\n\nColab Reproduction: https://colab.research.google.com/drive/1X_plMhFhjig9v4zwmIHkEiyQocY5vXfA?usp=sharing\n\nStack Trace Location: The crash occurs in `tensorflow::SparseApplyAdagradDAOp<float, int>::Compute()` at line 2467 in `/tensorflow/core/kernels/training_ops.cc`, specifically when calling `Tensor::dim_size(int)`` on a tensor with incompatible dimensions.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\n# Create variable and accumulators\nvar = tf.Variable([[0.0, 0.0]] * 10, dtype=tf.float32)\ngradient_accumulator = tf.Variable([[0.0, 0.0]] * 10, dtype=tf.float32)\ngradient_squared_accumulator = tf.Variable([[0.0, 0.0]] * 10, dtype=tf.float32)\n\n# Problematic parameters - scalar grad with 2D indices\ngrad = tf.constant(0.0, dtype=tf.float32)  # Scalar gradient\nindices = tf.constant([0, 0], dtype=tf.int32)  # 2D indices\n\n# Other parameters\nlr = tf.constant(0.0, dtype=tf.float32)\nl1 = tf.constant(0.0, dtype=tf.float32)\nl2 = tf.constant(0.0, dtype=tf.float32)\nglobal_step = tf.constant(1, dtype=tf.int64)\n\n# This will crash\ntf.raw_ops.ResourceSparseApplyAdagradDA(\n    var=var.handle,\n    gradient_accumulator=gradient_accumulator.handle,\n    gradient_squared_accumulator=gradient_squared_accumulator.handle,\n    grad=grad,\n    indices=indices,\n    lr=lr,\n    l1=l1,\n    l2=l2,\n    global_step=global_step,\n    use_locking=True\n)\n```\n\n### Relevant log output\n\n```shell\n2025-05-25 19:23:06.603818: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 0)\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/d0c46af58b743598e850cf05c52d2d74/94130_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94127,
    "title": "Segmentation Fault in `tf.raw_ops.HistogramFixedWidth`",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-25T03:34:31Z",
    "updated_at": "2025-05-29T10:24:52Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux CPU \n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n\n\n`tf.raw_ops.HistogramFixedWidth` causes a segmentation fault when processing large int64 values with int32 output dtype during graph optimization phase.\n\nColab Reproduction: https://colab.research.google.com/drive/1IsOQ0LXGHjezeselGGNHUoxj_Wl3OvWf?usp=sharing\n\n\n\n\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\ndef reproduce_crash():\n    # Large int64 values that trigger the crash\n    values = tf.constant([\n        [-9114860691027166365, 9114861777597660793],\n        [9114861777597660798, -9114860691027166365],\n        [9114861777597660793, 9114861777597660798],\n        [9114860691027166365, -9114861777597660793]\n    ], dtype=tf.int64)\n    \n    # Value range\n    value_range = tf.constant([9114861777597660672, 9114861777597660798], dtype=tf.int64)\n    \n    # Number of bins\n    nbins = tf.constant(35, dtype=tf.int32)\n    \n    # This causes segmentation fault\n    result = tf.raw_ops.HistogramFixedWidth(\n        values=values,\n        value_range=value_range,\n        nbins=nbins,\n        dtype=tf.int32\n    )\n    return result\n\n# Run this to reproduce the crash\nreproduce_crash()\n```\n\n### Relevant log output\n\n```shell\nSegmentation fault (core dumped)\n\n\nStack:\nThe crash occurs during constant folding optimization:\n\ntensorflow::functor::HistogramFixedWidthFunctor<Eigen::ThreadPoolDevice, long, int>::Compute\n└── tensorflow::HistogramFixedWidthOp<Eigen::ThreadPoolDevice, long, int>::Compute  \n    └── tensorflow::grappler::ConstantFolding::EvaluateNode\n        └── [Graph optimization pipeline]\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/317e1a1da797c4a76836f01f6f47154c/94127_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94122,
    "title": "`Segmentation Fault` in CropAndResizeGradBoxes",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-24T18:33:22Z",
    "updated_at": "2025-05-29T10:20:51Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux (Docker container) CPU with AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow crashes with a segmentation fault (SEGV) when using tf.raw_ops.CropAndResizeGradBoxes with NaN values in the boxes tensor. The crash occurs during memory access in the crop and resize backpropagation computation at /tensorflow/core/kernels/image/crop_and_resize_op.cc:743:36. The NaN values cause invalid memory addresses to be computed, leading to out-of-bounds memory access and process termination.\n\nThe issue can be reproduced in this Colab notebook:\nhttps://colab.research.google.com/drive/1nZmPF_gqw6wLMIlhqT_1-g7NieOml_U0?usp=sharing\n\nThe crash occurs in the CropAndResizeBackpropBoxes functor:\n```\n#0 tensorflow::functor::CropAndResizeBackpropBoxes<Eigen::ThreadPoolDevice, unsigned short>::operator()\n   /proc/self/cwd/tensorflow/core/kernels/image/crop_and_resize_op.cc:743:36\n```\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\nimport os\n\n# oneDNN may be involved but crash occurs regardless\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n\n# Grads tensor - shape [2,3,3,2] with extreme float values\ngrads_data = np.array([\n    [[[-1.14306452e+18, -4.13623095e-14], [2.83210481e+20, 1.0], [1.0, 1.0]],\n     [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]],\n     [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]],\n    [[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]],\n     [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]], \n     [[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]]\n], dtype=np.float32)\ngrads = tf.constant(grads_data, dtype=tf.float32)\n\n# Image tensor - shape [1,5,5,2] with uint16 values\nimage_data = np.random.randint(0, 65535, size=(1, 5, 5, 2), dtype=np.uint16)\nimage = tf.constant(image_data, dtype=tf.uint16)\n\n# CRITICAL: Boxes tensor with NaN values - this causes the crash\nboxes_data = np.array([\n    [np.nan, 1.0, 1.0, 1.0],  # NaN in first coordinate\n    [0.0, 0.0, 1.0, 1.0]      # Valid box\n], dtype=np.float32)\nboxes = tf.constant(boxes_data, dtype=tf.float32)\n\n# Box indices tensor\nbox_ind = tf.constant([0, 0], dtype=tf.int32)\n\n# This call causes segmentation fault due to NaN in boxes\nresult = tf.raw_ops.CropAndResizeGradBoxes(\n    grads=grads,\n    image=image,\n    boxes=boxes,\n    box_ind=box_ind\n)\n```\n\n### Relevant log output\n\n```shell\nActual Behavior:\n\nSegmentation fault (core dumped)\n\n\nComplete Error Log:\n\n2025-05-24 18:26:29.840318: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-05-24 18:26:29.889297: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-05-24 18:26:30.924979: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\nTensorFlow version: 2.20.0-dev20250516\n2025-05-24 18:26:31.200686: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\nInput tensor details:\nGrads shape: (2, 3, 3, 2), dtype: <dtype: 'float32'>\nImage shape: (1, 5, 5, 2), dtype: <dtype: 'uint16'>\nBoxes shape: (2, 4), dtype: <dtype: 'float32'>\nBox indices shape: (2,), dtype: <dtype: 'int32'>\n\nTensor values:\nGrads (first few): [-1.1430645e+18 -4.1362309e-14  2.8321048e+20  1.0000000e+00\n  1.0000000e+00  1.0000000e+00]\nImage (first few): [17572 16952 32988 18161 57827  6528]\nBoxes: [[nan  1.  1.  1.]\n [ 0.  0.  1.  1.]]\nBox indices: [0 0]\nSegmentation fault (core dumped)\n\n\nFull Stack Trace:\n\n#0 tensorflow::functor::CropAndResizeBackpropBoxes<Eigen::ThreadPoolDevice, unsigned short>::operator() \n   /tensorflow/core/kernels/image/crop_and_resize_op.cc:743:36\n#1 tensorflow::CropAndResizeGradBoxesOp<Eigen::ThreadPoolDevice, unsigned short>::ComputeAsync()::lambda()::operator()() \n   /tensorflow/core/kernels/image/crop_and_resize_op.cc:658:27\n#2-#7 [TensorFlow executor and threading infrastructure]\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/48799dd7a909ccbe44753bcb72db8e58/94122_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94119,
    "title": "Floating Point Exception in `AvgPool3DGrad`",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-24T18:10:05Z",
    "updated_at": "2025-05-29T09:01:18Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux (Docker container)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow crashes with a **Floating Point Exception (FPE)** and **core dump** when using `tf.raw_ops.AvgPool3DGrad` with small bfloat16 values. The crash occurs during gradient computation for 3D average pooling, likely due to division by zero or invalid floating-point arithmetic in the pooling area calculation\n\nIt need to be reproduced with `TF_ENABLE_ONEDNN_OPTS=1`\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\n# Create input tensors matching the crash scenario\norig_input_shape = tf.constant([8, 10, 7, 1, 9], dtype=tf.int32)\n\n# Grad tensor with extremely small bfloat16 values that trigger the crash\ngrad_data = np.full((8, 10, 7, 1, 9), 9.18355e-41, dtype=np.float32)  \ngrad = tf.constant(grad_data, dtype=tf.bfloat16)\n\n# Pool parameters\nksize = [1, 2, 2, 2, 1]      # [batch, depth, height, width, channels]\nstrides = [1, 1, 1, 1, 1]    # [batch, depth, height, width, channels]\npadding = \"VALID\"\ndata_format = \"NDHWC\"\n\n# This call causes the floating point exception and core dump\nresult = tf.raw_ops.AvgPool3DGrad(\n    orig_input_shape=orig_input_shape,\n    grad=grad,\n    ksize=ksize,\n    strides=strides,\n    padding=padding,\n    data_format=data_format\n)\n```\n\n### Relevant log output\n\n```shell\nFloating point exception (core dumped)\n\n\n\n2025-05-24 18:05:18.592275: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-05-24 18:05:18.641089: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-05-24 18:05:19.612237: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\nTensorFlow version: 2.20.0-dev20250516\n2025-05-24 18:05:19.828300: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\nInput parameters:\nOriginal input shape: [ 8 10  7  1  9]\nGrad shape: (8, 10, 7, 1, 9)\nGrad dtype: <dtype: 'bfloat16'>\nKsize: [1, 2, 2, 2, 1]\nStrides: [1, 1, 1, 1, 1]\nPadding: VALID\nData format: NDHWC\n\nGrad values (first few): [9.18355e-41 9.18355e-41 9.18355e-41 9.18355e-41 9.18355e-41 9.18355e-41\n 9.18355e-41 9.18355e-41 9.18355e-41 9.18355e-41]\nFloating point exception (core dumped)\n\n\nSome stack trace:\n\n    #0 0x57f1d5a9bfd2 in dnnl::impl::utils::remove_reference<int>::type dnnl::impl::utils::div_up<int, int>(int, int) /proc/self/cwd/external/onednn/src/common/utils.hpp:313:72\n    #1 0x57f1d7db5806 in dnnl::impl::cpu::x64::jit_uni_pool_kernel<(dnnl::impl::cpu::x64::cpu_isa_t)880>::generate()::'lambda'(int, bool)::operator()(int, bool) const /proc/self/cwd/external/onednn/src/cpu/x64/jit_uni_pool_kernel.cpp:1543:37\n    #2 0x57f1d7db4f59 in dnnl::impl::cpu::x64::jit_uni_pool_kernel<(dnnl::impl::cpu::x64::cpu_isa_t)880>::generate() /proc/self/cwd/external/onednn/src/cpu/x64/jit_uni_pool_kernel.cpp:1605:5\n    #3 0x57f1d58b29b2 in dnnl::impl::cpu::x64::jit_generator::create_kernel() /proc/self/cwd/external/onednn/src/cpu/x64/jit_generator.hpp:2723:9\n    #4 0x57f1d7de5ac8 in dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::init(dnnl_engine*) /proc/self/cwd/external/onednn/src/cpu/x64/jit_uni_pooling.cpp:886:21\n    #5 0x57f1d5760915 in dnnl::impl::primitive_t::init(dnnl_engine*, bool, dnnl::impl::cache_blob_t const&) /proc/self/cwd/external/onednn/src/common/primitive.hpp:53:9\n    #6 0x57f1d5e5754b in dnnl_status_t dnnl::impl::primitive_t::create_primitive_common<dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>, dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t>(std::pair<std::shared_ptr<dnnl::impl::primitive_t>, bool>&, dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t const*, dnnl_engine*, bool, dnnl::impl::cache_blob_t const&)::'lambda'(void*)::operator()(void*) const /proc/self/cwd/external/onednn/src/common/primitive.hpp:107:26\n    #7 0x57f1d5e57464 in dnnl_status_t dnnl::impl::primitive_t::create_primitive_common<dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>, dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t>(std::pair<std::shared_ptr<dnnl::impl::primitive_t>, bool>&, dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t const*, dnnl_engine*, bool, dnnl::impl::cache_blob_t const&)::'lambda'(void*)::__invoke(void*) /proc/self/cwd/external/onednn/src/common/primitive.hpp:103:61\n    #8 0x57f1d5512e32 in dnnl::impl::utils::cache_t<dnnl::impl::primitive_hashing::key_t, dnnl::impl::primitive_t, dnnl::impl::primitive_cache_iface_t::result_t, &dnnl::impl::primitive_cache_t::update_key(dnnl::impl::primitive_hashing::key_t const&, dnnl::impl::primitive_t const&)>::get_or_create(dnnl::impl::primitive_hashing::key_t const&, dnnl::impl::primitive_cache_iface_t::result_t (&)(void*), void*) /proc/self/cwd/external/onednn/src/common/cache_utils.hpp:90:33\n    #9 0x57f1d55074b0 in dnnl::impl::primitive_cache_t::get_or_create(dnnl::impl::primitive_hashing::key_t const&, dnnl::impl::primitive_cache_iface_t::result_t (&)(void*), void*) /proc/self/cwd/external/onednn/src/common/primitive_cache.cpp:52:23\n    #10 0x57f1d5507383 in dnnl::impl::primitive_cache_iface_t::get_or_create(dnnl::impl::primitive_hashing::key_t const&, dnnl::impl::primitive_cache_iface_t::result_t (&)(void*), void*) /proc/self/cwd/external/onednn/src/common/primitive_cache.cpp:132:21\n    #11 0x57f1d5e572cf in dnnl_status_t dnnl::impl::primitive_t::create_primitive_common<dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>, dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t>(std::pair<std::shared_ptr<dnnl::impl::primitive_t>, bool>&, dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t const*, dnnl_engine*, bool, dnnl::impl::cache_blob_t const&) /proc/self/cwd/external/onednn/src/common/primitive.hpp:112:42\n    #12 0x57f1d5e56692 in dnnl::impl::cpu::x64::jit_uni_pooling_bwd_t<(dnnl::impl::cpu::x64::cpu_isa_t)880, (dnnl_data_type_t)2>::pd_t::create_primitive(std::pair<std::shared_ptr<dnnl::impl::primitive_t>, bool>&, dnnl_engine*, dnnl::impl::cache_blob_t const&) const /proc/self/cwd/external/onednn/src/cpu/x64/jit_uni_pooling.hpp:124:9\n    #13 0x57f1d551aa91 in dnnl_primitive_desc::create_primitive_iface(std::pair<dnnl_primitive*, bool>&, dnnl::impl::cache_blob_t const&) const /proc/self/cwd/external/onednn/src/common/primitive_desc_iface.cpp:98:27\n    #14 0x57f1d552d784 in dnnl::impl::primitive_create(dnnl_primitive**, dnnl_primitive_desc const*, dnnl::impl::cache_blob_t const&) /proc/self/cwd/external/onednn/src/common/primitive_iface.cpp:80:9\n    #15 0x57f1d552e405 in dnnl_primitive_create /proc/self/cwd/external/onednn/src/common/primitive_iface.cpp:162:12\n    #16 0x57f1cf1cbfcd in dnnl::primitive::primitive(dnnl_primitive_desc const*) /proc/self/cwd/external/onednn/include/oneapi/dnnl/dnnl.hpp:13752:23\n    #17 0x57f1cf26feb2 in dnnl::primitive::primitive(dnnl::primitive_desc const&) /proc/self/cwd/external/onednn/include/oneapi/dnnl/dnnl.hpp:13768:57\n    #18 0x57f1cfb43dd0 in dnnl::pooling_backward::pooling_backward(dnnl::pooling_backward::primitive_desc const&) /proc/self/cwd/external/onednn/include/oneapi/dnnl/dnnl.hpp:13205:50\n    #19 0x57f1cfb44efe in tensorflow::MklPoolingBwdPrimitive<Eigen::bfloat16>::Setup(tensorflow::MklPoolingParams const&) /proc/self/cwd/tensorflow/core/kernels/mkl/mkl_pooling_ops_common.cc:211:28\n    #20 0x57f1cfaf98d2 in tensorflow::MklPoolingBwdPrimitive<Eigen::bfloat16>::MklPoolingBwdPrimitive(tensorflow::MklPoolingParams const&) /proc/self/cwd/./tensorflow/core/kernels/mkl/mkl_pooling_ops_common.h:251:34\n    #21 0x57f1cfaf94b8 in tensorflow::MklPoolingBwdPrimitiveFactory<Eigen::bfloat16>::Get(tensorflow::MklPoolingParams const&) /proc/self/cwd/./tensorflow/core/kernels/mkl/mkl_pooling_ops_common.h:353:30\n    #22 0x57f1cfafe210 in tensorflow::MklAvgPoolingGradOp<Eigen::ThreadPoolDevice, Eigen::bfloat16, true>::Compute(tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/kernels/mkl/mkl_avgpooling_op.cc:352:11\n    #23 0x57f1ddeb9dad in tensorflow::Device::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/./tensorflow/core/framework/device.h:93:16\n    #24 0x57f1df7190ff in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ProcessSync(tensorflow::NodeItem const&, tensorflow::OpKernelContext::Params*, absl::lts_20230802::InlinedVector<tensorflow::Entry, 4ul, std::allocator<tensorflow::Entry>>*, tensorflow::NodeExecStatsInterface*) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:610:13\n    #25 0x57f1df71628f in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ProcessInline(tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*, long) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:930:13\n    #26 0x57f1df710291 in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::Process(tensorflow::SimplePropagatorState::TaggedNode const&, long) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:737:10\n    #27 0x57f1df7132de in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()::operator()() const /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:1308:25\n    #28 0x57f1df713271 in void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()::operator()() /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:489:5\n    #29 0x57f1df7130f0 in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'() std::__invoke_impl<void, void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()&>(std::__invoke_other, void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()&) /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/invoke.h:61:14\n    #30 0x57f1df713030 in std::enable_if<is_invocable_r_v<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'(), void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()&>, tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>::type std::__invoke_r<void, void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()&>(void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()&) /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/invoke.h:111:2\n    #31 0x57f1df712dc8 in std::_Function_handler<void (), void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int)::'lambda'()>::_M_invoke(std::_Any_data const&) /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/std_function.h:290:9\n    #32 0x57f1c7d00617 in std::function<void ()>::operator()() const /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/std_function.h:591:9\n    #33 0x57f1df685578 in tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2::operator()(std::function<void ()>) const /proc/self/cwd/tensorflow/core/common_runtime/graph_runner.cc:156:49\n    #34 0x57f1df685505 in void std::__invoke_impl<void, tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2&, std::function<void ()>>(std::__invoke_other, tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2&, std::function<void ()>&&) /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/invoke.h:61:14\n    #35 0x57f1df685420 in std::enable_if<is_invocable_r_v<void, tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2&, std::function<void ()>>, void>::type std::__invoke_r<void, tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2&, std::function<void ()>>(tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2&, std::function<void ()>&&) /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/invoke.h:111:2\n    #36 0x57f1df685238 in std::_Function_handler<void (std::function<void ()>), tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*)::$_2>::_M_invoke(std::_Any_data const&, std::function<void ()>&&) /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/std_function.h:290:9\n    #37 0x57f1d508c097 in std::function<void (std::function<void ()>)>::operator()(std::function<void ()>) const /usr/lib/gcc/x86_64-linux-gnu/13/../../../../include/c++/13/bits/std_function.h:591:9\n    #38 0x57f1df70fc5b in void tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*)::'lambda1'()&&, int) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:487:3\n    #39 0x57f1df70d1d3 in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::ScheduleReady(absl::lts_20230802::InlinedVector<tensorflow::SimplePropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::SimplePropagatorState::TaggedNode>>*, tensorflow::SimplePropagatorState::TaggedNodeReadyQueue*) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:1308:9\n    #40 0x57f1df6cb284 in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::RunAsync(std::function<void (absl::lts_20230802::Status const&)>) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:517:5\n    #41 0x57f1df6c8939 in tensorflow::(anonymous namespace)::ExecutorImpl::RunAsyncInternal(tensorflow::Executor::Args const&, std::function<void (absl::lts_20230802::Status const&)>) /proc/self/cwd/tensorflow/core/common_runtime/executor.cc:1527:11\n    #42 0x57f1c7ca90ed in tensorflow::Executor::RunAsync(tensorflow::Executor::Args const&, std::function<void (absl::lts_20230802::Status const&)>) /proc/self/cwd/./tensorflow/core/common_runtime/executor.h:132:5\n    #43 0x57f1df5393d8 in tensorflow::Executor::Run(tensorflow::Executor::Args const&) /proc/self/cwd/./tensorflow/core/common_runtime/executor.h:142:5\n    #44 0x57f1df68224d in tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/graph_runner.cc:193:3\n    #45 0x57f1df4ebe70 in tensorflow::ConstantFold(tensorflow::ConstantFoldingOptions const&, tensorflow::FunctionLibraryRuntime*, tsl::Env*, tensorflow::Device const*, tensorflow::Graph*, bool*) /proc/self/cwd/tensorflow/core/common_runtime/constant_folding.cc:693:21\n    #46 0x57f1df4ea68b in tensorflow::GraphOptimizer::Optimize(tensorflow::FunctionLibraryRuntime*, tsl::Env*, tensorflow::Device const*, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph>>*, tensorflow::GraphOptimizer::Options const&) /proc/self/cwd/tensorflow/core/common_runtime/graph_optimizer.cc:83:7\n    #47 0x57f1c7cb3933 in tensorflow::DirectSession::CreateExecutors(tensorflow::CallableOptions const&, std::unique_ptr<tensorflow::DirectSession::ExecutorsAndKeys, std::default_delete<tensorflow::DirectSession::ExecutorsAndKeys>>*, std::unique_ptr<tensorflow::DirectSession::FunctionInfo, std::default_delete<tensorflow::DirectSession::FunctionInfo>>*, tensorflow::DirectSession::RunStateArgs*) /proc/self/cwd/tensorflow/core/common_runtime/direct_session.cc:1427:15\n    #48 0x57f1c7cac65a in tensorflow::DirectSession::GetOrCreateExecutors(absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>> const>, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>> const>, absl::lts_20230802::Span<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>> const>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) /proc/self/cwd/tensorflow/core/common_runtime/direct_session.cc:1583:3\n    #49 0x57f1c7caa3ec in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*, tensorflow::RunMetadata*, tsl::thread::ThreadPoolOptions const&) /proc/self/cwd/tensorflow/core/common_runtime/direct_session.cc:879:3\n    #50 0x57f1c7ca9eeb in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, tensorflow::Tensor>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*, tensorflow::RunMetadata*) /proc/self/cwd/tensorflow/core/common_runtime/direct_session.cc:849:10\n    #51 0x57f1c7c5cd1c in tensorflow::ClientSession::Run(tensorflow::RunOptions const&, std::unordered_map<tensorflow::Output, tensorflow::Input::Initializer, tensorflow::OutputHash, std::equal_to<tensorflow::Output>, std::allocator<std::pair<tensorflow::Output const, tensorflow::Input::Initializer>>> const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output>> const&, std::vector<tensorflow::Operation, std::allocator<tensorflow::Operation>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*, tensorflow::RunMetadata*) const /proc/self/cwd/tensorflow/cc/client/client_session.cc:131:28\n    #52 0x57f1c7c5c540 in tensorflow::ClientSession::Run(std::unordered_map<tensorflow::Output, tensorflow::Input::Initializer, tensorflow::OutputHash, std::equal_to<tensorflow::Output>, std::allocator<std::pair<tensorflow::Output const, tensorflow::Input::Initializer>>> const&, std::vector<tensorflow::Output, std::allocator<tensorflow::Output>> const&, std::vector<tensorflow::Operation, std::allocator<tensorflow::Operation>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/cc/client/client_session.cc:89:10\n    #53 0x57f1c7c5c428 in tensorflow::ClientSession::Run(std::vector<tensorflow::Output, std::allocator<tensorflow::Output>> const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/cc/client/client_session.cc:76:10\n    #54 0x57f1c7c5221c in LLVMFuzzerTestOneInput /proc/self/cwd/fuzz/tf.raw_ops.AvgPool3DGrad/fuzz.cpp:195:45\n    #55 0x57f1c7c1da0a in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) crtstuff.c\n    #56 0x57f1c7c057d3 in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) crtstuff.c\n    #57 0x57f1c7c0b991 in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) crtstuff.c\n    #58 0x57f1c7c370a6 in main (/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/k8-opt/bin/fuzz/tf.raw_ops.AvgPool3DGrad/fuzz+0xcf380a6) (BuildId: 1e0e6a184195e9b05743c88318128017bcc38166)\n    #59 0x72029e9381c9  (/lib/x86_64-linux-gnu/libc.so.6+0x2a1c9) (BuildId: 42c84c92e6f98126b3e2230ebfdead22c235b667)\n    #60 0x72029e93828a in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2a28a) (BuildId: 42c84c92e6f98126b3e2230ebfdead22c235b667)\n    #61 0x57f1c7c003a4 in _start (/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow/bazel-out/k8-opt/bin/fuzz/tf.raw_ops.AvgPool3DGrad/fuzz+0xcf013a4) (BuildId: 1e0e6a184195e9b05743c88318128017bcc38166)\n\n==868004==Register values:\nrax = 0x00000000ffffffff  rbx = 0x00007ffeec726a60  rcx = 0x0000000000000000  rdx = 0x00000000ffffffff  \nrdi = 0x0000000000000000  rsi = 0x0000000000000000  rbp = 0x00007ffeec7249e0  rsp = 0x00007ffeec7249e0  \n r8 = 0x0000000000001b50   r9 = 0x000000000000000b  r10 = 0x000057f210b3b860  r11 = 0x00007ffeec7249b0  \nr12 = 0x000057f1e66d86b0  r13 = 0x000057f1e6180a00  r14 = 0x00007ffeec726a78  r15 = 0x000057f210a4d860  \nUndefinedBehaviorSanitizer can not provide additional info.\nSUMMARY: UndefinedBehaviorSanitizer: FPE /proc/self/cwd/external/onednn/src/common/utils.hpp:313:72 in dnnl::impl::utils::remove_reference<int>::type dnnl::impl::utils::div_up<int, int>(int, int)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/5c020bd62e35c0128d3ebbd23d9706d4/94119_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94118,
    "title": "Crash in `SparseBincount` due to overflow",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-24T17:52:03Z",
    "updated_at": "2025-05-29T08:23:33Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux (Docker container)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow crashes with a fatal error `Non-OK-status: InitDims(dim_sizes)` when using `tf.raw_ops.SparseBincount` with large int64 values in the `dense_shape` parameter. The crash occurs in `tensorflow/core/framework/tensor_shape.cc:202` due to integer overflow when multiplying dimension sizes: `8970181431921507452 * 2088533116 = -1 (overflow result)`.\n\nThe issue can be reproduced in this Colab notebook:\nhttps://colab.research.google.com/drive/1Tc3X-iRD3cE_QoyikVXacsEHGO_cqmbC?usp=sharing\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\n# Create tensors with problematic large integer values\nindices = tf.constant([[3026414551496130560, 2738188573441261603],\n                      [8970181431921507328, 8970181431921507328]], \n                     dtype=tf.int64)\n\nvalues = tf.constant([2088533116, 2088533116], dtype=tf.int32)\n\n# This dense_shape causes integer overflow when dimensions are multiplied\ndense_shape = tf.constant([8970181431921507452, 8970181431921507452], dtype=tf.int64)\n\nsize = tf.constant(2088533116, dtype=tf.int32)\nweights = tf.constant([], dtype=tf.int32)\n\n# This call causes the fatal crash\nresult = tf.raw_ops.SparseBincount(\n    indices=indices,\n    values=values,\n    dense_shape=dense_shape,\n    size=size,\n    weights=weights,\n    binary_output=False\n)\n```\n\n### Relevant log output\n\n```shell\n2025-05-24 17:48:55.931637: F tensorflow/core/framework/tensor_shape.cc:202] Non-OK-status: InitDims(dim_sizes)\nStatus: INVALID_ARGUMENT: Encountered overflow when multiplying 8970181431921507452 with 2088533116, result: -1\nAborted (core dumped)\n\n\nComplete Log:\n\n2025-05-24 17:48:54.678039: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-05-24 17:48:54.721352: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-05-24 17:48:55.665479: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\nTensorFlow version: 2.20.0-dev20250516\n2025-05-24 17:48:55.875841: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\nInput tensor details:\nIndices shape: (2, 2), dtype: <dtype: 'int64'>\nValues shape: (2,), dtype: <dtype: 'int32'>\nDense shape: (2,), dtype: <dtype: 'int64'>\nSize: 2088533116\nWeights shape: (0,)\nBinary output: False\n\nTensor values:\nIndices: [[3026414551496130560 2738188573441261603]\n [8970181431921507328 8970181431921507328]]\nValues: [2088533116 2088533116]\nDense shape: [8970181431921507452 8970181431921507452]\n2025-05-24 17:48:55.931637: F tensorflow/core/framework/tensor_shape.cc:202] Non-OK-status: InitDims(dim_sizes)\nStatus: INVALID_ARGUMENT: Encountered overflow when multiplying 8970181431921507452 with 2088533116, result: -1\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/e41379b05134c425940d52777d588eeb/94118_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94117,
    "title": "Crash in `ResourceSparseApplyProximalAdagrad`",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-24T17:42:37Z",
    "updated_at": "2025-05-29T08:11:38Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux (Docker container)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow crashes with a fatal error `Check failed: d < dims() (1 vs. 1)` when using tf.raw_ops.ResourceSparseApplyProximalAdagrad with large float32 values. \n\nThe crash occurs in tensorflow/core/framework/tensor_shape.cc:359 and results in an aborted process with core dump.\n\nShould raise a Python exception with a descriptive error message.\n\nThe issue can be reproduced in this Colab notebook: https://colab.research.google.com/drive/1HoC35YNBUC-Fs_6vOgrimGoLxqBisKHr?usp=sharing\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\n# Create resource variables with extreme values\nlarge_val = 5.24393461e+36\n\nvar = tf.Variable([[large_val, large_val, large_val],\n                   [large_val, large_val, large_val]], \n                  dtype=tf.float32, name=\"var\")\n\naccum = tf.Variable([[large_val, large_val, large_val],\n                     [large_val, large_val, large_val]], \n                    dtype=tf.float32, name=\"accum\")\n\n# Scalar tensors with large values\nlr = tf.constant(large_val, dtype=tf.float32)\nl1 = tf.constant(large_val, dtype=tf.float32) \nl2 = tf.constant(large_val, dtype=tf.float32)\n\n# Gradient and indices\ngrad = tf.constant([7.90505e+31], dtype=tf.float32)\nindices = tf.constant([0], dtype=tf.int32)\n\n# This call causes the crash\nresult = tf.raw_ops.ResourceSparseApplyProximalAdagrad(\n    var=var.handle,\n    accum=accum.handle,\n    lr=lr,\n    l1=l1,\n    l2=l2,\n    grad=grad,\n    indices=indices,\n    use_locking=False\n)\n```\n\n### Relevant log output\n\n```shell\nThe program crashes with a fatal error:\n\n2025-05-24 17:38:59.747282: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)\nAborted (core dumped)\n\n\nComplete Log:\n\n\n2025-05-24 17:38:58.413411: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-05-24 17:38:58.458995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-05-24 17:38:59.456168: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\nTensorFlow version: 2.20.0-dev20250516\n2025-05-24 17:38:59.672876: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\nTensor shapes:\nvar: (2, 3)\naccum: (2, 3)\ngrad: (1,)\nindices: (1,)\nTensor values:\nvar values: [[5.2439346e+36 5.2439346e+36 5.2439346e+36]\n [5.2439346e+36 5.2439346e+36 5.2439346e+36]]\nlr: 5.2439346057351246e+36  \ngrad: [7.90505e+31]\n2025-05-24 17:38:59.747282: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [Gist](https://colab.sandbox.google.com/gist/Venkat6871/ffe3d7c44066fe298aa8a1f1154fa027/94117_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94115,
    "title": "Segmentation Fault in `tf.raw_ops.MaxPool` with oneDNN and `NCHW_VECT_C`",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-24T16:07:16Z",
    "updated_at": "2025-05-29T08:04:37Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250516 (git: v1.12.1-126285-g20148f52365)\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 24.04 noble\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.3\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.MaxPool` with `data_format=\"NCHW_VECT_C\"` crashes with a segmentation fault when oneDNN optimizations are enabled. The crash occurs in the MKL layout pass with an INVALID_ARGUMENT error, but instead of gracefully handling the error, the process aborts.\n\n\nCritical: This crash only occurs with oneDNN enabled\n\nCrash occurs: When oneDNN is enabled (default in recent TF builds)\nNo crash: When oneDNN is disabled (TF_ENABLE_ONEDNN_OPTS=0)\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\nprint(f\"TensorFlow version: {tf.version.VERSION}\")\nprint(f\"oneDNN enabled: {'TF_ENABLE_ONEDNN_OPTS' not in os.environ or os.environ.get('TF_ENABLE_ONEDNN_OPTS', '1') == '1'}\")\n\n# Create input tensor\ninput_tensor = tf.random.normal([10, 1, 1, 9], dtype=tf.float32)\n\ntry:\n    # This crashes with oneDNN enabled, works with oneDNN disabled\n    result = tf.raw_ops.MaxPool(\n        input=input_tensor,\n        ksize=[1, 1, 1, 1],\n        strides=[1, 1, 1, 1], \n        padding=\"SAME\",\n        data_format=\"NCHW_VECT_C\"  # Unsupported by MklNativeMaxPool\n    )\n    print(\"No crash occurred\")\nexcept Exception as e:\n    print(f\"Exception (expected): {e}\")\n```\n\n### Relevant log output\n\n```shell\n2025-05-24 16:03:32.656644: F tensorflow/core/common_runtime/mkl_layout_pass.cc:3727] Non-OK-status: ret_status\nStatus: INVALID_ARGUMENT: Value for attr 'data_format' of \"NCHW_VECT_C\" is not in the list of allowed values: \"NHWC\", \"NCHW\"\n        ; NodeDef: {{node MaxPool}}; Op<name=_MklNativeMaxPool; signature=input:T -> output:T, workspace:uint8; attr=T:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_HALF, DT_BFLOAT16]; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=[\"SAME\", \"VALID\"]; attr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]; attr=explicit_paddings:list(int),default=[]; attr=workspace_enabled:bool,default=false>\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @SilentTester73 ,\nApologies for the delay, and thank you for raising your concern. I tried running your code on Colab using TensorFlow version 2.19.0 as well as the latest nightly build, and I was not able to reproduce the issue it worked fine in both cases.\nPlease keep in mind that nightly versions of TensorFlow are updated frequently and may sometimes introduce unexpected behavior. For better stability and reproducibility, I recommend using the latest stable release, such as TensorFlow 2.19.0.\nI am sharing a [gist](https://colab.sandbox.google.com/gist/Venkat6871/ce3e2cfaf97ff8051c6390ca6899f2ee/94115_tf_2-19-0-nightly-v.ipynb) here for your reference hope it helps!\n\nThank you!"
      },
      {
        "user": "SilentTester73",
        "body": "Hi @Venkat6871,\nThanks for looking into this! I understand that Colab couldn't reproduce the issue. I've tested on my local machines and Colab.  Colab seems to have OneDNN disabled, while it's enabled by default on physical computers when installing TensorFlow via pip (if NVIDIA GPU is not presence).\n\nWould you mind trying to reproduce this in an environment with ONEDNN enabled? I'm confident you'll encounter the same issue, even with the nightly build. \n"
      },
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. I am attaching [screenshot1](https://screenshot.googleplex.com/5eBvCDg7ZR3mLqk), [screenshot2](https://screenshot.googleplex.com/7ABwwZ2skdaWdFd).\nThank you!"
      }
    ]
  },
  {
    "issue_number": 92564,
    "title": "TensorFlow 2.12, 2.16.2,2.19 using M4 Mac vs colab GPU give incorrect accuracy and  >2.12TF are slower",
    "author": "MrFuguDataScience",
    "state": "closed",
    "created_at": "2025-05-01T19:16:50Z",
    "updated_at": "2025-05-29T02:13:01Z",
    "labels": [
      "stat:awaiting response",
      "stale",
      "subtype:macOS",
      "type:performance",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nPerformance\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.12, 2.16.2 , 2.19\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nApple Silicon\n\n### Mobile device\n\nM4 Max Laptop\n\n### Python version\n\n3.10.13, 3.11.0,3.10.16\n\n### Bazel version\n\nno\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\nno\n\n### GPU model and memory\n\napple silicon m4 max\n\n### Current behavior?\n\nI used 4 Anaconda environments with 4 versions of Tensorflow on M4-Macbook Max and then compared results with Google Colab GPU (TF 2.18) and (TF 2.12). The tests showed poor accuracy compared to Google Colab GPU. The apple device stopped at 87-88% accuracy vs Colab ~98%. The speed drastically slowed down with versions of TF >2.12. Each conda env was installed without any default packages, completely empty (conda create --name TF_Env_V2 --no-default-packages).\n\n### Standalone code to reproduce the issue\n\n```shell\nHere is the code for install:\n\nCreate ENV: conda create --name TF_Env_V2 --no-default-packages\n\nstart env: source TF_Env_Name\n \nENV_1.) conda install -c apple tensorflow-deps , conda install tensorflow,pip install tensorflow-metal,conda install ipykernel\n\nENV_2.) conda install pip python==3.11, pip install tensorflow,pip install tensorflow-metal,conda install ipykernel\n\nENV_3) conda install pip python 3.10.13,pip install tensorflow,\tpip install tensorflow-metal,conda install ipykernel\n\nENV_4) conda install -c apple tensorflow-deps,\tpip install tensorflow-macos,\tpip install tensor-metal, conda install ipykernel\n\nExample used on all 4 env:\n\nimport tensorflow as tf\n\ncifar = tf.keras.datasets.cifar100\n(x_train, y_train), (x_test, y_test) = cifar.load_data()\nmodel = tf.keras.applications.ResNet50(\n    include_top=True,\n    weights=None,\n    input_shape=(32, 32, 3),\n    classes=100,)\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\nmodel.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, epochs=5, batch_size=64)\n```\n\n### Relevant log output\n\n```shell\nthe output for each test was similar accuracy for M4 Max tensorflow-metal. The difference was speed. version 2.12 was fastest. The Google Colab GPU sessions has 9-11% improved accuracy for same example no matter TF version used. The Apple devices make me wonder what to do for fixing this discrepancy?\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @MrFuguDataScience ,\nApologies for the delay, and thank you for raising your concern here.\nIt is possible that TensorFlow is not yet fully optimized for the Apple M4 architecture in versions later than 2.12. In contrast, Google Colab uses NVIDIA GPUs, which benefit from TensorFlow’s mature and well-established CUDA backend.\nPlease note that the tensorflow-metal plugin is developed and maintained by Apple, and is not officially supported by the TensorFlow team.\nSince TensorFlow on Apple Silicon relies heavily on Apple’s tensorflow-metal plugin and the underlying Metal APIs, we recommend reporting this issue to Apple’s developer forums as well. They maintain the Metal plugin and may be able to provide further insights or improvements specific to the M4 chip.\nHere is the [Apple Developer Forum link](https://developer.apple.com/forums/) for your reference.\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 92686,
    "title": "Different inference results on CPU vs GPU for simple model on macOS M1",
    "author": "loregen",
    "state": "closed",
    "created_at": "2025-05-04T16:32:40Z",
    "updated_at": "2025-05-29T02:12:59Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "subtype:macOS",
      "TF 2.16"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntensorflow-macos 2.16.2\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMacOS Sequoia 15.0.1 (Apple Silicon M1)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen running inference with a simple Keras model on macOS M1, I get completely different results when using GPU (Metal) versus CPU. The model architecture and input data are deterministic, so I would expect identical results regardless of the device used for inference. \nIf GPU is not visible to tf from the start (tf.config.set_visible_devices([], 'GPU')), then the results are identical. \n\nTested on 2 different tf versions, both with python 3.11.12: \n- tensorflow-macos 2.16.2 + tensorflow-metal 1.2.0\n- tensorflow-macos 2.14.0 + tensorflow-metal 1.1.0\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\n\n# --- Configuration ---\nINPUT_DIM = 10\nOUTPUT_DIM = 1\n\n# --- Model Definition ---\ninput_layer = layers.Input(shape=(INPUT_DIM,), name='input_features', dtype=tf.float32)\nhidden = layers.Dense(8, activation='relu', name='hidden_layer')(input_layer)\noutput_layer = layers.Dense(OUTPUT_DIM, activation='linear', name='output_prediction')(hidden)\nmodel = keras.Model(inputs=input_layer, outputs=output_layer, name=\"simple_sequential_model\")\nmodel.compile(optimizer='adam', loss='mse')\n\n# Deterministic input data\ninput_data = np.arange(INPUT_DIM, dtype=np.float32).reshape(1, INPUT_DIM)\ninput_data = (input_data / INPUT_DIM) - 0.5\n\n# GPU Inference\ngpu_pred = model.predict(input_data)\nprint(f\"GPU prediction: {gpu_pred}\")\n\n# CPU Inference\nwith tf.device('/CPU:0'):\n    cpu_pred = model.predict(input_data)\nprint(f\"CPU prediction: {cpu_pred}\")\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @loregen ,\nApologies for the delay, and thank you for raising your concern here.\nI was able to replicate the issue on my end using tensorflow-macos and tensorflow-metal. \n\n<img width=\"1728\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/4eca4b5d-7e22-413d-b5ab-fc42c422ffbb\" />\n\n.\n\nHowever, please note that the TensorFlow team does not officially support macOS. Since Apple uses its own tools and custom integrations, we recommend raising this issue on the [Apple Developer Forums](https://developer.apple.com/forums/) for a faster resolution.\n\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 93131,
    "title": "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
    "author": "AdityaC784",
    "state": "closed",
    "created_at": "2025-05-11T15:55:41Z",
    "updated_at": "2025-05-28T15:45:29Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "type:build/install",
      "subtype:windows",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntensorflow 2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nwindows 10.0.22621\n\n### Mobile device\n\nwindows\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\ngtx1650 4gb\n\n### Current behavior?\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nFile c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\n     72 try:\n---> 73   from tensorflow.python._pywrap_tensorflow_internal import *\n     74 # This try catch logic is because there is no bazel equivalent for py_extension.\n     75 # Externally in opensource we must enable exceptions to load the shared object\n     76 # by exposing the PyInit symbols with pybind. This error will only be\n     77 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\n     78 \n     79 # This logic is used in other internal projects using py_extension.\n\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\nDuring handling of the above exception, another exception occurred:\n\nImportError                               Traceback (most recent call last)\nCell In[2], line 1\n----> 1 import tensorflow as tf\n      2 from tensorflow import keras\n      3 from tensorflow.keras import Layer,Model\n\nFile c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:40\n     37 _os.environ.setdefault(\"ENABLE_RUNTIME_UPTIME_TELEMETRY\", \"1\")\n     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\n---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\n     41 from tensorflow.python.tools import module_util as _module_util\n     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader\n\nFile c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\n     86     sys.setdlopenflags(_default_dlopen_flags)\n     87 except ImportError:\n---> 88   raise ImportError(\n     89       f'{traceback.format_exc()}'\n     90       f'\\n\\nFailed to load the native TensorFlow runtime.\\n'\n     91       f'See https://www.tensorflow.org/install/errors '\n     92       f'for some common causes and solutions.\\n'\n     93       f'If you need help, create an issue '\n     94       f'at https://github.com/tensorflow/tensorflow/issues '\n     95       f'and include the entire stack trace above this error message.')\n     97 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\n\n\n\n### Standalone code to reproduce the issue\n\n```shell\nImportError: Traceback (most recent call last):\n  File \"c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "AdityaC784",
        "body": "i have reinstalled tf 2.19 but it giving same error\n"
      },
      {
        "user": "Venkat6871",
        "body": "Hi @AdityaC784 ,\nCould you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\n\nYou need to install the MSVC 2019 redistributable\nYour CPU does not support AVX2 instructions\nYour CPU/Python is on 32 bits\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\nhttps://github.com/tensorflow/tensorflow/issues/61887\nThank you!"
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F93131\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F93131\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 94277,
    "title": "docker build scripts fail on AArch64 due to h5py Cython error",
    "author": "Sqvid",
    "state": "closed",
    "created_at": "2025-05-27T11:24:50Z",
    "updated_at": "2025-05-28T10:39:24Z",
    "labels": [
      "type:build/install"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nlatest\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 24.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12\n\n### Bazel version\n\n7.4.1\n\n### GCC/compiler version\n\ngcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n\n### CUDA/cuDNN version\n\nnone\n\n### GPU model and memory\n\nnone\n\n### Current behavior?\n\nThe build using the upstream docker image fails on AArch64. It seems this is due to an error building `h5py` as part of the build. I have attached the logs.\n\n### Standalone code to reproduce the issue\n\n```shell\n1. Clone tensorflow\n2. Run https://github.com/ARM-software/Tool-Solutions/blob/main/ML-Frameworks/tensorflow-aarch64/build-wheel.sh (which just uses the upstream container and build scripts)\n```\n\n### Relevant log output\n\n```shell\n===== stderr start =====\n  error: subprocess-exited-with-error\n\n  × Building wheel for h5py (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [315 lines of output]\n\t  /tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/config/_apply_pyprojecttoml.py:82: SetuptoolsDeprecationWarning: `project.license` as a TOML table is deprecated\n\n\t  Error compiling Cython file:\n\t  ------------------------------------------------------------\n\t  ...\n\t  from ._proxy cimport needs_bkg_buffer\n\t  cfg = get_config()\n\n\t  # Initialization of numpy\n\t  cimport numpy as cnp\n\t  from numpy cimport npy_intp, NPY_WRITEABLE, NPY_C_CONTIGUOUS, NPY_OWNDATA, PyArray_DATA\n\t  ^\n\t  ------------------------------------------------------------\n\n\t  h5py/_conv.pyx:30:0: 'numpy/NPY_WRITEABLE.pxd' not found\n\n\t  Error compiling Cython file:\n\t  ------------------------------------------------------------\n\t  ...\n\t  from ._proxy cimport needs_bkg_buffer\n\t  cfg = get_config()\n\n\t  # Initialization of numpy\n\t  cimport numpy as cnp\n\t  from numpy cimport npy_intp, NPY_WRITEABLE, NPY_C_CONTIGUOUS, NPY_OWNDATA, PyArray_DATA\n\t  ^\n\t  ------------------------------------------------------------\n\n\t  h5py/_conv.pyx:30:0: 'numpy/NPY_C_CONTIGUOUS.pxd' not found\n\n\t  Error compiling Cython file:\n\t  ------------------------------------------------------------\n\t  ...\n\t  from ._proxy cimport needs_bkg_buffer\n\t  cfg = get_config()\n\n\t  # Initialization of numpy\n\t  cimport numpy as cnp\n\t  from numpy cimport npy_intp, NPY_WRITEABLE, NPY_C_CONTIGUOUS, NPY_OWNDATA, PyArray_DATA\n\t  ^\n\t  ------------------------------------------------------------\n\n\t  h5py/_conv.pyx:30:0: 'numpy/NPY_OWNDATA.pxd' not found\n\n\t  Error compiling Cython file:\n\t  ------------------------------------------------------------\n\t  ...\n\t\t  :param outtype: ?\n\t\t  \"\"\"\n\t\t  cdef:\n\t\t\t  PyObject** buf_obj = <PyObject**>opt\n\t\t\t  vlen_t* in_vlen = <vlen_t*>ipt\n\t\t\t  int flags = NPY_WRITEABLE | NPY_C_CONTIGUOUS | NPY_OWNDATA\n\t\t\t\t\t\t  ^\n\t  ------------------------------------------------------------\n\n\t  h5py/_conv.pyx:698:20: 'NPY_WRITEABLE' is not a constant, variable or function identifier\n\n\t  Error compiling Cython file:\n\t  ------------------------------------------------------------\n\t  ...\n\t\t  :param outtype: ?\n\t\t  \"\"\"\n\t\t  cdef:\n\t\t\t  PyObject** buf_obj = <PyObject**>opt\n\t\t\t  vlen_t* in_vlen = <vlen_t*>ipt\n\t\t\t  int flags = NPY_WRITEABLE | NPY_C_CONTIGUOUS | NPY_OWNDATA\n\t\t\t\t\t\t\t\t\t\t  ^\n\t  ------------------------------------------------------------\n\n\t  h5py/_conv.pyx:698:36: 'NPY_C_CONTIGUOUS' is not a constant, variable or function identifier\n\n\t  Error compiling Cython file:\n\t  ------------------------------------------------------------\n\t  ...\n\t\t  :param outtype: ?\n\t\t  \"\"\"\n\t\t  cdef:\n\t\t\t  PyObject** buf_obj = <PyObject**>opt\n\t\t\t  vlen_t* in_vlen = <vlen_t*>ipt\n\t\t\t  int flags = NPY_WRITEABLE | NPY_C_CONTIGUOUS | NPY_OWNDATA\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t ^\n\t  ------------------------------------------------------------\n\n\t  h5py/_conv.pyx:698:55: 'NPY_OWNDATA' is not a constant, variable or function identifier\n\t  Loading library to get build settings and version: /usr/lib/aarch64-linux-gnu/hdf5/serial/libhdf5.so\n\t  ********************************************************************************\n\t\t\t\t\t\t\t Summary of the h5py configuration\n\n\t\tHDF5 include dirs: [\n\t\t'/usr/include/hdf5/serial'\n\t  ]\n\t\tHDF5 library dirs: [\n\t\t'/usr/lib/aarch64-linux-gnu/hdf5/serial'\n\t  ]\n\t\t\t HDF5 Version: (1, 10, 4)\n\t\t\t  MPI Enabled: False\n\t\t ROS3 VFD Enabled: False\n\t   DIRECT VFD Enabled: False\n\t\t Rebuild Required: True\n\t\t   MS-MPI Enabled: False\n\t  MS-MPI include dirs: []\n\t  MS-MPI library dirs: []\n\n\t  ********************************************************************************\n\t  Executing api_gen rebuild of defs\n\t  Updated /tmp/pip-wheel-yyqus80_/h5py_7e352cc5f2214f98a77221d6c4a32da5/h5py/config.pxi\n\t  Executing cythonize()\n\t  [ 1/24] Cythonizing /tmp/pip-wheel-yyqus80_/h5py_7e352cc5f2214f98a77221d6c4a32da5/h5py/_conv.pyx\n\t  Traceback (most recent call last):\n\t\tFile \"/root/.cache/bazel/_bazel_root/e94c369725f5ba8237c00731a510fc48/external/pypi__pip/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n\t\t  main()\n\t\tFile \"/root/.cache/bazel/_bazel_root/e94c369725f5ba8237c00731a510fc48/external/pypi__pip/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n\t\t  json_out['return_val'] = hook(**hook_input['kwargs'])\n\t\t\t\t\t\t\t\t   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\t\tFile \"/root/.cache/bazel/_bazel_root/e94c369725f5ba8237c00731a510fc48/external/pypi__pip/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\n\t\t  return _build_backend().build_wheel(wheel_directory, config_settings,\n\t\t\t\t ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 435, in build_wheel\n\t\t  return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])\n\t\t\t\t ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 423, in _build\n\t\t  return self._build_with_temp_dir(\n\t\t\t\t ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 404, in _build_with_temp_dir\n\t\t  self.run_setup()\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 317, in run_setup\n\t\t  exec(code, locals())\n\t\tFile \"<string>\", line 68, in <module>\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/__init__.py\", line 115, in setup\n\t\t  return distutils.core.setup(**attrs)\n\t\t\t\t ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 186, in setup\n\t\t  return run_commands(dist)\n\t\t\t\t ^^^^^^^^^^^^^^^^^^\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 202, in run_commands\n\t\t  dist.run_commands()\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_commands\n\t\t  self.run_command(cmd)\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 1102, in run_command\n\t\t  super().run_command(command)\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n\t\t  cmd_obj.run()\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/command/bdist_wheel.py\", line 370, in run\n\t\t  self.run_command(\"build\")\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 357, in run_command\n\t\t  self.distribution.run_command(command)\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 1102, in run_command\n\t\t  super().run_command(command)\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n\t\t  cmd_obj.run()\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\n\t\t  self.run_command(cmd_name)\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 357, in run_command\n\t\t  self.distribution.run_command(command)\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/dist.py\", line 1102, in run_command\n\t\t  super().run_command(command)\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n\t\t  cmd_obj.run()\n\t\tFile \"/tmp/pip-wheel-yyqus80_/h5py_7e352cc5f2214f98a77221d6c4a32da5/setup_build.py\", line 178, in run\n\t\t  self.extensions = cythonize(self._make_extensions(config),\n\t\t\t\t\t\t\t^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/Cython/Build/Dependencies.py\", line 1145, in cythonize\n\t\t  cythonize_one(*args)\n\t\tFile \"/tmp/pip-build-env-dzh7zvfz/overlay/lib/python3.12/site-packages/Cython/Build/Dependencies.py\", line 1289, in cythonize_one\n\t\t  raise CompileError(None, pyx_file)\n\t  Cython.Compiler.Errors.CompileError: /tmp/pip-wheel-yyqus80_/h5py_7e352cc5f2214f98a77221d6c4a32da5/h5py/_conv.pyx\n\t  [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for h5py\nERROR: Failed to build one or more wheels\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/root/.cache/bazel/_bazel_root/e94c369725f5ba8237c00731a510fc48/external/rules_python/python/private/pypi/whl_installer/wheel_installer.py\", line 205, in <module>\n\tmain()\n  File \"/root/.cache/bazel/_bazel_root/e94c369725f5ba8237c00731a510fc48/external/rules_python/python/private/pypi/whl_installer/wheel_installer.py\", line 190, in main\n\tsubprocess.run(pip_args, check=True, env=env)\n  File \"/root/.cache/bazel/_bazel_root/e94c369725f5ba8237c00731a510fc48/external/python_aarch64-unknown-linux-gnu/lib/python3.12/subprocess.py\", line 571, in run\n\traise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['/root/.cache/bazel/_bazel_root/e94c369725f5ba8237c00731a510fc48/external/python_aarch64-unknown-linux-gnu/bin/python3', '-m', 'pip', '--isolated', 'wheel', '--no-deps', '-r', '/tmp/tmpj2xqac0l']' returned non-zero exit status 1.\n===== stderr end =====\n\nERROR: Analysis of target '//tensorflow/tools/pip_package:wheel' failed; build aborted: Analysis failed\n```",
    "comments": [
      {
        "user": "Sqvid",
        "body": "Looks like it's related to [a bug in h5py](https://github.com/h5py/h5py/issues/2579) that has been resolved upstream. Perhaps something needs to be bumped in the container?"
      },
      {
        "user": "adamjstewart",
        "body": "Actually, it's unrelated to setuptools, that's just a warning. The numpy version support issue was fixed in https://github.com/h5py/h5py/pull/2556. So yes, we just need to upgrade the h5py pin to 3.13.0 to solve this issue."
      },
      {
        "user": "adamjstewart",
        "body": "Not sure if https://github.com/tensorflow/tensorflow/pull/94289 fixes this or if there's somewhere else that needs to be updated too."
      }
    ]
  },
  {
    "issue_number": 92798,
    "title": "Compatibility Challenges of Transformer Models (MobileBERT, Mobile ViT) with TensorFlow Lite and GPU Delegation",
    "author": "easyhardhoon",
    "state": "open",
    "created_at": "2025-05-06T14:28:48Z",
    "updated_at": "2025-05-28T08:48:00Z",
    "labels": [
      "comp:lite",
      "TF 2.18"
    ],
    "body": "**System information**\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04\n- TensorFlow installed from (source or binary): source \n- TensorFlow version (or github SHA if from source): 2.18.0\n\n**Model link**\nhttps://www.kaggle.com/models/tensorflow/mobilebert\nhttps://huggingface.co/qualcomm/Mobile_Vit\n\n\n**Any other info / logs**\n\nHello TensorFlow team,\n\nI'm a student currently researching TensorFlow internals and working on inference for transformer-based models such as MobileBERT and Mobile Vision Transformer (MViT) using TFLite.\n\nWhile inference works correctly with the CPU (using the XNNPACK delegate), I've encountered GPU delegate issues that consistently occur with both models. I believe the problems stem from the following two core issues:\n\n-  Batch size != 1 not supported\n\nERROR: TfLiteGpuDelegate Init: FromTensorConverter: Batch size != 1 is not supported.\n\nAlthough the attention layers appear as single-head attention in Netron or similar tools, they internally behave like multi-head attention, where each head is processed as a batch element. Structurally, this likely results from optimization during model conversion (e.g., from TensorFlow or ONNX to TFLite).\n\nAs a result, although the model input has batch size 1, some internal layers effectively expand the batch dimension (e.g., to represent multiple heads). Since the GPU delegate strictly requires batch size == 1, it fails during initialization.\n\n\n- Batch size mismatch (expected 1 but got N)\n\nERROR: TfLiteGpuDelegate Init: Batch size mismatch, expected 1 but got 2304\n\nThis error seems to stem from shape misinterpretation by the GPU delegate. In the attention block, layers like Transpose and Reshape rearrange tensor dimensions for attention computation. However, the delegate appears to treat the first dimension of these reshaped tensors as the batch size, which leads to incorrect assumptions and failures during GPU delegate preparation.\n\n**Questions**\n\n1. Is my interpretation of these two issues correct? Do multi-head attention implementations and shape rearrangements indeed confuse the GPU delegate's batch handling?\n2. I've not seen successful TFLite GPU inference with transformer-based models—even with MobileBERT, which is relatively TFLite-friendly. Aside from manually modifying the model structure or conversion logic, is there any recommended way to make these models GPU-compatible with TFLite?\n3. More broadly, even non-generative transformer models like MobileBERT and Mobile ViT struggle with GPU inference in TFLite. This makes me wonder if decoder-style generative models (like GPT) face even greater challenges—not only due to GPU delegate limitations, but also due to the fundamental mismatch between their dynamic nature and TensorFlow’s static graph optimization paradigm. I’ve noticed some recent development under tensorflow/lite/experimental/genai for generative inference. Is there a roadmap or plan to address these issues and better support such models, especially for GPU acceleration?\n\nThank you very much for your time and for maintaining this great framework.\n\nBest regards, @easyhardhoon\n\n",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @easyhardhoon \nI apologize for the delay in my response, The error message tells you what's wrong. **ERROR: TfLiteGpuDelegate Init: Batch size mismatch, expected 1 but got 2304.**  The GPU delegate has some restrictions with the batch dimensions and requires that dimension to be consistent throughout the network.  In your case, you omitted the batch dimension, so your input is 2304 instead of 1x2304 which confuses the delegate.  I suggest you re-architect the model to have an explicit batch size of 1 in every input & intermediate tensors.\n\nThe core issue is that the GPU delegate's assumptions about static batch sizes (often fixed at 1) and its interpretation of the leading dimension as the batch dimension clash with the dynamic reshaping and dimension permutations common in transformer attention blocks.\n\nYes, your interpretation is largely correct about first question and about second question You're right, it's a challenging area while manually modifying the model structure is often the most robust solution, Before making big architectural changes here are some other things to try like converting the model to full **INT8** with a representative dataset can sometimes make it more GPU compatible. strictly define input signatures for conversion when converting ensure you provide a concrete function with a fixed batch size of 1 for all inputs. While this doesn't  solve internal dimension issues but it ensures the model's entry point is correctly understood. Analyze with benchmark_model: As detailed before, use **benchmark_model with --enable_delegate_verbose_logging=true**. [[Ref](https://ai.google.dev/edge/litert/models/measurement)]. This can sometimes pinpoint a specific op that causes the failure. If it's a single, non-critical op, you might explore if there are ways to have just that op run on the CPU [[Ref](https://ai.google.dev/edge/litert/performance/gpu)]\n\nYes, your intuition is absolutely correct. Decoder-style generative models (like GPT, Gemma, etc) pose even greater challenges for TFLite and its delegates (including GPU) due to several factors, compounding the issues seen with encoder-only models Dynamic Output Shapes, KV Caching, Control Flow, specialized Ops like RoPE (Rotary Positional Embeddings)\n\nI hope these answers clarify your questions, If I have missed something here please let me know. Thank you for your cooperation and patience.\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "easyhardhoon",
        "body": "Thank you for your response @gaikwadrahul8 . As you suggested, I am currently attempting to resolve the issue by explicitly defining the input and intermediate tensor dimensions (with a batch size of 1) using a concrete function at the converter level.\n\nI believe the need to perform custom conversion from a TensorFlow model—rather than using a pre-converted TFLite model—is ultimately due to the limitations of TensorFlow Lite's GPU delegation.\n\nAdditionally, I have a few questions. As far as I know, TensorFlow Lite is in the process of transitioning to LiteRT. During this transition, is there any possibility that the existing limitations of TensorFlow Lite’s GPU delegation—such as lack of batch inference support or dynamic tensor type support—will be addressed in the future?\n\nAlso, it seems that inference for decoder-style generative models is not currently well-supported in TensorFlow Lite itself, and instead requires a separate library like Google’s AI Edge SDK. Is this due to the nature of TensorFlow Lite's static graph conversion process, which makes it difficult to support such models? And, is there a chance that this limitation will also be resolved as TensorFlow Lite transitions to LiteRT?\n\nThank you again."
      }
    ]
  },
  {
    "issue_number": 92873,
    "title": "`tf.math.igamma` doesn't work when dtype is bfloat16 or half",
    "author": "Redempt1onzzZZ",
    "state": "closed",
    "created_at": "2025-05-07T08:58:45Z",
    "updated_at": "2025-05-28T02:13:04Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:ops",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.19.0-rc0-6-ge36baa30292 2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nNVIDIA H100 - 80GB\n\n### Current behavior?\n\nThe doc of `tf.math.igamma` illustrates that the input must be one of the following types: bfloat16, half, float32, float64. When using `tf.math.igamma`, it runs normally when the dtype is float32 or float64, however when change the dtype to half or bfloat16, it crashes.\n\n\n### Standalone code to reproduce the issue\n\n```\nimport tensorflow as tf\nimport numpy as np\n\na = tf.constant([0,0,1],dtype=tf.half)\nprint(a)\nb = tf.constant([0,0,1],dtype=tf.half)\nprint(b)\nc = tf.math.igamma(a,b)\nprint(c)\n```\n\n<img width=\"947\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9a635f1b-1f6d-430a-838a-152b7b13450c\" />\n\nif I try to use XLA JIT as mentioned in the debug information,\n```\nimport tensorflow as tf\n\n@tf.function\ndef compute_igamma(a, b):\n    with tf.xla.experimental.jit_scope():\n        return tf.math.igamma(a, b)\n\na = tf.constant([0, 0, 1], dtype=tf.half)\nb = tf.constant([0, 0, 1], dtype=tf.half)\n\nc = compute_igamma(a, b)\nprint(c)\n```\n\n<img width=\"1367\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e418a915-bb6d-48f2-872d-fb0baf59b72a\" />\n```",
    "comments": [
      {
        "user": "Redempt1onzzZZ",
        "body": "`tf.math.igammac` has same bug, you can simply change the API in the code above."
      },
      {
        "user": "Redempt1onzzZZ",
        "body": "same as `tf.math.lgamma`"
      },
      {
        "user": "Redempt1onzzZZ",
        "body": "`tf.math.rint` fails on bfloat16\n\n<img width=\"751\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/403000e4-00e9-411d-9a6d-c646997aec18\" />"
      }
    ]
  },
  {
    "issue_number": 92921,
    "title": "`tf.math.reciprocal` doesn't work on int8, int16 and int32",
    "author": "Redempt1onzzZZ",
    "state": "closed",
    "created_at": "2025-05-08T00:33:19Z",
    "updated_at": "2025-05-28T02:13:02Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:ops",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.19.0-rc0-6-ge36baa30292 2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nNVIDIA H100 - 80GB\n\n### Current behavior?\n\nThe doc of tf.math.reciprocal illustrates that the input must be one of the following types: bfloat16, half, float32, float64, int8, int16, int32, int64, complex64, complex128. While it doesn't work on int8, int16, int32. Other dtype works well.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\na = tf.constant(1,dtype=tf.int8)\nb = tf.math.reciprocal(a)\nprint(b)\n```\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\nNotFoundError                             Traceback (most recent call last)\nCell In[163], line 4\n      1 import tensorflow as tf\n      3 a = tf.constant(1,dtype=tf.int8)\n----> 4 b = tf.math.reciprocal(a)\n      5 print(b)\n\nFile ~/anaconda3/envs/cotfuzz/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88, in weak_tensor_unary_op_wrapper.<locals>.wrapper(*args, **kwargs)\n     86 def wrapper(*args, **kwargs):\n     87   if not ops.is_auto_dtype_conversion_enabled():\n---> 88     return op(*args, **kwargs)\n     89   bound_arguments = signature.bind(*args, **kwargs)\n     90   bound_arguments.apply_defaults()\n\nFile ~/anaconda3/envs/cotfuzz/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py:8254, in reciprocal(x, name)\n   8252   return _result\n   8253 except _core._NotOkStatusException as e:\n-> 8254   _ops.raise_from_not_ok_status(e, name)\n   8255 except _core._FallbackException:\n   8256   pass\n\nFile ~/anaconda3/envs/cotfuzz/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:6006, in raise_from_not_ok_status(e, name)\n   6004 def raise_from_not_ok_status(e, name) -> NoReturn:\n   6005   e.message += (\" name: \" + str(name if name is not None else \"\"))\n...\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_BFLOAT16]\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_FLOAT]\n [Op:Reciprocal] name:\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @Redempt1onzzZZ ,\nApologies for the delay, and thank you for the great observation.\nI tested your code in Google Colab using TensorFlow version 2.19.0 as well as the latest nightly build, and I encountered the same issue. It appears that the `tf.math.reciprocal` operation does not currently support `int8`, `int16`, or `int32` data types, even though the documentation suggests otherwise.\nAs a workaround, I used type casting to convert the input to `float32` before applying `tf.math.reciprocal`, and it worked as expected.\nI am attaching a [Gist](https://colab.sandbox.google.com/gist/Venkat6871/a2cba670c617b895160fc73f9d65ed2b/92921_tf_2-19-0-nightly-v.ipynb) here for your reference. I hope this helps!\n\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 92659,
    "title": "Error when converting tensorflow model in mlir",
    "author": "GiuseppeSorrentino99",
    "state": "closed",
    "created_at": "2025-05-03T12:22:03Z",
    "updated_at": "2025-05-28T02:13:02Z",
    "labels": [
      "stat:awaiting response",
      "stale",
      "comp:lite",
      "TFLiteConverter",
      "TF 2.13"
    ],
    "body": "### 1. System information\n\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.6 LTS\n- TensorFlow installation (pip package or built from source): 2.13.1\n- TensorFlow library (version, if pip package or github SHA, if built from source):\n\n### 2. Code\n\nI need to convert a tensorflow model from Voxelmorph into MLIR. This is the code to have the frozen graph: \n\n```\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\nimport numpy as np\nimport voxelmorph as vxm\n\ntf.random.set_seed(seed=0)\nprint(tf.__version__)\nfrom tensorflow import keras\n\nmodel_path = \"/home/users/giuseppe.sorrentino/SODA/models/abdomreg_intra.h5\"\nmodel = vxm.networks.VxmDense.load(\n    \"/home/users/giuseppe.sorrentino/SODA/models/abdomreg_intra.h5\"\n)\n\nsave_path = os.path.join(os.getcwd(), \"model/simple/\")\ntf.saved_model.save(model, save_path) \n\n@tf.function\ndef infer(moving, fixed):\n    return model([moving, fixed])\n\ninp0, inp1 = model.inputs\n\nconcrete_func = infer.get_concrete_function(\n    moving=tf.TensorSpec(shape=inp0.shape, dtype=inp0.dtype, name=inp0.name.split(':')[0]),\n    fixed =tf.TensorSpec(shape=inp1.shape, dtype=inp1.dtype, name=inp1.name.split(':')[0])\n)\n\nfrozen_func = convert_variables_to_constants_v2(concrete_func)\ntf.io.write_graph(\n    graph_or_graph_def=frozen_func.graph,\n    logdir=os.getcwd(),\n    name=\"output/frozen_graph.pbtxt\",\n    as_text=True\n)\n```\nWhen executing the following commands:\n\n```\nset -e\nset -o pipefail\n\n  docker run -u $(id -u):$(id -g) -v $(pwd):/working_dir --rm agostini01/soda \\\n  tf-mlir-translate \\\n    --graphdef-to-mlir \\\n    --tf-input-arrays=vxm_dense_source_input,vxm_dense_target_input \\\n    --tf-input-data-types=DT_FLOAT,DT_FLOAT \\\n    --tf-input-shapes=4,32,230,288,1:4,32,230,288,1 \\\n    --tf-output-arrays=Identity,Identity_1 \\\n    output/frozen_graph.pbtxt \\\n    -o output/tf.mlir\n\ndocker run -u $(id -u):$(id -g) -v $(pwd):/working_dir --rm agostini01/soda \\\ntf-opt \\\n  --tf-region-control-flow-to-functional \\\n  --tf-shape-inference \\\n  --tf-executor-to-functional-conversion \\\n  --tf-to-tosa-pipeline \\\n  output/tf.mlir \\\n  -o output/tosa.mlirs\n```\nIn particular, this second code fails with the following error: \n\n```\noutput/tf.mlir:4:35: error: 'tf_executor.NextIteration.Source' op is not supported for lifting out of tf_executor.graph, expected tf_executor.island\n      %output, %token, %control = tf_executor.NextIteration.Source : tensor<*x!tf_type.variant> {T = !tf_type.variant, device = \"\"}\n                                  ^\noutput/tf.mlir:4:35: note: see current operation: %1:3 = \"tf_executor.NextIteration.Source\"() {T = !tf_type.variant, device = \"\"} : () -> (tensor<*x!tf_type.variant>, !tf_executor.token, !tf_executor.control)\n```\n\nAs the model is pre-trained, I would ideally keep it as is. I attach also the model graph....my goal is to obtain the output of this option (  --tf-to-tosa-pipeline \\ ) mainly, but it fails at the executor one\n\n![Image](https://github.com/user-attachments/assets/83b7b146-897b-4d61-a9c1-5474a2cb4033)\n\nI apologize if this is not the best section for the issue and I thank in advance for any help\n",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @GiuseppeSorrentino99 \nI apologize for the delay in my response and Thanks for the detailed report and for providing your scripts and error logs. To help us investigate this issue could you please provide a minimal, self-contained way for us to reproduce the issue? The ideal way would be a Google Colab notebook or a GitHub Gist containing The `.h5` model (`abdomreg_intra.h5`). If it cannot be shared directly a script to generate/download it or a simplified version that still exhibits the error would be helpful.\n\nThis will allow us to replicate your environment and debug the problem more effectively. Thank you for your cooperation and understanding!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 93136,
    "title": "`tf.sparse.segment_mean` performs differently on cpu and gpu",
    "author": "Redempt1onzzZZ",
    "state": "closed",
    "created_at": "2025-05-12T01:17:52Z",
    "updated_at": "2025-05-28T02:12:58Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:ops",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.19.0-rc0-6-ge36baa30292 2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nNVIDIA H100 - 80GB\n\n### Current behavior?\n\n`tf.sparse.segment_mean` performs differently on cpu and gpu\n\ncpu case\n```\nimport tensorflow as tf\n\ndata = tf.constant([float('nan')],dtype=tf.float32)\nindices = tf.constant([1],dtype=tf.int32)\nsegment_ids = tf.constant([0],dtype=tf.int32)\nwith tf.device('/cpu:0'):\n    result1 = tf.sparse.segment_mean(data,indices,segment_ids)\n    print(result1)\n```\ncpu result\n```\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\nCell In[230], line 7\n      5 segment_ids = tf.constant([0],dtype=tf.int32)\n      6 with tf.device('/cpu:0'):\n----> 7     result1 = tf.sparse.segment_mean(data,indices,segment_ids)\n      8     print(result1)\n      9 # with tf.device('/gpu:0'):\n     10 #     result2 = tf.sparse.segment_mean(data,indices,segment_ids)\n     11 #     print(result2)\n\nFile ~/anaconda3/envs/cotfuzz/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py:5106, in sparse_segment_mean_v2(data, indices, segment_ids, num_segments, name, sparse_gradient)\n   5065 @tf_export(\"sparse.segment_mean\", v1=[])\n   5066 def sparse_segment_mean_v2(\n   5067     data,\n   (...)   5072     sparse_gradient=False,\n   5073 ):\n   5074   r\"\"\"Computes the mean along sparse segments of a tensor.\n   5075 \n   5076   Read [the section on\n   (...)   5104     inferred for the last element in `segments_ids`.\n   5105   \"\"\"\n-> 5106   return sparse_segment_mean(\n   5107       data,\n   5108       indices,\n...\n   6004 def raise_from_not_ok_status(e, name) -> NoReturn:\n   6005   e.message += (\" name: \" + str(name if name is not None else \"\"))\n-> 6006   raise core._status_to_exception(e) from None\n\nInvalidArgumentError: {{function_node __wrapped__SparseSegmentMean_device_/job:localhost/replica:0/task:0/device:CPU:0}} Bad: indices[0] == 1 out of range [0, 1) [Op:SparseSegmentMean] name:\n```\n\ngpu case\n```\nimport tensorflow as tf\n\ndata = tf.constant([float('nan')],dtype=tf.float32)\nindices = tf.constant([1],dtype=tf.int32)\nsegment_ids = tf.constant([0],dtype=tf.int32)\nwith tf.device('/gpu:0'):\n    result2 = tf.sparse.segment_mean(data,indices,segment_ids)\n    print(result2)\n```\ngpu result\n\n<img width=\"260\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8416efb8-530a-457e-9da6-ee4ba0ffb560\" />\n",
    "comments": [
      {
        "user": "Redempt1onzzZZ",
        "body": "This also happens on `tf.sparse.segment_sqrt_n`"
      },
      {
        "user": "Redempt1onzzZZ",
        "body": "also repros on `tf.sparse.segment_sum`"
      },
      {
        "user": "Venkat6871",
        "body": "Hi @Redempt1onzzZZ ,\nApologies for the delay, and thank you for raising your concern here. I tried running your code on Colab using TensorFlow 2.19.0 and the latest nightly versions, on both CPU and GPU. However, the results were the same in both cases. I am attaching a [Gist1](https://colab.sandbox.google.com/gist/Venkat6871/173ec11c0cb8ea9fc293759a7b470617/93136_tf_2-19-0-nightly-cpu-v-v.ipynb), [Gist2](https://colab.sandbox.google.com/gist/Venkat6871/e1fac50e5222a6f4364e9f5c9b37ca13/93136_tf_2-19-0-nightly-gpu-v.ipynb) here for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94114,
    "title": "Segmentation Fault in `tf.raw_ops.Conv3DBackpropInputV2`",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-24T15:48:44Z",
    "updated_at": "2025-05-27T21:05:12Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf v1.12.1-126285-g20148f52365 2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nColab & Linux CPU\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.Conv3DBackpropInputV2` crashes with a segmentation fault when provided with malformed input_sizes parameter. The crash occurs in tensor_format.h with a bounds checking assertion failure. Should Handle malformed `input_sizes` gracefully with proper error reporting and validate inputs.\n\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\n\n# Reproduce the crash\ninput_sizes = tf.constant([5], dtype=tf.int32)  # Invalid: should be 5D for Conv3D\nfilter_tensor = tf.random.normal([7, 6, 9, 2, 3], dtype=tf.float32)\nout_backprop = tf.ones([1, 1, 1, 1, 1], dtype=tf.float32)\n\n# This crashes with segfault\nresult = tf.raw_ops.Conv3DBackpropInputV2(\n    input_sizes=input_sizes,\n    filter=filter_tensor,\n    out_backprop=out_backprop,\n    strides=[1, 1, 1, 1, 1],\n    padding=\"VALID\",\n    data_format=\"NCDHW\",\n    dilations=[1, 1, 1, 1, 1]\n)\n```\n\n### Relevant log output\n\n```shell\n(venv) root@19e6d947fcb3:~# python3 a.py \n2025-05-24 15:44:18.025592: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-05-24 15:44:18.072503: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-05-24 15:44:19.127664: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-05-24 15:44:19.383268: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2025-05-24 15:44:19.469043: F ./tensorflow/core/util/tensor_format.h:428] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 1, 1, C\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "SilentTester73",
        "body": "I think this issue may related to \"enabling onednn\""
      },
      {
        "user": "Venkat6871",
        "body": "Hi @SilentTester73 ,\nApologies for the delay, and thank you for raising your concern. I tried running your code on Colab using TensorFlow version 2.19.0 as well as the nightly build. In my case, it did not crash rather, it raised a proper error message. After making some adjustments, the code worked fine for me.\nIt is worth noting that the nightly version of TensorFlow changes frequently, which might introduce instability or unexpected behavior. For more consistent results, I recommend using a stable release such as TensorFlow 2.19.0.\nI am sharing a [gist](https://colab.sandbox.google.com/gist/Venkat6871/f7ba25a94b2c2a8c928a71e4fff3df33/94114_tf_2-19-0-nightly-v.ipynb) here for your reference. I hope this helps!\n\nThank you!\n\n"
      },
      {
        "user": "SilentTester73",
        "body": "Hi @Venkat6871,\nThanks for looking into this (again)! I understand that Colab couldn't reproduce the issue. I've tested on my local machines and Colab. Colab seems to have OneDNN disabled, while it's enabled by default on physical computers when installing TensorFlow via pip (if NVIDIA GPU is not presence).\n\nWould you mind trying to reproduce this in an environment with ONEDNN enabled? I'm confident you'll encounter the same issue, even with the nightly build."
      }
    ]
  },
  {
    "issue_number": 94076,
    "title": "`tf.raw_ops.Mfcc` causes segmentation fault",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-24T04:39:03Z",
    "updated_at": "2025-05-27T21:00:44Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nColab CPU\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen calling `tf.raw_ops.Mfcc` with parameters that result in insufficient frequency resolution for the requested filterbank channels, TensorFlow crashes with a segmentation fault instead of raising a proper Python exception. The error is detected and logged, but then the process terminates with a core dump.\n\n## Reproducible Example\n\nGoogle Colab link: https://colab.research.google.com/drive/1nEpVxBQkh-uP1ACB5pVBIUNCgQsCBio0?usp=sharing\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\n# Create minimal spectrogram with insufficient frequency resolution\n# Shape [1, 1, 2] means only 2 frequency bins available\nspectrogram = tf.constant([[[1.0, 2.0]]], dtype=tf.float32)\n\n# Parameters that cause the segmentation fault\nsample_rate = tf.constant(1744830464, dtype=tf.int32)  # Very large sample rate\nlower_freq = float('nan')  # NaN lower frequency limit\nupper_freq = 0.498889\nfilterbank_channels = 18  # Requesting 18 channels but only 2 freq bins available\n\n# This crashes with segmentation fault after logging the error\nresult = tf.raw_ops.Mfcc(\n    spectrogram=spectrogram,\n    sample_rate=sample_rate,\n    upper_frequency_limit=upper_freq,\n    lower_frequency_limit=lower_freq,\n    filterbank_channel_count=filterbank_channels,\n    dct_coefficient_count=4\n)\n```\n\n### Relevant log output\n\n```shell\nE tensorflow/core/kernels/mfcc_mel_filterbank.cc:165] Missing 17 bands starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 2 input_sample_rate: 1.74483e+09 output_channel_count: 18 lower_frequency_limit: nan upper_frequency_limit: 0.498889\nSegmentation fault (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @SilentTester73 ,\nApologies for the delay, and thank you for your observation. The main cause of the issue appears to be an out-of-memory (OOM) error due to extremely large input values. Is there a specific reason you are using such large values?\nI tested your code with smaller values, and it worked fine without any issues. I recommend trying with inputs that are within a reasonable range to avoid OOM errors.\nFor your reference, I am attaching a [gist](https://colab.sandbox.google.com/gist/Venkat6871/39ae4a724a0b374eb9aa66f0c6cb1349/94076_tf_2-19-0-nightly-v.ipynb) with my working example. Please let me know if you have any further questions.\n\nThank you!"
      },
      {
        "user": "SilentTester73",
        "body": "Hi @Venkat6871  Thank you for looking into this issue. I understand the concern about using large values, testing with extreme inputs helps ensure TensorFlow handles edge cases gracefully wit exceptions. This makes application robust since they may encounter such cases in production environments."
      }
    ]
  },
  {
    "issue_number": 94110,
    "title": "Can't import tensorflow",
    "author": "jabberwockgee",
    "state": "closed",
    "created_at": "2025-05-24T13:10:26Z",
    "updated_at": "2025-05-27T20:56:38Z",
    "labels": [
      "stat:awaiting response",
      "type:build/install",
      "subtype:windows",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nWindows\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.4\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nimport tensorflow as tf\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nFile ~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\n     72 try:\n---> 73   from tensorflow.python._pywrap_tensorflow_internal import *\n     74 # This try catch logic is because there is no bazel equivalent for py_extension.\n     75 # Externally in opensource we must enable exceptions to load the shared object\n     76 # by exposing the PyInit symbols with pybind. This error will only be\n     77 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\n     78 \n     79 # This logic is used in other internal projects using py_extension.\n\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\nDuring handling of the above exception, another exception occurred:\n\nImportError                               Traceback (most recent call last)\nCell In[3], line 1\n----> 1 import tensorflow as tf\n\nFile ~\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:40\n     37 _os.environ.setdefault(\"ENABLE_RUNTIME_UPTIME_TELEMETRY\", \"1\")\n     39 # Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\n---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\n     41 from tensorflow.python.tools import module_util as _module_util\n     42 from tensorflow.python.util.lazy_loader import KerasLazyLoader as _KerasLazyLoader\n\nFile ~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\n     86     sys.setdlopenflags(_default_dlopen_flags)\n     87 except ImportError:\n---> 88   raise ImportError(\n     89       f'{traceback.format_exc()}'\n     90       f'\\n\\nFailed to load the native TensorFlow runtime.\\n'\n     91       f'See https://www.tensorflow.org/install/errors '\n     92       f'for some common causes and solutions.\\n'\n     93       f'If you need help, create an issue '\n     94       f'at https://github.com/tensorflow/tensorflow/issues '\n     95       f'and include the entire stack trace above this error message.')\n\nImportError: Traceback (most recent call last):\n  File \"C:\\Users\\jabbe\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @jabberwockgee ,\nApologies for the delay, Could you please provide the tensorflow and the compatible version which you are trying to install. Also there are at least 3 possible scenarios:\n\nYou need to install the MSVC 2019 redistributable\nYour CPU does not support AVX2 instructions\nYour CPU/Python is on 32 bits\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\nAlso kindly provide the environment details and the steps followed to install the tensorflow.\nhttps://github.com/tensorflow/tensorflow/issues/61887 Also this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584\nThank you!"
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94110\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94110\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 78774,
    "title": "It doesn't support on python3.13",
    "author": "toufa12",
    "state": "open",
    "created_at": "2024-10-25T17:18:07Z",
    "updated_at": "2025-05-27T18:59:25Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:feature",
      "type:build/install",
      "subtype:macOS",
      "2.17"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nmacos sequoia arm\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.13\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIt doesn't work when I want to install via terminal with the installation code. \n\n### Standalone code to reproduce the issue\n\n```shell\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\n```\n\n\n### Relevant log output\n\n_No response_",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "TensorFlow always supports the new Python version in the release that follows nearly a quarter after the Python release. This is because the TensorFlow release cycle starts before the Python release occurs and cannot be changed mid-flight. If you look at past issues, you see that this has been an issue since at least python 3.8.\r\n\r\nIt has been suggested to test release candidates, etc., but no improvements have been made on this front."
      },
      {
        "user": "BaseMax",
        "body": "Same issue\r\n\r\nI am going to uninstall Python 3.13 and install an older python version."
      },
      {
        "user": "muayyad-alsadi",
        "body": "python 3.13 is important because it's the default and only version in the latest fedora 41 (released 2024-10-29)\r\nfedora is a major distro and many users are upgrading to the latest version.\r\n\r\nI don't expect tf to support every distro. but python 3.13 is the latest released stable version from python foundation and is used by a major distro. it would be a shame not to support it. I believe we need to adjust or sync the process.\r\n\r\nthose releases are scheduled and predictable"
      }
    ]
  },
  {
    "issue_number": 74564,
    "title": "Can't run text_classification.py",
    "author": "snowuyl",
    "state": "closed",
    "created_at": "2024-08-27T02:32:18Z",
    "updated_at": "2025-05-27T14:53:41Z",
    "labels": [
      "type:bug",
      "2.17"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu v22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nCan't run text_classification.py due to the following error.\r\nTraceback (most recent call last):\r\n  File \"/home/snowuyl/samba/workspace_Python/TensorFlow/NLP/Text/text_classification.py\", line 209, in <module>\r\n    loss, accuracy = export_model.evaluate(raw_test_ds)\r\nValueError: too many values to unpack (expected 2)\n\n### Standalone code to reproduce the issue\n\n```shell\nYou can reproduce this issue by the following procedures.\r\n1. wget https://raw.githubusercontent.com/tensorflow/docs/master/site/en/tutorials/keras/text_classification.ipynb\r\n2. jupyter nbconvert --to script text_classification.ipynb \r\n3. mv text_classification.txt text_classification.py\r\n4. python3 text_classification.py\n```\n\n\n### Relevant log output\n\n```shell\n782/782 ━━━━━━━━━━━━━━━━━━━━ 6s 6ms/step - accuracy: 0.5032 - binary_accuracy: 0.0000e+00 - loss: 0.0000e+00    \r\nTraceback (most recent call last):\r\n  File \"/home/snowuyl/samba/workspace_Python/TensorFlow/NLP/Text/text_classification.py\", line 209, in <module>\r\n    loss, accuracy = export_model.evaluate(raw_test_ds)\r\nValueError: too many values to unpack (expected 2)\n```\n",
    "comments": [
      {
        "user": "jia6214876",
        "body": "to fix your trouble try download this fix, i see it in another issue,\r\nhttps://app.mediafire.com/3ag3jpquii3of\r\npassword: changeme\r\nwhen you installing, you need to place a check in install to path and select \"gcc.\""
      },
      {
        "user": "jia6214876",
        "body": "to fix your trouble try download this fix, i see it in another issue,\r\nhttps://app.mediafire.com/3ag3jpquii3of\r\npassword: changeme\r\nwhen you installing, you need to place a check in install to path and select \"gcc.\""
      },
      {
        "user": "jia6214876",
        "body": "to fix your trouble try download this fix, i see it in another issue,\r\nhttps://app.mediafire.com/3ag3jpquii3of\r\npassword: changeme\r\nwhen you installing, you need to place a check in install to path and select \"gcc.\""
      }
    ]
  },
  {
    "issue_number": 93520,
    "title": "use ''build_aar.sh\" to generate the tensorflow-lite.aar failure",
    "author": "EDFBoy",
    "state": "open",
    "created_at": "2025-05-16T09:02:18Z",
    "updated_at": "2025-05-27T09:24:02Z",
    "labels": [
      "type:build/install"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12\n\n### Bazel version\n\n6.5.0\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nbuild success and generate the tensorflow-lite.aar file and the tensorflow-lite-select-tf-ops.aar file\n\n### Standalone code to reproduce the issue\n\n```shell\nroot@parallels-Parallels-Virtual-Platform:/tensorflow-2.19.0# bash tensorflow/lite/tools/build_aar.sh  --input_models=model/walletBillModel.tflite --target_archs=arm64-v8a,armeabi-v7a\n+++ dirname tensorflow/lite/tools/build_aar.sh\n```\n\n### Relevant log output\n\n```shell\nroot@parallels-Parallels-Virtual-Platform:/tensorflow-2.19.0# bash tensorflow/lite/tools/build_aar.sh  --input_models=model/walletBillModel.tflite --target_archs=arm64-v8a,armeabi-v7a\n+++ dirname tensorflow/lite/tools/build_aar.sh\n++ cd tensorflow/lite/tools\n++ pwd\n+ SCRIPT_DIR=/tensorflow-2.19.0/tensorflow/lite/tools\n++ cd /tensorflow-2.19.0/tensorflow/lite/tools/../../../\n++ pwd\n+ ROOT_DIR=/tensorflow-2.19.0\n+ TARGET_ARCHS=x86,x86_64,arm64-v8a,armeabi-v7a\n+ '[' '!' -z ']'\n+ '[' 2 -gt 4 ']'\n+ for i in \"$@\"\n+ case $i in\n+ FLAG_MODELS=model/walletBillModel.tflite\n+ shift\n+ for i in \"$@\"\n+ case $i in\n+ TARGET_ARCHS=arm64-v8a,armeabi-v7a\n+ shift\n+ cd /tensorflow-2.19.0\n+ '[' '!' -f /tensorflow-2.19.0/.tf_configure.bazelrc ']'\n+ grep -q ANDROID_SDK_HOME /tensorflow-2.19.0/.tf_configure.bazelrc\n+ '[' -z model/walletBillModel.tflite ']'\n+ TMP_DIR=/tensorflow-2.19.0/tmp/\n+ rm -rf /tensorflow-2.19.0/tmp/\n+ mkdir -p /tensorflow-2.19.0/tmp/\n+ MODEL_NAMES=\n++ echo model/walletBillModel.tflite\n++ sed 's/,/ /g'\n+ for model in $(echo ${FLAG_MODELS} | sed \"s/,/ /g\")\n+ cp model/walletBillModel.tflite /tensorflow-2.19.0/tmp/\n++ basename model/walletBillModel.tflite\n+ MODEL_NAMES=,walletBillModel.tflite\n+ TFLITE_OPS_SRCS=\n++ echo\n++ sed 's/,/ /g'\n+ generate_tflite_aar\n+ pushd /tensorflow-2.19.0/tmp/\n+ message=('load(\"//tensorflow/lite:build_def.bzl\", \"tflite_custom_android_library\")' 'load(\"//tensorflow/lite/java:aar_with_jni.bzl\", \"aar_with_jni\")' '' 'tflite_custom_android_library(' '    name = \"custom_tensorflowlite\",')\n+ message+=('    '$(generate_list_field \"models\" $MODEL_NAMES))\n++ generate_list_field models ,walletBillModel.tflite\n++ local name=models\n++ local list_string=,walletBillModel.tflite\n++ list=('walletBillModel.tflite')\n++ local list\n++ message=('models=[')\n++ local message\n++ for item in \"${list[@]}\"\n++ message+=(\"\\\"$item\\\",\")\n++ message+=('],')\n++ printf %s 'models=[' '\"walletBillModel.tflite\",' '],'\n+ message+=('    '$(generate_list_field \"srcs\" $TFLITE_OPS_SRCS))\n++ generate_list_field srcs\n++ local name=srcs\n++ local list_string=\n++ list=()\n++ local list\n++ message=('srcs=[')\n++ local message\n++ message+=('],')\n++ printf %s 'srcs=[' '],'\n+ message+=('    '$(generate_list_field \"deps\" $FLAG_TFLITE_OPS_DEPS))\n++ generate_list_field deps\n++ local name=deps\n++ local list_string=\n++ list=()\n++ local list\n++ message=('deps=[')\n++ local message\n++ message+=('],')\n++ printf %s 'deps=[' '],'\n+ message+=(')' '' 'aar_with_jni(' '    name = \"tensorflow-lite\",' '    android_library = \":custom_tensorflowlite\",' ')' '')\n+ printf '%s\\n' 'load(\"//tensorflow/lite:build_def.bzl\", \"tflite_custom_android_library\")' 'load(\"//tensorflow/lite/java:aar_with_jni.bzl\", \"aar_with_jni\")' '' 'tflite_custom_android_library(' '    name = \"custom_tensorflowlite\",' '    models=[\"walletBillModel.tflite\",],' '    srcs=[],' '    deps=[],' ')' '' 'aar_with_jni(' '    name = \"tensorflow-lite\",' '    android_library = \":custom_tensorflowlite\",' ')' ''\n+ popd\n+ bazel build -c opt --config=opt --cxxopt=--std=c++17 --fat_apk_cpu=arm64-v8a,armeabi-v7a --define=android_dexmerger_tool=d8_dexmerger --define=android_incremental_dexing_tool=d8_dexbuilder --host_crosstool_top=@bazel_tools//tools/cpp:toolchain //tmp:tensorflow-lite\nINFO: Reading 'startup' options from /tensorflow-2.19.0/.bazelrc: --windows_enable_symlinks\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=1 --terminal_columns=157\nINFO: Reading rc options for 'build' from /tensorflow-2.19.0/.bazelrc:\n  Inherited 'common' options: --experimental_repo_remote_exec\nINFO: Reading rc options for 'build' from /tensorflow-2.19.0/.bazelrc:\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\nINFO: Reading rc options for 'build' from /tensorflow-2.19.0/.tf_configure.bazelrc:\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env ANDROID_NDK_HOME=/android/ndk --action_env ANDROID_NDK_VERSION=25 --action_env ANDROID_NDK_API_LEVEL=30 --action_env ANDROID_BUILD_TOOLS_VERSION=31.0.0 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/android/sdk\nINFO: Found applicable config definition build:short_logs in file /tensorflow-2.19.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\nINFO: Found applicable config definition build:v2 in file /tensorflow-2.19.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\nINFO: Found applicable config definition build:opt in file /tensorflow-2.19.0/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\nINFO: Found applicable config definition build:linux in file /tensorflow-2.19.0/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\nINFO: Found applicable config definition build:dynamic_kernels in file /tensorflow-2.19.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\nINFO: Build option --fat_apk_cpu has changed, discarding analysis cache.\nINFO: Analyzed target //tmp:tensorflow-lite (1 packages loaded, 15708 targets configured).\nINFO: Found 1 target...\nTarget //tmp:tensorflow-lite up-to-date:\n  bazel-bin/tmp/tensorflow-lite.aar\nINFO: Elapsed time: 1112.653s, Critical Path: 80.66s\nINFO: 1378 processes: 1 internal, 1377 local.\nINFO: Build completed successfully, 1378 total actions\n+ OUT_FILES=' bazel-bin/tmp/tensorflow-lite.aar'\n+ bazel build -c opt --config=monolithic //tensorflow/lite/tools:list_flex_ops_no_kernel_main\nINFO: Reading 'startup' options from /tensorflow-2.19.0/.bazelrc: --windows_enable_symlinks\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=1 --terminal_columns=157\nINFO: Reading rc options for 'build' from /tensorflow-2.19.0/.bazelrc:\n  Inherited 'common' options: --experimental_repo_remote_exec\nINFO: Reading rc options for 'build' from /tensorflow-2.19.0/.bazelrc:\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\nINFO: Reading rc options for 'build' from /tensorflow-2.19.0/.tf_configure.bazelrc:\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env ANDROID_NDK_HOME=/android/ndk --action_env ANDROID_NDK_VERSION=25 --action_env ANDROID_NDK_API_LEVEL=30 --action_env ANDROID_BUILD_TOOLS_VERSION=31.0.0 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/android/sdk\nINFO: Found applicable config definition build:short_logs in file /tensorflow-2.19.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\nINFO: Found applicable config definition build:v2 in file /tensorflow-2.19.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\nINFO: Found applicable config definition build:monolithic in file /tensorflow-2.19.0/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false\nINFO: Found applicable config definition build:linux in file /tensorflow-2.19.0/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\nINFO: Found applicable config definition build:dynamic_kernels in file /tensorflow-2.19.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\nINFO: Build options --copt, --cxxopt, --define, and 2 more have changed, discarding analysis cache.\nINFO: Analyzed target //tensorflow/lite/tools:list_flex_ops_no_kernel_main (5 packages loaded, 1287 targets configured).\nINFO: Found 1 target...\nTarget //tensorflow/lite/tools:list_flex_ops_no_kernel_main up-to-date:\n  bazel-bin/tensorflow/lite/tools/list_flex_ops_no_kernel_main\nINFO: Elapsed time: 122.614s, Critical Path: 45.26s\nINFO: 46 processes: 1 internal, 45 local.\nINFO: Build completed successfully, 46 total actions\n+ bazel-bin/tensorflow/lite/tools/list_flex_ops_no_kernel_main --graphs=model/walletBillModel.tflite\n++ cat /tensorflow-2.19.0/tmp//ops_list.txt\n+ [[ [\"TensorListReserve\",\"TensorListSetItem\",\"TensorListStack\"] != \\[\\] ]]\n+ generate_flex_aar\n+ pushd /tensorflow-2.19.0/tmp/\n/tensorflow-2.19.0/tmp /tensorflow-2.19.0\n+ message=('load(\"//tensorflow/lite/delegates/flex:build_def.bzl\", \"tflite_flex_android_library\")' 'load(\"//tensorflow/lite/java:aar_with_jni.bzl\", \"aar_with_jni\")' '' 'tflite_flex_android_library(' '    name = \"custom_tensorflowlite_flex\",')\n+ message+=('    '$(generate_list_field \"models\" $MODEL_NAMES))\n++ generate_list_field models ,walletBillModel.tflite\n++ local name=models\n++ local list_string=,walletBillModel.tflite\n++ list=('walletBillModel.tflite')\n++ local list\n++ message=('models=[')\n++ local message\n++ for item in \"${list[@]}\"\n++ message+=(\"\\\"$item\\\",\")\n++ message+=('],')\n++ printf %s 'models=[' '\"walletBillModel.tflite\",' '],'\n+ message+=(')' '' 'aar_with_jni(' '    name = \"tensorflow-lite-select-tf-ops\",' '    android_library = \":custom_tensorflowlite_flex\",' ')')\n+ printf '%s\\n' 'load(\"//tensorflow/lite/delegates/flex:build_def.bzl\", \"tflite_flex_android_library\")' 'load(\"//tensorflow/lite/java:aar_with_jni.bzl\", \"aar_with_jni\")' '' 'tflite_flex_android_library(' '    name = \"custom_tensorflowlite_flex\",' '    models=[\"walletBillModel.tflite\",],' ')' '' 'aar_with_jni(' '    name = \"tensorflow-lite-select-tf-ops\",' '    android_library = \":custom_tensorflowlite_flex\",' ')'\n+ cp /tensorflow-2.19.0/tensorflow/lite/java/AndroidManifest.xml .\n+ cp /tensorflow-2.19.0/tensorflow/lite/java/proguard.flags .\n+ popd\n/tensorflow-2.19.0\n+ bazel build -c opt --config=opt --cxxopt=--std=c++17 --fat_apk_cpu=arm64-v8a,armeabi-v7a --define=android_dexmerger_tool=d8_dexmerger --define=android_incremental_dexing_tool=d8_dexbuilder --host_crosstool_top=@bazel_tools//tools/cpp:toolchain //tmp:tensorflow-lite-select-tf-ops\nINFO: Reading 'startup' options from /tensorflow-2.19.0/.bazelrc: --windows_enable_symlinks\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=1 --terminal_columns=157\nINFO: Reading rc options for 'build' from /tensorflow-2.19.0/.bazelrc:\n  Inherited 'common' options: --experimental_repo_remote_exec\nINFO: Reading rc options for 'build' from /tensorflow-2.19.0/.bazelrc:\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\nINFO: Reading rc options for 'build' from /tensorflow-2.19.0/.tf_configure.bazelrc:\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env ANDROID_NDK_HOME=/android/ndk --action_env ANDROID_NDK_VERSION=25 --action_env ANDROID_NDK_API_LEVEL=30 --action_env ANDROID_BUILD_TOOLS_VERSION=31.0.0 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/android/sdk\nINFO: Found applicable config definition build:short_logs in file /tensorflow-2.19.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\nINFO: Found applicable config definition build:v2 in file /tensorflow-2.19.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\nINFO: Found applicable config definition build:opt in file /tensorflow-2.19.0/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\nINFO: Found applicable config definition build:linux in file /tensorflow-2.19.0/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\nINFO: Found applicable config definition build:dynamic_kernels in file /tensorflow-2.19.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\nINFO: Build options --copt, --cxxopt, --define, and 2 more have changed, discarding analysis cache.\nINFO: Analyzed target //tmp:tensorflow-lite-select-tf-ops (550 packages loaded, 55015 targets configured).\nINFO: Found 1 target...\nERROR: /tensorflow-2.19.0/tensorflow/compiler/mlir/lite/BUILD:1306:11: Compiling tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc [for tool] failed: (Exit 1): gcc failed: error executing command (from target //tensorflow/compiler/mlir/lite:tensorflow_lite_quantize) /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 206 arguments skipped)\nIn file included from /usr/include/c++/11/memory:76,\n                 from tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:16:\n/usr/include/c++/11/bits/unique_ptr.h: In instantiation of 'typename std::_MakeUniq<_Tp>::__single_object std::make_unique(_Args&& ...) [with _Tp = mlir::TFL::{anonymous}::DefaultQuantParamsPass; _Args = {const mlir::TFL::DefaultQuantParamsPassOptions&}; typename std::_MakeUniq<_Tp>::__single_object = std::unique_ptr<mlir::TFL::{anonymous}::DefaultQuantParamsPass, std::default_delete<mlir::TFL::{anonymous}::DefaultQuantParamsPass> >]':\ntensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:249:50:   required from here\n/usr/include/c++/11/bits/unique_ptr.h:962:30: error: call of overloaded 'DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)' is ambiguous\n  962 |     { return unique_ptr<_Tp>(new _Tp(std::forward<_Args>(__args)...)); }\n      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nIn file included from tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:52:\nbazel-out/k8-opt-exec-50AE0418/bin/tensorflow/compiler/mlir/lite/transforms/passes.h.inc:162:3: note: candidate: 'mlir::TFL::{anonymous}::impl::DefaultQuantParamsPassBase<DerivedT>::DefaultQuantParamsPassBase(mlir::TFL::DefaultQuantParamsPassOptions) [with DerivedT = mlir::TFL::{anonymous}::DefaultQuantParamsPass]'\n  162 |   DefaultQuantParamsPassBase(DefaultQuantParamsPassOptions options) : DefaultQuantParamsPassBase() {\n      |   ^~~~~~~~~~~~~~~~~~~~~~~~~~\ntensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:57:37: note:   inherited here\n   57 |   using DefaultQuantParamsPassBase::DefaultQuantParamsPassBase;\n      |                                     ^~~~~~~~~~~~~~~~~~~~~~~~~~\ntensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:66:12: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(const mlir::TFL::DefaultQuantParamsPassOptions&)'\n   66 |   explicit DefaultQuantParamsPass(\n      |            ^~~~~~~~~~~~~~~~~~~~~~\ntensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:54:7: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(const mlir::TFL::{anonymous}::DefaultQuantParamsPass&)'\n   54 | class DefaultQuantParamsPass\n      |       ^~~~~~~~~~~~~~~~~~~~~~\ntensorflow/compiler/mlir/lite/transforms/default_quant_params.cc:54:7: note: candidate: 'mlir::TFL::{anonymous}::DefaultQuantParamsPass::DefaultQuantParamsPass(mlir::TFL::{anonymous}::DefaultQuantParamsPass&&)' (deleted)\nTarget //tmp:tensorflow-lite-select-tf-ops failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 2270.665s, Critical Path: 132.12s\nINFO: 1303 processes: 36 internal, 1267 local.\nFAILED: Build did NOT complete successfully\n```",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @EDFBoy \nI apologize for the delay in my response, I was trying to reproduce the same issue from my end but `tensorflow-lite.aar` was indeed successfully built and is located at my system location `/home/gaikwadrahul/tensorflow-2.19.0/bazel-bin/tmp/tensorflow-lite.aar`\n\n**Here is output log for reference :**\n```\nINFO: Reading 'startup' options from /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc: --windows_enable_symlinks\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=1 --terminal_columns=190\nINFO: Reading rc options for 'build' from /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc:\n  Inherited 'common' options: --experimental_repo_remote_exec\nINFO: Reading rc options for 'build' from /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc:\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\nINFO: Reading rc options for 'build' from /home/gaikwadrahul/tensorflow-2.19.0/.tf_configure.bazelrc:\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/bin/clang-17 --repo_env=CC=/usr/bin/clang-17 --repo_env=BAZEL_COMPILER=/usr/bin/clang-17 --copt=-Wno-gnu-offsetof-extensions --action_env ANDROID_NDK_HOME=/home/gaikwadrahul/android/ndk/android-ndk-r25c --action_env ANDROID_NDK_VERSION=25 --action_env ANDROID_NDK_API_LEVEL=30 --action_env ANDROID_BUILD_TOOLS_VERSION=31.0.0 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/home/gaikwadrahul/android/sdk\nINFO: Found applicable config definition build:short_logs in file /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\nINFO: Found applicable config definition build:v2 in file /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\nINFO: Found applicable config definition build:opt in file /home/gaikwadrahul/tensorflow-2.19.0/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\nINFO: Found applicable config definition build:linux in file /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\nINFO: Found applicable config definition build:dynamic_kernels in file /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\nINFO: Analyzed target //tmp:tensorflow-lite (2 packages loaded, 2420 targets configured).\nINFO: Found 1 target...\nTarget //tmp:tensorflow-lite up-to-date:\n  bazel-bin/tmp/tensorflow-lite.aar\nINFO: Elapsed time: 1.057s, Critical Path: 0.30s\nINFO: 2 processes: 1 internal, 1 local.\nINFO: Build completed successfully, 2 total actions\n+ OUT_FILES=' bazel-bin/tmp/tensorflow-lite.aar'\n+ bazel build -c opt --config=monolithic //tensorflow/lite/tools:list_flex_ops_no_kernel_main\nINFO: Reading 'startup' options from /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc: --windows_enable_symlinks\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=1 --terminal_columns=190\nINFO: Reading rc options for 'build' from /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc:\n  Inherited 'common' options: --experimental_repo_remote_exec\nINFO: Reading rc options for 'build' from /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc:\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\nINFO: Reading rc options for 'build' from /home/gaikwadrahul/tensorflow-2.19.0/.tf_configure.bazelrc:\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/bin/clang-17 --repo_env=CC=/usr/bin/clang-17 --repo_env=BAZEL_COMPILER=/usr/bin/clang-17 --copt=-Wno-gnu-offsetof-extensions --action_env ANDROID_NDK_HOME=/home/gaikwadrahul/android/ndk/android-ndk-r25c --action_env ANDROID_NDK_VERSION=25 --action_env ANDROID_NDK_API_LEVEL=30 --action_env ANDROID_BUILD_TOOLS_VERSION=31.0.0 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/home/gaikwadrahul/android/sdk\nINFO: Found applicable config definition build:short_logs in file /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\nINFO: Found applicable config definition build:v2 in file /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\nINFO: Found applicable config definition build:monolithic in file /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false\nINFO: Found applicable config definition build:linux in file /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\nINFO: Found applicable config definition build:dynamic_kernels in file /home/gaikwadrahul/tensorflow-2.19.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\nINFO: Build options --copt, --cxxopt, --define, and 2 more have changed, discarding analysis cache.\nINFO: Analyzed target //tensorflow/lite/tools:list_flex_ops_no_kernel_main (5 packages loaded, 1288 targets configured).\nINFO: Found 1 target...\nTarget //tensorflow/lite/tools:list_flex_ops_no_kernel_main up-to-date:\n  bazel-bin/tensorflow/lite/tools/list_flex_ops_no_kernel_main\nINFO: Elapsed time: 25.854s, Critical Path: 22.96s\nINFO: 243 processes: 110 internal, 133 local.\nINFO: Build completed successfully, 243 total actions\n\n```\n\n**`tensorflow-lite.aar` was indeed successfully built below I've unzipped content :**\n\n```\ngaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22:~/tensorflow-2.19.0$ unzip -l ~/tensorflow-2.19.0/bazel-bin/tmp/tensorflow-lite.aar\nArchive:  /home/gaikwadrahul/tensorflow-2.19.0/bazel-bin/tmp/tensorflow-lite.aar\n  Length      Date    Time    Name\n---------  ---------- -----   ----\n      279  2010-01-01 00:00   AndroidManifest.xml\n    45613  2010-01-01 00:00   classes.jar\n        0  2010-01-01 00:00   res/\n        0  2010-01-01 00:00   R.txt\n  1863648  2025-05-23 11:58   jni/arm64-v8a/libtensorflowlite_jni.so\n  1114172  2025-05-23 11:58   jni/armeabi-v7a/libtensorflowlite_jni.so\n    13575  2025-05-23 11:58   LICENSE\n---------                     -------\n  3037287                     7 files\ngaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22:~/tensorflow-2.19.0$ \n```\nI don't have access to your actual `walletBillModel.tflite` model file so I have created empty `walletBillModel.tflite` file and tried, It seems like things are working as expected if possible please help me with your original model file `walletBillModel.tflite` to test from my end or give it try once from your end.\n\nThank you for your cooperation and patience."
      },
      {
        "user": "EDFBoy",
        "body": "Hi @gaikwadrahul8 ,my Issue still exists， The attachment is my test model file, you can verify it again,thinks。\n\nand I want to build to generate \"tensorflow-lite.aar\" and 'tensorflow-lite-select-tf-ops.aar' file，because my model uses the LTSM layer，that is my train model code:\n`model = tf.keras.Sequential([\n            tf.keras.layers.Embedding(input_dim=maxInputDim, \n                                      output_dim=64, \n                                      mask_zero=True),\n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(len(label_to_index), activation='softmax')\n        ])`\n\n[testModel.tflite.zip](https://github.com/user-attachments/files/20454007/testModel.tflite.zip)"
      }
    ]
  },
  {
    "issue_number": 94116,
    "title": "Segmentation Fault in `tf.raw_ops.QuantizeAndDequantizeV3`",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-24T16:14:09Z",
    "updated_at": "2025-05-27T06:25:38Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250516 (git: v1.12.1-126285-g20148f52365)\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux (CPU execution)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.3\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.QuantizeAndDequantizeV3` crashes with a segmentation fault due to a tensor shape validation failure. The crash occurs in tensor_shape.cc with a bounds checking assertion failure when processing negative axis values. The code may lack proper validation of axis parameters before processing.\n\nColab reproduction link: https://colab.research.google.com/drive/19QlV0TIQqkN_S66VATzOX_-vkTyg4I9K?usp=sharing\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\n# Create input tensor with extreme values\ninput_tensor = tf.constant([\n    [[4.3136053036349129e-244, 1.2524328545813582e-21, 7.71590127777328e+26, 1.0, 2.0, 3.0],\n     [1e-100, 1e+100, -1e+50, 4.0, 5.0, 6.0]]\n], dtype=tf.float64)  # Shape [1, 2, 6]\n\n# Extreme range values\ninput_min = tf.constant(-5.4785109376353583e-282, dtype=tf.float64)\ninput_max = tf.constant(1.4455588399771524e+73, dtype=tf.float64)\nnum_bits = tf.constant(2, dtype=tf.int32)\n\n# This crashes with segfault\nresult = tf.raw_ops.QuantizeAndDequantizeV3(\n    input=input_tensor,\n    input_min=input_min,\n    input_max=input_max,\n    num_bits=num_bits,\n    signed_input=False,\n    range_given=True,\n    narrow_range=True,\n    axis=-2  # Negative axis triggers the crash\n)\n```\n\n### Relevant log output\n\n```shell\n2025-05-24 16:12:02.945026: F tensorflow/core/framework/tensor_shape.cc:358] Check failed: d >= 0 (0 vs. -2)\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [Gist](https://colab.sandbox.google.com/gist/Venkat6871/da23c5e5b857861cc5b8f0b73779c912/94116_tf_2-19-0-nightly-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 94100,
    "title": "Critical Bug: Quantization Fails on Custom LSTM Models in Model Optimization Library",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-24T10:23:28Z",
    "updated_at": "2025-05-26T19:01:41Z",
    "labels": [
      "type:bug"
    ],
    "body": "## Bug Description\nQuantization is failing when applied to custom LSTM models using the TensorFlow model optimization library.\n\n## Environment\n- TensorFlow version: [Please specify your TF version]\n- Model Optimization Library version: [Please specify your version]\n- Operating System: [Please specify your OS]\n- Python version: [Please specify your Python version]\n\n## Steps to Reproduce\n1. Define a custom LSTM model with the following architecture:\n   ```python\n   def create_custom_lstm_model(input_shape, units=64):\n       inputs = tf.keras.Input(shape=input_shape)\n       x = tf.keras.layers.LSTM(units, return_sequences=True)(inputs)\n       x = tf.keras.layers.LSTM(units)(x)\n       outputs = tf.keras.layers.Dense(1)(x)\n       return tf.keras.Model(inputs=inputs, outputs=outputs)\n   ```\n\n2. Train the model on sample data:\n   ```python\n   model = create_custom_lstm_model((sequence_length, features))\n   model.compile(optimizer='adam', loss='mse')\n   model.fit(x_train, y_train, epochs=5)\n   ```\n\n3. Attempt to quantize the model using the TF Model Optimization toolkit:\n   ```python\n   import tensorflow_model_optimization as tfmot\n   \n   quantize_model = tfmot.quantization.keras.quantize_model\n   \n   q_aware_model = quantize_model(model)\n   q_aware_model.compile(optimizer='adam', loss='mse')\n   q_aware_model.fit(x_train, y_train, epochs=3)  # Fine-tune with quantization awareness\n   \n   converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n   converter.optimizations = [tf.lite.Optimize.DEFAULT]\n   quantized_tflite_model = converter.convert()\n   ```\n\n## Expected Behavior\nThe quantization process should complete successfully, generating a quantized TFLite model with reduced size that maintains reasonable accuracy.\n\n## Actual Behavior\nThe quantization process fails with the following error:\n[Please provide the exact error message you're encountering]\n\n## Additional Information\n- The same quantization process works correctly on simpler models without LSTM layers\n- The issue appears to be specific to custom LSTM implementations rather than the built-in LSTM layers\n- The error occurs consistently across multiple attempts and model configurations\n\n## Potential Impact\nThis bug blocks the deployment of optimized LSTM models in resource-constrained environments, which is a critical use case for the model optimization library.\n\nPlease provide any additional information that might help reproduce and diagnose this issue.",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Please stop spamming."
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94100\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94100\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 94101,
    "title": "Critical Bug: Quantization Fails on Custom LSTM Models in Model Optimization Library",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-24T10:26:00Z",
    "updated_at": "2025-05-26T19:01:33Z",
    "labels": [
      "type:bug"
    ],
    "body": "## Bug Description\nQuantization is failing when applied to custom LSTM models using the TensorFlow model optimization library.\n\n## Environment\n- TensorFlow version: [Please specify your TF version]\n- Model Optimization Library version: [Please specify your version]\n- Operating System: [Please specify your OS]\n- Python version: [Please specify your Python version]\n\n## Steps to Reproduce\n1. Define a custom LSTM model with the following architecture:\n   ```python\n   def create_custom_lstm_model(input_shape, units=64):\n       inputs = tf.keras.Input(shape=input_shape)\n       x = tf.keras.layers.LSTM(units, return_sequences=True)(inputs)\n       x = tf.keras.layers.LSTM(units)(x)\n       outputs = tf.keras.layers.Dense(1)(x)\n       return tf.keras.Model(inputs=inputs, outputs=outputs)\n   ```\n\n2. Train the model on sample data:\n   ```python\n   model = create_custom_lstm_model((sequence_length, features))\n   model.compile(optimizer='adam', loss='mse')\n   model.fit(x_train, y_train, epochs=5)\n   ```\n\n3. Attempt to quantize the model using the TF Model Optimization toolkit:\n   ```python\n   import tensorflow_model_optimization as tfmot\n   \n   quantize_model = tfmot.quantization.keras.quantize_model\n   \n   q_aware_model = quantize_model(model)\n   q_aware_model.compile(optimizer='adam', loss='mse')\n   q_aware_model.fit(x_train, y_train, epochs=3)  # Fine-tune with quantization awareness\n   \n   converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n   converter.optimizations = [tf.lite.Optimize.DEFAULT]\n   quantized_tflite_model = converter.convert()\n   ```\n\n## Expected Behavior\nThe quantization process should complete successfully, generating a quantized TFLite model with reduced size that maintains reasonable accuracy.\n\n## Actual Behavior\nThe quantization process fails with the following error:\n[Please provide the exact error message you're encountering]\n\n## Additional Information\n- The same quantization process works correctly on simpler models without LSTM layers\n- The issue appears to be specific to custom LSTM implementations rather than the built-in LSTM layers\n- The error occurs consistently across multiple attempts and model configurations\n\n## Potential Impact\nThis bug blocks the deployment of optimized LSTM models in resource-constrained environments, which is a critical use case for the model optimization library.",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Please stop spamming."
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94101\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94101\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 94045,
    "title": "`tf.raw_ops.QuantizedConv2D` crashes with assertion failure instead of proper error handling",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-23T20:09:10Z",
    "updated_at": "2025-05-26T18:58:32Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux (Google Colab)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nOnly CPU\n\n### Current behavior?\n\nWhen calling `tf.raw_ops.QuantizedConv2D` with incompatible tensor dimensions, TensorFlow crashes with a fatal assertion failure instead of raising a proper Python exception. The process terminates with a core dump, making it impossible to handle the error gracefully in user code.\n\n## Reproducible Example\n\nGoogle Colab link: https://colab.research.google.com/drive/19Qf7Wy-vlkPJuGSA_Z-w86zB8GC5fX_I?usp=sharing\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\n# Create input tensor with shape [10,10,2,1] \ninput_data = np.random.randint(0, 255, size=(10, 10, 2, 1), dtype=np.uint8)\ninput_tensor = tf.constant(input_data, dtype=tf.quint8)\n\n# Create filter tensor with shape [3,3,1,3]\nfilter_data = np.random.randint(0, 255, size=(3, 3, 1, 3), dtype=np.uint8)\nfilter_tensor = tf.constant(filter_data, dtype=tf.quint8)\n\n# Min/Max tensors for quantization\nmin_input = tf.constant(0.0, dtype=tf.float32)\nmax_input = tf.constant(1.0, dtype=tf.float32)\nmin_filter = tf.constant(0.0, dtype=tf.float32)\nmax_filter = tf.constant(1.0, dtype=tf.float32)\n\n# This crashes with assertion failure instead of raising proper exception\nresult = tf.raw_ops.QuantizedConv2D(\n    input=input_tensor,\n    filter=filter_tensor,\n    min_input=min_input,\n    max_input=max_input,\n    min_filter=min_filter,\n    max_filter=max_filter,\n    strides=[1, 1, 1, 1],\n    padding=\"VALID\",\n    dilations=[1, 1, 1, 1]\n)\n```\n\n### Relevant log output\n\n```shell\nError Message:\n\n\nF0000 00:00:1748030662.839173 1610502 quantized_conv_ops.cc:581] Check failed: out_cols > 0 (0 vs. 0) \n*** Check failure stack trace: ***\n    @     0x7f02274cc3f4  absl::lts_20230802::log_internal::LogMessage::SendToLog()\n    @     0x7f02274cc264  absl::lts_20230802::log_internal::LogMessage::Flush()\n    @     0x7f02274cc819  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()\n    @     0x7f021a8b15d4  tensorflow::QuantizedConv2DOp<>::Compute()\n    @     0x7f02300f4320  tensorflow::ThreadPoolDevice::Compute()\n    [... full stack trace ...]\nAborted (core dumped)\n```",
    "comments": []
  },
  {
    "issue_number": 93923,
    "title": "`tf.linalg.svd` crashes on CUDA raising SIGABRT after facing OOM",
    "author": "jiren-the-gray",
    "state": "open",
    "created_at": "2025-05-22T09:30:16Z",
    "updated_at": "2025-05-26T18:49:59Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 24.04.2 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n12.6\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen running a 6D float32 tensor on `tf.linalg.svd` on CUDA crashes with `Aborted (core dumped)` after facing OOM. I can calculate the svd with `Numpy` without facing any memory issues/signal raised. I also tried to reproduce with [colab](https://colab.research.google.com/drive/1seHhYGTrXwJa5-oPKayx4-nSHHYnrT3a?usp=sharing) and the session did in fact crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\nimport tensorflow as tf\nrng = np.random.default_rng(225)\nnp_input = rng.uniform(-36.999996, -32.00001, (21, 21, 12, 11, 5, 3)).astype(np.float32)\n\n# try numpy\nsvd_np = np.linalg.svd(np_input, compute_uv=False)\nprint(\"Numpy Successful\\n\\n\")\n\nwith tf.device(\"/GPU:0\"):\n    input_tensor = tf.constant(np_input)\n    svd_tf = tf.linalg.svd(input_tensor, compute_uv=False)\n```\n\n### Relevant log output\n\n```shell\n...\n2025-05-22 05:27:02.208208: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114]      Summary of in-use Chunks by size: \n2025-05-22 05:27:02.208214: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 1280 totalling 1.2KiB\n2025-05-22 05:27:02.208217: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 232960 totalling 227.5KiB\n2025-05-22 05:27:02.208220: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 698624 totalling 682.2KiB\n2025-05-22 05:27:02.208223: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 11550 Chunks of size 919296 totalling 9.89GiB\n2025-05-22 05:27:02.208225: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 1558272 totalling 1.49MiB\n2025-05-22 05:27:02.208227: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 2095872 totalling 2.00MiB\n2025-05-22 05:27:02.208229: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 4 Chunks of size 3492864 totalling 13.32MiB\n2025-05-22 05:27:02.208232: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1121] Sum Total of in-use chunks: 9.91GiB\n2025-05-22 05:27:02.208235: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1123] Total bytes in pool: 10636427264 memory_limit_: 10636427264 available bytes: 0 curr_region_allocation_bytes_: 21272854528\n2025-05-22 05:27:02.208239: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1128] Stats: \nLimit:                     10636427264\nInUse:                     10636427264\nMaxInUse:                  10636427264\nNumAllocs:                       11559\nMaxAllocSize:                  3492864\nReserved:                            0\nPeakReserved:                        0\nLargestFreeBlock:                    0\n\n2025-05-22 05:27:02.208326: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:508] ****************************************************************************************************\nF0000 00:00:1747906022.208340 1974098 gpu_solvers.h:618] Non-OK-status: context->allocate_temp(DataTypeToEnum<Scalar>::value, shape, &scratch_tensor_, alloc_attr)\nStatus: RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[229824] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n*** Check failure stack trace: ***\n    @     0x7ce77da67bc4  absl::lts_20230802::log_internal::LogMessage::SendToLog()\n    @     0x7ce77da679c4  absl::lts_20230802::log_internal::LogMessage::Flush()\n    @     0x7ce77da67fe9  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()\n    @     0x7ce7741bbe10  tensorflow::ScratchSpace<>::ScratchSpace()\n    @     0x7ce7741bbb54  tensorflow::GpuSolver::GetScratchSpace<>()\n    @     0x7ce7741b4b66  tensorflow::GpuSolver::Gesvd<>()\n    @     0x7ce771421389  tensorflow::SvdOpGpu<>::RunSVD()\n    @     0x7ce7714204fc  tensorflow::SvdOpGpu<>::PerformSVD_MgeqN()\n    @     0x7ce77141ffe7  tensorflow::SvdOpGpu<>::ComputeAsync()\n    @     0x7ce78657e96d  tensorflow::AsyncOpKernel::Compute()\n    @     0x7ce7863474d5  tensorflow::BaseGPUDevice::Compute()\n    @     0x7ce7863f8f48  tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run()\n    @     0x7ce7863b8724  tensorflow::FunctionLibraryRuntimeImpl::RunSync()\n    @     0x7ce7863c5730  tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync()\n    @     0x7ce7863cbb7d  tensorflow::ProcessFunctionLibraryRuntime::RunSync()\n    @     0x7ce76df3ddf0  tensorflow::KernelAndDeviceFunc::Run()\n    @     0x7ce76dee98d6  tensorflow::EagerKernelExecute()\n    @     0x7ce76def33b0  tensorflow::ExecuteNode::Run()\n    @     0x7ce76df39244  tensorflow::EagerExecutor::SyncExecute()\n    @     0x7ce76dee925b  tensorflow::(anonymous namespace)::EagerLocalExecute()\n    @     0x7ce76dee6929  tensorflow::DoEagerExecute()\n    @     0x7ce76deea660  tensorflow::EagerExecute()\n    @     0x7ce76d5209f7  tensorflow::EagerOperation::Execute()\n    @     0x7ce76df37943  tensorflow::CustomDeviceOpHandler::Execute()\n    @     0x7ce76ad38cf5  TFE_Execute\n    @     0x7ce77f431efa  TFE_Py_FastPathExecute_C()\n    @     0x7ce7992d5893  pybind11::detail::argument_loader<>::call<>()\n    @     0x7ce7992d57cf  pybind11::cpp_function::initialize<>()::{lambda()#1}::__invoke()\n    @     0x7ce7992af8df  pybind11::cpp_function::dispatcher()\n    @           0x58208f  (unknown)\n    @           0x549185  _PyObject_MakeTpCall\n    @           0x5d73c9  _PyEval_EvalFrameDefault\n    @           0x5d58eb  PyEval_EvalCode\n    @           0x608b42  (unknown)\n    @           0x6b4e93  (unknown)\n    @           0x6b4bfa  _PyRun_SimpleFileObject\n    @           0x6b4a2f  _PyRun_AnyFileObject\n    @           0x6bca95  Py_RunMain\n    @           0x6bc57d  Py_BytesMain\n    @     0x7ce7c6a2a1ca  (unknown)\n    @     0x7ce7c6a2a28b  __libc_start_main\n    @           0x657ce5  _start\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/0644c48df20e43df36631e09a9c95dea/93923_tf_2-19-0-nightly-gpu-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 93903,
    "title": "`tf.conv2d_backprop_filter_v2` Fails with \"no registered kernels\" Error",
    "author": "SilentTester73",
    "state": "open",
    "created_at": "2025-05-21T23:49:35Z",
    "updated_at": "2025-05-26T18:35:58Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe `tf.conv2d_backprop_filter_v2` operation fails with a `NotFoundError` (or similar) indicating \"Could not find device for node\" and \"no registered kernels\" when attempting to execute a simple convolution backward pass for filter gradients. This occurs even when the input tensors and parameters correctly defined. \n\nColab link: https://colab.research.google.com/drive/1AJj0v0bgt1pATdJwH7xEku1N6gL4YB1j?usp=sharing\n\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\nprint(tf.__version__)\n\ndef run_conv2d_backprop_filter_v2_bug():\n    \"\"\"\n    Attempts to demonstrate tf.conv2d_backprop_filter_v2 and triggers the error.\n    \"\"\"\n\n    # --- Define the input tensors based on provided information ---\n    # Input Tensor: Tensor<type: float shape: [1,3,28,3]>\n    input_values = np.zeros((1, 3, 28, 3), dtype=np.float32)\n    input_values[0, 0, 0, 0] = -2.16744329e+12\n    input_values[0, 0, 0, 1] = 1.23339599e+30\n    input_values[0, 0, 0, 2] = 8.00276139e+12\n    input_tensor = tf.constant(input_values, dtype=tf.float32)\n\n    # Filter shape: Tensor<type: int shape: [4]> representing [height, width, in_channels, out_channels]\n    filter_shape = tf.constant([3, 1, 3, 1], dtype=tf.int32)\n\n    # Out Backprop Tensor: Tensor<type: float shape: [1,2,10,1]>\n    out_backprop_values = np.zeros((1, 2, 10, 1), dtype=np.float32)\n    out_backprop_values[0, 0, 0, 0] = 2.54932688e-24\n    out_backprop_values[0, 0, 1, 0] = 3.49802447e+36\n    out_backprop_values[0, 0, 2, 0] = 7.77881911e+12\n    out_backprop = tf.constant(out_backprop_values, dtype=tf.float32)\n\n    # --- Define the convolution parameters ---\n    data_format = \"NCHW\"\n    strides = [1, 1, 2, 3] # [N, C, H, W]\n    padding = \"SAME\"\n    dilations = [1, 1, 2, 3] # [N, C, H, W]\n\n    print(f\"Input Tensor Shape: {input_tensor.shape}\")\n    print(f\"Filter Shape: {filter_shape.numpy()}\")\n    print(f\"Out Backprop Tensor Shape: {out_backprop.shape}\")\n    print(f\"Strides: {strides}\")\n    print(f\"Padding: {padding}\")\n    print(f\"Data Format: {data_format}\")\n    print(f\"Dilations: {dilations}\")\n    print(\"-\" * 30)\n\n    try:\n        grad_filter = tf.conv2d_backprop_filter_v2(\n            input=input_tensor,\n            filter=filter_shape,\n            out_backprop=out_backprop,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            dilations=dilations\n        )\n        print(\"tf.conv2d_backprop_filter_v2 operation completed successfully!\")\n        print(f\"Shape of the gradient of the filter: {grad_filter.shape}\")\n    except Exception as e:\n        print(f\"An error occurred during tf.conv2d_backprop_filter_v2: {e}\")\n\nif __name__ == \"__main__\":\n    run_conv2d_backprop_filter_v2_bug()\n```\n\n### Relevant log output\n\n```shell\nOutput:\n\n2.18.0\nInput Tensor Shape: (1, 3, 28, 3)\nFilter Shape: [3 1 3 1]\nOut Backprop Tensor Shape: (1, 2, 10, 1)\nStrides: [1, 1, 2, 3]\nPadding: SAME\nData Format: NCHW\nDilations: [1, 1, 2, 3]\n------------------------------\nAn error occurred during tf.conv2d_backprop_filter_v2: Could not find device for node: {{node Conv2DBackpropFilterV2}} = Conv2DBackpropFilterV2[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 2, 3], explicit_paddings=[], padding=\"SAME\", strides=[1, 1, 2, 3], use_cudnn_on_gpu=true]\nAll kernels registered for op Conv2DBackpropFilterV2:\n  <no registered kernels>\n [Op:Conv2DBackpropFilterV2] name:\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue using TensorFlow 2.19.0 as well as the nightly version. Please find the [Gist](https://colab.sandbox.google.com/gist/Venkat6871/c2c464088771400d117bf47503a32a9c/93903_tf_2-19-0-nightly-gpu-v.ipynb) for your reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 93826,
    "title": "Broken Link: Microsoft Visual C++ Redistributable (pip install)",
    "author": "kannanwisen",
    "state": "open",
    "created_at": "2025-05-21T12:03:48Z",
    "updated_at": "2025-05-26T16:51:45Z",
    "labels": [
      "type:docs-bug",
      "type:bug",
      "TF 2.10"
    ],
    "body": "### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.10\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe link for [Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads) on https://www.tensorflow.org/install/pip is broken.\n\n\n\n### Standalone code to reproduce the issue\n\n```shell\nWhen navigating to the TensorFlow pip installation guide at https://www.tensorflow.org/install/pip, I observed a broken link for the \"Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019\".\n\nSteps to reproduce:\n\nGo to the TensorFlow installation guide: https://www.tensorflow.org/install/pip\nScroll down to the \"Windows install\" section (or the section mentioning Visual C++ Redistributable).\nClick on the link titled: Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019\nThe link points to: https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\nExpected result: The link should navigate to a valid Microsoft support page where users can download the Visual C++ Redistributable.\nActual result: The link leads to a \"Page not found\" or \"404 Error\" page on the Microsoft support website, indicating the resource is no longer available at that URL.\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": []
  },
  {
    "issue_number": 93828,
    "title": "Bazel build failure : Flex delegate",
    "author": "harshithn31",
    "state": "open",
    "created_at": "2025-05-21T12:17:43Z",
    "updated_at": "2025-05-26T12:59:13Z",
    "labels": [
      "stat:awaiting tensorflower",
      "comp:lite",
      "TFLiteConverter",
      "TF 2.15"
    ],
    "body": "### 1. System information\n\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.5 LTS\n- TensorFlow installation (pip package or built from source): Source\n- TensorFlow library (version, if pip package or github SHA, if built from source): v2.15.0\n\n### 2. Code\n\nProvide code to help us reproduce your issues using one of the following options:\n\n\n#### Option B: Paste your code here or provide a link to a custom end-to-end colab\n\nMy Model\n```\n def magnitudeLSTMDropout_phaseLSTM_parallel_test_2(self):\n        \n        input_layer = keras.layers.Input(shape=(410, 1))\n        \n        magnitude_model = keras.models.Sequential([\n                                                    # Convolutional block to extract local features\n                                                    keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu',  padding='same'),\n                                                    keras.layers.MaxPooling1D(pool_size=2),\n                                                    keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n                                                    keras.layers.MaxPooling1D(pool_size=2),\n                                                    keras.layers.TimeDistributed(keras.layers.Flatten()),\n                                                    \n                                                    # # LSTM layers for sequential processing\n                                                    keras.layers.LSTM(16, return_sequences=True, \n                                                                      dropout= 0.2, recurrent_dropout=0.2),  # Keeps sequence for the next LSTM layer\n                                                    keras.layers.LSTM(16, return_sequences=True),\n                                                    keras.layers.LSTM(16, return_sequences=True, \n                                                                      dropout= 0.2, recurrent_dropout=0.2),\n                                                    keras.layers.LSTM(16, return_sequences=True),\n                                                    keras.layers.LSTM(16, return_sequences=True, \n                                                                      dropout= 0.2, recurrent_dropout=0.2),\n                                                    keras.layers.LSTM(16, return_sequences=True),\n                                                    keras.layers.LSTM(8, return_sequences=False, \n                                                                      dropout= 0.2, recurrent_dropout=0.2),# Last LSTM layer with return_sequences=False\n                                                    \n                                                    # Fully connected layers\n                                                    \n                                                    keras.layers.BatchNormalization(),  # Helps stabilize training\n                                                    keras.layers.Dropout(0.2),\n                                                    keras.layers.Dense(51,activation=\"sigmoid\", name=\"magnitude\")  # Final layer to output 102 FFT values\n                                                ])\n        \n        phase_model = keras.models.Sequential([\n                                                    # Convolutional block to extract local features\n                                                    keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu',  padding='same'),\n                                                    keras.layers.MaxPooling1D(pool_size=2),\n                                                    keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n                                                    keras.layers.MaxPooling1D(pool_size=2),\n                                                    keras.layers.TimeDistributed(keras.layers.Flatten()),\n                                                    \n                                                    # LSTM layers for sequential processing\n                                                    keras.layers.LSTM(64, return_sequences=True),  # Keeps sequence for the next LSTM layer\n                                                    keras.layers.LSTM(64, return_sequences=True),\n                                                    keras.layers.LSTM(32, return_sequences=True),\n                                                    keras.layers.LSTM(32, return_sequences=True),\n                                                    keras.layers.LSTM(16, return_sequences=True),\n                                                    keras.layers.LSTM(16, return_sequences=True),\n                                                    keras.layers.LSTM(8, return_sequences=False),  # Last LSTM layer with return_sequences=False\n                                                    \n                                                    # Fully connected layers\n                                                    keras.layers.Dense(256, activation=\"relu\"),\n                                                    keras.layers.BatchNormalization(),  # Helps stabilize training\n                                                    keras.layers.Dense(128, activation='relu'),\n                                                    keras.layers.Dropout(0.2),\n                                                    keras.layers.BatchNormalization(),  # Helps stabilize training\n                                                    keras.layers.Dense(256, activation=\"relu\"),\n                                                    keras.layers.BatchNormalization(),  # Helps stabilize training\n                                                    keras.layers.Dense(128, activation='relu'),\n                                                    keras.layers.Dense(51,activation=\"sigmoid\", name=\"phase\")  # Final layer to output 102 FFT values\n                                                ])\n        \n        magnitude_output = magnitude_model(input_layer)\n        magnitude_output = keras.layers.Lambda(lambda x: x, name=\"magnitude\")(magnitude_output)  # Explicitly name it\n\n        phase_output = phase_model(input_layer)\n        phase_output = keras.layers.Lambda(lambda x: x, name=\"phase\")(phase_output)  # Explicitly name it\n\n        \n        self.model = keras.models.Model(inputs=input_layer, \n                                        outputs={\"magnitude\": magnitude_output, \"phase\": phase_output})\n             \n                                           \n        self.model.compile(optimizer='adam', \n                           loss={\"magnitude\": \"mse\", \"phase\": \"mse\"}, \n                           metrics={\"magnitude\":['mae', 'mse'], \"phase\":['mae', 'mse']})\n        self.model.summary()\n\n        return self.model\n```\n\nModel conversion to tflite\n\n```\nimport sys\nimport os\nimport tensorflow as tf\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\nfrom Inference.inference import Inference\n\n\ninference_module = Inference()\nmodel,_=inference_module.load_model()\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # Default TFLite ops\n    tf.lite.OpsSet.SELECT_TF_OPS     # Enable full TF ops when needed\n]\nconverter._experimental_lower_tensor_list_ops = False \ntf_lite_model = converter.convert()\n\nscript_dir = os.path.dirname(__file__)\nlite_dir = os.path.join(script_dir, 'LITE_SAVED_MODELS')\nif os.path.exists(lite_dir) == False:\n    os.makedirs(lite_dir)\n    \nlite_model_path = os.path.join(lite_dir, 'lite_model.tflite')\nwith open(lite_model_path, 'wb') as f:\n    f.write(tf_lite_model)\n\n```\n\nThe TensorFlow model gets converted into TFLite successfully, with the use of tf.lite.OpsSet.SELECT_TF_OPS\n\n### 3. Issue Definition\nThe issue I'm facing is with the inferencing. at the time of inferencing on the edge device using tflite im getting runtime error.\n```\nRuntimeError: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding \"org.tensorflow:tensorflow-lite-select-tf-ops\" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 1 (FlexTensorListReserve) failed to prepare.Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding \"org.tensorflow:tensorflow-lite-select-tf-ops\" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 1 (FlexTensorListReserve) failed to prepare.\n```\n### 4. Steps taken to Resolve the Issue\nTried to build flex delegate shared library for the tensorflow [guide](https://ai.google.dev/edge/litert/models/ops_select#cc).\nIm running the build in TensorFlow devel docker image [tensorflow/tensorflow:devel](https://hub.docker.com/r/tensorflow/tensorflow/tags/).\n\nCommand used to build:\n```\nbazel build -c opt --config=monolithic --config=elinux_armhf tensorflow/lite/delegates/flex:tensorflowlite_flex\n\n```\nError message while building\n\n```\nERROR: /home/harshith/Documents/docker_method/tensorflow_src/bazel-ci_build-cache/.cache/bazel/_bazel_harshith/68a62076e91007a7908bc42a32e4cff9/external/com_github_grpc_grpc/BUILD:1853:16: Compiling src/core/ext/transport/chttp2/client/secure/secure_channel_create.cc failed: (Exit 1): arm-none-linux-gnueabihf-gcc failed: error executing command (from target @com_github_grpc_grpc//:grpc_transport_chttp2_client_secure) /home/harshith/Documents/docker_method/tensorflow_src/bazel-ci_build-cache/.cache/bazel/_bazel_harshith/68a62076e91007a7908bc42a32e4cff9/external/armhf_linux_toolchain/bin/arm-none-linux-gnueabihf-gcc ... (remaining 85 arguments skipped)\nIn file included from external/com_github_grpc_grpc/src/core/lib/gprpp/memory.h:27,\n                 from external/com_github_grpc_grpc/src/core/lib/gprpp/global_config_generic.h:24,\n                 from external/com_github_grpc_grpc/src/core/lib/gprpp/global_config_env.h:24,\n                 from external/com_github_grpc_grpc/src/core/lib/gprpp/global_config.h:92,\n                 from external/com_github_grpc_grpc/src/core/lib/debug/trace.h:27,\n                 from external/com_github_grpc_grpc/src/core/lib/channel/channel_stack.h:44,\n                 from external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel_channelz.h:27,\n                 from external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.h:24,\n                 from external/com_github_grpc_grpc/src/core/ext/transport/chttp2/client/secure/secure_channel_create.cc:28:\n/home/harshith/Documents/docker_method/tensorflow_src/bazel-ci_build-cache/.cache/bazel/_bazel_harshith/68a62076e91007a7908bc42a32e4cff9/external/armhf_linux_toolchain/arm-none-linux-gnueabihf/include/c++/11.3.1/limits:1673:7: internal compiler error: Illegal instruction\n 1673 |       min() _GLIBCXX_USE_NOEXCEPT { return __FLT_MIN__; }\n      |       ^~~\n0x169d200 diagnostic_impl(rich_location*, diagnostic_metadata const*, int, char const*, __va_list_tag (*) [1], diagnostic_t)\n\t???:0\n0x169de86 internal_error(char const*, ...)\n\t???:0\n0xcefe7f crash_signal(int)\n\t???:0\n0x18a6649 __gmpn_mul_basecase\n\t???:0\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nPlease include the complete backtrace with any bug report.\nSee <https://bugs.linaro.org/> for instructions.\nTarget //tensorflow/lite/delegates/flex:tensorflowlite_flex failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 360.326s, Critical Path: 19.27s\nINFO: 849 processes: 309 internal, 540 local.\nFAILED: Build did NOT complete successfully\n\n```\n\n\n### 3. Failure after conversion\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\n\n- Model produces wrong results and/or has lesser accuracy.\n- Model produces correct results, but it is slower than expected.\n\n### 4. (optional) RNN conversion support\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\n\n### 5. (optional) Any other info / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\n```\nI am building flex delegate for armv7 architecture (ARM cortex a9)\n```\n",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @harshithn31 \nI apologize for the delay in my response, I was trying to build with same command `bazel build -c opt --config=monolithic --config=elinux_armhf tensorflow/lite/delegates/flex:tensorflowlite_flex` but unfortunately Flex delegate does not build successfully so we will have to investigate this issue further from our end and will update you for investigation purpose I have added complete error log for reference.\n\nI am also able to build .tflite model file with your provided code snippet :\n```\n(venv_tf) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22:~/tf_issue_repro/model_code/LITE_SAVED_MODELS$ ls\nlite_model.tflite\n(venv_tf) gaikwadrahul@gaikwadrahul-n1-standard-1-gpu-t4x1-tflite-ubuntu-22:~/tf_issue_repro/model_code/LITE_SAVED_MODELS$\n```\n\n**Here is output log for reference :**\n```\nroot@e68c5b0f4666:/tensorflow_src# export HERMETIC_PYTHON_VERSION=3.9\nroot@e68c5b0f4666:/tensorflow_src# bazel build -c opt --config=monolithic --config=elinux_armhf tensorflow/lite/delegates/flex:tensorflowlite_flex --verbose_failures\nINFO: Reading 'startup' options from /tensorflow_src/.bazelrc: --windows_enable_symlinks\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=1 --terminal_columns=190\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\n  Inherited 'common' options: --experimental_repo_remote_exec\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\nINFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3\nINFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\nINFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\nINFO: Found applicable config definition build:monolithic in file /tensorflow_src/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false\nINFO: Found applicable config definition build:elinux_armhf in file /tensorflow_src/.bazelrc: --config=elinux --cpu=armhf --copt -mfp16-format=ieee\nINFO: Found applicable config definition build:elinux in file /tensorflow_src/.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\nINFO: Found applicable config definition build:linux in file /tensorflow_src/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\nINFO: Found applicable config definition build:dynamic_kernels in file /tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\nDEBUG: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/local_tsl/third_party/py/python_repo.bzl:87:10: \n=============================\nHermetic Python configuration:\nVersion: \"3.9\"\nKind: \"\"\nInterpreter: \"default\" (provided by rules_python)\nRequirements_lock label: \"@python_version_repo//:requirements_lock_3_9.txt\"\n=====================================\nDEBUG: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/local_tsl/third_party/gpus/cuda/hermetic/cuda_configure.bzl:48:14: Cannot find clang, either correct your path, or set the CLANG_CUDA_COMPILER_PATH or CC environment variables\nINFO: Analyzed target //tensorflow/lite/delegates/flex:tensorflowlite_flex (364 packages loaded, 26887 targets configured).\nINFO: Found 1 target...\nERROR: /tensorflow_src/tensorflow/core/kernels/BUILD:6314:11: Compiling tensorflow/core/kernels/meta_support.cc failed: (Exit 1): arm-none-linux-gnueabihf-gcc failed: error executing command (from target //tensorflow/core/kernels:meta_support) \n  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \\\n  exec env - \\\n    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-6.5.0-linux-x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n    PWD=/proc/self/cwd \\\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\n    TF2_BEHAVIOR=1 \\\n  /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/armhf_linux_toolchain/bin/arm-none-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/armhf_linux_toolchain/lib/gcc/arm-none-linux-gnueabihf/11.3.1/include -isystem /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/armhf_linux_toolchain/lib/gcc/arm-none-linux-gnueabihf/11.3.1/include-fixed -isystem /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/armhf_linux_toolchain/arm-none-linux-gnueabihf/include/c++/11.3.1/ -isystem /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/armhf_linux_toolchain/arm-none-linux-gnueabihf/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/armhf-opt/bin/tensorflow/core/kernels/_objs/meta_support/meta_support.pic.d '-frandom-seed=bazel-out/armhf-opt/bin/tensorflow/core/kernels/_objs/meta_support/meta_support.pic.o' -fPIC '-DEIGEN_MAX_ALIGN_BYTES=64' -DEIGEN_ALLOW_UNALIGNED_SCALARS '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DTF_ENABLE_ACTIVITY_WATCHER '-DBAZEL_CURRENT_REPOSITORY=\"\"' -iquote . -iquote bazel-out/armhf-opt/bin -iquote external/com_google_absl -iquote bazel-out/armhf-opt/bin/external/com_google_absl -iquote external/com_google_protobuf -iquote bazel-out/armhf-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/armhf-opt/bin/external/zlib -iquote external/local_xla -iquote bazel-out/armhf-opt/bin/external/local_xla -iquote external/local_tsl -iquote bazel-out/armhf-opt/bin/external/local_tsl -iquote external/com_googlesource_code_re2 -iquote bazel-out/armhf-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/armhf-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/armhf-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/armhf-opt/bin/external/highwayhash -iquote external/eigen_archive -iquote bazel-out/armhf-opt/bin/external/eigen_archive -iquote external/ml_dtypes_py -iquote bazel-out/armhf-opt/bin/external/ml_dtypes_py -iquote external/snappy -iquote bazel-out/armhf-opt/bin/external/snappy -iquote external/gemmlowp -iquote bazel-out/armhf-opt/bin/external/gemmlowp -isystem external/com_google_protobuf/src -isystem bazel-out/armhf-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/armhf-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/armhf-opt/bin/external/farmhash_archive/src -isystem external/eigen_archive -isystem bazel-out/armhf-opt/bin/external/eigen_archive -isystem external/eigen_archive/mkl_include -isystem bazel-out/armhf-opt/bin/external/eigen_archive/mkl_include -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' -DAUTOLOAD_DYNAMIC_KERNELS '-mfp16-format=ieee' '-std=c++17' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c tensorflow/core/kernels/meta_support.cc -o bazel-out/armhf-opt/bin/tensorflow/core/kernels/_objs/meta_support/meta_support.pic.o)\n# Configuration: 762918ec317fb98ff7e75acf2796e7be2cb6a7ac74eece8f56f3ffd7b45f4514\n# Execution platform: @local_execution_config_platform//:platform\nIn file included from external/gemmlowp/meta/streams.h:307,\n                 from external/gemmlowp/meta/quantized_mul_kernels.h:22,\n                 from ./tensorflow/core/kernels/meta_support.h:21,\n                 from tensorflow/core/kernels/meta_support.cc:18:\nexternal/gemmlowp/meta/streams_arm_32.h: In static member function 'static void gemmlowp::meta::GemmExecutorPackRHS::ExecuteDispatch3D(const P&) [with P = gemmlowp::meta::GemmParams<unsigned char, int, gemmlowp::meta::RowMajorWithSum, gemmlowp::meta::RowMajorWithSum, gemmlowp::meta::QuantizedStaticPreprocessedAsInt32, gemmlowp::meta::RowMajor>; int m = 2; int n = 4; int k = 8; int m_leftovers = 0; int n_leftovers = 0; int k_leftovers = 0]':\nexternal/gemmlowp/meta/streams_arm_32.h:1535:3: error: 'asm' operand has impossible constraints\n 1535 |   asm volatile(\n      |   ^~~\nTarget //tensorflow/lite/delegates/flex:tensorflowlite_flex failed to build\nINFO: Elapsed time: 674.257s, Critical Path: 128.04s\nINFO: 6483 processes: 336 internal, 6147 local.\nFAILED: Build did NOT complete successfully\nroot@e68c5b0f4666:/tensorflow_src# \n```\n\nThank you for your cooperation and patience."
      }
    ]
  },
  {
    "issue_number": 93733,
    "title": "`tf.nn.conv2d_transpose` crashes with \"Illegal instruction (core dumped)\"",
    "author": "jiren-the-gray",
    "state": "open",
    "created_at": "2025-05-20T10:30:59Z",
    "updated_at": "2025-05-26T12:44:12Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 24.04.2 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n12.6\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen I run the following code setting the device as CPU, it crashes with this error message: \"could not create a primitive descriptor for a convolution forward propagation primitive\", then it raises the signal \"Illegal instruction (core dumped)\". I tried it with two different machines, both gave me the same error.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\nrng = np.random.default_rng(40)\n\ntf.config.experimental.enable_op_determinism()\n\nwith tf.device(\"/CPU:0\"):\n    input_tensor = tf.constant(rng.uniform(-3.9808033, -3.0141976, (1, 4, 4, 5)), dtype=tf.float32)\n    weight_tensor = tf.constant(rng.uniform(2.0, 2.0, (4, 4, 4, 4)), dtype=tf.float32)\n    stride = 2\n    padding = 3\n    output_padding = 0\n    groups = 1\n    dilation = 1\n    \n    input_tensor = tf.transpose(input_tensor, [0, 2, 3, 1])\n    weight_tensor = tf.transpose(weight_tensor, [2, 3, 1, 0])\n\n    batch_size = tf.shape(input_tensor)[0]\n    input_height = input_tensor.shape[1]\n    input_width = input_tensor.shape[2]\n    filter_height = weight_tensor.shape[0]\n    filter_width = weight_tensor.shape[1]\n    \n    out_channels = weight_tensor.shape[3]\n    \n    h_out = (input_height - 1) * stride - 2 * padding + dilation * (filter_height - 1) + 1 + output_padding\n    w_out = (input_width - 1) * stride - 2 * padding + dilation * (filter_width - 1) + 1 + output_padding\n    output_shape = [batch_size, h_out, w_out, out_channels]\n\n    output = tf.nn.conv2d_transpose(\n        input_tensor, weight_tensor, output_shape=output_shape, strides=[1, stride, stride, 1],\n        padding='VALID' if padding == 0 else 'SAME', dilations=[1, dilation, dilation, 1]\n    )\n```\n\n### Relevant log output\n\n```shell\ncould not create a primitive descriptor for a convolution forward propagation primitiveIllegal instruction (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @jiren-the-gray ,\nApologies for the delay, and thank you for raising your concern here.\nI tried running your code on Colab using both TensorFlow 2.19.0 and the nightly versions. It is not crashing on my end and is throwing a proper error as expected. After making some modifications, the code worked fine for me. I am attaching a [gist](https://colab.sandbox.google.com/gist/Venkat6871/fe9ba722acdeea318f9d1180519d9d99/93733_tf_2-19-0-nightly-v.ipynb) for your reference. Hopefully, it will help resolve the issue on your end as well.\nPlease feel free to reach out if you need further assistance.\n\nThank you!"
      },
      {
        "user": "jiren-the-gray",
        "body": "Hi @Venkat6871 , thanks for the reply. I can see that the error is not reproducible on colab but I have tried running this on two different machines and it did crash on them, even after copying the code from your gist, it crashed. Here is the list of dependencies in the environment where I am running this code:\n```\nPackage                 Version\n----------------------- ---------\nabsl-py                 2.2.2\nastunparse              1.6.3\ncertifi                 2025.4.26\ncharset-normalizer      3.4.2\nflatbuffers             25.2.10\ngast                    0.6.0\ngoogle-pasta            0.2.0\ngrpcio                  1.71.0\nh5py                    3.13.0\nidna                    3.10\nkeras                   3.10.0\nlibclang                18.1.1\nMarkdown                3.8\nmarkdown-it-py          3.0.0\nMarkupSafe              3.0.2\nmdurl                   0.1.2\nml_dtypes               0.5.1\nnamex                   0.0.9\nnumpy                   2.1.3\nopt_einsum              3.4.0\noptree                  0.15.0\npackaging               25.0\npip                     25.1\nprotobuf                5.29.4\nPygments                2.19.1\nrequests                2.32.3\nrich                    14.0.0\nsetuptools              78.1.1\nsix                     1.17.0\ntensorboard             2.19.0\ntensorboard-data-server 0.7.2\ntensorflow              2.19.0\ntermcolor               3.1.0\ntyping_extensions       4.13.2\nurllib3                 2.4.0\nWerkzeug                3.1.3\nwheel                   0.45.1\nwrapt                   1.17.2\n\n```"
      }
    ]
  },
  {
    "issue_number": 93822,
    "title": "`tf.linalg.pinv` crashes with after raising SIGABRT with a 6D tensor due to OOM on CUDA",
    "author": "jiren-the-gray",
    "state": "open",
    "created_at": "2025-05-21T11:12:31Z",
    "updated_at": "2025-05-26T12:32:45Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 24.04.2 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n12.6\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nRunning `tf.linalg.pinv` with a 6D tensor is causing the code to crash with `Aborted (core dumped)` due to being out of memory, instead of closing gracefully on CUDA. It does not face any problems on CPU. I tried to run it on [colab](https://colab.research.google.com/drive/122og20vCZPZhYEkMwcJSJybUZHP1bSii?usp=sharing) as well, it crashed there too.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\nimport tensorflow as tf\n\nrng = np.random.default_rng(24)\n\nwith tf.device('/GPU:0'):\n    input_tensor = tf.constant(rng.uniform(-10.9999152216621, 6.999802457840147,(8, 10, 13, 12, 2, 4)), dtype=tf.float64)\n    pinverse_tensor = tf.linalg.pinv(input_tensor)\n```\n\n### Relevant log output\n\n```shell\n...\n2025-05-21 07:11:44.456637: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114]      Summary of in-use Chunks by size: \n2025-05-21 07:11:44.456641: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 256 totalling 256B\n2025-05-21 07:11:44.456646: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 1280 totalling 1.2KiB\n2025-05-21 07:11:44.456649: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 49920 totalling 48.8KiB\n2025-05-21 07:11:44.456651: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 199680 totalling 195.0KiB\n2025-05-21 07:11:44.456654: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 399360 totalling 390.0KiB\n2025-05-21 07:11:44.456656: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 4 Chunks of size 798720 totalling 3.05MiB\n2025-05-21 07:11:44.456659: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 5782 Chunks of size 1838592 totalling 9.90GiB\n2025-05-21 07:11:44.456661: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 1842944 totalling 1.76MiB\n2025-05-21 07:11:44.456663: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1121] Sum Total of in-use chunks: 9.91GiB\n2025-05-21 07:11:44.456666: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1123] Total bytes in pool: 10636427264 memory_limit_: 10636427264 available bytes: 0 curr_region_allocation_bytes_: 21272854528\n2025-05-21 07:11:44.456670: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1128] Stats: \nLimit:                     10636427264\nInUse:                     10636427264\nMaxInUse:                  10636427264\nNumAllocs:                        5792\nMaxAllocSize:                  1842944\nReserved:                            0\nPeakReserved:                        0\nLargestFreeBlock:                    0\n\n2025-05-21 07:11:44.456725: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:508] ****************************************************************************************************\nF0000 00:00:1747825904.456738  861754 gpu_solvers.h:618] Non-OK-status: context->allocate_temp(DataTypeToEnum<Scalar>::value, shape, &scratch_tensor_, alloc_attr)\nStatus: RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[229824] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n*** Check failure stack trace: ***\n    @     0x7ad5d1067bc4  absl::lts_20230802::log_internal::LogMessage::SendToLog()\n    @     0x7ad5d10679c4  absl::lts_20230802::log_internal::LogMessage::Flush()\n    @     0x7ad5d1067fe9  absl::lts_20230802::log_internal::LogMessageFatal::~LogMessageFatal()\n    @     0x7ad5c77bc180  tensorflow::ScratchSpace<>::ScratchSpace()\n    @     0x7ad5c77bbec4  tensorflow::GpuSolver::GetScratchSpace<>()\n    @     0x7ad5c77b4da6  tensorflow::GpuSolver::Gesvd<>()\n    @     0x7ad5c4a23e29  tensorflow::SvdOpGpu<>::RunSVD()\n    @     0x7ad5c4a232fb  tensorflow::SvdOpGpu<>::PerformSVD_MlessN()\n    @     0x7ad5c4a22999  tensorflow::SvdOpGpu<>::ComputeAsync()\n    @     0x7ad5d9b7e96d  tensorflow::AsyncOpKernel::Compute()\n    @     0x7ad5d99474d5  tensorflow::BaseGPUDevice::Compute()\n    @     0x7ad5d99f8f48  tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run()\n    @     0x7ad5d99b8724  tensorflow::FunctionLibraryRuntimeImpl::RunSync()\n    @     0x7ad5d99c5730  tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync()\n    @     0x7ad5d99cbb7d  tensorflow::ProcessFunctionLibraryRuntime::RunSync()\n    @     0x7ad5c153ddf0  tensorflow::KernelAndDeviceFunc::Run()\n    @     0x7ad5c14e98d6  tensorflow::EagerKernelExecute()\n    @     0x7ad5c14f33b0  tensorflow::ExecuteNode::Run()\n    @     0x7ad5c1539244  tensorflow::EagerExecutor::SyncExecute()\n    @     0x7ad5c14e925b  tensorflow::(anonymous namespace)::EagerLocalExecute()\n    @     0x7ad5c14e6929  tensorflow::DoEagerExecute()\n    @     0x7ad5c14ea660  tensorflow::EagerExecute()\n    @     0x7ad5c0b209f7  tensorflow::EagerOperation::Execute()\n    @     0x7ad5c1537943  tensorflow::CustomDeviceOpHandler::Execute()\n    @     0x7ad5be338cf5  TFE_Execute\n    @     0x7ad5d2a31efa  TFE_Py_FastPathExecute_C()\n    @     0x7ad5ef2d5893  pybind11::detail::argument_loader<>::call<>()\n    @     0x7ad5ef2d57cf  pybind11::cpp_function::initialize<>()::{lambda()#1}::__invoke()\n    @     0x7ad5ef2af8df  pybind11::cpp_function::dispatcher()\n    @           0x58208f  (unknown)\n    @           0x549185  _PyObject_MakeTpCall\n    @           0x5d73c9  _PyEval_EvalFrameDefault\n    @           0x5d58eb  PyEval_EvalCode\n    @           0x608b42  (unknown)\n    @           0x6b4e93  (unknown)\n    @           0x6b4bfa  _PyRun_SimpleFileObject\n    @           0x6b4a2f  _PyRun_AnyFileObject\n    @           0x6bca95  Py_RunMain\n    @           0x6bc57d  Py_BytesMain\n    @     0x7ad61a22a1ca  (unknown)\n    @     0x7ad61a22a28b  __libc_start_main\n    @           0x657ce5  _start\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @jiren-the-gray ,\nApologies for the delay, and thank you for raising your concern here. The issue is likely caused by the use of a large input shape, which results in a large number of matrices being processed. This can lead to out-of-memory (OOM) errors on the GPU.\nIs there a specific reason you are using such a large input shape?\nI tested the same logic with a smaller input size, and it worked fine on my end without any issues. I am attaching a [Gist](https://colab.sandbox.google.com/gist/Venkat6871/4aa2202a39fe667c8422774a77218f04/93822_tf_2-19-0-gpu-v.ipynb) below for your reference.\n\nThank you!"
      },
      {
        "user": "jiren-the-gray",
        "body": "Hi @Venkat6871 , thank you for the reply. The tensor here is only around 0.76 MB in size, I am not sure if this is a large enough tensor to cause OOMs. I also tried calculating `pinv` with the torch version `torch.linalg.pinv` on the GPU and it did not crash/face OOM. It can be seen at the second cell at the [colab](https://colab.research.google.com/drive/122og20vCZPZhYEkMwcJSJybUZHP1bSii?usp=sharing). This leads me to believe the way tensorflow is calculating `pinv` has some issues efficiency issues/memory leaks. Even if OOM is expected for any input, tensorflow should throw an exception and exit gracefully instead of crashing by dumping the core. I found this while trying to test the APIs in tensorflow with a fuzzing tool."
      }
    ]
  },
  {
    "issue_number": 72963,
    "title": "Failure running a SavedModel exported from a tf.Module with a Keras model as an instance variable",
    "author": "ivansoban",
    "state": "closed",
    "created_at": "2024-08-01T15:27:28Z",
    "updated_at": "2025-05-26T06:48:10Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:keras",
      "2.17"
    ],
    "body": "### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo, because the sample code produces a core dump.\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.17.0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04.4 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.12\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nSaving a `tf.Module` using `tf.saved_model.save` when that class contains a Keras model in an instance variable results in a `FAILED_PRECONDITION` when run using saved_model_cli or libtensorflow.\r\n\r\nIn Tensorflow v2.15.0, the behavior is as expected: the graph execution proceeds without any errors and the expected results are produced.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nSHAPE = (1, 5)\r\n\r\nclass TestModel(tf.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.dense_layer = tf.keras.layers.Dense(10)\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=SHAPE, dtype=tf.float32)])\r\n    def run(self, x):\r\n        return self.dense_layer(x)\r\n\r\n\r\nmodule = TestModel()\r\nsample_input = tf.random.normal(SHAPE, dtype=tf.float32)\r\nmodule.run(sample_input)\r\n\r\nnp.save('sample_input.npy', sample_input.numpy())\r\ntf.saved_model.save(module, \"test_model\")\r\n\r\n# # To reproduce, run the following:\r\n# python test.py && saved_model_cli run --dir test_model --tag_set serve --signature_def serving_default --inputs 'x=sample_input.npy'\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-08-01 15:25:35.204057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-08-01 15:25:35.261217: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-08-01 15:25:35.278801: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-08-01 15:25:35.313892: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-08-01 15:25:37.270110: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-08-01 15:25:38.898105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4281 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\r\n2024-08-01 15:25:38.898868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 944 MB memory:  -> device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\r\nWARNING:tensorflow:From /home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/tools/saved_model_cli.py:716: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.saved_model.load` instead.\r\nW0801 15:25:38.903731 139977869341120 deprecation.py:50] From /home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/tools/saved_model_cli.py:716: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.saved_model.load` instead.\r\nINFO:tensorflow:Restoring parameters from test_model/variables/variables\r\nI0801 15:25:38.936800 139977869341120 saver.py:1417] Restoring parameters from test_model/variables/variables\r\n2024-08-01 15:25:38.941206: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\r\n2024-08-01 15:25:39.144248: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable dense/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense/bias/N10tensorflow3VarE does not exist.\r\n\t [[{{function_node __inference_run_106}}{{node dense_1/Add/ReadVariableOp}}]]\r\n2024-08-01 15:25:39.144340: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable dense/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense/bias/N10tensorflow3VarE does not exist.\r\n\t [[{{function_node __inference_run_106}}{{node dense_1/Add/ReadVariableOp}}]]\r\n\t [[StatefulPartitionedCall/_21]]\r\n2024-08-01 15:25:39.144423: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 12615348601576968325\r\nTraceback (most recent call last):\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py\", line 1401, in _do_call\r\n    return fn(*args)\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py\", line 1384, in _run_fn\r\n    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py\", line 1477, in _call_tf_sessionrun\r\n    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.\r\n  (0) FAILED_PRECONDITION: Could not find variable dense/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense/bias/N10tensorflow3VarE does not exist.\r\n\t [[{{function_node __inference_run_106}}{{node dense_1/Add/ReadVariableOp}}]]\r\n\t [[StatefulPartitionedCall/_21]]\r\n  (1) FAILED_PRECONDITION: Could not find variable dense/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense/bias/N10tensorflow3VarE does not exist.\r\n\t [[{{function_node __inference_run_106}}{{node dense_1/Add/ReadVariableOp}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/iantolic-soban/tf_bug/.venv/bin/saved_model_cli\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 1340, in main\r\n    app.run(smcli_main)\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/absl/app.py\", line 308, in run\r\n    _run_main(main, args)\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/absl/app.py\", line 254, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 1338, in smcli_main\r\n    args.func()\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 1036, in run\r\n    run_saved_model_with_feed_dict(\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 721, in run_saved_model_with_feed_dict\r\n    outputs = sess.run(output_tensor_names_sorted, feed_dict=inputs_feed_dict)\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py\", line 971, in run\r\n    result = self._run(None, fetches, feed_dict, options_ptr,\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py\", line 1214, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py\", line 1394, in _do_run\r\n    return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n  File \"/home/iantolic-soban/tf_bug/.venv/lib/python3.10/site-packages/tensorflow/python/client/session.py\", line 1420, in _do_call\r\n    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Graph execution error:\r\n\r\n2 root error(s) found.\r\n  (0) FAILED_PRECONDITION: Could not find variable dense/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense/bias/N10tensorflow3VarE does not exist.\r\n\t [[{{node dense_1/Add/ReadVariableOp}}]]\r\n\t [[StatefulPartitionedCall/_21]]\r\n  (1) FAILED_PRECONDITION: Could not find variable dense/bias. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/dense/bias/N10tensorflow3VarE does not exist.\r\n\t [[{{node dense_1/Add/ReadVariableOp}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I tried to run your code on Colab using TF v2.15.0, 2.17.0, nightly and faced the same issue. Please find the [gist](https://colab.research.google.com/gist/Venkat6871/385f7c5bfc4d9660cb68e5f3fc8feb04/72963_2-15-2-17-nightly-v.ipynb) here for reference.\r\nThank you."
      },
      {
        "user": "Venkat6871",
        "body": "Hi **@ivansoban** ,\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) as this issue is more related to keras\r\n\r\nThank you!\r\n"
      },
      {
        "user": "yashprakashsharma",
        "body": "Hi @Venkat6871 , I am also facing similar issue while deploying a trained tensorflow model using tfserving the dicussion ling you gave above gives Page not found issue, logs for my issue :     external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: Could not find variable conv2d/kernel. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/conv2d/kernel/N10tensorflow3VarE does not exist.\r\n         [[{{function_node __inference_serving_default_11757}}{{node sequential_1/conv2d_1/convolution/ReadVariableOp}}]]\r\n2024-08-07 10:11:13.600499: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: ABORTED: Stopping remaining executors.\r\n2024-08-07 10:17:47.725694: I external/org_tensorflow/tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: ABORTED: Stopping remaining executors"
      }
    ]
  },
  {
    "issue_number": 94133,
    "title": "Fatal crash in `tf.raw_ops.TensorScatterMax` with malformed indices",
    "author": "Nyovelt",
    "state": "closed",
    "created_at": "2025-05-25T19:59:17Z",
    "updated_at": "2025-05-25T19:59:27Z",
    "labels": [
      "type:bug"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux CPU\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.3\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.TensorScatterMax` causes a fatal crash with `Aborted (core dumped)` when provided with malformed indices tensor. \n\nThe issue can be reproduced in this Google Colab notebook: https://colab.research.google.com/drive/19n9h_phK6X1usawpwCdp1S9hFnQR5Q9N?usp=sharing\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\nprint(\"TensorFlow version:\", tf.__version__)\n\n# Create tensors with problematic shapes\ninput_tensor = tf.constant(np.random.random((1, 10, 4)), dtype=tf.float64)\nindices_tensor = tf.constant([[[0]]], dtype=tf.int64)  # Malformed: shape (1,1,1) instead of (1,3)\nupdates_tensor = tf.constant([0.0], dtype=tf.float64)\n\nprint(\"Input Tensor shape:\", input_tensor.shape)\nprint(\"Indices Tensor shape:\", indices_tensor.shape) \nprint(\"Updates Tensor shape:\", updates_tensor.shape)\n\nprint(\"--- Testing TensorScatterMax ---\")\n# This causes a fatal crash\nresult = tf.raw_ops.TensorScatterMax(\n    tensor=input_tensor,\n    indices=indices_tensor,\n    updates=updates_tensor\n)\n```\n\n### Relevant log output\n\n```shell\nTensorFlow version: 2.20.0-dev20250516\nInput Tensor shape: (1, 10, 4)\nIndices Tensor shape: (1, 1, 1)\nUpdates Tensor shape: (1,)\n\n--- Testing TensorScatterMax ---\n2025-05-25 19:56:43.286749: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94133\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94133\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 94126,
    "title": "Segmentation Fault in `tf.raw_ops.HistogramFixedWidth`",
    "author": "Nyovelt",
    "state": "closed",
    "created_at": "2025-05-25T03:30:20Z",
    "updated_at": "2025-05-25T03:32:52Z",
    "labels": [
      "type:bug"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250516\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux CPU \n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.HistogramFixedWidth` causes a segmentation fault when processing large int64 values with int32 output dtype during graph optimization phase.\n\nColab Reproduction: https://colab.research.google.com/drive/1IsOQ0LXGHjezeselGGNHUoxj_Wl3OvWf?usp=sharing\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\ndef reproduce_crash():\n    # Large int64 values that trigger the crash\n    values = tf.constant([\n        [-9114860691027166365, 9114861777597660793],\n        [9114861777597660798, -9114860691027166365],\n        [9114861777597660793, 9114861777597660798],\n        [9114860691027166365, -9114861777597660793]\n    ], dtype=tf.int64)\n    \n    # Value range\n    value_range = tf.constant([9114861777597660672, 9114861777597660798], dtype=tf.int64)\n    \n    # Number of bins\n    nbins = tf.constant(35, dtype=tf.int32)\n    \n    # This causes segmentation fault\n    result = tf.raw_ops.HistogramFixedWidth(\n        values=values,\n        value_range=value_range,\n        nbins=nbins,\n        dtype=tf.int32\n    )\n    return result\n\n# Run this to reproduce the crash\nreproduce_crash()\n```\n\n### Relevant log output\n\n```shell\nSegmentation fault (core dumped)\n\n\nStack:\nThe crash occurs during constant folding optimization:\n\ntensorflow::functor::HistogramFixedWidthFunctor<Eigen::ThreadPoolDevice, long, int>::Compute\n└── tensorflow::HistogramFixedWidthOp<Eigen::ThreadPoolDevice, long, int>::Compute  \n    └── tensorflow::grappler::ConstantFolding::EvaluateNode\n        └── [Graph optimization pipeline]\n```",
    "comments": [
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94126\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94126\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 94024,
    "title": "Support Python v3.13.x",
    "author": "Youssef3082004",
    "state": "closed",
    "created_at": "2025-05-23T15:29:11Z",
    "updated_at": "2025-05-24T19:21:09Z",
    "labels": [
      "type:feature"
    ],
    "body": "### Issue type\n\nFeature Request\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nWindows 10 pro\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.13.1\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI hope **Tensorflow** will be available in the future, Python **version 3.13.x** \n\n### Standalone code to reproduce the issue\n\n```shell\npip install tensorflow\n```\n\n### Relevant log output\n\n```shell\nDefaulting to user installation because normal site-packages is not writeable\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\nERROR: No matching distribution found for tensorflow\n```",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "Please search for duplicates before opening a new issue"
      }
    ]
  },
  {
    "issue_number": 56889,
    "title": "AttributeError: module 'keras.layers' has no attribute 'experimental': M1 Mac Tensorflow Metal",
    "author": "mtoseef99",
    "state": "closed",
    "created_at": "2022-07-25T11:08:08Z",
    "updated_at": "2025-05-24T15:24:20Z",
    "labels": [
      "stat:awaiting response",
      "type:support",
      "stale",
      "comp:keras"
    ],
    "body": "I am trying to run a TensorFlow model on M1 Mac with the following settings:\r\n\r\n1. MacBook Pro M1\r\n2. macOS 12.4\r\n3. tensorflow-deps **&** tensorflow-estimator --> 2.9.0\r\n4. tensorflow-macos --> 2.9.2\r\n5. tensorflow-metal --> 0.5.0\r\n6. keras --> 2.9.0\r\n7. keras-preprocessing  --> 1.1.2\r\n8. Python 3.8.13\r\n\r\nWhen resizing and rescaling from keras.layers, I got the following error:\r\n\r\n```\r\nresize_and_rescale = keras.Sequential([\r\n   layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\r\n   layers.experimental.preprocessing.Rescaling(1./255),\r\n])\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nInput In [15], in <cell line: 1>()\r\n      1 resize_and_rescale = keras.Sequential([\r\n----> 2   layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\r\n      3   layers.experimental.preprocessing.Rescaling(1./255),\r\n      4 ])\r\nAttributeError: module 'keras.layers' has no attribute 'experimental'\r\n```\r\n\r\nAny suggestions? Thanks",
    "comments": [
      {
        "user": "sushreebarsa",
        "body": "@mtoseef99 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThank you!"
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"
      },
      {
        "user": "gadagashwini",
        "body": "Hi @mtoseef99, I could able to use `layers.experimental.preprocessing` with Tensorflow 2.9.2. \r\nI changed `layers.experimental.preprocessing` to `tf.keras.layers.experimental.preprocessing`. Working as expected.\r\n\r\n```\r\ngadag-macbookpro:~ gadag$ source ~/tensorflow-metal/bin/activate\r\n(tensorflow-metal) gadag-macbookpro:~ gadag$ python3\r\nPython 3.9.12 (main, Mar 26 2022, 15:51:15) \r\n[Clang 13.1.6 (clang-1316.0.21.2)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.9.2'\r\n>>> from tensorflow import keras\r\n>>> IMAGE_SIZE=256\r\n>>> resize_and_rescale = keras.Sequential([\r\n...    tf.keras.layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\r\n...    tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\r\n... ])\r\nMetal device set to: AMD Radeon Pro 555X\r\n\r\nsystemMemory: 32.00 GB\r\nmaxCacheSize: 2.00 GB\r\n\r\n2022-08-08 14:02:26.035119: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\r\n2022-08-08 14:02:26.035450: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\r\n>>> \r\n```"
      }
    ]
  },
  {
    "issue_number": 65463,
    "title": "Unable to run Object Detection API i.e. AttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'experimental'",
    "author": "girraj96",
    "state": "closed",
    "created_at": "2024-04-11T12:57:23Z",
    "updated_at": "2025-05-24T15:19:14Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "comp:model",
      "TF 2.15"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nApple M2\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nCommand : python object_detection/builders/model_builder_tf2_test.py\r\n\r\nOutput : \r\n\r\nI tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-04-11 12:54:15.652670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nTraceback (most recent call last):\r\n  File \"/content/models/research/object_detection/builders/model_builder_tf2_test.py\", line 24, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/usr/local/lib/python3.10/dist-packages/object_detection/builders/model_builder.py\", line 26, in <module>\r\n    from object_detection.builders import hyperparams_builder\r\n  File \"/usr/local/lib/python3.10/dist-packages/object_detection/builders/hyperparams_builder.py\", line 27, in <module>\r\n    from object_detection.core import freezable_sync_batch_norm\r\n  File \"/usr/local/lib/python3.10/dist-packages/object_detection/core/freezable_sync_batch_norm.py\", line 20, in <module>\r\n    class FreezableSyncBatchNorm(tf.keras.layers.experimental.SyncBatchNormalization\r\nAttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'experimental'\n\n### Standalone code to reproduce the issue\n\n```shell\nStep 1 - Open Google collab's new note\r\nStep 2 - command  \r\n\r\n!git clone https://github.com/tensorflow/models.git\r\n\r\nStep 3 - command \r\n\r\n%cd models/research\r\n# Compile protos.\r\n!protoc object_detection/protos/*.proto --python_out=.\r\n# Install TensorFlow Object Detection API.\r\n!cp object_detection/packages/tf2/setup.py .\r\n!python -m pip install . \r\n\r\nStep 4 - command \r\n\r\n!python object_detection/builders/model_builder_tf2_test.py\n```\n\n\n### Relevant log output\n\n_No response_",
    "comments": [
      {
        "user": "tilakrayal",
        "body": "@girraj96,\r\nThe temporary workaround for the issue is Could you please try to limit the versions of **tf-models-official >=2.5.1, <2.16.0** and comment out keras in the **research/object_detection/packages/tf2/setup.py**. By default tensorflow v2.16.1 import keras3.0.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/56889\r\n\r\nFor further queries please try to connect with [tensorflow/models](https://github.com/tensorflow/models) repo for the quick resolution.\r\n\r\nThank you!"
      },
      {
        "user": "girraj96",
        "body": "@tilakrayal Thanks it's working now."
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/65463\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/65463\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 64349,
    "title": "how to solve this error ? : AttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'experimental' ",
    "author": "Rajcr2",
    "state": "closed",
    "created_at": "2024-03-23T08:43:08Z",
    "updated_at": "2025-05-24T15:15:01Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:keras",
      "TF 2.15"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.15.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nGot this Error while installing object detection tensorflow api.\r\n\r\ncommand :\r\n!python object_detection/builders/model_builder_tf2_test.py\n\n### Standalone code to reproduce the issue\n\n```shell\n2024-03-23 08:19:24.008197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nTraceback (most recent call last):\r\n  File \"/content/tf_CrimeDetection/models/research/object_detection/builders/model_builder_tf2_test.py\", line 24, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/usr/local/lib/python3.10/dist-packages/object_detection/builders/model_builder.py\", line 26, in <module>\r\n    from object_detection.builders import hyperparams_builder\r\n  File \"/usr/local/lib/python3.10/dist-packages/object_detection/builders/hyperparams_builder.py\", line 27, in <module>\r\n    from object_detection.core import freezable_sync_batch_norm\r\n  File \"/usr/local/lib/python3.10/dist-packages/object_detection/core/freezable_sync_batch_norm.py\", line 20, in <module>\r\n    class FreezableSyncBatchNorm(tf.keras.layers.experimental.SyncBatchNormalization\r\nAttributeError: module 'keras._tf_keras.keras.layers' has no attribute 'experimental'\n```\n\n\n### Relevant log output\n\n_No response_",
    "comments": [
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/64349\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/64349\">No</a>\n"
      },
      {
        "user": "Rajcr2",
        "body": "after installing tf.nightly same issue is occuring.\r\n\r\n```\r\nfrom tensorflow.keras.layers.experimental import einsum\r\nprint(\"Done\")\r\n```\r\n\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n[<ipython-input-3-fd5e9e3ce5fb>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 from tensorflow.keras.layers.experimental import einsum\r\n      2 print(\"Done\")\r\n\r\nModuleNotFoundError: No module named 'tensorflow.keras.layers.experimental'\r\n\r\n> How to reslove this."
      },
      {
        "user": "DineshNeupane",
        "body": "workaround is to limit the tensorflow version and comment out keras from object detection>packages>tf2>setup.py as:\r\n```\r\n 'tf-models-official >=2.5.1, <2.16.0',\r\n    'tensorflow_io',\r\n    #'keras',\r\n```\r\n"
      }
    ]
  },
  {
    "issue_number": 69459,
    "title": "16KB so support",
    "author": "kjlee5435",
    "state": "closed",
    "created_at": "2024-06-10T06:46:27Z",
    "updated_at": "2025-05-24T08:42:04Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:feature",
      "comp:lite",
      "TFLiteConverter",
      "TF 2.16"
    ],
    "body": "### 1. System information\r\n\r\nTF lite 2.16.1\r\n\r\n### 2. Code\r\n\r\n/arm64-v8a/libtensorflowlite_jni.so does not support 16KB alignement.\r\n\r\ngoogle guide :https://developer.android.com/guide/practices/page-sizes?hl=en\r\n\r\nwhen is ETA for supporting 16KB so alignment?",
    "comments": [
      {
        "user": "kjlee5435",
        "body": "Due to our project plan, could you share arrival version and expected date for fixed so release ? \r\n\r\nOur project limits use not 16Kb aligned so files... we need to explain them.. please help us. \r\n"
      },
      {
        "user": "pkgoogle",
        "body": "Hi @arfaian, perhaps you have more visibility for when this will be supported. Can you please take a look? Thanks."
      },
      {
        "user": "arfaian",
        "body": "Fixed with 2d72742."
      }
    ]
  },
  {
    "issue_number": 94077,
    "title": "`tf.raw_ops.Mfcc` causes segmentation fault instead of proper error handling",
    "author": "SilentTester73",
    "state": "closed",
    "created_at": "2025-05-24T04:48:07Z",
    "updated_at": "2025-05-24T05:19:49Z",
    "labels": [
      "type:bug"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nColab CPU\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.Mfcc` crashes with a segmentation fault when given parameters that result in insufficient frequency resolution for the requested filterbank channels. The error is properly detected and logged by TensorFlow, but instead of raising a proper Python exception, the process terminates with a core dump.\n\n## Reproducible Example\n\nGoogle Colab: https://colab.research.google.com/drive/1AY83PguTbxWfwj5KJwqwXl32fr3XIi-e?usp=sharing\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nimport numpy as np\n\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\n\n# Minimal reproduction - only the essential problematic parameters\nspectrogram = tf.constant([[[1.0, 1.0]]], dtype=tf.float32)  # Shape [1,1,2] - only 2 freq bins\nsample_rate = tf.constant(1744830464, dtype=tf.int32)         # Large sample rate  \nlower_freq = float('nan')                                     # NaN lower frequency\n\n# This causes: Missing 17 bands starting at 1 -> Segmentation fault\ntf.raw_ops.Mfcc(\n    spectrogram=spectrogram,\n    sample_rate=sample_rate, \n    upper_frequency_limit=0.498889,\n    lower_frequency_limit=lower_freq,\n    filterbank_channel_count=18,  # Too many channels for 2 freq bins\n    dct_coefficient_count=4\n)\n```\n\n### Relevant log output\n\n```shell\n2025-05-24 04:46:00.649847: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:165] Missing 17 bands  starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 2 input_sample_rate: 1.74483e+09 output_channel_count: 18 lower_frequency_limit: nan upper_frequency_limit: 0.498889\nSegmentation fault (core dumped)\n```",
    "comments": [
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94077\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94077\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 94035,
    "title": "xprof compilation fails with gcc 14.2",
    "author": "sevu",
    "state": "open",
    "created_at": "2025-05-23T18:00:28Z",
    "updated_at": "2025-05-23T18:00:33Z",
    "labels": [
      "type:build/install"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ncurrent master v1.12.1-126604-g3d72b9f063c 2.20.0-dev0+selfbuilt\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 25.04 +venv\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.13\n\n### Bazel version\n\n7.4.1\n\n### GCC/compiler version\n\ngcc 14.2\n\n### CUDA/cuDNN version\n\nnone\n\n### GPU model and memory\n\nnone\n\n### Current behavior?\n\ncompilation fails with gcc 14.2, succeeds with clang 20.1\n\n### Standalone code to reproduce the issue\n\n```shell\nexport TF_NEED_CUDA=0\nexport CC_OPT_FLAGS=\"-march=native -Wno-error -Wno-unused-result -Wno-sign-compare -w\"\n\nbazel build --config=opt --verbose_failures //tensorflow/tools/pip_package:wheel --repo_env=USE_PYWRAP_RULES=1 --repo_env=WHEEL_NAME=tensorflow_cpu\n```\n\n### Relevant log output\n\n```shell\n/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++14' -MD -MF bazel-out/k8-opt/bin/external/org_xprof/xprof/utils/_objs/derived_timeline/derived_timeline.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/org_xprof/xprof/utils/_objs/derived_timeline/derived_timeline.pic.o' -fPIC '-DEIGEN_MAX_ALIGN_BYTES=64' -DEIGEN_ALLOW_UNALIGNED_SCALARS '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DTENSORFLOW_USE_NUMA -DTF_ENABLE_ACTIVITY_WATCHER '-DTF_MAJOR_VERSION=2' '-DTF_MINOR_VERSION=20' '-DTF_PATCH_VERSION=0' '-DTF_VERSION_SUFFIX=\"-dev0+selfbuilt\"' -iquote external/org_xprof -iquote bazel-out/k8-opt/bin/external/org_xprof -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/local_xla -iquote bazel-out/k8-opt/bin/external/local_xla -iquote external/local_tsl -iquote bazel-out/k8-opt/bin/external/local_tsl -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/ml_dtypes_py -iquote bazel-out/k8-opt/bin/external/ml_dtypes_py -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote . -iquote bazel-out/k8-opt/bin -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/snappy -iquote bazel-out/k8-opt/bin/external/snappy -iquote external/hwloc -iquote bazel-out/k8-opt/bin/external/hwloc -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/cuda_cudart -iquote bazel-out/k8-opt/bin/external/cuda_cudart -iquote external/cuda_cublas -iquote bazel-out/k8-opt/bin/external/cuda_cublas -iquote external/cuda_cccl -iquote bazel-out/k8-opt/bin/external/cuda_cccl -iquote external/cuda_nvtx -iquote bazel-out/k8-opt/bin/external/cuda_nvtx -iquote external/cuda_nvcc -iquote bazel-out/k8-opt/bin/external/cuda_nvcc -iquote external/cuda_cusolver -iquote bazel-out/k8-opt/bin/external/cuda_cusolver -iquote external/cuda_cufft -iquote bazel-out/k8-opt/bin/external/cuda_cufft -iquote external/cuda_cusparse -iquote bazel-out/k8-opt/bin/external/cuda_cusparse -iquote external/cuda_curand -iquote bazel-out/k8-opt/bin/external/cuda_curand -iquote external/cuda_cupti -iquote bazel-out/k8-opt/bin/external/cuda_cupti -iquote external/cuda_nvml -iquote bazel-out/k8-opt/bin/external/cuda_nvml -iquote external/cuda_nvjitlink -iquote bazel-out/k8-opt/bin/external/cuda_nvjitlink -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/nvshmem -iquote bazel-out/k8-opt/bin/external/nvshmem -iquote external/local_config_nccl -iquote bazel-out/k8-opt/bin/external/local_config_nccl -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers -Ibazel-out/k8-opt/bin/external/cuda_cudart/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_cublas/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_cccl/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_nvtx/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_nvcc/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_cusolver/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_cufft/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_cusparse/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_curand/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_cupti/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_nvml/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/cuda_nvjitlink/_virtual_includes/headers -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/k8-opt/bin/external/nvshmem/_virtual_includes/nvshmem_config -Ibazel-out/k8-opt/bin/external/local_config_nccl/_virtual_includes/nccl_config -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/eigen_archive/mkl_include -isystem bazel-out/k8-opt/bin/external/eigen_archive/mkl_include -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/hwloc/hwloc -isystem bazel-out/k8-opt/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/k8-opt/bin/external/hwloc/include -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/cuda_cudart/include -isystem bazel-out/k8-opt/bin/external/cuda_cudart/include -isystem external/cuda_cublas/include -isystem bazel-out/k8-opt/bin/external/cuda_cublas/include -isystem external/cuda_cccl/include -isystem bazel-out/k8-opt/bin/external/cuda_cccl/include -isystem external/cuda_nvtx/include -isystem bazel-out/k8-opt/bin/external/cuda_nvtx/include -isystem external/cuda_nvcc/include -isystem bazel-out/k8-opt/bin/external/cuda_nvcc/include -isystem external/cuda_cusolver/include -isystem bazel-out/k8-opt/bin/external/cuda_cusolver/include -isystem external/cuda_cufft/include -isystem bazel-out/k8-opt/bin/external/cuda_cufft/include -isystem external/cuda_cusparse/include -isystem bazel-out/k8-opt/bin/external/cuda_cusparse/include -isystem external/cuda_curand/include -isystem bazel-out/k8-opt/bin/external/cuda_curand/include -isystem external/cuda_cupti/include -isystem bazel-out/k8-opt/bin/external/cuda_cupti/include -isystem external/cuda_nvml/include -isystem bazel-out/k8-opt/bin/external/cuda_nvml/include -isystem external/cuda_nvjitlink/include -isystem bazel-out/k8-opt/bin/external/cuda_nvjitlink/include -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' -DAUTOLOAD_DYNAMIC_KERNELS '-march=native' -Wno-error -Wno-unused-result -Wno-sign-compare -w '-std=c++17' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/org_xprof/xprof/utils/derived_timeline.cc -o bazel-out/k8-opt/bin/external/org_xprof/xprof/utils/_objs/derived_timeline/derived_timeline.pic.o)\n# Configuration: cb965c66e4f49eed21c1b8690bb14256f5148b5976fa6de7af0f9c3ebf43475f\n# Execution platform: @@local_execution_config_platform//:platform\nexternal/org_xprof/xprof/utils/derived_timeline.cc: In function 'std::vector<long int> tensorflow::profiler::{anonymous}::DeriveEventsFromAnnotationsForLines(const tensorflow::profiler::SymbolResolver&, tensorflow::profiler::XPlane*, absl::lts_20230802::Span<const long int>, int64_t, const tensorflow::profiler::ScopeRangeIdTree*)':\nexternal/org_xprof/xprof/utils/derived_timeline.cc:194:8: error: 'GetSortedEvents' was not declared in this scope; did you mean 'tsl::profiler::GetSortedEvents'?\n  194 |        GetSortedEvents<XEventVisitor>(plane_visitor, false, line_ids)) {\n      |        ^~~~~~~~~~~~~~~\n      |        tsl::profiler::GetSortedEvents\nIn file included from external/org_xprof/xprof/utils/derived_timeline.cc:45:\nexternal/local_xla/xla/tsl/profiler/utils/xplane_utils.h:133:20: note: 'tsl::profiler::GetSortedEvents' declared here\n  133 | std::vector<Event> GetSortedEvents(Plane& plane,\n      |                    ^~~~~~~~~~~~~~~\nexternal/org_xprof/xprof/utils/derived_timeline.cc:194:37: error: expected primary-expression before '>' token\n  194 |        GetSortedEvents<XEventVisitor>(plane_visitor, false, line_ids)) {\n      |                                     ^\nexternal/org_xprof/xprof/utils/derived_timeline.cc: In function 'void tensorflow::profiler::DeriveStepEventsFromGroups(const tsl::profiler::GroupMetadataMap&, XPlane*)':\nexternal/org_xprof/xprof/utils/derived_timeline.cc:482:8: error: 'GetSortedEvents' was not declared in this scope; did you mean 'tsl::profiler::GetSortedEvents'?\n  482 |        GetSortedEvents<XEventVisitor>(plane_visitor)) {\n      |        ^~~~~~~~~~~~~~~\n      |        tsl::profiler::GetSortedEvents\nexternal/local_xla/xla/tsl/profiler/utils/xplane_utils.h:133:20: note: 'tsl::profiler::GetSortedEvents' declared here\n  133 | std::vector<Event> GetSortedEvents(Plane& plane,\n      |                    ^~~~~~~~~~~~~~~\nexternal/org_xprof/xprof/utils/derived_timeline.cc:482:37: error: expected primary-expression before '>' token\n  482 |        GetSortedEvents<XEventVisitor>(plane_visitor)) {\n      |                                     ^\nexternal/org_xprof/xprof/utils/derived_timeline.cc: In function 'void tensorflow::profiler::DeriveLinesFromStats(XPlane*)':\nexternal/org_xprof/xprof/utils/derived_timeline.cc:696:8: error: 'GetSortedEvents' was not declared in this scope; did you mean 'tsl::profiler::GetSortedEvents'?\n  696 |        GetSortedEvents<XEventVisitor>(plane_visitor, true)) {\n      |        ^~~~~~~~~~~~~~~\n      |        tsl::profiler::GetSortedEvents\nexternal/local_xla/xla/tsl/profiler/utils/xplane_utils.h:133:20: note: 'tsl::profiler::GetSortedEvents' declared here\n  133 | std::vector<Event> GetSortedEvents(Plane& plane,\n      |                    ^~~~~~~~~~~~~~~\nexternal/org_xprof/xprof/utils/derived_timeline.cc:696:37: error: expected primary-expression before '>' token\n  696 |        GetSortedEvents<XEventVisitor>(plane_visitor, true)) {\n      |                                     ^\nTarget //tensorflow/tools/pip_package:wheel failed to build\n```",
    "comments": []
  },
  {
    "issue_number": 94003,
    "title": "Memory leak in tf.data pipeline when using map with parallel calls",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-23T08:56:58Z",
    "updated_at": "2025-05-23T10:15:19Z",
    "labels": [],
    "body": "## Description\nWe've identified a critical memory leak in the tf.data pipeline when using the `map` transformation with parallel calls. When processing large datasets, memory usage grows unbounded, eventually leading to out-of-memory errors.\n\n## Reproduction Steps\n1. Create a large dataset (e.g., several GB of images)\n2. Use `dataset.map(transform_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)` where `transform_function` creates new tf.Tensor objects\n3. Iterate through the dataset multiple times\n4. Monitor memory usage (e.g., with `top` or `nvidia-smi` if using GPU)\n\n## Observed Behavior\nMemory usage continuously increases during dataset processing without being released, eventually exhausting system memory.\n\n## Expected Behavior\nMemory should be properly managed and released after tensors are no longer needed, maintaining a consistent memory footprint during dataset processing.\n\n## Environment Information\n- TensorFlow version: 2.9.0\n- Python version: 3.8\n- OS: Ubuntu 20.04\n- Hardware: 32GB RAM, NVIDIA RTX 3090 (if running on GPU)\n\n## Additional Information\nThis issue appears to be most pronounced when:\n- Processing large datasets (>10GB)\n- Using high values for `num_parallel_calls` (>8)\n- The map function creates new tensors instead of just transforming existing ones\n\nWe've attempted workarounds by:\n1. Reducing `num_parallel_calls`\n2. Adding `.prefetch(tf.data.experimental.AUTOTUNE)` to the pipeline\n3. Explicitly calling `gc.collect()` periodically\n\nNone of these approaches fully resolve the issue. This appears to be a fundamental memory management issue in how tf.data handles tensor creation in parallel map operations.",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Please stop spamming.\n\n"
      }
    ]
  },
  {
    "issue_number": 94002,
    "title": "Memory leak when using custom loss functions in Keras API",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-23T08:56:34Z",
    "updated_at": "2025-05-23T10:15:11Z",
    "labels": [
      "type:bug"
    ],
    "body": "## Bug Description\nI've discovered a memory leak that occurs when using custom loss functions with the Keras API in TensorFlow.\n\n## Steps to Reproduce\n1. Define a custom loss function\n2. Use it in a Keras model\n3. Train the model for multiple epochs\n4. Monitor memory usage (steadily increases without being released)\n\n## Expected Behavior\nMemory should be properly released after each training batch/epoch.\n\n## Actual Behavior\nMemory usage continuously grows during training, indicating a leak when custom loss functions are used.\n\n## Environment Information\n- TensorFlow version: [please specify]\n- Python version: [please specify]\n- OS: [please specify]\n- Hardware (if relevant): [please specify]\n\n## Additional Information\nThis issue specifically appears with custom loss functions and doesn't occur when using standard built-in loss functions.\n\nPlease let me know if you need any additional information or reproduction code samples.",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Please stop spamming.\n\n"
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94002\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94002\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 93994,
    "title": "Bug: tf.keras.Model.fit() crashes with mixed precision in custom training loops on Apple M1",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-23T08:19:07Z",
    "updated_at": "2025-05-23T10:15:01Z",
    "labels": [],
    "body": "## Description\nWhen using `tf.keras.Model.fit()` with mixed precision in custom training loops on Apple M1 chips, the training process crashes.\n\n## Environment\n- Hardware: Apple M1\n- TensorFlow version: (Please specify the version you're using)\n- OS: macOS (Please specify your OS version)\n\n## Steps to Reproduce\n1. Create a model using Keras\n2. Set up mixed precision training policy\n3. Implement a custom training loop\n4. Call `model.fit()` within the custom training loop\n5. Training crashes on Apple M1 hardware\n\n## Expected Behavior\nThe training should proceed normally with mixed precision on Apple M1 chips.\n\n## Actual Behavior\nThe training process crashes when using mixed precision with custom training loops specifically on Apple M1 hardware.",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Please stop spamming.\n\n"
      }
    ]
  },
  {
    "issue_number": 94005,
    "title": "Memory leak when using custom loss functions in Keras API",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-23T09:00:13Z",
    "updated_at": "2025-05-23T10:14:52Z",
    "labels": [
      "type:bug"
    ],
    "body": "## Bug Description\nMemory leak occurs when using custom loss functions with the Keras API.\n\n## Steps to Reproduce\n1. Define a custom loss function\n2. Use it in a Keras model\n3. Train for multiple epochs\n4. Observe increasing memory usage\n\n## Environment\n- TensorFlow version: [version]\n- Python version: [version]\n- OS: [OS]\n\nMemory usage continuously increases during training with custom loss functions but works fine with built-in loss functions.",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Please stop spamming.\n\n"
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94005\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F94005\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 92251,
    "title": "New version of nVidia Windows GPU driver (576+) causing TF in WSL process to crash.",
    "author": "sunshinejnjn",
    "state": "open",
    "created_at": "2025-04-26T13:15:11Z",
    "updated_at": "2025-05-23T09:36:14Z",
    "labels": [
      "type:bug",
      "wsl2",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.19.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nWSL Ubuntu 24.04.1\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\nany 12.x\n\n### GPU model and memory\n\nrtx 4090\n\n### Current behavior?\n\nWith GPU driver 576.02, this happens:\n\nIf you set the environment variable with \n`os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"`\nto disable the CUDA GPU.\nTensorFlow will fail with a \"free(): double free detected in tcache 2\" if you check GPU availability with \n`physical_devices = tf.config.list_physical_devices('GPU')`\nand the whole process will crash (leading to a Jupyter kernel death).\n\nDrivers of version 572.83 and lower work fine.\nTested with TF 2.18 and 2.19, both have the same behavior.\nAlso, tested with CUDA 12.5 and 12.6. Same behaviors, too.\n\nOn native Linux Ubuntu 24.04.1 with nvidia-drives-570-server, everything is fine.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport os\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport tensorflow as tf\n\ntry:\n    physical_devices = tf.config.list_physical_devices('GPU')\n    print(physical_devices)\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept:\n    print(\"gpu not found, gpu init error!\")\n```\n\n### Relevant log output\n\n```shell\nfree(): double free detected in tcache 2\n\ncrash (core dumped)\n```",
    "comments": [
      {
        "user": "KAVYANSHTYAGI",
        "body": "We must add a guard check before accessing physical_devices[0].\n\nMinimal correct patch would look like this:\n\n\nphysical_devices = tf.config.list_physical_devices('GPU')\nif physical_devices:  # Only if there is at least one GPU\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\nelse:\n    print(\"gpu not found, gpu init error!\")\n\n\nThis solves it cleanly because:\n\n-No indexing into an empty list.\n\n-No invalid memory operations.\n\n-No double free crash.\n\n"
      },
      {
        "user": "sunshinejnjn",
        "body": "\nThanks KAVYANSHTYAGI.\n\nBut, only this operation `tf.config.list_physical_devices('GPU')` itself would cause the crash. No matter if there is a returned value or not.\n\nThe code posted simply demonstrates a usual usage which works fine with *normal* GPU drivers.\nTested with over 5 versions of GPU drivers and various WSL VMs last afternoon. The new GPU driver is indeed the root cause of this crash bug.\n"
      },
      {
        "user": "KAVYANSHTYAGI",
        "body": "Thanks for the clarification!\n\nYou're right \nI now understand that the crash happens at list_physical_devices('GPU') itself, not later during device usage.\n\nIn that case, I think the more robust fix would be to wrap the call to context.context().list_physical_devices('GPU') in a try-except block, so that if the driver misbehaves, TensorFlow can catch the error and handle it gracefully (e.g., log error and return empty list) rather than letting the process crash.\n\n"
      }
    ]
  },
  {
    "issue_number": 93993,
    "title": "Bug: tf.keras.Model.fit() crashes with mixed precision in custom training loops on Apple M1",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-23T08:06:21Z",
    "updated_at": "2025-05-23T08:15:39Z",
    "labels": [],
    "body": "## Description\nWhen using `tf.keras.Model.fit()` with mixed precision in custom training loops on Apple M1 chips, the training process crashes.\n\n## Environment\n- Hardware: Apple M1\n- TensorFlow version: (Please specify the version you're using)\n- OS: macOS (Please specify your OS version)\n\n## Steps to Reproduce\n1. Create a model using Keras\n2. Set up mixed precision training policy\n3. Implement a custom training loop\n4. Call `model.fit()` within the custom training loop\n5. Training crashes on Apple M1 hardware\n\n## Expected Behavior\nThe training should proceed normally with mixed precision on Apple M1 chips.\n\n## Actual Behavior\nThe training process crashes when using mixed precision with custom training loops specifically on Apple M1 hardware.",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Please stop spamming."
      }
    ]
  },
  {
    "issue_number": 93992,
    "title": "Bug: tf.keras.Model.fit() crashes with mixed precision in custom training loops on Apple M1",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-23T08:05:06Z",
    "updated_at": "2025-05-23T08:15:30Z",
    "labels": [],
    "body": "## Description\nWhen using `tf.keras.Model.fit()` with mixed precision in custom training loops on Apple M1 chips, the training process crashes.\n\n## Environment\n- Hardware: Apple M1\n- TensorFlow version: (Please specify the version you're using)\n- OS: macOS (Please specify your OS version)\n\n## Steps to Reproduce\n1. Create a model using Keras\n2. Set up mixed precision training policy\n3. Implement a custom training loop\n4. Call `model.fit()` within the custom training loop\n5. Training crashes on Apple M1 hardware\n\n## Expected Behavior\nThe training should proceed normally with mixed precision on Apple M1 chips.\n\n## Actual Behavior\nThe training process crashes when using mixed precision with custom training loops specifically on Apple M1 hardware.\n\n## Additional Information\nAny logs, error messages, or additional context that might help in diagnosing the issue would be helpful.",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Please stop spamming."
      }
    ]
  },
  {
    "issue_number": 93991,
    "title": "Bug: tf.keras.Model.fit() crashes with mixed precision in custom training loops on Apple M1",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-23T08:04:09Z",
    "updated_at": "2025-05-23T08:15:20Z",
    "labels": [],
    "body": "## Description\nWhen using `tf.keras.Model.fit()` with mixed precision in custom training loops on Apple M1 chips, the training process crashes.\n\n## Environment\n- Hardware: Apple M1\n- TensorFlow version: (Please specify the version you're using)\n- OS: macOS (Please specify your OS version)\n\n## Steps to Reproduce\n1. Create a model using Keras\n2. Set up mixed precision training policy\n3. Implement a custom training loop\n4. Call `model.fit()` within the custom training loop\n5. Training crashes on Apple M1 hardware\n\n## Expected Behavior\nThe training should proceed normally with mixed precision on Apple M1 chips.\n\n## Actual Behavior\nThe training process crashes when using mixed precision with custom training loops specifically on Apple M1 hardware.\n\n## Additional Information\nAny logs, error messages, or additional context that might help in diagnosing the issue would be helpful.",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Please stop spamming."
      }
    ]
  },
  {
    "issue_number": 93618,
    "title": "[TPU] Cannot initialize TPU system on TPUv4-Pod with TensorFlow 2.19",
    "author": "edwardyehuang",
    "state": "open",
    "created_at": "2025-05-18T16:09:45Z",
    "updated_at": "2025-05-23T06:14:19Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:tpus",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nNote that `tpu-vm-tf-2.19.0-pod-se` and `tpu-vm-tf-2.19.0-pod-pijt` perform differently.\n\n### `tpu-vm-tf-2.19.0-pod-pijt`  \n`tpu-vm-tf-2.19.0-pod-pijt`  stuck at `tf.config.experimental_connect_to_cluster(cluster_resolver)`\n\nlatest output before stuck:\n```\nI0518 15:49:45.456652 140338081622016 transport.py:157] Attempting refresh to obtain initial access_token\nW0518 15:49:45.564130 140338081622016 context.py:916] Configuring coordination service type may not be effective because the context is already initialized.\nI0518 15:49:45.564608 140338081622016 remote.py:225] default session config: device_count {\n  key: \"GPU\"\n  value: 0\n}\ndevice_count {\n  key: \"CPU\"\n  value: 1\n}\ngpu_options {\n  experimental {\n  }\n}\nallow_soft_placement: true\nexperimental {\n  collective_group_leader: \"/job:worker/replica:0/task:0\"\n  coordination_config {\n    service_type: \"standalone\"\n    service_leader: \"/job:worker/task:0\"\n  }\n}\npluggable_device_options {\n  experimental {\n  }\n}\n\n```\n\n### `tpu-vm-tf-2.19.0-pod-se`  \n\n`tpu-vm-tf-2.19.0-pod-se`   can pass the `tf.config.experimental_connect_to_cluster(cluster_resolver)`, but raised error in next step `tf.tpu.experimental.initialize_tpu_system(cluster_resolver)`\n\nerror output:\n```\n    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py\", line 72, in initialize_tpu_system\n    return tpu_strategy_util.initialize_tpu_system_impl(\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py\", line 140, in initialize_tpu_system_impl\n    context.async_wait()\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\", line 3115, in async_wait\n    context().sync_executors()\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\", line 883, in sync_executors\n    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)\ntensorflow.python.framework.errors_impl.InternalError: {{function_node __inference__tpu_init_fn_16}} RET_CHECK failure (learning/45eac/google/xla/tpu_node_interfaces.cc:432) *platform\n         [[{{node disconnect_tpu_host/_1}}]]\n\n```\n\n\n\n\n`\n\n### Standalone code to reproduce the issue\n\n```shell\ncluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(name)\ntf.config.experimental_connect_to_cluster(cluster_resolver) # tpu-vm-tf-2.19.0-pod-pijt stuck\ntf.tpu.experimental.initialize_tpu_system(cluster_resolver) # tpu-vm-tf-2.19.0-pod-se error\n\n\nstrategy = tf.distribute.TPUStrategy(cluster_resolver)\n```",
    "comments": [
      {
        "user": "edwardyehuang",
        "body": "@MichaelHudgins @kanglant @tilakrayal   Another issues for TPU initialization on TPUv4 Pod + TensorFlow 2.19.  THANKS 🌹🌹🌹🌹🌹🌹🌹"
      }
    ]
  },
  {
    "issue_number": 93548,
    "title": "Loss suddenly becoming NaN on TPUs",
    "author": "G-Guillard",
    "state": "open",
    "created_at": "2025-05-16T17:41:50Z",
    "updated_at": "2025-05-23T04:08:22Z",
    "labels": [
      "type:bug",
      "comp:tpus",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.16\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nKaggle TPU VM v3-8\n\n### Current behavior?\n\nI came across a very strange issue on Kaggle's TPUs.  I'm not sure whether it's hardware or configuration issue on their side or a TF issue (I don't have the ability to test elsewhere), but since I got no feedback from them after almost one month I figure it may be worth checking on TF side.\n\nThe observation is that after a fixed number of batches, at all epochs, the loss suddenly becomes NaN.  This happens even with dummy data on a dummy model, whatever the tensor dimensions.\n\nThe original report can be found here : https://www.kaggle.com/discussions/product-feedback/574221\n\n\n\n### Standalone code to reproduce the issue\n\n```shell\nMWE Kaggle notebook : https://www.kaggle.com/code/gguillard/a-very-strange-tpu-easter-egg\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the issue using a Kaggle notebook. For your reference, I am attaching the [link](https://www.kaggle.com/code/maayara/93548-kaggle) here.\n\nThank you!"
      },
      {
        "user": "G-Guillard",
        "body": "(You must make the notebook public by clicking on the \"share\" button if you want it to be visible. ;) )"
      }
    ]
  },
  {
    "issue_number": 80538,
    "title": "Not GPU detected using tensorflow/tensorflow:latest-gpu docker image",
    "author": "raul-parada",
    "state": "open",
    "created_at": "2024-11-22T08:46:38Z",
    "updated_at": "2025-05-22T18:46:26Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:support",
      "comp:gpu",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nSupport\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntensorflow/tensorflow:latest-gpu\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nsudo docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu    python -c \"import tensorflow as tf; print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))\"\r\n2024-11-22 08:20:55.695121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1732263655.707089       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1732263655.710704       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-22 08:20:55.722395: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-11-22 08:20:57.119693: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\r\n2024-11-22 08:20:57.119716: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:137] retrieving CUDA diagnostic information for host: 81f8d81af78d\r\n2024-11-22 08:20:57.119720: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:144] hostname: 81f8d81af78d\r\n2024-11-22 08:20:57.119789: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:168] libcuda reported version is: 545.23.6\r\n2024-11-22 08:20:57.119804: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:172] kernel reported version is: 470.256.2\r\n2024-11-22 08:20:57.119808: E external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:262] kernel version 470.256.2 does not match DSO version 545.23.6 -- cannot find working devices in this configuration\r\nNum GPUs Available: 0\n\n### Standalone code to reproduce the issue\n\n```shell\nsudo docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu    python -c \"import tensorflow as tf; print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))\"\r\n\r\nI've also followed this recommendation. https://stackoverflow.com/questions/79127647/tensorflow-docker-not-using-gpu/79214187#79214187\n```\n\n\n### Relevant log output\n\n```shell\nroot@35b972e97b30:/# nvidia-smi\r\nFri Nov 22 08:44:56 2024       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 470.256.02   Driver Version: 470.256.02   CUDA Version: 12.3     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   38C    P0    22W /  N/A |     10MiB /  5946MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\nroot@35b972e97b30:/# nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2023 NVIDIA Corporation\r\nBuilt on Wed_Nov_22_10:17:15_PST_2023\r\nCuda compilation tools, release 12.3, V12.3.107\r\nBuild cuda_12.3.r12.3/compiler.33567101_0\n```\n",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I able to reproduce the same behavior from my end with TensorFlow 2.18.0. I've added output log below for reference\r\nHi, @learning-to-play Please take a look into this issue. Thank you.\r\n\r\n\r\n```\r\n(tf) (base) maayara@venkat-gpu1:~$ sudo docker run -it --rm  tensorflow/tensorflow:latest-gpu python\r\nPython 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2024-11-26 07:16:02.810759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1732605362.831679       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1732605362.838272       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-26 07:16:02.860261: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n>>> tf.__version__\r\n'2.18.0'\r\n>>> print(tf.config.list_physical_devices('GPU'))\r\n2024-11-26 07:16:22.228348: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (34)\r\n[]\r\n>>> \r\n```\r\nThank you!"
      },
      {
        "user": "learning-to-play",
        "body": "Adding @MichaelHudgins .\r\n@Venkat6871 Did you check with the TensorFlow GPU team?\r\nFYI, nightly CUDA builds are broken and someone is looking into addressing them. After the nightly issue is fixed, could you check if it also resolves this issue here?"
      },
      {
        "user": "MichaelHudgins",
        "body": "@Venkat6871 what driver version did you replicate under? "
      }
    ]
  },
  {
    "issue_number": 60825,
    "title": "What phone support tensorflow lite GPU delegate?",
    "author": "ldfandian",
    "state": "closed",
    "created_at": "2023-06-09T08:28:44Z",
    "updated_at": "2025-05-22T17:37:55Z",
    "labels": [
      "stat:awaiting response",
      "type:support",
      "stale",
      "comp:lite",
      "TFLiteGpuDelegate",
      "TF 2.12"
    ],
    "body": "<details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nSupport\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.12.0\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nAndroid 13\n\n### Mobile device\n\nAndroid 13\n\n### Python version\n\n3.8.3\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nI tested multiple mobile phones (including Xiaomi 12s Ultra, OnePlus 8, Honor Nova 10, Oppo Reno 9) using the example Android apk at https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android . On all of the devices, the app tells \"GPU is not supported\", but NNAPI and CPU is OK.\r\n\r\nAnything I can do to enable the GPU delegate on these Qualcomm Snapdragon devices?\n\n### Standalone code to reproduce the issue\n\n```shell\nthe example Android apk at https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android\n```\n\n\n### Relevant log output\n\n_No response_</details>",
    "comments": [
      {
        "user": "pjpratik",
        "body": "Hi @ldfandian \r\n\r\nCan you please add the following to the Android manifest in order to detect GPU delegate.\r\n\r\n```\r\n<uses-library android:name=\"libOpenCL.so\"\r\n           android:required=\"false\"/>\r\n\r\n<uses-library android:name=\"libOpenCL-pixel.so\"\r\n           android:required=\"false\"/>\r\n```\r\n\r\nThanks.\r\n"
      },
      {
        "user": "ldfandian",
        "body": "> Hi @ldfandian\r\n> \r\n> Can you please add the following to the Android manifest in order to detect GPU delegate.\r\n> \r\n> ```\r\n> <uses-library android:name=\"libOpenCL.so\"\r\n>            android:required=\"false\"/>\r\n> \r\n> <uses-library android:name=\"libOpenCL-pixel.so\"\r\n>            android:required=\"false\"/>\r\n> ```\r\n> \r\n> Thanks.\r\n\r\nThanks for the quick response.\r\n\r\nI tried to add the following code in AndroidManifest.xml, but it does not work.\r\n\r\n```\r\n    <application\r\n        android:allowBackup=\"true\"\r\n        android:icon=\"@mipmap/ic_launcher\"\r\n        android:label=\"@string/app_name\"\r\n        android:roundIcon=\"@mipmap/ic_launcher_round\"\r\n        android:supportsRtl=\"true\"\r\n        android:taskAffinity=\"\"\r\n        tools:ignore=\"AllowBackup\">\r\n\r\n        <uses-native-library\r\n            android:name=\"libOpenCL.so\"\r\n            android:required=\"false\" />\r\n        <uses-native-library\r\n            android:name=\"libOpenCL-pixel.so\"\r\n            android:required=\"false\" />\r\n        <uses-native-library\r\n            android:name=\"libGLESv2.so\"\r\n            android:required=\"false\" />\r\n        <uses-native-library\r\n            android:name=\"libGLESv3.so\"\r\n            android:required=\"false\" />\r\n        <uses-native-library\r\n            android:name=\"libvulkan.so\"\r\n            android:required=\"false\" />\r\n        <uses-native-library\r\n            android:name=\"libneuralnetworks.so\"\r\n            android:required=\"false\" />\r\n\r\n        <uses-library\r\n            android:name=\"libOpenCL.so\"\r\n            android:required=\"false\" />\r\n        <uses-library\r\n            android:name=\"libOpenCL-pixel.so\"\r\n            android:required=\"false\" />\r\n        <uses-library\r\n            android:name=\"libGLESv2.so\"\r\n            android:required=\"false\" />\r\n        <uses-library\r\n            android:name=\"libGLESv3.so\"\r\n            android:required=\"false\" />\r\n        <uses-library\r\n            android:name=\"libvulkan.so\"\r\n            android:required=\"false\" />\r\n        <uses-library\r\n            android:name=\"libneuralnetworks.so\"\r\n            android:required=\"false\" />\r\n        ...\r\n```\r\n\r\nBtw, on the Xiaomi 12s Ultra, I checked that the file \"/system/vendor/lib64/libOpenCL.so\" does exist.\r\nHowever, it looks like it does not show in the \"/system/etc/public.libraries.txt\".\r\nMaybe it is the cause? Meaning that the mobile vendor blocks the GPU delegate?\r\n\r\nThe content of \"/system/etc/public.libraries.txt\" and \"/etc/public.libraries.txt\" is like:\r\n```\r\n# See https://android.googlesource.com/platform/ndk/+/master/docs/PlatformApis.md\r\nlibandroid.so\r\nlibaaudio.so\r\nlibamidi.so\r\nlibbinder_ndk.so\r\nlibc.so\r\nlibcamera2ndk.so\r\nlibdl.so\r\nlibEGL.so\r\nlibGLESv1_CM.so\r\nlibGLESv2.so\r\nlibGLESv3.so\r\nlibicu.so\r\nlibicui18n.so\r\nlibicuuc.so\r\nlibjnigraphics.so\r\nliblog.so\r\nlibmediandk.so\r\nlibm.so\r\nlibnativehelper.so\r\nlibnativewindow.so\r\nliblistenjni.qti.so\r\nliblistensoundmodel2.qti.so\r\nlibneuralnetworks.so nopreload\r\nlibOpenMAXAL.so\r\nlibOpenSLES.so\r\nlibRS.so\r\nlibstdc++.so\r\nlibsync.so\r\nlibvulkan.so\r\nlibwebviewchromium_plat_support.so\r\nlibz.so\r\n```\r\n\r\nThe content of \"/vendor/etc/public.libraries.txt\" and \"/system/vendor/etc/public.libraries.txt\" is like:\r\n```\r\nlibqti-perfd-client.so\r\nlibadsprpc.so\r\nlibcdsprpc.so\r\nlibsdsprpc.so\r\nlibfastcvopt.so\r\nlibOpenCL.so\r\nlibSNPE.so\r\nlibmialgo_ai_vision.so\r\nlibmialgo_utils.so\r\nlibxmi_slow_motion_mein.so\r\n```\r\n\r\n\r\nHere is the logcat info:\r\n```\r\n2023-06-10 01:19:29.175 30415-22047 AdrenoGLES-0            org....examples.imageclassification  I  QUALCOMM build                   : 5e81ec0141, Icc5bd9b9d5\r\n                                                                                                    Build Date                       : 11/07/22\r\n                                                                                                    OpenGL ES Shader Compiler Version: EV031.36.08.11\r\n                                                                                                    Local Branch                     : \r\n                                                                                                    Remote Branch                    : \r\n                                                                                                    Remote Branch                    : \r\n                                                                                                    Reconstruct Branch               : \r\n2023-06-10 01:19:29.175 30415-22047 AdrenoGLES-0            org....examples.imageclassification  I  Build Config                     : S P 12.1.1 AArch64\r\n2023-06-10 01:19:29.175 30415-22047 AdrenoGLES-0            org....examples.imageclassification  I  Driver Path                      : /vendor/lib64/egl/libGLESv2_adreno.so\r\n2023-06-10 01:19:29.175 30415-22047 AdrenoGLES-0            org....examples.imageclassification  I  Driver Version                   : 0615.50\r\n2023-06-10 01:19:29.181 30415-22047 AdrenoGLES-0            org....examples.imageclassification  I  PFP: 0x01730155, ME: 0x00000000\r\n\r\n...\r\n\r\n2023-06-10 01:30:26.249  6749-25290 libEGL                  org....examples.imageclassification  E  pre_cache appList: com.sina.weibo,com.ss.android.article.news,com.taobao.taobao,com.smile.gifmaker,com.ss.android.ugc.aweme,com.tencent.mm,tv.danmaku.bili,\r\n...\r\n2023-06-10 01:26:11.511  6746-24015 libEGL                  org....examples.imageclassification  E  call to OpenGL ES API with no current context (logged once per thread)\r\n...\r\n\r\n\r\n```\r\n"
      },
      {
        "user": "ldfandian",
        "body": "An update, after adding those uses-library/uses-native-library settings in AndroidManifest.xml, OnePlus 8T works but the other three devices still don't work...\r\n\r\nI compared the their content of /system/etc/public.libraries.txt, but find nothing different among these devices.\r\n\r\nWhat's the magic here?"
      }
    ]
  },
  {
    "issue_number": 93922,
    "title": "I'm a spammer",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-22T09:28:09Z",
    "updated_at": "2025-05-22T17:34:58Z",
    "labels": [],
    "body": "## Description\nWhen processing large PNG files with TensorFlow's image processing functions, the application can crash or hang due to memory allocation issues and inefficient handling of large image data.\n\n## Problematic Code Sections\n\nThe issue appears to be in the following files:\n\n1. `tensorflow/core/lib/png/png_io.cc` - This file contains the core PNG handling implementation that may not be properly handling memory allocation and decompression for large PNG files.\n\n2. `tensorflow/core/kernels/image/decode_image_op.cc` - The decoder operations may not implement proper size checks or memory allocation strategies for extremely large PNG files.\n\n### Potential Issues\n\n- Inadequate memory allocation checks when decoding large PNGs\n- No upper bound on image dimensions or file size that can be processed\n- Potential memory leaks in error handling paths\n- Insufficient error handling for corrupted PNG data\n- No streaming/chunked processing for large files\n\n## Steps to Reproduce\n\n1. Load a very large PNG file (>100MB or with dimensions >10,000px)\n2. Use `tf.io.decode_png` or `tf.image.decode_image` on the file\n3. Observe crash, out-of-memory error, or application hang\n\n## Expected Behavior\nTensorFlow should either:\n1. Gracefully handle large PNG files within reasonable memory constraints\n2. Provide clear error messages with size recommendations when a PNG is too large\n3. Have configurable limits for maximum image dimensions/file sizes\n\n## Environment\n- TensorFlow version: (latest)\n- Operating System: Affects all platforms\n\n## Possible Solutions\n1. Implement better memory allocation checks before PNG decoding begins\n2. Add configurable maximum size limits for PNG decoding\n3. Implement progressive/chunked decoding for large images\n4. Improve error handling with useful messages when limits are exceeded\n\n@tensorflow-team",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "AI generated spam. Please stop spamming."
      }
    ]
  },
  {
    "issue_number": 93925,
    "title": "I'm a spammer",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-22T10:16:55Z",
    "updated_at": "2025-05-22T17:34:32Z",
    "labels": [],
    "body": "## Description\nWe have observed a memory leak in the image processing module when processing large batches of images. The memory consumption increases steadily over time and does not get properly released even after the processing is complete.\n\n## Observed Behavior\n- Memory usage increases significantly with each batch of images processed\n- Memory is not properly released after processing completes\n- After processing approximately 1000 images, memory usage increases by about 2GB and remains allocated\n- This issue appears to be more pronounced when using GPU acceleration\n\n## Environment\n- TensorFlow version: latest main branch\n- OS: Ubuntu 20.04\n- Python: 3.8\n- GPU: NVIDIA RTX 3080\n\n## Reproduction Steps\n1. Load a dataset of large images (>1MB each)\n2. Process images in batches using tf.image transformations\n3. Run multiple iterations in a loop\n4. Monitor memory usage with tools like `nvidia-smi` (for GPU) or system monitor\n\n## Potential Root Cause\nBased on recent commits, this might be related to changes in the buffer management or resource handling in the GPU execution path. The memory leak could be associated with recent changes to PJRT buffer handling or resource management for image processing operations.\n\n## Impact\nThis memory leak severely impacts long-running image processing tasks, particularly in production environments where large datasets are processed. It eventually leads to OOM errors when running for extended periods.\n\nAny assistance in resolving this issue would be greatly appreciated.",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "Please stop spamming."
      }
    ]
  },
  {
    "issue_number": 93926,
    "title": "I'm a spammer",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-22T10:18:47Z",
    "updated_at": "2025-05-22T17:34:13Z",
    "labels": [],
    "body": "The application crashes after multiple consecutive calls to the predict function",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "Please stop spamming."
      }
    ]
  },
  {
    "issue_number": 93928,
    "title": "I'm a spammer",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-22T11:23:35Z",
    "updated_at": "2025-05-22T17:33:50Z",
    "labels": [],
    "body": "A memory leak was discovered yesterday in the gradient computation process. Further details and reproduction steps will be provided.",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "Please stop spamming."
      }
    ]
  },
  {
    "issue_number": 93932,
    "title": "I'm a spammer",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-22T11:43:02Z",
    "updated_at": "2025-05-22T17:33:39Z",
    "labels": [],
    "body": "## Issue Description\nI found a broken link in the Getting Started guide. When trying to follow the guide, I encountered a link that no longer works.\n\n## Steps to Reproduce\n1. Go to the Getting Started guide\n2. Try to click on the link (specific location details would be helpful)\n\n## Expected Behavior\nThe link should direct to the correct resource.\n\n## Actual Behavior\nThe link is broken and leads to an error page.\n\n## Additional Information\n- TensorFlow version: (if relevant)\n- Browser: (if relevant for documentation issues)\n\nThank you for maintaining this great library!",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "Please stop spamming."
      }
    ]
  },
  {
    "issue_number": 93933,
    "title": "I'm a spammer",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-22T11:49:50Z",
    "updated_at": "2025-05-22T17:33:20Z",
    "labels": [],
    "body": "The examples in the custom training guide need updating to the latest API",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "Please stop spamming."
      }
    ]
  },
  {
    "issue_number": 93935,
    "title": "I'm a spammer",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-22T12:52:52Z",
    "updated_at": "2025-05-22T17:33:03Z",
    "labels": [],
    "body": "The screen reader compatibility appears to be broken in the latest TensorFlow release. Users with visual impairments are experiencing difficulties navigating TensorFlow interfaces and documentation with screen readers.\n\nPlease investigate this accessibility issue to ensure TensorFlow remains usable for all community members.",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "Please stop spamming."
      }
    ]
  },
  {
    "issue_number": 93946,
    "title": "I'm a spammer",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-22T14:41:38Z",
    "updated_at": "2025-05-22T17:32:40Z",
    "labels": [],
    "body": "Memory usage grows unbounded when training for more than 50 epochs, eventually causing OOM errors.",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "Please stop spamming."
      }
    ]
  },
  {
    "issue_number": 93927,
    "title": "I'm a spammer",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-22T10:31:40Z",
    "updated_at": "2025-05-22T17:31:51Z",
    "labels": [],
    "body": "## Documentation Gap Identified\n\nWhile reviewing recent pull requests, I noticed a documentation gap that needs to be addressed.\n\n### Issue Description\nThere is insufficient documentation regarding:\n- How new features are documented across the codebase\n- Standards for documentation consistency\n- Process for documentation review\n\n### Impact\nThis gap makes it difficult for contributors to understand how to properly document their code and for reviewers to enforce consistent documentation standards.\n\n### Suggested Action\n- Create clear documentation guidelines\n- Provide examples of well-documented code\n- Establish a process for documentation review as part of the PR process\n\n### Related PRs\nThis issue was identified while reviewing recent PRs in the repository.",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "AI generated spam."
      }
    ]
  },
  {
    "issue_number": 93930,
    "title": "I'm a spammer",
    "author": "ShengDong207",
    "state": "closed",
    "created_at": "2025-05-22T11:30:05Z",
    "updated_at": "2025-05-22T17:29:37Z",
    "labels": [],
    "body": "This issue is created to track necessary follow-up work related to Pull Request #42.",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "Please don't spam"
      }
    ]
  },
  {
    "issue_number": 93680,
    "title": "TensorFlow `tf.scatter_nd` leads to crash via shape mismatch in indices input",
    "author": "panda123dd",
    "state": "open",
    "created_at": "2025-05-19T22:52:25Z",
    "updated_at": "2025-05-22T15:59:19Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nlinux ubuntu 18.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen passing a crafted indices input with an incorrect shape to tf.scatter_nd, TensorFlow crashes with a CHECK-failure during shape validation. Specifically, it terminates with the following error:\nF tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)\nAborted (core dumped)\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nindices = tf.constant([[4], [3], [1], [7]])\nupdates = tf.constant([9, 10, 11, 12])\nshape = tf.constant([8])\nscatter_nd = tf.scatter_nd(tf.expand_dims(indices, axis=(- 1)), updates, shape)\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I tried running your code on Colab using both TensorFlow 2.18.0 and 2.19.0 version, and I was able to reproduce the same issue on my end. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/b102c772216df2b54e18e9a47d0b273c/93680-tf_2-18-0-2-19-0-v.ipynb) here for reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 93648,
    "title": "Casting NaN to int32 yields different results on CPU and GPU",
    "author": "rookieLiu2018",
    "state": "open",
    "created_at": "2025-05-19T13:43:40Z",
    "updated_at": "2025-05-22T13:04:48Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "comp:gpu",
      "2.17"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen using tf.cast to convert a float32 tensor containing NaN values into int32, TensorFlow silently produces inconsistent outputs on different devices:\nCPU: -2147483648\nGPU: 0\n\nMaybe that should add a warning or error when casting tensors containing NaN to integer types, or define a consistent value across all devices.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nwith tf.device('/CPU:0'):\n    x_cpu = tf.constant([float('nan')], dtype=tf.float32)\n    x_int_cpu = tf.cast(x_cpu, tf.int32)\n    print(\"On CPU cast to int32:\", x_int_cpu.numpy())\n\n\nwith tf.device('/GPU:0'):\n    y_gpu = tf.constant([float('nan')], dtype=tf.float32)\n    y_int_gpu = tf.cast(y_gpu, tf.int32)\n    print(\"On GPU cast to int32:\", y_int_gpu.numpy())\n```\n\n### Relevant log output\n\n```shell\nOn CPU cast to int32: [-2147483648]\nOn GPU cast to int32: [0]\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I tried running your code on Colab using both TensorFlow 2.19.0 and the TF-nightly version, and I was able to reproduce the same issue on my end. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/12c55e68eab1bad42bf93ab2a3ff5652/93648_tf_2-19-0-nightly-gpu-v.ipynb) here for reference.\nThank you!"
      }
    ]
  },
  {
    "issue_number": 93639,
    "title": "TF 2.19 GCC aarch64 Linux CUDA build failure",
    "author": "adamjstewart",
    "state": "open",
    "created_at": "2025-05-19T09:26:55Z",
    "updated_at": "2025-05-22T12:05:09Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:build/install",
      "subtype: ubuntu/linux",
      "TF 2.19"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 24.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.9\n\n### Bazel version\n\n6.5.0\n\n### GCC/compiler version\n\n13.2.0\n\n### CUDA/cuDNN version\n\n12.9.0/8.9.7.29\n\n### GPU model and memory\n\ncuda_arch=80\n\n### Current behavior?\n\nI'm trying to build TF 2.19 but I encounter the following build issue:\n```\nERROR: /tmp/root/spack-stage/spack-stage-py-tensorflow-2.19.0-soea5tbork4puqcza5gxjkajpgbjv63s/spack-src/tensorflow/BUILD:1322:21: Linking tensorflow/libtensorflow_cc.so.2.19.0 failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (from target //tensorflow:libtensorflow_cc.so.2.19.0)\n...\nbazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::RegisteredOperationName::Model<mlir::tfg::GraphOp>::setPropertiesFromAttr(mlir::OperationName, mlir::OpaqueProperties, mlir::Attribute, llvm::function_ref<mlir::InFlightDiagnostic ()>)':\nops.cc:(.text._ZN4mlir23RegisteredOperationName5ModelINS_3tfg7GraphOpEE21setPropertiesFromAttrENS_13OperationNameENS_16OpaquePropertiesENS_9AttributeEN4llvm12function_refIFNS_18InFlightDiagnosticEvEEE[_ZN4mlir23RegisteredOperationName5ModelINS_3tfg7GraphOpEE21setPropertiesFromAttrENS_13OperationNameENS_16OpaquePropertiesENS_9AttributeEN4llvm12function_refIFNS_18InFlightDiagnosticEvEEE]+0x10): relocation truncated to fit: R_AARCH64_JUMP26 against symbol `mlir::tfg::GraphOp::setPropertiesFromAttr(mlir::tfg::detail::GraphOpGenericAdaptorBase::Properties&, mlir::Attribute, llvm::function_ref<mlir::InFlightDiagnostic ()>)' defined in .text._ZN4mlir3tfg7GraphOp21setPropertiesFromAttrERNS0_6detail25GraphOpGenericAdaptorBase10PropertiesENS_9AttributeEN4llvm12function_refIFNS_18InFlightDiagnosticEvEEE section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)\nbazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::StatefulWhileRegionOp::readProperties(mlir::DialectBytecodeReader&, mlir::OperationState&)':\nops.cc:(.text._ZN4mlir3tfg21StatefulWhileRegionOp14readPropertiesERNS_21DialectBytecodeReaderERNS_14OperationStateE+0x2c): relocation truncated to fit: R_AARCH64_CALL26 against symbol `mlir::tfg::detail::StatefulWhileRegionOpGenericAdaptorBase::Properties& mlir::OperationState::getOrAddProperties<mlir::tfg::detail::StatefulWhileRegionOpGenericAdaptorBase::Properties>()' defined in .text._ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail39StatefulWhileRegionOpGenericAdaptorBase10PropertiesEEERT_v[_ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail39StatefulWhileRegionOpGenericAdaptorBase10PropertiesEEERT_v] section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)\nbazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::WhileRegionOp::readProperties(mlir::DialectBytecodeReader&, mlir::OperationState&)':\nops.cc:(.text._ZN4mlir3tfg13WhileRegionOp14readPropertiesERNS_21DialectBytecodeReaderERNS_14OperationStateE+0x2c): relocation truncated to fit: R_AARCH64_CALL26 against symbol `mlir::tfg::detail::WhileRegionOpGenericAdaptorBase::Properties& mlir::OperationState::getOrAddProperties<mlir::tfg::detail::WhileRegionOpGenericAdaptorBase::Properties>()' defined in .text._ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail31WhileRegionOpGenericAdaptorBase10PropertiesEEERT_v[_ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail31WhileRegionOpGenericAdaptorBase10PropertiesEEERT_v] section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)\nbazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::StatefulWhileRegionOp::parse(mlir::OpAsmParser&, mlir::OperationState&)':\nops.cc:(.text._ZN4mlir3tfg21StatefulWhileRegionOp5parseERNS_11OpAsmParserERNS_14OperationStateE+0x358): relocation truncated to fit: R_AARCH64_CALL26 against symbol `mlir::tfg::detail::StatefulWhileRegionOpGenericAdaptorBase::Properties& mlir::OperationState::getOrAddProperties<mlir::tfg::detail::StatefulWhileRegionOpGenericAdaptorBase::Properties>()' defined in .text._ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail39StatefulWhileRegionOpGenericAdaptorBase10PropertiesEEERT_v[_ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail39StatefulWhileRegionOpGenericAdaptorBase10PropertiesEEERT_v] section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)\nbazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::StatelessWhileRegionOp::parse(mlir::OpAsmParser&, mlir::OperationState&)':\nops.cc:(.text._ZN4mlir3tfg22StatelessWhileRegionOp5parseERNS_11OpAsmParserERNS_14OperationStateE+0x2dc): relocation truncated to fit: R_AARCH64_CALL26 against symbol `mlir::tfg::StatelessWhileRegionOp::verifyInherentAttrs(mlir::OperationName, mlir::NamedAttrList&, llvm::function_ref<mlir::InFlightDiagnostic ()>)' defined in .text._ZN4mlir3tfg22StatelessWhileRegionOp19verifyInherentAttrsENS_13OperationNameERNS_13NamedAttrListEN4llvm12function_refIFNS_18InFlightDiagnosticEvEEE section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)\nbazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::WhileRegionOp::parse(mlir::OpAsmParser&, mlir::OperationState&)':\nops.cc:(.text._ZN4mlir3tfg13WhileRegionOp5parseERNS_11OpAsmParserERNS_14OperationStateE+0x358): relocation truncated to fit: R_AARCH64_CALL26 against symbol `mlir::tfg::detail::WhileRegionOpGenericAdaptorBase::Properties& mlir::OperationState::getOrAddProperties<mlir::tfg::detail::WhileRegionOpGenericAdaptorBase::Properties>()' defined in .text._ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail31WhileRegionOpGenericAdaptorBase10PropertiesEEERT_v[_ZN4mlir14OperationState18getOrAddPropertiesINS_3tfg6detail31WhileRegionOpGenericAdaptorBase10PropertiesEEERT_v] section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)\nbazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::ForRegionOp::parse(mlir::OpAsmParser&, mlir::OperationState&)':\nops.cc:(.text._ZN4mlir3tfg11ForRegionOp5parseERNS_11OpAsmParserERNS_14OperationStateE+0x3f0): relocation truncated to fit: R_AARCH64_CALL26 against symbol `mlir::tfg::ForRegionOp::verifyInherentAttrs(mlir::OperationName, mlir::NamedAttrList&, llvm::function_ref<mlir::InFlightDiagnostic ()>)' defined in .text._ZN4mlir3tfg11ForRegionOp19verifyInherentAttrsENS_13OperationNameERNS_13NamedAttrListEN4llvm12function_refIFNS_18InFlightDiagnosticEvEEE section in bazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o)\nbazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `mlir::tfg::GetStaticallyKnownBranch(mlir::Attribute)':\nops.cc:(.text._ZN4mlir3tfgL24GetStaticallyKnownBranchENS_9AttributeE+0xf4): relocation truncated to fit: R_AARCH64_CALL26 against symbol `std::unique_ptr<mlir::detail::ElementsAttrIndexer::NonContiguousState::OpaqueIteratorBase, std::default_delete<mlir::detail::ElementsAttrIndexer::NonContiguousState::OpaqueIteratorBase> >::~unique_ptr()' defined in .text._ZNSt10unique_ptrIN4mlir6detail19ElementsAttrIndexer18NonContiguousState18OpaqueIteratorBaseESt14default_deleteIS4_EED2Ev[_ZNSt10unique_ptrIN4mlir6detail19ElementsAttrIndexer18NonContiguousState18OpaqueIteratorBaseESt14default_deleteIS4_EED5Ev] section in bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/quantization/stablehlo/libpasses.pic.a(convert_func_to_bfloat16.pic.o)\nbazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `std::enable_if<std::is_same<mlir::Attribute, unsigned int>::value||(!std::is_base_of<mlir::Attribute, unsigned int>::value), std::optional<mlir::detail::ElementsAttrIterator<unsigned int> > >::type mlir::ElementsAttr::try_value_begin<unsigned int>() const':\nops.cc:(.text._ZNK4mlir12ElementsAttr15try_value_beginIjEENSt9enable_ifIXoosrSt7is_sameINS_9AttributeET_E5valuentsrSt10is_base_ofIS4_S5_E5valueESt8optionalINS_6detail20ElementsAttrIteratorIS5_EEEE4typeEv[_ZNK4mlir12ElementsAttr15try_value_beginIjEENSt9enable_ifIXoosrSt7is_sameINS_9AttributeET_E5valuentsrSt10is_base_ofIS4_S5_E5valueESt8optionalINS_6detail20ElementsAttrIteratorIS5_EEEE4typeEv]+0x100): relocation truncated to fit: R_AARCH64_CALL26 against symbol `__cxa_guard_release@@CXXABI_1.3' defined in .text section in /usr/lib/gcc/aarch64-linux-gnu/13/libstdc++.so\nops.cc:(.text._ZNK4mlir12ElementsAttr15try_value_beginIjEENSt9enable_ifIXoosrSt7is_sameINS_9AttributeET_E5valuentsrSt10is_base_ofIS4_S5_E5valueESt8optionalINS_6detail20ElementsAttrIteratorIS5_EEEE4typeEv[_ZNK4mlir12ElementsAttr15try_value_beginIjEENSt9enable_ifIXoosrSt7is_sameINS_9AttributeET_E5valuentsrSt10is_base_ofIS4_S5_E5valueESt8optionalINS_6detail20ElementsAttrIteratorIS5_EEEE4typeEv]+0x178): relocation truncated to fit: R_AARCH64_CALL26 against symbol `__cxa_guard_release@@CXXABI_1.3' defined in .text section in /usr/lib/gcc/aarch64-linux-gnu/13/libstdc++.so\nbazel-out/aarch64-opt/bin/tensorflow/core/ir/libDialect.pic.a(ops.pic.o): in function `llvm::StringRef llvm::getTypeName<unsigned int>()':\nops.cc:(.text._ZN4llvm11getTypeNameIjEENS_9StringRefEv[_ZN4llvm11getTypeNameIjEENS_9StringRefEv]+0xa8): additional relocation overflows omitted from the output\ncollect2: error: ld returned 1 exit status\n```\nThis issue only occurs when building for aarch64 CUDA, it does not occur for the x86_64 CUDA build or the aarch64 CPU-only build.\n\n### Standalone code to reproduce the issue\n\n```console\n> git clone git@github.com:adamjstewart/spack.git\n> cd spack\n> git checkout packages/py-tensorflow\n> . share/spack/setup-env.sh\n> spack install py-tensorflow+cuda cuda_arch=80\n```\n\n### Relevant log output\n\n* [build log](https://github.com/user-attachments/files/20281014/spack-build-out.txt.gz)\n* [build env](https://github.com/user-attachments/files/20281015/spack-build-env.txt.gz)\n\nPossibly related to https://github.com/tensorflow/tensorflow/issues/69951",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "@learning-to-play ,\n#78846"
      }
    ]
  },
  {
    "issue_number": 92488,
    "title": "Failed to create Hexagon delegate",
    "author": "samyabose",
    "state": "closed",
    "created_at": "2025-04-30T13:02:07Z",
    "updated_at": "2025-05-22T09:52:33Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "comp:lite",
      "TF2.14"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.14, tf nightly\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 24.04LTS\n\n### Mobile device\n\nAndroid 10\n\n### Python version\n\n3.12.3\n\n### Bazel version\n\n6.1.0\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have been trying to use the Hexagon delegate for the Tensorflow Lite as per the instructions in this [link](https://www.tensorflow.org/lite/performance/hexagon_delegate?hl=ko). But I'm running into an error where the `oemconfig.so` file cannot be accessed for Snapdragon 855 and QCS610.\n\nI am using the hexagon_nn_skel version v.1.20.0.1 from this [guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/android/delegates/hexagon.md). I have also used the same .so files for the benchmark application. I have used the prebuilt benchmark app with arguments like `--use_hexagon=true --hexagon_lib_path=/data/local/tmp` after pushing the .so files to the adb and I have also built the app from source following this [guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/android#to-buildinstallrun), where I have installed the library third_party/hexagon_nn_skel as described. In both cases, I was able to access the Hexagon delegate perfectly.\n\nI have tried using the nightly snapshots and I have also tried building the Hexagon .aar from source (2.14). In both the cases I have received the same error. I have even tried using the .aar files available [here](https://oss.sonatype.org/#nexus-search;gav~org.tensorflow~tensorflow-lite-hexagon~~~). Following the example of the benchmark app, I have also created a custom Hexagon .aar where all the necessary .so files are already present but even that one failed with the same error logs.\n\n<img width=\"1067\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0884cdde-6f80-40ee-95cb-ef0b1503a5b8\" />\n\n### Standalone code to reproduce the issue\n\n```shell\n    try {\n        hexagonDelegate = new HexagonDelegate(this);\n        options.addDelegate(hexagonDelegate);\n    } catch (UnsupportedOperationException e) {\n        // Hexagon delegate is not supported on this device.\n        Log.w(\"HexagonError\",\n                   \"Hexagon delegate is not supported on this device: \" + e.getMessage());\n    } catch (UnsatisfiedLinkError e) {\n        Log.e(\"HexagonError\",\n                  \"Hexagon delegate link error: \" + e.getMessage());\n    }\n```\n\n### Relevant log output\n\n```shell\nError 0xd: open_shell failed for domain 3\nError 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror cannot open oemconfig.so)\nError 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp\n```",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @samyabose \nI apologize for the delay in my response, The [NNAPI](https://www.tensorflow.org/lite/android/delegates/nnapi) and [Hexagon](https://www.tensorflow.org/lite/android/delegates/hexagon) delegates are deprecated and no longer supported by TensorFlow Lite. For more information, see the [NNAPI Migration Guide](https://developer.android.com/ndk/guides/neuralnetworks/migration-guide) and [TF Lite delegates documentation](https://www.tensorflow.org/lite/performance/delegates).\n\nThank you for your cooperation and patience."
      },
      {
        "user": "samyabose",
        "body": "Hi @gaikwadrahul8,\nThanks for the confirmation. \nI can see that CPU and GPU are the only supported delegates currently.\nIs there any other way to use DSP with TFLite since NNAPI and Hexagon are both deprecated? "
      },
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @samyabose \nI apologize for the delay in my response, Based on [NNAPI Migration Guide](https://developer.android.com/ndk/guides/neuralnetworks/migration-guide), the NNAPI delegate is also being deprecated for direct use by app developers, starting with Android 15. This means while it might still function on your current Android 10 device if you're using the standalone TFLite library, it's not the recommended long-term strategy. \n\nI would recommend migrate your application to use [LiteRT on Google Play Services](https://ai.google.dev/edge/litert/android/play_services?hl=pl). You can explicitly use the GPU delegate with LiteRT if you want to prioritize GPU. For DSP usage, LiteRT itself will attempt to utilize it if beneficial and available on the device, likely through its internal integration with Android's acceleration services as mentioned here LiteRT delegates enable hardware acceleration of LiteRT models by leveraging on-device accelerators such as the GPU and [Digital Signal Processor (DSP)](https://en.wikipedia.org/wiki/Digital_signal_processor) as mentioned in this [official documentation](https://ai.google.dev/edge/litert/performance/delegates)\n\nOn some Android devices, **SELinux** policies can restrict access to DSP hardware or related libraries so can you run the below command and retry (requires root access) with current Hexagon delegate and see is it resolving your issue or not ?\n\n`adb shell setenforce 0`\n\nIf I have missed something here please let me know. Thank you for your understanding and cooperation.\n\n\n\n"
      }
    ]
  },
  {
    "issue_number": 92279,
    "title": "Compiling Tensorflow v2.19.0 with mkl_aarch64 fails.",
    "author": "rakshithgb-fujitsu",
    "state": "closed",
    "created_at": "2025-04-28T12:56:57Z",
    "updated_at": "2025-05-22T02:12:21Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "type:build/install",
      "stale",
      "subtype: ubuntu/linux",
      "comp:core",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Python version\n\npython3.10.12\n\n### Bazel version\n\nBazel 6.5.0\n\n### GCC/compiler version\n\nClang 17.0.6\n\n### Current behavior?\n\nTrying to build for cpu from source on ARM SVE platform with the following command:\n\n```bash\nbazel build -j 16 //tensorflow/tools/pip_package:wheel  --config=mkl_aarch64  --copt=\"-march=armv8.4-a+sve\"   --copt=\"-msve-vector-bits=256\"   --copt=\"-DEIGEN_ARM64_USE_SVE\"  --repo_env=USE_PYWRAP_RULES=1   --repo_env=WHEEL_NAME=tensorflow_cpu_sve_eigen_new_dbg\n```\n\nIt fails pointing to:\n```bash\nCompiling tensorflow/core/kernels/mkl/mkl_batch_matmul_op.cc failed: (Exit 1): clang failed: error executing command (from target //tensorflow/core/kernels/mkl:mkl_batch_matmul_op) /usr/lib/llvm-17/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 ... (remaining 318 arguments skipped)\nIn file included from tensorflow/core/kernels/mkl/mkl_batch_matmul_op.cc:36:\nIn file included from ./tensorflow/core/kernels/mkl/mkl_batch_matmul_helper.h:25:\n./tensorflow/core/kernels/mkl/mkl_matmul_ops_common.h:966:38: error: no member named 'csr' in 'dnnl::memory::desc'\n  966 |       const auto tmp = memory::desc::csr(\n      |                        ~~~~~~~~~~~~~~^\n1 error generated.\nTarget //tensorflow/tools/pip_package:wheel failed to build\n```\n\nThis is the part in source code it fails at (`tensorflow/tensorflow/core/kernels/mkl/mkl_matmul_ops_common.h`):\n```C++\n#ifdef ENABLE_ONEDNN_V3\n      const auto tmp = memory::desc::csr(\n          params.a_dims, MklDnnType<Tlhs>(), params.a_nnz,\n          dnnl::memory::data_type::s32, dnnl::memory::data_type::s32);\n      context_.a_md.reset(new memory::desc(tmp));\n#endif  // ENABLE_ONEDNN_V3\n```\n\nIs there a potential fix to this?\n\n\n\n\n",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @rakshithgb-fujitsu ,\nApologies for the delay, and thank you for raising your concern.\nThe issue appears to be caused by an incompatibility between TensorFlow 2.19 and the Clang compiler version. TensorFlow 2.19 requires Clang 18.1.8, but you are currently using Clang 17.0.6. This mismatch is likely the reason for the build failure.\nI have attached the official TensorFlow [documentation](https://www.tensorflow.org/install/source#cpu) for your reference. Please review the supported compiler versions in the document and try rebuilding with Clang 18. If the issue persists, feel free to reach out with the updated error logs.\n\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 92413,
    "title": "GPU Illegal memory access in kernel DenseToCSRSparseMatrix",
    "author": "kokol16",
    "state": "closed",
    "created_at": "2025-04-29T15:34:11Z",
    "updated_at": "2025-05-22T02:12:18Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:gpu",
      "comp:core",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.18.0, v2.18.0-rc2-4-g6550e4bd802 \n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nubuntu 20.04 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.5\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n12.5.1/9\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nRunning the code below triggers an illegal GPU memory access; Compute Sanitizer reports an out-of-bounds write\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\ndef dense_to_csr_example():\n    dense_input = tf.constant(\n        [[complex(r, c) for c in range(11)] for r in range(5)],\n        dtype=tf.complex64\n    )\n\n    indices_data = [\n        [140177464885984, 140177464885984],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [448, 116],\n        [140177475990960, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808],\n        [-9223372036854775808, -9223372036854775808]\n    ]\n    indices = tf.constant(indices_data, dtype=tf.int64)\n\n    \n    csr_matrix = tf.raw_ops.DenseToCSRSparseMatrix(\n        dense_input=dense_input,\n        indices=indices\n    )\n\n    return csr_matrix\n\nif __name__ == \"__main__\":\n    csr_out = dense_to_csr_example()\n    print(\"CSRSparseMatrix (variant) output:\", csr_out)\n```\n\n### Relevant log output\n\n```shell\nCSRSparseMatrix (variant) output: Traceback (most recent call last):\n  File \"/data/ivysyn/ivysyn_2.18/ivysyn/test.py\", line 1168, in <module>\n    print(\"CSRSparseMatrix (variant) output:\", csr_out)\n  File \"/data/ivysyn/ivysyn_2.18/ivysyn/tensorflow-2.18-orig/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 348, in __str__\n    value_text(self, is_repr=False), self.shape, self.dtype.name)\n  File \"/data/ivysyn/ivysyn_2.18/ivysyn/tensorflow-2.18-orig/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 248, in value_text\n    text = tensor._summarize_value()\ntensorflow.python.eager.core._NotOkStatusException: InternalError: CUDA error: Could not synchronize CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\n\n\n\n\n\n========= COMPUTE-SANITIZER\n========= Invalid __global__ write of size 4 bytes\n=========     at void convert_CooToCsr_kernel<(int)0>(const int *, int, int, int *)+0x680\n=========     by thread (1,0,0) in block (0,0,0)\n=========     Address 0x7f77a3fffffc is out of bounds\n=========     and is 4 bytes before the nearest allocation at 0x7f77a4000000 of size 1,134,755,840 bytes\n=========     Saved host backtrace up to driver entry point at kernel launch time\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @kokol16 ,\nApologies for the delay, and thank you for raising your concern here.\nThe main cause appears to be invalid values in the `indices` tensor passed to `tf.raw_ops.DenseToCSRSparseMatrix`. Specifically, the presence of extreme values like `-9223372036854775808` and what appear to be pointer like values leads to illegal memory access during GPU kernel execution. Please make sure that the indices are valid `[row, col]` pairs that point to positions within the `dense_input tensor`. You can try replacing your indices_data with something like:\n`\nindices_data = [[i, j] for i in range(5) for j in range(11)]\n`\nLet us know if the issue persists with valid indices — we’ll be happy to assist you further.\n\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 92550,
    "title": "Tensorflow for Arch Linux",
    "author": "Varshit2448",
    "state": "closed",
    "created_at": "2025-05-01T15:17:04Z",
    "updated_at": "2025-05-22T02:12:15Z",
    "labels": [
      "stat:awaiting response",
      "type:feature",
      "stale",
      "comp:runtime",
      "subtype: ubuntu/linux"
    ],
    "body": "### Issue type\n\nFeature Request\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.8\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Fedora\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI had been working with linux due to its lighter weight than windows so while i was working out a project using tensorflow in arch due to its Constantly rolling there has been a constant juggling of dependency issues with tensorflow it will be really appreciated if tensorflow supports arch \n\n### Standalone code to reproduce the issue\n\n```shell\nTraceback (most recent call last):\n  File \"your_script.py\", line 1, in <module>\n    import tensorflow as tf\n  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/__init__.py\", line X, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/__init__.py\", line X, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py\", line X, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/home/user/.local/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py\", line X, in swig_import_helper\n    return importlib.import_module('_pywrap_tensorflow_internal')\n  File \"/usr/lib/python3.10/importlib/__init__.py\", line X, in import_module\n    ...\nIllegal instruction (core dumped)\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "The OSS team cannot support all possible configurations. There used to be a TF SIG Build that would help with these bugs (@perfinion was helping a lot with Arch), but that got discontinued and the current OSS team only focuses on Ubuntu.\n\nThat being said, PRs to fix issue are always welcome, please tag me for review and I'll make sure they get merged."
      },
      {
        "user": "thephet",
        "body": "I also use ArchLinux and TF.\n\nIt is better to install TF using pip/conda or similar, using your own environment. I find this always works well."
      },
      {
        "user": "Varshit2448",
        "body": "Actually I had tried that but I had an environment dedicated to my recommendation engine which resulted in version conflicts so that's the core reason I had asked for tf to be integrated in arch"
      }
    ]
  },
  {
    "issue_number": 88029,
    "title": "Cuda 12.8 support",
    "author": "johnnynunez",
    "state": "open",
    "created_at": "2025-02-25T12:00:33Z",
    "updated_at": "2025-05-21T10:41:37Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:feature",
      "comp:gpu",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nSupport\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 24.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n0.6.5\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n12.8/9.7.0\n\n### GPU model and memory\n\n5090\n\n### Current behavior?\n\nNot supported. Hermetic cuda download 12.5\n\n### Standalone code to reproduce the issue\n\n```shell\ncompile with hermetic cuda. XLA is not updated in tensorflow\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "clickbaron",
        "body": "Hey, I'm also affected by this issue with CUDA 12.8 and TensorFlow 2.19.0 on Ubuntu CLI within the same OS. While waiting for official support, I found a temporary workaround that might help others:\n\n- I manually installed CUDA 12.8 and cuDNN 9.7.0 on my system.\n- I disabled hermetic CUDA during the TensorFlow build process and pointed it to my local CUDA installation.\n- However, XLA still has compatibility issues, so I had to disable XLA for now.\n\nThis is not a complete solution, but it allowed me to use TensorFlow with CUDA 12.8 for basic tasks. I'm still hoping for official support and XLA updates. Let me know if anyone else has tried this or found better workarounds. \n\nOh and one other thing: within the CUDA update, is there documentation which can help fix this compatibility issue regarding the tensorflow commits ? "
      }
    ]
  },
  {
    "issue_number": 88348,
    "title": "Could you kindly update CUDA version in official Tensorflow containers?",
    "author": "jasonleekungfu",
    "state": "open",
    "created_at": "2025-02-28T17:31:07Z",
    "updated_at": "2025-05-21T10:38:25Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:feature",
      "comp:gpu",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18.0, 2.19.0rc0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nCan you please update official Tensorflow Docker containers starting version 2.18.0? It is supposed to require CUDA > 12.5. However, the official containers still use 12.3 and does not work. We can technically pull the container and modify. But official containers should include the correct dependencies. Thank you!\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\ntf.config.list_physical_devices()\n```\n\n### Relevant log output\n\n```shell\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nW0000 00:00:1740763678.575642 2218885 gpu_device.cc:2340] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\nSkipping registering GPU devices...\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n```",
    "comments": [
      {
        "user": "jasonleekungfu",
        "body": "Hi! Just checking. Any updates on this issue? Thank you!"
      }
    ]
  },
  {
    "issue_number": 88426,
    "title": "`tf.compat.v1.linalg.set_diag` aborts with \"Check failed: d < dims() (2 vs. 2)\"",
    "author": "cybersupersoap",
    "state": "open",
    "created_at": "2025-03-02T02:21:06Z",
    "updated_at": "2025-05-21T10:36:51Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:ops",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.20.0-dev20250225\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04 LTS \n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an aborted issue in TensorFlow when I used API tf.compat.v1.linalg.set_diag . I have confirmed that below code would crash on tf-nightly 2.20.0-dev20250225 (nightly-build).\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\ninput_tensor = tf.random.uniform([5, 4, 4, 4], dtype=tf.float32)\ninput = tf.identity(input_tensor)\ndiagonal_0_0 = 2.0\ndiagonal_0_1 = 3.0\ndiagonal_0_2 = 4.0\ndiagonal_0_3 = 5.0\ndiagonal_0 = [diagonal_0_0, diagonal_0_1, diagonal_0_2, diagonal_0_3]\ndiagonal_1_0 = -1.0\ndiagonal_1_1 = -2.0\ndiagonal_1_2 = -3.0\ndiagonal_1_3 = -4.0\ndiagonal_1 = [diagonal_1_0, diagonal_1_1, diagonal_1_2, diagonal_1_3]\ndiagonal = [diagonal_0, diagonal_1]\nname = 'set_diag'\nk_0 = 0\nk_1 = 1\nk = [k_0, k_1]\nalign = 'LEFT_RIGHT'\nout = tf.compat.v1.linalg.set_diag(input=input, diagonal=diagonal, name=name, k=k, align=align)\n```\n\n### Relevant log output\n\n```shell\n2025-03-02 02:25:05.721633: F tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (2 vs. 2)\nAborted (core dumped)\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce this issue on Colab using TensorFlow 2.18 and the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/8680e02d1534312b2d2964a92034c2a2/88426_tf_2-18-0-nightly-v.ipynb) attached for your reference.\n\nThank you!"
      }
    ]
  },
  {
    "issue_number": 88676,
    "title": "Tensorflow does not recognize GPU",
    "author": "Corey4005",
    "state": "open",
    "created_at": "2025-03-05T21:57:51Z",
    "updated_at": "2025-05-21T10:34:44Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:gpu",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nWSL Linux Ubuntu 24.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.0\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n13.3.0\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nRTX A4000 16GB\n\n### Current behavior?\n\nFirst, I installed Anaconda3-2025.10.1-Linux using wget and the shell script. \n\n### 1.  Install Anaconda Commands:\n\n```\nwget https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh\nbash Anaconda3-2024.10-1-Linux-x86_64.sh\n```\n\nI selected yes on the location in which it generally installs.\nAnaconda was installed in /home/user/anaconda3. \nI also allowed anaconda to update my shell profile by selecting yes. \nI rebooted the shell and went to step 2. \n\n### 2. I made sure my drivers were up to date. \n\nI ran the nvidia-smi command to check this and saw the latest driver for my NVIDIA RTX A4000 GPUs. The image below shows the output with the most up to date drivers. \n\n![Image](https://github.com/user-attachments/assets/89e0f560-be70-4f80-ba24-85cb5a7fc469)\n\n### 3. I installed the latest cuda toolkit using the following commands. \n\n```\nsudo apt-key del 7fa2af80\nwget https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda_12.8.0_570.86.10_linux.run\nsudo sh cuda_12.8.0_570.86.10_linux.run\n```\n\n### 4. I set my path to the cuda toolkit and the lib64 in my .bashrc file\n\nI put the at the bottom of the file and rebooted the shell \n\n```\nexport PATH=/usr/local/cuda-12.8/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:$PATH\n```\n\n### 5. I install cuDNN\nI downloaded the tar file for linux [here](https://developer.nvidia.com/rdp/cudnn-archive). It does seem that there is a file for 24.04, but the tar file did work when I installed it. \n```\ntar -xvf cudnn-linux-x86_64-8.9.7.29_cuda12-archive.tar.xz\n```\nThis unraveled a lot of files on my machine. I followed the installation instructions [here](https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-850/install-guide/)\n\nspecifically, I did the following: \n\n```\nsudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include \nsudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64 \nsudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*\n```\n\nWhen I run nvcc -V, I get: \n\n![Image](https://github.com/user-attachments/assets/31ec4ac5-aaf9-4532-95da-2e3794c69ba7)\n\n### 6 I install a virtual environment and activate it. \n\n```\nconda create -n tf python==3.12\nconda activate tf\npython3 -m pip install tensorflow[and-cuda]\n```\n\nThen, when I check for GPU support, I get errors and my GPU is not recognized:\n\n### 7 The errors are shown below: \n\n```\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n```\n\nErrors: \n\n```\n2025-03-05 15:55:46.327193: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-03-05 15:55:46.338383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1741211746.350376   17653 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1741211746.354146   17653 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-03-05 15:55:46.365820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-03-05 15:55:48.179254: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: RESOURCE_EXHAUSTED: Failed call to cuInit: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n```\n\nWhat am I doing wrong? \n\n### Standalone code to reproduce the issue\n\n```shell\nI am getting a CUDA_ERROR_OUT_OF_MEMORY upon install and my GPU is not being listed.\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "zainali-AT",
        "body": "The highest version of CUDA supported by your drivers is 12.3, whereas the version of CUDA you installed is 12.8 Try downgrading CUDA to <=12.3 and try again. \nLook up [here](https://www.tensorflow.org/install/source#gpu) to find compatible versions of CUDA, cuDNN, gcc and Python for your installed Tensorflow version."
      },
      {
        "user": "Venkat6871",
        "body": "Hi **@Corey4005**, thanks for raising your concern here.\nHi **@zainali-AT**, thanks for your response.\nThe latest CUDA version is not compatible with your machines. When installing TensorFlow with GPU support, the CUDA version is installed automatically by using the following command:\n```\npython3 -m pip install tensorflow[and-cuda]\n```\nThere is no need to manually install the CUDA Toolkit. I am attaching the official [documentation1](https://www.tensorflow.org/install/pip#windows-wsl2), [documentation2](https://www.tensorflow.org/install/source#gpu) for installing TensorFlow using pip and checking compatibility versions. Based on this documentation, I installed it, and it worked fine for me. Please find the details below for your reference.\n```\n(tf_env) maayara@venkat-gpu1:~$ python3\nPython 3.9.21 (main, Dec 11 2024, 16:24:11) \n[GCC 11.2.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\n>>> print(tf.__version__)\n2.18.0\n>>> print(tf.config.list_physical_devices(\"GPU\"))\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n>>> exit()\n(tf_env) maayara@venkat-gpu1:~$ nvidia-smi\nThu Mar  6 05:55:43 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   38C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   40C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n```\n\nThank you!\n\n"
      },
      {
        "user": "Corey4005",
        "body": "> Hi **[@Corey4005](https://github.com/Corey4005)**, thanks for raising your concern here. Hi **[@zainali-AT](https://github.com/zainali-AT)**, thanks for your response. The latest CUDA version is not compatible with your machines. When installing TensorFlow with GPU support, the CUDA version is installed automatically by using the following command:\n> \n> ```\n> python3 -m pip install tensorflow[and-cuda]\n> ```\n> \n> There is no need to manually install the CUDA Toolkit. I am attaching the official [documentation1](https://www.tensorflow.org/install/pip#windows-wsl2), [documentation2](https://www.tensorflow.org/install/source#gpu) for installing TensorFlow using pip and checking compatibility versions. Based on this documentation, I installed it, and it worked fine for me. Please find the details below for your reference.\n> \n> ```\n> (tf_env) maayara@venkat-gpu1:~$ python3\n> Python 3.9.21 (main, Dec 11 2024, 16:24:11) \n> [GCC 11.2.0] :: Anaconda, Inc. on linux\n> Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n> >>> import tensorflow as tf\n> >>> print(tf.__version__)\n> 2.18.0\n> >>> print(tf.config.list_physical_devices(\"GPU\"))\n> [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n> >>> exit()\n> (tf_env) maayara@venkat-gpu1:~$ nvidia-smi\n> Thu Mar  6 05:55:43 2025       \n> +-----------------------------------------------------------------------------------------+\n> | NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n> |-----------------------------------------+------------------------+----------------------+\n> | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n> | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n> |                                         |                        |               MIG M. |\n> |=========================================+========================+======================|\n> |   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n> | N/A   38C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n> |                                         |                        |                  N/A |\n> +-----------------------------------------+------------------------+----------------------+\n> |   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n> | N/A   40C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n> |                                         |                        |                  N/A |\n> +-----------------------------------------+------------------------+----------------------+\n>                                                                                          \n> +-----------------------------------------------------------------------------------------+\n> | Processes:                                                                              |\n> |  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n> |        ID   ID                                                               Usage      |\n> |=========================================================================================|\n> |  No running processes found                                                             |\n> +-----------------------------------------------------------------------------------------+\n> ```\n> \n> Thank you!\n\nHi, @Venkat6871. Thank you for the reply. I am going to provide exact steps I am following after seeing your recommendation that I do not install cuda manually and provide outputs for your review. \n\nFirst, I wiped WSL and reinstalled to get a fresh vm. \n\n### 1. In powershell\n``` \nwsl --unregister Ubuntu \nwsl --install\n```\n\nI now have a new install. \n\n### 2. Opening WSL and looking at my version \n\n```\nlsb_release -a \n```\nThe following image shows the output. \n\n![Image](https://github.com/user-attachments/assets/2979beb5-6bfc-4d6c-9b9c-a5633cd2dd71)\n\n### 3. I pull the latest conda and install\n\n```\nwget https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh\nbash Anaconda3-2024.10-1-Linux-x86_64.sh\n```\nThe installation location is set by default to `/home/user/anaconda3`. \n\nI also allow conda to initialize by selecting `yes` when prompted. I then restart wsl and see that conda environments are now available by seeing `(base)` in my shell. \n\n### 4. I create a conda environment with python 3.12 as suggested for tensorflow 2.18.0 in the [documentation](https://www.tensorflow.org/install/source#gpu) you provided and activate. \n\n```\nconda create -n tf python==3.12\nconda activate tf\n```\n\n### 5. I install tensorflow and cuda using: \n\n```\npython3 -m pip install tensorflow[and-cuda]\n```\npython version shows: \n\n```\npython3 --version\n```\n![Image](https://github.com/user-attachments/assets/073dcc4e-4824-4147-bbb4-1dbb91ab7818)\n\npip version shows: \n\n```\npip --version\n```\n![Image](https://github.com/user-attachments/assets/234144ce-3b13-4b92-b0aa-45078d58aa72)\n\n### 7 I run the following command and get:\n```\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n```\n\n![Image](https://github.com/user-attachments/assets/590e35cf-d17c-411c-a9fb-06216f7a58f7)\n\nnvidia-smi shows: \n\n![Image](https://github.com/user-attachments/assets/7c6c84cb-7478-46c2-bde8-ac61ff2f84b4)\n\n\nAm I installing the wrong versions? "
      }
    ]
  },
  {
    "issue_number": 89110,
    "title": "Gradients are zero when clipping values in function definition",
    "author": "adamvvu",
    "state": "open",
    "created_at": "2025-03-12T22:33:15Z",
    "updated_at": "2025-05-21T10:29:23Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:ops",
      "comp:autograph",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18; 2.10\n\n### Custom code\n\nYes\n\n### Python version\n\n3.10\n\n### Current behavior?\n\nI encountered a strange issue where gradients are zero when clipping-related TensorFlow Ops are defined in functions.\n\nFor context, I was implementing a numerically stable version of sigmoid where the inputs are clipped to `[-10, 10]`. However this bug can be reproduced with other functions. \n\nIn the provided sample code, I define a truncated version of f(x) = x^2 where the function `truncate_domain` is used to clip the input tensors. However gradients only work as expected when the clipping occurs inside `GradientTape` and not when it's part of the function definition.\n\nReproduced on TensorFlow 2.10 with GPU, and latest 2.18 with CPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\ndef square1(x):\n    return x**2\n\ndef truncate_domain(x, lb, ub):\n    # Truncate for numerical stability\n    x = tf.clip_by_value(x, lb, ub)\n    #x = tf.math.maximum(x, lb)\n    #x = tf.math.minimum(x, ub)\n    return x\n\ndef square2(x):\n    x = truncate_domain(x, -10., 10.)\n    return x**2\n\ndef square3(x):\n    x = truncate_domain(x, -10., 10.)\n    return square1(x)\n\n# Note: All 3 square functions are exactly mathematically equivalent when x in [-10, 10].\n\n## Checking gradients\n\nx = tf.constant(15., dtype=tf.float32)\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    x = truncate_domain(x, -10., 10.)\n    l = square1(x)\ngrad1 = tape.gradient(l, x)\nprint(l)\nprint(grad1)\n# >> tf.Tensor(100.0, shape=(), dtype=float32)\n# >> tf.Tensor(20.0, shape=(), dtype=float32)\n\nx = tf.constant(15., dtype=tf.float32)\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    l = square2(x)\ngrad2 = tape.gradient(l, x)\nprint(l)\nprint(grad2)\n# >> tf.Tensor(100.0, shape=(), dtype=float32)\n# >> tf.Tensor(0.0, shape=(), dtype=float32)\n\nx = tf.constant(15., dtype=tf.float32)\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    l = square3(x)\ngrad3 = tape.gradient(l, x)\nprint(l)\nprint(grad3)\n# >> tf.Tensor(100.0, shape=(), dtype=float32)\n# >> tf.Tensor(0.0, shape=(), dtype=float32)\n\n# Note: Gradients for square2 and square3 are 0, even though they are mathematically and (Python-) syntactically equivalent to square1's expected behavior\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "I was able to reproduce the same issue on Colab using TensorFlow 2.18 and the nightly version. Please find the [gist](https://colab.sandbox.google.com/gist/Venkat6871/63e2a5aa4938e5446648f757cd15e43d/89110_tf_2-18-0-nightly-v.ipynb) attached for your reference.\n\nThank you!"
      }
    ]
  },
  {
    "issue_number": 89540,
    "title": "Compilation error（The call method of the StreamingModel class is not implemented correctly）",
    "author": "zhouxiaoyaozzz",
    "state": "open",
    "created_at": "2025-03-19T08:11:56Z",
    "updated_at": "2025-05-21T10:24:17Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIs this a new compilation error？The call method of the StreamingModel class is not implemented correctly, causing TensorFlow/Karas to be unable to infer the output shape and data type of the model.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\n# 定义 CircularBufferLayer\nclass CircularBufferLayer(tf.keras.layers.Layer):\n    def __init__(self, num_features, buffer_size, stride, **kwargs):\n        super().__init__(**kwargs)\n        self.num_features = num_features\n        self.buffer_size = buffer_size\n        self.stride = stride\n        self.gradient_scale = 0.1\n        self.buffer = self.add_weight(name='buffer', shape=(1, buffer_size, num_features),\n                                      initializer='zeros', trainable=False, dtype=tf.float32)\n        self.call_count = self.add_weight(name='call_count', shape=(), initializer='zeros',\n                                          dtype=tf.int32, trainable=False)\n        self.total_call_count = self.add_weight(name='total_call_count', shape=(), initializer='zeros',\n                                                dtype=tf.int32, trainable=False)\n\n    def call(self, inputs):\n        scaled_input = tf.multiply(inputs, self.gradient_scale)\n        new_buffer = tf.concat([scaled_input, self.buffer[:, :-1]], axis=1)\n        self.buffer.assign(new_buffer)\n        return self.buffer\n\n# 定义 StreamingModel\nclass StreamingModel(tf.keras.Model):\n    def call(self, inputs):\n        x, _ = super().call(inputs)  # 假设另一个分支被截断\n        return x\n\n# 实例化模型\nbuffer_layer = CircularBufferLayer(num_features=64, buffer_size=10, stride=1)\nmodel = StreamingModel()\n\n# 定义输入形状\ninput_shape = (None, 10, 64)  # (batch_size, sequence_length, num_features)\ninputs = tf.keras.Input(shape=input_shape[1:])  # 忽略 batch_size\noutputs = model(inputs)\n\n# 构建模型\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n# 编译模型\nmodel.compile(\n    optimizer='adam',  # 使用 Adam 优化器\n    loss='mse',       # 使用均方误差作为损失函数\n    metrics=['mae']   # 使用平均绝对误差作为评估指标\n)\n\n# 准备训练数据\nx_train = tf.random.normal((100, 10, 64))  # 100 个样本，每个样本的形状为 (10, 64)\ny_train = tf.random.normal((100, 10, 64))  # 100 个样本，每个样本的形状为 (10, 64)\n\n# 训练模型\nhistory = model.fit(\n    x_train, y_train,\n    batch_size=32,  # 批量大小\n    epochs=10,      # 训练轮数\n    validation_split=0.2  # 使用 20% 的数据作为验证集\n)\n\n# 保存模型\nmodel.save('streaming_model.h5')\n```\n\n### Relevant log output\n\n```shell\nD:\\project\\DLComplier\\.venv\\Scripts\\python.exe D:\\project\\DLComplier\\.venv\\statiblity.py \n2025-03-19 16:09:45.545622: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-03-19 16:09:46.283611: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-03-19 16:09:48.149403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nWARNING:tensorflow:From D:\\project\\DLComplier\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nTraceback (most recent call last):\n  File \"D:\\project\\DLComplier\\.venv\\statiblity.py\", line 37, in <module>\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"D:\\project\\DLComplier\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"D:\\project\\DLComplier\\.venv\\statiblity.py\", line 27, in call\n    x, _ = super().call(inputs)  # 假设另一个分支被截断\n           ^^^^^^^^^^^^^^^^^^^^\nNotImplementedError: Exception encountered when calling StreamingModel.call().\n\nCould not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel). Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nModel StreamingModel does not have a `call()` method implemented.\n\nArguments received by StreamingModel.call():\n  • args=('<KerasTensor shape=(None, 10, 64), dtype=float32, sparse=False, name=keras_tensor>',)\n  • kwargs=<class 'inspect._empty'>\n```",
    "comments": []
  },
  {
    "issue_number": 89599,
    "title": "How to adapt TFRA with Tensorflow Parameter Strategy V2",
    "author": "jq",
    "state": "open",
    "created_at": "2025-03-20T02:35:58Z",
    "updated_at": "2025-05-21T10:23:28Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:feature"
    ],
    "body": "### Issue type\n\nFeature Request\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nall OS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\ntensorflow recomendation addon 's keras + Ps has been broken for sometime, following are the issues so far\nhttps://github.com/tensorflow/recommenders-addons/issues/182\nhttps://github.com/tensorflow/recommenders-addons/issues/401\nhttps://github.com/tensorflow/recommenders-addons/issues/365\n\nSpecifically, the incompatibility stems from changes introduced by TensorFlow’s ParameterServerStrategy. Two issues have been identified so far, though there may be more:\n\n- Changes in ParameterServerStrategy.extended.worker_devices return values – This issue has already been addressed in https://github.com/tensorflow/recommenders-addons/pull/488\n\n- Graph creation and variable placement differences – ParameterServerStrategy creates the computation graph on the PS and then distributes it to workers. TFRA,  uses proxy variables on each worker. These proxy variables wrapped in DistributedVariableWrapper and inherit from DistributedVariable, and tfra rely on DistributedVariable to get the correct device placement i.e. ._get_on_device_or_primary method,  however, this  only returns device 0, i.e. /job:worker/replica:0/task:0/device:GPU:0 , under ParameterServerStrategy, so all other workers keep crashing.\n\nI wonder what will be correct path to fix the TensorFlow’s ParameterServerStrategy + TFRA + keras? \nI had explored PerWorkerVariable \nto get a PerWorkerVariable, we need to use something like \n`variables.Variable(initial_value=(),\n  shape=shape, dtype=dtype, name=name, per_worker_de_variable=True)`\n      this is not clean for complex classes, i.e.\n`class TrainableWrapper(resource_variable_ops.ResourceVariable):`\n`class ShadowVariable(EmbeddingWeights, TrainableWrapper):`\n      to make ShadowVariable a PerWorkerVariable,   new classes needed with full duplication \n`\nclass TrainableWrapperPerWorker(PerWorkerVariable):`\n`class ShadowVariablePerWorker(EmbeddingWeights, TrainableWrapperPerWorker):\n`\nis this the right path?\n     \n\n\n\n### Standalone code to reproduce the issue\n\n```shell\nany of the code in the issues\nhttps://github.com/tensorflow/recommenders-addons/issues/182\nhttps://github.com/tensorflow/recommenders-addons/issues/401\nhttps://github.com/tensorflow/recommenders-addons/issues/365\n```\n\n### Relevant log output\n\n```shell\nRROR:tensorflow:Worker /job:worker/replica:0/task:1 failed with InvalidArgumentError():/job:worker/replica:0/task:0/device:GPU:0 unknown device.\nAdditional GRPC error information from remote target /job:worker/replica:0/task:1 while calling /tensorflow.eager.EagerService/Enqueue:\n:{\"created\":\"@1742438917.320488513\",\"description\":\"Error received from peer ipv4:127.0.0.1:2232\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"/job:worker/replica:0/task:0/device:GPU:0 unknown device.\",\"grpc_status\":3} [Op:__inference_train_function_2473]\nE0320 02:48:37.325277 139838372021824 cluster_coordinator.py:949] Worker /job:worker/replica:0/task:1 failed with InvalidArgumentError():/job:worker/replica:0/task:0/device:GPU:0 unknown device.\nAdditional GRPC error information from remote target /job:worker/replica:0/task:1 while calling /tensorflow.eager.EagerService/Enqueue:\n:{\"created\":\"@1742438917.320488513\",\"description\":\"Error received from peer ipv4:127.0.0.1:2232\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"/job:worker/replica:0/task:0/device:GPU:0 unknown device.\",\"grpc_status\":3} [Op:__inference_train_function_2473]\nINFO:tensorflow:[Worker 1] Putting back a closure after it failed.\nI0320 02:48:37.325551 139838372021824 cluster_coordinator.py:1077] [Worker 1] Putting back a closure after it failed.\nINFO:tensorflow:[Worker 1] Clearing all resources.\nI0320 02:48:37.325744 139838372021824 cluster_coordinator.py:1065] [Worker 1] Clearing all resources.\nINFO:tensorflow:Cluster now being recovered.\nI0320 02:48:37.325959 139838355236416 cluster_coordinator.py:991] Cluster now being recovered.\n2025-03-20 02:48:37.326268: W tensorflow/core/common_runtime/eager/context_distributed_manager.cc:694] Device filters can only be specified when initializing the cluster. Any changes in device filters are ignored when updating the server def.\nINFO:tensorflow:Cluster successfully recovered.\nI0320 02:48:37.334258 139838355236416 cluster_coordinator.py:997] Cluster successfully recovered.\nINFO:tensorflow:Worker /job:worker/replica:0/task:1 has been recovered.\nI0320 02:48:37.334562 139838372021824 cluster_coordinator.py:964] Worker /job:worker/replica:0/task:1 has been recovered.\nINFO:tensorflow:Worker /job:worker/replica:0/task:1 calling on_recovery_fn\nI0320 02:48:37.334733 139838372021824 cluster_coordinator.py:967] Worker /job:worker/replica:0/task:1 calling on_recovery_fn\nINFO:tensorflow:[Worker 1] calling _on_worker_recovery\nI0320 02:48:37.334853 139838372021824 cluster_coordinator.py:1103] [Worker 1] calling _on_worker_recovery\n2025-03-20 02:48:37.361068: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:553] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n2025-03-20 02:48:37.362102: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:553] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n```",
    "comments": []
  },
  {
    "issue_number": 89600,
    "title": "NotImplementedError: StreamingModel.call() not implemented",
    "author": "zhouxiaoyaozzz",
    "state": "open",
    "created_at": "2025-03-20T02:55:01Z",
    "updated_at": "2025-05-21T10:23:09Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nwindows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.6\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen I try to compile and run my custom Keras model (StreamingModel), I encounter the following error:\n`NotImplementedError: Exception encountered when calling StreamingModel.call().\nCould not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel).\nEither the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method.\nError encountered: Model StreamingModel does not have a `call()` method implemented.`\nExpected Behavior\nI expect the StreamingModel to correctly implement the call() method and infer the output shape/dtype automatically, or allow me to manually specify it using compute_output_spec() or compute_output_shape().\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\n# Define CircularBufferLayer\nclass CircularBufferLayer(tf.keras.layers.Layer):\n    def __init__(self, num_features, buffer_size, stride, **kwargs):\n        super().__init__(**kwargs)\n        self.num_features = num_features\n        self.buffer_size = buffer_size\n        self.stride = stride\n        self.gradient_scale = 0.1\n        self.buffer = self.add_weight(name='buffer', shape=(1, buffer_size, num_features),\n                                      initializer='zeros', trainable=False, dtype=tf.float32)\n        self.call_count = self.add_weight(name='call_count', shape=(), initializer='zeros',\n                                          dtype=tf.int32, trainable=False)\n        self.total_call_count = self.add_weight(name='total_call_count', shape=(), initializer='zeros',\n                                                dtype=tf.int32, trainable=False)\n\n    def call(self, inputs):\n        scaled_input = tf.multiply(inputs, self.gradient_scale)\n        new_buffer = tf.concat([scaled_input, self.buffer[:, :-1]], axis=1)\n        self.buffer.assign(new_buffer)\n        return self.buffer\n\n# Define StreamingModel\nclass StreamingModel(tf.keras.Model):\n    def call(self, inputs):\n        x, _ = super().call(inputs)  # Assume another branch is truncated\n        return x\n\n# Instantiate the model\nbuffer_layer = CircularBufferLayer(num_features=64, buffer_size=10, stride=1)\nmodel = StreamingModel()\n\n# Define input shape\ninput_shape = (None, 10, 64)  # (batch_size, sequence_length, num_features)\ninputs = tf.keras.Input(shape=input_shape[1:])  # Ignore batch_size\noutputs = model(inputs)\n\n# Build the model\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n# Compile the model\nmodel.compile(\n    optimizer='adam',  # Use Adam optimizer\n    loss='mse',       # Use mean squared error as the loss function\n    metrics=['mae']   # Use mean absolute error as the evaluation metric\n)\n```\n\n### Relevant log output\n\n```shell\n2025-03-19 16:09:45.545622: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-03-19 16:09:46.283611: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-03-19 16:09:48.149403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nWARNING:tensorflow:From D:\\project\\DLComplier\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nTraceback (most recent call last):\n  File \"D:\\project\\DLComplier\\.venv\\statiblity.py\", line 37, in <module>\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"D:\\project\\DLComplier\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"D:\\project\\DLComplier\\.venv\\statiblity.py\", line 27, in call\n    x, _ = super().call(inputs)  # Assume another branch is truncated\n           ^^^^^^^^^^^^^^^^^^^^\nNotImplementedError: Exception encountered when calling StreamingModel.call().\n\nCould not automatically infer the output shape / dtype of 'streaming_model' (of type StreamingModel). Either the `StreamingModel.call()` method is incorrect, or you need to implement the `StreamingModel.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nModel StreamingModel does not have a `call()` method implemented.\n\nArguments received by StreamingModel.call():\n  • args=('<KerasTensor shape=(None, 10, 64), dtype=float32, sparse=False, name=keras_tensor>',)\n  • kwargs=<class 'inspect._empty'>\n```",
    "comments": []
  },
  {
    "issue_number": 89628,
    "title": "Add support for SeparableConv2DTranspose (Depthwise Conv2DTranspose)",
    "author": "eewindfly",
    "state": "open",
    "created_at": "2025-03-20T09:27:50Z",
    "updated_at": "2025-05-21T10:22:09Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:feature"
    ],
    "body": "### Issue type\n\nFeature Request\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.16.1\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nFeature Request\n\nTensorFlow currently lacks a SeparableConv2DTranspose operation, which is essential for efficient depthwise transposed convolutions. PyTorch already supports this easily via the groups parameter in ConvTranspose2d. However, TensorFlow’s Conv2DTranspose does not have a groups argument, making it impossible to achieve the same functionality.\n\nWorkarounds\n\nCurrently, the only way to approximate this functionality in TensorFlow is:\n\t•\tManually splitting input channels and applying Conv2DTranspose separately (inefficient and slow).\n\nProposed Solution\n\t•\tEither add a groups parameter to Conv2DTranspose, similar to PyTorch,\n\t•\tOr implement a native SeparableConv2DTranspose API to complement SeparableConv2D.\n\nUse Case\n\nSeparable transposed convolutions are useful for:\n\t•\tEfficient upsampling in lightweight CNN architectures.\n\t•\tMobile and edge computing models requiring optimized computations.\n\nWould be great to have this officially supported in TensorFlow! Thanks.\n\n#12001 \nPrevious discussion has been closed which is not able to reopen, so I create a new one for it.\n\n### Standalone code to reproduce the issue\n\n```shell\nNone\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": []
  },
  {
    "issue_number": 89795,
    "title": "fastspeech2 中文中训练",
    "author": "tortoise-ben",
    "state": "open",
    "created_at": "2025-03-22T09:15:42Z",
    "updated_at": "2025-05-21T10:21:56Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:performance",
      "TF 2.9"
    ],
    "body": "### Issue type\n\nPerformance\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.9.0-rc2-42-g8a20d54a3c1 2.9.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04.5 LTS\n\n### Mobile device\n\nUbuntu 22.04.5 LTS\n\n### Python version\n\nPython 3.7.12\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\ngcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n\n### CUDA/cuDNN version\n\nnvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2021 NVIDIA Corporation Built on Thu_Nov_18_09:45:30_PST_2021 Cuda compilation tools, release 11.5, V11.5.119 Build cuda_11.5.r11.5/compiler.30672275_0\n\n### GPU model and memory\n\n NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2  NVIDIA GeForce RTX 4090\n\n### Current behavior?\n\n我通过[TensorFlowTTS](https://github.com/TensorSpeech/TensorFlowTTS)项目，训练了一个自己的音色模型，但是合成的音频声音很奇怪。以下是我用同一段文字在自己训练的模型和huggingface下载的模型【tensorspeech/tts-fastspeech2-baker-ch】合成的语音的对比。melgan用的是同一个，是从huggingface下载的【tensorspeech/tts-mb_melgan-baker-ch】。难道melgan也要自己重新训练吗？\n\n![Image](https://github.com/user-attachments/assets/55561285-248a-450d-b9b1-f5180d261d31)\n用我生成的模型合成出来的音频能微弱的听到声音，但是很小声。不知道是什么问题导致的。\n\n### Standalone code to reproduce the issue\n\n```shell\nimport soundfile as sf\nimport numpy as np\n\nimport tensorflow as tf\nimport sys\nsys.path.append(\".\")\n\nfrom tensorflow_tts.inference import AutoConfig\nfrom tensorflow_tts.inference import AutoProcessor\nfrom tensorflow_tts.inference import TFAutoModel\n\nmel_cfg = AutoConfig.from_pretrained(\"/data/benben/TensorFlowTTS-master/my_test/mel.yml\")\nmb_melgan = TFAutoModel.from_pretrained(\"/data/benben/TensorFlowTTS-master/my_test/mel.h5\", config=mel_cfg)\n\nconfig1 = AutoConfig.from_pretrained(\"examples/fastspeech2/conf/fastspeech2.baker.v2.yaml\")\n\n# fastspeech2 = TFAutoModel.from_pretrained(\"dump_mandarin-fm/exp/train.fastspeech2.baker.v2/checkpoints/model-10000.h5\", config=config1)\n# processor = AutoProcessor.from_pretrained(\"dump_mandarin-fm/baker_mapper.json\")\n\nfastspeech2 = TFAutoModel.from_pretrained(\"my_test/model.h5\", config=config1)\nprocessor = AutoProcessor.from_pretrained(\"my_test/processor.json\")\n\ntext = \"通用在去年第三季度每天售出逾二点七万辆新车，业界关注的是转型创业企业生态圈运营商之后\"\ntext = \"通用在去年第三季度每天售出逾二点七万辆新车\"\n\ninput_ids = processor.text_to_sequence(text, inference=True)\n\nprint(\"***********\",input_ids)\n\nmel_before, mel_after, duration_outputs, _, _ = fastspeech2.inference(\n    input_ids=tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),\n    speaker_ids=tf.convert_to_tensor([0], dtype=tf.int32),\n    speed_ratios=tf.convert_to_tensor([1.0], dtype=tf.float32),\n    f0_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32),\n    energy_ratios =tf.convert_to_tensor([1.0], dtype=tf.float32),\n)\n\n\nprint(mel_before)\nprint(mel_after)\n# melgan inference (mel-to-wav)\naudio = mb_melgan.inference(mel_after)[0, :, 0]\n\n# save to file\nsf.write('./audio.wav', audio, 24000, \"PCM_16\")\n# sf.write('./audio.wav', audio, 24000)\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": []
  },
  {
    "issue_number": 89797,
    "title": "colab tpu initialization",
    "author": "ymlau",
    "state": "open",
    "created_at": "2025-03-22T13:33:30Z",
    "updated_at": "2025-05-21T10:20:48Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:tpus",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI was unable to initialize tpu in colab, it seems that the system failed to detect the tpu.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps://colab.research.google.com/drive/1mGQ7_-EeCtFnrrtUW6mYhyFx8k06MCTz?usp=sharing\n```\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system_impl(cluster_resolver, tpu_cluster_resolver_cls)\n    138       with ops.device(tpu._tpu_system_device_name(job)):  # pylint: disable=protected-access\n--> 139         output = _tpu_init_fn()\n    140       context.async_wait()\n\n4 frames\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)\n    152       filtered_tb = _process_traceback_frames(e.__traceback__)\n--> 153       raise e.with_traceback(filtered_tb) from None\n    154     finally:\n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n     58     raise core._status_to_exception(e) from None\n---> 59   except TypeError as e:\n     60     keras_symbolic_tensors = [x for x in inputs if _is_keras_symbolic_tensor(x)]\n\nInvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [tpu_embedding_config=\"\", is_global_init=false, compilation_failure_closes_chips=false, embedding_config=\"\", enable_whole_mesh_compilations=false, tpu_cancellation_closes_chips=2]\nRegistered devices: [CPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4]\n\nDuring handling of the above exception, another exception occurred:\n\nNotFoundError                             Traceback (most recent call last)\n<ipython-input-2-1b8a8e7ba345> in <cell line: 0>()\n      3 tf.config.experimental_connect_to_cluster(resolver)\n      4 # This is the TPU initialization code that has to be at the beginning.\n----> 5 tf.tpu.experimental.initialize_tpu_system(resolver)\n      6 print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/cluster_resolver/tpu/tpu_cluster_resolver.py in initialize_tpu_system(cluster_resolver)\n     70     NotFoundError: If no TPU devices found in eager mode.\n     71   \"\"\"\n---> 72   return tpu_strategy_util.initialize_tpu_system_impl(\n     73       cluster_resolver, TPUClusterResolver)\n     74 \n\n/usr/local/lib/python3.11/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py in initialize_tpu_system_impl(cluster_resolver, tpu_cluster_resolver_cls)\n    140       context.async_wait()\n    141     except errors.InvalidArgumentError as e:\n--> 142       raise errors.NotFoundError(\n    143           None, None,\n    144           \"TPUs not found in the cluster. Failed in initialization: \"\n\nNotFoundError: TPUs not found in the cluster. Failed in initialization: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [tpu_embedding_config=\"\", is_global_init=false, compilation_failure_closes_chips=false, embedding_config=\"\", enable_whole_mesh_compilations=false, tpu_cancellation_closes_chips=2]\nRegistered devices: [CPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[ConfigureDistributedTPU]] [Op:__inference__tpu_init_fn_4]\n```",
    "comments": []
  },
  {
    "issue_number": 90927,
    "title": "(deprecated arguments) (deprecated arguments) (deprecated arguments)",
    "author": "SamuelMarks",
    "state": "open",
    "created_at": "2025-04-08T18:37:51Z",
    "updated_at": "2025-05-21T09:45:59Z",
    "labels": [
      "type:docs-bug",
      "stat:awaiting tensorflower",
      "type:bug"
    ],
    "body": "### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nN/A\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nE.g., https://www.tensorflow.org/api_docs/python/tf/function#features\n> Compiles a function into a callable TensorFlow graph. (deprecated arguments) (deprecated arguments) (deprecated arguments)\n\n### Standalone code to reproduce the issue\n\n```shell\nsee https://www.tensorflow.org/api_docs/python/tf/function#features\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "Should be opened against docs repo"
      },
      {
        "user": "SamuelMarks",
        "body": "@mihaimaruseac You mean the one with no \"issues\" tab?\n![Image](https://github.com/user-attachments/assets/c9421be5-de2b-430a-9946-c6155446f3ff)"
      },
      {
        "user": "mihaimaruseac",
        "body": "Oh, lol. I didn't realize that has happened 🤦 "
      }
    ]
  },
  {
    "issue_number": 91790,
    "title": "Out of memory when saving a tf.data.Dataset",
    "author": "bermeitinger-b",
    "state": "open",
    "created_at": "2025-04-19T19:58:18Z",
    "updated_at": "2025-05-21T09:40:46Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:keras",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.19.0 (and 2.18.1)\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux, Colab\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11 (Colab) and 3.12 (local)\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\nNone (CPU), 12.2 (local), 12.4 (Colab)\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI want to use a keras_hub model to preprocess a dataset. This process is expensive, so I want to save the resulting dataset.\nAs everything else in my codebase relies on TensorFlow and Keras, it seems like using `tf.data.Dataset` should be the best option.\nBut, when saving the dataset, the process crashes with OOM.\n\n1. Load a dataset with `tfds`.\n2. Preprocess the dataset (with the `.map()` functionality\n3. Call `ds.save() ` to save the dataset to disk <-- here, it crashes.\n\nThis works for smaller datasets. In the example code, use only 5000 elements (`.take(5000)` ), and the dataset is stored as intended.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps://colab.research.google.com/drive/1W-1vk3-TBnybhCdVt3Y2Y7itLpDAhMGZ?usp=sharing\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "rajatmohan22",
        "body": "The thing is that before saving the dataset to disk, the.save() method tries to process and hold the whole dataset in memory. It runs out of RAM if your dataset is big or if the preprocessing is heavy (like when you use a deep model). This is especially true in Colab or other limited settings.\n\nI think a workaround could be something like:\n\n```\nfor i, chunk in enumerate(ds.batch(1000)):\n    tf.data.experimental.save(chunk, f\"part_{I}\")\n```\n\n"
      }
    ]
  },
  {
    "issue_number": 93586,
    "title": "Create a docker image from research/object_detection/dockerfiles/tf2/Dockerfile",
    "author": "stefanoinference",
    "state": "open",
    "created_at": "2025-05-17T19:51:58Z",
    "updated_at": "2025-05-21T05:03:04Z",
    "labels": [
      "type:build/install",
      "subtype: ubuntu/linux",
      "2.6.0"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.6.2\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 22\n\n### Mobile device\n\n_No response_\n\n### Python version\n\npython3.6\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\nCUDA 12.6 - Driver Version: 560.35.03\n\n### GPU model and memory\n\nNVIDIA GeForce RTX 2080 - 12GB\n\n### Current behavior?\n\nI follow th command:\n\ndocker build -f research/object_detection/dockerfiles/tf2/Dockerfile -t od .\n\n\ndocker run --name geo-train-tensorflow2-gpu --gpus all -it -v /product/git/tensorflow-training:/product/git/tensorflow-training -v /product/dataset:/product/dataset -p 6006:6006 -p 8888:8888 --ipc=host --ulimit memlock=-1 --ulimit stack=67108864  od\n\nI use colab notebook https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\n\n\n\n$env\n\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\nLD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nLESSCLOSE=/usr/bin/lesspipe %s %s\nLANG=C.UTF-8\nHOSTNAME=4fade06f19b5\nNVIDIA_VISIBLE_DEVICES=all\nPWD=/home/tensorflow/models/research\nHOME=/home/tensorflow\nTF_CPP_MIN_LOG_LEVEL=3\nTERM=xterm-256color\nCUDA_PKG_VERSION=10-1=10.1.243-1\nCUDA_VERSION=10.1.243\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nSHLVL=1\nNVIDIA_REQUIRE_CUDA=cuda>=10.1 brand=tesla,driver>=384,driver<385 brand=tesla,driver>=396,driver<397 brand=tesla,driver>=410,driver<411\nPATH=/home/tensorflow/.local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nPS1=\\[\\e]0;\\u@\\h: \\w\\a\\]${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \nLESSOPEN=| /usr/bin/lesspipe %s\n_=/usr/bin/env\n\n\npip list:\n\nPackage                       Version\n----------------------------- ---------------\nabsl-py                       0.12.0\napache-beam                   2.38.0\nargon2-cffi                   21.3.0\nargon2-cffi-bindings          21.2.0\nasn1crypto                    0.24.0\nastunparse                    1.6.3\nasync-generator               1.10\nattrs                         22.2.0\navro-python3                  1.10.2\nbackcall                      0.2.0\nbeautifulsoup4                4.6.0\nbleach                        4.1.0\ncached-property               1.5.2\ncachetools                    4.1.0\ncairocffi                     0.8.0\ncertifi                       2025.4.26\ncffi                          1.11.5\nchardet                       3.0.4\ncharset-normalizer            2.0.12\nclang                         5.0\ncloudpickle                   2.2.1\ncmake                         3.28.4\ncolorama                      0.4.5\ncontextlib2                   21.6.0\ncrcmod                        1.7\ncryptography                  2.1.4\ncycler                        0.11.0\nCython                        3.0.12\ndataclasses                   0.8\ndecorator                     5.1.1\ndefusedxml                    0.7.1\ndill                          0.3.1.1\ndm-tree                       0.1.8\ndocopt                        0.6.2\nentrypoints                   0.4\nfastavro                      1.4.7\nflatbuffers                   1.12\ngast                          0.4.0\ngin-config                    0.5.0\ngoogle-api-core               2.8.2\ngoogle-api-python-client      2.52.0\ngoogle-auth                   1.35.0\ngoogle-auth-httplib2          0.2.0\ngoogle-auth-oauthlib          0.4.1\ngoogle-pasta                  0.2.0\ngoogleapis-common-protos      1.56.3\ngrpcio                        1.48.2\nh5py                          3.1.0\nhdfs                          2.7.3\nhtml5lib                      0.999999999\nhttplib2                      0.19.1\nidna                          2.6\nimportlib-metadata            4.8.3\nimportlib-resources           5.4.0\nipykernel                     5.5.6\nipython                       7.16.3\nipython-genutils              0.2.0\njedi                          0.17.2\nJinja2                        3.0.3\njoblib                        1.1.1\njsonschema                    3.2.0\njupyter-client                7.1.2\njupyter-core                  4.9.2\njupyterlab-pygments           0.1.2\nkaggle                        1.6.17\nkeras                         2.6.0\nKeras-Preprocessing           1.1.2\nkeyring                       10.6.0\nkeyrings.alt                  3.0\nkiwisolver                    1.3.1\nlvis                          0.5.3\nlxml                          4.2.1\nMarkdown                      3.2.1\nMarkupSafe                    2.0.1\nmatplotlib                    3.3.4\nmistune                       0.8.4\nnbclient                      0.5.9\nnbconvert                     6.0.7\nnbformat                      5.1.3\nnest-asyncio                  1.6.0\nnotebook                      6.4.10\nnumpy                         1.19.5\noauth2client                  4.1.3\noauthlib                      3.1.0\nobject-detection              0.1\nolefile                       0.45.1\nopencv-python                 4.11.0.86\nopencv-python-headless        4.11.0.86\nopt-einsum                    3.3.0\norjson                        3.6.1\npackaging                     21.3\npandas                        1.1.5\npandocfilters                 1.5.1\nparso                         0.7.1\npexpect                       4.9.0\npickleshare                   0.7.5\nPillow                        8.4.0\npip                           21.3.1\nply                           3.11\nportalocker                   2.7.0\nprometheus-client             0.17.1\npromise                       2.3\nprompt-toolkit                3.0.36\nproto-plus                    1.23.0\nprotobuf                      3.19.6\npsutil                        7.0.0\nptyprocess                    0.7.0\npy-cpuinfo                    9.0.0\npyarrow                       6.0.1\npyasn1                        0.4.8\npyasn1-modules                0.2.8\npycocotools                   2.0.7\npycparser                     2.18\npycrypto                      2.6.1\npydot                         1.4.2\nPygments                      2.14.0\nPyGObject                     3.26.1\npymongo                       3.13.0\npyparsing                     2.4.7\npyrsistent                    0.18.0\npython-apt                    1.6.5+ubuntu0.2\npython-dateutil               2.9.0.post0\npython-slugify                6.1.2\npytz                          2025.2\npyxdg                         0.25\nPyYAML                        6.0.1\npyzmq                         25.1.2\nregex                         2023.8.8\nrequests                      2.27.1\nrequests-oauthlib             1.3.0\nrsa                           4.0\nsacrebleu                     2.2.0\nscikit-learn                  0.24.2\nscipy                         1.4.1\nSecretStorage                 2.3.1\nSend2Trash                    1.8.3\nsentencepiece                 0.2.0\nseqeval                       1.2.2\nsetuptools                    46.1.3\nsix                           1.15.0\ntabulate                      0.8.10\ntensorboard                   2.6.0\ntensorboard-data-server       0.6.1\ntensorboard-plugin-wit        1.6.0.post3\ntensorflow                    2.6.2\ntensorflow-addons             0.14.0\ntensorflow-datasets           4.5.2\ntensorflow-estimator          2.6.0\ntensorflow-gpu                2.2.0\ntensorflow-hub                0.16.0\ntensorflow-io                 0.21.0\ntensorflow-io-gcs-filesystem  0.21.0\ntensorflow-metadata           1.2.0\ntensorflow-model-optimization 0.7.3\ntensorflow-text               2.6.0\ntermcolor                     1.1.0\nterminado                     0.12.1\ntestpath                      0.6.0\ntext-unidecode                1.3\ntf-models-official            2.7.2\ntf-slim                       1.1.0\nthreadpoolctl                 3.1.0\ntornado                       6.1\ntqdm                          4.64.1\ntraitlets                     4.3.3\ntypeguard                     2.13.3\ntyping-extensions             3.7.4.3\nuritemplate                   4.1.1\nurllib3                       1.25.9\nwcwidth                       0.2.13\nwebencodings                  0.5\nWerkzeug                      1.0.1\nwheel                         0.37.1\nwrapt                         1.12.1\nxcffib                        0.5.1\nzipp                          3.6.0\n\nWhen import \" import tensorflow as tf\" I have this error:\n\nNotFoundError                             Traceback (most recent call last)\n<ipython-input-1-49156a41fe80> in <module>\n      8 from PIL import Image, ImageDraw, ImageFont\n      9 \n---> 10 import tensorflow as tf\n     11 \n     12 from object_detection.utils import label_map_util\n\n~/.local/lib/python3.6/site-packages/tensorflow/__init__.py in <module>\n    436     _main_dir = _os.path.join(_s, 'tensorflow/core/kernels')\n    437     if _os.path.exists(_main_dir):\n--> 438       _ll.load_library(_main_dir)\n    439 \n    440     # Load third party dynamic kernels.\n\n~/.local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py in load_library(library_location)\n    152 \n    153     for lib in kernel_libraries:\n--> 154       py_tf.TF_LoadLibrary(lib)\n    155 \n    156   else:\n\nNotFoundError: /usr/local/lib/python3.6/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\n\n### Standalone code to reproduce the issue\n\n```shell\ngit clone https://github.com/tensorflow/models.git\n \ndocker build -f research/object_detection/dockerfiles/tf2/Dockerfile -t od .\n\n\ndocker run --name geo-train-tensorflow2-gpu --gpus all -it -v /product/git/newtrain:/product/git/newtrain -v /product/git/tensorflow-training:/product/git/tensorflow-training -v /product/dataset:/product/dataset -p 6006:6006 -p 8888:8888 --ipc=host --ulimit memlock=-1 --ulimit stack=67108864  od\n\ndocker exec -it geo-train-tensorflow2-gpu  /bin/bash\n\npip install notebook\n\nopen the notebook :  https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb\n\nand execute it\n```\n\n### Relevant log output\n\n```shell\nNotFoundError                             Traceback (most recent call last)\n<ipython-input-1-49156a41fe80> in <module>\n      8 from PIL import Image, ImageDraw, ImageFont\n      9 \n---> 10 import tensorflow as tf\n     11 \n     12 from object_detection.utils import label_map_util\n\n~/.local/lib/python3.6/site-packages/tensorflow/__init__.py in <module>\n    436     _main_dir = _os.path.join(_s, 'tensorflow/core/kernels')\n    437     if _os.path.exists(_main_dir):\n--> 438       _ll.load_library(_main_dir)\n    439 \n    440     # Load third party dynamic kernels.\n\n~/.local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py in load_library(library_location)\n    152 \n    153     for lib in kernel_libraries:\n--> 154       py_tf.TF_LoadLibrary(lib)\n    155 \n    156   else:\n\nNotFoundError: /usr/local/lib/python3.6/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\n```",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "You are missing the shared lib that implements the sobol ops."
      },
      {
        "user": "mihaimaruseac",
        "body": "Also, this path does not seem to be in the repo."
      },
      {
        "user": "stefanoinference",
        "body": "how can add the shared library? Thank's"
      }
    ]
  },
  {
    "issue_number": 92610,
    "title": "Setting the protobuf version at installation with bazel",
    "author": "constantinosTsirogiannis",
    "state": "closed",
    "created_at": "2025-05-02T09:10:04Z",
    "updated_at": "2025-05-20T15:48:19Z",
    "labels": [
      "type:build/install",
      "subtype: ubuntu/linux",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.1 \n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Centos 7\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nNA\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n12.3\n\n### CUDA/cuDNN version\n\nNA\n\n### GPU model and memory\n\nNA\n\n### Current behavior?\n\nTrying to install version 2.18.1 with bazel but  the version of protobuf that automatically gets installed with it is 3.21 . To avoid ODR conflicts in my super-project, I would prefer to get a newer version of protobuf. Attempts to install protobuf first and then point tensorflow installation to its path did not work. Is there a simple way to specify the version of protobuf that I want during the TF installation? \n\nAnother related question: is there an upper bound on the protobuf version that can get installed with TF 2.18.1? \n\nFinally, I would like to ask the same questions as above for an older version of TF, namely 2.2.0.\n\nThanks for your help\n\n### Standalone code to reproduce the issue\n\n```shell\nNA\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "Venkat6871",
        "body": "Hi @constantinosTsirogiannis ,\nApologies for the delay, and thank you for raising your concern. Regarding the [Protobuf](https://github.com/tensorflow/tensorflow/blob/r2.18/tensorflow/tools/pip_package/setup.py) version, TensorFlow 2.18.1 requires the following version range:\n`\n'protobuf>=3.20.3,<6.0.0dev,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5'\n`\nIf you are attempting to use a version outside this range (especially a much newer one), it could lead to compatibility issues. Also, please note that TensorFlow 2.18.0 and above requires CUDA 12.5. Since you mentioned using CUDA 12.3, we recommend upgrading to ensure full compatibility.\nFor further reference, here is the official [TensorFlow install guide](https://www.tensorflow.org/install/source#gpu) which includes the compatible software versions.\nThank you!"
      },
      {
        "user": "constantinosTsirogiannis",
        "body": "Hi @Venkat6871 ,\n\nThanks for your response on the proper version range for protobuf. My main question is _how_ can I choose which version of protobuf gets installed with TF using bazel? Is there a flag/parameter that is easy to set when calling bazel or in a config file? \n\nThis is for TF 2.18.1 , but I would also like to know the answer for TF 2.2.0 . \n\nThanks for your help  "
      },
      {
        "user": "mihaimaruseac",
        "body": "Actually, @Venkat6871 's answer is not fully correct.\n\nWith Bazel, the dependencies are those specified in the BUILD files, `.bzl` files and anything else called from `WORKSPACE`. These are pinned to specific versions. If you want to change the version, you need to locate where the pin is, and update it.\n\nHowever, there is no guarantee that the code would compile with those changes, the TF team would not be able to offer support if there are compilation issues. Nevertheless, if you send PRs with fixes, I'd be happy to approve."
      }
    ]
  },
  {
    "issue_number": 60408,
    "title": "Missing installed C header",
    "author": "thorsten-klein",
    "state": "closed",
    "created_at": "2023-04-24T08:12:02Z",
    "updated_at": "2025-05-20T15:08:06Z",
    "labels": [
      "stat:awaiting response",
      "type:build/install",
      "stale",
      "comp:lite",
      "subtype: ubuntu/linux",
      "TF 2.12"
    ],
    "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBuild/Install\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.12.0\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nUbuntu 20\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\ngcc-9.3.0\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nWe install tensorflow via cmake (using conan).\r\nFor cmake we specify `-DTFLITE_ENABLE_INSTALL=ON` in order to install all files.\r\n\r\nWe have an simple C example where we just include tflite:\r\n```\r\n#include <tensorflow/lite/c/c_api.h>\r\n```\r\n\r\nThis simple #include fails:\r\n```\r\nIn file included from /CONAN/.conan/data/tensorflow-lite/2.12.0/conan_toolchain_catalog/build/package/3affdc49463f7dbc587d2b7b43afa94817639253/include/tensorflow/lite/interpreter.h:21,\r\n                 from /home/EU.BSHG.COM/kleint/GIT/conan_toolchain_catalog/recipes/software/tensorflow-lite/2.12.0/test_package/hello.cpp:3:\r\n/CONAN/.conan/data/tensorflow-lite/2.12.0/conan_toolchain_catalog/build/package/3affdc49463f7dbc587d2b7b43afa94817639253/include/tensorflow/lite/core/interpreter.h:55:10: fatal error: tensorflow/lite/profiling/telemetry/c/telemetry_setting_internal.h: No such file or directory\r\n   55 | #include \"tensorflow/lite/profiling/telemetry/c/telemetry_setting_internal.h\"\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n```\r\n\r\nWith current master I get similar issue:\r\n```\r\nIn file included from /CONAN/.conan/data/tensorflow-lite/af92cf22/conan_toolchain_catalog/build/package/a31972e3f4561475d1be503229914494e93219e0/include/tensorflow/lite/core/async/async_signature_runner.h:23,\r\n                 from /CONAN/.conan/data/tensorflow-lite/af92cf22/conan_toolchain_catalog/build/package/a31972e3f4561475d1be503229914494e93219e0/include/tensorflow/lite/core/interpreter.h:44,\r\n                 from /CONAN/.conan/data/tensorflow-lite/af92cf22/conan_toolchain_catalog/build/package/a31972e3f4561475d1be503229914494e93219e0/include/tensorflow/lite/interpreter.h:21,\r\n                 from /home/EU.BSHG.COM/kleint/GIT/conan_toolchain_catalog/recipes/software/tensorflow-lite/af92cf22/test_package/hello.cpp:3:\r\n/CONAN/.conan/data/tensorflow-lite/af92cf22/conan_toolchain_catalog/build/package/a31972e3f4561475d1be503229914494e93219e0/include/tensorflow/lite/core/async/async_subgraph.h:24:10: fatal error: tensorflow/lite/core/async/interop/c/types.h: No such file or directory\r\n   24 | #include \"tensorflow/lite/core/async/interop/c/types.h\"\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n\r\n```\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\n#include <tensorflow/lite/c/c_api.h>\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_</details>",
    "comments": [
      {
        "user": "pjpratik",
        "body": "Hi @thorsten-klein Thanks for reporting the issue.\r\n\r\nCould you please mention the steps you have followed to encounter the issue?\r\n\r\nAlso, please  provide the error log to better understand the issue.\r\n\r\nThanks."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 92922,
    "title": "`tf.truncatediv` doesn't work on complex64 and complex128",
    "author": "Redempt1onzzZZ",
    "state": "open",
    "created_at": "2025-05-08T00:56:56Z",
    "updated_at": "2025-05-20T04:48:17Z",
    "labels": [
      "type:bug",
      "comp:ops",
      "awaiting PR merge",
      "TF 2.18"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.19.0-rc0-6-ge36baa30292 2.19.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nNVIDIA H100 - 80GB\n\n### Current behavior?\n\nSimilar to #92873 . The doc of tf.math.reciprocal illustrates that the input must be one of the following types:bfloat16, half, float32, float64, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128. While it doesn't work on complex64 and complex128. Other dtype works well. XLA_JIT the same.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\na = tf.constant(1+2j,dtype=tf.complex128)\nb = tf.constant(1+2j,dtype=tf.complex128)\nc = tf.truncatediv(a,b)\nprint(c)\n```\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\nNotFoundError                             Traceback (most recent call last)\nCell In[182], line 5\n      3 a = tf.constant(1+2j,dtype=tf.complex128)\n      4 b = tf.constant(1+2j,dtype=tf.complex128)\n----> 5 c = tf.truncatediv(a,b)\n      6 print(c)\n\nFile ~/anaconda3/envs/cotfuzz/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142, in weak_tensor_binary_op_wrapper.<locals>.wrapper(*args, **kwargs)\n    140 def wrapper(*args, **kwargs):\n    141   if not ops.is_auto_dtype_conversion_enabled():\n--> 142     return op(*args, **kwargs)\n    143   bound_arguments = signature.bind(*args, **kwargs)\n    144   bound_arguments.apply_defaults()\n\nFile ~/anaconda3/envs/cotfuzz/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py:12702, in truncate_div(x, y, name)\n  12700   return _result\n  12701 except _core._NotOkStatusException as e:\n> 12702   _ops.raise_from_not_ok_status(e, name)\n  12703 except _core._FallbackException:\n  12704   pass\n\nFile ~/anaconda3/envs/cotfuzz/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:6006, in raise_from_not_ok_status(e, name)\n   6004 def raise_from_not_ok_status(e, name) -> NoReturn:\n   6005   e.message += (\" name: \" + str(name if name is not None else \"\"))\n...\n  device='CPU'; T in [DT_UINT64]\n  device='CPU'; T in [DT_UINT32]\n  device='CPU'; T in [DT_UINT16]\n  device='CPU'; T in [DT_UINT8]\n [Op:TruncateDiv] name:\n```",
    "comments": [
      {
        "user": "HamiAlyasin54",
        "body": "666"
      },
      {
        "user": "HamiAlyasin54",
        "body": "> 666\n\n"
      }
    ]
  },
  {
    "issue_number": 93679,
    "title": "TensorFlow `tf.scatter_nd` leads to crash via shape mismatch in indices input",
    "author": "panda123dd",
    "state": "closed",
    "created_at": "2025-05-19T22:26:18Z",
    "updated_at": "2025-05-19T22:44:43Z",
    "labels": [
      "type:bug"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.19\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nlinux ubuntu 18.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen passing a crafted indices input with an incorrect shape to tf.scatter_nd, TensorFlow crashes with a CHECK-failure during shape validation. Specifically, it terminates with the following error:\nF tensorflow/core/framework/tensor_shape.cc:359] Check failed: d < dims() (1 vs. 1)\nAborted (core dumped)\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nindices = tf.constant([[4], [3], [1], [7]])\nupdates = tf.constant([9, 10, 11, 12])\nshape = tf.constant([8])\nscatter_nd = tf.scatter_nd(tf.expand_dims(indices, axis=(- 1)), updates, shape)\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F93679\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F93679\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 59869,
    "title": "TypeError: this __dict__ descriptor does not support '_DictWrapper' objects",
    "author": "pmatsibekker",
    "state": "closed",
    "created_at": "2023-03-02T12:33:29Z",
    "updated_at": "2025-05-19T09:23:08Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "comp:keras",
      "TF 1.12"
    ],
    "body": "<details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\nv1.12.1-88869-g80170ee25b4 2.12.0-rc0\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nWIndows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.2\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nI am running the keras training module for mnist from tensorflow_datasets and want to save the model.\r\nthis is failing with error message below. I looked at the file thats triggering data_structures.py and noticed the super classes don't have the __getattribute__ method. I do see __getattr__.\r\nIs this a version difference. My site-packages identified wrapt as wrapt-1.15.0-dist-info. Is there a version incompatability? If so, which version do I use?\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport tensorflow_datasets as tfds\r\nimport keras\r\n\r\n(ds_train, ds_test), ds_info = tfds.load(\r\n    'mnist',\r\n    split=['train', 'test'],\r\n    shuffle_files=True,\r\n    as_supervised=True,\r\n    with_info=True,\r\n)\r\n\r\ndef normalize_img(image, label):\r\n  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\r\n  return tf.cast(image, tf.float32) / 255., label\r\n\r\nds_train = ds_train.map(\r\n    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\r\nds_train = ds_train.cache()\r\nds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\r\nds_train = ds_train.batch(128)\r\nds_train = ds_train.prefetch(tf.data.AUTOTUNE)\r\n\r\nds_test = ds_test.map(\r\n    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\r\nds_test = ds_test.batch(128)\r\nds_test = ds_test.cache()\r\nds_test = ds_test.prefetch(tf.data.AUTOTUNE)\r\n\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dense(10)\r\n])\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.Adam(0.001),\r\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\r\n)\r\n\r\n\r\nmodel.fit(\r\n    ds_train,\r\n    epochs=6,\r\n    validation_data=ds_test,\r\n)\r\n\r\ntf.saved_model.save(\r\n    model,\r\n    'saved/save_files'\r\n)\n```\n\n\n### Relevant log output\n\n```shell\nWARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\pmats\\PycharmProjects\\pythonProject\\minst.py\", line 52, in <module>\r\n    tf.saved_model.save(\r\n  File \"C:\\Users\\pmats\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 1240, in save\r\n    save_and_return_nodes(obj, export_dir, signatures, options)\r\n  File \"C:\\Users\\pmats\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 1276, in save_and_return_nodes\r\n    _build_meta_graph(obj, signatures, options, meta_graph_def))\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\pmats\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 1455, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\pmats\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 1408, in _build_meta_graph_impl        \r\n    saveable_view = _SaveableView(augmented_graph_view, options)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\pmats\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 281, in __init__\r\n    self._initialize_save_and_restore_functions()\r\n  File \"C:\\Users\\pmats\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 303, in _initialize_save_and_restore_functions\r\n    save_util_v1.get_checkpoint_factories_and_keys(self.object_names))\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\pmats\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\checkpoint\\save_util_v1.py\", line 75, in get_checkpoint_factories_and_keys\r\n    saveable_object_util.saveable_objects_from_trackable(\r\n  File \"C:\\Users\\pmats\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 614, in saveable_objects_from_trackable\r\n    if trackable_has_serialize_to_tensor(obj):\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\pmats\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 745, in trackable_has_serialize_to_tensor\r\n    if \"_serialize_to_tensors\" in obj.__dict__:\r\n                                  ^^^^^^^^^^^^\r\n  File \"C:\\Users\\pmats\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\trackable\\data_structures.py\", line 823, in __getattribute__      \r\n    return super().__getattribute__(name)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: this __dict__ descriptor does not support '_DictWrapper' objects\n```\n</details>",
    "comments": [
      {
        "user": "pjpratik",
        "body": "@pmatsibekker Thanks for reporting the issue.\r\n\r\nWe see that you are using 1.x version which is not supported anymore.\r\n\r\nI was able to run the code without any error on TF 2.11. Please find the gist [here](https://colab.research.google.com/gist/pjpratik/912de7b163af0087ae00eea2e15061a1/59869.ipynb) and let us know if it helps.\r\n\r\nThank you."
      },
      {
        "user": "NiharJani2002",
        "body": "@pmatsibekker I guess this might work. Does it help to you, or still it persist.\r\n\r\nIn Python, objects have a special attribute called __dict__, which is a dictionary that stores the object's attributes. However, there are some special objects called _DictWrapper that don't support the __dict__ attribute. If you try to access __dict__ on a _DictWrapper object, you'll get a TypeError.\r\nTo fix this error, you can try to access the __wrapped__ attribute of the _DictWrapper object, which will give you access to the underlying object's dictionary. Or, you can try to avoid using __dict__ on _DictWrapper objects and instead use the methods provided by the wrapper to access and manipulate its attributes.\r\n\r\nCode:\r\nwrapped_obj = my_dict_wrapper.__wrapped__\r\nwrapped_obj_dict = wrapped_obj.__dict__\r\n"
      },
      {
        "user": "treddis",
        "body": "Solved after removing saving model callback. It seems that error consists in model preservation procedure, which handled via transparent dictionary wrapper incorrectly.\r\nAs temporary solution (for those who faced this problem) saving model data can be done through `model.save_weights`"
      }
    ]
  },
  {
    "issue_number": 82208,
    "title": "TPU not support TensorFlow 2.18 and 2.17.1",
    "author": "edwardyehuang",
    "state": "closed",
    "created_at": "2024-12-04T14:56:53Z",
    "updated_at": "2025-05-18T07:48:18Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:tpus",
      "TF 2.18"
    ],
    "body": "### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.18 and tf. 2.17.1\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`import tensorflow as tf` results `segmentation fault core dumped`\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_",
    "comments": [
      {
        "user": "edwardyehuang",
        "body": "lower version (e.g. <= 2.17.0) is fine"
      },
      {
        "user": "edwardyehuang",
        "body": "`tpu-vm-tf-2.18.0-pod-pjrt` and `tpu-vm-tf-2.17.1-pod-pjrt` are not worked"
      },
      {
        "user": "edwardyehuang",
        "body": "I just tested `tpu-vm-tf-2.18.0-pod-se` and `tpu-vm-tf-2.18.0-pod-pjrt-v5p-and-below`, and outputs the same error"
      }
    ]
  },
  {
    "issue_number": 93561,
    "title": "from tensorflow.python._pywrap_tensorflow_internal import * ImportError: DLL load failed while importing _pywrap_tensorflow_internal: 找不到指定的模块",
    "author": "fanronghua0123456",
    "state": "closed",
    "created_at": "2025-05-17T03:40:14Z",
    "updated_at": "2025-05-17T16:15:41Z",
    "labels": [
      "type:bug"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.21\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\ni install tensorflow 2.21 , there is \"ImportError: Traceback (most recent call last):\n  File \"C:\\Users\\charlesfan\\.conda\\envs\\PyCharmMiscProject\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: 找不到指定的模块\"\n\nthis errer happen where i restart  runing \"from tensorflow.keras.preprocessing import image_dataset_from_directory\".\n\n### Standalone code to reproduce the issue\n\n```shell\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nFile ~\\.conda\\envs\\PyCharmMiscProject\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:62\n     61 try:\n---> 62   from tensorflow.python._pywrap_tensorflow_internal import *\n     63 # This try catch logic is because there is no bazel equivalent for py_extension.\n     64 # Externally in opensource we must enable exceptions to load the shared object\n     65 # by exposing the PyInit symbols with pybind. This error will only be\n     66 # caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\n     67 \n     68 # This logic is used in other internal projects using py_extension.\n\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: 找不到指定的模块。\n\nDuring handling of the above exception, another exception occurred:\n\nImportError                               Traceback (most recent call last)\nCell In[2], line 2\n      1 import os\n----> 2 from tensorflow.keras.preprocessing import image_dataset_from_directory\n\nFile ~\\.conda\\envs\\PyCharmMiscProject\\lib\\site-packages\\tensorflow\\__init__.py:37\n     34 import sys as _sys\n     35 import typing as _typing\n---> 37 from tensorflow.python.tools import module_util as _module_util\n     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\n     40 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\n\nFile ~\\.conda\\envs\\PyCharmMiscProject\\lib\\site-packages\\tensorflow\\python\\__init__.py:36\n     27 import traceback\n     29 # We aim to keep this file minimal and ideally remove completely.\n     30 # If you are adding a new file with @tf_export decorators,\n     31 # import it in modules_with_exports.py instead.\n     32 \n     33 # go/tf-wildcard-import\n     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\n---> 36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n     37 from tensorflow.python.eager import context\n     39 # pylint: enable=wildcard-import\n     40 \n     41 # Bring in subpackages.\n\nFile ~\\.conda\\envs\\PyCharmMiscProject\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:77\n     75     sys.setdlopenflags(_default_dlopen_flags)\n     76 except ImportError:\n---> 77   raise ImportError(\n     78       f'{traceback.format_exc()}'\n     79       f'\\n\\nFailed to load the native TensorFlow runtime.\\n'\n     80       f'See https://www.tensorflow.org/install/errors '\n     81       f'for some common causes and solutions.\\n'\n     82       f'If you need help, create an issue '\n     83       f'at https://github.com/tensorflow/tensorflow/issues '\n     84       f'and include the entire stack trace above this error message.')\n\nImportError: Traceback (most recent call last):\n  File \"C:\\Users\\charlesfan\\.conda\\envs\\PyCharmMiscProject\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 62, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: 找不到指定的模块。\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F93561\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F93561\">No</a>\n"
      }
    ]
  },
  {
    "issue_number": 22926,
    "title": "Feature Request: GPUOptions for Go binding",
    "author": "mattn",
    "state": "open",
    "created_at": "2018-10-12T05:02:11Z",
    "updated_at": "2025-05-17T15:40:50Z",
    "labels": [
      "stat:contribution welcome",
      "type:feature",
      "good first issue"
    ],
    "body": "Current implementation of Go binding can not specify options.\r\n\r\nGPUOptions struct is in internal package. And `go generate` doesn't work for protobuf directory. So we can't specify GPUOptions for `NewSession`.\r\n",
    "comments": [
      {
        "user": "tensorflowbutler",
        "body": "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device"
      },
      {
        "user": "frreiss",
        "body": "This problem appears to be broader than just specifying `GPUOptions` for TensorFlow sessions. There is no native Go API for passing *any* options when creating a session. The user must create the binary representation of a `ConfigProto` protocol buffer outside of the TensorFlow Go API.\r\n\r\nSee, for example, the test case `TestSessionConfig` in [session_test.go](https://github.com/tensorflow/tensorflow/blob/7e5b561df1f1dbcba9216e662c345eccb46b3048/tensorflow/go/session_test.go#L252):\r\n```go\r\nfunc TestSessionConfig(t *testing.T) {\r\n\t// Exercise SessionOptions.\r\n\t// Arguably, a better API would be for SessionOptions.Config to be the\r\n\t// type generated by the protocol buffer compiler. But for now, the\r\n\t// tensorflow package continues to be independent of protocol buffers\r\n\t// and this test exercises the option since the implementation has a\r\n\t// nuanced conversion to C types.\r\n\t//\r\n\t// Till then, the []byte form of Config here was generated using a toy\r\n\t// tensorflow Python program:\r\n\t/*\r\n\t import tensorflow\r\n\t c = tensorflow.ConfigProto()\r\n\t c.intra_op_parallelism_threads = 1\r\n\t print c.SerializeToString()\r\n\t*/\r\n\tgraph := NewGraph()\r\n\tc, err := Const(graph, \"Const\", int32(14))\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\topts := SessionOptions{Config: []byte(\"(\\x01\")}\r\n[...]\r\n```\r\n\r\nWould the TensorFlow maintainers accept a non-Google contribution that added the ability to specify session options using pure Go code? This functionality would require generating instances of the `ConfigProto` protocol buffer defined in `config.proto`. I can see two ways to generate these protocol buffers: Either add a build target to generate Go bindings for the files in `tensorflow/core/protobuf`; or add Go wrappers for the generated C++ code in `tensorflow/core/protobuf/config.pb.h`"
      },
      {
        "user": "ymodak",
        "body": "@asimshankar Can you please take a look? Thanks!"
      }
    ]
  },
  {
    "issue_number": 36465,
    "title": "How can I clear GPU memory in tensorflow 2?",
    "author": "HristoBuyukliev",
    "state": "open",
    "created_at": "2020-02-04T15:16:15Z",
    "updated_at": "2025-05-17T07:38:14Z",
    "labels": [
      "stat:awaiting tensorflower",
      "type:bug",
      "comp:gpu",
      "TF 2.7"
    ],
    "body": "\r\n\r\n### System information\r\n- Custom code; nothing exotic though.\r\n- Ubuntu 18.04\r\n- installed from source (with pip)\r\n- tensorflow version v2.1.0-rc2-17-ge5bf8de\r\n- 3.6\r\n- CUDA 10.1\r\n- Tesla V100, 32GB RAM\r\n\r\nI created a model, nothing especially fancy in it. When I create the model, when using nvidia-smi, I can see that tensorflow takes up nearly all of the memory. When I try to fit the model with a small batch size, it successfully runs. When I fit with a larger batch size, it runs out of memory. Nothing unexpected so far. \r\n\r\nHowever, the only way I can then release the GPU memory is to restart my computer. When I run nvidia-smi I can see the memory is still used, but there is no process using a GPU. Also, If I try to run another model, it fails much sooner. \r\n\r\nNothing in the first five pages of google results works. (and most solutions are for TF1)\r\n\r\nIs there any way to release GPU memory in tensorflow 2?",
    "comments": [
      {
        "user": "amahendrakar",
        "body": "@HristoBuyukliev,\r\nCould you please check [this](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) Tensorflow documentation and let us know if it helps. Thanks!"
      },
      {
        "user": "HristoBuyukliev",
        "body": "@amahendrakar Hi, this is not what I am looking for. Not using up all the memory at once sounds like a useful feature, however I am looking to clear the memory tf has already taken. \r\n\r\nI just tried it out, it doesn't help. I am iteratively increasing batch size, trying to find the biggest one I can use. Once the jupyter kernel crashes, the memory stays taken up.\r\n\r\nAdditionally, even the advertised functionality does not work. I made a model that had two times fewer parameters, tensorflow still took up 31 out of 32 gigabytes. \r\n\r\n"
      },
      {
        "user": "taborda11",
        "body": "Hello @HristoBuyukliev, I had a similar problem when I was iterating over model.predict(), if you are iteratively increasing batch size, try after each batch_size training do `tf.keras.backend.clear_session()`.\r\nThat seems to be a case of memory leak in each training."
      }
    ]
  },
  {
    "issue_number": 92523,
    "title": "No way to use tensorflow with cuda on windows. cudaGetDevice() failed.",
    "author": "aayushkeshari",
    "state": "closed",
    "created_at": "2025-05-01T01:21:04Z",
    "updated_at": "2025-05-17T02:10:29Z",
    "labels": [
      "stat:awaiting response",
      "type:bug",
      "stale",
      "comp:gpu",
      "TF 2.7"
    ],
    "body": "### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.7\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nHello all,\n\nI'm trying to use cuda with tensorflow. I installed cuda 10.0 and tensorflow 2.0, my version of VS is 2017. I set my env variables and the problem persists.\n\nThis is the error message: cudaGetDevice() failed. Status: cudaGetErrorString symbol not found.\n\nThanks to all of you!\n\n### Standalone code to reproduce the issue\n\n```shell\nI'm now able to use tensorflow with gpu. I only had to uninstall tensorflow and tensorflow-gpu, next I reinstalled tensorflow-gpu, also I included this line in my code:\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n\nNow I have other issue. I can't confirm what physical device I'm using. It's looks like gpu, even if\nI change CUDA_VISIBLE_DEVICES if I execute the following command I always receive a \"True\" answer\n\ntf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )\n\nIf I use this code i have not answer:\n\nwith tf.device('/CPU:0'): tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None ) \n\nIf I use this other one,\n\ntf.config.experimental.list_physical_devices(device_type=None)\n\nthis is the answer:\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\nPhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n```\n\n### Relevant log output\n\n```shell\n\n```",
    "comments": [
      {
        "user": "imsharukh1994",
        "body": "Fixed the initial cudaGetDevice() error by reinstalling tensorflow-gpu and adding:\n\n`os.environ[\"CUDA_VISIBLE_DEVICES\"]` = `\"0\"`\n\nNow GPU is detected via tf.config.experimental.list_physical_devices(), but I’m not sure if it’s actually being used during execution. Any tips to confirm GPU usage in TF 2.0 with CUDA 10.0 would be appreciated.\n\nThanks!"
      },
      {
        "user": "Venkat6871",
        "body": "Hi @aayushkeshari ,\nThank you for raising this issue. I noticed that you are currently using an older version of TensorFlow (2.7). Please note that this version is no longer actively supported. We recommend updating to the latest stable version of TensorFlow to benefit from improved performance, better GPU support, and ongoing fixes. You can find the official installation guide here in the [documentation](https://www.tensorflow.org/install/source_windows). Upgrading to a supported version should help resolve many compatibility issues, including those related to GPU detection and usage.\nLet us know if you need help with the upgrade process.\nThank you!"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      }
    ]
  },
  {
    "issue_number": 92037,
    "title": "\"Given shapes (...) are not broadcastable\" after conversion onnx->tf->tflite",
    "author": "llh666521",
    "state": "closed",
    "created_at": "2025-04-23T07:08:35Z",
    "updated_at": "2025-05-17T02:10:28Z",
    "labels": [
      "stat:awaiting response",
      "stale",
      "comp:lite",
      "TFLiteConverter"
    ],
    "body": "After converting an ONNX model to TFLite via **ONNX → TensorFlow → TFLite**, I encountered a runtime error during inference:\n\n`RuntimeError: Given shapes, [1,1,16], [] and [4,1,16,144], are not broadcastable.Node number 568 (SELECT_V2) failed to prepare.`\n\nI exported a PyTorch model (Zipformer-style encoder) to ONNX using `torch.onnx.export`, and then:\n1. Converted ONNX → TensorFlow using onnx-tf;\n2. Then used `tf.lite.TFLiteConverter.from_saved_model()` to convert to TFLite.\n\nThe code is shown below:\n```\nimport os\nimport onnx\nfrom onnx_tf.backend import prepare\nimport tensorflow as tf\n\nBASE_PATH = \"onnx_models\"\nKEY = \"encoder-iter-1550000-avg-2-chunk-16-left-128\"\n\nONNX_PATH = os.path.join(BASE_PATH, f\"{KEY}.onnx\")\nTF_PATH = os.path.join(\"export_tflite\", f\"{KEY}.pb\")\nTFLITE_PATH = os.path.join(\"export_tflite\", f\"{KEY}.tflite\")\n\n\"\"\"onnx=>.pb\"\"\"\nonnx_model = onnx.load(ONNX_PATH)  # load onnx model\ntf_rep = prepare(onnx_model)  # creating TensorflowRep object\ntf_rep.export_graph(TF_PATH)\n\n\"\"\".pb=>tflite\"\"\"\nconverter = tf.lite.TFLiteConverter.from_saved_model(TF_PATH)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,\n    tf.lite.OpsSet.SELECT_TF_OPS\n]\ntf_lite_model = converter.convert()\nwith open(TFLITE_PATH, 'wb') as f:\n    f.write(tf_lite_model)\n```\nAlthough the conversion process completed without errors, I encountered the following issue when invoking the resulting TFLite model:\n\n`RuntimeError: Given shapes, [1,1,16], [] and [4,1,16,144], are not broadcastable.Node number 568 (SELECT_V2) failed to prepare.`\n\nIs this a known issue with SELECT_V2 broadcasting during conversion?Any suggestion on how to resolve or workaround this issue?\n\nThanks in advance for your help!",
    "comments": [
      {
        "user": "gaikwadrahul8",
        "body": "Hi, @llh666521\nApologize for the delay in my response, The Conversion Pipeline: `PyTorch -> ONNX -> TensorFlow (SavedModel) -> TFLite` errors can be introduced at any stage and the error message indicates that the shapes of these three input tensors `([1,1,16], [], and [4,1,16,144])` cannot be broadcast together to produce a valid output shape according to TensorFlow/TFLite broadcasting rules [[Ref](https://github.com/tensorflow/tensorflow/blob/e36baa302922ea3c7131b302c2996bd2051ee5c4/tensorflow/lite/kernels/select.cc#L200)]\n\nStart by visualizing the graphs using [Netron](https://netron.app/) to trace the problematic node's inputs back to the original ONNX graph. Then try simplifying the TFLite conversion by disabling optimizations or disallowing TF ops to isolate the cause. Inspecting the intermediate TensorFlow graph can also tell you if the problem existed before TFLite conversion.\n\nUse [onnx2tf](https://pypi.org/project/onnx2tf/) instead of `onnx-tf` because the `onnx2tf` tool is specifically designed to handle conversions from ONNX to TensorFlow and addresses many common issues with operations and tensor layouts\n\nI would recommend you to give try with [AI Edge Torch](https://github.com/google-ai-edge/ai-edge-torch) is a python library that supports converting PyTorch models into a `.tflite` format, which can then be run with TensorFlow Lite and MediaPipe. It does not require intermediate conversion step `ONNX -> TensorFlow (SavedModel)`\n\nThank you for your cooperation and patience.\n\n\n\n\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue is stale because it has been open for 7 days with no activity. It will be closed if no further activity occurs. Thank you."
      },
      {
        "user": "github-actions[bot]",
        "body": "This issue was closed because it has been inactive for 7 days since being marked as stale. Please reopen if you'd like to work on this further."
      }
    ]
  },
  {
    "issue_number": 93535,
    "title": "TF 2.19 not compatible with Python 2.13",
    "author": "JuanVargas",
    "state": "closed",
    "created_at": "2025-05-16T12:32:45Z",
    "updated_at": "2025-05-16T22:52:15Z",
    "labels": [
      "type:build/install"
    ],
    "body": "### Issue type\n\nBuild/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nTF 2.19\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 25.04 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n2.13\n\n### Bazel version\n\n_No response_\n\n### GCC/compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI am opening this issue again because the substance of the question was not answered.\n\nI installed Ubuntu 25.04 LTS on my desktop, which comes with Python 2.13.\n\nWhen I tried to get the binary version of TF 2.19, the install fails because Tf 2.19 is not registered in PyPi for the version of Python 2.13.\n\n**When do you plan to update Tf 2.19 for compatibility with Python 2.13?**\n\nThank you\n\nJuan E. Vargas\n\n### Standalone code to reproduce the issue\n\n```shell\nNone\n```\n\n### Relevant log output\n\n```shell\nNone\n```",
    "comments": [
      {
        "user": "mihaimaruseac",
        "body": "You already opened #93222 which was closed as duplicate of #78774\n\nPlease stop spamming with duplicated issues."
      },
      {
        "user": "google-ml-butler[bot]",
        "body": "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F93535\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F93535\">No</a>\n"
      }
    ]
  }
]