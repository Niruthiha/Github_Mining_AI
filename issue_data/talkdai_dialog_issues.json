[
  {
    "issue_number": 224,
    "title": "Add tools to the default LCEL class",
    "author": "vmesel",
    "state": "open",
    "created_at": "2024-08-20T19:26:45Z",
    "updated_at": "2024-08-20T19:26:46Z",
    "labels": [],
    "body": "In order to definitely migrate to LCEL syntax as the default, we need to find a simple way to encapsulate tools in LCEL, making it super agnostic and extensible.\r\n\r\nA good API would be to have the environment variable/toml setting that holds all of the possible tools for a certain LLM and import it with the `llm.bind_tools()` signature from LangChain.",
    "comments": []
  },
  {
    "issue_number": 223,
    "title": "Tidy session on main.py",
    "author": "vmesel",
    "state": "open",
    "created_at": "2024-08-15T12:49:06Z",
    "updated_at": "2024-08-20T19:22:02Z",
    "labels": [],
    "body": "@ecarrara reported a necessity to tidy the main.py file, removing unnecessary imports, one of those is:\r\n\r\nhttps://github.com/talkdai/dialog/blob/3330c0ebccea0d2632aed809cf72812072ad146f/src/main.py#L33",
    "comments": [
      {
        "user": "Himasnhu-AT",
        "body": "so by tidy, you mean modularize the code, and lint it (remove unnecessary files, so on..."
      },
      {
        "user": "vmesel",
        "body": "Actually, by tidy, I mean just removing the unnecessary import!\r\n\r\nAnd thanks for commenting on the Issue! =)\r\n\r\nOn Sun, 18 Aug 2024 at 02:31 Himanshu ***@***.***> wrote:\r\n\r\n> so by tidy, you mean modularize the code, and lint it (remove unnecessary\r\n> files, so on...\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/talkdai/dialog/issues/223#issuecomment-2295120427>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ABGA2U55VRYZRGQUH2NRRH3ZSAWUDAVCNFSM6AAAAABMSEU3JGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDEOJVGEZDANBSG4>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      },
      {
        "user": "Himasnhu-AT",
        "body": "can you assign it to me?\r\n"
      },
      {
        "user": "vmesel",
        "body": "Sure\r\n\r\nOn Tue, 20 Aug 2024 at 02:36 Himanshu ***@***.***> wrote:\r\n\r\n> can you assign it to me?\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/talkdai/dialog/issues/223#issuecomment-2298004841>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ABGA2U4HKJ4OFJEPCOPQBNDZSLIV5AVCNFSM6AAAAABMSEU3JGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDEOJYGAYDIOBUGE>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      }
    ]
  },
  {
    "issue_number": 222,
    "title": "Change behavior of session creation",
    "author": "vmesel",
    "state": "open",
    "created_at": "2024-07-25T01:23:02Z",
    "updated_at": "2024-07-25T01:23:16Z",
    "labels": [
      "API"
    ],
    "body": "**Is your feature request related to a problem? Please describe.**\r\nNowadays we need to create a session before we even send a message, this process occurs while we do a POST request to the `/session` endpoint.\r\n\r\n**Describe the solution you'd like**\r\nWe should make it possible also to create sessions when a new POST request is made to the `/chat/<session_id>` endpoint",
    "comments": []
  },
  {
    "issue_number": 218,
    "title": "Remove psycopg-binary as dependency",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2024-07-11T15:01:06Z",
    "updated_at": "2024-07-18T02:52:41Z",
    "labels": [],
    "body": "We need to remove the psycopg-binary as dependency from dialog and dialog-lib.\r\n\r\nThis was originally reported by @lgabs on issue #217.",
    "comments": []
  },
  {
    "issue_number": 213,
    "title": "access to the database should avoid sessions to be left open",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-06-28T20:36:07Z",
    "updated_at": "2024-07-18T02:52:41Z",
    "labels": [],
    "body": "Some parts of database access are not controlling the ending of sessions, I think [this one in dialog-lib](https://github.com/talkdai/dialog-lib/blob/d92482eb0743412100f007ef2a9c1b6923b6a85e/dialog_lib/db/memory.py#L67) is an example. If the user of that function `generate_memory_instance`(like [that example](https://github.com/talkdai/dialog-lib/blob/d92482eb0743412100f007ef2a9c1b6923b6a85e/samples/openai/main.py#L12)) does not close the session, It may lead to problems related to the limits of the sql alchemy pool of connections. I think there are other parts of the lib with this problems as well, we'd have to investigate better.\r\n\r\n**Possible solution**\r\nA possible solution could use context management inside functions that access the database to avoid this, removing this session managemente from the user interface (in the open ai example above, the user wouldn't need to instantiate a db session).",
    "comments": [
      {
        "user": "lgabs",
        "body": "Some other parts I've seen the session being used in a way that I think the session could be left open are:\r\n- [the retriever function](https://github.com/talkdai/dialog-lib/blob/d92482eb0743412100f007ef2a9c1b6923b6a85e/dialog_lib/embeddings/generate.py#L32), where, for example, the [default](https://github.com/talkdai/dialog/blob/14e19f619155a710de4c350c0a78adc1942ae9d6/src/dialog/llm/agents/default.py#L22) LLM_CLASS instantiate the session directly\r\n- the retriever as a langchain runnable for LCEL case, [here](https://github.com/talkdai/dialog/blob/14e19f619155a710de4c350c0a78adc1942ae9d6/src/dialog/llm/agents/lcel.py#L56)\r\n"
      },
      {
        "user": "vmesel",
        "body": "Interesting, this can cause several problems on production, but I honestly don't know if we should pass on a session or inject a dependency to solve this issue."
      }
    ]
  },
  {
    "issue_number": 215,
    "title": "Chat history not being passed into default DialogLLM when using Open WebUI",
    "author": "wdec",
    "state": "closed",
    "created_at": "2024-07-08T11:59:20Z",
    "updated_at": "2024-07-17T09:43:10Z",
    "labels": [
      "bug"
    ],
    "body": "**Describe the bug**\r\nWhen using the default dialogLLM, the retrieved chat history is not passed into the actual API call.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Start with default settings\r\n2. Make query via a POST call to /chat/{chat_id} (already visible at this stage)\r\n3. Repeat the POST, asking for the last query\r\n\r\n**Expected behavior**\r\nchat history should be passed\r\n\r\n**Additional context**\r\nThe problem appears to be in the lower dialog-lib package (tried both 0.0.2.0 and 0.0.1.19) that's missing the chat_history, or alternatively the withChatHistory instance isn't used on the dialogLLM\r\n\r\nai_message = llm_class.invoke(\r\n    {\r\n        \"input\": message,\r\n        \"chat_history\": chat_history  # Assuming chat_history is a variable holding the history\r\n    },\r\n",
    "comments": [
      {
        "user": "vmesel",
        "body": "Hey @wdec, thank you for reporting this bug. Have you used the same session id on both requests? How was the request done, was it through Open WebUI or cURL?"
      },
      {
        "user": "wdec",
        "body": "> Hey @wdec, thank you for reporting this bug. Have you used the same session id on both requests? How was the request done, was it through Open WebUI or cURL?\r\n\r\nHi. Yes, used the same session id. The history is correctly updated in the DB and all. BTW The problem does NOT happen when using the lcel agent with the lang chain style. I used the open WebUI."
      },
      {
        "user": "vmesel",
        "body": "Oh! Got it! So whenever you use Open WebUI, there is still some work to do regarding session's memory there.\n\nOpen WebUI use the base for the /ask endpoint.\n\nI'm quite sure we can address this by generating a single session id that's usable in Open WebUI"
      },
      {
        "user": "vmesel",
        "body": "Hey @wdec, thank you for reporting this bug! I was able to work on a fix for it and would love you to test it and see if it works for you. The branch with the fix is `fix/openwebui-session-management`."
      },
      {
        "user": "wdec",
        "body": "Hi. Well, the problem is not in using the openweb ui, which worked fine, but rather that with the /chat/{chat_id} DialogLLM endpoint the history is not sent to the LLM. The branch you indicate still has that issue."
      },
      {
        "user": "vmesel",
        "body": "@wdec Going to verify it! Thanks for more context"
      },
      {
        "user": "vmesel",
        "body": "@wdec some updates: It looks like the code itself was ok, but for some reason, the chat history wasn't being used as a part of the answer.\r\n\r\nI've updated the branch `fix/openwebui-session-management` with the proper history management. I would love you to test it and check if it solves your issue.\r\n\r\nIn the latest commit I've added a line to the toml, so if you are using a custom toml file with your prompt instead of the default provided one, you need to add the `[prompt.history_header]` session.\r\n\r\nHere is the prompt file that I've used:\r\n\r\n```\r\n[model]\r\nmodel_name = \"gpt-4o\"\r\ntemperature = 0.1\r\n\r\n[prompt]\r\nprompt = \"\"\"\r\nAnswer to any question that the user may have\r\n\"\"\"\r\n\r\nheader = \"\"\"\r\nYou are a nice bot, say something nice to the user and try to help him with his question,\r\nbut also say to the user that you don't know totally about the content he asked for.\r\n\"\"\"\r\n\r\nsuggested = \"\"\"\r\nHere are some contents that you need to use in order to help you with the user's question:\r\n\"\"\"\r\n\r\nhistory_header = \"\"\"\r\nThis is the history of the conversation:\r\n\"\"\"\r\n\r\n[fallback]\r\nprompt = \"\"\"\r\nAnswer the following message to the user.\r\n\"\"\"\r\n```\r\n\r\n\r\nTo reproduce the behavior that I've had locally with this branch you will run the following cURLs:\r\n\r\n\r\n```bash\r\n$ curl -X 'POST' \\\r\n  'http://localhost:8000/session' \\\r\n  -H 'accept: application/json' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n  \"chat_id\": \"test_session\"\r\n}'\r\n\r\n# Expected output: {\"chat_id\":\"test_session\"}\r\n\r\ncurl -X 'POST' \\\r\n  'http://localhost:8000/chat/test_session' \\\r\n  -H 'accept: application/json' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n  \"message\": \"My name is vinnie!\"\r\n}'\r\n\r\n# Expected output: {\"message\":\"Hi Vinnie! It's so nice to meet you. How are you doing today? If you have any questions or need help with something, feel free to ask. I'll do my best to assist you, even though I might not know everything about the topic.\"}\r\n\r\ncurl -X 'POST' \\\r\n  'http://localhost:8000/chat/test_session' \\\r\n  -H 'accept: application/json' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n  \"message\": \"What is my name?\"\r\n}'\r\n\r\n# Expected output: {\"message\":\"Your name is Vinnie! How are you doing today? If you have any questions or need assistance with something, I'm here to help.\"}\r\n```"
      },
      {
        "user": "wdec",
        "body": "Thanks"
      }
    ]
  },
  {
    "issue_number": 217,
    "title": "psycopg2-binary is not advised to be used for production environment",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-07-11T14:41:54Z",
    "updated_at": "2024-07-11T14:59:25Z",
    "labels": [],
    "body": "dialog-lib [has a dependency](https://github.com/talkdai/dialog-lib/blob/5b07ec6080b296438a8d001323ffb3130957bebe/pyproject.toml#L16) for psycopg2-binary, which is not advised to be used for production environment as the [official documentation](https://www.psycopg.org/docs/install.html#psycopg-vs-psycopg-binary) warns. Maybe it would be better to keep the library functional without it. For reference, langchain_postgres depends on psycopg3 explicitly.\r\n\r\n",
    "comments": [
      {
        "user": "vmesel",
        "body": "@lgabs I'm closing this issue since it's regarding the library, I'll reopen a new one on the dependency for dialog itself."
      }
    ]
  },
  {
    "issue_number": 176,
    "title": "Protect endpoints and docs",
    "author": "vmesel",
    "state": "open",
    "created_at": "2024-04-09T01:05:53Z",
    "updated_at": "2024-07-11T13:31:26Z",
    "labels": [],
    "body": "We need to add a way to protect endpoints and the documentation, in order to do this, we must be able to still send and list messages given a certain session id, but all of the other endpoints must be used with authentication.\r\n\r\nMaybe we should bring [dialog-api-key](https://github.com/talkdai/dialog-api-key/tree/main) as a key component of the project.",
    "comments": [
      {
        "user": "hbharath327",
        "body": "I am beginner to this project. Can you pls provide more context @vmesel "
      },
      {
        "user": "vmesel",
        "body": "hey @hbharath327, I'm so so sorry that I missed your comment! Thank you for asking for more context.\r\n\r\nRight now, whenever you have a dialog instance running, you can access the docs through `/docs` in the URL, but this is not good for a safety standard. I would like to add a query param or the possibility for the user to change this URL in the environment variables, making it more secret.\r\n\r\nDoes this context help more?"
      }
    ]
  },
  {
    "issue_number": 212,
    "title": "get_session should be available in dialog-lib instead",
    "author": "lgabs",
    "state": "open",
    "created_at": "2024-06-28T14:58:48Z",
    "updated_at": "2024-07-02T13:52:16Z",
    "labels": [
      "brainstorm"
    ],
    "body": "https://github.com/talkdai/dialog/blob/833ae634e33b143d9038b7729f58cddca5f1a0e1/src/dialog/db/__init__.py#L1-L12\r\n\r\nWhen making plugins using dialog, it's useful to leverage from dialog-lib modules, which are available during development since dialog-lib is distributed package. But since dialog itself is not a package, this function `get_session`, for example, can not be inspected or accessed (specially when pytests runs and look at available modules). It's only accessed when we run the application using dialog's image. \r\n\r\nI think I'd be a good idea to move this functionality to dialog-lib to make it available in the distributed package. ",
    "comments": [
      {
        "user": "lgabs",
        "body": "Even though we could copy/move this to the distributed package, I think the best solution would be to keep session management inside each function that access the database, like I've suggested in #213 , in a way that dialog users shouldn't care about this when using ready functions like [`generate_memory_instance`](https://github.com/talkdai/dialog-lib/blob/d92482eb0743412100f007ef2a9c1b6923b6a85e/dialog_lib/db/memory.py#L67-L87). Also, each db access would be close to the session creation in code (with context manager), making it easier to maintain and debug the code as the project grows."
      }
    ]
  },
  {
    "issue_number": 195,
    "title": "Output containing messages that guided to prompt fallbacks",
    "author": "llemonS",
    "state": "open",
    "created_at": "2024-05-31T21:35:08Z",
    "updated_at": "2024-06-29T15:15:18Z",
    "labels": [
      "LLM: ChatGPT",
      "AI",
      "database"
    ],
    "body": "In order to increase the knowledge base for a specific subject over time, would be interesting if we could collect the messages that guided to the prompt fallbacks for later analysis. Maybe initially creating an output folder with a csv file containing those scenareos.",
    "comments": [
      {
        "user": "lgabs",
        "body": "Nice idea! Langsmith is a nice platform to debug each interaction with LLMs, but it's limited to langchain's chain calls, and indeed dialog's fallback implementation occurs _before_ these calls, when no relevant documents are found from the retriever.  Currently, this idea of saving fallbacks would probably occur optionally in the prompt generation step (e.g [generate_prompt method](https://github.com/talkdai/dialog-lib/blob/main/dialog_lib/agents/abstract.py#L116) of `AbstractRAG` class. The fallback cases could be saved in a csv intially, but maybe later it could be saved directly in the postgres database."
      },
      {
        "user": "vmesel",
        "body": "What would the implementation look like in your POV @llemonS ?"
      },
      {
        "user": "llemonS",
        "body": "Well, considering the concept of knowledge base (.csv file) being used as an input and taking a look at the project structure, maybe we could create a path _dialog/data/output_ to store the messages that guided to prompt fallbacks in order to enable a sort of feedback cycle for the own user adapt into the input .csv file later on."
      },
      {
        "user": "lgabs",
        "body": "Recently, I've managed to use fallback in a more LCEL way, making it a runnable component as well, so it's trace goes to langsmith and we gain the information @llemonS talked about \"for free\", with all other langsmith benefits. It may help not only this issue but also making the chain fully LCEL adherent.\r\n\r\nIn the first print screenshot, the rag chain example received a question outside its designed context (see second screenshot), and a _chain_router_ (python function with rules to route, in this case check the number of documents returned from the retriever) guided the chain to a _FallbackChain_, which imposes the fixed AI's fallback message you see in the first screenshot. \r\n\r\n![image](https://github.com/talkdai/dialog/assets/27077298/d95cc8f6-17ca-4c40-a5b1-ce827dd2e5c1)\r\n\r\n![image](https://github.com/talkdai/dialog/assets/27077298/a2b15bd7-5112-4563-9cc7-2691062fa427)\r\n\r\nThis can be achieved with a more complex LCEL chain, which I've adapted from the dialog's plugin I work with to this:\r\n```\r\nfrom typing import Dict, Any\r\nfrom operator import itemgetter\r\n\r\nfrom langchain_core.prompts import (\r\n    ChatPromptTemplate,\r\n)\r\nfrom langchain_openai import ChatOpenAI\r\nfrom langchain_core.output_parsers import StrOutputParser\r\nfrom langchain_core.runnables import Runnable, RunnableLambda, RunnableParallel\r\nfrom langchain_core.messages import AIMessage\r\n\r\n# I've invented these imports based on my plugin, real implementation has to adapt here\r\nfrom dialog_lib.settings import settings\r\nfrom dialog_lib.vectorstore import get_retriever, combine_documents\r\nfrom dialog_lib.models import Input\r\n\r\n\r\n# Prompt\r\nPROMPT_TEMPLATES = settings.PROJECT_CONFIG.get(\"prompt\")\r\nHEADER_PROMPT_TEMPLATE: str = PROMPT_TEMPLATES.get(\"header\")\r\nCONTEXT_PROMPT_TEMPLATE: str = PROMPT_TEMPLATES.get(\"context\")\r\nPROMPT_FALLBACK_MESSAGE: str = PROMPT_TEMPLATES.get(\r\n    \"fallback_not_found_relevant_contents\"\r\n)\r\nfallback_message = ChatPromptTemplate.from_messages(\r\n    [\r\n        (\"ai\", PROMPT_FALLBACK_MESSAGE),\r\n    ]\r\n)\r\nanswer_prompt = ChatPromptTemplate.from_messages(\r\n    [\r\n        (\"system\", HEADER_PROMPT_TEMPLATE),\r\n        (\"system\", CONTEXT_PROMPT_TEMPLATE),\r\n        (\"human\", \"{question}\"),\r\n    ]\r\n)\r\n\r\n# LLM\r\nMODEL_PARAMS = settings.PROJECT_CONFIG.get(\"model\", {})\r\nllm = ChatOpenAI(**MODEL_PARAMS, openai_api_key=settings.OPENAI_API_KEY)\r\n\r\n# Build Chains\r\n\r\n## Fallback Chain\r\ndef parse_fallback(ai_message: AIMessage) -> str:\r\n    return ai_message.content\r\n\r\n\r\nfallback_chain = (\r\n    fallback_message | RunnableLambda(lambda x: x.messages[-1]) | parse_fallback\r\n).with_config({\"run_name\": \"FallbackChain\"})\r\n\r\n## Answer Chain\r\nanswer_chain = (\r\n    (\r\n        RunnableParallel(\r\n            {\r\n                \"context\": itemgetter(\"relevant_docs\")\r\n                | RunnableLambda(combine_documents),\r\n                \"question\": itemgetter(\"question\"),\r\n            }\r\n        ).with_config({\"run_name\": \"QuestionWithContext\"})\r\n        | answer_prompt.with_config({\"run_name\": \"AnswerPrompt\"})\r\n        | llm\r\n        | StrOutputParser()\r\n    )\r\n).with_config({\"run_name\": \"AnswerChain\"})\r\n\r\n\r\ndef chain_router(inputs: Dict[str, Any]) -> Runnable:\r\n    \"\"\"\r\n    Route conversation. If no relevant docs are found, we answer with a fixed fallback template, otherwise go ahead with our LLM Chain.\r\n    \"\"\"\r\n    if len(inputs[\"relevant_docs\"]) == 0:\r\n        return fallback_chain\r\n    else:\r\n        return answer_chain\r\n\r\n\r\n## Retriever Chain\r\nretriever = get_retriever().with_config({\"run_name\": \"Retriever\"})\r\nretriever_chain = itemgetter(\"question\") | retriever\r\n\r\n## Full Chain\r\nfull_chain = (\r\n    (\r\n        RunnableParallel(\r\n            {\"relevant_docs\": retriever_chain, \"question\": lambda x: x[\"question\"]}\r\n        ).with_config({\"run_name\": \"GetQuestionAndRelevantDocs\"})\r\n        | RunnableLambda(chain_router)\r\n    )\r\n    .with_types(input_type=Input)\r\n    .with_config({\"run_name\": \"FullChain\"})\r\n)\r\n```"
      }
    ]
  },
  {
    "issue_number": 198,
    "title": "Support for one model per endpoint approach",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2024-06-02T20:13:56Z",
    "updated_at": "2024-06-20T01:27:58Z",
    "labels": [
      "enhancement",
      "AI",
      "API",
      "EPIC"
    ],
    "body": "Hey, I've been wondering what the next big implementation we need to breed here inside @talkdai. After a quick chat with @avelino, we agreed that we need to start working on allowing users to use multiple models in the same deployment, making it less resource-expensive to have multiple LLMs and prompts.\r\n\r\nThis approach is quite simple, we need to support any LLM class that a user supplies to us through the `.toml` file and allow the user to choose the URL path for that model and also the prompt it should be using.\r\n\r\nA quick draft of this new session in the `.toml` file looks like this:\r\n\r\n```toml\r\n[multimodels]\r\n[[endpoint]]\r\nendpoint = \"/model1\"\r\nclass = \"dialog-lib.models.Model1\"\r\nprompt = \"You are a wonderful bot called Justin.\"\r\n\r\n[[endpoint]]\r\nendpoint = \"/default\"\r\nclass = \"dialog-lib.models.Model2\"\r\nprompt = \"You are a nice bot called Mike.\"\r\n```\r\n\r\nThe modification on dialog would be simple, on the loading of the project, our system should iterate through the toml endpoints and get all of the settings, setting up routers using the settings passed.\r\n",
    "comments": [
      {
        "user": "vmesel",
        "body": "https://stackoverflow.com/questions/76635770/how-to-test-fastapi-application-without-sharing-the-same-application-between-tes"
      },
      {
        "user": "avelino",
        "body": "@vmesel the pr \"only has\" python code, don't you need to update the documentation?\r\n\r\nhow do i configure the models on different endpoints?\r\n\r\ndo i have to read the code to understand ?\r\n\r\n> the basis of open source is communication and simplification of use, we are getting both wrong "
      },
      {
        "user": "vmesel",
        "body": "@avelino you are right! i missed that part"
      }
    ]
  },
  {
    "issue_number": 209,
    "title": "Log Injection",
    "author": "avelino",
    "state": "closed",
    "created_at": "2024-06-13T23:31:04Z",
    "updated_at": "2024-06-18T01:48:03Z",
    "labels": [
      "bug"
    ],
    "body": "since it is possible to inject information into the log (stdout), this is not secure\r\n\r\nhttps://github.com/talkdai/dialog/blob/941ca28f851d4cbd56e875afa88631b89418fb6f/src/dialog/routers/dialog.py#L38-L38\r\n\r\nhttps://github.com/talkdai/dialog/blob/941ca28f851d4cbd56e875afa88631b89418fb6f/src/dialog/routers/openai.py#L47-L47\r\n\r\n## References\r\n\r\nOWASP: [Log Injection](https://owasp.org/www-community/attacks/Log_Injection).\r\nCommon Weakness Enumeration: [CWE-117](https://cwe.mitre.org/data/definitions/117.html).\r\n\r\n**Tracking issue for:**\r\n\r\n- [ ] https://github.com/talkdai/dialog/security/code-scanning/1\r\n- [ ] https://github.com/talkdai/dialog/security/code-scanning/2\r\n",
    "comments": [
      {
        "user": "vmesel",
        "body": "@avelino is this a vulnerability? I've seen it on the scan, but as it's not identifiable, I thought it wasn't worth fixing it sometime."
      },
      {
        "user": "vmesel",
        "body": "Fixed!"
      },
      {
        "user": "avelino",
        "body": "> Fixed!\r\n\r\n@vmesel in which pr or commit?\r\nwe need to be easy to work with, for people to get to the solution in a simple way - the premise of being an open source solution "
      },
      {
        "user": "vmesel",
        "body": "Sure @avelino, you are totally right!\r\n\r\nCommits are:\r\n\r\n[Hotfix: Langchain version bumping for fixing](https://github.com/talkdai/dialog/commit/25cc1ba63ab4706dece46387474992bd95cbcd26) https://github.com/advisories/GHSA-3hjh-jh2h-vrg6 [- bumping dialog-lib to 0.0.1.19](https://github.com/talkdai/dialog/commit/25cc1ba63ab4706dece46387474992bd95cbcd26)\r\n\r\n[Bump urllib3 from 2.2.1 to 2.2.2 in the pip group across 1 directory](https://github.com/talkdai/dialog/commit/66b425092345490e7247e239ec7388a3b49c6c0f)\r\n\r\n[hotfix: Removes log injection vulnerability](https://github.com/talkdai/dialog/commit/6651d1a14d87c101180b21008226928c500600b7)"
      }
    ]
  },
  {
    "issue_number": 206,
    "title": "Let document retrieval be more flexible",
    "author": "lgabs",
    "state": "open",
    "created_at": "2024-06-07T20:25:05Z",
    "updated_at": "2024-06-14T17:08:11Z",
    "labels": [
      "enhancement",
      "embedding"
    ],
    "body": "Currently, the LCEL retriever in dialog-lib forces the document content to join question and content together:\r\n\r\nhttps://github.com/talkdai/dialog-lib/blob/4e8de796be1a21c877eb393066a78235e6a193ac/dialog_lib/embeddings/retrievers.py#L31-L39\r\n\r\nHowever, the user already defines which fields should be embedded in [`load_csv.py`](https://github.com/talkdai/dialog/blob/main/src/load_csv.py)`, so this retriever should keep this choice with a simple return  like \r\n```\r\n        return [\r\n            Document(\r\n                page_content=content.content,\r\n                metadata={\r\n                    \"title\": content.question,\r\n                    \"category\": content.category,\r\n                    \"subcategory\": content.subcategory,\r\n                    \"dataset\": content.dataset,\r\n                    \"link\": content.link,\r\n                },\r\n            )\r\n            for content in relevant_contents\r\n        ]\r\n```\r\n\r\nMoreover, since the default embedding way of langchain's CSVLoader is to already embedd the field name prefixed to the field value, e.g. `category: cat1\\nsubcategory: subcat1\\ncontent: content1` (see this test), it already achieves the same idea that the current implementation does, but in generic way.\r\n\r\nThat proposition works normally with default project chains, while giving flexibility to users that would implement their own prompt design. For example, the project default RAG Chain has this `format_docs`:\r\nhttps://github.com/talkdai/dialog/blob/fbb13af3b3ee70d36b8ece499828ec74ab593f36/src/dialog/llm/agents/lcel.py#L60-L61 \r\n\r\nand users can customize this as they wish to achieve their ideas. Later, when we implement metadata saving to the vectorstore, we could even return other metadata dynamically as well.",
    "comments": [
      {
        "user": "vmesel",
        "body": "@lgabs want to handle this change?"
      },
      {
        "user": "lgabs",
        "body": "Sure, I can do it "
      }
    ]
  },
  {
    "issue_number": 207,
    "title": " TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'",
    "author": "alexfilothodoros",
    "state": "closed",
    "created_at": "2024-06-08T14:52:47Z",
    "updated_at": "2024-06-12T15:43:45Z",
    "labels": [
      "bug"
    ],
    "body": "Hi.\r\nI've been trying to run  docker-compose up for the first time.\r\nThe process is stuck when a TypeError appears.\r\n\r\nAny ideas why this is happening?\r\n\r\n\r\n`[+] Running 2/0\r\n ✔ Container dialog-db-1      Created                                                                                                                                                                            0.0s\r\n ✔ Container dialog-dialog-1  Created                                                                                                                                                                            0.0s\r\nAttaching to db-1, dialog-1\r\ndb-1      |\r\ndb-1      | PostgreSQL Database directory appears to contain a database; Skipping initialization\r\ndb-1      |\r\ndb-1      | 2024-06-08 14:48:16.139 UTC [1] LOG:  starting PostgreSQL 15.7 (Debian 15.7-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit\r\ndb-1      | 2024-06-08 14:48:16.139 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\r\ndb-1      | 2024-06-08 14:48:16.139 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\r\ndb-1      | 2024-06-08 14:48:16.140 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\r\ndb-1      | 2024-06-08 14:48:16.142 UTC [29] LOG:  database system was shut down at 2024-06-08 14:48:14 UTC\r\ndb-1      | 2024-06-08 14:48:16.145 UTC [1] LOG:  database system is ready to accept connections\r\ndialog-1  | Traceback (most recent call last):\r\ndialog-1  |   File \"/usr/local/bin/alembic\", line 8, in <module>\r\ndialog-1  |     sys.exit(main())\r\ndialog-1  |              ^^^^^^\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/alembic/config.py\", line 641, in main\r\ndialog-1  |     CommandLine(prog=prog).main(argv=argv)\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/alembic/config.py\", line 631, in main\r\ndialog-1  |     self.run_cmd(cfg, options)\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/alembic/config.py\", line 608, in run_cmd\r\ndialog-1  |     fn(\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/alembic/command.py\", line 403, in upgrade\r\ndialog-1  |     script.run_env()\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/alembic/script/base.py\", line 583, in run_env\r\ndialog-1  |     util.load_python_file(self.dir, \"env.py\")\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/alembic/util/pyfiles.py\", line 95, in load_python_file\r\ndialog-1  |     module = load_module_py(module_id, path)\r\ndialog-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/alembic/util/pyfiles.py\", line 113, in load_module_py\r\ndialog-1  |     spec.loader.exec_module(module)  # type: ignore\r\ndialog-1  |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  |   File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\r\ndialog-1  |   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\r\ndialog-1  |   File \"/app/src/dialog/migrations/env.py\", line 7, in <module>\r\ndialog-1  |     from dialog_lib.db.models import Base\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/dialog_lib/db/__init__.py\", line 2, in <module>\r\ndialog-1  |     from .memory import (\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/dialog_lib/db/memory.py\", line 3, in <module>\r\ndialog-1  |     from langchain_postgres import PostgresChatMessageHistory\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_postgres/__init__.py\", line 9, in <module>\r\ndialog-1  |     from langchain_postgres.vectorstores import PGVector\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_postgres/vectorstores.py\", line 29, in <module>\r\ndialog-1  |     from langchain_core.documents import Document\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/documents/__init__.py\", line 6, in <module>\r\ndialog-1  |     from langchain_core.documents.compressor import BaseDocumentCompressor\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/documents/compressor.py\", line 6, in <module>\r\ndialog-1  |     from langchain_core.callbacks import Callbacks\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/callbacks/__init__.py\", line 22, in <module>\r\ndialog-1  |     from langchain_core.callbacks.manager import (\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/callbacks/manager.py\", line 29, in <module>\r\ndialog-1  |     from langsmith.run_helpers import get_run_tree_context\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 40, in <module>\r\ndialog-1  |     from langsmith import client as ls_client\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langsmith/client.py\", line 52, in <module>\r\ndialog-1  |     from langsmith import env as ls_env\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langsmith/env/__init__.py\", line 3, in <module>\r\ndialog-1  |     from langsmith.env._runtime_env import (\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langsmith/env/_runtime_env.py\", line 10, in <module>\r\ndialog-1  |     from langsmith.utils import get_docker_compose_command\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langsmith/utils.py\", line 31, in <module>\r\ndialog-1  |     from langsmith import schemas as ls_schemas\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langsmith/schemas.py\", line 69, in <module>\r\ndialog-1  |     class Example(ExampleBase):\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/main.py\", line 286, in __new__\r\ndialog-1  |     cls.__try_update_forward_refs__()\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/main.py\", line 807, in __try_update_forward_refs__\r\ndialog-1  |     update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (NameError,))\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 554, in update_model_forward_refs\r\ndialog-1  |     update_field_forward_refs(f, globalns=globalns, localns=localns)\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 520, in update_field_forward_refs\r\ndialog-1  |     field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)\r\ndialog-1  |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 66, in evaluate_forwardref\r\ndialog-1  |     return cast(Any, type_)._evaluate(globalns, localns, set())\r\ndialog-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  | TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'\r\ndialog-1  | Traceback (most recent call last):\r\ndialog-1  |   File \"/app/src/load_csv.py\", line 6, in <module>\r\ndialog-1  |     from langchain_community.document_loaders.csv_loader import CSVLoader\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_community/document_loaders/csv_loader.py\", line 6, in <module>\r\ndialog-1  |     from langchain_core.documents import Document\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/documents/__init__.py\", line 6, in <module>\r\ndialog-1  |     from langchain_core.documents.compressor import BaseDocumentCompressor\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/documents/compressor.py\", line 6, in <module>\r\ndialog-1  |     from langchain_core.callbacks import Callbacks\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/callbacks/__init__.py\", line 22, in <module>\r\ndialog-1  |     from langchain_core.callbacks.manager import (\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/callbacks/manager.py\", line 29, in <module>\r\ndialog-1  |     from langsmith.run_helpers import get_run_tree_context\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 40, in <module>\r\ndialog-1  |     from langsmith import client as ls_client\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langsmith/client.py\", line 52, in <module>\r\ndialog-1  |     from langsmith import env as ls_env\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langsmith/env/__init__.py\", line 3, in <module>\r\ndialog-1  |     from langsmith.env._runtime_env import (\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langsmith/env/_runtime_env.py\", line 10, in <module>\r\ndialog-1  |     from langsmith.utils import get_docker_compose_command\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langsmith/utils.py\", line 31, in <module>\r\ndialog-1  |     from langsmith import schemas as ls_schemas\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langsmith/schemas.py\", line 69, in <module>\r\ndialog-1  |     class Example(ExampleBase):\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/main.py\", line 286, in __new__\r\ndialog-1  |     cls.__try_update_forward_refs__()\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/main.py\", line 807, in __try_update_forward_refs__\r\ndialog-1  |     update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (NameError,))\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 554, in update_model_forward_refs\r\ndialog-1  |     update_field_forward_refs(f, globalns=globalns, localns=localns)\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 520, in update_field_forward_refs\r\ndialog-1  |     field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)\r\ndialog-1  |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 66, in evaluate_forwardref\r\ndialog-1  |     return cast(Any, type_)._evaluate(globalns, localns, set())\r\ndialog-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  | TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'\r\ndialog-1  | INFO:     Will watch for changes in these directories: ['/app/src']\r\ndialog-1  | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\ndialog-1  | INFO:     Started reloader process [1] using WatchFiles\r\ndialog-1  | Process SpawnProcess-1:\r\ndialog-1  | Traceback (most recent call last):\r\ndialog-1  |   File \"/usr/local/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\r\ndialog-1  |     self.run()\r\ndialog-1  |   File \"/usr/local/lib/python3.12/multiprocessing/process.py\", line 108, in run\r\ndialog-1  |     self._target(*self._args, **self._kwargs)\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/uvicorn/_subprocess.py\", line 76, in subprocess_started\r\ndialog-1  |     target(sockets=sockets)\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/uvicorn/server.py\", line 61, in run\r\ndialog-1  |     return asyncio.run(self.serve(sockets=sockets))\r\ndialog-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  |   File \"/usr/local/lib/python3.12/asyncio/runners.py\", line 194, in run\r\ndialog-1  |     return runner.run(main)\r\ndialog-1  |            ^^^^^^^^^^^^^^^^\r\ndialog-1  |   File \"/usr/local/lib/python3.12/asyncio/runners.py\", line 118, in run\r\ndialog-1  |     return self._loop.run_until_complete(task)\r\ndialog-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/uvicorn/server.py\", line 68, in serve\r\ndialog-1  |     config.load()\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/uvicorn/config.py\", line 467, in load\r\ndialog-1  |     self.loaded_app = import_from_string(self.app)\r\ndialog-1  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/uvicorn/importer.py\", line 21, in import_from_string\r\ndialog-1  |     module = importlib.import_module(module_str)\r\ndialog-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  |   File \"/usr/local/lib/python3.12/importlib/__init__.py\", line 90, in import_module\r\ndialog-1  |     return _bootstrap._gcd_import(name[level:], package, level)\r\ndialog-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  |   File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\r\ndialog-1  |   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\r\ndialog-1  |   File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\r\ndialog-1  |   File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\r\ndialog-1  |   File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\r\ndialog-1  |   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\r\ndialog-1  |   File \"/app/src/main.py\", line 10, in <module>\r\ndialog-1  |     from dialog.routers import api_router, open_ai_api_router\r\ndialog-1  |   File \"/app/src/dialog/routers/__init__.py\", line 1, in <module>\r\ndialog-1  |     from .dialog import *\r\ndialog-1  |   File \"/app/src/dialog/routers/dialog.py\", line 9, in <module>\r\ndialog-1  |     from dialog.llm import process_user_message, add_langserve_routes\r\ndialog-1  |   File \"/app/src/dialog/llm/__init__.py\", line 6, in <module>\r\ndialog-1  |     from dialog_lib.agents.abstract import AbstractLLM\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/dialog_lib/agents/__init__.py\", line 1, in <module>\r\ndialog-1  |     from .abstract import *\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/dialog_lib/agents/abstract.py\", line 1, in <module>\r\ndialog-1  |     from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain/prompts/__init__.py\", line 32, in <module>\r\ndialog-1  |     from langchain_core.example_selectors import (\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/example_selectors/__init__.py\", line 6, in <module>\r\ndialog-1  |     from langchain_core.example_selectors.length_based import (\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/example_selectors/length_based.py\", line 6, in <module>\r\ndialog-1  |     from langchain_core.prompts.prompt import PromptTemplate\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/prompts/__init__.py\", line 27, in <module>\r\ndialog-1  |     from langchain_core.prompts.base import (\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/prompts/base.py\", line 22, in <module>\r\ndialog-1  |     from langchain_core.output_parsers.base import BaseOutputParser\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/output_parsers/__init__.py\", line 15, in <module>\r\ndialog-1  |     from langchain_core.output_parsers.base import (\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/output_parsers/base.py\", line 18, in <module>\r\ndialog-1  |     from langchain_core.language_models import LanguageModelOutput\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/language_models/__init__.py\", line 25, in <module>\r\ndialog-1  |     from langchain_core.language_models.base import (\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/langchain_core/language_models/base.py\", line 75, in <module>\r\ndialog-1  |     class BaseLanguageModel(\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/main.py\", line 286, in __new__\r\ndialog-1  |     cls.__try_update_forward_refs__()\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/main.py\", line 807, in __try_update_forward_refs__\r\ndialog-1  |     update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (NameError,))\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 554, in update_model_forward_refs\r\ndialog-1  |     update_field_forward_refs(f, globalns=globalns, localns=localns)\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 520, in update_field_forward_refs\r\ndialog-1  |     field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)\r\ndialog-1  |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  |   File \"/usr/local/lib/python3.12/site-packages/pydantic/v1/typing.py\", line 66, in evaluate_forwardref\r\ndialog-1  |     return cast(Any, type_)._evaluate(globalns, localns, set())\r\ndialog-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  | TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'`",
    "comments": [
      {
        "user": "vmesel",
        "body": "@alexfilothodoros just updated the docker image, should be good to go! Let me know if the problem pops up again!\r\n\r\nThe error was reported here: https://stackoverflow.com/questions/78593700/langchain-community-langchain-packages-giving-error-missing-1-required-keywor"
      },
      {
        "user": "alexfilothodoros",
        "body": "Hi @vmesel \r\nThanks for that but I still get the error."
      },
      {
        "user": "vmesel",
        "body": "@alexfilothodoros can you force the rebuild of the docker image with `docker-compose up --build` command?"
      },
      {
        "user": "alexfilothodoros",
        "body": "@vmesel that did the trick! Thanks :)"
      }
    ]
  },
  {
    "issue_number": 208,
    "title": "[Documentation] image preview not available",
    "author": "alexfilothodoros",
    "state": "closed",
    "created_at": "2024-06-08T14:56:06Z",
    "updated_at": "2024-06-11T19:57:55Z",
    "labels": [],
    "body": "Here\r\n\r\nhttps://dev.to/vmesel/deploy-your-own-chatgpt-in-5-minutes-5d41\r\n\r\nThe image preview after the title \"PGVector - Our first service\"  is not displayed\r\n<img width=\"802\" alt=\"Screenshot 2024-06-08 at 16 55 20\" src=\"https://github.com/talkdai/dialog/assets/6419847/14fbf481-292c-4052-a01a-c99ada33a71c\">\r\n\r\n",
    "comments": [
      {
        "user": "vmesel",
        "body": "@alexfilothodoros strange, here it is up and running... Can you send me your browser (which browser you are using, the version and any relevant extensions)?\r\n\r\nHere it shows normally:\r\n\r\n<img width=\"1141\" alt=\"image\" src=\"https://github.com/talkdai/dialog/assets/4984147/f59750a9-346a-4c5c-a3ca-02313de61870\">\r\n"
      },
      {
        "user": "alexfilothodoros",
        "body": "Actually it's OK now!\r\n\r\nFor the record I am using Safari Version 17.5 (19618.2.12.11.6)."
      }
    ]
  },
  {
    "issue_number": 155,
    "title": "Increase test coverage",
    "author": "vmesel",
    "state": "open",
    "created_at": "2024-03-13T23:05:09Z",
    "updated_at": "2024-06-07T06:00:18Z",
    "labels": [],
    "body": "We need to add testing for LCEL, Abstract and Default LLMs and the API parts of the code.",
    "comments": [
      {
        "user": "vmesel",
        "body": "Right now we have the following coverage:\r\n```coverage\r\n---------- coverage: platform linux, python 3.11.8-final-0 -----------\r\nName                            Stmts   Miss Branch BrPart  Cover   Missing\r\n---------------------------------------------------------------------------\r\ndialog/learn/helpers.py             1      0      0      0   100%\r\ndialog/learn/idf.py                11      6      2      0    38%   14-19\r\ndialog/llm/__init__.py             20     13      4      0    29%   12-29\r\ndialog/llm/abstract_llm.py         38      9     10      1    75%   24, 73, 80-89\r\ndialog/llm/default.py              48     29     16      0    36%   23-28, 31-59, 63-84, 87-89\r\ndialog/llm/embeddings.py           18      8      2      0    50%   16, 23, 27-41\r\ndialog/llm/lcel_default.py         52     52     16      0     0%   1-110\r\ndialog/llm/memory.py               32     11      4      0    58%   34-37, 41-47, 67-71\r\ndialog/models/__init__.py          27      6      0      0    78%   18-19, 27-28, 40-41\r\ndialog/models/db.py                 8      0      0      0   100%\r\ndialog/models/helpers.py           12      0      4      2    88%   8->11, 12->17\r\ndialog/schemas/__init__.py          1      0      0      0   100%\r\ndialog/schemas/api_schemas.py       5      0      0      0   100%\r\ndialog/settings.py                 23      0      2      0   100%\r\nload_csv.py                        41     41     16      0     0%   1-72\r\nmain.py                            72     14     20      4    80%   62-63, 76, 112, 127, 133-142\r\n---------------------------------------------------------------------------\r\nTOTAL                             409    189     96      7    50%\r\n4 empty files skipped.\r\n============================== 7 passed in 4.68s ===============================\r\n```"
      },
      {
        "user": "vmesel",
        "body": "#171 made a improvement on 9%, totaling our coverage to 59% if approved"
      },
      {
        "user": "alexfilothodoros",
        "body": "Hi @vmesel \r\nAre you still intested in this issue? I was thinking that I could help with that."
      },
      {
        "user": "vmesel",
        "body": "Hey @alexfilothodoros, yes! We are interested!"
      },
      {
        "user": "alexfilothodoros",
        "body": "Awesome! I will try to set up everything during the weekend and start working on it :)"
      }
    ]
  },
  {
    "issue_number": 72,
    "title": "Adds OpenAI API support",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-12-07T12:05:57Z",
    "updated_at": "2024-06-06T14:06:54Z",
    "labels": [
      "API: WebHooks"
    ],
    "body": "Basically we need to add support to the OpenAI request and response, so we emulate their API.",
    "comments": []
  },
  {
    "issue_number": 149,
    "title": "PendingRollbackError",
    "author": "avelino",
    "state": "closed",
    "created_at": "2024-03-11T20:38:01Z",
    "updated_at": "2024-06-06T14:06:44Z",
    "labels": [
      "bug"
    ],
    "body": "Can't reconnect until invalid transaction is rolled back. Please `rollback()` fully before proceeding (Background on this error at: https://sqlalche.me/e/20/8s2b)\r\n\r\n\r\n![Screenshot 2024-03-11 at 17 37 46](https://github.com/talkdai/dialog/assets/31996/4f500524-2d27-4ca9-a334-6a0c2f1b4a33)\r\n",
    "comments": []
  },
  {
    "issue_number": 154,
    "title": "Add support for multiple LLMs",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2024-03-13T23:04:38Z",
    "updated_at": "2024-06-06T14:06:30Z",
    "labels": [],
    "body": "Adding support for multiple LLMs would make it easier to build agents that require more abstraction or more inputs from different models. We could implement this using the same behavior as Django uses for settings databases, which is basically a dictionary of databases.\r\n\r\nWe need to think on how to implement this.\r\n\r\n```\r\nDATABASES = {\r\n    'default': {\r\n        'ENGINE': 'django.db.backends.sqlite3',\r\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\r\n    }\r\n}\r\n```\r\n\r\nOne way we could make this work is by using the `get_llm_class(llm_name)` passing the LLM name. This could be loaded inside the toml or another config method.",
    "comments": [
      {
        "user": "avelino",
        "body": "@vmesel duplicate?\r\nhttps://github.com/talkdai/dialog/issues/194"
      },
      {
        "user": "vmesel",
        "body": "#198 is also a duplicate, but will keep it there as it has more context on the approach done right now."
      }
    ]
  },
  {
    "issue_number": 166,
    "title": "Rename LLM module to Agent module",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2024-03-19T13:43:29Z",
    "updated_at": "2024-06-06T14:05:40Z",
    "labels": [],
    "body": "What we are currently implementing in the files that inherit from AbstractLLM (and the original itself) is an agent, which consists of a chain or more. Inside a chain, we have a LLM (which can be a LCEL or regular LLM), a prompt and some pre/post processing.\r\n\r\nWhat you guys think?",
    "comments": []
  },
  {
    "issue_number": 170,
    "title": "Add support to LangServe with FastAPI",
    "author": "avelino",
    "state": "closed",
    "created_at": "2024-04-04T09:48:11Z",
    "updated_at": "2024-06-06T14:05:21Z",
    "labels": [
      "enhancement",
      "API",
      "devx"
    ],
    "body": "https://python.langchain.com/docs/langserve\r\n\r\n> This library is integrated with [FastAPI](https://fastapi.tiangolo.com/) and uses [pydantic](https://docs.pydantic.dev/latest/) for data validation.\r\n\r\ntalkd/dialog uses LangChain and we probably won't switch frameworks. Given this statement, it's important to follow the evolution of LangChain and the projects they maintain to keep up to date.\r\n\r\n**warning:** I know that LangChain is a new project and will have updates that break our implementations. That's fine, it's part of our choice to use LangChain - we'd have this kind of problem using another framework.\r\n\r\n## proposal\r\nWith the above context in mind, it makes sense to swap the FastAPI us for the LangServer abstraction (which uses FastAPI underneath).\r\n\r\n## premise\r\n\r\nwe can't break the signatures of the current endpoints\r\nwe can have other endpoints and warn those who are using the old one to switch to the new one, but at this point we can't break what works",
    "comments": [
      {
        "user": "lgabs",
        "body": "I can take this one 🤘 I'm already testing langserve in the RAG context and I think it's a good idea. "
      },
      {
        "user": "vmesel",
        "body": "Awesome @lgabs, thanks for helping with that!\r\n\r\nLet's make it as our second stable release of the project so we have the benefits of the langserver as well. 🚀 "
      },
      {
        "user": "vmesel",
        "body": "@avelino and @lgabs do you guys know if we can keep our current source code and just swap the server library? Would this be a good starting point to move this needle to this new version?"
      },
      {
        "user": "lgabs",
        "body": "@vmesel I thought about the implementation and checked some examples and as far as I know it should be possible to follow the proposal while keeping the premise (keeping the current source code for what already works), exposing new endpoints without breaking current ones. I'll focus on that in following days to bring some ideas here 🦜 "
      },
      {
        "user": "vmesel",
        "body": "Awesome @lgabs, thanks for the feedback! I'm looking forward to this. Let's do this implementation in small chunks so we are able to make it less impactful through version changes."
      }
    ]
  },
  {
    "issue_number": 199,
    "title": "Improve load_csv.py to use langchain's CSVLoader",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-06-02T22:40:47Z",
    "updated_at": "2024-06-06T01:16:49Z",
    "labels": [
      "enhancement",
      "devx",
      "EPIC",
      "dependencies",
      "database"
    ],
    "body": "Currently, [load_csv](https://github.com/talkdai/dialog/blob/main/src/load_csv.py) make the document embeddings using pandas to load the csv and making some preprocessing to assert necessary columns, builds a primary key with category/subcategory/question to compare with already present keys and keep only new ones, and then insert embeddings in the vectordb. \r\n\r\nHowever, langchain's already has many document loaders, including CSVLoader, which does not even use pandas, and maybe we could simplify our code using it without the pandas dependency here. The usage of a langchain's document loader goes in the direction of making a more flexible `load_csv` with langchain's components and also for a more generic approach for the other csv fields not embedded, which in langchain's terms are document _metadata_, and in its vectorstores integrations these metadata are stores typically as a jsonb field. In the future, we could use these metadata for better retrieval (e.g. filter some metadata when making the vectordb query) or custom context prompt creation (currently examples in [lcel](https://github.com/talkdai/dialog/blob/main/src/dialog/llm/agents/lcel.py#L60) and [default](https://github.com/talkdai/dialog/blob/main/src/dialog/llm/agents/default.py#L53) rags) in a more flexible way, allowing user specific metadata. For example, in a RAG about movies, metadata could be `director`, `actors`, `genre`. \r\n\r\nA quickstart to see the usage of CSVLoader is available in this [dialog fork](https://github.com/lgabs/dialog/blob/main/src/scripts/make_embeddings.py) file, which makes the same as `load_csv.py`, but not full compatible with dialog main due to the different vectordb approach. For the current state of dialog, the actual implementation would start with something like the following, but taking care of keeping current compatibility and maybe adding the metadata jsonb new field (for reference, here is the [langchain_postgres](https://github.com/langchain-ai/langchain-postgres/blob/main/langchain_postgres/vectorstores.py#L138)'s schema):\r\n\r\n```python\r\nimport argparse\r\nfrom typing import List\r\n\r\nimport hashlib\r\n\r\nimport csv\r\nfrom sqlalchemy import text\r\n\r\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\r\nfrom langchain_core.documents import Document\r\n\r\nfrom dialog_lib.embeddings.generate import generate_embeddings\r\nfrom dialog.llm.embeddings import EMBEDDINGS_LLM\r\nfrom dialog_lib.db.models import CompanyContent\r\nfrom dialog.db import get_session\r\nfrom dialog.settings import Settings\r\n\r\nimport logging\r\n\r\nlogging.basicConfig(\r\n    level=Settings().LOGGING_LEVEL,\r\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\r\n)\r\n\r\nlogger = logging.getLogger(\"make_embeddings\")\r\n\r\nsession = next(get_session())\r\nNECESSARY_COLS = [\"category\", \"subcategory\", \"question\", \"content\"]\r\n\r\ndef _get_csv_cols(path: str):\r\n    with open(path) as f:\r\n        reader = csv.DictReader(f)\r\n        return reader.fieldnames\r\n\r\ndef load_csv_and_generate_embeddings(path, cleardb=False, embed_columns=(\"content\",)):\r\n    \r\n    metadata_columns = [\r\n        col for col in _get_csv_cols(path) if col not in embed_columns\r\n    ]\r\n    \r\n    loader = CSVLoader(path, metadata_columns=metadata_columns)\r\n    docs: List[Document] = loader.load()\r\n\r\n    logger.info(\"Metadata columns: %s\", metadata_columns)\r\n    logger.info(\"Embedding columns: %s\", embed_columns)\r\n    logger.info(\"Glimpse over the first doc: %s\", docs[0].page_content[:100])\r\n    \r\n    for col in NECESSARY_COLS:\r\n        if col not in metadata_columns + embed_columns:\r\n            raise Exception(f\"Column {col} not found in {path}\")\r\n\r\n    # Create primary key column using category, subcategory, and question\r\n    def add_document_pk(doc: Document) -> Document:\r\n        pk = doc.metadata[\"category\"] + doc.metadata[\"subcategory\"] + doc.metadata[\"question\"]\r\n        doc.metadata[\"primary_key\"] = hashlib.md5(pk.encode()).hexdigest()\r\n        return doc\r\n    docs: List[Document] = [add_document_pk(doc) for doc in docs]\r\n\r\n    if cleardb:\r\n        session.query(CompanyContent).delete()\r\n        session.commit()\r\n\r\n    # TODO: read from the vectorstore without pandas and return as List[Document]\r\n    docs_in_db: List[Document] = ...\r\n    existing_pks: List[str] =  [add_document_pk(doc).metadata[\"primary_key\"] for doc in docs_in_db]\r\n\r\n    # Filter df for keys present in df and not present in df_in_db\r\n    docs_filtered: List[Document] = [doc for doc in docs if doc.metadata[\"primary_key\"] not in existing_pks]\r\n    if len(docs_filtered) == 0:\r\n        return\r\n    for doc in docs_filtered:\r\n        doc.pop('primary_key', None)\r\n\r\n    print(\"Generating embeddings for new questions...\")\r\n    print(\"New questions:\", len(docs_filtered))\r\n    print(\"embed_columns: \", embed_columns)\r\n\r\n    embedded_docs = generate_embeddings([doc.page_content for doc in docs_filtered], embedding_llm_instance=EMBEDDINGS_LLM)\r\n    # TODO: write new embedding from the vectorstore with sqlalchemy\r\n    for doc, embedding in zip(docs_filtered, embedded_docs):\r\n        ...\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--path\", type=str, required=False, default=\"./know.csv\")\r\n    parser.add_argument(\"--cleardb\", action=\"store_true\")\r\n    parser.add_argument(\"--embed-columns\", default=\"content\")\r\n    args = parser.parse_args()\r\n\r\n    load_csv_and_generate_embeddings(\r\n        args.path, args.cleardb, args.embed_columns.split(','))\r\n       \r\n```",
    "comments": [
      {
        "user": "vmesel",
        "body": "Thanks for making this discussion public @lgabs, we really need to move forward towards langchain's approach on document retrieval.\n\nMy only question is what should be the first change to start and make our format comply to langchain's."
      },
      {
        "user": "lgabs",
        "body": "The smallest change I see is like the snippet above, where pandas is not used, the code uses CSVLoader and terminology of metadata, but postgres models could still be the same. The \"...\" I've left would be the actual interface from Langchain Document object and objects read from db using sqlachemy (\"read from db ->list of Documents\" and \"list of Documents\" -> write in db\" operations)"
      },
      {
        "user": "lgabs",
        "body": "This  #201 PR shows the idea, I've manager to make the two operations I've said above and it's now working for some local tests I've done (need to test more). Take a look @vmesel."
      }
    ]
  },
  {
    "issue_number": 47,
    "title": "slow request for gpt when prompt is large",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-23T02:47:06Z",
    "updated_at": "2024-06-02T22:36:38Z",
    "labels": [
      "LLM: ChatGPT",
      "brainstorm"
    ],
    "body": "when the prompt is too large, the request to the GPT is too slow\r\n\r\nexample of what I'm calling a large prompt (2674 characters):\r\n\r\n```\r\nYou are a virtual assistant who answers any question about traveling with XXX. \r\nYou receive a question and a set of information related to XXX.\r\nIf the user's question requires you to provide specific information from the information provided, give your answer based only on the examples below (Documents section). Do NOT generate an answer that is NOT written in the examples (documents) provided.\r\nIf you do not find the answer to the user's question with the examples provided below, reply that you did not find the answer in the information and propose that they reformulate their query with more details or contact human support. In these cases, inject the text '__ANSWER_NOT_FOUND__' at the end of your answer, to indicate to the backend that this is a case in which you were unable to answer.\r\nIf you notice that the question deals with critical cases during the trip, such as accidents, mechanical failure of the bus (when the bus \"breaks down\"), harassment, fights or road checks, regardless of whether you were able to answer or not, inject the term '__EMERGENCY__' at the end of the text for our backend to capture this case.\r\nUse bullet points if you need to make a list, only if necessary.\r\n\r\nRecalling the business model: XXX connects travelers with executive charter companies ('XXX Passage' sales model or charter model) to sell trips. \r\nWe also have a second business model where we resell tickets from bus companies (Ticket Resale model), so be aware of the different rules when this is the case.\r\nTherefore, whenever the answer has different rules for these two business models, explain both rules clearly, preferably using bullet points. In the charter model, \r\nsome partners accept that we 'plot' the bus with XXX's branding (in pink with our logo), but other partners use buses with their own existing branding.\r\n\r\nUse this XXX manifesto as inspiration for Buser's role in society:\r\n'We believe that life is better when we have choices. \r\nIn XXX, the vast majority of cities are connected by just one bus company, and our airports and railways are insufficient. As a result, we are forced to pay dearly for the lack of choice. Many Brazilians take their chances with clandestine transportation because they can't afford traditional options.\r\nXXX is a safe and modern alternative. We connect people who want to travel to the same destination with executive charter companies. Our shared and sustainable technology promotes mobility in Brazil, creating a new option for safe, quality transportation at fair prices.\r\nAs has already happened with private app-based transportation in cities, it's time for intercity transportation to change for the better. And for good.\r\n```",
    "comments": [
      {
        "user": "vmesel",
        "body": "@avelino maybe, in this prompt, we would need to make it a multi-agent prompt. What I mean is basically we have a parent agent where we can get which one of the major intents the user is trying to talk about, and then, whenever this intent is recognized, we send to the right agent with a prompt that is smaller and with a more concise text."
      },
      {
        "user": "lgabs",
        "body": "another ideia is that the prompt engineering is probably the best way to tackle the problem, and that includes not only changing the prompt but also adapting the knowledge base, avoiding very big documents. Langchain already [has a module](https://python.langchain.com/docs/modules/data_connection/document_transformers/) to make document transformations, like breaking vig docs into smaller ones, this would generate smaller final prompts and hence faster responses.\r\n"
      },
      {
        "user": "vmesel",
        "body": "Maybe we could also use langchain's caching module: https://python.langchain.com/docs/modules/model_io/llms/llm_caching"
      },
      {
        "user": "avelino",
        "body": "> Maybe we could also use langchain's caching module: https://python.langchain.com/docs/modules/model_io/llms/llm_caching\r\n\r\n@vmesel I probably don't understand how this cache works, it's not clear how to make the request to ChatGPT faster\r\n\r\nDoes the cache work? if so, we can implement it"
      },
      {
        "user": "vmesel",
        "body": "@avelino this cache makes less requests to GPT, reading further right now, it would make some requests easier to get a response, since they do not query LLMs APIs every time."
      },
      {
        "user": "lgabs",
        "body": "Is this still _really a problem_ solvable for arbitrary large prompts and configs? We're using dialog in production here with GPT-4o and for a median 800 tokens (input+completion) we have p50 latency of  ~1.40s and p99 of ~7s.\r\n\r\nModels are becoming faster recently (like GPT-4o) and I think the slowness you likely occur for many input tokens, which will be slower by definition for any model. I still think that _prompt engineering_ is probably the best way to tackle the problem (iterating in your `prompt.toml` to evaluate answers), but for RAG Apps like I said before the size of context from the retriever (which was not mentioned in the issue) also affects the final prompt size and demands user specific iteration without unique solution (e.g. number of relevant docs, cosine threshold). The choice of the model also affects speed, another parameter to iterate.\r\n\r\nCaching is only useful for equal _completion requests_, which save money for large prompts, but would probably not help much due to non-deterministic inputs from real users.\r\n\r\nMulti-Agent approach can be very useful to tackle this, since a simple solution in a customer support case would be agents per departments with smaller prompts (in which case user will still take care for these prompt not becoming large again), but agents will actually be good for a lot of new things, specially the usage of tools, and much more complex (LangGraph is now the recommended way to build Agents in Langchain). \r\n\r\nI'd say the agents approach will come in the roadmap for many reasons, but it's not a solution to \"large prompts\", but rather a completely new approach. \r\n\r\nI think this issue could be closed in favor of giving good practices in our docs about how to iterate through configurations to achieve good results and tips about prompt engineering."
      },
      {
        "user": "vmesel",
        "body": "@lgabs, I think we can close this issue for now and then reopen if needed"
      }
    ]
  },
  {
    "issue_number": 124,
    "title": "Adopt LangChain Expression Language (LCEL) ",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-02-22T22:05:24Z",
    "updated_at": "2024-06-02T22:35:32Z",
    "labels": [
      "enhancement",
      "LLM: ChatGPT"
    ],
    "body": "Langchain introduced in its 0.1.0 version many new concepts centered around the main concept of LangChain Expression Language ([LCEL](https://python.langchain.com/docs/expression_language/)). This is essentially a declarative way to compose chains together in a much more concise and objective manner than before. Chains built with LCEL are designed to offer out-of-the-box functionalities/support like, for example, streaming, async, parallelism, retries/fallbacks, access to intermediate results, and tracing with langsmith. Since this is a significant design change in Langchain and it really clarifies (see examples [here](https://python.langchain.com/docs/expression_language/why)) and makes everything more composable, it would be a good idea for us to refactor this project around this core concept. This would require major changes.\r\n\r\nCurrently, it seems that we process the user's input with a custom LLM instance either from a custom class or a default one, both inheriting from the DialogLLM Class, which in turn inherits from the AbstractLLM Class. The whole process of processing the input query into a prompt engineering with templates, retrieving documents for RAG, building a final prompt with retrieved docs and finally calling an LLM like OpenAI's `gpt-3.5-turbo` is architectured manually compared to what LCEL can offer. \r\n\r\nThis opportunity of following LCEL can be seen in this [RAG Cookbook](https://python.langchain.com/docs/expression_language/cookbook/retrieval) from Langchain's docs, which can guide the changes we need to make to achieve this guideline.\r\n\r\n",
    "comments": [
      {
        "user": "vmesel",
        "body": "@lgabs do you want to do this yourself? Let me know so I can assign this issue to you! =)"
      },
      {
        "user": "lgabs",
        "body": "@vmesel yes, I want to explore it! "
      },
      {
        "user": "lgabs",
        "body": "@vmesel, Ok, I understand there were many changes at once. However, I needed to make the whole process to fully understand the final solution. I can break these ideas into smaller steps trying to keep compatibility (although the project is in the beginning and at some time we'll break structures in favor of new ideas and deprecate old ones). \r\n\r\nSome questions I have:\r\n\r\n###  Plugins:\r\nAbout signature plugins, the `dialog-whatsapp` plugin would be affected in [here](https://github.com/talkdai/dialog-whatsapp/blob/main/dialog_whatsapp/responses.py#L47-L50) and [here](https://github.com/talkdai/dialog-whatsapp/blob/main/dialog_whatsapp/__init__.py#L137-L145) only? I think with LCEL and chains usage as proposed, any plugins would use the same interface as import a chain and use `chain.invoke` to process the user's message, still keeping a simple interface. \r\n\r\nAnd how a contributor is supposed to test plugins (currently just one) for changes? Maybe mocking dialog's call inside plugins? I don't know yet.\r\n\r\n### Vector Store and Memory\r\nThey still use postgres, but I've implemented in a more compatible way with LCEL and langchain components (taking advantage of their methods like saving/retrieving messages and embeddings). The main breaking changes I see here are: \r\n\r\n(A) the change of collections (schemas), which are already implemented by langchain's class and I think we should follow it so we keep focus on chains and serving them (for message history it keep records as langchain's Message format; for embeddings it keeps a good format including doc's metadata), and indeed this does not change the dialog behavior externally (endpoints work the same way, it would break user's usage if the user is consuming the collections externally somehow);\r\n(B) creating a session before interacting is not necessary anymore, since langchain's `PostgresChatMessageHistory`already creates one when it does not exist. This would not break `/chat/{chat_id}` actually, but would indeed remove necessity for `/session` and would break user's usage.\r\n\r\nI think these breaking changes make sense to take most advantage of the langchain framework. Can you think of other breaking changes? \r\n\r\nWhat do you contributors think?\r\n\r\nApart from these topics, I'll update this issue soon with a proposed \"roadmap\" to achieve LCEL as proposed across the whole process so we can also discuss: embeddings, chains (prompt templating, RAG, routing, LLM call, output parsing) and history management."
      },
      {
        "user": "lgabs",
        "body": "Some ideas in the direction of following LCEL:\r\n\r\nCurrent LLM processing in endpoint `/chat/{chat_id}`:\r\n  - get_llm_class returns a class to `LLM` variable, without it's instance (which inherents from `AbstractLLM`)\r\n  - llm_instance receives the instance of the class now defined in `LLM` variable with project config and session (which is used to get user history and save messages to the history)\r\n  - ai_message receives the result of `llm_instance.process`, and the `process` method processes the message until it's final output (in LCEL, this would be the chain invocation through `invoke` method)\r\n\r\n\r\nWhat I'm suggesting:\r\n  1. With a project config, get_llm_class can return a AbstractLLM subclass definition (say LLMChain), such that its method `process` implements the invocation of it's associated chain using LCEL under the hood. This would not change the interface in `main.py` (and neither in plugins), since it's still back-compatible and allow new chains as well. \r\n  2. Now, we have to deal with the LLMChain implementation, specially the `process` method. I think most parts of it would not be a problem, but two topics need attention: the retriever (using the vector store) and the memory, both already implemented. \r\n    - memory: currently, memory is accessed with class's memory property which returns a memory instance, which I think can be used directly in Runnable Chains.\r\n    - retriever: currently we use functions inside `embeddings.py` which already implement manually retrieval from the vector db. I'd have to check how to take advantage of this implementation or adapt langchain's PGVector to find a way to use the retriever as a runnable (all chain components must be langchain runnables ideally)\r\n "
      },
      {
        "user": "lgabs",
        "body": "https://github.com/talkdai/dialog/blob/83676dd2b54da38c59eaba56fc130e57f2e51d8f/src/dialog/llm/lcel_default.py#L106-L113\r\n\r\n@vmesel , this is a good first step towards adopting LCEL, since it has small changes, but i does not close the issue, I think we should discuss better in the issue (where I gave several ideas and it i'd be good to hear other ideas) how the roadmap could be to really get a LCEL interface which langchain's users will easily understand. \r\n\r\nFor example, I think that `AbstractLLM` forces a flow which langchain's chains already were designed for (the idea of input data, retrieval phase, prompt generation, branches and if-else cases, llm calls, post processing and output parsers), and, I my opinion, to really make dialog flexible with LCEL, the roadmap should go in the direction of removing this custom class in favor of building chains entirely with langchain's objects in every step (runnables), since this is what guarantees new functionalities like streaming, async, parallelism, retries/fallbacks, access to intermediate results, and tracing with langsmith etc. In this direction, I imagine dialog exposing in the api any chain the user wants to build or even combine, with a default chain available (using runnables in all steps). This would demand several changes."
      },
      {
        "user": "vmesel",
        "body": "@lgabs when you talk about the LCEL interface, do you mean attaching it directly to the endpoint or creating a instance of the LLM class we already have and implementing necessities there?\r\n\r\nWe created the class abstraction so we could add more flexibility to pre/post-processing and the processing of the LLM.\r\n\r\nThe AbstractLLM was created purposefully to make a strict signature that we could call any LLM that we crafted and add the necessary pre/post processing to it.\r\n\r\n\r\n> since this is what guarantees new functionalities like streaming, async, parallelism, retries/fallbacks, access to intermediate results, and tracing with langsmith etc.\r\n\r\nIf we implement the class having this features, we are still going to have the updated tooling, but with the restrictions of our class."
      }
    ]
  },
  {
    "issue_number": 196,
    "title": "[docs] Show an image of dialog architecture in README",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-05-31T22:08:59Z",
    "updated_at": "2024-06-01T14:30:12Z",
    "labels": [
      "documentation"
    ],
    "body": "**Idea:** Sometimes images can really help the understanding of software architectures. Since dialog deals with two projects (dialog and dialog-lib) + optional plugins, it seems like a good idea to show a picture of our architecture in README. I've made one diagram as a start example (in excalidraw, which allows us to export it and version it here as well):\r\n\r\n![image](https://github.com/talkdai/dialog/assets/27077298/ef65f04d-8061-4fc0-a3e2-b4b4189706ab)\r\n",
    "comments": [
      {
        "user": "lgabs",
        "body": "Here is the .zip with the above image (you can download it and import it in https://excalidraw.com/)\r\n[dialog-architecture.excalidraw.zip](https://github.com/user-attachments/files/15519125/dialog-architecture.excalidraw.zip)\r\n"
      },
      {
        "user": "vmesel",
        "body": "@lgabs thank you for the awesome diagram!\n\nWanna do a PR so we put this into the README.md and in the docs?"
      },
      {
        "user": "lgabs",
        "body": "Sure :) "
      }
    ]
  },
  {
    "issue_number": 191,
    "title": "Improve LCEL support through new Absctract class",
    "author": "vmesel",
    "state": "open",
    "created_at": "2024-05-18T19:51:48Z",
    "updated_at": "2024-06-01T00:51:16Z",
    "labels": [
      "enhancement",
      "devx"
    ],
    "body": "Our AbstractLLM class is very useful, but also very limited in the scope of the previous Langchain's API version, to continue improving, we need to start adding features to our default AbstractLLM class or creating a new one that provides us the structure for Abstract LCEL.\r\n\r\nThe first attempt wasn't very well thought out on my part, @lgabs suggested a new structure that would contemplate a comprehensive way of having LCEL objects in the class and allow us to still maintain retro compatibility.\r\n\r\n```python\r\nclass AbstractLCELClass(AbstractLLM):\r\n    def init(self, *args, **kwargs):\r\n        pass\r\n\r\n    def process(self, *args, **kwargs):\r\n        # calls chain through the invoke method from langchain\r\n        pass\r\n\r\n    @property\r\n    def chain(self):\r\n        # chain as a property, compiles all of it here\r\n\r\n    @property\r\n    def retriver(self):\r\n        # retriever instance\r\n        pass\r\n\r\n    @property\r\n    def memory(self):\r\n        # memory instance\r\n        pass\r\n\r\n    def invoke(self, *args, **kwargs):\r\n        # calls the class process method that calls the original invoke\r\n        pass\r\n\r\n```",
    "comments": [
      {
        "user": "lgabs",
        "body": "This is been discussed in this [PR in dialog-lib](https://github.com/talkdai/dialog-lib/pull/3)"
      }
    ]
  },
  {
    "issue_number": 194,
    "title": "Add support for multiple LLM Models on different endpoints",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2024-05-31T18:53:06Z",
    "updated_at": "2024-05-31T19:07:41Z",
    "labels": [
      "help wanted",
      "strategic",
      "API",
      "devx"
    ],
    "body": "We are starting to see more strategies mixing models between steps, the first step for us to start implementing this type of approach is letting users use different LLMs on different endpoints.",
    "comments": [
      {
        "user": "vmesel",
        "body": "Duplicate issue"
      }
    ]
  },
  {
    "issue_number": 175,
    "title": "Future of the Project",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2024-04-07T02:41:47Z",
    "updated_at": "2024-05-30T22:55:11Z",
    "labels": [
      "brainstorm"
    ],
    "body": "Hey guys, I'm writing this because this week I was discussing with @avelino and we came to a common ground on: we don't know what we want to achieve with the project anymore.\r\n\r\nThis project started as a simple RAG PoC, trying to understand how to implement and use langchain and other technologies to create humanized LLMs, but now we are walking through a path that is not a common ground for everyone.\r\n\r\nIMO, I think we should aim to be an open-source tech to allow developers to plug and play any LLMs and test them through CLI and REST APIs, also providing ways to easily deploy them and modify the context in order to apply Talkd.ai Dialog in their day-to-day problems.\r\n\r\n@avelino, @walison17, @lgabs and any other interested user in this project, could you share your thoughts?",
    "comments": [
      {
        "user": "lgabs",
        "body": "Nice discussion and very important to this project, @vmesel ! This kind of debate helps us to be in the same page about the roadmap, i.e., it helps our collaboration here. I'll give my thoughts here, but I find it useful to give some context about me.\r\n\r\n### Context\r\nTo give some context, I've used some frameworks for chatbots for some time now, including:\r\n- open-sourced [Rasa](https://rasa.com/): very nice project and community, it helps not only to build but also serve chatbot applications; it's a quite old but active framework built before this \"llm times\", when and dialogs were designed throught _intents_ and ML models were trained over dialogs of sequential intents to predict the next intent. For some intents, you could have _custom actions_ like calling api's to do more complex things. It was nice, but it was complex to predict dialogs in code, and llm's came and pretty much killed these approaches (now they're in the way to incorporate LLMs) . At that time I've used it (~2020), I found a [brazilian project](https://github.com/lappis-unb/rasa-ptbr-boilerplate) which aimed to wrap this tech in a more production-ready project using rasa, which helped my a lot to put a chatbot in production at work;\r\n- [Google's Dialogflow](https://cloud.google.com/dialogflow?hl=en): easy to build chatbots in the UI (browser), but very limited. Was good for PoCs with almost no-code requirements. Again, it was based on intent prediction, and only recently it seems to incorporate generative models. But i's paid and only allow Google's models. The same happened when I used [IBM Watson](https://www.ibm.com/br-pt/watson?utm_content=SRCWW&p1=Search&p4=43700078882371979&p5=e&gad_source=1&gclid=Cj0KCQjwztOwBhD7ARIsAPDKnkAXL8e_9hRloHdpQIrHasMB6eMDsVKH6xowwSOEKhWrCwifsuulCtAaAmpGEALw_wcB&gclsrc=aw.ds).\r\n\r\nSo far, in all cases I found it difficult not only to build a working application, but to deploy it as well. Now, LLMs introduced new possibilites for dialogs and tasks that could be accomplished with them. In October/2022, [Langchain came](https://github.com/langchain-ai/langchain) with the idea of build a new kind of applications, as they say: \r\n>LangChain is a framework for developing applications powered by large language models (LLMs).\r\n\r\nLater, the community saw the necessity to create technologies to monitor and deploy these new kind of applications, as they say:\r\n> [LangSmith](https://python.langchain.com/docs/langsmith): A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.\r\n>[LangServe](https://python.langchain.com/docs/langserve): A library for deploying LangChain chains as REST APIs.\r\n\r\n### Creating LLM Applications\r\nThe number of use cases of langhchain are as big as the number of applications of LLMs, i.e., it's impractical to make a unique product that tackles many different tasks (as a LLM Task), even using these frameworks. When I first used langchain at my current work (last year), I had a clear problem: _a chatbot capable of answering FAQs_, typical case of customer support where you don't want to let your customer lost in some ruled-decision tree. \r\n\r\nThis kind of _Task_ is usually solved with _RAG technique_, which is a better approach for factual recall (there is a very nice [OpenAI's cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb) about that) and it's a typical [use case of langchain](https://js.langchain.com/docs/use_cases/question_answering/). I've used it with [chromadb](https://github.com/chroma-core/chroma) to build an application (served with django), but _it was not clear for me how to deploy my chain properly_, and we had some performance issues in the api latency (I didn't test langserve, which [appeared by Oct/23](https://github.com/langchain-ai/langserve/releases?page=6)). \r\n\r\nThen, while I was taking some days off, @avelino told me about the creation of this open-sourced project, following the idea of RAG for Q&A, but delivering a much better production-ready project compared to the PoC I've done, so I immediately liked it, since It could help me and the community to deploy such an LLM application. Today, we saw that langserve's objective is also about deploying _chains_, but not full _LLM Applications_.\r\n\r\n### TL, DR\r\nNow, looking at the current application and using it, I still think this project is about getting a specific (difficult) problem, _Q&A with RAG_, and building an infrastructure around it to deliver to the community a production-ready LLM Application for this problem. At first, one may think this application is just about answering FAQs like a company's FAQ: you plug a vectordb for retrival and _violà_, your llm can now answer your specific domain. \r\n\r\nIt's seams easy in theory (since everyone talks about it now), but practice will demand many custom tools to create, test, evaluate, debug and extend such an application, e. g.: evaluate vector search performance, evaluate llm's answers, setup the vectordb and/or db for memory, allow classification of conversations, use Agent tools to extend actions (call apis or process specific tasks like canceling an order or find something somewhere else the knowledge base) etc. All of this orbit over the same problem of _answering factual questions with RAG_, and projects like this could empower developers to build and deploy these applications much faster and and with a lot of control and transparency of the whole process."
      },
      {
        "user": "vmesel",
        "body": "@lgabs thanks for your whole context and POV. I agree with you on some points, we don't have an easy way to deploy a RAG or any other LLM application yet and we still lack some testing capabilities on most of the frameworks created (even langchain's family is quite limited in some aspects).\r\n\r\nThe only addition/change to your POV that I would do is that we could start adding features for more general LLM approaches such as toolings or other structures.\r\n\r\nOne of the frameworks I'm observing right is [crewAI](https://github.com/joaomdmoura/crewai) -> its pretty cool, brazilian made and brazilian built, it helps us get more architectures for LLMs.\r\n\r\nI do agree that we shouldn't focus on multiple LLM approaches right now.\r\n\r\nWhat I have in my mental roadmap:\r\n- Add support for native authentication through JWT\r\n- Breakdown what is the server and what is a \"library\" so we can make dialog functional with other frameworks such as Django, Flask or whatever. \r\n- Add support for tooling in our base abstract class, in a way that the user just need to add his function and all of the magic is done on background\r\n- Create a basic chat frontend so we can test out our applications\r\n- Create a basic CLI that would allow us to invoke the LLM from the terminal and test approaches on an `input()` call\r\n- Add support for user tests for LLM through simple configuration\r\n\r\n"
      },
      {
        "user": "avelino",
        "body": "> after 3 weeks I've arrived here to share my vision for the project and which path it could follow - sorry for the delay in responding publicly, even though I've shared my vision with all of you via a private call.\r\n\r\nLooking at the RAG/LLM \"market,\" it is very much tied to engineers developing code using RAG/LLM and deploying it. I miss a solution that would allow someone with no software engineering knowledge to \"play\" with developed RAGs and switch them for testing (and even use them in production easily).\r\n\r\nIn other words, I would steer the `talkd/dialog` towards being a solution that provides autonomy for someone with Ops knowledge (devops, not necessarily a software engineer) to swap/test other developed RAGs for the `talkd/dialog`.\r\n\r\nThe `talkd/dialog` would have a **marketplace of RAGs**, making it easy for users to utilize them, **for example:** by installing the package from PyPI and configuring the `﻿class` that will be invoked at the endpoint in the ﻿`.toml`.\r\n\r\n## for engineers\r\n\r\nI would split the project into two parts:\r\n\r\n1. **talkd/dialog:** a lib distributed on PyPI for projects to use `talkd/dialog` as a framework and co-create atop what we will have in the lib;\r\n2. **talkd/dialog-server:** an API/server that serves the HTTP server, developed to put a RAG/LLM solution into production \"without reliance\" on an engineer - the server uses the ﻿talkd/dialog.\r\n\r\n## Will we no longer be just another \"humanized conversation API\"?\r\n\r\n**`﻿talkd/dialog` will not be**, _but_ a RAG will be distributed for this type of solution.\r\n\r\n---\r\n\r\n**Comment on what you think of this path @walison17, @lgabs, @vmesel** "
      },
      {
        "user": "vmesel",
        "body": "@avelino dialog is already taken in pypi, that's why I've got the dialog-lib for the library.\r\n\r\n> Looking at the RAG/LLM \"market,\" it is very much tied to engineers developing code using RAG/LLM and deploying it. I miss a solution that would allow someone with no software engineering knowledge to \"play\" with developed RAGs and switch them for testing (and even use them in production easily).\r\n\r\nEngineering perspective: Looking at your perspective, it would make sense to work on UI improvements as well, so we have a front-end for testing and researching new LLMs, also adding support for multiple LLMs while running.\r\n\r\nI agree with your vision for the project invocation, just not on the TOML (don't think it's the best interface, but it's just an engineering thing)."
      },
      {
        "user": "avelino",
        "body": "Apparently, we have the path we will follow, right?\r\n\r\nNow we need to publicly communicate this 'change of course', making it clear:\r\n\r\n* why it started as a 'humanized conversation API'\r\n* why we changed the direction of the project\r\n* and that 'humanized conversation API' will become a RAG (plugin) and no longer the only core of the project\r\n\r\n> **we need to tell the story**; having a changelog in the documentation would be the best place!"
      }
    ]
  },
  {
    "issue_number": 182,
    "title": "Add support for LLama CPP",
    "author": "vmesel",
    "state": "open",
    "created_at": "2024-05-01T00:00:23Z",
    "updated_at": "2024-05-30T22:49:14Z",
    "labels": [],
    "body": "In order for us to enable users to run their finetuned models that are LlamaCPP compatible, we need to add this compatibility in our class structure. ",
    "comments": [
      {
        "user": "lgabs",
        "body": "Nice, local models can be very useful for local tasks like prompt evaluations or even for the main task of Q&A. Langchain [seams to have the integration](https://python.langchain.com/docs/integrations/llms/llamacpp/) with llama.cpp."
      }
    ]
  },
  {
    "issue_number": 178,
    "title": "Breaking down our library to server coupling",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2024-04-10T17:22:35Z",
    "updated_at": "2024-05-14T17:51:20Z",
    "labels": [],
    "body": "Nowadays we are completely attached with the server and we cannot operate as a stand-alone library, in order to make the project grow and be more sustainable, we need to start splitting responsibilities between our server and our library.\r\n\r\nMy recommendation here would be that we start splitting the library and server approaches, removing the functionalities from the views and making them look more like a service/library that is still very coupled with the server - still manage connections to the database, generate sessions and etc.\r\n\r\nThe next step I would take is to make our Agent/LLM/Chain classes less dependent on the database interactions, so we would achieve the usage of the database through dependency injection to the real library.\r\n\r\nWhat are your thoughts?",
    "comments": []
  },
  {
    "issue_number": 186,
    "title": "Alter docs to improve context on past and future developments",
    "author": "vmesel",
    "state": "open",
    "created_at": "2024-05-11T18:38:15Z",
    "updated_at": "2024-05-12T21:11:49Z",
    "labels": [
      "documentation"
    ],
    "body": "As we spoke on #175, we are altering a little bit our path of the project. In order to do this, we need to update our docs with more details:\r\n\r\n> Now we need to publicly communicate this 'change of course', making it clear:\r\n>\r\n> - why it started as a 'humanized conversation API'\r\n> - why we changed the direction of the project\r\n> - and that 'humanized conversation API' will become a RAG (plugin) and no longer the only core of the project",
    "comments": [
      {
        "user": "vmesel",
        "body": "As well as adding some context on naming conventions we are using, as remembered by @lgabs:\r\n\r\n> I'd like to take the opportunity here to point something important: the taxonomy we use when speaking about LLM Applications is important for our communication. There are many confusing terms and many recent concepts, so I just want to help us to use the same names for the same things:\r\n> \r\n> * _LLM (Large Language Models)_: this is the model per-se, not the chains, not an agent. We sometimes use _llm_ to denote a _chain_ that calls an _llm_ (even langchain's example call instances of chains by `llm` in this case), but it's important to us to keep this in mind.\r\n> * [LangChain Expression Language](https://python.langchain.com/v0.1/docs/expression_language/), or LCEL, is a declarative way to easily compose chains together. It's the a way to build chains, so we could avoid calling chains by this name.\r\n> * _Chains_ are a more abstract concept from langchain (it's its base concept), of [_\"Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step\"_](https://python.langchain.com/v0.1/docs/modules/chains/). They are built preferably with LCEL joining one or more runnables.\r\n> * [_Runnables_](https://python.langchain.com/v0.1/docs/expression_language/interface/): the new protocol used by most of langchain components, making it easier to compose chains like `runnable1 | runnable2`. It's kind of okay to call a chain by _runnable_ since it is its subclass, but the name makes it easier to understand that chains are compositions, and runnables are components. For example, langchain's prompt templates, memory components (like RunnableWithMessageHistory) and retrievers (like PGVector) are also runnables.\r\n> * _RAG_: RAG is a technique for augmenting LLM knowledge with additional data. It's just the technique, so it'd not be good to call chains or runnables by RAG. A chain _uses_ RAG technique to enhance it's performance and knowledge of the world.\r\n> * _LLM Applications_: BTW this is a better name for what dialog project produces: an application that uses LLM to power its capability. It's not _only_ a _LLM_, but a whole infrastructure around it."
      }
    ]
  },
  {
    "issue_number": 11,
    "title": "replace `pgvector` with `timescaledb`",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-15T14:57:14Z",
    "updated_at": "2024-04-16T12:41:58Z",
    "labels": [
      "enhancement",
      "strategic"
    ],
    "body": "improving vector performance [ref](https://www.timescale.com/blog/how-we-made-postgresql-the-best-vector-database/\r\n)\r\n\r\nhttps://www.timescale.com/ai",
    "comments": [
      {
        "user": "lgabs",
        "body": "Its seems a very nice vectordb, but given the dozens of free [vectordbs available](https://python.langchain.com/docs/integrations/vectorstores/) (I think there are more than 50) in langchain intergrations ([including timescale](https://python.langchain.com/docs/integrations/vectorstores/timescalevector/)) and given that timescale is [not free](https://www.timescale.com/pricing) and not self-hosted as of now, we'd better maintain postgres as a wide known choice, I think."
      },
      {
        "user": "vmesel",
        "body": "@lgabs i agree. I'll close this issue as we are not going to support this right now."
      }
    ]
  },
  {
    "issue_number": 177,
    "title": "Broken load_csv after update on version 0.0.7",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2024-04-10T13:00:52Z",
    "updated_at": "2024-04-10T14:53:43Z",
    "labels": [
      "bug"
    ],
    "body": null,
    "comments": [
      {
        "user": "vmesel",
        "body": "Fixed on new tag"
      }
    ]
  },
  {
    "issue_number": 174,
    "title": "Add Prompt testing using a LLM testing suite",
    "author": "vmesel",
    "state": "open",
    "created_at": "2024-04-07T02:31:52Z",
    "updated_at": "2024-04-07T13:43:54Z",
    "labels": [
      "LLM: ChatGPT",
      "Prompt Engineering",
      "devx"
    ],
    "body": "We need to enable our users to prevent having regressions on their prompt without noticing it clearly. In order to achieve this, we must implement a way for our users to run a test suite on their own test cases with a user set similarity score. This must be simply set and must be extensible to use beyond toml files.",
    "comments": [
      {
        "user": "avelino",
        "body": "https://github.com/confident-ai/deepeval?tab=readme-ov-file#writing-your-first-test-case\r\n\r\nthis week @lgabs shared this project with me, apparently we were able to use it to test our prompts"
      },
      {
        "user": "vmesel",
        "body": "Yes, I saw the README on ragtalks and forgot to attach the link, thats the\r\nlibrary I was looking for.\r\n\r\n\r\n\r\nOn Sun, 7 Apr 2024 at 10:05 Avelino ***@***.***> wrote:\r\n\r\n>\r\n> https://github.com/confident-ai/deepeval?tab=readme-ov-file#writing-your-first-test-case\r\n>\r\n> this week @lgabs <https://github.com/lgabs> shared this project with me,\r\n> apparently we were able to use it to test our prompts\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/talkdai/dialog/issues/174#issuecomment-2041464624>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ABGA2UYJKDWV5JV4VIOMKCLY4FADHAVCNFSM6AAAAABF25SA3CVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDANBRGQ3DINRSGQ>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      },
      {
        "user": "lgabs",
        "body": "Yeah, I couldn't study too much of llm evals, but I did check that the way the community seams to evaluate llms applications is using several standard metrics such that another llm evaluates the llm apppication outputs against expected results (this evaluator could even be a free local llm, for a task is much simpler).\n\nI saved this [Andre Ng short course ](https://learn.deeplearning.ai/courses/automated-testing-llmops?_gl=1*12438vp*_ga*MTEwODUyNjk1NS4xNzEyNDk1ODIw*_ga_PZF1GBS1R1*MTcxMjQ5NTgyMC4yLjEuMTcxMjQ5NTg5Ny41OS4wLjA.)to see soon, maybe it'll help   🤘\n"
      },
      {
        "user": "lgabs",
        "body": "Also, I think it would be a good idea for us to use the same common dataset for local dev, and also for tests that depend on dataset (generate embeddings or even these llm evals), one idea is to download some one from hugging face like this [wiki_qa](https://huggingface.co/datasets/wiki_qa). What do you think? This would a new issue, of course.\n\nThese llm evals change a lot depending on the domain, so i think we could just make good documentation about how to add these tests without trying to write generic cases for all datasets."
      },
      {
        "user": "vmesel",
        "body": "So I was thinking on making it available for the developer to create new\r\ntest cases, not necessarily our software writing generic test cases.\r\n\r\nThink of having just a single test case, instead of having to implement it\r\nby yourself inside dialog, you can just write a new toml file that could\r\nenable this feature.\r\n\r\nOn using other LLMs, I’m not aware on what should be implemented in those\r\ncases, going to watch the video here to research more about this.\r\n\r\n\r\n\r\n\r\nOn Sun, 7 Apr 2024 at 10:36 Luan Fernandes ***@***.***> wrote:\r\n\r\n> Also, I think it would be a good idea for us to use the same common\r\n> dataset for local dev, and also for tests that depend on dataset (generate\r\n> embeddings or even these llm evals), one idea is to download some one from\r\n> hugging face like this [wiki_qa](https://huggingface.co/datasets/wiki_qa.\r\n> What do you think? This would a new issue, of course.\r\n>\r\n> These llm evals change a lot depending on the domain, so i think we could\r\n> just make good documentation about how to add these tests without trying to\r\n> write generic cases for all datasets.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/talkdai/dialog/issues/174#issuecomment-2041474214>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ABGA2U2FJT2U72XBOKCJD63Y4FDVFAVCNFSM6AAAAABF25SA3CVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDANBRGQ3TIMRRGQ>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      }
    ]
  },
  {
    "issue_number": 28,
    "title": "message bucket: collect messages and send in a single prompt",
    "author": "avelino",
    "state": "open",
    "created_at": "2023-11-19T15:42:54Z",
    "updated_at": "2024-04-07T02:30:06Z",
    "labels": [
      "enhancement",
      "EPIC"
    ],
    "body": "The `POST /chat/{chat_id}` receives one message at a time, when it receives it, it processes it in the llm and returns a message to you, example:\r\n\r\n- **QA 1:** hello how are you?\r\n- **QA 2:** I need help, can you help me, please?\r\n- **QA 3:** how do I perform operation XYZ on the web page?\r\n\r\nThis flow of conversation is common in a chat environment (e.g. WhatsApp), where the user breaks the line. What the user would like to receive is only the answer to **QA 3**, the other messages are introductory (\"presentation\").\r\n\r\n**As implemented today, we answer one message at a time:**\r\n\r\n- **Reply from QA1:** Hi, how can I help?\r\n- **Reply from QA2:** It would be a pleasure to help you, how can I help you?\r\n- **Reply from QA3:** You should access, ... the answer to the question\r\n\r\n> the answer that matters is **QA3**, QA1 and QA2 are ~~\"duplicated\"~~\r\n\r\n## solution\r\n\r\nParameter in the endpoint _(POST, create message)_ called `message bucket`, which activates intelligence to collect messages in the backend and make a single call to the LLM sending the collection of messages.\r\n\r\nI can think of a solution to collect requests and if no message is received at X after the last message received, call the LLM aggregating all the messages not sent.\r\n\r\n> it's not the best solution, but it's the solution that comes to mind at first - this issue is to discuss the best solution, probably the proposed solution is not the best\r\n\r\n",
    "comments": [
      {
        "user": "vmesel",
        "body": "@avelino I'm thinking that we will need a message buffer that stores the messages for a certain period before sending them to the server. Would we need a Redis or Memcached server or would you implement this by hand?"
      },
      {
        "user": "avelino",
        "body": "> Would we need a Redis or Memcached server or would you implement this by hand?\r\n\r\nI don't want to define technology (database) but rather discuss architecture, so we deal with \"storage\" as a storage resource (redis, memcached or other) and not a solution to the problem.\r\n\r\n\r\n1. bucket endpoint:\r\n    - temporary solution for storing messages\r\n    - control is on the client side and calls the created bucket and sends all bucket messages to the LLM\r\n    > I don't like this solution, I believe that we wouldn't have \"intelligence\" on the server side, but on the client side.\r\n\r\n2. \"Intelligence\" based on the delay time for receiving messages:\r\n    - provisional solution for storing messages, activated by a \"parameter\" of the endpoint\r\n    - if it doesn't receive messages for X amount of time, it collects all the messages not sent to the LLM and sends them all at once (to the prompt)\r\n\r\n\r\n> version \"2\" is the way I'd like to see it working, but one doesn't prevent the other\r\n"
      },
      {
        "user": "vmesel",
        "body": "We can achieve this by checking how langchain implemented the approach on batch/streaming endpoints on langserve."
      }
    ]
  },
  {
    "issue_number": 168,
    "title": "Add token count to the Chat history table",
    "author": "vmesel",
    "state": "open",
    "created_at": "2024-03-23T19:39:08Z",
    "updated_at": "2024-04-07T02:23:01Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "lgabs",
        "body": "I think it's possible to monitor this manually, but langsmith already helps with that in a very elegant way in terms of observability, including latency and other metrics. This is a very good way of monitoring the application for dialog developers. Maybe this issue would be about logging this kind of metric in the history table for other purposes?\r\n\r\nHere is an example:\r\n\r\n<img width=\"1382\" alt=\"image\" src=\"https://github.com/talkdai/dialog/assets/27077298/7f3cfa4f-b6a9-46a8-acbe-998583a3a236\">\r\n"
      },
      {
        "user": "vmesel",
        "body": "@lgabs I didn't know about this feature on langsmith, @avelino told me you were using it and I got really excited to know more about it.\r\n\r\nMy original plan was just to look at costs on tokens and how we are spending them through messages."
      }
    ]
  },
  {
    "issue_number": 172,
    "title": "Make settings be an object so we are able to use it through PSQL or TOML",
    "author": "vmesel",
    "state": "open",
    "created_at": "2024-04-05T19:48:47Z",
    "updated_at": "2024-04-07T02:15:36Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "lgabs",
        "body": "what object it would be? One of these days I was thing about [pydantic settings](https://docs.pydantic.dev/latest/concepts/pydantic_settings/), any ideas on that? Langserve uses pydantic as well, so that's why I was looking at this."
      },
      {
        "user": "vmesel",
        "body": "@lgabs so I took a look at your ragtalk repo and I really enjoyed your implementation of the settings object, I've made a change on #171 that bumps the settings of the project to an object, but it's not the best approach ever."
      }
    ]
  },
  {
    "issue_number": 169,
    "title": "Create session listing API and send human message",
    "author": "vmesel",
    "state": "open",
    "created_at": "2024-04-03T22:31:54Z",
    "updated_at": "2024-04-03T22:37:59Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "vmesel",
        "body": "Human messages should pause the AI"
      }
    ]
  },
  {
    "issue_number": 165,
    "title": "Pandas to_list doesnt exist",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2024-03-16T20:15:56Z",
    "updated_at": "2024-03-17T18:23:24Z",
    "labels": [],
    "body": "```\r\napi-1  | Traceback (most recent call last):\r\napi-1  |   File \"/app/src/load_csv.py\", line 72, in <module>\r\napi-1  |     load_csv_and_generate_embeddings(\r\napi-1  |   File \"/app/src/load_csv.py\", line 56, in load_csv_and_generate_embeddings\r\napi-1  |     df_filtered[\"embedding\"] = generate_embeddings(df_filtered[embed_columns].agg('\\n'.join, axis=1).to_list())\r\napi-1  |                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\napi-1  |   File \"/usr/local/lib/python3.11/site-packages/pandas/core/generic.py\", line 6293, in __getattr__\r\napi-1  |     return object.__getattribute__(self, name)\r\napi-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\napi-1  | AttributeError: 'DataFrame' object has no attribute 'to_list'. Did you mean: 'to_dict'?\r\n```",
    "comments": []
  },
  {
    "issue_number": 83,
    "title": "Add hot reload for environment variables",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-12-27T13:16:49Z",
    "updated_at": "2024-03-16T13:24:20Z",
    "labels": [],
    "body": "Adds hot reload on environment variables, making them dynamic. If changed, they must be set to the new value assigned in the system.\r\n\r\nThis will enable plugin hot-swap and other projects.",
    "comments": [
      {
        "user": "vmesel",
        "body": "Not necessary anymore, this can be easily fixed by a deploy strategy"
      }
    ]
  },
  {
    "issue_number": 145,
    "title": "Embedding is not ocurring over question and document together",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-03-07T15:11:15Z",
    "updated_at": "2024-03-14T13:59:52Z",
    "labels": [
      "embedding"
    ],
    "body": "Currently, we're only embedding the content from knowledge base:\r\nhttps://github.com/talkdai/dialog/blob/f97728a06883d3deeb1e1af58006be0fd92b56ef/src/load_csv.py#L57\r\n\r\nHowever, is it a better practice to embed  both question and answer together, as the retrieval will compare the user question over documents and they should have questions as well for better context. Maybe we can explain that in the documentation, to achieve that, `content` should be the question (which is already a column) concatenated with the answer, or if we keep the word `content` reserved for answers, we join question and answer in code.\r\n\r\n### Suggestion\r\nI think we should change the term `content` to `document` (maintaining compatibility, but langchain treat each unit of knowledge base as _document_, so it'd be good to align with that) and explain in our documentation that this will be the main input for embeddings (other columns will act as metadata). We cal also give an example of Q&A document \"Question: ... Answer: ...\" to give more clarity (if the user want to embed just answers, he/she prepares a csv with answer in `document` column)",
    "comments": [
      {
        "user": "avelino",
        "body": "the change makes sense to maintain the langchain standard.\r\n\r\n@lgabs checking for the column \"document\" and seeking the column \"content\" ensures compatibility while making the alteration."
      },
      {
        "user": "lgabs",
        "body": "Good, you can assign that to me"
      },
      {
        "user": "lgabs",
        "body": "I'll wait #147 first"
      },
      {
        "user": "vmesel",
        "body": "@lgabs can I close this issue?"
      },
      {
        "user": "lgabs",
        "body": "I was looking at the code, and if we would change the term to the more standard name `document`, it would confuse with the database column `content` in the `contents` table. It would be a breaking change to change all names to `document`, including the database references, and duplicate information would be also confusing. It's better to keep the current term.\r\n\r\nTo mitigate possible confusions, I've made some suggestions in the documentation about the embeddings for more clarification and to add explanation about embedding more columns: https://github.com/talkdai/dialog/pull/161/files"
      }
    ]
  },
  {
    "issue_number": 108,
    "title": "Docker image versioning",
    "author": "walison17",
    "state": "open",
    "created_at": "2024-02-19T13:24:07Z",
    "updated_at": "2024-03-14T12:20:37Z",
    "labels": [
      "devx",
      "Docker"
    ],
    "body": "Today we only have 1 tag/version (latest) of the docker image, this approach means that consumers always use the most recent version of dialog which may have some breaking changes. It would be amazing if we had some versioning system like semantic versioning to avoid problems like this.",
    "comments": [
      {
        "user": "vmesel",
        "body": "@walison17 its a cool idea, we need to start working on releases so we can get this kind of approach done."
      },
      {
        "user": "avelino",
        "body": "First, we need to focus on versioning the **Python** software and then replicate that to the Docker image."
      },
      {
        "user": "vmesel",
        "body": "I've added the auto-tagger, so every commit that goes to main, we generate a new tag"
      }
    ]
  },
  {
    "issue_number": 74,
    "title": "Add tests on the platform",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-12-07T12:58:22Z",
    "updated_at": "2024-03-14T12:16:54Z",
    "labels": [
      "devx"
    ],
    "body": "Adds test coverage in the platform",
    "comments": []
  },
  {
    "issue_number": 38,
    "title": "Add content recommendation endpoints based on L2 Distance of the existing embeddings",
    "author": "vmesel",
    "state": "open",
    "created_at": "2023-11-22T13:08:48Z",
    "updated_at": "2024-03-13T23:10:55Z",
    "labels": [
      "enhancement",
      "embedding",
      "API"
    ],
    "body": "Maybe we could add an endpoint that enables users to fetch similar content based on their inquiry. This could possibly add more options for answers on the front-end.",
    "comments": [
      {
        "user": "vmesel",
        "body": "We changed our algorithm right now to use cosine similarity, but we still need to add this endpoint, so we can let people, based on a certain content, get the K contents and return them into a JSON response."
      }
    ]
  },
  {
    "issue_number": 37,
    "title": "webhook support",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-11-22T13:06:29Z",
    "updated_at": "2024-03-13T23:08:08Z",
    "labels": [
      "enhancement",
      "strategic",
      "API",
      "EPIC"
    ],
    "body": "In order for talkd/dialog to be a simple-to-use solution (API), we need to implement webhook support for \"market standard\" solutions, i.e. support the format of the main tools, for example:\r\n\r\n - [X] whatsapp\r\n - [ ] openai\r\n\r\n> It is possible to make the adoption of talkd/dialog simple, **such as changing the webhook URL and tokens**, so that everything \"magically\" continues to work.",
    "comments": [
      {
        "user": "vmesel",
        "body": "We are going to implement a plugin for every provider, a base example is: [dialog-whatsapp](https://github.com/talkdai/dialog-whatsapp)"
      }
    ]
  },
  {
    "issue_number": 73,
    "title": "Adds API Token support",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-12-07T12:06:36Z",
    "updated_at": "2024-03-13T23:06:17Z",
    "labels": [
      "API"
    ],
    "body": "We need to add some auth to our API so we can keep it secure. Maybe a JWT would work here, or some other sort of bearer token.",
    "comments": [
      {
        "user": "vmesel",
        "body": "This was done using an external plugin called [dialog-api-token](https://github.com/talkdai/dialog-api-key/)"
      }
    ]
  },
  {
    "issue_number": 106,
    "title": "Add docs for undocumented `question_signalizer` prompt config",
    "author": "walison17",
    "state": "closed",
    "created_at": "2024-02-17T13:51:24Z",
    "updated_at": "2024-03-13T23:00:32Z",
    "labels": [],
    "body": "https://github.com/talkdai/dialog/blob/ad948b4f844189507029d1e3bc86bff4904a099e/src/dialog/llm/default.py#L56-L57",
    "comments": [
      {
        "user": "vmesel",
        "body": "@walison17 closing this issue because we don't support this signalizer anymore"
      }
    ]
  },
  {
    "issue_number": 152,
    "title": "`load_csv.py` is failing due to wrong concatenation of embeddings cols",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-03-12T21:22:26Z",
    "updated_at": "2024-03-13T19:47:26Z",
    "labels": [],
    "body": "`load_csv.py` is failing due to wrong concatenation of embeddings cols. When starting `dialog` service:\r\n```\r\ndialog-1  | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\ndialog-1  | INFO  [alembic.runtime.migration] Will assume transactional DDL.\r\ndialog-1  | Generating embeddings for new questions...\r\ndialog-1  | New questions: 44\r\ndialog-1  | Traceback (most recent call last):\r\ndialog-1  |   File \"/app/src/load_csv.py\", line 74, in <module>\r\ndialog-1  |     load_csv_and_generate_embeddings(\r\ndialog-1  |   File \"/app/src/load_csv.py\", line 58, in load_csv_and_generate_embeddings\r\ndialog-1  |     \"\\n\".join(df_filtered[column] for column in columns))\r\ndialog-1  |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-1  | TypeError: sequence item 0: expected str instance, Series found\r\n```\r\nwhich is trying to concat a list of pd.Series instead of a list of strings. One way to achieve that is to concatenate the desired columns along row axis and then transform the series to a list, I'm working on that fix",
    "comments": []
  },
  {
    "issue_number": 146,
    "title": "embedding multiple fields, not just content/document",
    "author": "avelino",
    "state": "closed",
    "created_at": "2024-03-07T22:02:29Z",
    "updated_at": "2024-03-10T14:26:02Z",
    "labels": [
      "embedding"
    ],
    "body": "We force generating the embed for the content field; this likely covers many use cases.\r\n\r\nTo improve support for other use cases, we could consider adding support for concatenating other fields from the knowledge base in the ﻿`load_csv` function.\r\n\r\n\r\n```bash\r\n./load_csv.py --columns=question,content\r\n```",
    "comments": [
      {
        "user": "lgabs",
        "body": "@avelino it would be good to udpate the README about this new feature."
      }
    ]
  },
  {
    "issue_number": 117,
    "title": "Fallback confuse behaviour",
    "author": "schumannc",
    "state": "closed",
    "created_at": "2024-02-22T14:46:41Z",
    "updated_at": "2024-03-06T22:18:56Z",
    "labels": [
      "question",
      "AI"
    ],
    "body": "**Current Behavior:**\r\nIn the current implementation of `fallback`, when a document retrieval process yields no results, the system adds fallback text directly into the prompt body. This approach delegates the responsibility of handling empty or no-result scenarios to the LLM, which may not always align with the expected or desired output for end-users.\r\n\r\n**Issue**\r\nIt is unclear if the current behavior was intentionally designed this way. From an outside perspective, integrating fallback text into the prompt seems to obscure the model's capacity to appropriately handle cases where no documents are retrieved.\r\n\r\n**Suggested Improvement:**\r\nIt may be more logical and user-friendly to have a fixed fallback text or a predefined response strategy for scenarios where the document retrieval process returns no results. ",
    "comments": [
      {
        "user": "vmesel",
        "body": "@schumannc we handled it like this originally because we didn't want to use the LLM outside of the scope of the documents, if this needs to be changed, we can add a new LLM implementation on top of the abstract one."
      },
      {
        "user": "lgabs",
        "body": "Here is an example asking \"quem foi o presidente do Brasil em 2001?\" using the `.toml` from README (assuming the db is already up) and its answer:\r\n\r\nlogs:\r\n```\r\n  dialog git:(main) ✗ docker compose up dialog\r\n[+] Building 0.0s (0/0)                                                                                  docker:desktop-linux\r\n[+] Running 2/2\r\n ✔ Container dialog-db-1      Recreated                                                                                  0.4s\r\n ✔ Container dialog-dialog-1  Recreated                                                                                  0.1s\r\nAttaching to dialog-dialog-1\r\ndialog-dialog-1  | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\ndialog-dialog-1  | INFO  [alembic.runtime.migration] Will assume transactional DDL.\r\ndialog-dialog-1  | /app/src/load_csv.py:4: DeprecationWarning:\r\ndialog-dialog-1  | Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\r\ndialog-dialog-1  | (to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\r\ndialog-dialog-1  | but was not found to be installed on your system.\r\ndialog-dialog-1  | If this would cause problems for you,\r\ndialog-dialog-1  | please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\r\ndialog-dialog-1  |\r\ndialog-dialog-1  |   import pandas as pd\r\ndialog-dialog-1  | Generating embeddings for new questions...\r\ndialog-dialog-1  | New questions: 0\r\ndialog-dialog-1  | INFO:     Will watch for changes in these directories: ['/app/src']\r\ndialog-dialog-1  | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\ndialog-dialog-1  | INFO:     Started reloader process [44] using WatchFiles\r\ndialog-dialog-1  | INFO:     Started server process [46]\r\ndialog-dialog-1  | INFO:     Waiting for application startup.\r\ndialog-dialog-1  | INFO:     Application startup complete.\r\ndialog-dialog-1  | 2024-02-27 01:44:33,168 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\r\ndialog-dialog-1  | ================================ System Message ================================\r\ndialog-dialog-1  |\r\ndialog-dialog-1  | I'm sorry, I didn't understand your question. Could you rephrase it?\r\ndialog-dialog-1  |\r\ndialog-dialog-1  | ================================ Human Message =================================\r\ndialog-dialog-1  |\r\ndialog-dialog-1  | {user_message}\r\ndialog-dialog-1  | 2024-02-27 01:44:33,220 - INFO - Verbose LLM prompt: None\r\ndialog-dialog-1  | /usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\r\ndialog-dialog-1  |   warn_deprecated(\r\ndialog-dialog-1  | 2024-02-27 01:44:35,417 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\r\ndialog-dialog-1  | 2024-02-27 01:44:35,462 - INFO - Request processing time for chat_id cb2c6ada8d6948d794144051ece695ee: 0:00:03.478990\r\ndialog-dialog-1  | INFO:     192.168.65.1:32340 - \"POST /chat/cb2c6ada8d6948d794144051ece695ee HTTP/1.1\" 200 OK\r\ndialog-dialog-1  | 2024-02-27 01:44:35,467 - INFO - keywords: cancelo, reserva, cancelar, siga, passos, abaixo, acesse, aba, viagens, selecione, deseja, final, página, seção, ajuda, clique, informe, motivo, cancelamento, conclua, processo, caso, alguma, dúvida, precise, adicional, disposição, ajudar, presidente, brasil, momento, jair, bolsonaro, 2001, fernando, henrique, cardoso, 1995, 200\r\n```\r\nanswer:\r\n```\r\n{\r\n  \"message\": \"Em 2001, o presidente do Brasil era Fernando Henrique Cardoso. Ele foi presidente do Brasil de 1995 a 2003.\"\r\n}\r\n```\r\n\r\nThis actually _uses the LLM outside the scope of the documents_, since, after discovering there are no documents retrieved, we still build a strange prompt with the fallback phrase and send its to GPT to answer. However, I've already tested that GPT can follow instructions to answer some fallback phrase in case of question outside document's context or if there are none in dedicated \"Context\" section.\r\n\r\nI think the solution here can go in two (at minimum) ways:\r\n\r\n1. Avoid making a prompt to GPT and, in case of zero documents retrieved, answer the fixed fallback as @schumannc suggested, solving the problem right away;\r\n2. Prompt Engineering: delegate the decision about fallback to GPT. In this case, even with relevant documents (it can happen, it's difficult to say if a query will hit the cosine distance threshold), if GPT sees that the question is outside context, it answers accordingly. This is not a bad option either, I've tested prompts with this kind of instruction and it works (for example, langchain's prompt hub for [rag promp includes this instruction](https://smith.langchain.com/hub/rlm/rag-prompt)). There is a simple test of this [in this colab](https://colab.research.google.com/drive/1gLHen4Q-ACStbB5IUXtIsQd0wywtZCjH#scrollTo=Le9yuM2qLa3h), taken from the [Q&A docs](https://python.langchain.com/docs/use_cases/question_answering/quickstart)"
      },
      {
        "user": "lgabs",
        "body": "There is a second problem we have to deal/discuss with follow-up questions: when using memory history, option (1) I've said above can not work so well, for example:\r\n\r\nfirst user question\r\n```\r\nuser: how can I cancel my order?\r\nAI: you can cancel your have to come to the store and bring the product or I can cancel it online for you and send someone to get the product [...]\r\n```\r\nsecond user question (with previous memory):\r\n```\r\nuser: can you do this for me?\r\n[no documents retrieved from vector db]\r\nAI: I'm sorry, I could not understand your question. Can you rephrase it?\r\n```\r\nThis would happen since the last question has no meaning without previous history context, but with option (2) GPT would have context to decide how to answer."
      },
      {
        "user": "lgabs",
        "body": "Probably the default behavior should be the fixed fallback, and if the developer wants to delegate to GPT the answer using instructions from the prompt template, than he/she can make its own custom subclass of `AbstractLLM` dealing with desired behavior. "
      }
    ]
  },
  {
    "issue_number": 140,
    "title": "knowledge base behavior can create confusion",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-02-28T00:25:43Z",
    "updated_at": "2024-02-29T12:42:03Z",
    "labels": [
      "embedding"
    ],
    "body": "### Current Behavior\r\nCurrently, when starting the dialog service for the first time, it populates the vector store with embeddings from the specified CSV path. During this process, it creates a unique key for each CSV entry by concatenating certain column fields and applying a hash function to them. If the service is restarted after the CSV has been updated, it attempts to update only the entries with new hashes.\r\n\r\n### Issue\r\nThis behavior may be problematic because the CSV typically represents the entire knowledge base to be considered and should not be mixed with past entries already in the vector database during updates. For example, if we have an entry Q&A:\r\n\r\n```\r\nQuestion: Until when can I cancel my order?\r\nAnswer: You can cancel your order up to 3 hours before the trip.\r\n```\r\nand the team responsible for maintaining the knowledge base changes this to 1 hour, the new entry will be stored in the database along with the old information, resulting in duplicate and potentially conflicting entries.\r\n\r\n### Suggested Improvement\r\nUpon starting the dialog service, delete the existing collection) and create a new one with the contents of the current CSV. Providing an option for updating may be beneficial if retaining every entry indefinitely is not a problem. This would demand changes in `load_csv.py`.",
    "comments": [
      {
        "user": "avelino",
        "body": "today a **diff** is made of what is in the `CSV` (knowledge base) with the records that are in the database, keeping the existing ones and inserting the non-existent ones.\r\n\r\n**We don't *\"delete\"* the non-existent ones:** I want to remove the records that no longer exist in the CSV.\r\n\r\nWe could have an option in `load_csv.py` to reset the database before populating the database with the knowledge base (CSV).\r\n\r\n`--cleardb` might be a good name"
      }
    ]
  },
  {
    "issue_number": 138,
    "title": "ValueError raised when dialog tries to use fallback behavior",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-02-27T16:40:25Z",
    "updated_at": "2024-02-28T09:20:02Z",
    "labels": [],
    "body": "Asking something that falls into fallback raises an `ValueError`\r\n```\r\n➜  dialog git:(main) docker compose up dialog\r\n[+] Building 0.0s (0/0)                                                                                  docker:desktop-linux\r\n[+] Running 2/0\r\n ✔ Container dialog-db-1      Running                                                                                    0.0s\r\n ✔ Container dialog-dialog-1  Created                                                                                    0.0s\r\nAttaching to dialog-dialog-1\r\ndialog-dialog-1  | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\ndialog-dialog-1  | INFO  [alembic.runtime.migration] Will assume transactional DDL.\r\ndialog-dialog-1  | /app/src/load_csv.py:4: DeprecationWarning:\r\ndialog-dialog-1  | Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\r\ndialog-dialog-1  | (to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\r\ndialog-dialog-1  | but was not found to be installed on your system.\r\ndialog-dialog-1  | If this would cause problems for you,\r\ndialog-dialog-1  | please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\r\ndialog-dialog-1  |\r\ndialog-dialog-1  |   import pandas as pd\r\ndialog-dialog-1  | Generating embeddings for new questions...\r\ndialog-dialog-1  | New questions: 0\r\ndialog-dialog-1  | INFO:     Will watch for changes in these directories: ['/app/src']\r\ndialog-dialog-1  | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\ndialog-dialog-1  | INFO:     Started reloader process [44] using WatchFiles\r\ndialog-dialog-1  | INFO:     Started server process [46]\r\ndialog-dialog-1  | INFO:     Waiting for application startup.\r\ndialog-dialog-1  | INFO:     Application startup complete.\r\ndialog-dialog-1  | 2024-02-27 16:16:24,392 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\r\ndialog-dialog-1  | INFO:     192.168.65.1:34084 - \"POST /ask HTTP/1.1\" 500 Internal Server Error\r\ndialog-dialog-1  | ERROR:    Exception in ASGI application\r\ndialog-dialog-1  | Traceback (most recent call last):\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 426, in run_asgi\r\ndialog-dialog-1  |     result = await app(  # type: ignore[func-returns-value]\r\ndialog-dialog-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\r\ndialog-dialog-1  |     return await self.app(scope, receive, send)\r\ndialog-dialog-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\ndialog-dialog-1  |     await super().__call__(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\r\ndialog-dialog-1  |     await self.middleware_stack(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\ndialog-dialog-1  |     raise exc\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\ndialog-dialog-1  |     await self.app(scope, receive, _send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 91, in __call__\r\ndialog-dialog-1  |     await self.simple_response(scope, receive, send, request_headers=headers)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 146, in simple_response\r\ndialog-dialog-1  |     await self.app(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\ndialog-dialog-1  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\ndialog-dialog-1  |     raise exc\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\ndialog-dialog-1  |     await app(scope, receive, sender)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 758, in __call__\r\ndialog-dialog-1  |     await self.middleware_stack(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 778, in app\r\ndialog-dialog-1  |     await route.handle(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 299, in handle\r\ndialog-dialog-1  |     await self.app(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 79, in app\r\ndialog-dialog-1  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\ndialog-dialog-1  |     raise exc\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\ndialog-dialog-1  |     await app(scope, receive, sender)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\r\ndialog-dialog-1  |     response = await func(request)\r\ndialog-dialog-1  |                ^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 299, in app\r\ndialog-dialog-1  |     raise e\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 294, in app\r\ndialog-dialog-1  |     raw_response = await run_endpoint_function(\r\ndialog-dialog-1  |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\ndialog-dialog-1  |     return await dependant.call(**values)\r\ndialog-dialog-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/app/src/main.py\", line 93, in post_message_no_memory\r\ndialog-dialog-1  |     ai_message = llm_instance.process(message.message)\r\ndialog-dialog-1  |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/app/src/dialog/llm/abstract_llm.py\", line 81, in process\r\ndialog-dialog-1  |     self.generate_prompt(processed_input)\r\ndialog-dialog-1  |   File \"/app/src/dialog/llm/default.py\", line 46, in generate_prompt\r\ndialog-dialog-1  |     messages.append(SystemMessagePromptTemplate.from_template(fallback))\r\ndialog-dialog-1  |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/langchain_core/prompts/chat.py\", line 389, in from_template\r\ndialog-dialog-1  |     raise ValueError()\r\ndialog-dialog-1  | ValueError\r\n```\r\n### Suggested fix\r\nThis happens due to `.toml` configuration, where fallbacks lies into `[fallback]` section and `prompt` key, so the proper way of getting the fallback prompt here \r\n\r\nhttps://github.com/talkdai/dialog/blob/bd108f2916b52fb8769363e8ca7bea5703a925ae/src/dialog/llm/default.py#L33\r\n\r\nis with `fallback = self.config.get(\"fallback\").get(\"prompt\")`. ",
    "comments": []
  },
  {
    "issue_number": 136,
    "title": "UnboundLocalError: cannot access local variable 'session_uuid' where it is not associated with a value",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-02-26T23:22:55Z",
    "updated_at": "2024-02-27T19:28:50Z",
    "labels": [],
    "body": "After cloning the project and starting services, I'm getting an error when hitting the `/session/` for session creation:\r\n\r\n```\r\n➜  dialog git:(main) docker compose up dialog\r\n[+] Building 0.0s (0/0)                                                                                  docker:desktop-linux\r\n[+] Running 2/0\r\n ✔ Container dialog-db-1      Running                                                                                    0.0s\r\n ✔ Container dialog-dialog-1  Created                                                                                    0.0s\r\nAttaching to dialog-dialog-1\r\ndialog-dialog-1  | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\ndialog-dialog-1  | INFO  [alembic.runtime.migration] Will assume transactional DDL.\r\ndialog-dialog-1  | /app/src/load_csv.py:4: DeprecationWarning:\r\ndialog-dialog-1  | Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\r\ndialog-dialog-1  | (to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\r\ndialog-dialog-1  | but was not found to be installed on your system.\r\ndialog-dialog-1  | If this would cause problems for you,\r\ndialog-dialog-1  | please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\r\ndialog-dialog-1  |\r\ndialog-dialog-1  |   import pandas as pd\r\ndialog-dialog-1  | Generating embeddings for new questions...\r\ndialog-dialog-1  | New questions: 0\r\ndialog-dialog-1  | /app/src/load_csv.py:52: SettingWithCopyWarning:\r\ndialog-dialog-1  | A value is trying to be set on a copy of a slice from a DataFrame\r\ndialog-dialog-1  |\r\ndialog-dialog-1  | See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\r\ndialog-dialog-1  |   df_filtered.drop(columns=[\"primary_key\"], inplace=True)\r\ndialog-dialog-1  | INFO:     Will watch for changes in these directories: ['/app/src']\r\ndialog-dialog-1  | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\ndialog-dialog-1  | INFO:     Started reloader process [44] using WatchFiles\r\ndialog-dialog-1  | INFO:     Started server process [46]\r\ndialog-dialog-1  | INFO:     Waiting for application startup.\r\ndialog-dialog-1  | INFO:     Application startup complete.\r\ndialog-dialog-1  | INFO:     192.168.65.1:29982 - \"POST /session HTTP/1.1\" 500 Internal Server Error\r\ndialog-dialog-1  | ERROR:    Exception in ASGI application\r\ndialog-dialog-1  | Traceback (most recent call last):\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 426, in run_asgi\r\ndialog-dialog-1  |     result = await app(  # type: ignore[func-returns-value]\r\ndialog-dialog-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\r\ndialog-dialog-1  |     return await self.app(scope, receive, send)\r\ndialog-dialog-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\ndialog-dialog-1  |     await super().__call__(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\r\ndialog-dialog-1  |     await self.middleware_stack(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\ndialog-dialog-1  |     raise exc\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\ndialog-dialog-1  |     await self.app(scope, receive, _send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 91, in __call__\r\ndialog-dialog-1  |     await self.simple_response(scope, receive, send, request_headers=headers)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 146, in simple_response\r\ndialog-dialog-1  |     await self.app(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\ndialog-dialog-1  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\ndialog-dialog-1  |     raise exc\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\ndialog-dialog-1  |     await app(scope, receive, sender)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 758, in __call__\r\ndialog-dialog-1  |     await self.middleware_stack(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 778, in app\r\ndialog-dialog-1  |     await route.handle(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 299, in handle\r\ndialog-dialog-1  |     await self.app(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 79, in app\r\ndialog-dialog-1  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\ndialog-dialog-1  |     raise exc\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\ndialog-dialog-1  |     await app(scope, receive, sender)\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\r\ndialog-dialog-1  |     response = await func(request)\r\ndialog-dialog-1  |                ^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 299, in app\r\ndialog-dialog-1  |     raise e\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 294, in app\r\ndialog-dialog-1  |     raw_response = await run_endpoint_function(\r\ndialog-dialog-1  |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\ndialog-dialog-1  |     return await dependant.call(**values)\r\ndialog-dialog-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/app/src/main.py\", line 122, in create_session\r\ndialog-dialog-1  |     return db_create_session(identifier=identifier)\r\ndialog-dialog-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ndialog-dialog-1  |   File \"/app/src/dialog/models/helpers.py\", line 11, in create_session\r\ndialog-dialog-1  |     chat = session.query(ChatEntity).filter_by(uuid=session_uuid).first()\r\ndialog-dialog-1  |                                                     ^^^^^^^^^^^^\r\ndialog-dialog-1  | UnboundLocalError: cannot access local variable 'session_uuid' where it is not associated with a value\r\n```",
    "comments": []
  },
  {
    "issue_number": 133,
    "title": "A value is trying to be set on a copy of a slice from a DataFrame",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-02-24T18:11:11Z",
    "updated_at": "2024-02-26T13:52:57Z",
    "labels": [],
    "body": "When reunning dialog service, a pandas warning appears warning about changing a copy of a slice dataframe. To avoid this, we can explicitly declare df_filtered a copy.\r\n\r\n```\r\n➜  dialog git:(luan/adopt-lcel-pt1) ✗ docker compose up dialog\r\n[+] Building 0.0s (0/0)                                                                                  docker:desktop-linux\r\n[+] Running 2/2\r\n ✔ Container dialog-db-1      Running                                                                                    0.0s\r\n ✔ Container dialog-dialog-1  Created                                                                                    0.1s\r\nAttaching to dialog-dialog-1\r\ndialog-dialog-1  | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\ndialog-dialog-1  | INFO  [alembic.runtime.migration] Will assume transactional DDL.\r\ndialog-dialog-1  | /app/src/load_csv.py:4: DeprecationWarning:\r\ndialog-dialog-1  | Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\r\ndialog-dialog-1  | (to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\r\ndialog-dialog-1  | but was not found to be installed on your system.\r\ndialog-dialog-1  | If this would cause problems for you,\r\ndialog-dialog-1  | please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\r\ndialog-dialog-1  |\r\ndialog-dialog-1  |   import pandas as pd\r\ndialog-dialog-1  | Generating embeddings for new questions...\r\ndialog-dialog-1  | New questions: 0\r\ndialog-dialog-1  | /app/src/load_csv.py:52: SettingWithCopyWarning:\r\ndialog-dialog-1  | A value is trying to be set on a copy of a slice from a DataFrame\r\ndialog-dialog-1  |\r\ndialog-dialog-1  | See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\r\ndialog-dialog-1  |   df_filtered.drop(columns=[\"primary_key\"], inplace=True)\r\ndialog-dialog-1  | INFO:     Will watch for changes in these directories: ['/app/src']\r\ndialog-dialog-1  | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\ndialog-dialog-1  | INFO:     Started reloader process [44] using WatchFiles\r\ndialog-dialog-1  | INFO:     Started server process [46]\r\n```",
    "comments": []
  },
  {
    "issue_number": 134,
    "title": "Cosine distance misunderstanding",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-02-24T20:14:29Z",
    "updated_at": "2024-02-26T13:07:30Z",
    "labels": [],
    "body": "For cosine distance, 1 means maximum similarity, while 0 means decorrelation. That means we should be checking for documents with equal or higher cosine distance than a threshold, not lower than a threshold. This would invert the comparison in the code for retrieval:\r\n\r\nhttps://github.com/talkdai/dialog/blob/25d073122a58cc5707d18e3aafe3f33a18b96c97/src/dialog/llm/embeddings.py#L29\r\n\r\nFrom [wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity):\r\n> The resulting similarity ranges from -1 meaning exactly opposite, to 1 meaning exactly the same, with 0 indicating [orthogonality](https://en.wikipedia.org/wiki/Orthogonality) or [decorrelation](https://en.wikipedia.org/wiki/Decorrelation), while in-between values indicate intermediate similarity or dissimilarity.\r\n> For [text matching](https://en.wikipedia.org/wiki/Approximate_string_matching), the attribute vectors A and B are usually the [term frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) vectors of the documents. Cosine similarity can be seen as a method of [normalizing](https://en.wikipedia.org/wiki/Normalization_(statistics)) document length during comparison",
    "comments": [
      {
        "user": "schumannc",
        "body": "This is not true for pgvector implementation, equal vectors have cosine similarity of zero.  It possible to check the implementation using the following query:\r\n`SELECT id, content, embedding <=> (\r\n\tSELECT embedding FROM contents WHERE id = 22\r\n) v\r\n FROM contents WHERE id != 1\r\nORDER BY embedding <=> (\r\n\tSELECT embedding FROM contents WHERE id = 22\r\n) LIMIT 100`"
      },
      {
        "user": "lgabs",
        "body": "hmm sorry, that's true! Indeed these terms sometimes are confused. [Wikipedia](https://en.wikipedia.org/wiki/Cosine_similarity#:~:text=.-,Cosine%20distance,-%5Bedit%5D) also has a section detailing cosine distance as `1 - cosine_similarity` (so 0 is closer vectors), and [pgvector's repo](https://github.com/pgvector/pgvector#:~:text=For%20cosine%20similarity%2C%20use%201%20%2D%20cosine%20distance) also says that.\r\n"
      }
    ]
  },
  {
    "issue_number": 131,
    "title": "ERROR:  extension \"pgvector\" is not available",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-02-24T04:06:29Z",
    "updated_at": "2024-02-24T10:08:34Z",
    "labels": [],
    "body": "After reseting my local environment and cloning the project again, I'm getting an error from `db` service about the extension \"pgvector\" when running `docker compose up`. When running services separately, the trace is more clear (next time I'll copy the logs into the markdown, sorry):\r\n\r\n<img width=\"1035\" alt=\"image\" src=\"https://github.com/talkdai/dialog/assets/27077298/4674fb03-c103-4e5d-8554-89cc369e10f1\">\r\n\r\n",
    "comments": []
  },
  {
    "issue_number": 128,
    "title": "db container shows log FATAL:  role \"root\" does not exist",
    "author": "walison17",
    "state": "closed",
    "created_at": "2024-02-24T02:30:04Z",
    "updated_at": "2024-02-24T02:55:52Z",
    "labels": [],
    "body": "After running `docker compose up`, `db` container keeps printing the same error\r\n\r\n```shell\r\ndb-1  | 2024-02-24 02:25:28.803 UTC [1] LOG:  database system is ready to accept connections\r\ndb-1  | 2024-02-24 02:25:58.920 UTC [41] FATAL:  role \"root\" does not exist\r\n```",
    "comments": []
  },
  {
    "issue_number": 118,
    "title": "chat_history could be optional",
    "author": "schumannc",
    "state": "closed",
    "created_at": "2024-02-22T16:03:50Z",
    "updated_at": "2024-02-23T19:18:29Z",
    "labels": [],
    "body": "**Current Behavior:**\r\nThe existing workflow requires the client to interact with a session endpoint to generate or maintain a session_id before making requests to the chat endpoint. This session_id is used to retrieve chat_history in LLM memory. \r\n\r\n**Issue**\r\nFor clients operating within SaaS (Software as a Service) environments, initiating an additional call to the session endpoint before accessing the chat functionality may not be feasible or efficient. Furthermore, including chat_history as context in the QA (Question and Answer) prompts may not always enhance the interaction. In certain scenarios, this history can contribute to confusion, preventing the Large Language Model (LLM) from providing clear and direct answers.\r\n\r\n**Suggested Improvement:**\r\nIntroduce an alternative chat endpoint that does not require a session_id for operation. This endpoint would inherently disregard chat_history, focusing solely on the current prompt. ",
    "comments": []
  },
  {
    "issue_number": 120,
    "title": "Cosine distances for document retrieval",
    "author": "schumannc",
    "state": "closed",
    "created_at": "2024-02-22T16:18:40Z",
    "updated_at": "2024-02-23T19:10:16Z",
    "labels": [
      "enhancement",
      "LLM: ChatGPT"
    ],
    "body": "**Current Behavior:**\r\nThe function `get_most_relevant_contents_from_message` is designed to fetch documents from a database, ordering them based on the L2 score (Euclidean distance) of similarity with the customer's request. This retrieval is capped at the top N documents. The L2 score, an unbounded metric, varies significantly depending on the embeddings of the documents it analyzes. Currently, utilizing an L2 score filter of less than 1 can potentially always yield a document—even if the document lacks semantic relevance to the query. This behavior inadvertently bypasses the intended fallback mechanisms that should activate in the absence of meaningful answers within the database.\r\n\r\n**Issue:**\r\nThe core of the problem lies in the unbounded nature of the L2 distance metric, which leads to inconsistent filtering thresholds. This inconsistency can result in the retrieval of documents that do not semantically align with the customer's request, thereby impeding the system's ability to default to fallback options when genuinely relevant answers are not present in the database.\r\n\r\n**Suggested Improvement:**\r\nTransition the similarity metric from L2 (Euclidean distance) to cosine distance, which operates within a bounded range of 0 to 1. This change would normalize the comparison scale, ensuring that document similarity scores are consistently interpretable. Additionally, introducing a configurable parameter for the similarity threshold would provide greater flexibility, allowing for fine-tuning of the retrieval process based on specific needs or contexts. ",
    "comments": [
      {
        "user": "lgabs",
        "body": "I also think that cosine distance should be the default behavior since it's usually better in NLP use cases. Maybe in the future the search type itself should be a parameter to be defined in the `.toml` with parameters just as llm parameters are configurable there, to allow different tests."
      }
    ]
  },
  {
    "issue_number": 122,
    "title": "Add Langsmith env variables into env sample",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2024-02-22T20:14:15Z",
    "updated_at": "2024-02-23T09:29:42Z",
    "labels": [
      "documentation",
      "LLM: ChatGPT"
    ],
    "body": "[Langsmith](https://docs.smith.langchain.com/) is [a recent but powerful](https://blog.langchain.dev/langsmith-ga/) for observability of llm applications, and it's currently available for public for personal use. It's easy to integrate with Langchain with just some env vars added to the application (it's good to use LCEL to guarantee full observability), so it's just the case of adding this example to the `.env.sample` file.",
    "comments": []
  },
  {
    "issue_number": 119,
    "title": "Static folder does not exist in docker image",
    "author": "walison17",
    "state": "closed",
    "created_at": "2024-02-22T16:13:09Z",
    "updated_at": "2024-02-22T19:08:39Z",
    "labels": [],
    "body": "```shell \r\nFile \"/app/src/main.py\", line 52, in <module>\r\n    app.mount(\"/static\", StaticFiles(directory=STATIC_FILE_LOCATION), name=\"static\")\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/starlette/staticfiles.py\", line 59, in __init__\r\n    raise RuntimeError(f\"Directory '{directory}' does not exist\")\r\nRuntimeError: Directory '/app/static' does not exist\r\n```",
    "comments": []
  },
  {
    "issue_number": 85,
    "title": "Adds support for custom library installation on Plugins",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-12-28T19:47:45Z",
    "updated_at": "2024-02-19T02:28:43Z",
    "labels": [],
    "body": "We need to enable plugins to have its own requirements.txt, enabling better dependency handling and not editing the main dialog container file.",
    "comments": [
      {
        "user": "walison17",
        "body": "I have a proposal for this\r\n\r\nAll dialog plugins must be a valid python package published on pypi with some entrypoint metadata (like pytest plugins). The `PLUGINS`  env variable will receive a list of installable plugins and all of them will be installed and loaded at runtime "
      }
    ]
  },
  {
    "issue_number": 98,
    "title": "AWarning: Did not recognize type 'vector' of column 'embedding'",
    "author": "avelino",
    "state": "open",
    "created_at": "2024-02-15T17:52:47Z",
    "updated_at": "2024-02-17T20:19:35Z",
    "labels": [
      "bug"
    ],
    "body": "```log\r\n/Users/avelino/projects/avelino/talkd/dialog/src/dialog/models/__init__.py:34: SAWarning: Did not recognize type 'vector' of column 'embedding'\r\n  __table__ = Table(\"contents\", metadata, autoload_with=engine)\r\nTraceback (most recent call last):\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3790, in get_loc\r\n    return self._engine.get_loc(casted_key)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"index.pyx\", line 152, in pandas._libs.index.IndexEngine.get_loc\r\n  File \"index.pyx\", line 181, in pandas._libs.index.IndexEngine.get_loc\r\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\nKeyError: 'primary_key'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/src/load_csv.py\", line 65, in <module>\r\n    load_csv_and_generate_embeddings(args.path)\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/src/load_csv.py\", line 44, in load_csv_and_generate_embeddings\r\n    new_keys = set(df[\"primary_key\"]) - set(df_in_db[\"primary_key\"])\r\n                                            ~~~~~~~~^^^^^^^^^^^^^^^\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/.venv/lib/python3.11/site-packages/pandas/core/frame.py\", line 3893, in __getitem__\r\n    indexer = self.columns.get_loc(key)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3797, in get_loc\r\n    raise KeyError(key) from err\r\nKeyError: 'primary_key'\r\n```",
    "comments": [
      {
        "user": "vmesel",
        "body": "@avelino have you ran the migrations?"
      },
      {
        "user": "avelino",
        "body": "> have you ran the migrations?\r\n\r\n@vmesel yep"
      }
    ]
  },
  {
    "issue_number": 97,
    "title": "langchain version upgrade, to `+v0.1.7`",
    "author": "avelino",
    "state": "closed",
    "created_at": "2024-02-15T16:37:19Z",
    "updated_at": "2024-02-16T22:41:30Z",
    "labels": [
      "enhancement",
      "devx",
      "dependencies"
    ],
    "body": "Today we are using version `^0.0.3333` of LangChain, and we need to update to the latest version. This update may potentially break many implementations made in the dialog.\r\n\r\n> It is a ~\"risk\"~ that comes with using a library developed by a large community. ",
    "comments": []
  },
  {
    "issue_number": 99,
    "title": "empty database is not populating the db",
    "author": "avelino",
    "state": "closed",
    "created_at": "2024-02-15T18:01:17Z",
    "updated_at": "2024-02-15T18:28:17Z",
    "labels": [
      "bug"
    ],
    "body": "```log\r\nTraceback (most recent call last):\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3790, in get_loc\r\n    return self._engine.get_loc(casted_key)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"index.pyx\", line 152, in pandas._libs.index.IndexEngine.get_loc\r\n  File \"index.pyx\", line 181, in pandas._libs.index.IndexEngine.get_loc\r\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\nKeyError: 'primary_key'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/src/load_csv.py\", line 69, in <module>\r\n    load_csv_and_generate_embeddings(args.path)\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/src/load_csv.py\", line 48, in load_csv_and_generate_embeddings\r\n    new_keys = set(df[\"primary_key\"]) - set(df_in_db[\"primary_key\"])\r\n                                            ~~~~~~~~^^^^^^^^^^^^^^^\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/.venv/lib/python3.11/site-packages/pandas/core/frame.py\", line 3893, in __getitem__\r\n    indexer = self.columns.get_loc(key)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3797, in get_loc\r\n    raise KeyError(key) from err\r\nKeyError: 'primary_key'\r\n```",
    "comments": []
  },
  {
    "issue_number": 87,
    "title": "plugin: error initial load",
    "author": "avelino",
    "state": "closed",
    "created_at": "2024-02-01T14:24:52Z",
    "updated_at": "2024-02-09T15:58:18Z",
    "labels": [
      "bug",
      "documentation"
    ],
    "body": "We're having a problem uploading the dialog (api), it seems to me that our \"documentation\" _(`README.md`, `toml` configuration topic)_ is out of date.\r\n\r\n**prompt:**\r\n```toml\r\n[prompt]\r\nheader = \"\"\"Você é um operador de atendimento chamada Lerolero\"\"\"\r\n\r\nsuggested = \"Aqui está um possível conteúdo que pode ajudar o usuário de uma melhor forma.\"\r\n```\r\n\r\n**backtrace:**\r\n```log\r\n  File \"/usr/local/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.p\r\ny\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/src/main.py\", line 6, in <module>\r\n    from dialog.llm import get_llm_class\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/src/dialog/llm/__init__.py\", line 6, in <module>\r\n    from .default import DialogLLM\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/src/dialog/llm/default.py\", line 6, in <module>\r\n    from dialog.llm.memory import generate_memory_instance\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/src/dialog/llm/memory.py\", line 4, in <module>\r\n    from dialog.models import Chat, ChatMessages\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/src/dialog/models/__init__.py\", line 3, in <module>\r\n    from .db import engine, Base\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/src/dialog/models/db.py\", line 1, in <module>\r\n    from dialog.settings import DATABASE_URL\r\n  File \"/Users/avelino/projects/avelino/talkd/dialog/src/dialog/settings.py\", line 16, in <module>\r\n    PLUGINS = config(\"PLUGINS\", cast=Csv(), default=[])\r\n...\r\n...\r\n...\r\nAttributeError: 'list' object has no attribute 'read'\r\n```",
    "comments": [
      {
        "user": "vmesel",
        "body": "@avelino i will take a look"
      },
      {
        "user": "vmesel",
        "body": "@avelino can you share the `.env` content without api keys pls?"
      },
      {
        "user": "avelino",
        "body": "> can you share the `.env` content without api keys pls?\r\n\r\n@vmesel \r\n\r\n```toml\r\nPORT=8000\r\nOPENAI_API_KEY=\r\nVERBOSE_LLM=True\r\nDIALOG_DATA_PATH=data/buser.csv\r\nPROJECT_CONFIG=data/buser.toml\r\nDATABASE_URL=postgresql://talkdai:talkdai@db:5432/talkdai\r\n```\r\n\r\n"
      },
      {
        "user": "walison17",
        "body": "#92 solves this"
      }
    ]
  },
  {
    "issue_number": 89,
    "title": "Broken docker image",
    "author": "walison17",
    "state": "closed",
    "created_at": "2024-02-07T19:16:32Z",
    "updated_at": "2024-02-09T13:05:45Z",
    "labels": [],
    "body": "When I try to run a container with the latest docker image available on ghrc I have the following error\r\n\r\n```shell\r\ndocker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"/app/etc/run.sh\": stat /app/etc/run.sh: no such file or directory: unknown.\r\n```  ",
    "comments": []
  },
  {
    "issue_number": 90,
    "title": "Customizable CORS configurations",
    "author": "walison17",
    "state": "closed",
    "created_at": "2024-02-08T04:11:15Z",
    "updated_at": "2024-02-09T13:02:41Z",
    "labels": [],
    "body": "I think that we can have some CORS configurations on `settings.py` like  `CORS_ALLOW_ORIGINS`, `CORS_ALLOW_METHODS` and `CORS_ALLOW_HEADERS`.",
    "comments": []
  },
  {
    "issue_number": 91,
    "title": "Broken PLUGINS default config",
    "author": "walison17",
    "state": "closed",
    "created_at": "2024-02-08T04:21:21Z",
    "updated_at": "2024-02-09T13:02:18Z",
    "labels": [],
    "body": "When I try to run the application with default `PLUGINS` config I receive the following error:\r\n\r\n```shell\r\n File \"/app/src/settings.py\", line 16, in <module>\r\n    PLUGINS = config(\"PLUGINS\", cast=Csv(), default=[])\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/decouple.py\", line 248, in __call__\r\n    return self.config(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/decouple.py\", line 107, in __call__\r\n    return self.get(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/decouple.py\", line 101, in get\r\n    return cast(value)\r\n           ^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/decouple.py\", line 286, in __call__\r\n    return self.post_process(transform(s) for s in splitter)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/decouple.py\", line 286, in <genexpr>\r\n    return self.post_process(transform(s) for s in splitter)\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/shlex.py\", line 300, in __next__\r\n    token = self.get_token()\r\n            ^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/shlex.py\", line 109, in get_token\r\n    raw = self.read_token()\r\n          ^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/shlex.py\", line 140, in read_token\r\n    nextchar = self.instream.read(1)\r\n               ^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'list' object has no attribute 'read'\r\n```",
    "comments": []
  },
  {
    "issue_number": 93,
    "title": "Broker run.sh",
    "author": "walison17",
    "state": "closed",
    "created_at": "2024-02-08T14:58:04Z",
    "updated_at": "2024-02-08T19:36:12Z",
    "labels": [],
    "body": "When I run `docker compose up` I receive the following error: \r\n\r\n```shell\r\netc/run.sh: line 6: : command not found\r\netc/run.sh: line 11: : command not found\r\n```\r\n\r\nThe problem in script happens because the condition `if \"${ENVVAR}\"; then` attempts to execute the content of the variable `ENVVAR` as a command. ",
    "comments": []
  },
  {
    "issue_number": 84,
    "title": "Add support for multi-dataset data",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-12-27T13:18:13Z",
    "updated_at": "2024-01-10T20:37:12Z",
    "labels": [],
    "body": "Now, if you want to load data into dialog, you must use one single CSV that is the source of truth from the table you are loading, we could add a new approach enabling multiple CSVs, ie., so we can split datasets into FAQ, Technical Questions, Historical Questions and etc.",
    "comments": []
  },
  {
    "issue_number": 81,
    "title": "Split WhatsApp processor to a Plugin",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-12-21T19:48:45Z",
    "updated_at": "2023-12-23T16:15:15Z",
    "labels": [],
    "body": "Make whatsapp requests an external API pluggable by the plugin.",
    "comments": [
      {
        "user": "vmesel",
        "body": "Closed by #80 "
      }
    ]
  },
  {
    "issue_number": 46,
    "title": "`process_user_intent`: this is very logical",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-23T00:31:39Z",
    "updated_at": "2023-12-23T16:09:05Z",
    "labels": [
      "enhancement"
    ],
    "body": "`llm/__init__` needs to be refactored, thinking about the API and how it will be consumed, removing as much cognitive load as possible (isolating responsibilities).",
    "comments": [
      {
        "user": "vmesel",
        "body": "We need to make the LLM instance a class, so we can make it extensible via plugins. I'm going to think on a structure to do that, so we can make it extensible on the settings.py.\r\n\r\nAlso, if the new LLM needs more parameters, we need to be able to fetch them from the settings.py or environment variable. I dont know which is the best approach, but its doable in both ways."
      },
      {
        "user": "vmesel",
        "body": "Closed by #80, and may require some changes, but its better than it was"
      }
    ]
  },
  {
    "issue_number": 77,
    "title": "Add plugins support to talkd dialog",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-12-11T18:37:38Z",
    "updated_at": "2023-12-23T16:08:32Z",
    "labels": [],
    "body": "We should be able to make dialog extensible without messing around the original code. We can set this as a dependency injection and setting the original router to support it as importing modules. This needs to be carefully set so we don't open any security breaches such as evals and stuff.\r\n\r\nMaybe we could use importlib as described: https://stackoverflow.com/questions/64916281/importing-modules-dynamically-in-python-3-x",
    "comments": [
      {
        "user": "vmesel",
        "body": "Closed and stabilized by #80 "
      }
    ]
  },
  {
    "issue_number": 79,
    "title": "Extensible LLM template",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-12-19T22:59:00Z",
    "updated_at": "2023-12-23T16:08:13Z",
    "labels": [],
    "body": "I'm working currently on a way that we can extend and deploy any LLM on Dialog API without having to reimplement the code and touch the existing code base.\r\n\r\nThe approach I'm aiming is basically an Abstract class with some default methods. Example:\r\n\r\n```python\r\nfrom typing import List, Dict\r\n\r\nfrom langchain.chains.llm import LLMChain\r\nfrom langchain.memory.chat_memory import BaseChatMemory\r\nfrom langchain.prompts import ChatPromptTemplate\r\n\r\nclass AbstractLLM:\r\n    def __init__(self, config):\r\n        \"\"\"\r\n        :param config: Configuration dictionary\r\n\r\n        The constructor of the AbstractLLM class allows users to pass\r\n        a configuration dictionary to the LLM. This configuration dictionary\r\n        can be used to configure the LLM temperature, prompt and other\r\n        necessities.\r\n        \"\"\"\r\n        if config is None or not isinstance(config, dict):\r\n            raise ValueError(\"Config must be a dictionary\")\r\n\r\n        self.config = config\r\n        self.prompt = None\r\n\r\n    def get_prompt(self, input) -> ChatPromptTemplate:\r\n        \"\"\"\r\n        Function that generates the prompt for the LLM.\r\n        \"\"\"\r\n        raise NotImplementedError(\"Prompt must be implemented\")\r\n\r\n    @property\r\n    def memory(self) -> BaseChatMemory:\r\n        \"\"\"\r\n        Returns the memory instance\r\n        \"\"\"\r\n        raise NotImplementedError(\"Memory must be implemented\")\r\n\r\n    @property\r\n    def llm(self) -> LLMChain:\r\n        \"\"\"\r\n        Returns the LLM instance. If a memory instance is provided,\r\n        the LLM instance should be initialized with the memory instance.\r\n\r\n        If no memory instance is provided, the LLM instance should be\r\n        initialized without a memory instance.\r\n        \"\"\"\r\n        raise NotImplementedError(\"LLM must be implemented\")\r\n\r\n    def preprocess(self, input: str) -> str:\r\n        \"\"\"\r\n        Function that pre-process the LLM input, enabling users\r\n        to modify the input before it is processed by the LLM.\r\n\r\n        This can be used to add context or prefixes to the LLM.\r\n        \"\"\"\r\n        return input\r\n\r\n    def generate_prompt(self, input: str) -> str:\r\n        \"\"\"\r\n        Function that generates the prompt using PromptTemplate for the LLM.\r\n        \"\"\"\r\n        return input\r\n\r\n    def postprocess(self, input: str) -> str:\r\n        \"\"\"\r\n        Function that post-process the LLM output, enabling users\r\n        to modify the output before it is returned to the user.\r\n        \"\"\"\r\n        return input\r\n\r\n    def process(self, input: str):\r\n        \"\"\"\r\n        Function that encapsulates the pre-processing, processing and post-processing\r\n        of the LLM.\r\n        \"\"\"\r\n        processed_input = self.preprocess(input)\r\n        output = self.llm({\r\n            \"user_message\": processed_input,\r\n        })\r\n        processed_output = self.postprocess(output)\r\n        return processed_output\r\n\r\n```\r\n\r\nAnd our LLM as an example (it's not yet tested):\r\n\r\n```python\r\nfrom llm.llm import AbstractLLM\r\nfrom llm.memory import generate_memory_instance\r\n\r\nfrom typing import List\r\n\r\nfrom langchain.chains.llm import LLMChain\r\nfrom langchain.memory.chat_memory import BaseChatMemory\r\n\r\nfrom typing import List\r\n\r\nfrom langchain.chains import LLMChain\r\nfrom langchain.chat_models import ChatOpenAI\r\nfrom langchain.memory import ConversationBufferWindowMemory\r\nfrom langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,\r\n                               MessagesPlaceholder,\r\n                               SystemMessagePromptTemplate)\r\nfrom sqlalchemy import select\r\n\r\nfrom learn.idf import categorize_conversation_history\r\nfrom llm.memory import generate_memory_instance\r\nfrom models import CompanyContent\r\nfrom models.db import session\r\nfrom settings import PROMPT, EMBEDDINGS_LLM, FALLBACK_PROMPT\r\n\r\n\r\ndef generate_embeddings(documents: List[str]):\r\n    \"\"\"\r\n    Generate embeddings for a list of documents\r\n    \"\"\"\r\n    return EMBEDDINGS_LLM.embed_documents(documents)\r\n\r\ndef generate_embedding(document: str):\r\n    \"\"\"\r\n    Generate embeddings for a single instance of document\r\n    \"\"\"\r\n    return EMBEDDINGS_LLM.embed_query(document)\r\n\r\ndef get_most_relevant_contents_from_message(message, top=5):\r\n    message_embedding = generate_embedding(message)\r\n    possible_contents = session.scalars(\r\n        select(CompanyContent)\r\n        .filter(CompanyContent.embedding.l2_distance(message_embedding) < 1)\r\n        .order_by(CompanyContent.embedding.l2_distance(message_embedding).asc())\r\n        .limit(top)\r\n    ).all()\r\n    return possible_contents\r\n\r\n\r\nclass DialogLLM(AbstractLLM):\r\n    @property\r\n    def memory(self) -> BaseChatMemory:\r\n        if self.config.get(\"session_id\"):\r\n            return generate_memory_instance(\r\n                session_id=self.config.get(\"session_id\"),\r\n                parent_session_id=self.config.get(\"parent_session_id\")\r\n            )\r\n        return None\r\n\r\n    def preprocess(self, input: str) -> str:\r\n        \"\"\"\r\n        Function that pre-process the LLM input, enabling users\r\n        to modify the input before it is processed by the LLM.\r\n\r\n        This can be used to add context or prefixes to the LLM.\r\n        \"\"\"\r\n        return input\r\n\r\n\r\n    def generate_prompt(self):\r\n        relevant_contents = get_most_relevant_contents_from_message(input, top=1)\r\n\r\n        if len(relevant_contents) == 0:\r\n            prompt_templating = [\r\n                SystemMessagePromptTemplate.from_template(FALLBACK_PROMPT),\r\n                HumanMessagePromptTemplate.from_template(\"{user_message}\"),\r\n            ]\r\n            relevant_contents = []\r\n        else:\r\n            suggested_content = \"Contexto: \\n\".join(\r\n                [f\"{c.question}\\n{c.content}\\n\" for c in relevant_contents]\r\n            )\r\n\r\n            prompt_templating = [\r\n                SystemMessagePromptTemplate.from_template(PROMPT.get(\"header\")),\r\n                MessagesPlaceholder(variable_name=\"chat_history\"),\r\n            ]\r\n\r\n        if len(relevant_contents) > 0:\r\n            prompt_templating.append(\r\n                SystemMessagePromptTemplate.from_template(\r\n                    f\"{PROMPT.get('suggested')}. {suggested_content}\"\r\n                )\r\n            )\r\n\r\n        question_text = PROMPT.get(\"question_signalizer\")\r\n        prompt_templating.append(HumanMessagePromptTemplate.from_template(f\"{question_text}\" + \":\\n{user_message}\"))\r\n\r\n        self.prompt = ChatPromptTemplate(messages=prompt_templating)\r\n\r\n    @property\r\n    def llm_chain(self) -> LLMChain:\r\n        llm_config = {\r\n            k: v\r\n            for k, v in self.config.items()\r\n            if k in [\"openai_api_key\", \"model_name\", \"temperature\"]\r\n        }\r\n        conversation_options ={\r\n            \"llm\": ChatOpenAI(\r\n                **llm_config\r\n            ),\r\n            \"prompt\": self.prompt,\r\n            \"verbose\": self.config.get(\"verbose\", False)\r\n        }\r\n\r\n        if self.memory:\r\n            buffer_config = {\r\n                \"chat_memory\": self.memory,\r\n                \"memory_key\": \"chat_history\",\r\n                \"return_messages\": True,\r\n                \"k\": self.config.get(\"memory_size\", 5)\r\n            }\r\n            conversation_options[\"memory\"] = ConversationBufferWindowMemory(\r\n                **buffer_config\r\n            )\r\n\r\n        return LLMChain(**conversation_options)\r\n\r\n    def postprocess(self, input: str) -> str:\r\n        \"\"\"\r\n        Function that post-process the LLM output, enabling users\r\n        to modify the output before it is returned to the user.\r\n        \"\"\"\r\n        return input\r\n\r\n    def process(self, input: str):\r\n        \"\"\"\r\n        Function that encapsulates the pre-processing, processing and post-processing\r\n        of the LLM.\r\n        \"\"\"\r\n        processed_input = self.preprocess(input)\r\n        self.generate_prompt(processed_input)\r\n        output = self.llm_chain({\r\n            \"user_message\": processed_input,\r\n        })\r\n        processed_output = self.postprocess(output)\r\n        return processed_output\r\n```\r\n\r\n",
    "comments": [
      {
        "user": "vmesel",
        "body": "Closed by #80 "
      }
    ]
  },
  {
    "issue_number": 63,
    "title": "whatsapp support",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-26T11:01:25Z",
    "updated_at": "2023-12-07T12:02:29Z",
    "labels": [
      "API: WebHooks"
    ],
    "body": "[WhatsApp documentation](https://developers.facebook.com/docs/whatsapp/cloud-api/guides/set-up-webhooks#set-up-webhooks)\r\n\r\n**EPIC:** #37\r\n\r\n```json\r\n{\r\n  \"object\": \"whatsapp_business_account\",\r\n  \"entry\": [{\r\n      \"id\": \"WHATSAPP_BUSINESS_ACCOUNT_ID\",\r\n      \"changes\": [{\r\n          \"value\": {\r\n              \"messaging_product\": \"whatsapp\",\r\n              \"metadata\": {\r\n                  \"display_phone_number\": \"PHONE_NUMBER\",\r\n                  \"phone_number_id\": \"PHONE_NUMBER_ID\"\r\n              },\r\n              # specific Webhooks payload            \r\n          },\r\n          \"field\": \"messages\"\r\n        }]\r\n    }]\r\n}\r\n```\r\n\r\n> Webhooks payloads can be up to 3MB.\r\n\r\n[more exemples](https://developers.facebook.com/docs/whatsapp/cloud-api/webhooks/payload-examples#unknown-messages)",
    "comments": [
      {
        "user": "vmesel",
        "body": "Developing the API for external integrations\r\n\r\nPrimary formats to be supported:\r\n\r\n - WhatsApp Webhook\r\n - OpenAI Chat Completion API\r\n\r\nIm thinking of implementing these two services in a different router than the main that we are using right now. In this router, we will keep all of the external integrations that we support.\r\n\r\n```python\r\n# external_apis.py\r\n\r\nfrom fastapi import APIRouter\r\n\r\nrouter = APIRouter()\r\n\r\nSERIALIZERS = {\r\n    \"whatsapp\": WhatsAppSerializer,\r\n    \"openai\": OpenAISerializer\r\n}\r\n\r\n@router.post(\"/whatsapp\")\r\nasync def message_webhook():\r\n    service = detect_service(request) # gets where request is from\r\n    serializer = SERIALIZERS.get(service) # gets the serializer for the service\r\n    data = serializer(request) # serializes the request\r\n    # process the data and send a response to the webhook\r\n    response_serializer = SERIALIZERS.get(service)\r\n    response = response_serializer(data)\r\n    return response\r\n\r\n\r\n@router.post(\"/api/v1/\")\r\nasync def openai_style_chat_completion():\r\n    \"\"\"\r\n    Receives a request with the format:\r\n    {\r\n        \"model\": \"davinci\",\r\n        \"response_format\": {\r\n            \"type\": \"json_object\"\r\n        },\r\n        \"messages\": [\r\n            {\r\n                \"content\": \"Hello, how are you?\",\r\n                \"role\": \"user\"\r\n            }\r\n        ]\r\n    }\r\n    \"\"\"\r\n    messages = request.json.get(\"messages\", [])\r\n    processed_content = llm_function(data[])\r\n    processed_response = {\r\n    \"choices\": [\r\n        {\r\n            \"finish_reason\": \"stop\",\r\n            \"index\": 0,\r\n            \"message\": {\r\n                \"content\": \"{}\",\r\n                \"role\": \"assistant\"\r\n            }\r\n        }]\r\n    }\r\n    pass\r\n\r\n# main.py\r\n\r\nfrom external_apis import router as external_apis_router\r\n\r\napp.include_router(external_apis_router, prefix=\"/api/v1/\")\r\n```\r\n\r\n## messages in - Webhook\r\n\r\nI'm planning to implement all webhooks with the same function, just changing which serializer we are going to use to process the request, as the same function in the middle is going to be used.\r\n\r\nSo for every service we are going to integrate, we are sending a request to the same endpoint, and the only thing that changes is the serializer that we are going to use to process the request. This is going to be done in the `external_apis.py` file.\r\n\r\n## messages out - Webhook\r\n\r\nThe same thing is going to happen with the messages out, we are going to have a serializer for each service that we are going to integrate, and we are going to use the same function to send the message to the webhook.\r\n\r\n## openai style api\r\n\r\nAs we are going to try to emulate the OpenAI API, we are going to have a serializer for the request and another one for the response. The request serializer is going to be used to process the request and the response serializer is going to be used to process the response.\r\n\r\nThere are some issues with their existing response, because they mention tokens and other data that we supposedly dont have access through our Langchain usage (we might need to hack it a little bit)."
      }
    ]
  },
  {
    "issue_number": 52,
    "title": "Add setup for dev container ",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2023-11-23T16:37:59Z",
    "updated_at": "2023-12-02T12:30:28Z",
    "labels": [
      "devx"
    ],
    "body": "Dev Container is very helpful for those who prefer VS Code :) ",
    "comments": [
      {
        "user": "vmesel",
        "body": "@lgabs can you send us an example on how to use a Dev Container? Never used it before on VSCode."
      },
      {
        "user": "lgabs",
        "body": "A made a simple example referencing the Dockerfile, but with the usage of docker compose I'm not sure how the config should be to make a good setup for dev, since [it seems](https://stackoverflow.com/questions/67773973/how-to-run-docker-compose-inside-vs-code-devcontainer) that vs code can only use one service as the \"workspace\" for the IDE. Maybe @avelino can help here.\r\n\r\n```\r\n{\r\n\t\"name\": \"Existing Dockerfile\",\r\n\t\"build\": {\r\n\t\t\"context\": \"..\",\r\n\t\t\"dockerfile\": \"../Dockerfile\"\r\n\t}\r\n}\r\n```\r\n\r\nhttps://code.visualstudio.com/docs/devcontainers/containers#_getting-started"
      },
      {
        "user": "avelino",
        "body": "@lgabs implemented, when you can test it please\r\n\r\nyou must open the local vscode with the [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers), the popup will appear asking if you want to open the project\r\n\r\n![Screenshot 2023-12-02 at 09 30 08](https://github.com/talkdai/dialog/assets/31996/88713587-3375-46fc-8b24-fab92e8b4885)"
      }
    ]
  },
  {
    "issue_number": 67,
    "title": "Add docs on alembic migration ",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-11-27T17:21:34Z",
    "updated_at": "2023-11-28T00:45:58Z",
    "labels": [],
    "body": "Add docs on how to run migrations manually.",
    "comments": [
      {
        "user": "vmesel",
        "body": "Disregarding this issue, because I've already added on the initialization of the migrations commit."
      }
    ]
  },
  {
    "issue_number": 64,
    "title": "append memory to the prompt should be optional, is added by default",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-26T11:17:49Z",
    "updated_at": "2023-11-27T17:38:31Z",
    "labels": [
      "LLM: ChatGPT",
      "config"
    ],
    "body": "**related:** #47\r\n\r\nThe longer the prompt, the slower OpenAI's response time.\r\nNot in all cases do we want to pass all the session memory to the prompt\r\n\r\n**I'm thinking of two solutions in configuration (`toml`):**\r\n\r\n- memory in prompt\r\n- last X messages at the prompt (not the entire history)",
    "comments": [
      {
        "user": "walison17",
        "body": "We can use `ConversationBufferWindowMemory` and make the window size configurable via settings "
      }
    ]
  },
  {
    "issue_number": 40,
    "title": "Add link to data",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-11-22T13:09:28Z",
    "updated_at": "2023-11-25T15:24:11Z",
    "labels": [
      "enhancement",
      "good first issue",
      "LLM: ChatGPT",
      "Prompt Engineering"
    ],
    "body": "We need to support links on the embeddings table as a way of us sending a reference to the user so they can read more about one issue/article.",
    "comments": [
      {
        "user": "lgabs",
        "body": "Very interesting! You mean like answering a FAQ question and then give a link for the company's FAQ website? Maybe new fields in the csv would solve this?"
      },
      {
        "user": "vmesel",
        "body": "@lgabs yep! exactly like that! If we add a new field called `link` in the CSV and we add it on the `load_csv` and to the table, we would be able to do a prompt engineering on this and enable it to be used."
      }
    ]
  },
  {
    "issue_number": 48,
    "title": "Suggest the \"data\" folder to store .toml and knowledge base",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2023-11-23T14:21:09Z",
    "updated_at": "2023-11-25T15:24:08Z",
    "labels": [
      "documentation",
      "devx"
    ],
    "body": "In the project's root directory, should we indicate in the README that .toml and the knowledge base (.csv) should be in a `data/` folder? It's already ignored by [.gitignore](https://github.com/talkdai/dialog/blob/main/.gitignore#L161) to avoid their versioning.",
    "comments": [
      {
        "user": "vmesel",
        "body": "@lgabs we can suggest it in the README for sure, but its not a must to be there. The user can use another structure for storing their data."
      }
    ]
  },
  {
    "issue_number": 42,
    "title": "Create migrations for DB",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-11-22T22:55:52Z",
    "updated_at": "2023-11-25T15:24:05Z",
    "labels": [
      "enhancement"
    ],
    "body": "We need a database migration service so we can deploy changes to the table without dropping them.",
    "comments": []
  },
  {
    "issue_number": 53,
    "title": "Add response time into logs",
    "author": "lgabs",
    "state": "closed",
    "created_at": "2023-11-23T16:49:40Z",
    "updated_at": "2023-11-25T15:20:18Z",
    "labels": [
      "API",
      "devx"
    ],
    "body": "Large prompts or extensive documents (knowledge base) can significantly lengthen the LLM's response time for certain tasks, and it would be helpful to observe this response time in the terminal while running uvicorn. It will be common for a developer to try different prompts to check the rensponse time.",
    "comments": []
  },
  {
    "issue_number": 60,
    "title": "Classify conversations with LLMs",
    "author": "lgabs",
    "state": "open",
    "created_at": "2023-11-24T20:44:07Z",
    "updated_at": "2023-11-24T23:38:35Z",
    "labels": [
      "AI"
    ],
    "body": "Currenty, [we generate keywords](https://github.com/talkdai/dialog/blob/main/src/learn/idf.py) from the conversation as a start point to classify the conversation. The classification of conversation can have several downstream uses, such as support allocation, prioritization of emergencies, and understanding what topics drive your support, etc.\r\n\r\nLLMs are also good at classification, and we can take advantage of [langchain's implementation](https://python.langchain.com/docs/use_cases/tagging) for tagging documents. The ideia is to allow the user to specify a prompt and tags description, and then at each request an async task will tag the conversion as we already do [here](https://github.com/talkdai/dialog/blob/main/src/llm/__init__.py#L123).",
    "comments": [
      {
        "user": "lgabs",
        "body": "I've done some experiments with this before, so I can take this issue."
      }
    ]
  },
  {
    "issue_number": 31,
    "title": "model in toml",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-21T21:49:01Z",
    "updated_at": "2023-11-24T21:27:01Z",
    "labels": [
      "good first issue",
      "LLM: ChatGPT",
      "config"
    ],
    "body": "The model is defined in hard code `gpt-3.5-turbo`, we need to take it to the configuration (in TOML), making the project customizable\r\n\r\nhttps://github.com/talkdai/dialog/blob/012c73c02b30382d3dd95068dd0255e1242d8789/src/llm/__init__.py#L22\r\n\r\n```toml\r\n[prompt]\r\nmodel_name = \"gpt-3.5-turbo\"\r\n```\r\n\r\n> if not set, it should be model `gpt-3.5-turbo` by default",
    "comments": []
  },
  {
    "issue_number": 32,
    "title": "temperature in toml",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-21T21:50:35Z",
    "updated_at": "2023-11-24T11:59:55Z",
    "labels": [
      "good first issue",
      "LLM: ChatGPT",
      "config"
    ],
    "body": "The temperature is defined in hard code `0.2`, we need to take it to the configuration (in TOML), making the project customizable\r\n\r\nhttps://github.com/talkdai/dialog/blob/012c73c02b30382d3dd95068dd0255e1242d8789/src/llm/__init__.py#L23\r\n\r\n```toml\r\n[prompt]\r\ntemperature = 0.2\r\n```\r\n\r\n> if not set, it should be temperature `0.2` by default",
    "comments": [
      {
        "user": "lgabs",
        "body": "Maybe I can take this one to get used to the project :) \r\n\r\nIs there a specific reason for choosing 0.2? Not that the default would matter to much since each project user will have to test some values, but in a Q&A setting, I thought it might be preferable to select 0, as it is the minimum value and helps to minimize randomness (0 would give almost the same results for the same prompt). Of course there isn't a unique answer, but I saw briefly some discussion [here, suggesting 0.2 for chatbot](https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api-a-few-tips-and-tricks-on-controlling-the-creativity-deterministic-output-of-prompt-responses/172683) and [here](https://community.openai.com/t/is-the-lower-the-temperature-the-more-correct-the-answer-is/316029). "
      },
      {
        "user": "avelino",
        "body": "> Generates data analysis scripts that are more likely to be correct and efficient. Output is more deterministic and focused.\r\n\r\n@lgabs thinking about a **humanized conversation**, I believe it's important to have answers that are _\"likely to be correct and efficient\"_, which is why I believe `0.2` should be our standard thermometer, but with support for the user to change the settings."
      }
    ]
  },
  {
    "issue_number": 49,
    "title": "New Docker image not installing dependencies correctly",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-11-23T15:02:57Z",
    "updated_at": "2023-11-23T16:56:31Z",
    "labels": [
      "devx"
    ],
    "body": "<img width=\"715\" alt=\"image\" src=\"https://github.com/talkdai/dialog/assets/4984147/8f320158-6f2d-44aa-8dd5-14d89a184d96\">\r\n",
    "comments": []
  },
  {
    "issue_number": 30,
    "title": "docker image distribution",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-21T19:49:29Z",
    "updated_at": "2023-11-23T01:55:16Z",
    "labels": [
      "enhancement"
    ],
    "body": "distribute docker image via github packages `ghcr.io/talkdai/dialog:latest`\r\n\r\nref: https://github.com/prest/prest/blob/1f78715ca7711a9812dda8081d7b18e1e6589dda/.github/workflows/build.yml#L48",
    "comments": []
  },
  {
    "issue_number": 44,
    "title": "when fallback config does not exist in toml",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-23T00:23:42Z",
    "updated_at": "2023-11-23T01:53:44Z",
    "labels": [
      "bug"
    ],
    "body": "related: #36\r\n\r\nfallback prompt is not defined in toml it doesn't find the index",
    "comments": []
  },
  {
    "issue_number": 33,
    "title": "hallucination: when asking a question outside the content base you are responding to, we must limit",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-21T21:58:20Z",
    "updated_at": "2023-11-22T23:13:09Z",
    "labels": [
      "LLM: ChatGPT",
      "AI"
    ],
    "body": "**QA:** how to make carrot cake?\r\n**answer:** To make a carrot cake, you will need the following ingredients: grated carrots, flour, sugar, eggs, vegetable oil, baking powder, baking soda, cinnamon, and salt. Mix all the dry ingredients together in one bowl, and the wet ingredients in another. Then, gradually add the dry mixture to the wet mixture, stirring until well combined. Pour the batter into a greased cake pan and bake in a preheated oven at 350°F (175°C) for about 30-35 minutes, or until a toothpick inserted into the center comes out clean. Let it cool before frosting with cream cheese frosting. Enjoy!\r\n\r\nif the knowledge base is about a subject that has how to prepare cake with the ingredient, it's OK to give a cake recipe using the ingredient that the knowledge base has, but if it's a bus company and the knowledge base has nothing related to carrot cake, **I'd expect an answer along those lines:**\r\n\r\noops, I think you're asking a question that's way out of the context of a service, is there anything else I can help you with?",
    "comments": [
      {
        "user": "lgabs",
        "body": "I think this approach is good to avoid passing the prompt to the llm resolve. In addition, an approach that worked for me before is to add to the prompt template instructions that if the question can not be answered using the given content, than the assistant must answer some default phrase like you've shown."
      }
    ]
  },
  {
    "issue_number": 34,
    "title": "send memory to LLM",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-21T23:51:35Z",
    "updated_at": "2023-11-22T16:34:01Z",
    "labels": [
      "enhancement",
      "LLM: ChatGPT"
    ],
    "body": "We're not sending the memory to llm, i.e. LLM doesn't have the conversation history, so it can't respond in a personalized way based on the conversation history.\r\n\r\nreference https://python.langchain.com/docs/modules/memory/adding_memory\r\n\r\nThe `PostgresChatMessageHistory` class is not an instance of `BaseMemory`, it inherits from `ABC` (abstract class).\r\n\r\n\r\n## possible solution\r\n\r\nimplement conversion of the message list to an instance of the `BaseMemory` class",
    "comments": []
  },
  {
    "issue_number": 19,
    "title": "embedding: regenerate and popularize vector when changing scope",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-18T13:25:51Z",
    "updated_at": "2023-11-22T13:26:06Z",
    "labels": [
      "enhancement",
      "embedding"
    ],
    "body": "our csv has the following fields:\r\n- category\r\n- subcategory\r\n- question\r\n- content\r\n\r\nwe can use the first 3 fields as a \"key\", making it simpler to know which embedding we need to rewrite and which we need to delete (if it is removed from the csv).",
    "comments": [
      {
        "user": "vmesel",
        "body": "So as far as I understood, we already do this partially on the load_csv.py file.\r\n\r\n```python\r\ndf = df[~df[\"question\"].isin(df_in_db[\"question\"])] # https://github.com/talkdai/dialog/blob/main/src/load_csv.py#L24\r\n```\r\n\r\nBasically, we would change this logic to use these 3 fields in order to generate a PK and then, if any PK is not matched, insert it into the database, right?\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 18,
    "title": "improve prompt (logic) for not identifying as AI - \"Peço desculpas, mas como assistente de IA\"",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-18T11:36:41Z",
    "updated_at": "2023-11-21T17:26:25Z",
    "labels": [
      "I'm not AI"
    ],
    "body": "**QA:** qual telefone de compra pelo whatsapp?\r\n**answer:** Peço desculpas, mas como assistente de IA, não tenho acesso a informações atualizadas em tempo real. Recomendo que você verifique o número de telefone atualizado para compras pelo WhatsApp no site oficial da Buser ou entre em contato com o suporte da empresa para obter essa informação.\r\n\r\n![Screenshot 2023-11-18 at 08 35 57](https://github.com/talkdai/dialog/assets/31996/f47ff88c-0fc6-482d-bd66-f0cae6fca334)",
    "comments": [
      {
        "user": "vmesel",
        "body": "Maybe this is easily solvable by adding a default fallback message."
      },
      {
        "user": "vmesel",
        "body": "@avelino just deployed a change on this. I've added to the embedding a new text explaining who is the bot."
      }
    ]
  },
  {
    "issue_number": 14,
    "title": "not externalise that it's bot",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-15T17:43:22Z",
    "updated_at": "2023-11-19T22:05:28Z",
    "labels": [
      "bug"
    ],
    "body": "Bot must respond like a human, it's fine not to know, but not to externalise that it's a model (AI)\r\n\r\n![image](https://github.com/talkdai/dialog/assets/31996/9e6d2966-9b01-4293-a8ed-e4f84ebd9726)\r\n\r\n> {\"message\":\"Desculpe, mas como sou um modelo de linguagem de IA, não tenho acesso a informações específicas como números de telefone. Recomendo que você verifique o site da empresa ou entre em contato com o serviço de atendimento ao cliente para obter o número correto do WhatsApp.\"}",
    "comments": []
  },
  {
    "issue_number": 21,
    "title": "\"relationship\" of the answer to the question",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-18T16:35:21Z",
    "updated_at": "2023-11-19T19:45:56Z",
    "labels": [
      "enhancement"
    ],
    "body": "Have identification field (`parent`) about which **question** is the **answer**\r\n\r\nthe `parent` field is populated with the LLM answer",
    "comments": []
  },
  {
    "issue_number": 20,
    "title": "OpenAI sensitive data is putting `XXXXXXXXXX`",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-18T13:45:48Z",
    "updated_at": "2023-11-19T14:57:58Z",
    "labels": [
      "bug",
      "LLM: ChatGPT"
    ],
    "body": "**QA:** vc tem whatsapp?\r\n**answer:** Peço desculpas pela confusão. O número de WhatsApp da Buser é XXXXXXXXXX. Sinta-se à vontade para entrar em contato conosco por lá. Estamos aqui para ajudar!\r\n\r\n![Screenshot 2023-11-18 at 10 44 39](https://github.com/talkdai/dialog/assets/31996/4ed60f73-7f99-4a15-8ca0-4653a9c1e768)\r\n",
    "comments": [
      {
        "user": "vmesel",
        "body": "Não consegui replicar, tu já resolveu?\r\n\r\n<img width=\"1331\" alt=\"image\" src=\"https://github.com/talkdai/dialog/assets/4984147/62ab759c-7813-46b5-bba1-e791ba384aa8\">\r\n"
      },
      {
        "user": "vmesel",
        "body": "O bug está em ter uma memória sem mensagem relacionando a Buser, não permitindo a IA entender que ela pode se apresentar como Lídia\r\n\r\nSem histórico/memória:\r\n<img width=\"1406\" alt=\"image\" src=\"https://github.com/talkdai/dialog/assets/4984147/90d737ec-3f1c-4433-b0f2-e6afa2df8eba\">\r\n\r\nCom histórico/memória:\r\n\r\n<img width=\"1395\" alt=\"image\" src=\"https://github.com/talkdai/dialog/assets/4984147/cb22fd0a-3d3a-4353-a630-7939e99f1700\">\r\n"
      },
      {
        "user": "vmesel",
        "body": "this is solved with the new categories approach on the TOML file."
      }
    ]
  },
  {
    "issue_number": 24,
    "title": "categorização por sessão - o que foi conversado na sessão",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-18T23:21:44Z",
    "updated_at": "2023-11-18T23:48:15Z",
    "labels": [
      "enhancement",
      "AI"
    ],
    "body": "é importante ter palavras-chave sobre o que foi conversado na sessão para análise futura e retreinar a base de conhecimento",
    "comments": []
  },
  {
    "issue_number": 22,
    "title": "prompt por sub-categoria",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-18T18:51:26Z",
    "updated_at": "2023-11-18T21:10:33Z",
    "labels": [
      "enhancement"
    ],
    "body": "- carregar campos de categoria e sub-categoria\r\n- criar nó para header de subcategoria no arquivo de configuração (toml)",
    "comments": []
  },
  {
    "issue_number": 16,
    "title": "add timestamp on messages table",
    "author": "vmesel",
    "state": "closed",
    "created_at": "2023-11-15T19:43:48Z",
    "updated_at": "2023-11-15T23:16:05Z",
    "labels": [
      "enhancement"
    ],
    "body": "Nowadays we are saving messages without their datetime. We should figure it out on how to add this field to langchain's table.",
    "comments": []
  },
  {
    "issue_number": 8,
    "title": "docker: build error",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-15T12:51:12Z",
    "updated_at": "2023-11-15T17:58:22Z",
    "labels": [
      "bug"
    ],
    "body": "**errors:**\r\n\r\n```\r\n~/p/a/t/dialog ❯❯❯ docker compose up                                                                                                                                                                                        ...\r\n...\r\n...\r\n => CACHED [web stage-1 3/6] COPY --from=dependencies /dependencies/requirements.txt ./requirements.txt                                                                                                                        0.0s\r\n => ERROR [web stage-1 4/6] COPY /bundle/ /bundle/                                                                                                                                                                             0.0s\r\n------\r\n > [web stage-1 4/6] COPY /bundle/ /bundle/:\r\n------\r\nfailed to solve: failed to compute cache key: failed to calculate checksum of ref b9ec112a-a178-474a-9e8e-ed1fa40ad3bb::dq7mjpoo4myi052qz0z310xmq: \"/bundle\": not found\r\n~/p/a/t/dialog ❯❯❯                                                                                                                                                     \r\n```\r\n\r\n```\r\ndialog-web-1  |   File \"/app/main.py\", line 6, in <module>\r\ndialog-web-1  |     from .models.db import session, engine\r\ndialog-web-1  | ImportError: attempted relative import with no known parent package\r\ndialog-web-1 exited with code 1\r\n```\r\n\r\n**need:**\r\n\r\n- [ ] when the container comes up, it must process the `load_data` (of makefile), docker `ENTRYPOINT`\r\n  - update the database with the new embeds\r\n  - if there is an \"index\" we should update",
    "comments": [
      {
        "user": "vmesel",
        "body": "@avelino try running it from the main branch and latest commit, please."
      }
    ]
  },
  {
    "issue_number": 6,
    "title": "`DATABASE_URL` env var",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-15T10:43:10Z",
    "updated_at": "2023-11-15T14:48:29Z",
    "labels": [
      "enhancement"
    ],
    "body": "https://devcenter.heroku.com/changelog-items/438\r\n\r\nwe won't use heroku, but this is a standard for applications running in the cloud, avoid using user/pass",
    "comments": [
      {
        "user": "vmesel",
        "body": "Fixed on #4 update"
      },
      {
        "user": "avelino",
        "body": "would be `DATABASE_URL` not `PSQL_URL`\n\n[here's](https://medium.com/@gurpreet.singh_89/the-twelve-factor-app-modern-principles-for-cloud-native-development-66d67bd5c7af) more about it, it's a blogpost mentioning [12factor](https://12factor.net/config)\n\n"
      },
      {
        "user": "vmesel",
        "body": "fixed on #10 "
      }
    ]
  },
  {
    "issue_number": 7,
    "title": "fastapi config recommendation",
    "author": "avelino",
    "state": "closed",
    "created_at": "2023-11-15T10:45:00Z",
    "updated_at": "2023-11-15T13:56:27Z",
    "labels": [
      "enhancement"
    ],
    "body": "https://fastapi.tiangolo.com/advanced/settings/#settings-in-another-module\r\n\r\nit is important to look at how fastapi recommends structuring the configuration",
    "comments": [
      {
        "user": "vmesel",
        "body": "Fixed on #4 update"
      }
    ]
  }
]