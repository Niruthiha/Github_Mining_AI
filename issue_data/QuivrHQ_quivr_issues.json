[
  {
    "issue_number": 3609,
    "title": "Finaliser la dataset zendesk LeJuge",
    "author": "AmineDiro",
    "state": "open",
    "created_at": "2025-03-10T08:19:39Z",
    "updated_at": "2025-06-08T12:09:43Z",
    "labels": [
      "Stale"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-367/finaliser-la-dataset-zendesk-lejuge\">CORE-367 Finaliser la dataset zendesk LeJuge</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3610,
    "title": "Simple implementation to get the API infos and add to prompt",
    "author": "AmineDiro",
    "state": "open",
    "created_at": "2025-03-10T08:20:12Z",
    "updated_at": "2025-06-08T12:09:42Z",
    "labels": [
      "Stale"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-368/simple-implementation-to-get-the-api-infos-and-add-to-prompt\">CORE-368 Simple implementation to get the API infos and add to prompt</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3611,
    "title": "Recreate metadata tickets",
    "author": "AmineDiro",
    "state": "open",
    "created_at": "2025-03-10T10:54:53Z",
    "updated_at": "2025-06-08T12:09:40Z",
    "labels": [
      "Stale"
    ],
    "body": "fetch ticket from db (gobocom and prod) where ticket metadata is null and refectch ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-369/recreate-metadata-tickets\">CORE-369 Recreate metadata tickets</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3603,
    "title": "Implement sentry cache hit/miss",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-02-25T16:49:36Z",
    "updated_at": "2025-06-01T00:36:37Z",
    "labels": [
      "Stale"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-366/implement-sentry-cache-hitmiss\">CORE-366 Implement sentry cache hit/miss</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3587,
    "title": "Create a transition script between new and old schema brain",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2025-02-03T10:51:13Z",
    "updated_at": "2025-05-09T12:10:31Z",
    "labels": [
      "Stale",
      "area: scripts"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-356/create-a-transition-script-between-new-and-old-schema-brain\">CORE-356 Create a transition script between new and old schema brain</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3572,
    "title": "Retrieval + generation eval: run RAG on dataset questions",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-28T18:14:07Z",
    "updated_at": "2025-05-04T00:31:35Z",
    "labels": [
      "Stale"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-349/retrieval-generation-eval-run-rag-on-dataset-questions\">CORE-349 Retrieval + generation eval: run RAG on dataset questions</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3485,
    "title": "Problem with Anthropic tool call",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-11-18T12:36:38Z",
    "updated_at": "2025-05-03T16:06:31Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "We can us LangGraph for tool calling with OpenAI, but we encounter issues with Anthropic.\n\nThe problem is explained here [https://github.com/langchain-ai/langchain/discussions/21871](https://github.com/langchain-ai/langchain/discussions/21871) and in particular in [https://github.com/langchain-ai/langchain/discussions/21871#discussioncomment-9968997](https://github.com/langchain-ai/langchain/discussions/21871#discussioncomment-9968997), which refers to [https://github.com/langchain-ai/langchain/discussions/21871#discussioncomment-9968997](https://github.com/langchain-ai/langchain/discussions/21871#discussioncomment-9968997)\n\nThe issue seems actually this one [https://github.com/langchain-ai/langchain/discussions/21871#discussioncomment-10052603](https://github.com/langchain-ai/langchain/discussions/21871#discussioncomment-10052603), where the output from Anthropic is different then the one from OpenAI, so that the two providers are not exchangeableâ€¦",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-298/problem-with-anthropic-tool-call\">CORE-298 Problem with Anthropic tool call</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3597,
    "title": "Deploying MegaParse service v2",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-02-14T08:11:41Z",
    "updated_at": "2025-04-18T12:16:43Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-360/deploying-megaparse-service-v2\">CORE-360 Deploying MegaParse service v2</a></p>"
      }
    ]
  },
  {
    "issue_number": 3595,
    "title": "Core/Enterprise new megaparse version",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-02-12T15:49:18Z",
    "updated_at": "2025-04-18T12:16:38Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-359/coreenterprise-new-megaparse-version\">CORE-359 Core/Enterprise new megaparse version</a></p>"
      }
    ]
  },
  {
    "issue_number": 3573,
    "title": "Retrieval + generation eval: alerting thresholds",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-29T09:26:30Z",
    "updated_at": "2025-04-18T12:16:24Z",
    "labels": [],
    "body": "By running some early evaluations, we should determine the thresholds on the different metrics defined in [CORE-340](https://linear.app/getquivr/issue/CORE-340/retrieval-generation-eval-metrics) that should trigger an alert",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-350/retrieval-generation-eval-alerting-thresholds\">CORE-350 Retrieval + generation eval: alerting thresholds</a></p>"
      }
    ]
  },
  {
    "issue_number": 3591,
    "title": "Strategy dynamic embedding for chunks",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-02-11T08:14:25Z",
    "updated_at": "2025-04-18T12:16:19Z",
    "labels": [],
    "body": "* Vector table should have dynamic size of DIM \n* Create EmbeddingModel table with model name and dim ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-358/strategy-dynamic-embedding-for-chunks\">CORE-358 Strategy dynamic embedding for chunks</a></p>"
      }
    ]
  },
  {
    "issue_number": 3567,
    "title": "Parsing eval: automatic alert",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-28T13:28:10Z",
    "updated_at": "2025-04-18T12:13:56Z",
    "labels": [],
    "body": "The goal is to be alerted if the parsing metrics go below a certain threshold.\n\nFor this we need to:\n\n1. Define metrics used for alerts: global metrics, or individual ones?\n2. Define thresholds for the different metrics: we need to run a few initial tests to setup these thresholds, once we have defined which metrics to use for the alerts\n3. Create the alert service, or use service already included in exp. tracking platforms.\n4. Send alerts to Slack or other channels",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-344/parsing-automatic-alert\">CORE-344 Parsing: automatic alert</a></p>"
      }
    ]
  },
  {
    "issue_number": 3550,
    "title": "Parsing eval: compute layout metrics",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-22T09:03:16Z",
    "updated_at": "2025-04-18T12:13:56Z",
    "labels": [],
    "body": "**Note that the ground-truth is based on the image-pdf, most of which have abandon areas, i.e. in the image-pdf some special graphics in headers and footers have been removed.** This means that when comparing the extracted vs ground-truth layouts based on the native pdfs, we should expect some differences in the headers/footers areas.\n\nStarting from the ground-truth layout, which, for each PDF page, looks like\n\n```json\n{\n  \"extra\": {\n    \"relation\": [\n      {\n        \"relation_type\": \"parent_son\",\n        \"source_anno_id\": 2,\n        \"target_anno_id\": 3\n      },\n      {\n        \"relation_type\": \"parent_son\",\n        \"source_anno_id\": 5,\n        \"target_anno_id\": 8\n      }\n    ]\n  },\n  \"layout_dets\": [\n    {\n      \"anno_id\": 6,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"title\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            109.3333333333331,\n            121.73651418039208,\n            722.1022134807848,\n            121.73651418039208,\n            722.1022134807848,\n            195.75809149176507,\n            109.3333333333331,\n            195.75809149176507\n          ],\n          \"text\": \"å›½èµ„èƒŒæ™¯åŸºé‡‘æƒ…å†µ\"\n        }\n      ],\n      \"order\": 1,\n      \"poly\": [\n        102.5999912116609,\n        120.87255879760278,\n        719.3118659856144,\n        120.87255879760278,\n        719.3118659856144,\n        194.14083813380114,\n        102.5999912116609,\n        194.14083813380114\n      ],\n      \"text\": \"å›½èµ„èƒŒæ™¯åŸºé‡‘æƒ…å†µ\"\n    },\n    {\n      \"anno_id\": 4,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"text_block\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            99.66504579139392,\n            227.6650457913944,\n            1269.333333333333,\n            227.6650457913944,\n            1269.333333333333,\n            271.3365750838786,\n            99.66504579139392,\n            271.3365750838786\n          ],\n          \"text\": \"2022å¹´å¤‡æ¡ˆåŸºé‡‘è§„æ¨¡å°å¹…å›å‡ï¼Œä½†ä»æœªæ¢å¤è‡³èµ„ç®¡æ–°è§„å‡ºå°å‰çš„æ°´å¹³\"\n        }\n      ],\n      \"order\": 2,\n      \"poly\": [\n        97.71487020898245,\n        226.92028692633914,\n        1271.9932332148471,\n        226.92028692633914,\n        1271.9932332148471,\n        264.88925750697814,\n        97.71487020898245,\n        264.88925750697814\n      ],\n      \"text\": \"2022å¹´å¤‡æ¡ˆåŸºé‡‘è§„æ¨¡å°å¹…å›å‡ï¼Œä½†ä»æœªæ¢å¤è‡³èµ„ç®¡æ–°è§„å‡ºå°å‰çš„æ°´å¹³\"\n    },\n    {\n      \"anno_id\": 3,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"figure_caption\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            253.94664201855937,\n            321.21295194692755,\n            1076.1203813864063,\n            321.21295194692755,\n            1076.1203813864063,\n            364.93470762745034,\n            253.94664201855937,\n            364.93470762745034\n          ],\n          \"text\": \"2014å¹´-2023Q3å›½èµ„èƒŒæ™¯åŸºé‡‘çš„å¤‡æ¡ˆæ•°é‡åŠè§„æ¨¡\"\n        }\n      ],\n      \"order\": 3,\n      \"poly\": [\n        246.96994018554688,\n        318.7444152832031,\n        1088.26025390625,\n        318.7444152832031,\n        1088.26025390625,\n        369.0964660644531,\n        246.96994018554688,\n        369.0964660644531\n      ],\n      \"text\": \"2014å¹´-2023Q3å›½èµ„èƒŒæ™¯åŸºé‡‘çš„å¤‡æ¡ˆæ•°é‡åŠè§„æ¨¡\"\n    },\n    {\n      \"anno_id\": 2,\n      \"category_type\": \"figure\",\n      \"ignore\": false,\n      \"order\": 4,\n      \"poly\": [\n        118.08102792118407,\n        379.29373168945347,\n        1299.4279383691976,\n        379.29373168945347,\n        1299.4279383691976,\n        1028.2773128579047,\n        118.08102792118407,\n        1028.2773128579047\n      ]\n    },\n    {\n      \"anno_id\": 8,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"figure_caption\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            1509.6758069519938,\n            324.34247361866034,\n            2292.4771492866826,\n            324.34247361866034,\n            2292.4771492866826,\n            364.8196229053426,\n            1509.6758069519938,\n            364.8196229053426\n          ],\n          \"text\": \"2014å¹´-2023Q3å›½èµ„èƒŒæ™¯åŸºé‡‘æ•°é‡TOP10åœ°åŒº\"\n        }\n      ],\n      \"order\": 5,\n      \"poly\": [\n        1497.726318359375,\n        318.7418518066406,\n        2301.80224609375,\n        318.7418518066406,\n        2301.80224609375,\n        367.1272888183594,\n        1497.726318359375,\n        367.1272888183594\n      ],\n      \"text\": \"2014å¹´-2023Q3å›½èµ„èƒŒæ™¯åŸºé‡‘æ•°é‡TOP10åœ°åŒº\"\n    },\n    {\n      \"anno_id\": 5,\n      \"category_type\": \"figure\",\n      \"ignore\": false,\n      \"order\": 6,\n      \"poly\": [\n        1370.0374839590943,\n        424.35013794251097,\n        2552.3561471143494,\n        424.35013794251097,\n        2552.3561471143494,\n        1026.8955618700252,\n        1370.0374839590943,\n        1026.8955618700252\n      ]\n    },\n    {\n      \"anno_id\": 9,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"title\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            169.67751098302242,\n            1071.225836994341,\n            328.08580770628134,\n            1071.225836994341,\n            328.08580770628134,\n            1111.655822350311,\n            169.67751098302242,\n            1111.655822350311\n          ],\n          \"text\": \"æ ¸å¿ƒå‘ç°\"\n        }\n      ],\n      \"order\": 7,\n      \"poly\": [\n        170.92340081387997,\n        1069.7956822171332,\n        326.21460986860313,\n        1069.7956822171332,\n        326.21460986860313,\n        1111.7494049722532,\n        170.92340081387997,\n        1111.7494049722532\n      ],\n      \"text\": \"æ ¸å¿ƒå‘ç°\"\n    },\n    {\n      \"anno_id\": 7,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"text_block\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            165.603649650326,\n            1150.009124125815,\n            2509.333333333333,\n            1150.009124125815,\n            2509.333333333333,\n            1198.666666666666,\n            165.603649650326,\n            1198.666666666666\n          ],\n          \"text\": \"- 2018å¹´4æœˆèµ„ç®¡æ–°è§„å‡ºå°åï¼Œå›½èµ„èƒŒæ™¯åŸºé‡‘å¤‡æ¡ˆæ•°é‡å¢é€Ÿæ”¾ç¼“ä¸”è§„æ¨¡éª¤å‡ï¼Œå—æ–°å† ç–«æƒ…å½±å“ï¼Œ2021å¹´æ–°å¢åŸºé‡‘è§„æ¨¡å†æ¬¡ä¸‹é™ï¼Œè™½ç„¶\"\n        },\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            219.22996126565647,\n            1201.1457902508969,\n            2250.770752144285,\n            1201.1457902508969,\n            2250.770752144285,\n            1243.9433217869077,\n            219.22996126565647,\n            1243.9433217869077\n          ],\n          \"text\": \"2022å¹´åŸºé‡‘è§„æ¨¡å›å‡è‡³1.25ä¸‡äº¿å…ƒï¼Œä½†ä»æœªæ¢å¤è‡³èµ„ç®¡æ–°è§„å‡ºå°å‰çš„æ°´å¹³ï¼Œ2023å‰ä¸‰å­£åº¦æ–°å¢è§„æ¨¡ç•¥ä½äº2022å¹´åŒæœŸã€‚\"\n        }\n      ],\n      \"order\": 8,\n      \"poly\": [\n        172.66793877059249,\n        1155.2640660519091,\n        2514.2408071863138,\n        1155.2640660519091,\n        2514.2408071863138,\n        1241.6284871157177,\n        172.66793877059249,\n        1241.6284871157177\n      ],\n      \"text\": \"- 2018å¹´4æœˆèµ„ç®¡æ–°è§„å‡ºå°åï¼Œå›½èµ„èƒŒæ™¯åŸºé‡‘å¤‡æ¡ˆæ•°é‡å¢é€Ÿæ”¾ç¼“ä¸”è§„æ¨¡éª¤å‡ï¼Œå—æ–°å† ç–«æƒ…å½±å“ï¼Œ2021å¹´æ–°å¢åŸºé‡‘è§„æ¨¡å†æ¬¡ä¸‹é™ï¼Œè™½ç„¶ 2022å¹´åŸºé‡‘è§„æ¨¡å›å‡è‡³1.25ä¸‡äº¿å…ƒï¼Œä½†ä»æœªæ¢å¤è‡³èµ„ç®¡æ–°è§„å‡ºå°å‰çš„æ°´å¹³ï¼Œ2023å‰ä¸‰å­£åº¦æ–°å¢è§„æ¨¡ç•¥ä½äº2022å¹´åŒæœŸã€‚\"\n    },\n    {\n      \"anno_id\": 1,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"text_block\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            161.7899369148969,\n            1278.308761376868,\n            2508,\n            1278.308761376868,\n            2508,\n            1317.333333333333,\n            161.7899369148969,\n            1317.333333333333\n          ],\n          \"text\": \"- æˆªè‡³2023Q3å…¨å›½å›½èµ„èƒŒæ™¯åŸºé‡‘å¤‡æ¡ˆæ•°é‡ç´¯è®¡9196åªï¼ŒåŸºé‡‘è§„æ¨¡ç´¯è®¡8.91ä¸‡äº¿å…ƒã€‚åŸºé‡‘æ³¨å†ŒåŒºåŸŸé›†ä¸­äºå¹¿ä¸œçœã€æµ™æ±Ÿçœå’Œæ±Ÿè‹çœï¼Œå¹¿ä¸œ\"\n        },\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            222.66666666666688,\n            1325.3333333333335,\n            1623.8331583485456,\n            1325.3333333333335,\n            1623.8331583485456,\n            1365.333333333333,\n            222.66666666666688,\n            1365.333333333333\n          ],\n          \"text\": \"çœå›½èµ„èƒŒæ™¯åŸºé‡‘æ€»è§„æ¨¡é¥é¥é¢†å…ˆã€‚å¤‡æ¡ˆåŸºé‡‘æ•°é‡å‰10çš„çœä»½åŸºé‡‘æ•°é‡å å…¨å›½æ€»é‡çš„\"\n        },\n        {\n          \"category_type\": \"equation_ignore\",\n          \"poly\": [\n            1624.4165959289367,\n            1327.0154193159506,\n            1703.7259660435407,\n            1327.0154193159506,\n            1703.7259660435407,\n            1363.1237504250385,\n            1624.4165959289367,\n            1363.1237504250385\n          ],\n          \"text\": \"73%\"\n        },\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            1704.6905743174548,\n            1322.6134268787764,\n            2053.985160092844,\n            1322.6134268787764,\n            2053.985160092844,\n            1370.6736155849724,\n            1704.6905743174548,\n            1370.6736155849724\n          ],\n          \"text\": \"ï¼Œè§„æ¨¡å å…¨å›½æ€»é‡çš„\"\n        },\n        {\n          \"category_type\": \"equation_ignore\",\n          \"poly\": [\n            2055.1374027302004,\n            1326.3706276890023,\n            2149.276980264608,\n            1326.3706276890023,\n            2149.276980264608,\n            1365.7029169328305,\n            2055.1374027302004,\n            1365.7029169328305\n          ],\n          \"text\": \"68%ã€‚\"\n        }\n      ],\n      \"order\": 9,\n      \"poly\": [\n        171.69999831539863,\n        1278.820932742719,\n        2512.084408886781,\n        1278.820932742719,\n        2512.084408886781,\n        1365.690053585406,\n        171.69999831539863,\n        1365.690053585406\n      ],\n      \"text\": \"- æˆªè‡³2023Q3å…¨å›½å›½èµ„èƒŒæ™¯åŸºé‡‘å¤‡æ¡ˆæ•°é‡ç´¯è®¡9196åªï¼ŒåŸºé‡‘è§„æ¨¡ç´¯è®¡8.91ä¸‡äº¿å…ƒã€‚åŸºé‡‘æ³¨å†ŒåŒºåŸŸé›†ä¸­äºå¹¿ä¸œçœã€æµ™æ±Ÿçœå’Œæ±Ÿè‹çœï¼Œå¹¿ä¸œçœå›½èµ„èƒŒæ™¯åŸºé‡‘æ€»è§„æ¨¡é¥é¥é¢†å…ˆã€‚å¤‡æ¡ˆåŸºé‡‘æ•°é‡å‰10çš„çœä»½åŸºé‡‘æ•°é‡å å…¨å›½æ€»é‡çš„ 73% ï¼Œè§„æ¨¡å å…¨å›½æ€»é‡çš„ 68%ã€‚\"\n    },\n    {\n      \"anno_id\": 10,\n      \"category_type\": \"abandon\",\n      \"ignore\": false,\n      \"order\": null,\n      \"poly\": [\n        114.12910090860571,\n        1403.1676953230935,\n        175.21358196554792,\n        1403.1676953230935,\n        175.21358196554792,\n        1462.6586681785502,\n        114.12910090860571,\n        1462.6586681785502\n      ]\n    },\n    {\n      \"anno_id\": 0,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_en_ch_mixed\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"footer\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            178.18192276049803,\n            1409.8767302579377,\n            288.0868232114207,\n            1409.8767302579377,\n            288.0868232114207,\n            1467.2607048296584,\n            178.18192276049803,\n            1467.2607048296584\n          ],\n          \"text\": \"CVINFO æŠ•ä¸­ä¿¡æ¯\"\n        }\n      ],\n      \"order\": null,\n      \"poly\": [\n        180.18207532211585,\n        1404.2778174322868,\n        289.9793827860912,\n        1404.2778174322868,\n        289.9793827860912,\n        1462.652231000048,\n        180.18207532211585,\n        1462.652231000048\n      ],\n      \"text\": \"CVINFO æŠ•ä¸­ä¿¡æ¯\"\n    }\n  ],\n  \"page_info\": {\n    \"height\": 1500,\n    \"image_path\": \"eastmoney_59cde7e939acc3124df9d3f2c85b5a0ec41b9da1157d5be38e098672022b47cb.pdf_11.jpg\",\n    \"page_attribute\": {\n      \"data_source\": \"PPT2PDF\",\n      \"language\": \"simplified_chinese\",\n      \"layout\": \"1andmore_column\",\n      \"special_issue\": [\n        \"watermark\"\n      ]\n    },\n    \"page_no\": 11,\n    \"width\": 2667\n  }\n}\n```\n\nWe consider the array `layout_dets` and extract the list of `category_type` with its corresponding `poly`, which encodes the position information: coordinates (x,y) for top-left, top-right, bottom-right, bottom-left corners of the bounding box.\n\n`category_type` can be one among\n\n```\n# Block level annotation boxes\n'title'               # Title\n'text_block'          # Paragraph level plain text\n'figure',             # Figure type\n'figure_caption',     # Figure description/title\n'figure_footnote',    # Figure notes\n'table',              # Table body\n'table_caption',      # Table description/title\n'table_footnote',     # Table notes\n'equation_isolated',  # Display formula\n'equation_caption',   # Formula number\n'header'              # Header\n'footer'              # Footer\n'page_number'         # Page number\n'page_footnote'       # Page notes\n'abandon',            # Other discarded content (e.g. irrelevant information in middle of page)\n'code_txt',           # Code block\n'code_txt_caption',   # Code block description\n'reference'          # References\n```\n\nWe extract the same information from the Megaparse output, i.e. for each page we extract the element type and the bounding box, group the pages per document type, per language, per layout type, and compute:\n\n* **fraction of correctly extracted blocks** in each block category. A block is correctly extracted if\n  * the normalized category match with the ground truth (we need to establish the correspondance between the Megaparse categories and the those above)\n  * AND the bounding box coordinates match within some errors. We can set the error to 1 pixel initially and refine (increase/decrease) the error after the first tests.\n* **average fraction of correctly extracted blocks,** i.e. we compute the average of the fraction of each block (this means that each block category will contribute equally to the metric)\n* **fraction of correctly** **extracted blocks** across all categories (more numerous blocks, likely text blocks, will contribute more to the metric)\n\nWe can also have compute the metrics above across all document types.",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-331/implement-layout-metrics\">CORE-331 Implement layout metrics</a></p>"
      }
    ]
  },
  {
    "issue_number": 3544,
    "title": "Implement first version of parsing evaluation",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2025-01-02T15:40:03Z",
    "updated_at": "2025-04-18T12:13:53Z",
    "labels": [],
    "body": "For a list of potential datasets for parsing see [CORE-335](https://linear.app/getquivr/issue/CORE-335/parsing-evaluation-datasets).\n\nFor details on OmniDocBench see [CORE-332](https://linear.app/getquivr/issue/CORE-332/parsing-save-omnidocbench-dataset-to-experiment-tracker)  or [Notion](https://www.notion.so/getquivr/Parsing-evaluation-189312462d4b80d69632c448e1b49d56?pvs=4)\n\n# Evaluation steps for CI/CD\n\nWe only using a single EN subset, where we have excluded masked documents. Each subset contains 57 documents of 1 page each. We will run the evaluation on both native and image pdfs.\n\n1. Load dataset --> [CORE-355](https://linear.app/getquivr/issue/CORE-355/parsing-eval-load-reference-dataset) \n   1. For each row in the subset, retrieve the native (original) pdf from the url `https://huggingface.co/datasets/Quivr/OmniDocBench/blob/main/ori_pdfs/file_name`  where  `file_name` is extracted from `page_info.image_path`\n   2. For each row in the subset, retrieve the image pdf from the url `https://huggingface.co/datasets/Quivr/OmniDocBench/blob/main/pdfs/file_name`\n2. Run Megaparse on each document, on both the native-pdf and image-pdf versions, and store the results in a JSON file --> [CORE-342](https://linear.app/getquivr/issue/CORE-342/parsing-run-megaparse-on-reference-dataset)\n3. Compute the parsing metrics -->  [CORE-331](https://linear.app/getquivr/issue/CORE-331/parsing-layout-metrics)\n4. Compute the OCR metrics --> [CORE-333](https://linear.app/getquivr/issue/CORE-333/parsing-ocr-metrics)\n5. Push the different results (Megaparse output of step 5, output of steps 6 and 7) as JSON files to the exp. tracker, along with  --> [CORE-343](https://linear.app/getquivr/issue/CORE-343/parsing-push-results-to-exp-tracker)\n6. Alert if metrics are below a given threshold --> [CORE-344](https://linear.app/getquivr/issue/CORE-344/parsing-automatic-alert)\n\n# Evaluation steps for optimising Megaparse\n\nWe should also be able to manually run these evaluations using the full dataset, i.e. subsets 1 to 5, for the purpose of optimizing / improving our parsing service.",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-326/implement-a-first-draft-of-metrics\">CORE-326 Implement a first draft of metrics</a></p>"
      }
    ]
  },
  {
    "issue_number": 3623,
    "title": "[Bug]: Doc Update",
    "author": "pAulseperformance",
    "state": "closed",
    "created_at": "2025-04-03T04:25:33Z",
    "updated_at": "2025-04-18T12:11:51Z",
    "labels": [
      "bug",
      "area: docs"
    ],
    "body": "### What happened?\n\nTried to install with python 3.10.6 and package was not found in pypi.\n\nMaybe update docs as \"Python 3.10 and newer\" is not accurate. \nAlso tried to use Python 3.13 and ran into issues with numpy dependency.\n\nI am using Python 3.11.5 now and it downloaded successfully.\n\n### Relevant log output\n\n```bash\n\n```\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey there, @pAulseperformance! I'm here to help you with any bugs, questions, or contributions you might have. Let's get this sorted out together!\n\n<!-- Answer -->\nThe documentation should indeed be updated to reflect the correct Python version requirements. The package requires at least Python 3.11.9, not 3.10 as currently stated in the documentation <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/docs/.python-version)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/requirements.txt)</sup><sup>[[3]](https://github.com/QuivrHQ/quivr/blob/main/core/.python-version)</sup><sup>[[4]](https://github.com/QuivrHQ/quivr/blob/main/core/pyproject.toml)</sup><sup>[[5]](https://github.com/QuivrHQ/quivr/blob/main/examples/simple_question/.python-version)</sup>. Additionally, the numpy dependency is specified as `numpy==1.26.4` for Python versions `>= '3.9'`, which might be causing issues with Python 3.13 <sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/requirements.txt)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n\n---\n<sup>Help Dosu learn! Give it feedback: [Great Response](https://app.dosu.dev/response-feedback/644caf5f-b789-4752-a590-99c7d5d80705?feedback_type=great_response) | [Irrelevant Answer](https://app.dosu.dev/response-feedback/644caf5f-b789-4752-a590-99c7d5d80705?feedback_type=irrelevant_answer) | [Incorrect Sources](https://app.dosu.dev/response-feedback/644caf5f-b789-4752-a590-99c7d5d80705?feedback_type=incorrect_sources) | [Too Verbose](https://app.dosu.dev/response-feedback/644caf5f-b789-4752-a590-99c7d5d80705?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/644caf5f-b789-4752-a590-99c7d5d80705?feedback_type=hallucination) | [Bug Report](https://app.dosu.dev/response-feedback/644caf5f-b789-4752-a590-99c7d5d80705?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/644caf5f-b789-4752-a590-99c7d5d80705?feedback_type=other)</sup>\n"
      }
    ]
  },
  {
    "issue_number": 3616,
    "title": "[Bug]: stopped installing quivr-core due to (error: subprocess-exited-with-error)",
    "author": "abdulazizalmass",
    "state": "closed",
    "created_at": "2025-03-16T09:43:59Z",
    "updated_at": "2025-04-18T12:11:44Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\nDear All\n\nThanks a lot for this amazing application.\n\nkindly not that a bug happened running on windows while installing quivr after entering this `pip install quivr-core # Check that the installation worked`, the following stopped me \n\n```\nCollecting uvloop>=0.18.0 (from megaparse==0.0.43->megaparse[all]==0.0.43->quivr-core)\n  Using cached uvloop-0.21.0.tar.gz (2.5 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... error\n  error: subprocess-exited-with-error\n\n  Ã— Getting requirements to build wheel did not run successfully.\n  â”‚ exit code: 1\n  â•°â”€> [20 lines of output]\n      Traceback (most recent call last):\n        File \"C:\\Users\\ai\\.conda\\envs\\quivr\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n          main()\n          ~~~~^^\n        File \"C:\\Users\\ai\\.conda\\envs\\quivr\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n                                   ~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"C:\\Users\\ai\\.conda\\envs\\quivr\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 143, in get_requires_for_build_wheel\n          return hook(config_settings)\n        File \"C:\\Users\\ai\\AppData\\Local\\Temp\\pip-build-env-uofmz46i\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 334, in get_requires_for_build_wheel\n          return self._get_build_requires(config_settings, requirements=[])\n                 ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"C:\\Users\\ai\\AppData\\Local\\Temp\\pip-build-env-uofmz46i\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 304, in _get_build_requires\n          self.run_setup()\n          ~~~~~~~~~~~~~~^^\n        File \"C:\\Users\\ai\\AppData\\Local\\Temp\\pip-build-env-uofmz46i\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n          exec(code, locals())\n          ~~~~^^^^^^^^^^^^^^^^\n        File \"<string>\", line 8, in <module>\n      RuntimeError: uvloop does not support Windows at the moment\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\nÃ— Getting requirements to build wheel did not run successfully.\nâ”‚ exit code: 1\nâ•°â”€> See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n```\n\n### Relevant log output\n\n```bash\n\n```\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-370/[bug]-stopped-installing-quivr-core-due-to-error-subprocess-exited\">CORE-370 [Bug]: stopped installing quivr-core due to (error: subprocess-exited-with-error)</a></p>"
      }
    ]
  },
  {
    "issue_number": 3548,
    "title": "[Feature]: UI/UX improvement",
    "author": "kartikscript",
    "state": "closed",
    "created_at": "2025-01-20T08:14:19Z",
    "updated_at": "2025-04-18T12:10:43Z",
    "labels": [
      "enhancement"
    ],
    "body": "### The Feature\n\nI have tagged you in X where I have redesigned your website , please review it my username is kartikscript on X.\n\n### Motivation, pitch\n\nhttps://github.com/kartikscript/quivr\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-330/[feature]-uiux-improvement\">CORE-330 [Feature]: UI/UX improvement</a></p>"
      },
      {
        "user": "StanGirard",
        "body": "Thanks a lot! Can you share screenshot ? "
      },
      {
        "user": "kartikscript",
        "body": "![Image](https://github.com/user-attachments/assets/73443b64-e157-45c8-a17b-b9622d1e14de)\n![Image](https://github.com/user-attachments/assets/85354a01-8c3e-4126-8ccf-2b3ba679ab66)\n![Image](https://github.com/user-attachments/assets/62196112-f0e4-44ba-8b44-d8b8038ecddb)\n![Image](https://github.com/user-attachments/assets/9b1d01f3-5e43-4146-8123-6441973f4931)\n\nsince the contents were meant for light theme like the screenshots, logos. The page is not 100% suitable but if we design everything in context of dark mode and typography and colour combinations it can look way better. \nLet me know what do you think of the designs."
      },
      {
        "user": "kartikscript",
        "body": "\nDo you like it or does it need some improvements ?\nFeel free to tell me what type of layout do you need or if you need some more styling in the layout like the blobs, spotlight as this time I am keeping it simple ."
      }
    ]
  },
  {
    "issue_number": 3512,
    "title": "[Feature]: Integration with AI/ML API",
    "author": "waterstark",
    "state": "closed",
    "created_at": "2024-12-09T09:39:09Z",
    "updated_at": "2025-04-18T12:10:28Z",
    "labels": [
      "enhancement"
    ],
    "body": "### The Feature\n\nReceiving and sending requests to our API\n\n### Motivation, pitch\n\nHi!\r\nI am a Python developer working at AI/ML API.\r\nWe want to give your users the opportunity to use our models, such as Qwen/Qwen2.5-72B-Instruct-Turbo and 200 other ones. If you like the idea - give us a green light, and I'll make the PR\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-309/[feature]-integration-with-aiml-api\">CORE-309 [Feature]: Integration with AI/ML API</a></p>"
      },
      {
        "user": "OctavianTheI",
        "body": "Ping ping!\n\nIt's Sergey - from the Integrations team at AI/ML API\n\nIf you guys could give us a green light, we'd love to go ahead and integrate with the repo"
      },
      {
        "user": "StanGirard",
        "body": "Go ahead ğŸ™‚ "
      }
    ]
  },
  {
    "issue_number": 3578,
    "title": "Remove dep 'pympler'",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-01-30T10:02:21Z",
    "updated_at": "2025-04-18T12:10:25Z",
    "labels": [],
    "body": "We can discuss details @jacopo ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-353/remove-dep-pympler\">CORE-353 Remove dep 'pympler'</a></p>"
      }
    ]
  },
  {
    "issue_number": 3557,
    "title": "CancelledError: Cancelled by cancel scope 7fd22135b490",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2025-01-27T13:21:22Z",
    "updated_at": "2025-04-18T12:10:12Z",
    "labels": [
      "bug"
    ],
    "body": "Sentry Issue: [PYTHON-FASTAPI-19X](https://quivr-brain.sentry.io/issues/6250728865/?referrer=Linear)\n\n```\nCancelledError: Cancelled by cancel scope 7fd22135b490\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 576, in astream\n    async for chunk in aiterator:\n  File \"langchain_core/tracers/log_stream.py\", line 254, in tap_output_aiter\n    async for chunk in output:\n  File \"langchain_core/runnables/base.py\", line 1455, in atransform\n    async for ichunk in input:\n  File \"langchain_core/runnables/base.py\", line 1455, in atransform\n    async for ichunk in input:\n  File \"langchain_core/runnables/base.py\", line 1018, in astream\n    yield await self.ainvoke(input, config, **kwargs)\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 236, in ainvoke\n    ret = await asyncio.create_task(coro, context=context)\n  File \"quivr_core/rag/quivr_rag_langgraph.py\", line 650, in retrieve\n    await asyncio.gather(*(task[0] for task in async_jobs))\n  File \"langchain_core/retrievers.py\", line 310, in ainvoke\n    result = await self._aget_relevant_documents(\n  File \"langchain/retrievers/contextual_compression.py\", line 70, in _aget_relevant_documents\n    docs = await self.base_retriever.ainvoke(\n  File \"langchain_core/retrievers.py\", line 310, in ainvoke\n    result = await self._aget_relevant_documents(\n  File \"langchain_core/vectorstores/base.py\", line 1101, in _aget_relevant_documents\n    docs = await self.vectorstore.asimilarity_search(\n  File \"quivr_api/vectorstore/supabase.py\", line 80, in asimilarity_search\n    match_result = await self.vector_service.similarity_search(\n  File \"quivr_api/modules/vector/service/vector_service.py\", line 55, in similarity_search\n    vectors = await self.repository.similarity_search(\n  File \"quivr_api/modules/vector/repository/vectors_repository.py\", line 105, in similarity_search\n    result = await self.session.execute(sql_query, params=params)\n  File \"sqlmodel/ext/asyncio/session.py\", line 143, in execute\n    return await super().execute(\n  File \"sqlalchemy/ext/asyncio/session.py\", line 461, in execute\n    result = await greenlet_spawn(\n  File \"sqlalchemy/util/_concurrency_py3k.py\", line 201, in greenlet_spawn\n    result = context.throw(*sys.exc_info())\n  File \"sqlmodel/orm/session.py\", line 127, in execute\n    return super().execute(\n  File \"sqlalchemy/orm/session.py\", line 2362, in execute\n    return self._execute_internal(\n  File \"sqlalchemy/orm/session.py\", line 2256, in _execute_internal\n    result = conn.execute(\n  File \"sqlalchemy/engine/base.py\", line 1418, in execute\n    return meth(\n  File \"sqlalchemy/sql/elements.py\", line 515, in _execute_on_connection\n    return connection._execute_clauseelement(\n  File \"sqlalchemy/engine/base.py\", line 1640, in _execute_clauseelement\n    ret = self._execute_context(\n  File \"sqlalchemy/engine/base.py\", line 1846, in _execute_context\n    return self._exec_single_context(\n  File \"sqlalchemy/engine/base.py\", line 1986, in _exec_single_context\n    self._handle_dbapi_exception(\n  File \"sqlalchemy/engine/base.py\", line 2358, in _handle_dbapi_exception\n    raise exc_info[1].with_traceback(exc_info[2])\n  File \"sqlalchemy/engine/base.py\", line 1967, in _exec_single_context\n    self.dialect.do_execute(\n  File \"sqlalchemy/engine/default.py\", line 941, in do_execute\n    cursor.execute(statement, parameters)\n  File \"sqlalchemy/dialects/postgresql/asyncpg.py\", line 568, in execute\n    self._adapt_connection.await_(\n  File \"sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\n    return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\n  File \"sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\n    value = await result\n  File \"sqlalchemy/dialects/postgresql/asyncpg.py\", line 510, in _prepare_and_execute\n    prepared_stmt, attributes = await adapt_connection._prepare(\n  File \"sqlalchemy/dialects/postgresql/asyncpg.py\", line 756, in _prepare\n    prepared_stmt = await self._connection.prepare(\n  File \"asyncpg/connection.py\", line 635, in prepare\n    return await self._prepare(\n  File \"asyncpg/connection.py\", line 653, in _prepare\n    stmt = await self._get_statement(\n  File \"asyncpg/connection.py\", line 432, in _get_statement\n    statement = await self._protocol.prepare(\n  File \"asyncpg/protocol/protocol.pyx\", line 165, in prepare\n```\n\n```\n27T13:18:43.008107Z [error    ] [quivr_api.modules.rag_service.rag_service] msg=Exception during stream : string indices must be integers, not 'str'\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚ /app/api/quivr_api/modules/rag_service/rag_service.py:321 in generate_answer_stream              â”‚\nâ”‚                                                                                                  â”‚\nâ”‚   318 â”‚   â”‚   â”‚   â”‚   â”‚   brain_name=self.model_to_use,                                          â”‚\nâ”‚   319 â”‚   â”‚   â”‚   â”‚   )                                                                          â”‚\nâ”‚   320 â”‚   â”‚   â”‚                                                                                  â”‚\nâ”‚ â± 321 â”‚   â”‚   â”‚   async for response in brain_core.ask_streaming(                                â”‚\nâ”‚   322 â”‚   â”‚   â”‚   â”‚   question=question,                                                         â”‚\nâ”‚   323 â”‚   â”‚   â”‚   â”‚   retrieval_config=retrieval_config,                                         â”‚\nâ”‚   324 â”‚   â”‚   â”‚   â”‚   rag_pipeline=QuivrQARAGLangGraph,                                          â”‚\nâ”‚                                                                                                  â”‚\nâ”‚ /app/core/core/quivr_core/brain/brain.py:545 in ask_streaming                                    â”‚\nâ”‚                                                                                                  â”‚\nâ”‚   542 â”‚   â”‚   â”‚   \"langfuse_user_id\": str(self.user_id),                                         â”‚\nâ”‚   543 â”‚   â”‚   â”‚   \"langfuse_session_id\": str(self.chat_id),                                      â”‚\nâ”‚   544 â”‚   â”‚   }                                                                                  â”‚\nâ”‚ â± 545 â”‚   â”‚   async for response in rag_instance.answer_astream(                                 â”‚\nâ”‚   546 â”‚   â”‚   â”‚   question=question,                                                             â”‚\nâ”‚   547 â”‚   â”‚   â”‚   history=chat_history,                                                          â”‚\nâ”‚   548 â”‚   â”‚   â”‚   list_files=list_files,                                                         â”‚\nâ”‚                                                                                                  â”‚\nâ”‚ /app/core/core/quivr_core/rag/quivr_rag_langgraph.py:967 in answer_astream                       â”‚\nâ”‚                                                                                                  â”‚\nâ”‚    964 â”‚   â”‚   â”‚                                                                                 â”‚\nâ”‚    965 â”‚   â”‚   â”‚   if self._is_final_node_and_chat_model_stream(event):                          â”‚\nâ”‚    966 â”‚   â”‚   â”‚   â”‚   chunk = event[\"data\"][\"chunk\"]                                            â”‚\nâ”‚ â±  967 â”‚   â”‚   â”‚   â”‚   rolling_message, new_content, previous_content = parse_chunk_response(    â”‚\nâ”‚    968 â”‚   â”‚   â”‚   â”‚   â”‚   rolling_message,                                                      â”‚\nâ”‚    969 â”‚   â”‚   â”‚   â”‚   â”‚   chunk,                                                                â”‚\nâ”‚    970 â”‚   â”‚   â”‚   â”‚   â”‚   self.llm_endpoint.supports_func_calling(),                            â”‚\nâ”‚                                                                                                  â”‚\nâ”‚ /app/core/core/quivr_core/rag/utils.py:98 in parse_chunk_response                                â”‚\nâ”‚                                                                                                  â”‚\nâ”‚    95 â”‚   Returns:                                                                               â”‚\nâ”‚    96 â”‚   â”‚   Tuple of (updated rolling message, new content only, full content)                 â”‚\nâ”‚    97 â”‚   \"\"\"                                                                                    â”‚\nâ”‚ â±  98 â”‚   rolling_msg += raw_chunk                                                               â”‚\nâ”‚    99 â”‚                                                                                          â”‚\nâ”‚   100 â”‚   tool_calls = rolling_msg.tool_calls                                                    â”‚\nâ”‚   101                                                                                            â”‚\nâ”‚                                                                                                  â”‚\nâ”‚ /usr/local/lib/python3.11/site-packages/langchain_core/messages/ai.py:395 in __add__             â”‚\nâ”‚                                                                                                  â”‚\nâ”‚   392 â”‚                                                                                          â”‚\nâ”‚   393 â”‚   def __add__(self, other: Any) -> BaseMessageChunk:  # type: ignore                     â”‚\nâ”‚   394 â”‚   â”‚   if isinstance(other, AIMessageChunk):                                              â”‚\nâ”‚ â± 395 â”‚   â”‚   â”‚   return add_ai_message_chunks(self, other)                                      â”‚\nâ”‚   396 â”‚   â”‚   elif isinstance(other, (list, tuple)) and all(                                     â”‚\nâ”‚   397 â”‚   â”‚   â”‚   isinstance(o, AIMessageChunk) for o in other                                   â”‚\nâ”‚   398 â”‚   â”‚   ):                                                                                 â”‚\nâ”‚                                                                                                  â”‚\nâ”‚ /usr/local/lib/python3.11/site-packages/langchain_core/messages/ai.py:411 in                     â”‚\nâ”‚ add_ai_message_chunks                                                                            â”‚\nâ”‚                                                                                                  â”‚\nâ”‚   408 â”‚   â”‚   msg = \"Cannot concatenate AIMessageChunks with different example values.\"          â”‚\nâ”‚   409 â”‚   â”‚   raise ValueError(msg)                                                              â”‚\nâ”‚   410 â”‚                                                                                          â”‚\nâ”‚ â± 411 â”‚   content = merge_content(left.content, *(o.content for o in others))                    â”‚\nâ”‚   412 â”‚   additional_kwargs = merge_dicts(                                                       â”‚\nâ”‚   413 â”‚   â”‚   left.additional_kwargs, *(o.additional_kwargs for o in others)                     â”‚\nâ”‚   414 â”‚   )                                                                                      â”‚\nâ”‚                                                                                                  â”‚\nâ”‚ /usr/local/lib/python3.11/site-packages/langchain_core/messages/base.py:147 in merge_content     â”‚\nâ”‚                                                                                                  â”‚\nâ”‚   144 â”‚   â”‚   â”‚   â”‚   merged = [merged] + content  # type: ignore                                â”‚\nâ”‚   145 â”‚   â”‚   elif isinstance(content, list):                                                    â”‚\nâ”‚   146 â”‚   â”‚   â”‚   # If both are lists                                                            â”‚\nâ”‚ â± 147 â”‚   â”‚   â”‚   merged = merge_lists(cast(list, merged), content)  # type: ignore              â”‚\nâ”‚   148 â”‚   â”‚   # If the first content is a list, and the second content is a string               â”‚\nâ”‚   149 â”‚   â”‚   else:                                                                              â”‚\nâ”‚   150 â”‚   â”‚   â”‚   # If the last element of the first content is a string                         â”‚\nâ”‚                                                                                                  â”‚\nâ”‚ /usr/local/lib/python3.11/site-packages/langchain_core/utils/_merge.py:91 in merge_lists         â”‚\nâ”‚                                                                                                  â”‚\nâ”‚    88 â”‚   â”‚   else:                                                                              â”‚\nâ”‚    89 â”‚   â”‚   â”‚   for e in other:                                                                â”‚\nâ”‚    90 â”‚   â”‚   â”‚   â”‚   if isinstance(e, dict) and \"index\" in e and isinstance(e[\"index\"], int):   â”‚\nâ”‚ â±  91 â”‚   â”‚   â”‚   â”‚   â”‚   to_merge = [                                                           â”‚\nâ”‚    92 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   i                                                                  â”‚\nâ”‚    93 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   for i, e_left in enumerate(merged)                                 â”‚\nâ”‚    94 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   if e_left[\"index\"] == e[\"index\"]                                   â”‚\nâ”‚                                                                                                  â”‚\nâ”‚ /usr/local/lib/python3.11/site-packages/langchain_core/utils/_merge.py:94 in <listcomp>          â”‚\nâ”‚                                                                                                  â”‚\nâ”‚    91 â”‚   â”‚   â”‚   â”‚   â”‚   to_merge = [                                                           â”‚\nâ”‚    92 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   i                                                                  â”‚\nâ”‚    93 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   for i, e_left in enumerate(merged)                                 â”‚\nâ”‚ â±  94 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   if e_left[\"index\"] == e[\"index\"]                                   â”‚\nâ”‚    95 â”‚   â”‚   â”‚   â”‚   â”‚   ]                                                                      â”‚\nâ”‚    96 â”‚   â”‚   â”‚   â”‚   â”‚   if to_merge:                                                           â”‚\nâ”‚    97 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   # TODO: Remove this once merge_dict is updated with special        â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\nTypeError: string indices must be integers, not 'str'\n```",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-336/cancellederror-cancelled-by-cancel-scope-7fd22135b490\">CORE-336 CancelledError: Cancelled by cancel scope 7fd22135b490</a></p>"
      },
      {
        "user": "StanGirard",
        "body": "[https://cloud.langfuse.com/project/cm4tnv44415zfp9cqvir095ke/traces/4b59b937-819b-4148-b356-c3c247a61094?timestamp=2025-01-27T13:18:37.718Z](https://cloud.langfuse.com/project/cm4tnv44415zfp9cqvir095ke/traces/4b59b937-819b-4148-b356-c3c247a61094?timestamp=2025-01-27T13:18:37.718Z)"
      }
    ]
  },
  {
    "issue_number": 3170,
    "title": "[Backend] KMS upload limits",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-09T08:48:46Z",
    "updated_at": "2025-04-18T12:08:14Z",
    "labels": [
      "area: backend"
    ],
    "body": "## KMS Limits:\n\n### Uploads (Local)\n\n* \\[x\\] Single file size ?  (50Mb / file )\n\n\n* \\[x\\] Single Folder total size ? No limits\n* Concurrent file upload ? ( rate limit upload route, for now can't upload whole folders)\n* \\[x\\] # Files per folder ? -> No limits\n* Total storage per tier of users ?  \n  * 500Mb free\n  * 100GB premium \n\n### Knowledge linked to Brains ( sync & local )\n\n* Total size for all brains: \n  * free: 500 Mb \n  * premium:  100Gb for  all brains ( get size users in production) \n  * No limits for # brains\n\n\n* Nb or files ? \n  *  correct metric **# of chunks in brain** -> have the infos back to the user (too much chunks / files in brain .\n* File size across syncs + local ? No diff\n* \\# sync files per user to add to brain ? \n  * Limit to # chunks in a brain\n\n### KM Processing\n\n* \\# max concurrent processing of files ? \n  * max queue size of 10_000 (check ) 4 worker\n* \\# max concurrent processing files per user\n  * fairness scheduler / per user \n* Max time processing per file \n  * p99 celery worker\n* Max batch size sync \n  * Randomize across users \n  * Priorize refresh for connected users",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-68/[backend]-kms-upload-limits\">ENT-68 [Backend] KMS upload limits</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3620,
    "title": "ff",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2025-03-26T16:24:35Z",
    "updated_at": "2025-04-18T12:07:48Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-372/ff\">CORE-372 ff</a></p>"
      }
    ]
  },
  {
    "issue_number": 3553,
    "title": "Implement first version of retrieval/generation evaluation",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-22T11:11:58Z",
    "updated_at": "2025-04-08T12:34:26Z",
    "labels": [
      "enhancement"
    ],
    "body": "For a list of potential datasets for retrieval see [CORE-325](https://linear.app/getquivr/issue/CORE-325/retrieval-datasets) and [Notion](https://www.notion.so/getquivr/Retrieval-generation-evaluation-189312462d4b800eb9cedff39777e80c?pvs=4)\n\n# Evaluation steps for CI/CD\n\n1. Select a subset (1 to 20) .Each subset contains 135 Q&A, with 5 html documents for each question, so a total of 675 documents (in html format)\n2. Retrieve reference dataset --> [CORE-357](https://linear.app/getquivr/issue/CORE-357/retrieval-generation-load-reference-dataset) \n\n\n3. For each row, parse, chunk and embed the documents contained in `search_results.page_result` --> [CORE-348](https://linear.app/getquivr/issue/CORE-348/retrieval-generation-eval-parse-chunk-and-embed-dataset)\n4. For each row, extract the question from the `query` field and run the chosen RAG workflow to obtain an answer --> [CORE-345](https://linear.app/getquivr/issue/CORE-345/retrieval-generation-eval-run-quivr-rag-on-dataset-questions)\n5. Compute evaluation metrics comparing the ground truth answers and the answers produced in 4 --> [CORE-340](https://linear.app/getquivr/issue/CORE-340/retrieval-generation-eval-metrics)\n6. Push the results to exp. tracker --> [CORE-351](https://linear.app/getquivr/issue/CORE-351/retrieval-generation-eval-push-results-to-exp-tracker)\n7. Define thresholds for alerting --> [CORE-350](https://linear.app/getquivr/issue/CORE-350/retrieval-generation-eval-alerting-thresholds)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-334/implement-a-first-draft-of-retrieval-metrics\">CORE-334 Implement a first draft of retrieval metrics</a></p>"
      }
    ]
  },
  {
    "issue_number": 3429,
    "title": "[Bug]: Brain.from_files  Return error \"ValueError: can't initialize brain without documents\"(For pdf)",
    "author": "ccutyear",
    "state": "closed",
    "created_at": "2024-10-25T09:06:15Z",
    "updated_at": "2025-03-30T00:29:39Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### What happened?\n\nI run the following codeï¼š\r\n\r\n```python\r\nfrom quivr_core import Brain\r\n\r\nbrain = Brain.from_files(name = \"my smart brain\",\r\n                        file_paths = [\"/root/workplace/try_use_quivr/qa_file/txtQA/Bible.pdf\"],\r\n                        )\r\n```\r\n\r\nReturn error:  ValueError: can't initialize brain without documents\r\n\r\n\r\n[è¥¿æ¸¸è®°.pdf](https://github.com/user-attachments/files/17519439/default.pdf)\r\n[Bible.pdf](https://github.com/user-attachments/files/17519440/Bible.pdf)\r\n\r\nThe pdf file is small and the file format is simple\r\n\r\n\r\nMaybe the issue label is not appropriate, if so it can be modified.\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-261/[bug]-brainfrom-files-return-error-valueerror-cant-initialize-brain\">CORE-261 [Bug]: Brain.from_files Return error \"ValueError: can't initialize brain without documents\"(For pdf)</a></p>"
      },
      {
        "user": "LesConfirmed",
        "body": "I am able to reproduce this issue on a fresh install on Python 3.11.6.\r\n\r\nTried passing in relative / absolute file paths."
      },
      {
        "user": "naquad",
        "body": "I've encountered the same issue while trying to stuff a bunch of PDFs.\r\nLong-story short: MegaParse has dependencies not being installed.\r\n\r\nA check script:\r\n\r\n```python\r\nfrom megaparse.core.megaparse import MegaParse\r\nfrom langchain_openai import ChatOpenAI\r\nfrom megaparse.core.parser.unstructured_parser import UnstructuredParser\r\n\r\nparser = UnstructuredParser()\r\nmegaparse = MegaParse(parser)\r\nresponse = megaparse.load(\"./test.pdf\")\r\nprint(response)\r\n```\r\n\r\nAnd if you get errors like:\r\n```\r\nResource punkt_tab not found.\r\n  Please use the NLTK Downloader to obtain the resource:\r\n\r\n  >>> import nltk\r\n  >>> nltk.download('punkt_tab')\r\n  \r\n  For more information see: https://www.nltk.org/data.html\r\n\r\n  Attempted to load tokenizers/punkt_tab/english/\r\n\r\n  Searched in:\r\n    - '/root/nltk_data'\r\n    - '/usr/local/nltk_data'\r\n    - '/usr/local/share/nltk_data'\r\n    - '/usr/local/lib/nltk_data'\r\n    - '/usr/share/nltk_data'\r\n    - '/usr/local/share/nltk_data'\r\n    - '/usr/lib/nltk_data'\r\n    - '/usr/local/lib/nltk_data'\r\n    - '/usr/local/lib/python3.11/site-packages/llama_index/core/_static/nltk_cache'\r\n**********************************************************************\r\n```\r\n\r\nOr the same for `averaged_perceptron_tagger_eng` then you have to manually install those:\r\n\r\n```\r\n$ python\r\n>>> import nltk\r\n>>> nltk.download('punkt_tab')\r\n>>> nltk.download('averaged_perceptron_tagger_eng')\r\n```\r\n\r\nThe automatic download is disabled because of the security issues.\r\n\r\nHope this helps."
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3449,
    "title": "Enable use of different chunking strategies",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-11-04T15:31:18Z",
    "updated_at": "2025-03-25T20:06:30Z",
    "labels": [
      "enhancement",
      "Stale",
      "area: backend",
      "rag: ingestion"
    ],
    "body": "Currently, we adopt a single chunking strategy for all documents. We should allow the simple configuration and use of different chunking strategies, including:\n\n*  [late chunking](https://jina.ai/news/late-chunking-in-long-context-embedding-models/)\n* (potentially [contextual chunking](https://www.anthropic.com/news/contextual-retrieval))\n* [regex chunking](https://gist.github.com/hanxiao/3f60354cf6dc5ac698bc9154163b4e6a)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-277/enable-use-of-different-chunking-strategies\">CORE-277 Enable use of different chunking strategies</a></p>"
      },
      {
        "user": "vivek-official-tech",
        "body": "chunking_strategy:\r\n  default: \"regex\"\r\n  document_types:\r\n    - type: \"technical_report\"\r\n      strategy: \"contextual\"\r\n    - type: \"customer_feedback\"\r\n      strategy: \"late\"\r\nregex_patterns:\r\n  - pattern: \"\\n{2,}\" # Split on double newlines\r\n  - pattern: \"(?:\\.|\\?|!)\\s+\" # Split on sentence-ending punctuation\r\n  \r\n  \"An example\""
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3531,
    "title": "Fallback LLM if OpenAI is too slow or unavailable",
    "author": "StanGirard",
    "state": "closed",
    "created_at": "2024-12-18T10:52:57Z",
    "updated_at": "2025-03-23T16:05:56Z",
    "labels": [
      "Stale"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-318/fall-is-openai-is-too-slow\">CORE-318 Fall is openai is too slow</a></p>"
      },
      {
        "user": "StanGirard",
        "body": "fallback if openai is too slow or down.<br>Engros si erreur sur openai on fallback le call sur un autre LLM"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3516,
    "title": "Choosing a guardrailing approach",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-12-10T13:51:30Z",
    "updated_at": "2025-03-23T16:05:56Z",
    "labels": [
      "Stale"
    ],
    "body": "The goal is to adopt an initial approach for guardrailing, also taking into account the simplicity of the integration into our system.",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-312/choosing-a-guardrailing-approach\">CORE-312 Choosing a guardrailing approach</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3515,
    "title": "Research available models and libraries for LLM / chatbot guardrailing",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-12-10T13:50:10Z",
    "updated_at": "2025-03-22T16:06:00Z",
    "labels": [
      "Stale"
    ],
    "body": "The goal is to do a comprehensive, but quick, research of the available approaches (models, libraries, â€¦) that can be used as guardrails for our chatbot, to filter malicious/harmful inputs and outputs.\n\nUseful models (they are all multilingual):\n\n* Meta prompt-guard: [https://huggingface.co/meta-llama/Prompt-Guard-86M](https://huggingface.co/meta-llama/Prompt-Guard-86M) (prompt injection / jailbreak)\n* Toxicity classifier: [https://huggingface.co/textdetox/xlmr-large-toxicity-classifier](https://huggingface.co/textdetox/xlmr-large-toxicity-classifier)\n* Personal information detection: [https://huggingface.co/iiiorg/piiranha-v1-detect-personal-information](https://huggingface.co/iiiorg/piiranha-v1-detect-personal-information)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-311/research-available-models-and-libraries-for-llm-chatbot-guardrailing\">CORE-311 Research available models and libraries for LLM / chatbot guardrailing</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3617,
    "title": "[Bug]: Can't install library on Windows",
    "author": "AlbaRuth",
    "state": "closed",
    "created_at": "2025-03-21T12:39:36Z",
    "updated_at": "2025-03-21T13:13:48Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\n(.venv) PS C:\\quivr-main> pip install quivr-core\nCollecting quivr-core\n  Using cached quivr_core-0.0.26-py3-none-any.whl.metadata (1.0 kB)\nCollecting aiofiles>=23.1.0 (from quivr-core)\n  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\nCollecting faiss-cpu>=1.8.0.post1 (from quivr-core)\n  Using cached faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\nCollecting httpx>=0.27.0 (from quivr-core)\n  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-anthropic>=0.1.23 (from quivr-core)\n  Using cached langchain_anthropic-0.3.10-py3-none-any.whl.metadata (1.9 kB)\nCollecting langchain-cohere>=0.1.0 (from quivr-core)\n  Using cached langchain_cohere-0.4.3-py3-none-any.whl.metadata (6.6 kB)\nCollecting langchain-community>=0.2.12 (from quivr-core)\n  Using cached langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\nCollecting langchain-core>=0.2.38 (from quivr-core)\n  Using cached langchain_core-0.3.47-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain-openai>=0.1.0 (from quivr-core)\n  Using cached langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\nCollecting langchain<0.3.0,>=0.2.14 (from quivr-core)\n  Using cached langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\nCollecting langgraph>=0.2.38 (from quivr-core)\n  Using cached langgraph-0.3.18-py3-none-any.whl.metadata (7.5 kB)\nCollecting markupsafe>=2.1.5 (from quivr-core)\n  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\nINFO: pip is looking at multiple versions of quivr-core to determine which version is compatible with other requirements. This could take a while.\nCollecting quivr-core\n  Using cached quivr_core-0.0.25-py3-none-any.whl.metadata (1.0 kB)\n  Using cached quivr_core-0.0.24-py3-none-any.whl.metadata (1.0 kB)\nCollecting megaparse==0.0.43 (from megaparse[all]==0.0.43->quivr-core)\n  Using cached megaparse-0.0.43-py3-none-any.whl.metadata (5.6 kB)\nCollecting pydantic>=2.8.2 (from quivr-core)\n  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\nCollecting rapidfuzz>=3.10.1 (from quivr-core)\n  Using cached rapidfuzz-3.12.2-cp312-cp312-win_amd64.whl.metadata (12 kB)\nCollecting rich>=13.7.1 (from quivr-core)\n  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\nCollecting tiktoken>=0.7.0 (from quivr-core)\n  Using cached tiktoken-0.9.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\nCollecting transformers>=4.44.2 (from transformers[sentencepiece]>=4.44.2->quivr-core)\n  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\nCollecting types-pyyaml>=6.0.12.20240808 (from quivr-core)\n  Using cached types_PyYAML-6.0.12.20241230-py3-none-any.whl.metadata (1.8 kB)\nCollecting pydantic-settings>=2.6.1 (from megaparse==0.0.43->megaparse[all]==0.0.43->quivr-core)\n  Using cached pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\nCollecting python-dotenv>=1.0.0 (from megaparse==0.0.43->megaparse[all]==0.0.43->quivr-core)\n  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nCollecting uvloop>=0.18.0 (from megaparse==0.0.43->megaparse[all]==0.0.43->quivr-core)\n  Using cached uvloop-0.21.0.tar.gz (2.5 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... error\n  error: subprocess-exited-with-error\n\n  Ã— Getting requirements to build wheel did not run successfully.\n  â”‚ exit code: 1\n  â•°â”€> [18 lines of output]\n      Traceback (most recent call last):\n        File \"C:\\Users\\a.sakovich\\Desktop\\quivr-main\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n          main()\n        File \"C:\\Users\\a.sakovich\\Desktop\\quivr-main\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"C:\\Users\\a.sakovich\\Desktop\\quivr-main\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 143, in get_requires_for_build_wheel\n          return hook(config_settings)\n                 ^^^^^^^^^^^^^^^^^^^^^\n        File \"C:\\Users\\a.sakovich\\AppData\\Local\\Temp\\pip-build-env-ay_f2l8o\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 334, in get_requires_for_build_wheel\n          return self._get_build_requires(config_settings, requirements=[])\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        File \"C:\\Users\\a.sakovich\\AppData\\Local\\Temp\\pip-build-env-ay_f2l8o\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 304, in _get_build_requires\n          self.run_setup()\n        File \"C:\\Users\\a.sakovich\\AppData\\Local\\Temp\\pip-build-env-ay_f2l8o\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n          exec(code, locals())\n        File \"<string>\", line 8, in <module>\n      RuntimeError: uvloop does not support Windows at the moment\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\nÃ— Getting requirements to build wheel did not run successfully.\nâ”‚ exit code: 1\nâ•°â”€> See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n\n### Relevant log output\n\n```bash\n\n```\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-371/[bug]-cant-insatll-lib-on-windows\">CORE-371 [Bug]: Can't insatll lib on Windows</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey @AlbaRuth, I'm here to assist you with any bugs, questions, or contributions while you wait for a human maintainer. Let's tackle this together!\n\n<!-- Answer -->\nI found a similar open issue regarding the installation problem on Windows due to `uvloop` not supporting the platform. You can check the details of this issue [here](https://github.com/QuivrHQ/quivr/issues/3616). Unfortunately, no solution has been found in the comments of that issue <sup>[[1]](https://github.com/QuivrHQ/quivr/issues/3616)</sup>.\n\nSince `uvloop` is not listed as a required dependency in the setup configuration of the `quivr-core` package, you might try to exclude it from the installation if possible <sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/requirements.txt)</sup><sup>[[3]](https://github.com/QuivrHQ/quivr/blob/main/core/pyproject.toml)</sup>. Alternatively, you could look for a way to install `quivr-core` without the optional dependencies that require `uvloop`.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n\n---\n<sup>Help Dosu learn! Give it feedback: [Great Response](https://app.dosu.dev/response-feedback/fd9aa680-a610-4641-a607-335d04d8562d?feedback_type=great_response) | [Irrelevant Answer](https://app.dosu.dev/response-feedback/fd9aa680-a610-4641-a607-335d04d8562d?feedback_type=irrelevant_answer) | [Incorrect Sources](https://app.dosu.dev/response-feedback/fd9aa680-a610-4641-a607-335d04d8562d?feedback_type=incorrect_sources) | [Too Verbose](https://app.dosu.dev/response-feedback/fd9aa680-a610-4641-a607-335d04d8562d?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/fd9aa680-a610-4641-a607-335d04d8562d?feedback_type=hallucination) | [Bug Report](https://app.dosu.dev/response-feedback/fd9aa680-a610-4641-a607-335d04d8562d?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/fd9aa680-a610-4641-a607-335d04d8562d?feedback_type=other)</sup>\n"
      }
    ]
  },
  {
    "issue_number": 2004,
    "title": "[Bug]: ",
    "author": "zxc985330681",
    "state": "open",
    "created_at": "2024-01-10T06:51:39Z",
    "updated_at": "2025-03-19T16:07:08Z",
    "labels": [
      "bug",
      "area: frontend",
      "type: dependencies"
    ],
    "body": "### What happened?\n\nç”¨dockerè¿è¡Œï¼Œå‰ç«¯æŠ¥é”™ï¼Œå¯¼è‡´æ ·å¼æ— æ³•æˆåŠŸä½¿ç”¨\n\n### Relevant log output\n\n```bash\nRefused to load the stylesheet 'http://XXXX:3000/_next/static/css/29ebb1b2c1464ed7.css' because it violates the following Content Security Policy directive: \"style-src 'unsafe-inline' http://localhost:*\". Note that 'style-src-elem' was not explicitly set, so 'style-src' is used as a fallback.\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "dosubot[bot]",
        "body": "_ğŸ¤–_\n<!-- Greeting -->\nä½ å¥½ï¼Œ@zxc985330681ï¼ğŸ‘‹ æˆ‘æ˜¯[Dosu](https://dosu.dev)ï¼Œä¸€æ¬¾èƒ½å¸®åŠ©ä½ è§£å†³bugï¼Œè§£ç­”ç–‘æƒ‘ï¼Œä»¥åŠæˆä¸ºè´¡çŒ®è€…çš„æœºå™¨äººã€‚åœ¨ç­‰å¾…äººç±»ç»´æŠ¤è€…çš„åŒæ—¶ï¼Œæˆ‘åœ¨è¿™é‡Œå¸®åŠ©ä½ ã€‚æˆ‘æ­£åœ¨æŸ¥çœ‹ä½ çš„é—®é¢˜ï¼Œå¹¶å°†å°½å¿«ç»™ä½ ä¸€ä¸ªå®Œæ•´çš„ç­”å¤ã€‚è¯·ç¨å€™~ ğŸ˜\n\n\n"
      },
      {
        "user": "zxc985330681",
        "body": "è¿™æ˜¯æ¥è‡ªQQé‚®ç®±çš„å‡æœŸè‡ªåŠ¨å›å¤é‚®ä»¶ã€‚æ‚¨å¥½ï¼Œæˆ‘æœ€è¿‘æ­£åœ¨ä¼‘å‡ä¸­ï¼Œæ— æ³•äº²è‡ªå›å¤æ‚¨çš„é‚®ä»¶ã€‚æˆ‘å°†åœ¨å‡æœŸç»“æŸåï¼Œå°½å¿«ç»™æ‚¨å›å¤ã€‚"
      },
      {
        "user": "ingeniumdesign",
        "body": "i have the same problem...\r\ni edit in the .env all \"`localhost`\" in my local IP from the Server. But i cant login with the default user: admin@quir.app and pw: admin"
      },
      {
        "user": "zxc985330681",
        "body": "è¿™æ˜¯æ¥è‡ªQQé‚®ç®±çš„å‡æœŸè‡ªåŠ¨å›å¤é‚®ä»¶ã€‚æ‚¨å¥½ï¼Œæˆ‘æœ€è¿‘æ­£åœ¨ä¼‘å‡ä¸­ï¼Œæ— æ³•äº²è‡ªå›å¤æ‚¨çš„é‚®ä»¶ã€‚æˆ‘å°†åœ¨å‡æœŸç»“æŸåï¼Œå°½å¿«ç»™æ‚¨å›å¤ã€‚"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      },
      {
        "user": "zxc985330681",
        "body": "è¿™æ˜¯æ¥è‡ªQQé‚®ç®±çš„å‡æœŸè‡ªåŠ¨å›å¤é‚®ä»¶ã€‚æ‚¨å¥½ï¼Œæˆ‘æœ€è¿‘æ­£åœ¨ä¼‘å‡ä¸­ï¼Œæ— æ³•äº²è‡ªå›å¤æ‚¨çš„é‚®ä»¶ã€‚æˆ‘å°†åœ¨å‡æœŸç»“æŸåï¼Œå°½å¿«ç»™æ‚¨å›å¤ã€‚"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      },
      {
        "user": "zxc985330681",
        "body": "è¿™æ˜¯æ¥è‡ªQQé‚®ç®±çš„å‡æœŸè‡ªåŠ¨å›å¤é‚®ä»¶ã€‚æ‚¨å¥½ï¼Œæˆ‘æœ€è¿‘æ­£åœ¨ä¼‘å‡ä¸­ï¼Œæ— æ³•äº²è‡ªå›å¤æ‚¨çš„é‚®ä»¶ã€‚æˆ‘å°†åœ¨å‡æœŸç»“æŸåï¼Œå°½å¿«ç»™æ‚¨å›å¤ã€‚"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      },
      {
        "user": "zxc985330681",
        "body": "è¿™æ˜¯æ¥è‡ªQQé‚®ç®±çš„å‡æœŸè‡ªåŠ¨å›å¤é‚®ä»¶ã€‚æ‚¨å¥½ï¼Œæˆ‘æœ€è¿‘æ­£åœ¨ä¼‘å‡ä¸­ï¼Œæ— æ³•äº²è‡ªå›å¤æ‚¨çš„é‚®ä»¶ã€‚æˆ‘å°†åœ¨å‡æœŸç»“æŸåï¼Œå°½å¿«ç»™æ‚¨å›å¤ã€‚"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      },
      {
        "user": "zxc985330681",
        "body": "è¿™æ˜¯æ¥è‡ªQQé‚®ç®±çš„å‡æœŸè‡ªåŠ¨å›å¤é‚®ä»¶ã€‚æ‚¨å¥½ï¼Œæˆ‘æœ€è¿‘æ­£åœ¨ä¼‘å‡ä¸­ï¼Œæ— æ³•äº²è‡ªå›å¤æ‚¨çš„é‚®ä»¶ã€‚æˆ‘å°†åœ¨å‡æœŸç»“æŸåï¼Œå°½å¿«ç»™æ‚¨å›å¤ã€‚"
      }
    ]
  },
  {
    "issue_number": 3517,
    "title": "Integrate guardrails into RAG pipeline",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-12-10T13:52:57Z",
    "updated_at": "2025-03-15T20:05:50Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "The goal is enabling the simple addition of guardrails to filter the input / output of our chatbot. This can be easily achieved by implementing new langgraph nodes and configuring specific workflows",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-313/integrate-guardrails-into-rag-pipeline\">CORE-313 Integrate guardrails into RAG pipeline</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3518,
    "title": "Testing guardrails",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-12-10T13:54:12Z",
    "updated_at": "2025-03-15T20:05:49Z",
    "labels": [
      "Stale"
    ],
    "body": "While waiting for our evaluation service \"Le Juge\", we should manually test the implemented guardrails to ensure that they behave as expectec",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-314/testing-guardrails\">CORE-314 Testing guardrails</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3602,
    "title": "Add attributes in prompt",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-02-21T17:16:01Z",
    "updated_at": "2025-03-10T09:47:54Z",
    "labels": [],
    "body": "Enrich prompt based on set of attributes",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-365/add-attributes-in-prompt\">CORE-365 Add attributes in prompt</a></p>"
      },
      {
        "user": "PaperBoardOfficial",
        "body": "are the issues open for external contribution? will you accept the PR if I raise it?"
      }
    ]
  },
  {
    "issue_number": 3503,
    "title": "Mine good responses + with context",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-11-28T08:47:01Z",
    "updated_at": "2025-03-03T16:07:01Z",
    "labels": [
      "Stale"
    ],
    "body": "Crafted manually -> use intent classifier to get most asked '*type'* of questions",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-304/mine-good-responses-with-context\">CORE-304 Mine good responses + with context</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3504,
    "title": "Mine bad responses + with context",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-11-28T08:47:23Z",
    "updated_at": "2025-03-03T16:06:59Z",
    "labels": [
      "Stale"
    ],
    "body": "* Sentimental analysis on question > first_question\n* Correlation of intent between questions",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-305/mine-bad-responses-with-context\">CORE-305 Mine bad responses + with context</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3599,
    "title": "Core query logic to apply filters",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-02-17T08:50:44Z",
    "updated_at": "2025-02-27T08:22:04Z",
    "labels": [],
    "body": "* focus on \"and\" filters on now\n* Write query engine to filter a stream of tickets based on schema associated with a brain",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-362/core-logic-to-apply-filters\">CORE-362 Core logic to apply filters</a></p>"
      }
    ]
  },
  {
    "issue_number": 3488,
    "title": "Documentation not up to date with new changes",
    "author": "StanGirard",
    "state": "closed",
    "created_at": "2024-11-18T22:13:57Z",
    "updated_at": "2025-02-24T20:06:30Z",
    "labels": [
      "Stale",
      "area: docs"
    ],
    "body": "<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/5978517c-49ef-4491-a38d-3189509a5af3/09d81e58-233c-4032-8396-568e6131fef6?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC81OTc4NTE3Yy00OWVmLTQ0OTEtYTM4ZC0zMTg5NTA5YTVhZjMvMDlkODFlNTgtMjMzYy00MDMyLTgzOTYtNTY4ZTYxMzFmZWY2IiwiaWF0IjoxNzMyMjEzMTA4LCJleHAiOjMzMzAyNzczMTA4fQ.SWRDX_Deinet_oEyX3nExhW-nNBziRv1bMhEGK9u-Tk \" alt=\"image.png\" width=\"1446\" height=\"515\" />\n\nThe documentation has not been changed on core.quivr.app to reflect the new name for max_input_token",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-300/documentation-not-up-to-date-with-new-changes\">CORE-300 Documentation not up to date with new changes</a></p>"
      },
      {
        "user": "jacopo-chevallard",
        "body": "@stan can you point me to the documentation you are referring to?"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3574,
    "title": "Retrieval + generation eval: push results to exp tracker",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-29T09:29:21Z",
    "updated_at": "2025-02-21T15:38:21Z",
    "labels": [],
    "body": "We should push to the exp tracker:\n\n* parsing, chunking and embedding output from [CORE-348](https://linear.app/getquivr/issue/CORE-348/retrieval-generation-eval-parse-chunk-and-embed-dataset)\n* RAG output (and intermediate steps of retrieval workflow) from [CORE-345](https://linear.app/getquivr/issue/CORE-345/retrieval-generation-eval-run-quivr-rag-on-dataset-questions)\n* metrics computed in [CORE-340](https://linear.app/getquivr/issue/CORE-340/retrieval-generation-eval-metrics)\n* metadata, such as\n  * versions of the different services used (quivr-core, quivr-api, Megaparse, â€¦)\n  * date and time of test\n  * others?",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-351/retrieval-generation-eval-push-results-to-exp-tracker\">CORE-351 Retrieval + generation eval: push results to exp tracker</a></p>"
      }
    ]
  },
  {
    "issue_number": 3566,
    "title": "Parsing eval: push results to exp. tracker",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-28T13:21:40Z",
    "updated_at": "2025-02-21T15:38:21Z",
    "labels": [],
    "body": "We need to push to the exp. tracker:\n\n* Parsing output, i.e. output of [CORE-342](https://linear.app/getquivr/issue/CORE-342/parsing-run-megaparse-on-reference-dataset)\n* Layout metrics, output of [CORE-331](https://linear.app/getquivr/issue/CORE-331/parsing-compute-layout-metrics)\n* OCR metrics, output of [CORE-333](https://linear.app/getquivr/issue/CORE-333/parsing-compute-ocr-metrics)\n* metadata, such as\n  * versions of the different services used (Megaparse, â€¦)\n  * date and time of test\n  * others?",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-343/parsing-push-results-to-exp-tracker\">CORE-343 Parsing: push results to exp. tracker</a></p>"
      }
    ]
  },
  {
    "issue_number": 3542,
    "title": "Set up exp Tracker",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2025-01-02T15:37:03Z",
    "updated_at": "2025-02-21T15:38:21Z",
    "labels": [],
    "body": "Link the Evaluating script to an external Experiement tracker",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-324/set-up-exp-tracker\">CORE-324 Set up exp Tracker</a></p>"
      }
    ]
  },
  {
    "issue_number": 3601,
    "title": "Brains criteria should be mutually exclusive.",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-02-17T09:12:32Z",
    "updated_at": "2025-02-21T10:44:47Z",
    "labels": [],
    "body": "(can't have criteria that match 2 brains)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-364/brains-criteria-should-be-mutually-exclusive\">CORE-364 Brains criteria should be mutually exclusive.</a></p>"
      }
    ]
  },
  {
    "issue_number": 3575,
    "title": "Fix megaparse-service local",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-01-29T10:42:25Z",
    "updated_at": "2025-02-21T08:41:03Z",
    "labels": [],
    "body": "* fix megaparse version to be updated when we advance megaparse",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-352/fix-megaparse-service-local\">CORE-352 Fix megaparse-service local</a></p>"
      }
    ]
  },
  {
    "issue_number": 3600,
    "title": "Send and receive filters schema",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-02-17T08:52:06Z",
    "updated_at": "2025-02-19T13:57:07Z",
    "labels": [],
    "body": "```\n{ \n\"fields\":\n  [\n    \"name\":..\n    \"type\":...\n    \"supported_ops\": [\"AND\",\"OR\"],\n   \n  ],\n}\n```",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-363/send-and-receive-filters-schema\">CORE-363 Send and receive filters schema</a></p>"
      }
    ]
  },
  {
    "issue_number": 3598,
    "title": "Define full ticket schema and metadata to filter on",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-02-17T08:49:50Z",
    "updated_at": "2025-02-19T13:57:00Z",
    "labels": [],
    "body": "```\n{ \n\"fields\":\n  [\n    \"name\":..\n    \"type\":...\n    \"supported_ops\": [\"AND\",\"OR\"],\n   \n  ],\n}\n```",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-361/define-full-ticket-schema-and-metadata-to-filter-on\">CORE-361 Define full ticket schema and metadata to filter on</a></p>"
      }
    ]
  },
  {
    "issue_number": 3541,
    "title": "Create a script for evaluation in Enterprise",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2025-01-02T15:35:48Z",
    "updated_at": "2025-02-19T13:20:49Z",
    "labels": [
      "area: scripts"
    ],
    "body": "The scripts \n\n* takes a config and spawn supporting services (S3/DB)\n* Split the RAG in 3 tasks (Parsing, Retrieval, Generation) \n* Get specific datasets for evaluating each tasks from the Exp tracker\n* Run the task on their dataset\n* Compute the metrics based on GT and predicted results (This can be really simple logic at first and will iteratively grow complex)\n* Save Metrics in Exp Tracker",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-323/create-a-script-for-evaluation-in-enterprise\">CORE-323 Create a script for evaluation in Enterprise</a></p>"
      }
    ]
  },
  {
    "issue_number": 3589,
    "title": "Retrieval + generation: load reference dataset",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-02-05T13:55:53Z",
    "updated_at": "2025-02-19T08:52:04Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-357/retrieval-generation-load-reference-dataset\">CORE-357 Retrieval + generation: load reference dataset</a></p>"
      }
    ]
  },
  {
    "issue_number": 3571,
    "title": "Retrieval + generation eval: parse, chunk and embed dataset",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-28T18:10:58Z",
    "updated_at": "2025-02-19T08:52:04Z",
    "labels": [],
    "body": "Given an ingestion workflow/configuration, we should parse, chunk and embed the reference dataset and store the results into a database for successive retrieval.\n\nOpen questions:\n\n* create a temporary (supabase) instance and use it for evaluation purposes, so when the evaluation is finished, the instance can be deleted? Or shall we persist the results for successive inspection, for instance to understand why a certain evaluation run provided bad results? ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-348/retrieval-generation-eval-parse-chunk-and-embed-dataset\">CORE-348 Retrieval + generation eval: parse, chunk and embed dataset</a></p>"
      }
    ]
  },
  {
    "issue_number": 3568,
    "title": "Retrieval + generation eval: run quivr RAG on dataset questions",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-28T15:17:03Z",
    "updated_at": "2025-02-19T08:52:04Z",
    "labels": [],
    "body": "We should take all questions in the reference dataset, and perform RAG using a given retrieval/generation workflow.\n\nCRAG uses the following prompt, which allows for three different types of answer:\n\n1. Full answer\n2. answer of the type \"I donâ€™t know\"\n3. answer of the type \"invalid question\"\n\n```python\n\"\"\" You are given a Question, References and\nthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". The query\ntime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". The references may or may not help answer the\nquestion. Your task is to answer the question in as few words as possible.\nPlease follow these guidelines when formulating your answer:\n1. If the question contains a false premise or assumption, answer â€œinvalid questionâ€.\n2. If you are uncertain or donâ€™t know the answer, respond with â€œI donâ€™t knowâ€.\n### Question\n{query}\n### Query Time\n{query_time}\n### References\n{references}\n17\n### Answer\n\"\"\"\n```",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-345/retrieval-generation-run-quivr-rag-on-reference-dataset\">CORE-345 Retrieval + generation: run quivr RAG on reference dataset</a></p>"
      }
    ]
  },
  {
    "issue_number": 3563,
    "title": "Retrieval + generation eval: metrics",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-28T09:24:29Z",
    "updated_at": "2025-02-19T08:52:04Z",
    "labels": [],
    "body": "Given the answers obtained in [CORE-345](https://linear.app/getquivr/issue/CORE-345/retrieval-generation-eval-run-quivr-rag-on-dataset-questions)  and the ground-truth answers of the reference dataset, we should determine, for each question, how well the two match.\n\nWe will adopt an LLM-as-a-judge approach, using three different LLMs and a vote by majority to decide whether the answer is satisfactory.\n\nWe will follow the process below:\n\n* Consider the input question, ground truth answer, and generated answer\n* Prompt the LLM to judge whether the generated answer correctly answer the question, given the ground-truth answer. Note that CRAG allows answers of type \"I donâ€™t know\" and \"invalid question\", which should be accounted for in the evaluation\n* Repeat for the other LLMs (can do in parallel)\n* Decide whether the answer is good or not by majority vote of the different judges\n* Compute global metric (fraction of correct answers), and the metric by\n  * domain\n  * question_type\n  * answer type (\"valid\", \"invalid\", \"no_answer\")",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-340/retrieval-generation-mertrics\">CORE-340 Retrieval + generation: mertrics</a></p>"
      }
    ]
  },
  {
    "issue_number": 3547,
    "title": "Fix get_user_credit",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-01-09T16:00:32Z",
    "updated_at": "2025-02-19T08:52:04Z",
    "labels": [
      "bug"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-329/fix-get-user-credit\">CORE-329 Fix get_user_credit</a></p>"
      }
    ]
  },
  {
    "issue_number": 3266,
    "title": "Implement Notion sync KMS",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-26T07:20:26Z",
    "updated_at": "2025-02-18T16:06:50Z",
    "labels": [
      "Stale",
      "area: backend"
    ],
    "body": "* Fetch file\n* Background update\n* Delete notions",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-103/add-notion-sync\">ENT-103 Add notion sync</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3448,
    "title": "Allow use of different text embeddings",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-11-04T15:13:37Z",
    "updated_at": "2025-02-08T20:05:33Z",
    "labels": [
      "enhancement",
      "Stale",
      "area: backend",
      "rag: ingestion",
      "rag: retrieval"
    ],
    "body": "Given the constraints of PGVector, we are currently limited to using text embeddings with a fixed dimension. This limits our ability to test and use text embeddings from different suppliers (e.g., [closed- vs open-weights](https://huggingface.co/spaces/mteb/leaderboard)) and with different dimensions (e.g. to exploit \"[Matrioska](https://huggingface.co/blog/matryoshka)\" embeddings).",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-276/allow-use-of-different-embeddings\">CORE-276 Allow use of different embeddings</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3454,
    "title": "Introduce automatic LLM routing",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-11-04T16:15:23Z",
    "updated_at": "2025-02-08T20:05:32Z",
    "labels": [
      "enhancement",
      "Stale",
      "area: backend",
      "rag: retrieval"
    ],
    "body": "Currently, we manually configure LLMs, and associate the same LLM to the different nodes of a workflow.\n\nImplementing automatic routing, for instance using [https://github.com/lm-sys/RouteLLM](https://github.com/lm-sys/RouteLLM), will allow us to cut costs and latency, while maintaining performances largely unaltered.\n\nSee also:\n\n* [https://withmartian.com/products/model-router](https://withmartian.com/products/model-router)\n* [https://unify.ai/](https://unify.ai/)\n* [https://www.notdiamond.ai/](https://www.notdiamond.ai/)\n  * [https://docs.notdiamond.ai/docs/rag-workflows](https://docs.notdiamond.ai/docs/rag-workflows)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-282/introduce-automatic-llm-routing\">CORE-282 Introduce automatic LLM routing</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3453,
    "title": "Create workflow management systems (WMS)",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-11-04T16:10:30Z",
    "updated_at": "2025-02-07T20:06:32Z",
    "labels": [
      "Stale",
      "area: backend",
      "area: frontend"
    ],
    "body": "We should industrialise our workflow management by developing a system following this [scheme](https://www.figma.com/board/s8D352vFbVGXMERi6XOPV6/Workflow-Management?node-id=0-1&node-type=canvas&t=pSjkemXnvBGdf2wO-0) ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-281/create-workflow-management-systems-wms\">CORE-281 Create workflow management systems (WMS)</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3455,
    "title": "Enable evaluation of a retrieval workflow",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-11-04T16:17:45Z",
    "updated_at": "2025-02-07T20:06:31Z",
    "labels": [
      "Stale",
      "area: backend",
      "rag: retrieval"
    ],
    "body": "We need to be able to evaluate a retrieval workflow on several benchmarks / datasets",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-283/enable-evaluation-of-a-retrieval-workflow\">CORE-283 Enable evaluation of a retrieval workflow</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3456,
    "title": "Enable text-to-database retrieval",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-11-04T16:37:18Z",
    "updated_at": "2025-02-07T20:06:30Z",
    "labels": [
      "Stale",
      "area: backend",
      "rag: retrieval"
    ],
    "body": "We should enable the connection of our RAG system with our customers' internal databases.\n\nThis requires developing capabilities for text-to-sql, text-to-cypher, etcâ€¦\n\n* text-to-sql:\n  * [https://medium.com/pinterest-engineering/how-we-built-text-to-sql-at-pinterest-30bad30dabff](https://medium.com/pinterest-engineering/how-we-built-text-to-sql-at-pinterest-30bad30dabff)\n  * [https://vanna.ai/docs/](https://vanna.ai/docs/)\n  * benchmark: [https://bird-bench.github.io/](https://bird-bench.github.io/)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-284/enable-text-to-database-retrieval\">CORE-284 Enable text-to-database retrieval</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3132,
    "title": "Automatically extract document metadata",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-03T09:15:54Z",
    "updated_at": "2025-02-07T16:06:27Z",
    "labels": [
      "Stale",
      "area: backend",
      "rag: ingestion"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-183/automatically-extract-document-metadata\">CORE-183 Automatically extract document metadata</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3450,
    "title": "Extract and store document/chunk structure and relationships",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-11-04T15:41:48Z",
    "updated_at": "2025-02-07T16:06:26Z",
    "labels": [
      "enhancement",
      "Stale",
      "rag: ingestion"
    ],
    "body": "Currently, document chunks are stored individually into our vector database (PGVector), i.e. the only relationship we record is the one between a chunk and its original document.\n\nWe should expand this to extract the document layout (headers, footers, table, image, caption, â€¦) and the relationships (chunk --> page --> file, previous_chunk --> chunk --> next_chunk, â€¦) and store them into a database, see our [scheme](https://www.figma.com/board/KN4KpSv2w4qqB2bQ1iJ9QJ/Data-Ingestion?node-id=0-1&node-type=canvas&t=eSoUhChXl4slb7SN-0).",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-278/extract-and-save-documentchunk-structure-and-relationships\">CORE-278 Extract and save document/chunk structure and relationships</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3451,
    "title": "Extract knowledge graph from documents",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-11-04T15:45:54Z",
    "updated_at": "2025-02-07T16:06:25Z",
    "labels": [
      "enhancement",
      "Stale",
      "area: backend",
      "rag: ingestion"
    ],
    "body": "We should enable the extraction and storage of a knowledge graph from a set of documents",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-279/extract-knowledge-graph-from-documents\">CORE-279 Extract knowledge graph from documents</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3452,
    "title": "Enable use of embeddings from Vision Language models",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-11-04T15:56:27Z",
    "updated_at": "2025-02-07T16:06:24Z",
    "labels": [
      "enhancement",
      "Stale",
      "area: backend",
      "rag: ingestion",
      "rag: retrieval"
    ],
    "body": "Currently, we use text embeddings. This is fine for textual documents, while it present obvious drawbacks for documents containing non-textual content (images, graphs, schemes, â€¦).\n\nAn alternative, is to use [Visual Language models](https://huggingface.co/blog/manu/colpali) such as [ColPali](https://huggingface.co/blog/manu/colpali) (see also [https://huggingface.co/blog/manu/colpali](https://huggingface.co/blog/manu/colpali), [https://danielvanstrien.xyz/posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html](https://danielvanstrien.xyz/posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html), [https://blog.vespa.ai/retrieval-with-vision-language-models-colpali/](https://blog.vespa.ai/retrieval-with-vision-language-models-colpali/), [https://blog.vespa.ai/scaling-colpali-to-billions/](https://blog.vespa.ai/scaling-colpali-to-billions/))\n\n![image.png](https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/008df0de-ca40-40bd-b20c-777bc3339015/c8d3735b-c5ee-44a1-82df-499f94ed0219?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC8wMDhkZjBkZS1jYTQwLTQwYmQtYjIwYy03NzdiYzMzMzkwMTUvYzhkMzczNWItYzVlZS00NGExLTgyZGYtNDk5Zjk0ZWQwMjE5IiwiaWF0IjoxNzMwNzM1Nzg2LCJleHAiOjMzMzAxMjk1Nzg2fQ.XB9JwA5tQHQPabV00VjnTtm_xT2LQMGPxMul5_nS6pY \"=1506x\")",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-280/enable-use-of-embeddigns-vision-language-models\">CORE-280 Enable use of embeddigns Vision Language models</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3581,
    "title": "Parsing eval: load reference dataset",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-31T09:48:35Z",
    "updated_at": "2025-02-05T15:19:51Z",
    "labels": [],
    "body": "Build classes to load and store reference parsing datasets",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-355/parsing-eval-load-reference-dataset\">CORE-355 Parsing eval: load reference dataset</a></p>"
      }
    ]
  },
  {
    "issue_number": 3565,
    "title": "Parsing eval: run Megaparse on reference dataset",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-28T13:17:12Z",
    "updated_at": "2025-02-05T15:19:51Z",
    "labels": [],
    "body": "We need to be able to run Megaparse in an efficient way (in parallel?) on the different documents (pages) composing the reference dataset. ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-342/parsing-run-megaparse-on-reference-dataset\">CORE-342 Parsing: run Megaparse on reference dataset</a></p>"
      }
    ]
  },
  {
    "issue_number": 3552,
    "title": "Parsing eval: compute OCR metrics",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-22T10:43:19Z",
    "updated_at": "2025-02-05T15:19:51Z",
    "labels": [],
    "body": "From the ground-truth OCR, which, for each PDF page, looks like\n\n```json\n{\n  \"extra\": {\n    \"relation\": [\n      {\n        \"relation_type\": \"parent_son\",\n        \"source_anno_id\": 2,\n        \"target_anno_id\": 3\n      },\n      {\n        \"relation_type\": \"parent_son\",\n        \"source_anno_id\": 5,\n        \"target_anno_id\": 8\n      }\n    ]\n  },\n  \"layout_dets\": [\n    {\n      \"anno_id\": 6,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"title\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            109.3333333333331,\n            121.73651418039208,\n            722.1022134807848,\n            121.73651418039208,\n            722.1022134807848,\n            195.75809149176507,\n            109.3333333333331,\n            195.75809149176507\n          ],\n          \"text\": \"å›½èµ„èƒŒæ™¯åŸºé‡‘æƒ…å†µ\"\n        }\n      ],\n      \"order\": 1,\n      \"poly\": [\n        102.5999912116609,\n        120.87255879760278,\n        719.3118659856144,\n        120.87255879760278,\n        719.3118659856144,\n        194.14083813380114,\n        102.5999912116609,\n        194.14083813380114\n      ],\n      \"text\": \"å›½èµ„èƒŒæ™¯åŸºé‡‘æƒ…å†µ\"\n    },\n    {\n      \"anno_id\": 4,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"text_block\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            99.66504579139392,\n            227.6650457913944,\n            1269.333333333333,\n            227.6650457913944,\n            1269.333333333333,\n            271.3365750838786,\n            99.66504579139392,\n            271.3365750838786\n          ],\n          \"text\": \"2022å¹´å¤‡æ¡ˆåŸºé‡‘è§„æ¨¡å°å¹…å›å‡ï¼Œä½†ä»æœªæ¢å¤è‡³èµ„ç®¡æ–°è§„å‡ºå°å‰çš„æ°´å¹³\"\n        }\n      ],\n      \"order\": 2,\n      \"poly\": [\n        97.71487020898245,\n        226.92028692633914,\n        1271.9932332148471,\n        226.92028692633914,\n        1271.9932332148471,\n        264.88925750697814,\n        97.71487020898245,\n        264.88925750697814\n      ],\n      \"text\": \"2022å¹´å¤‡æ¡ˆåŸºé‡‘è§„æ¨¡å°å¹…å›å‡ï¼Œä½†ä»æœªæ¢å¤è‡³èµ„ç®¡æ–°è§„å‡ºå°å‰çš„æ°´å¹³\"\n    },\n    {\n      \"anno_id\": 3,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"figure_caption\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            253.94664201855937,\n            321.21295194692755,\n            1076.1203813864063,\n            321.21295194692755,\n            1076.1203813864063,\n            364.93470762745034,\n            253.94664201855937,\n            364.93470762745034\n          ],\n          \"text\": \"2014å¹´-2023Q3å›½èµ„èƒŒæ™¯åŸºé‡‘çš„å¤‡æ¡ˆæ•°é‡åŠè§„æ¨¡\"\n        }\n      ],\n      \"order\": 3,\n      \"poly\": [\n        246.96994018554688,\n        318.7444152832031,\n        1088.26025390625,\n        318.7444152832031,\n        1088.26025390625,\n        369.0964660644531,\n        246.96994018554688,\n        369.0964660644531\n      ],\n      \"text\": \"2014å¹´-2023Q3å›½èµ„èƒŒæ™¯åŸºé‡‘çš„å¤‡æ¡ˆæ•°é‡åŠè§„æ¨¡\"\n    },\n    {\n      \"anno_id\": 2,\n      \"category_type\": \"figure\",\n      \"ignore\": false,\n      \"order\": 4,\n      \"poly\": [\n        118.08102792118407,\n        379.29373168945347,\n        1299.4279383691976,\n        379.29373168945347,\n        1299.4279383691976,\n        1028.2773128579047,\n        118.08102792118407,\n        1028.2773128579047\n      ]\n    },\n    {\n      \"anno_id\": 8,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"figure_caption\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            1509.6758069519938,\n            324.34247361866034,\n            2292.4771492866826,\n            324.34247361866034,\n            2292.4771492866826,\n            364.8196229053426,\n            1509.6758069519938,\n            364.8196229053426\n          ],\n          \"text\": \"2014å¹´-2023Q3å›½èµ„èƒŒæ™¯åŸºé‡‘æ•°é‡TOP10åœ°åŒº\"\n        }\n      ],\n      \"order\": 5,\n      \"poly\": [\n        1497.726318359375,\n        318.7418518066406,\n        2301.80224609375,\n        318.7418518066406,\n        2301.80224609375,\n        367.1272888183594,\n        1497.726318359375,\n        367.1272888183594\n      ],\n      \"text\": \"2014å¹´-2023Q3å›½èµ„èƒŒæ™¯åŸºé‡‘æ•°é‡TOP10åœ°åŒº\"\n    },\n    {\n      \"anno_id\": 5,\n      \"category_type\": \"figure\",\n      \"ignore\": false,\n      \"order\": 6,\n      \"poly\": [\n        1370.0374839590943,\n        424.35013794251097,\n        2552.3561471143494,\n        424.35013794251097,\n        2552.3561471143494,\n        1026.8955618700252,\n        1370.0374839590943,\n        1026.8955618700252\n      ]\n    },\n    {\n      \"anno_id\": 9,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"title\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            169.67751098302242,\n            1071.225836994341,\n            328.08580770628134,\n            1071.225836994341,\n            328.08580770628134,\n            1111.655822350311,\n            169.67751098302242,\n            1111.655822350311\n          ],\n          \"text\": \"æ ¸å¿ƒå‘ç°\"\n        }\n      ],\n      \"order\": 7,\n      \"poly\": [\n        170.92340081387997,\n        1069.7956822171332,\n        326.21460986860313,\n        1069.7956822171332,\n        326.21460986860313,\n        1111.7494049722532,\n        170.92340081387997,\n        1111.7494049722532\n      ],\n      \"text\": \"æ ¸å¿ƒå‘ç°\"\n    },\n    {\n      \"anno_id\": 7,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"text_block\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            165.603649650326,\n            1150.009124125815,\n            2509.333333333333,\n            1150.009124125815,\n            2509.333333333333,\n            1198.666666666666,\n            165.603649650326,\n            1198.666666666666\n          ],\n          \"text\": \"- 2018å¹´4æœˆèµ„ç®¡æ–°è§„å‡ºå°åï¼Œå›½èµ„èƒŒæ™¯åŸºé‡‘å¤‡æ¡ˆæ•°é‡å¢é€Ÿæ”¾ç¼“ä¸”è§„æ¨¡éª¤å‡ï¼Œå—æ–°å† ç–«æƒ…å½±å“ï¼Œ2021å¹´æ–°å¢åŸºé‡‘è§„æ¨¡å†æ¬¡ä¸‹é™ï¼Œè™½ç„¶\"\n        },\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            219.22996126565647,\n            1201.1457902508969,\n            2250.770752144285,\n            1201.1457902508969,\n            2250.770752144285,\n            1243.9433217869077,\n            219.22996126565647,\n            1243.9433217869077\n          ],\n          \"text\": \"2022å¹´åŸºé‡‘è§„æ¨¡å›å‡è‡³1.25ä¸‡äº¿å…ƒï¼Œä½†ä»æœªæ¢å¤è‡³èµ„ç®¡æ–°è§„å‡ºå°å‰çš„æ°´å¹³ï¼Œ2023å‰ä¸‰å­£åº¦æ–°å¢è§„æ¨¡ç•¥ä½äº2022å¹´åŒæœŸã€‚\"\n        }\n      ],\n      \"order\": 8,\n      \"poly\": [\n        172.66793877059249,\n        1155.2640660519091,\n        2514.2408071863138,\n        1155.2640660519091,\n        2514.2408071863138,\n        1241.6284871157177,\n        172.66793877059249,\n        1241.6284871157177\n      ],\n      \"text\": \"- 2018å¹´4æœˆèµ„ç®¡æ–°è§„å‡ºå°åï¼Œå›½èµ„èƒŒæ™¯åŸºé‡‘å¤‡æ¡ˆæ•°é‡å¢é€Ÿæ”¾ç¼“ä¸”è§„æ¨¡éª¤å‡ï¼Œå—æ–°å† ç–«æƒ…å½±å“ï¼Œ2021å¹´æ–°å¢åŸºé‡‘è§„æ¨¡å†æ¬¡ä¸‹é™ï¼Œè™½ç„¶ 2022å¹´åŸºé‡‘è§„æ¨¡å›å‡è‡³1.25ä¸‡äº¿å…ƒï¼Œä½†ä»æœªæ¢å¤è‡³èµ„ç®¡æ–°è§„å‡ºå°å‰çš„æ°´å¹³ï¼Œ2023å‰ä¸‰å­£åº¦æ–°å¢è§„æ¨¡ç•¥ä½äº2022å¹´åŒæœŸã€‚\"\n    },\n    {\n      \"anno_id\": 1,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_simplified_chinese\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"text_block\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            161.7899369148969,\n            1278.308761376868,\n            2508,\n            1278.308761376868,\n            2508,\n            1317.333333333333,\n            161.7899369148969,\n            1317.333333333333\n          ],\n          \"text\": \"- æˆªè‡³2023Q3å…¨å›½å›½èµ„èƒŒæ™¯åŸºé‡‘å¤‡æ¡ˆæ•°é‡ç´¯è®¡9196åªï¼ŒåŸºé‡‘è§„æ¨¡ç´¯è®¡8.91ä¸‡äº¿å…ƒã€‚åŸºé‡‘æ³¨å†ŒåŒºåŸŸé›†ä¸­äºå¹¿ä¸œçœã€æµ™æ±Ÿçœå’Œæ±Ÿè‹çœï¼Œå¹¿ä¸œ\"\n        },\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            222.66666666666688,\n            1325.3333333333335,\n            1623.8331583485456,\n            1325.3333333333335,\n            1623.8331583485456,\n            1365.333333333333,\n            222.66666666666688,\n            1365.333333333333\n          ],\n          \"text\": \"çœå›½èµ„èƒŒæ™¯åŸºé‡‘æ€»è§„æ¨¡é¥é¥é¢†å…ˆã€‚å¤‡æ¡ˆåŸºé‡‘æ•°é‡å‰10çš„çœä»½åŸºé‡‘æ•°é‡å å…¨å›½æ€»é‡çš„\"\n        },\n        {\n          \"category_type\": \"equation_ignore\",\n          \"poly\": [\n            1624.4165959289367,\n            1327.0154193159506,\n            1703.7259660435407,\n            1327.0154193159506,\n            1703.7259660435407,\n            1363.1237504250385,\n            1624.4165959289367,\n            1363.1237504250385\n          ],\n          \"text\": \"73%\"\n        },\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            1704.6905743174548,\n            1322.6134268787764,\n            2053.985160092844,\n            1322.6134268787764,\n            2053.985160092844,\n            1370.6736155849724,\n            1704.6905743174548,\n            1370.6736155849724\n          ],\n          \"text\": \"ï¼Œè§„æ¨¡å å…¨å›½æ€»é‡çš„\"\n        },\n        {\n          \"category_type\": \"equation_ignore\",\n          \"poly\": [\n            2055.1374027302004,\n            1326.3706276890023,\n            2149.276980264608,\n            1326.3706276890023,\n            2149.276980264608,\n            1365.7029169328305,\n            2055.1374027302004,\n            1365.7029169328305\n          ],\n          \"text\": \"68%ã€‚\"\n        }\n      ],\n      \"order\": 9,\n      \"poly\": [\n        171.69999831539863,\n        1278.820932742719,\n        2512.084408886781,\n        1278.820932742719,\n        2512.084408886781,\n        1365.690053585406,\n        171.69999831539863,\n        1365.690053585406\n      ],\n      \"text\": \"- æˆªè‡³2023Q3å…¨å›½å›½èµ„èƒŒæ™¯åŸºé‡‘å¤‡æ¡ˆæ•°é‡ç´¯è®¡9196åªï¼ŒåŸºé‡‘è§„æ¨¡ç´¯è®¡8.91ä¸‡äº¿å…ƒã€‚åŸºé‡‘æ³¨å†ŒåŒºåŸŸé›†ä¸­äºå¹¿ä¸œçœã€æµ™æ±Ÿçœå’Œæ±Ÿè‹çœï¼Œå¹¿ä¸œçœå›½èµ„èƒŒæ™¯åŸºé‡‘æ€»è§„æ¨¡é¥é¥é¢†å…ˆã€‚å¤‡æ¡ˆåŸºé‡‘æ•°é‡å‰10çš„çœä»½åŸºé‡‘æ•°é‡å å…¨å›½æ€»é‡çš„ 73% ï¼Œè§„æ¨¡å å…¨å›½æ€»é‡çš„ 68%ã€‚\"\n    },\n    {\n      \"anno_id\": 10,\n      \"category_type\": \"abandon\",\n      \"ignore\": false,\n      \"order\": null,\n      \"poly\": [\n        114.12910090860571,\n        1403.1676953230935,\n        175.21358196554792,\n        1403.1676953230935,\n        175.21358196554792,\n        1462.6586681785502,\n        114.12910090860571,\n        1462.6586681785502\n      ]\n    },\n    {\n      \"anno_id\": 0,\n      \"attribute\": {\n        \"text_background\": \"white\",\n        \"text_language\": \"text_en_ch_mixed\",\n        \"text_rotate\": \"normal\"\n      },\n      \"category_type\": \"footer\",\n      \"ignore\": false,\n      \"line_with_spans\": [\n        {\n          \"category_type\": \"text_span\",\n          \"poly\": [\n            178.18192276049803,\n            1409.8767302579377,\n            288.0868232114207,\n            1409.8767302579377,\n            288.0868232114207,\n            1467.2607048296584,\n            178.18192276049803,\n            1467.2607048296584\n          ],\n          \"text\": \"CVINFO æŠ•ä¸­ä¿¡æ¯\"\n        }\n      ],\n      \"order\": null,\n      \"poly\": [\n        180.18207532211585,\n        1404.2778174322868,\n        289.9793827860912,\n        1404.2778174322868,\n        289.9793827860912,\n        1462.652231000048,\n        180.18207532211585,\n        1462.652231000048\n      ],\n      \"text\": \"CVINFO æŠ•ä¸­ä¿¡æ¯\"\n    }\n  ],\n  \"page_info\": {\n    \"height\": 1500,\n    \"image_path\": \"eastmoney_59cde7e939acc3124df9d3f2c85b5a0ec41b9da1157d5be38e098672022b47cb.pdf_11.jpg\",\n    \"page_attribute\": {\n      \"data_source\": \"PPT2PDF\",\n      \"language\": \"simplified_chinese\",\n      \"layout\": \"1andmore_column\",\n      \"special_issue\": [\n        \"watermark\"\n      ]\n    },\n    \"page_no\": 11,\n    \"width\": 2667\n  }\n}\n```\n\nWe consider the array `layout_dets` and for each element we check the presence of the `text` key. If the key is present, we extract the text with its corresponding `poly`, which encodes the position information: coordinates (x,y) for top-left, top-right, bottom-right, bottom-left corners of the bounding box.\n\nWe extract the same information from the Megaparse output, i.e. for each page we extract the text and and the bounding box, group the pages per block category, document type, language, layout type, and:\n\n* in each page, **find the matching bounding boxes** (within some errors). We want to compare texts for matching bounding boxes, otherwise we can consider that the problem is in the layout detection step.\n* compute the [Levenshtein (edit) distance](https://lightning.ai/docs/torchmetrics/stable/text/edit.html)\n* compute the [Character Error Rate](https://lightning.ai/docs/torchmetrics/stable/text/char_error_rate.html)\n* compute the [Word Error Rate](https://lightning.ai/docs/torchmetrics/stable/text/word_error_rate.html)\n* compute the [Match Error Rate](https://lightning.ai/docs/torchmetrics/stable/text/match_error_rate.html)\n\nWe can also have compute the metrics above across all document types.",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-333/parsing-ocr-metrics\">CORE-333 Parsing: OCR metrics</a></p>"
      }
    ]
  },
  {
    "issue_number": 3582,
    "title": "[Bug]:  Test Jacopo",
    "author": "StanGirard",
    "state": "closed",
    "created_at": "2025-01-31T11:25:08Z",
    "updated_at": "2025-02-05T08:26:33Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\nA bug happened!\n\n### Relevant log output\n\n```bash\nTootot\n```\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-549/[bug]-test-jacopo\">ENT-549 [Bug]: Test Jacopo</a></p>"
      }
    ]
  },
  {
    "issue_number": 3249,
    "title": "Refactor `QuivrQARAGLangGraph`",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-09-24T09:15:44Z",
    "updated_at": "2025-02-03T20:06:31Z",
    "labels": [
      "Stale",
      "area: backend"
    ],
    "body": "`QuivrQARAGLangGraph` should get leaner: it has method that don't directly operate on self. It should offload to some node and only hold a the graph itself: how to build the graph, how to run the graph , how to print the graph.\n\nWe should be able to easily register new nodes defined in external modules and using a common interface.",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-208/enable-langgraph-nodes-from-external-modules\">CORE-208 Enable LangGraph nodes from external modules</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3438,
    "title": "Designing a `PromptRegistry` class",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-10-31T14:51:30Z",
    "updated_at": "2025-02-03T20:06:30Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "Prompts are currently handled in [core/quivr_core/rag/prompts.py](https://github.com/QuivrHQ/quivr/blob/dd2e2ed4fd2d0fea4220594d5e66bc1df111c27c/core/quivr_core/rag/prompts.py) in a non-optimal way.\n\nWe need to implement a more flexible, maintainable and extensible approach, based on a registry pattern. ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-269/designing-a-promptregistry-class\">CORE-269 Designing a `PromptRegistry` class</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3439,
    "title": "Use Pydantic BaseModel in LangGraph states",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-10-31T14:55:47Z",
    "updated_at": "2025-02-03T20:06:29Z",
    "labels": [
      "Stale",
      "area: backend"
    ],
    "body": "Currently, LangGraph agent states are based on `TypedDict`.\n\nBy using Pydantic `BaseModel` we can make them more robust and easy to use. This, however, requires removing any existing dependency on Pydantic V1 in quivr-core",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-270/use-pydantic-basemodel-in-langgraph-states\">CORE-270 Use Pydantic BaseModel in LangGraph states</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3441,
    "title": "Dynamic retriever",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-10-31T15:02:49Z",
    "updated_at": "2025-02-03T20:06:28Z",
    "labels": [
      "Stale"
    ],
    "body": "Refactor the dynamic retriever currently in `QuivrQARAGLangGraph` to base it on the `langchain.Retriever` base class, that we can then pass as a LangGraph node",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-272/dynamic-retriever\">CORE-272 Dynamic retriever</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3442,
    "title": "Rationalise configuration modifiable at run-time",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-10-31T15:06:03Z",
    "updated_at": "2025-02-03T20:06:27Z",
    "labels": [
      "Stale",
      "area: backend"
    ],
    "body": "We should refactor the way we deal with the initial (user-provided) configuration of the workflow nodes and the modification of some of the configuration parameters at run-time, for instance in the methods `get_reranker` and   `filter_chunks_by_relevance`.\n\nWe currently use pop from kwargs to update the default parameters read from the input and saved in the different config classes ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-273/rationalise-configuration-modifiable-at-run-time\">CORE-273 Rationalise configuration modifiable at run-time</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3443,
    "title": "Using Pydantic validators in the configuration classes",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-10-31T15:08:58Z",
    "updated_at": "2025-02-03T20:06:26Z",
    "labels": [
      "Stale",
      "area: backend"
    ],
    "body": "Currently, we manually implement validation in [core/quivr_core/rag/entities/config.py](https://github.com/QuivrHQ/quivr/blob/dd2e2ed4fd2d0fea4220594d5e66bc1df111c27c/core/quivr_core/rag/entities/config.py) because of the mixing of Pydantic V1 and V2 in our codebase, which breaks the use of the V2 validator decorators.\n\nOnce Pydantic V1 is removed, we should refactor the configuration classes to use the V2 validator decorators",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-274/using-pydantic-validators-in-the-configuration-classes\">CORE-274 Using Pydantic validators in the configuration classes</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3444,
    "title": "Refactor `ToolRegistry`",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-10-31T15:11:48Z",
    "updated_at": "2025-02-03T20:06:25Z",
    "labels": [
      "Stale"
    ],
    "body": "Currently, we deal with tools as in [core/quivr_core/llm_tools/web_search_tools.py](https://github.com/QuivrHQ/quivr/blob/dd2e2ed4fd2d0fea4220594d5e66bc1df111c27c/core/quivr_core/llm_tools/web_search_tools.py).\n\nWe should rather define our own `QuivrToolBase` class where tools inherit from, e.g.\n\n`class TavilyTool(BaseTool)`\n\nWe can then use a ProxyMapping as a registry to the tools. We could create tools (as classes or objects) and retrieve them.",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-275/refactor-toolregistry\">CORE-275 Refactor `ToolRegistry`</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3421,
    "title": "Better logging",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-10-23T09:15:29Z",
    "updated_at": "2025-02-02T00:26:38Z",
    "labels": [
      "Stale"
    ],
    "body": "* log all error and exception level in parseable\n* log request body \n* log response body",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-254/better-logging\">CORE-254 Better logging</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3579,
    "title": "Remove pympler dependency",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-30T10:04:48Z",
    "updated_at": "2025-01-30T12:32:31Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-354/remove-pympler-dependency\">CORE-354 Remove pympler dependency</a></p>"
      }
    ]
  },
  {
    "issue_number": 3561,
    "title": "Restructure relevant information in Notion",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-28T09:08:14Z",
    "updated_at": "2025-01-29T14:38:00Z",
    "labels": [],
    "body": "Copy and restructure in [Notion](https://www.notion.so/getquivr/ADR-Lejuge-177312462d4b80409102c69bcad78152?pvs=4) relevant information from [CORE-325](https://linear.app/getquivr/issue/CORE-325/retrieval-generation-datasets) [CORE-335](https://linear.app/getquivr/issue/CORE-335/parsing-evaluation-datasets) [CORE-331](https://linear.app/getquivr/issue/CORE-331/parsing-layout-metrics) [CORE-333](https://linear.app/getquivr/issue/CORE-333/parsing-ocr-metrics)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-338/restructure-relevant-information-in-notion\">CORE-338 Restructure relevant information in Notion</a></p>"
      }
    ]
  },
  {
    "issue_number": 3570,
    "title": "Fix error on Hugging Face CRAG dataset",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-28T17:56:04Z",
    "updated_at": "2025-01-29T13:02:05Z",
    "labels": [
      "bug"
    ],
    "body": "We are observing the error below on certain subsets of CRAG, the reason being that the alt_ans field can sometimes contain an empty list (which apparently is interpreted as a number) or a list of strings. This prevents us from visualizing the dataset on the platform, but also to retrieve using `import load_dataset from dataset`\n\n```\nCannot load the dataset split (in streaming mode) to extract the first rows.\nError code:   StreamingRowsError\nException:    ArrowInvalid\nMessage:      JSON parse error: Column(/alt_ans/[]) changed from number to string in row 7\nTraceback:    Traceback (most recent call last):\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/packaged_modules/json/json.py\", line 160, in _generate_tables\n                  df = pandas_read_json(f)\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/packaged_modules/json/json.py\", line 38, in pandas_read_json\n                  return pd.read_json(path_or_buf, **kwargs)\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/pandas/io/json/_json.py\", line 815, in read_json\n                  return json_reader.read()\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/pandas/io/json/_json.py\", line 1025, in read\n                  obj = self._get_object_parser(self.data)\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/pandas/io/json/_json.py\", line 1051, in _get_object_parser\n                  obj = FrameParser(json, **kwargs).parse()\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/pandas/io/json/_json.py\", line 1187, in parse\n                  self._parse()\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/pandas/io/json/_json.py\", line 1403, in _parse\n                  ujson_loads(json, precise_float=self.precise_float), dtype=None\n              ValueError: Trailing data\n              \n              During handling of the above exception, another exception occurred:\n              \n              Traceback (most recent call last):\n                File \"/src/services/worker/src/worker/utils.py\", line 99, in get_rows_or_raise\n                  return get_rows(\n                File \"/src/libs/libcommon/src/libcommon/utils.py\", line 197, in decorator\n                  return func(*args, **kwargs)\n                File \"/src/services/worker/src/worker/utils.py\", line 77, in get_rows\n                  rows_plus_one = list(itertools.islice(ds, rows_max_number + 1))\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/iterable_dataset.py\", line 2093, in __iter__\n                  for key, example in ex_iterable:\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/iterable_dataset.py\", line 279, in __iter__\n                  for key, pa_table in self.generate_tables_fn(**gen_kwags):\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/packaged_modules/json/json.py\", line 163, in _generate_tables\n                  raise e\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/packaged_modules/json/json.py\", line 137, in _generate_tables\n                  pa_table = paj.read_json(\n                File \"pyarrow/_json.pyx\", line 308, in pyarrow._json.read_json\n                File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\n                File \"pyarrow/error.pxi\", line 91, in pyarrow.lib.check_status\n              pyarrow.lib.ArrowInvalid: JSON parse error: Column(/alt_ans/[]) changed from number to string in row 7\n```\n\nStill having issues with this error, we should download and load the JSON file ourself, without passing through the load_dataset function",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-347/fix-error-on-hugging-face-crag-dataset\">CORE-347 Fix error on Hugging Face CRAG dataset</a></p>"
      }
    ]
  },
  {
    "issue_number": 3428,
    "title": "Add Structured Parsing",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-25T08:23:53Z",
    "updated_at": "2025-01-29T08:03:52Z",
    "labels": [
      "enhancement"
    ],
    "body": "From a BaseModel class, we return the parsing of the file organized.",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-260/add-structured-parsing\">CORE-260 Add Structured Parsing</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3569,
    "title": "Retrieval + generation eval: prepare CRAG dataset in subsets",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-28T16:20:26Z",
    "updated_at": "2025-01-28T18:10:01Z",
    "labels": [],
    "body": "We need to split the CRAG dataset into smaller, more manageable datasets. We have split to initial set of questions into 20 subsets of 135 questions each, using stratified sampling to maintain the statistical properties of the initial dataset. The subsets are available from [https://huggingface.co/datasets/Quivr/CRAG/tree/main/subset/crag_task_1_and_2](https://huggingface.co/datasets/Quivr/CRAG/tree/main/subset/crag_task_1_and_2)\n\nEach subset will therefore contain 135 x 5 = 675 documents in html format",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-346/retrieval-generation-prepare-crag-dataset-in-subsets\">CORE-346 Retrieval + generation: prepare CRAG dataset in subsets</a></p>"
      }
    ]
  },
  {
    "issue_number": 3543,
    "title": "Retrieval + generation eval datasets",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2025-01-02T15:38:57Z",
    "updated_at": "2025-01-28T18:09:54Z",
    "labels": [],
    "body": "Related projects:\n\n* [https://github.com/RUC-NLPIR/FlashRAG](https://github.com/RUC-NLPIR/FlashRAG)\n* [https://github.com/DocAILab/XRAG](https://github.com/DocAILab/XRAG/tree/main)\n* [https://github.com/VenkteshV/DEXTER](https://github.com/VenkteshV/DEXTER)\n\n## MIRACL\n\n*MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages*\n\n* [https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00595/117438](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00595/117438)\n* [https://github.com/project-miracl/miracl](https://github.com/project-miracl/miracl)\n* [https://huggingface.co/miracl](https://huggingface.co/miracl)\n* Wikipedia based\n\n<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/f5ab251e-4132-403d-96c7-75f69f801f30/648090fb-647d-4288-95f7-04a5ca93e4ad?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC9mNWFiMjUxZS00MTMyLTQwM2QtOTZjNy03NWY2OWY4MDFmMzAvNjQ4MDkwZmItNjQ3ZC00Mjg4LTk1ZjctMDRhNWNhOTNlNGFkIiwiaWF0IjoxNzM3NjMwMTgyLCJleHAiOjMzMzA4MTkwMTgyfQ.71uuZ-b9MO4FTN_VPe_YYXLk__Lmrhjr1knuTOeFoRI \" alt=\"Screenshot 2025-01-23 at 12.02.24.png\" width=\"486\" data-linear-height=\"648\" />\n\n## UDA\n\n*UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis*\n\n* [https://arxiv.org/pdf/2406.15187](https://arxiv.org/pdf/2406.15187)\n* [https://huggingface.co/datasets/qinchuanhui/UDA-QA](https://huggingface.co/datasets/qinchuanhui/UDA-QA)\n* [https://github.com/qinchuanhui/UDA-Benchmark](https://github.com/qinchuanhui/UDA-Benchmark)\n* 2 965 documents\n* 2 9590 expert-annotated Q&A pairs\n* English only\n\n## REPLIQA\n\n*REPLIQA: A QUESTION-ANSWERING DATASET FOR BENCHMARKING LLMS ON UNSEEN REFERENCE CONTENT*\n\n* [https://arxiv.org/pdf/2406.11811](https://arxiv.org/pdf/2406.11811)\n* [https://huggingface.co/datasets/ServiceNow/repliqa](https://huggingface.co/datasets/ServiceNow/repliqa)\n* 89 770 question-answer pairs based on 17 954 reference documents (5 questions per document)\n* each document is quite short (3 pages)\n* English only\n\n<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/1c1bd14d-77d9-4354-91ac-708cf01425e3/7eaf9bf1-56a6-45e5-930b-759863145078?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC8xYzFiZDE0ZC03N2Q5LTQzNTQtOTFhYy03MDhjZjAxNDI1ZTMvN2VhZjliZjEtNTZhNi00NWU1LTkzMGItNzU5ODYzMTQ1MDc4IiwiaWF0IjoxNzM3NDUwOTMyLCJleHAiOjMzMzA4MDEwOTMyfQ.9al1eKe8kU2UGYalY-xB8LSUoFeyBmUK_fSCeX7vsDk \" alt=\"Screenshot 2025-01-21 at 10.14.53.png\" width=\"686\" data-linear-height=\"162\" />\n\n## CRAG\n\n*CRAG â€“ Comprehensive RAG Benchmark*\n\n* [https://arxiv.org/pdf/2406.04744](https://arxiv.org/pdf/2406.04744)\n* [https://github.com/facebookresearch/CRAG](https://github.com/facebookresearch/CRAG)\n* 4 409 question-answer pairs: 2 425 Web Questions and 1 984 KG Questions\n* For each Web Question, 50 HTML pages stored from the Brave search API (220 000 web pages in total)\n* five domains: Finance, Sports, Music, Movie, and Open domain\n* seven types of questions: questions with Conditions, Comparison questions, Aggregation questions, Multi-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions\n\n<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/1d7b9d41-8948-461d-9b8b-79b9796b3752/18504c65-90a3-4402-b7bd-14d6b9b00859?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC8xZDdiOWQ0MS04OTQ4LTQ2MWQtOWI4Yi03OWI5Nzk2YjM3NTIvMTg1MDRjNjUtOTBhMy00NDAyLWI3YmQtMTRkNmI5YjAwODU5IiwiaWF0IjoxNzM3NTYzODg4LCJleHAiOjMzMzA4MTIzODg4fQ.IJaoKLZA_ZOOMXN3cSoMnBdQPL4diY_8-yjnPQ2kFQc \" alt=\"Screenshot 2025-01-22 at 17.37.56.png\" width=\"712\" data-linear-height=\"445\" />\n\n* English only\n* 3 tasks\n  * Task 1: Retrieval Summarization. 5 web pages for each question, which are likely, but not guaranteed, to be relevant to the question.\n  * Task 2: KG and Web Retrieval Augmentation via mock APIs\n  * Task 3 provides both web search results (50 web pages instead of 5) and mock APIs\n\n## Air-Bench\n\n*AIR-BENCH: Automated Heterogeneous Information Retrieval Benchmark*\n\n* [https://arxiv.org/pdf/2412.13102](https://arxiv.org/pdf/2412.13102)\n* [https://github.com/AIR-Bench/AIR-Bench](https://github.com/AIR-Bench/AIR-Bench)\n* [https://huggingface.co/AIR-Bench](https://huggingface.co/AIR-Bench)\n* Different languages: English, Chinese, Spanish, French, German, Russian, Japanese, Korean, Arabic, Persian, Indonesian, Hindi, Bengali\n* Different topics: wiki, web, news, healthcare, law, finance, arxiv, book, science\n* Pretty interesting, but, starting from an initial corpus, they generate a synthetic set of positive and negative chunks, then a question whose answer is contained in the positive set of chunks. This means that with this benchmark we can test the retrieval, but not the chunking, since we don't have a full document corresponding to each question, just a set of (positive and negative) chunks.\n\n## GoogleNQ\n\n* [https://ai.google.com/research/NaturalQuestions](https://ai.google.com/research/NaturalQuestions)\n  * [https://huggingface.co/datasets/google-research-datasets/natural_questions](https://huggingface.co/datasets/google-research-datasets/natural_questions)\n  * 307K training examples, 8K examples for development, and a further 8K examples for testing\n* [https://research.google/pubs/natural-questions-a-benchmark-for-question-answering-research/](https://research.google/pubs/natural-questions-a-benchmark-for-question-answering-research/)\n* input = question, Wikipedia page\n* output = long answer (similar to chunk), short answer\n* Limitations:\n  * output (long answer) can be null if there is no answer on the page, or i**f the information required to answer the question is spread across many paragraphs.** The short answer(s) can be a span or set of spans (typically entities) within the long answer that answer the question.\n  * only english\n  * based on Wikipedia\n* Observations:\n  * can adapt this to make it more relevant for RAG:\n    * select a sub-sample of N Wikipedia pages\n      * how many different pages are present in the dataset?\n    * select all questions related to those pages\n    * chunk and embed all pages\n    * run RAG on corpus\n    * when N=1, this is equivalent to the original NQ benchmark, where each question is paired with its corresponding document\n\n## MMDocIR\n\n*Benchmarking Multi-Modal Retrieval for Long Documents*\n\n* [https://arxiv.org/pdf/2501.08828](https://arxiv.org/pdf/2501.08828)\n* [https://huggingface.co/MMDocIR](https://huggingface.co/MMDocIR)\n* 313 long documents averaging 65.1 pages\n* evaluation set encompasses 1 658 questions, 2 107 page labels, and 2 638 layout labels\n\n## MTRAG\n\n*MTRAG: A Multi-Turn Conversational Benchmark for Evaluating Retrieval-Augmented Generation Systems*\n\n* [https://arxiv.org/pdf/2501.03468](https://arxiv.org/pdf/2501.03468)\n* [https://github.com/ibm/mt-rag-benchmark](https://github.com/ibm/mt-rag-benchmark)\n* 110 multi-turn conversations that are converted to 842 evaluation tasks\n\n<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/8f33e771-7282-4c3a-926a-014fafa09bd8/e220d2f0-d890-42a1-8b5e-9be2c4a91200?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC84ZjMzZTc3MS03MjgyLTRjM2EtOTI2YS0wMTRmYWZhMDliZDgvZTIyMGQyZjAtZDg5MC00MmExLThiNWUtOWJlMmM0YTkxMjAwIiwiaWF0IjoxNzM3NDQ3Nzk4LCJleHAiOjMzMzA4MDA3Nzk4fQ.BWt8wMs9-lcR-lFQpiMEbEzZXnN_llWjV6-kTRsfYL0 \" alt=\"Screenshot 2025-01-21 at 09.23.10.png\" width=\"656\" data-linear-height=\"208\" />\n\n## CYPHERBENCH\n\n*CYPHERBENCH: Towards Precise Retrieval over Full-scale Modern Knowledge Graphs in the LLM Era*\n\n* [https://arxiv.org/pdf/2412.18702](https://arxiv.org/pdf/2412.18702)\n* [https://huggingface.co/datasets/megagonlabs/cypherbench](https://huggingface.co/datasets/megagonlabs/cypherbench)\n\n## **CQUAE**\n\n*CQuAE : Un nouveau corpus de question-rÃ©ponse pour lâ€™enseignement*\n\n* [https://gitlab.lisn.upsaclay.fr/gerald/cquae](https://gitlab.lisn.upsaclay.fr/gerald/cquae)\n* [https://aclanthology.org/2024.jeptalnrecital-taln.4.pdf](https://aclanthology.org/2024.jeptalnrecital-taln.4.pdf)\n\n## DEXTER\n\n*DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs*\n\n* [https://gitlab.tudelft.nl/anonymous_arr/bcqa_data](https://gitlab.tudelft.nl/anonymous_arr/bcqa_data)\n* [https://github.com/VenkteshV/DEXTER](https://github.com/VenkteshV/DEXTER)\n* [https://arxiv.org/pdf/2406.17158](https://arxiv.org/pdf/2406.17158)\n\n<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/735b3aa5-92cc-4f4d-aa61-93cfcc7345e4/7676b4b5-b1da-4082-b2e4-810ffec6df38?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC83MzViM2FhNS05MmNjLTRmNGQtYWE2MS05M2NmY2M3MzQ1ZTQvNzY3NmI0YjUtYjFkYS00MDgyLWIyZTQtODEwZmZlYzZkZjM4IiwiaWF0IjoxNzM3NDQ4MDcyLCJleHAiOjMzMzA4MDA4MDcyfQ.PyXfY1o4INIf83HvO31Kj5MCs9xTyhdjDFZxh0-wHHc \" alt=\"Screenshot 2025-01-21 at 09.27.33.png\" width=\"696\" data-linear-height=\"356\" />",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-325/first-datasets-finding\">CORE-325 First Datasets finding</a></p>"
      }
    ]
  },
  {
    "issue_number": 3564,
    "title": "Parsing eval: select and prepare OmniDocBench subsets",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-28T10:06:39Z",
    "updated_at": "2025-01-28T18:09:25Z",
    "labels": [],
    "body": "We need subsets of the OmniDocBench dataset: only EN pages, and subsets (using stratified sampling) over the different categories \"pdf type\", \"layout\" and \"special issues\". We also need to select a subset with pages without any masking, to be able to use both the \"original\" (native) pdf and the image-pdf\n\n* EN only subsets pushed to [https://huggingface.co/datasets/Quivr/OmniDocBench/tree/main/subset/EN](https://huggingface.co/datasets/Quivr/OmniDocBench/tree/main/subset/EN)\n* EN only subsets of pages without masking pushed to [https://huggingface.co/datasets/Quivr/OmniDocBench/tree/main/subset/EN/no_mask](https://huggingface.co/datasets/Quivr/OmniDocBench/tree/main/subset/EN/no_mask)\n\nScripts to perform filtering/stratified sampling also pushed to [https://huggingface.co/datasets/Quivr/OmniDocBench](https://huggingface.co/datasets/Quivr/OmniDocBench)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-341/parsing-select-omnidocbench-subsets\">CORE-341 Parsing: select OmniDocBench subsets</a></p>"
      }
    ]
  },
  {
    "issue_number": 3562,
    "title": "Retrieval + generation eval: save CRAG dataset",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-28T09:21:41Z",
    "updated_at": "2025-01-28T18:09:15Z",
    "labels": [],
    "body": "We have posted on [HugginFace](https://huggingface.co/datasets/Quivr/crag) the CRAG dataset for tasks 1 and 2 (5 web search result per each question), where we have splitted the full sample into 5 sub-samples for an easier download/usage.\n\n[*CRAG â€“ Comprehensive RAG Benchmark*](https://github.com/facebookresearch/CRAG)\\* (see\\* [https://arxiv.org/pdf/2406.04744](https://arxiv.org/pdf/2406.04744)):\n\n* 4 409 question-answer pairs (2 706 in the public, dev set): 2 425 Web Questions and 1 984 KG Questions\n* For each Web Question, 50 HTML pages stored from the Brave search API (220 000 web pages in total)\n* five domains: Finance, Sports, Music, Movie, and Open domain\n* seven types of questions: questions with Conditions, Comparison questions, Aggregation questions, Multi-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions\n* English only\n* 3 tasks\n  * Task 1: Retrieval Summarization. 5 web pages for each question, which are likely, but not guaranteed, to be relevant to the question.\n  * Task 2: KG and Web Retrieval Augmentation via mock APIs\n  * Task 3 provides both web search results (50 web pages instead of 5) and mock APIs\n\n<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/1d7b9d41-8948-461d-9b8b-79b9796b3752/18504c65-90a3-4402-b7bd-14d6b9b00859?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC8xZDdiOWQ0MS04OTQ4LTQ2MWQtOWI4Yi03OWI5Nzk2YjM3NTIvMTg1MDRjNjUtOTBhMy00NDAyLWI3YmQtMTRkNmI5YjAwODU5IiwiaWF0IjoxNzM3NTYzODg4LCJleHAiOjMzMzA4MTIzODg4fQ.IJaoKLZA_ZOOMXN3cSoMnBdQPL4diY_8-yjnPQ2kFQc \" alt=\"Screenshot 2025-01-22 at 17.37.56.png\" width=\"712\" data-linear-height=\"445\" />",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-339/retrieval-generation-save-dataset\">CORE-339 Retrieval + generation: save dataset</a></p>"
      }
    ]
  },
  {
    "issue_number": 3551,
    "title": "Parsing eval: save OmniDocBench dataset to experiment tracker",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-22T10:30:37Z",
    "updated_at": "2025-01-28T18:08:44Z",
    "labels": [],
    "body": "We pushed a copy of OmniDocBench on our Hugging Face organization: [https://huggingface.co/datasets/Quivr/OmniDocBench/](https://huggingface.co/datasets/Quivr/OmniDocBench/)\n\n[OmniDocBench](https://github.com/opendatalab/OmniDocBench) includes 9 different types of PDF pages, 5 layouts, and 3 languages.\n\n<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/a19838fb-6f56-423d-b2af-baa1571e0612/6efc397d-8f90-4691-a545-da4666356583?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC9hMTk4MzhmYi02ZjU2LTQyM2QtYjJhZi1iYWExNTcxZTA2MTIvNmVmYzM5N2QtOGY5MC00NjkxLWE1NDUtZGE0NjY2MzU2NTgzIiwiaWF0IjoxNzM3NTM0MDA0LCJleHAiOjMzMzA4MDk0MDA0fQ.zGZKA2FxntdG8q_pvLnXdGXKNKTy-9V0VkVjlstGXjM \" alt=\"Screenshot 2025-01-22 at 09.18.02.png\" width=\"344\" data-linear-height=\"435\" />\n\nPages can have a non-white background and rotated text.\n\n<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/6a746b60-9b13-48b7-ab15-2029079fbbe0/dad31bf8-5934-44a9-844f-6d1523153101?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC82YTc0NmI2MC05YjEzLTQ4YjctYWIxNS0yMDI5MDc5ZmJiZTAvZGFkMzFiZjgtNTkzNC00NGE5LTg0NGYtNmQxNTIzMTUzMTAxIiwiaWF0IjoxNzM3NTM0MTk2LCJleHAiOjMzMzA4MDk0MTk2fQ.oMLEJ-dzCHYnpWL21BWDD9cFi0V3oPTR16A8OdKgPVg \" alt=\"data_diversity.png\" width=\"6125\" data-linear-height=\"3424\" />\n\nBelow some examples of the different pages, taken from [https://huggingface.co/datasets/opendatalab/OmniDocBench/tree/main](https://huggingface.co/datasets/opendatalab/OmniDocBench/tree/main)\n\n<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/f55635e8-cfba-43ec-a52b-ea70a6451f6f/17dc0e28-6952-4c6d-91c1-30b728c97c26?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC9mNTU2MzVlOC1jZmJhLTQzZWMtYTUyYi1lYTcwYTY0NTFmNmYvMTdkYzBlMjgtNjk1Mi00YzZkLTkxYzEtMzBiNzI4Yzk3YzI2IiwiaWF0IjoxNzM3NTM0MjI0LCJleHAiOjMzMzA4MDk0MjI0fQ.7G2i4Zp8IL-VX0ernPCR0X-kpO2sA5crc-ac3c3c4uo \" alt=\"show_pdf_types_1.png\" width=\"6362\" data-linear-height=\"8410\" />\n\n<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/b31676d0-0d0c-4022-bd36-f3ba1276ff49/14f36e5e-33a9-48b2-8e22-f95b83882667?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC9iMzE2NzZkMC0wZDBjLTQwMjItYmQzNi1mM2JhMTI3NmZmNDkvMTRmMzZlNWUtMzNhOS00OGIyLThlMjItZjk1YjgzODgyNjY3IiwiaWF0IjoxNzM3NTM0MjcyLCJleHAiOjMzMzA4MDk0MjcyfQ.SBs1W1Uop62s-StXjHmfPVeRjuVfc1FQkNwnSuohOuo \" alt=\"show_pdf_types_2.png\" width=\"6355\" data-linear-height=\"6757\" />",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-332/parsing-save-benchmark-dataset-to-experiment-tracker\">CORE-332 Parsing: save benchmark dataset to experiment tracker</a></p>"
      }
    ]
  },
  {
    "issue_number": 3559,
    "title": "AttributeError: type object 'LLMEndpoint' has no attribute '_cache'",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2025-01-27T14:40:43Z",
    "updated_at": "2025-01-27T14:48:43Z",
    "labels": [
      "bug"
    ],
    "body": "Sentry Issue: [PYTHON-FASTAPI-1A4](https://quivr-brain.sentry.io/issues/6250899282/?referrer=Linear)\n\n```\nAttributeError: type object 'LLMEndpoint' has no attribute '_cache'\n  File \"quivr_api/modules/rag_service/rag_service.py\", line 293, in generate_answer_stream\n    brain_core = await self.build_brain_core(retrieval_config)\n  File \"quivr_api/modules/rag_service/rag_service.py\", line 251, in build_brain_core\n    llm = self.get_llm(retrieval_config)\n  File \"quivr_api/modules/rag_service/rag_service.py\", line 169, in get_llm\n    return LLMEndpoint.from_config(retrieval_config.llm_config)\n  File \"quivr_core/llm/llm_endpoint.py\", line 171, in from_config\n    cls._cache[cache_key] = instance\n```",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-337/attributeerror-type-object-llmendpoint-has-no-attribute-cache\">CORE-337 AttributeError: type object 'LLMEndpoint' has no attribute '_cache'</a></p>"
      }
    ]
  },
  {
    "issue_number": 3538,
    "title": "Return workflow nodes metadata",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-12-30T10:43:43Z",
    "updated_at": "2025-01-23T13:13:12Z",
    "labels": [
      "enhancement"
    ],
    "body": "We want to show the user the state / evolution of a workflow, i.e. which steps/nodes are being executed. This information is already provided by LangChain / LangGraph, we only need to yield it back to quivr-api.",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-322/return-workflow-nodes-metadata\">CORE-322 Return workflow nodes metadata</a></p>"
      },
      {
        "user": "jacopo-chevallard",
        "body": "Completed in [https://github.com/QuivrHQ/quivr/pull/3539](https://github.com/QuivrHQ/quivr/pull/3539)"
      }
    ]
  },
  {
    "issue_number": 3554,
    "title": "Parsing evaluation datasets",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2025-01-23T10:15:29Z",
    "updated_at": "2025-01-23T10:16:00Z",
    "labels": [],
    "body": "Related projects:\n\n* [https://github.com/RUC-NLPIR/FlashRAG](https://github.com/RUC-NLPIR/FlashRAG)\n* [https://github.com/DocAILab/XRAG](https://github.com/DocAILab/XRAG/tree/main)\n* [https://github.com/VenkteshV/DEXTER](https://github.com/VenkteshV/DEXTER)\n\n## OmniDocBench\n\n*OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations*\n\n* [https://arxiv.org/pdf/2412.07626](https://arxiv.org/pdf/2412.07626)\n* [https://huggingface.co/datasets/opendatalab/OmniDocBench](https://huggingface.co/datasets/opendatalab/OmniDocBench)\n* [https://github.com/opendatalab/OmniDocBench](https://github.com/opendatalab/OmniDocBench)\n* 981 PDF pages, covering 9 document types, 4 layout types, and 3 language types\n\n## Noisy OCR\n\n*OCR with Tesseract, Amazon Textract, and Google Document AI: a benchmarking experiment*\n\n* [https://github.com/Hegghammer/noisy-ocr-benchmark](https://github.com/Hegghammer/noisy-ocr-benchmark)\n* [https://link.springer.com/article/10.1007/s42001-021-00149-1](https://link.springer.com/article/10.1007/s42001-021-00149-1)\n* [https://zenodo.org/records/5068735](https://zenodo.org/records/5068735)\n* 51 304 pages\n\n## XFUND\n\n*XFUND: A Benchmark Dataset for Multilingual Visually Rich Form Understanding*\n\n* [https://aclanthology.org/2022.findings-acl.253.pdf](https://aclanthology.org/2022.findings-acl.253.pdf)\n* [https://github.com/doc-analysis/XFUND](https://github.com/doc-analysis/XFUND)\n* [https://github.com/doc-analysis/XFUND/releases/tag/v1.0](https://github.com/doc-analysis/XFUND/releases/tag/v1.0)\n* 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese) with 1 393 fully annotated forms. Each language includes 199 forms\n\n## VRDU\n\n*VRDU: A Benchmark for Visually-rich Document Understanding*\n\n* [https://dl.acm.org/doi/pdf/10.1145/3580305.3599929](https://dl.acm.org/doi/pdf/10.1145/3580305.3599929)\n* [https://github.com/google-research-datasets/vrdu](https://github.com/google-research-datasets/vrdu)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-335/parsing-evaluation-datasets\">CORE-335 Parsing evaluation datasets</a></p>"
      }
    ]
  },
  {
    "issue_number": 3464,
    "title": "Add Modular Composition & First Formatters",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-11-08T08:21:28Z",
    "updated_at": "2025-01-22T13:15:55Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-291/add-modular-composition-and-first-formatters\">CORE-291 Add Modular Composition &amp; First Formatters</a></p>"
      },
      {
        "user": "StanGirard",
        "body": "@chloe il y a une pr a review ? "
      },
      {
        "user": "chloedia",
        "body": "Non c'est done ! https://github.com/QuivrHQ/MegaParse/pull/175\n"
      }
    ]
  },
  {
    "issue_number": 3396,
    "title": "Remove Crawling dependencies",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-18T09:41:07Z",
    "updated_at": "2025-01-22T11:50:36Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-245/remove-crawling-dependencies\">CORE-245 Remove Crawling dependencies</a></p>"
      }
    ]
  },
  {
    "issue_number": 3395,
    "title": "Web Crawling through Megaparse SDK",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-18T09:40:58Z",
    "updated_at": "2025-01-22T11:45:09Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-244/replace-crawling-with-megaparse-service\">CORE-244 Replace crawling with Megaparse Service</a></p>"
      }
    ]
  },
  {
    "issue_number": 3536,
    "title": "Bug when URL in brain => stream error",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-12-20T17:56:41Z",
    "updated_at": "2025-01-22T10:58:16Z",
    "labels": [
      "bug"
    ],
    "body": "ValidationError: 3 validation errors for Sources\n\nbackend-api        | name\n\nbackend-api        |   Input should be a valid string \\[type=string_type, input_value=None, input_type=NoneType\\]\n\nbackend-api        |     For further information visit [https://errors.pydantic.dev/2.9/v/string_type](https://errors.pydantic.dev/2.9/v/string_type)\n\nbackend-api        | source_url\n\nbackend-api        |   Input should be a valid string \\[type=string_type, input_value=None, input_type=NoneType\\]\n\nbackend-api        |     For further information visit [https://errors.pydantic.dev/2.9/v/string_type](https://errors.pydantic.dev/2.9/v/string_type)\n\nbackend-api        | original_file_name\n\nbackend-api        |   Input should be a valid string \\[type=string_type, input_value=None, input_type=NoneType\\]\n\nbackend-api        |     For further information visit [https://errors.pydantic.dev/2.9/v/string_type](https://errors.pydantic.dev/2.9/v/string_type)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-320/bug-when-url-in-brain-=-stream-error\">CORE-320 Bug when URL in brain =&gt; stream error</a></p>"
      }
    ]
  },
  {
    "issue_number": 3457,
    "title": "[Bug]: How can i config the Quivr to use local Ollama model ? i can't find the config example and no env.example file ",
    "author": "lijinxinKing",
    "state": "closed",
    "created_at": "2024-11-06T03:15:27Z",
    "updated_at": "2025-01-20T10:05:21Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\nHow can i config the Quivr to use local Ollama model ? i can't find the config example and no env.example file \n\n### Relevant log output\n\n```bash\nHow can i config the Quivr to use local Ollama model ? i can't find the config example and no env.example file\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-285/[bug]-how-can-i-config-the-quivr-to-use-local-ollama-model-i-cant-find\">CORE-285 [Bug]: How can i config the Quivr to use local Ollama model ? i can't find the config example and no env.example file</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey @lijinxinKing! I'm here to help you with any bugs, questions, or contributions. Let's dive into this together!\n\n<!-- Answer -->\nTo configure Quivr to use a local Ollama model, follow these steps:\n\n1. **Install Ollama**: Download and install Ollama from their [website](https://ollama.ai/).\n\n2. **Run Ollama in the Background**: Use the command below to start Ollama:\n   ```bash\n   ollama run llama2\n   ```\n\n3. **Run Migrations**: Move the migration file and reset the database:\n   ```bash\n   mv supabase/migrations/local_20240107152745_ollama.sql supabase/migrations/20240107152745_ollama.sql\n   supabase db reset\n   ```\n\n4. **Add Ollama Model to Quivr**: Update the `user_settings` table in Supabase to include the Ollama models:\n   ```json\n   [\n     \"ollama/llama2\",\n     \"ollama/mistral\"\n   ]\n   ```\n   If you want to add it to your user only, you can add it to the `models` column in the `user_settings` table. If you want to set it as default for all new users, drop the entire table with the following command:\n   ```sql\n   DELETE TABLE user_settings;\n   ```\n\n5. **Configure .env File**: Update your `.env` file to activate the Ollama API by setting the `OLLAMA_API_BASE_URL`:\n   ```env\n   # LOCAL\n   OLLAMA_API_BASE_URL=http://host.docker.internal:11434\n   ```\n\n6. **Integrate with Quivr**: Ensure that the `QuivrRAG` class in `quivr_rag.py` is configured to use the Ollama model. The `ChatLiteLLM` class should be instantiated with the `api_base` parameter set to the `OLLAMA_API_BASE_URL` if the model starts with \"ollama\". This is handled in the `_create_llm` method of the `QuivrRAG` class.\n\nAdditionally, you may need to update the embedding dimension in `.env.example` to match the model's requirements and modify certain files to use `ChatOllama` instead of `ChatOpenAI`. Rebuild the image and supabase after making these changes <sup>[[1]](https://github.com/QuivrHQ/quivr/issues/3425)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/issues/2777)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "wonderu",
        "body": "your @dosubot is useless and harmful! It proposes to delete table. "
      },
      {
        "user": "gamingflexer",
        "body": "@admins where the main repo? using which we can install this https://docs.quivr.app/install"
      },
      {
        "user": "gamingflexer",
        "body": "@wonderu @lijinxinKing check this\r\n\r\n[stephane-lbf](https://github.com/stephane-lbf) commented [last month](https://github.com/QuivrHQ/quivr/issues/3397#issuecomment-2436209258)\r\n@caishanli if this can help, I was a bit lost too because Quivr's Team is working on a refacto.\r\n\r\nSo for now on, you can do this :\r\n\r\ngit clone https://github.com/QuivrHQ/quivr.git\r\ngit rev-list -n1 --before=2024-10-20 main | xargs git checkout\r\nThen just follow the README ğŸ‘ğŸ¼"
      }
    ]
  },
  {
    "issue_number": 3507,
    "title": "Write report",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-11-28T11:29:57Z",
    "updated_at": "2025-01-20T10:04:31Z",
    "labels": [],
    "body": "@chloe @amine ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-307/write-report\">CORE-307 Write report</a></p>"
      }
    ]
  },
  {
    "issue_number": 3505,
    "title": "Construct \"hard\" use cases",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-11-28T08:47:39Z",
    "updated_at": "2025-01-20T10:04:14Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-306/construct-hard-use-cases\">CORE-306 Construct \"hard\" use cases</a></p>"
      }
    ]
  },
  {
    "issue_number": 3418,
    "title": "Add to CI/CD automatic evaluation",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-22T13:47:52Z",
    "updated_at": "2025-01-20T10:04:11Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-253/add-to-cicd-automatic-evaluation\">CORE-253 Add to CI/CD automatic evaluation</a></p>"
      }
    ]
  },
  {
    "issue_number": 3417,
    "title": "Create an extensive proprietary dataset",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-22T13:47:44Z",
    "updated_at": "2025-01-20T10:04:10Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-252/create-an-extensive-proprietary-dataset\">CORE-252 Create an extensive proprietary dataset</a></p>"
      }
    ]
  },
  {
    "issue_number": 3416,
    "title": "Find 10+ relevant failing use case",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-22T13:47:37Z",
    "updated_at": "2025-01-20T10:04:04Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-251/find-10-relevant-failing-use-case\">CORE-251 Find 10+ relevant failing use case</a></p>"
      }
    ]
  },
  {
    "issue_number": 3415,
    "title": "Create the API that takes relevant input",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-22T13:47:27Z",
    "updated_at": "2025-01-20T10:03:59Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-250/create-the-api-that-takes-relevant-input\">CORE-250 Create the API that takes relevant input</a></p>"
      }
    ]
  },
  {
    "issue_number": 3414,
    "title": "Test on a small proprietary dataset with mistral small and gpt-4o",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-22T13:47:16Z",
    "updated_at": "2025-01-20T10:03:53Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-249/test-on-a-small-proprietary-dataset-with-mistral-small-and-gpt-4o\">CORE-249 Test on a small proprietary dataset with mistral small and gpt-4o</a></p>"
      }
    ]
  },
  {
    "issue_number": 3413,
    "title": "Calculate relevant metrics stated in Strategy",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-22T13:47:06Z",
    "updated_at": "2025-01-20T10:03:49Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-248/calculate-relevant-metrics-stated-in-strategy\">CORE-248 Calculate relevant metrics stated in Strategy</a></p>"
      }
    ]
  },
  {
    "issue_number": 3412,
    "title": "Create a Notebook that takes the input and extract claims",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-22T13:46:51Z",
    "updated_at": "2025-01-20T10:03:47Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-247/create-a-notebook-that-takes-the-input-and-extract-claims\">CORE-247 Create a Notebook that takes the input and extract claims</a></p>"
      }
    ]
  },
  {
    "issue_number": 3335,
    "title": "[Bug]: docker compose failed with \" JavaScript heap out of memory\"",
    "author": "settur1409",
    "state": "closed",
    "created_at": "2024-10-07T11:54:14Z",
    "updated_at": "2025-01-20T09:55:47Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\ncloned this repo. \r\nupdated .env with appropriate supabase and grow API keys. \r\ngetting error when performing, docker compose -f docker-compose.yml up --build\n\n### Relevant log output\n\n```bash\n=> ERROR [frontend builder 4/4] RUN yarn build                                                                                        148.4s\r\n------\r\n > [frontend builder 4/4] RUN yarn build:\r\n2.385 yarn run v1.22.19\r\n2.671 $ next build\r\n15.87 Attention: Next.js now collects completely anonymous telemetry regarding usage.\r\n15.89 This information is used to shape Next.js' roadmap and prioritize features.\r\n15.89 You can learn more, including how to opt-out if you'd not like to participate in this anonymous program, by visiting the following URL:\r\n15.89 https://nextjs.org/telemetry\r\n15.89 \r\n16.84    â–² Next.js 14.1.0\r\n16.84    - Experiments (use at your own risk):\r\n16.84      Â· instrumentationHook\r\n16.84 \r\n17.68    Creating an optimized production build ...\r\n46.90 Browserslist: caniuse-lite is outdated. Please run:\r\n46.90   npx update-browserslist-db@latest\r\n46.90   Why you should do it regularly: https://github.com/browserslist/update-db#readme\r\n120.1 \r\n120.1 <--- Last few GCs --->\r\n120.1 \r\n120.1 [26:0x7f6c03a77060]   115373 ms: Scavenge (reduce) 1843.3 (2005.3) -> 1843.3 (2006.3) MB, 23.8 / 0.0 ms  (average mu = 0.803, current mu = 0.253) allocation failure; \r\n120.1 [26:0x7f6c03a77060]   117100 ms: Mark-sweep (reduce) 1949.9 (2112.9) -> 1862.6 (2090.7) MB, 247.0 / 0.1 ms  (+ 122.1 ms in 661 steps since start of marking, biggest step 7.1 ms, walltime since start of marking 3051 ms) (average mu = 0.854, current mu = 0.\r\n120.1 \r\n120.1 <--- JS stacktrace --->\r\n120.1 \r\n120.1 FATAL ERROR: Reached heap limit Allocation failed - JavaScript heap out of memory\r\n148.1 error Command failed with signal \"SIGABRT\".\r\n148.1 info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\r\n------\r\nfailed to solve: process \"/bin/sh -c yarn build\" did not complete successfully: exit code: 1\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-226/[bug]-docker-compose-failed-with-javascript-heap-out-of-memory\">CORE-226 [Bug]: docker compose failed with \" JavaScript heap out of memory\"</a></p>"
      },
      {
        "user": "settur1409",
        "body": "> [CORE-226 [Bug]: docker compose failed with \" JavaScript heap out of memory\"](https://linear.app/getquivr/issue/CORE-226/[bug]-docker-compose-failed-with-javascript-heap-out-of-memory)\r\n\r\nI don't have permission to view this page.\r\n![image](https://github.com/user-attachments/assets/0c90aea1-cc64-4155-8cd5-8b78bc9c72b4)\r\n"
      },
      {
        "user": "maryinthelab",
        "body": "Same error here, and also canâ€™t access the doc thatâ€™s being attached."
      },
      {
        "user": "ChangjunDAI",
        "body": "I meet the same error and canâ€™t access the doc thatâ€™s being attached as well."
      },
      {
        "user": "atulgrover",
        "body": "Same issue"
      },
      {
        "user": "OtaconBYF",
        "body": "Add `ENV NODE_OPTIONS=--max_old_space_size=4096` to frontend/Dockerfile\r\n\r\n![Screenshot 2024-10-30 at 2 26 31â€¯PM](https://github.com/user-attachments/assets/03521ef5-d3f0-49af-b54d-390322824087)\r\n\r\np.s.: pretty confused on how to push this as a PR, the main branch is quite different from the release tag."
      },
      {
        "user": "teury",
        "body": "The same thing happens to me, my server has 4 cores and 12 GB of ram, but anyway it tells me that there is no space with 2 GB of ram"
      }
    ]
  },
  {
    "issue_number": 3513,
    "title": "[Bug]:  '.env.example': No such file or directory",
    "author": "teury",
    "state": "closed",
    "created_at": "2024-12-09T19:24:14Z",
    "updated_at": "2025-01-20T09:54:33Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\nTaking me from the official documentation: https://docs.quivr.app/install\r\n\r\n1. git clone https://github.com/QuivrHQ/quivr.git && cd quivr\r\n2. cp .env.example .env\r\n\r\nshows the error\r\n'.env.example': No such file or directory\r\n\r\nWhere can I find this file, what happened? The documentation is outdated or they have removed the .env file from the latest version\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-310/[bug]-envexample-no-such-file-or-directory\">CORE-310 [Bug]: '.env.example': No such file or directory</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey @teury! I'm here to help you with any bugs or questions you have. I can also provide guidance on becoming a contributor. Let's tackle this together!\n\n<!-- Answer -->\nThe `.env.example` file for the Quivr repository is located in the `examples/quivr-whisper/` directory and is named `.env_example` <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/examples/quivr-whisper/.env_example)</sup>. It seems the documentation might be outdated or not specific about the file's location. You can navigate to this directory and use the `.env_example` file as a reference to create your own `.env` file.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n\n---\n<sup>Help Dosu learn! Give it feedback: [Great Response](https://app.dosu.dev/response-feedback/73134bf1-56a2-486f-9f54-301fdbc24eaf?feedback_type=great_response) | [Irrelevant Answer](https://app.dosu.dev/response-feedback/73134bf1-56a2-486f-9f54-301fdbc24eaf?feedback_type=irrelevant_answer) | [Incorrect Sources](https://app.dosu.dev/response-feedback/73134bf1-56a2-486f-9f54-301fdbc24eaf?feedback_type=incorrect_sources) | [Too Verbose](https://app.dosu.dev/response-feedback/73134bf1-56a2-486f-9f54-301fdbc24eaf?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/73134bf1-56a2-486f-9f54-301fdbc24eaf?feedback_type=hallucination) | [Other](https://app.dosu.dev/response-feedback/73134bf1-56a2-486f-9f54-301fdbc24eaf?feedback_type=other)</sup>\n"
      }
    ]
  },
  {
    "issue_number": 3508,
    "title": "[Bug]: Quickstart guide not working for local setup",
    "author": "sujal03",
    "state": "closed",
    "created_at": "2024-11-28T13:29:18Z",
    "updated_at": "2025-01-20T09:53:55Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\nHi,\r\nI tried running Quivr locally by following the Quickstart guide, but it didnâ€™t work as expected. Below are the details of the issue:\r\n\r\nSteps Followed:\r\n[List the specific steps you took from the guide]\r\nError/Issue:\r\n[Mention the error message or issue encountered. If possible, include screenshots or logs.]\r\nEnvironment Details:\r\nOS: [Your OS and version]\r\nNode.js Version: [Your version, if applicable]\r\nAny other relevant configurations.\r\nLet me know if there are additional steps or configurations I need to follow. Thanks!\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-308/[bug]-quickstart-guide-not-working-for-local-setup\">CORE-308 [Bug]: Quickstart guide not working for local setup</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey @sujal03,\n\nI'm here to help you with any bugs, questions, or contributions while you wait for a human maintainer. Let's tackle this together!\n\n<!-- Answer -->\nI found a similar open issue where the suggested solution was to update the repository and rerun the setup. You can try running `git pull` and then `docker compose up` again. If you're using your own VPS, you might want to follow the deployment procedure outlined here: [Digital Ocean Deployment Guide](https://docs.quivr.app/deployment/digital_ocean) <sup>[[1]](https://github.com/QuivrHQ/quivr/issues/2408)</sup>.\n\nAdditionally, there are a couple of closed issues that might be relevant. One suggests that the problem could be related to the startup order of services in Docker Compose. You might need to ensure that services are fully ready before others try to connect to them. Another issue was resolved by converting a script file to Unix format using `dos2unix backend/core/scripts/start.sh` and then restarting Docker Compose <sup>[[2]](https://github.com/QuivrHQ/quivr/issues/1738)</sup><sup>[[3]](https://github.com/QuivrHQ/quivr/issues/914)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      }
    ]
  },
  {
    "issue_number": 3463,
    "title": "[Bug]: Login \"Failed to Fetch\" (request returned Internal Server Error for API route and version http://%2F%2F.%2Fpipe%2FdockerDesktopLinuxEngine/v1.46/containers/xxx/json, check if the server supports the requested API version)",
    "author": "windowshopr",
    "state": "closed",
    "created_at": "2024-11-08T02:53:44Z",
    "updated_at": "2025-01-20T09:53:43Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\nAttempting to login to local version run in docker compose desktop on Windows 10 with default credentials results in \"Failed to fetch\" popup:\r\n\r\n![image](https://github.com/user-attachments/assets/d0b3b4ec-3d58-4fc5-8bcd-6cadf231dd59)\r\n\r\nI see logging into this repo is sort of an ongoing issue, hopefully it gets fixed in future releases?\r\n\r\nNot sure how Supabase fits into this mix for the local version and would like to avoid it if possible. How to fix?\n\n### Relevant log output\n\n```bash\nrequest returned Internal Server Error for API route and version http://%2F%2F.%2Fpipe%2FdockerDesktopLinuxEngine/v1.46/containers/9d8644ecb2dd5d6aa0ffb6fca20a79e8704dbcaa22675195fc2adb64350f2a9e/json, check if the server supports the requested API version\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-290/[bug]-login-failed-to-fetch-request-returned-internal-server-error-for\">CORE-290 [Bug]: Login \"Failed to Fetch\" (request returned Internal Server Error for API route and version http://%2F%2F.%2Fpipe%2FdockerDesktopLinuxEngine/v1.46/containers/xxx/json, check if the server supports the requested API version)</a></p>"
      }
    ]
  },
  {
    "issue_number": 3432,
    "title": "Why was the frontend deleted, and project structure changed?",
    "author": "huynhbaobk",
    "state": "closed",
    "created_at": "2024-10-29T04:15:14Z",
    "updated_at": "2025-01-20T09:53:22Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\nI recently noticed that you changed the project by removing the frontend and focusing on a backend core. Could you explain the reason for this shift and outline the roadmap? Are there currently no plans to develop a frontend? Many thanks\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-263/why-was-the-frontend-deleted-and-project-structure-changed\">CORE-263 Why was the frontend deleted, and project structure changed?</a></p>"
      },
      {
        "user": "yanndebelgique",
        "body": "same question ...."
      },
      {
        "user": "StanGirard",
        "body": "We explain all here guys [https://www.quivr.com/blog/quivrs-latest-pivot-vision](https://www.quivr.com/blog/quivrs-latest-pivot-vision) ğŸ˜‰ "
      }
    ]
  },
  {
    "issue_number": 3425,
    "title": "[Bug]: Can't use local model with ollama",
    "author": "LronDC",
    "state": "closed",
    "created_at": "2024-10-25T08:14:49Z",
    "updated_at": "2025-01-20T09:52:34Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\ncan't find .env.example file which in quickstart.md and also can't find a guide to use local model\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-257/[bug]-cant-use-local-model-with-ollama\">CORE-257 [Bug]: Can't use local model with ollama</a></p>"
      },
      {
        "user": "iemafzalhassan",
        "body": "Hi ğŸ‘‹ \r\n\r\nI would like to work on this issue...\r\n"
      },
      {
        "user": "StanGirard",
        "body": "You can check docs.quivr.com :)"
      }
    ]
  },
  {
    "issue_number": 3397,
    "title": "[Bug]: how to run docker with ghcr.io/quivrhq/quivr:latest",
    "author": "caishanli",
    "state": "closed",
    "created_at": "2024-10-20T15:31:11Z",
    "updated_at": "2025-01-20T09:51:54Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\ni pulled this docker image, and start it with `docker run -d -p 3000:3000 --name myquivr ghcr.io/quivrhq/quivr:latest`, but nothing happened\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-246/[bug]-how-to-run-docker-with-ghcrioquivrhqquivrlatest\">CORE-246 [Bug]: how to run docker with ghcr.io/quivrhq/quivr:latest</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo run the Quivr Docker image successfully, ensure you have the necessary environment variables and configurations set up. Here are some steps you can follow:\n\n1. **Environment Variables**: Make sure you have all the required environment variables configured. You can refer to the `.env.example` file for the necessary variables, such as `OPENAI_API_KEY`, `SUPABASE_SERVICE_KEY`, and others <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/.env.example)</sup>.\n\n2. **Dependencies**: Ensure that external services like Redis, Supabase, and PostgreSQL are running, as they are required by the Quivr Docker image <sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.yml)</sup>.\n\n3. **Ports**: The command you used only maps port 3000, which is typically for the frontend. Ensure that other necessary ports are also mapped, such as 5050 for the backend API <sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.yml)</sup>.\n\n4. **Startup Command**: The Docker image expects a specific command to be executed on startup. For the `backend-api` service, it should be:\n   ```bash\n   /bin/bash -c \"python -m uvicorn quivr_api.main:app --host 0.0.0.0 --log-level info --reload --port 5050 --loop uvloop\"\n   ```\n   Ensure that this command is being executed if you are trying to run the backend service <sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.yml)</sup>.\n\n5. **Logs**: Check the Docker container logs for any error messages that might indicate what is going wrong. You can view logs using:\n   ```bash\n   docker logs myquivr\n   ```\n\nBy ensuring these configurations and dependencies are correctly set up, you should be able to run the Quivr Docker image successfully.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu so what's the full steps to run this docker image?"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo run the Quivr Docker image successfully, follow these steps:\n\n1. **Pull the Docker Image**:\n   Make sure you have the latest Quivr Docker image by pulling it from the GitHub Container Registry:\n   ```bash\n   docker pull ghcr.io/quivrhq/quivr:latest\n   ```\n\n2. **Set Up Environment Variables**:\n   Create a `.env` file based on the `.env.example` file provided in the repository. Ensure you configure all necessary environment variables, such as:\n   - `OPENAI_API_KEY`\n   - `SUPABASE_SERVICE_KEY`\n   - Database credentials and other required keys <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/cms/quivr/.env.example)</sup>.\n\n3. **Ensure Dependencies are Running**:\n   Start the required services like Redis, Supabase, and PostgreSQL. These services are essential for Quivr to function properly <sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/.env.example)</sup>.\n\n4. **Run the Docker Container**:\n   Use the following command to run the Docker container, ensuring you map the necessary ports:\n   ```bash\n   docker run -d --name myquivr \\\n     -p 3000:3000 \\  # Frontend\n     -p 5050:5050 \\  # Backend\n     --env-file .env \\\n     ghcr.io/quivrhq/quivr:latest\n   ```\n\n5. **Check Logs for Errors**:\n   If the container does not start as expected, check the logs for any error messages:\n   ```bash\n   docker logs myquivr\n   ```\n\nBy following these steps, you should be able to run the Quivr Docker image successfully. Make sure all configurations and dependencies are correctly set up.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu I followed your steps, but when I run `docker logs myquivr`, nothings shows, and no container named myquivr is running."
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nIf the container named `myquivr` is not running and `docker logs myquivr` shows nothing, it might not have started correctly. Here are some steps to troubleshoot:\n\n1. **Check for Exited Containers**:\n   Run the following command to see if the container started and then exited:\n   ```bash\n   docker ps -a\n   ```\n   Look for `myquivr` in the list. If it shows an \"Exited\" status, there might be an issue with the startup process.\n\n2. **Inspect the Container**:\n   If the container is listed as exited, inspect it to find out why:\n   ```bash\n   docker inspect myquivr\n   ```\n   Look for any error messages or issues in the output.\n\n3. **Review Docker Events**:\n   Check Docker events to see if there are any clues about why the container failed to start:\n   ```bash\n   docker events --since 1h\n   ```\n\n4. **Check Docker Daemon Logs**:\n   Review the Docker daemon logs for any errors or warnings:\n   ```bash\n   sudo journalctl -u docker.service\n   ```\n\n5. **Verify Environment Variables**:\n   Double-check your `.env` file to ensure all required environment variables are correctly set. Missing or incorrect variables can prevent the container from starting.\n\n6. **Run with Interactive Mode**:\n   Try running the container in interactive mode to see if any errors are displayed:\n   ```bash\n   docker run -it --name myquivr --env-file .env ghcr.io/quivrhq/quivr:latest /bin/bash\n   ```\n   This will allow you to manually start the application and see any immediate errors.\n\nBy following these steps, you should be able to identify the issue preventing the container from starting.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu run the command 6, and the docker container just exited after 1s."
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nIf the Docker container exits immediately after running it interactively, it might be due to a missing or misconfigured entrypoint or command. Here are some steps to further diagnose and resolve the issue:\n\n1. **Check the Entrypoint and Command**:\n   The default command in the Dockerfile for the Quivr image is `CMD [\"yarn\", \"start\"]` <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/cms/quivr/Dockerfile)</sup>. Ensure that this command is appropriate for your setup. If you're trying to run a specific service, you might need to adjust the command accordingly.\n\n2. **Run with a Shell**:\n   Try running the container with a shell to manually start the application and see any errors:\n   ```bash\n   docker run -it --name myquivr --env-file .env ghcr.io/quivrhq/quivr:latest /bin/sh\n   ```\n   Once inside the container, try running the startup command manually to see if any errors are displayed.\n\n3. **Verify Environment Variables**:\n   Double-check your `.env` file to ensure all required environment variables are correctly set. Missing or incorrect variables can cause the application to exit immediately.\n\n4. **Check for Logs or Errors**:\n   After running the container interactively, check for any log files or error messages within the container that might indicate what went wrong.\n\n5. **Dependencies**:\n   Ensure that all necessary services (like Redis and any databases) are running and accessible from the container, as missing dependencies can cause the application to fail on startup <sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.yml)</sup>.\n\nBy following these steps, you should be able to identify the issue causing the container to exit immediately.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "stephane-lbf",
        "body": "@caishanli if this can help, I was a bit lost too because Quivr's Team is working on a refacto.\r\n\r\nSo for now on, you can do this : \r\n\r\n```\r\ngit clone https://github.com/QuivrHQ/quivr.git\r\ngit rev-list -n1 --before=2024-10-20 main | xargs git checkout\r\n```\r\n\r\nThen just follow the README ğŸ‘ğŸ¼"
      }
    ]
  },
  {
    "issue_number": 3377,
    "title": "[Bug]: supabase_realtime_secondbrain container is not ready: unhealthy",
    "author": "zhukelili",
    "state": "closed",
    "created_at": "2024-10-16T05:15:28Z",
    "updated_at": "2025-01-20T09:51:43Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### What happened?\n\nWhen I execute the command cd backend && supabase start, the following error appears. How can I fix itï¼Ÿ\n\n### Relevant log output\n\n```bash\n----- Native stack trace -----\r\n\r\n 1: 0xcc7e17 node::Assert(node::AssertionInfo const&) [node]\r\n 2: 0xd4818e node::WorkerThreadsTaskRunner::WorkerThreadsTaskRunner(int) [node]\r\n 3: 0xd4826c node::NodePlatform::NodePlatform(int, v8::TracingController*, v8::PageAllocator*) [node]\r\n 4: 0xc7bd07  [node]\r\n 5: 0xc7d264 node::Start(int, char**) [node]\r\n 6: 0x7f076b26e24a  [/lib/x86_64-linux-gnu/libc.so.6]\r\n 7: 0x7f076b26e305 __libc_start_main [/lib/x86_64-linux-gnu/libc.so.6]\r\n 8: 0xbd12ee _start [node]\r\nStopping containers...\r\nsupabase_realtime_secondbrain container is not ready: unhealthy\r\nError status 503:\r\nsupabase_pg_meta_secondbrain container is not ready: unhealthy\r\nsupabase_studio_secondbrain container is not ready: unhealthy\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-238/[bug]-supabase-realtime-secondbrain-container-is-not-ready-unhealthy\">CORE-238 [Bug]: supabase_realtime_secondbrain container is not ready: unhealthy</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHello, @zhukelili! I'm a bot designed to help you solve bugs, answer questions, and become a contributor. I'm here to assist you while you wait for a human maintainer.\n\n<!-- Answer -->\nI couldn't find specific information about the `supabase_realtime_secondbrain` container, but there is a similar issue with the `supabase_vector_secondbrain` container being unhealthy. The suggested solution for that issue includes ensuring that all necessary environment variables are correctly set, such as `openai_api_key`, `azure_openai_embeddings_url`, `supabase_url`, `supabase_service_key`, `pg_database_url`, `pg_database_async_url`, and `embedding_dim`. Additionally, check the `backend/ci-migration.sh` script for `PROJECT_ID`, `SUPABASE_ACCESS_TOKEN`, and `SUPABASE_DB_PASSWORD` <sup>[[1]](https://github.com/QuivrHQ/quivr/issues/3206)</sup>.\n\nIf the issue persists, consider checking the Docker logs for more specific error messages related to the health checks. The logs might indicate a network issue, which could be related to Docker's network configuration or a firewall setting. You can try updating Docker Desktop, restarting it, checking firewall settings, and ensuring no other containers are conflicting with the `supabase_realtime_secondbrain` container <sup>[[1]](https://github.com/QuivrHQ/quivr/issues/3206)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "zhukelili",
        "body": "> CORE-238 [Bug]: supabase_realtime_secondbrain container is not ready: unhealthy\r\n\r\nthanks,but \r\n![image](https://github.com/user-attachments/assets/3e27404c-b44a-4dbf-8567-8db630075940)\r\n"
      },
      {
        "user": "StanGirard",
        "body": "Can you try to do a `supabase stop` and then a `supabase start`? "
      },
      {
        "user": "zhukelili",
        "body": "> Can you try to do a `supabase stop` and then a `supabase start`?\r\n\r\nI found the reason why the program failed to start on other websites: insufficient local disk space. When I used a computer with enough space, it could start normally."
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 863,
    "title": "Dependency Dashboard",
    "author": "renovate[bot]",
    "state": "closed",
    "created_at": "2023-08-06T22:45:50Z",
    "updated_at": "2025-01-20T08:07:53Z",
    "labels": [
      "Stale"
    ],
    "body": "This issue lists Renovate updates and detected dependencies. Read the [Dependency Dashboard](https://docs.renovatebot.com/key-concepts/dashboard/) docs to learn more.\n\n## Open\n\nThese updates have all been created already. Click a checkbox below to force a retry/rebase of any.\n\n - [ ] <!-- rebase-branch=renovate/pin-dependencies -->[chore(deps): pin python docker tag to 0c1fbb2](../pull/2837)\n - [ ] <!-- rebase-branch=renovate/langchain-0.x -->[chore(deps): update dependency langchain to >=0.3.3,<0.4.0](../pull/3220)\n - [ ] <!-- rebase-branch=renovate/python-3.x -->[chore(deps): update python docker tag to v3.12.6](../pull/3122)\n - [ ] <!-- rebase-all-open-prs -->**Click on this checkbox to rebase all open PRs at once**\n\n## Detected dependencies\n\n<details><summary>dockerfile</summary>\n<blockquote>\n\n<details><summary>backend/api/Dockerfile</summary>\n\n - `python 3.11.6-slim-bullseye`\n\n</details>\n\n<details><summary>backend/api/Dockerfile.dev</summary>\n\n - `python 3.11.6-slim-bullseye`\n\n</details>\n\n<details><summary>backend/core/Dockerfile.test</summary>\n\n - `python 3.11.6-slim-bullseye`\n\n</details>\n\n</blockquote>\n</details>\n\n<details><summary>pep621</summary>\n<blockquote>\n\n<details><summary>backend/api/pyproject.toml</summary>\n\n - `supabase >=2.0.0`\n - `fastapi >=0.100.0`\n - `uvloop >=0.18.0`\n - `python-jose >=3.0.0`\n - `python-multipart >=0.0.9`\n - `uvicorn >=0.25.0`\n - `redis >=5.0.0`\n - `asyncpg >=0.29.0`\n - `psycopg2-binary >=2.9.9`\n - `sqlmodel >=0.0.21`\n - `celery >=5.4.0`\n - `pydantic-settings >=2.4.0`\n - `python-dotenv >=1.0.1`\n - `unidecode >=1.3.8`\n - `colorlog >=6.8.2`\n - `posthog >=3.5.0`\n - `pyinstrument >=4.7.2`\n - `sentry-sdk >=2.13.0`\n - `google-api-python-client >=2.141.0`\n - `google-auth-httplib2 >=0.2.0`\n - `google-auth-oauthlib >=1.2.1`\n - `dropbox >=12.0.2`\n - `msal >=1.30.0`\n - `notion-client >=2.2.1`\n - `markdownify >=0.13.1`\n - `langchain-openai >=0.1.21`\n - `resend >=2.4.0`\n - `langchain >=0.2.14,<0.3.0`\n - `litellm >=1.43.15`\n - `openai >=1.40.8`\n - `tiktoken >=0.7.0`\n - `langchain-community >=0.2.12`\n - `langchain-cohere >=0.2.2`\n - `llama-parse >=0.4.9`\n - `pgvector >=0.3.2`\n\n</details>\n\n<details><summary>backend/core/pyproject.toml</summary>\n\n - `pydantic >=2.8.2`\n - `langchain-core >=0.2.38`\n - `langchain >=0.2.14,<0.3.0`\n - `langgraph >=0.2.14`\n - `httpx >=0.27.0`\n - `rich >=13.7.1`\n - `tiktoken >=0.7.0`\n - `aiofiles >=23.1.0`\n - `langchain-community >=0.2.12`\n - `langchain-anthropic >=0.1.23`\n - `types-pyyaml >=6.0.12.20240808`\n - `transformers >=4.44.2`\n - `unstructured >=0.15.5`\n - `faiss-cpu >=1.8.0.post1`\n - `docx2txt >=0.8`\n - `python-docx >=1.1.0`\n - `mammoth >=1.8.0`\n - `python-pptx >=1.0.2`\n - `llama-parse >=0.4.0`\n - `pdf2docx >=0.5.0`\n - `unstructured >=0.15.0`\n - `langchain >=0.2.0`\n - `langchain-community >=0.2.0`\n - `langchain-openai >=0.1.0`\n - `langchain-core >=0.2.0`\n - `python-dotenv >=1.0.0`\n - `pycryptodome >=3.20.0`\n - `llama-index >=0.10.0`\n - `pdfplumber >=0.11.0`\n\n</details>\n\n</blockquote>\n</details>\n\n<details><summary>pyenv</summary>\n<blockquote>\n\n<details><summary>backend/api/.python-version</summary>\n\n - `python 3.11.9`\n\n</details>\n\n<details><summary>backend/core/.python-version</summary>\n\n - `python 3.11.9`\n\n</details>\n\n</blockquote>\n</details>\n\n---\n\n- [ ] <!-- manual job -->Check this box to trigger a request for Renovate to run again on this repository\n\n",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3274,
    "title": "[BUG] Link and Update concurrency",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-30T08:16:21Z",
    "updated_at": "2025-01-19T20:05:21Z",
    "labels": [
      "bug",
      "Stale",
      "area: backend"
    ],
    "body": "There is concurrency issue with linking km to brain steps and removing the knowledge.\n\n* Link knowledge to brain :\n  * Links the knowledge and children to the brain. ( COMMIT)\n  * For knowledge in updated knowledge with status (not processing): \n    * Send celery task to process\n    * Update Knowledge status to PROCESSING\n    * COMMIT\n* IF Remove knowledge occurs between these two steps, the knowledge is deleted\n\nCheck postgres transactional guarantees ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-106/[bug]-link-and-update-concurrency\">ENT-106 [BUG] Link and Update concurrency</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3241,
    "title": "Asynchronously remove knowledge from storage",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-21T18:08:09Z",
    "updated_at": "2025-01-19T20:05:21Z",
    "labels": [
      "enhancement",
      "Stale",
      "area: backend"
    ],
    "body": "[https://quivr-brain.sentry.io/performance/trace/c48989fba52d43fe918cb8edd1cd9b8d/?environment=production&fov=0%2C538.625&node=span-a4d6f76b8e608621&node=txn-7cc8826f05934c3f87c628f3addac2e3&project=4505455607808000&query=http.method%3ADELETE&referrer=performance-transaction-summary&source=performance_transaction_summary&statsPeriod=7d&timestamp=1726539141&transaction=%2Fknowledge%2F%7Bknowledge_id%7D&unselectedSeries=p100%28%29&unselectedSeries=avg%28%29](https://quivr-brain.sentry.io/performance/trace/c48989fba52d43fe918cb8edd1cd9b8d/?environment=production&fov=0%2C538.625&node=span-a4d6f76b8e608621&node=txn-7cc8826f05934c3f87c628f3addac2e3&project=4505455607808000&query=http.method%3ADELETE&referrer=performance-transaction-summary&source=performance_transaction_summary&statsPeriod=7d&timestamp=1726539141&transaction=%2Fknowledge%2F%7Bknowledge_id%7D&unselectedSeries=p100%28%29&unselectedSeries=avg%28%29)<br><br>Shouldnt block on supbase slow delete",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-94/asynchronously-remove-knowledge-from-storage\">ENT-94 Asynchronously remove knowledge from storage</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3188,
    "title": "[Backend] Deduplicate file in same folder",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-11T08:55:58Z",
    "updated_at": "2025-01-17T12:09:03Z",
    "labels": [
      "Stale",
      "area: backend"
    ],
    "body": "* Can we have files with the same name in the same folder? \n  * Do we keep both versions? NO\n* Do we override the file? YES\n\n\\-> KnowledgeDTO should also return a list of duplicate Knowledge based on SHA-1",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-73/[backend]-deduplicate-file-in-same-folder\">ENT-73 [Backend] Deduplicate file in same folder</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3353,
    "title": "Deactivate document download in knowledge and sources",
    "author": "StanGirard",
    "state": "closed",
    "created_at": "2024-10-10T08:37:11Z",
    "updated_at": "2025-01-13T12:10:15Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "@ said:\n\n> salut salut, une petite feature sÃ©cu qui serait pas mal : pouvoir supprimer le download des documents",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-112/deactivate-document-download-in-knowledge-and-sources\">ENT-112 Deactivate document download in knowledge and sources</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3362,
    "title": "Add documentation for activating integrations",
    "author": "StanGirard",
    "state": "closed",
    "created_at": "2024-10-10T11:01:24Z",
    "updated_at": "2025-01-13T12:10:14Z",
    "labels": [
      "documentation",
      "Stale",
      "area: docs"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-113/add-documentation-for-activating-integrations\">ENT-113 Add documentation for activating integrations</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3316,
    "title": "[Feature]: Updatation of Documentation especially on \"Compsite brain connection\"",
    "author": "ChatIBC",
    "state": "closed",
    "created_at": "2024-10-03T17:33:00Z",
    "updated_at": "2025-01-13T04:07:35Z",
    "labels": [
      "enhancement",
      "Stale",
      "area: docs"
    ],
    "body": "### The Feature\n\nThe documentation needs an update... I am searchin on how to crate composite bbrain connections\n\n### Motivation, pitch\n\nThe documentation needs an update... I am searchin on how to crate composite bbrain connections\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-223/[feature]-updatation-of-documentation-especially-on-compsite-brain\">CORE-223 [Feature]: Updatation of Documentation especially on \"Compsite brain connection\"</a></p>"
      },
      {
        "user": "ChatIBC",
        "body": "Can we not use AI to automatically generate documentation based on the code and also answer any questions user may have. Is this what DOSUbot does? Doesn't answer immediately though."
      },
      {
        "user": "ChatIBC",
        "body": "How to join two brains (composite) for a common answer"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3336,
    "title": "[Bug]: How do I run the Ollama",
    "author": "caishanli",
    "state": "closed",
    "created_at": "2024-10-07T12:19:10Z",
    "updated_at": "2025-01-11T20:05:38Z",
    "labels": [
      "bug",
      "Stale",
      "area: backend"
    ],
    "body": "### What happened?\n\n/app/core/quivr_core/config.py:209 in set_llm_model                                              â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   206 â”‚   def set_llm_model(self, model: str):                                                   â”‚\r\nbackend-api  | â”‚   207 â”‚   â”‚   supplier = LLMModelConfig.get_supplier_by_model_name(model)                        â”‚\r\nbackend-api  | â”‚   208 â”‚   â”‚   if supplier is None:                                                               â”‚\r\nbackend-api  | â”‚ â± 209 â”‚   â”‚   â”‚   raise ValueError(                                                              â”‚\r\nbackend-api  | â”‚   210 â”‚   â”‚   â”‚   â”‚   f\"Cannot find the corresponding supplier for model {model}\"                â”‚\r\nbackend-api  | â”‚   211 â”‚   â”‚   â”‚   )                                                                              â”‚\r\nbackend-api  | â”‚   212 â”‚   â”‚   self.supplier = supplier\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-227/[bug]-how-do-i-run-the-ollama\">CORE-227 [Bug]: How do I run the Ollama</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nThe error you're encountering indicates that the `set_llm_model` function is unable to find a corresponding supplier for the model you are trying to use. This happens when the model name provided does not match any of the supported models listed in the `_model_defaults` dictionary.\n\nTo resolve this issue, ensure that the model name you are using is one of the supported models. Here are the possible values for the `model` parameter:\n\n- **OPENAI**: \"gpt-4o\", \"gpt-4o-mini\", \"gpt-4-turbo\", \"gpt-4\", \"gpt-3.5-turbo\", \"text-embedding-3-large\", \"text-embedding-3-small\", \"text-embedding-ada-002\"\n- **ANTHROPIC**: \"claude-3-5-sonnet\", \"claude-3-opus\", \"claude-3-sonnet\", \"claude-3-haiku\", \"claude-2-1\", \"claude-2-0\", \"claude-instant-1-2\"\n- **META**: \"llama-3.1\", \"llama-3\", \"llama-2\", \"code-llama\"\n- **GROQ**: \"llama-3.1\", \"llama-3\", \"llama-2\", \"code-llama\"\n- **MISTRAL**: \"mistral-large\", \"mistral-small\", \"mistral-nemo\", \"codestral\" <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/core/quivr_core/config.py)</sup>.\n\nMake sure the model name you are using matches one of these exactly. If you are trying to use the Ollama model, please verify if it is included in the supported models list or if there is any specific configuration required for it.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu I use ollama, so how to set ollama models? Is ollama support?"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nYes, the codebase does support Ollama models. The `BigBrain` class uses the `ChatLiteLLM` model, which can be configured to use an Ollama model. The `get_chain` method checks if the `ollama_api_base_url` is set and if the model name starts with \"ollama\" to configure the API base URL for the Ollama model <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/core/MegaParse/megaparse/unstructured_convertor.py)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/brain/integrations/Big/Brain.py)</sup>.\n\nTo set up Ollama models, ensure that the model name you are using starts with \"ollama\" and that the `ollama_api_base_url` is correctly configured in your settings. This will allow the system to recognize and configure the Ollama model appropriately.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu my ollama model name is starts with \"ollama\" and the ollama_api_base_url is correctly configured in my .env file. So why the code have this error?"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nIf your Ollama model name starts with \"ollama\" and the `ollama_api_base_url` is correctly configured, but you're still encountering the error, it might be due to the model name not being recognized in the `_model_defaults` dictionary. The `set_llm_model` function relies on the `get_supplier_by_model_name` method to find a matching supplier for the model name. If the model name is not listed under the supported models for any supplier, it will raise a `ValueError` <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/core/quivr_core/config.py)</sup>.\n\nTo resolve this, ensure that the specific Ollama model name you are using is included in the `_model_defaults` dictionary under the appropriate supplier. If it's not listed, you may need to add it to the configuration or verify if there are any additional settings required for your specific model.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu so I need to add ollama models to _model_defaults myself and all things will work ok? And after add, how to update this change to docker?"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nYes, you will need to add your Ollama models to the `_model_defaults` dictionary under the appropriate supplier to ensure they are recognized by the system. Once you've made these changes, you can update your Docker container by following these steps:\n\n1. **Navigate to the directory** containing the `Makefile` and the `Dockerfile`.\n\n2. **Rebuild the Docker image** by running the build command specified in the `Makefile`. You can execute the following command in your terminal:\n\n   ```bash\n   make build\n   ```\n\n   This command will rebuild the Docker image using the updated codebase.\n\n3. **Restart the Docker container** to apply the changes. You can do this by stopping the current container and starting a new one with the updated image.\n\nBy following these steps, your changes to the `_model_defaults` dictionary will be reflected in the Docker container <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/core/MegaParse/Makefile)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu Do you have sample code of _model_defaults of ollama models? "
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo add Ollama models to the `_model_defaults` dictionary, you can follow the existing structure used for other model suppliers. Here's a sample code snippet that you can use as a template:\n\n```python\n_model_defaults: Dict[DefaultModelSuppliers, Dict[str, LLMConfig]] = {\n    # Existing suppliers...\n    DefaultModelSuppliers.OLLAMA: {\n        \"ollama-model-1\": LLMConfig(context=128000, tokenizer_hub=\"Xenova/ollama-model-1-tokenizer\"),\n        \"ollama-model-2\": LLMConfig(context=64000, tokenizer_hub=\"Xenova/ollama-model-2-tokenizer\"),\n        # Add more Ollama models as needed\n    },\n}\n```\n\nMake sure to replace `\"ollama-model-1\"` and `\"ollama-model-2\"` with the actual names of your Ollama models, and update the `context` and `tokenizer_hub` values according to your model's specifications.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu what's the tokenizer_hub param in LLMConfig means? And for ollama mistral, what should I putï¼Ÿ"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nThe `tokenizer_hub` parameter in `LLMConfig` specifies the tokenizer to be used for a particular language model. For the Ollama Mistral model, you should use `\"Xenova/mistral-tokenizer-v3\"` as the value for the `tokenizer_hub` parameter <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/core/quivr_core/config.py)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu I added ollama model mistral to _model_defaults, and also update to docker image backend-api, but when I talk to this model, the page show \"network error\", and the terminal's last error is â€œAPIConnectionError: Connection error.â€ So how to fix this?\r\n"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nThe \"APIConnectionError: Connection error\" suggests there might be an issue with the network configuration or connectivity to the Ollama API. Here are some steps you can take to troubleshoot and resolve this issue:\n\n1. **Verify API Base URL**: Double-check that the `ollama_api_base_url` in your environment configuration is correct and accessible from your Docker container. Ensure there are no typos or incorrect URLs.\n\n2. **Check Network Connectivity**: Ensure that your Docker container has network access to the Ollama API. You can test this by executing a simple network command like `ping` or `curl` from within the container to the API base URL.\n\n3. **Inspect Firewall and Security Groups**: If your application is running in a cloud environment, verify that any firewalls or security groups allow outbound traffic to the Ollama API.\n\n4. **Review Proxy Settings**: If your network requires a proxy to access external services, ensure that the proxy settings are correctly configured in your Docker environment.\n\n5. **Examine Logs for More Details**: Check the logs for any additional error messages or stack traces that might provide more context about the connection error. This can help identify if the issue is related to authentication, DNS resolution, or other network-related problems.\n\n6. **Retry Logic**: Ensure that your application has retry logic implemented for network requests, as transient network issues can sometimes be resolved by retrying the request.\n\nIf these steps do not resolve the issue, you may need to further investigate the network configuration and ensure that all necessary dependencies and configurations for the Ollama Mistral model are correctly set up.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu network from docker to ollama is ok, I tested with wget. The main error is that why I use ollama, but the code run into openai?\r\n`backend-api  | The above exception was the direct cause of the following exception:\r\nbackend-api  |\r\nbackend-api  | â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py:670 in              â”‚\r\nbackend-api  | â”‚ _astream_log_implementation                                                                      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   667 â”‚   finally:                                                                               â”‚\r\nbackend-api  | â”‚   668 â”‚   â”‚   # Wait for the runnable to finish, if not cancelled (eg. by break)                 â”‚\r\nbackend-api  | â”‚   669 â”‚   â”‚   try:                                                                               â”‚\r\nbackend-api  | â”‚ â± 670 â”‚   â”‚   â”‚   await task                                                                     â”‚\r\nbackend-api  | â”‚   671 â”‚   â”‚   except asyncio.CancelledError:                                                     â”‚\r\nbackend-api  | â”‚   672 â”‚   â”‚   â”‚   pass                                                                           â”‚\r\nbackend-api  | â”‚   673                                                                                            â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py:624 in              â”‚\r\nbackend-api  | â”‚ consume_astream                                                                                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   621 â”‚   â”‚   â”‚   prev_final_output: Optional[Output] = None                                     â”‚\r\nbackend-api  | â”‚   622 â”‚   â”‚   â”‚   final_output: Optional[Output] = None                                          â”‚\r\nbackend-api  | â”‚   623 â”‚   â”‚   â”‚                                                                                  â”‚\r\nbackend-api  | â”‚ â± 624 â”‚   â”‚   â”‚   async for chunk in runnable.astream(input, config, **kwargs):                  â”‚\r\nbackend-api  | â”‚   625 â”‚   â”‚   â”‚   â”‚   prev_final_output = final_output                                           â”‚\r\nbackend-api  | â”‚   626 â”‚   â”‚   â”‚   â”‚   if final_output is None:                                                   â”‚\r\nbackend-api  | â”‚   627 â”‚   â”‚   â”‚   â”‚   â”‚   final_output = chunk                                                   â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1411 in astream             â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   1408 â”‚   â”‚   â”‚   â”‚   â”‚   interrupt_after=interrupt_after,                                      â”‚\r\nbackend-api  | â”‚   1409 â”‚   â”‚   â”‚   â”‚   â”‚   manager=run_manager,                                                  â”‚\r\nbackend-api  | â”‚   1410 â”‚   â”‚   â”‚   â”‚   ):                                                                        â”‚\r\nbackend-api  | â”‚ â± 1411 â”‚   â”‚   â”‚   â”‚   â”‚   async for _ in runner.atick(                                          â”‚\r\nbackend-api  | â”‚   1412 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   loop.tasks.values(),                                              â”‚\r\nbackend-api  | â”‚   1413 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   timeout=self.step_timeout,                                        â”‚\r\nbackend-api  | â”‚   1414 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   retry_policy=self.retry_policy,                                   â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langgraph/pregel/runner.py:145 in atick                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   142 â”‚   â”‚   â”‚   # give control back to the caller                                              â”‚\r\nbackend-api  | â”‚   143 â”‚   â”‚   â”‚   yield                                                                          â”‚\r\nbackend-api  | â”‚   144 â”‚   â”‚   # panic on failure or timeout                                                      â”‚\r\nbackend-api  | â”‚ â± 145 â”‚   â”‚   _panic_or_proceed(all_futures, asyncio.TimeoutError)                               â”‚\r\nbackend-api  | â”‚   146                                                                                            â”‚\r\nbackend-api  | â”‚   147                                                                                            â”‚\r\nbackend-api  | â”‚   148 def _should_stop_others(                                                                   â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langgraph/pregel/runner.py:190 in _panic_or_proceed      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   187 â”‚   â”‚   â”‚   while inflight:                                                                â”‚\r\nbackend-api  | â”‚   188 â”‚   â”‚   â”‚   â”‚   inflight.pop().cancel()                                                    â”‚\r\nbackend-api  | â”‚   189 â”‚   â”‚   â”‚   # raise the exception                                                          â”‚\r\nbackend-api  | â”‚ â± 190 â”‚   â”‚   â”‚   raise exc                                                                      â”‚\r\nbackend-api  | â”‚   191 â”‚   if inflight:                                                                           â”‚\r\nbackend-api  | â”‚   192 â”‚   â”‚   # if we got here means we timed out                                                â”‚\r\nbackend-api  | â”‚   193 â”‚   â”‚   while inflight:                                                                    â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langgraph/pregel/retry.py:76 in arun_with_retry          â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    73 â”‚   â”‚   â”‚   task.writes.clear()                                                            â”‚\r\nbackend-api  | â”‚    74 â”‚   â”‚   â”‚   # run the task                                                                 â”‚\r\nbackend-api  | â”‚    75 â”‚   â”‚   â”‚   if stream:                                                                     â”‚\r\nbackend-api  | â”‚ â±  76 â”‚   â”‚   â”‚   â”‚   async for _ in task.proc.astream(task.input, task.config):                 â”‚\r\nbackend-api  | â”‚    77 â”‚   â”‚   â”‚   â”‚   â”‚   pass                                                                   â”‚\r\nbackend-api  | â”‚    78 â”‚   â”‚   â”‚   else:                                                                          â”‚\r\nbackend-api  | â”‚    79 â”‚   â”‚   â”‚   â”‚   await task.proc.ainvoke(task.input, task.config)                           â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langgraph/utils/runnable.py:504 in astream               â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   501 â”‚   â”‚   â”‚   â”‚   â”‚   )                                                                      â”‚\r\nbackend-api  | â”‚   502 â”‚   â”‚   â”‚   â”‚   output: Output = None                                                      â”‚\r\nbackend-api  | â”‚   503 â”‚   â”‚   â”‚   â”‚   add_supported = False                                                      â”‚\r\nbackend-api  | â”‚ â± 504 â”‚   â”‚   â”‚   â”‚   async for chunk in aiterator:                                              â”‚\r\nbackend-api  | â”‚   505 â”‚   â”‚   â”‚   â”‚   â”‚   yield chunk                                                            â”‚\r\nbackend-api  | â”‚   506 â”‚   â”‚   â”‚   â”‚   â”‚   # collect final output                                                 â”‚\r\nbackend-api  | â”‚   507 â”‚   â”‚   â”‚   â”‚   â”‚   if add_supported:                                                      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py:258 in              â”‚\r\nbackend-api  | â”‚ tap_output_aiter                                                                                 â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   255 â”‚   â”‚   Yields:                                                                            â”‚\r\nbackend-api  | â”‚   256 â”‚   â”‚   â”‚   T: The output value.                                                           â”‚\r\nbackend-api  | â”‚   257 â”‚   â”‚   \"\"\"                                                                                â”‚\r\nbackend-api  | â”‚ â± 258 â”‚   â”‚   async for chunk in output:                                                         â”‚\r\nbackend-api  | â”‚   259 â”‚   â”‚   â”‚   # root run is handled in .astream_log()                                        â”‚\r\nbackend-api  | â”‚   260 â”‚   â”‚   â”‚   if run_id != self.root_id:                                                     â”‚\r\nbackend-api  | â”‚   261 â”‚   â”‚   â”‚   â”‚   # if we can't find the run silently ignore                                 â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:1314 in atransform      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   1311 â”‚   â”‚   final: Input                                                                      â”‚\r\nbackend-api  | â”‚   1312 â”‚   â”‚   got_first_val = False                                                             â”‚\r\nbackend-api  | â”‚   1313 â”‚   â”‚                                                                                     â”‚\r\nbackend-api  | â”‚ â± 1314 â”‚   â”‚   async for ichunk in input:                                                        â”‚\r\nbackend-api  | â”‚   1315 â”‚   â”‚   â”‚   # The default implementation of transform is to buffer input and              â”‚\r\nbackend-api  | â”‚   1316 â”‚   â”‚   â”‚   # then call stream.                                                           â”‚\r\nbackend-api  | â”‚   1317 â”‚   â”‚   â”‚   # It'll attempt to gather all input into a single chunk using                 â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:875 in astream          â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    872 â”‚   â”‚   Yields:                                                                           â”‚\r\nbackend-api  | â”‚    873 â”‚   â”‚   â”‚   The output of the Runnable.                                                   â”‚\r\nbackend-api  | â”‚    874 â”‚   â”‚   \"\"\"                                                                               â”‚\r\nbackend-api  | â”‚ â±  875 â”‚   â”‚   yield await self.ainvoke(input, config, **kwargs)                                 â”‚\r\nbackend-api  | â”‚    876 â”‚                                                                                         â”‚\r\nbackend-api  | â”‚    877 â”‚   @overload                                                                             â”‚\r\nbackend-api  | â”‚    878 â”‚   def astream_log(                                                                      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langgraph/utils/runnable.py:168 in ainvoke               â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   165 â”‚   â”‚   else:                                                                              â”‚\r\nbackend-api  | â”‚   166 â”‚   â”‚   â”‚   context.run(_set_config_context, config)                                       â”‚\r\nbackend-api  | â”‚   167 â”‚   â”‚   â”‚   if ASYNCIO_ACCEPTS_CONTEXT:                                                    â”‚\r\nbackend-api  | â”‚ â± 168 â”‚   â”‚   â”‚   â”‚   ret = await asyncio.create_task(                                           â”‚\r\nbackend-api  | â”‚   169 â”‚   â”‚   â”‚   â”‚   â”‚   self.afunc(input, **kwargs), context=context                           â”‚\r\nbackend-api  | â”‚   170 â”‚   â”‚   â”‚   â”‚   )                                                                          â”‚\r\nbackend-api  | â”‚   171 â”‚   â”‚   â”‚   else:                                                                          â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py:590 in                â”‚\r\nbackend-api  | â”‚ run_in_executor                                                                                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   587 â”‚                                                                                          â”‚\r\nbackend-api  | â”‚   588 â”‚   if executor_or_config is None or isinstance(executor_or_config, dict):                 â”‚\r\nbackend-api  | â”‚   589 â”‚   â”‚   # Use default executor with context copied from current context                    â”‚\r\nbackend-api  | â”‚ â± 590 â”‚   â”‚   return await asyncio.get_running_loop().run_in_executor(                           â”‚\r\nbackend-api  | â”‚   591 â”‚   â”‚   â”‚   None,                                                                          â”‚\r\nbackend-api  | â”‚   592 â”‚   â”‚   â”‚   cast(Callable[..., T], partial(copy_context().run, wrapper)),                  â”‚\r\nbackend-api  | â”‚   593 â”‚   â”‚   )                                                                                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/concurrent/futures/thread.py:58 in run                                 â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    55 â”‚   â”‚   â”‚   return                                                                         â”‚\r\nbackend-api  | â”‚    56 â”‚   â”‚                                                                                      â”‚\r\nbackend-api  | â”‚    57 â”‚   â”‚   try:                                                                               â”‚\r\nbackend-api  | â”‚ â±  58 â”‚   â”‚   â”‚   result = self.fn(*self.args, **self.kwargs)                                    â”‚\r\nbackend-api  | â”‚    59 â”‚   â”‚   except BaseException as exc:                                                       â”‚\r\nbackend-api  | â”‚    60 â”‚   â”‚   â”‚   self.future.set_exception(exc)                                                 â”‚\r\nbackend-api  | â”‚    61 â”‚   â”‚   â”‚   # Break a reference cycle with the exception 'exc'                             â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py:581 in wrapper        â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   578 â”‚                                                                                          â”‚\r\nbackend-api  | â”‚   579 â”‚   def wrapper() -> T:                                                                    â”‚\r\nbackend-api  | â”‚   580 â”‚   â”‚   try:                                                                               â”‚\r\nbackend-api  | â”‚ â± 581 â”‚   â”‚   â”‚   return func(*args, **kwargs)                                                   â”‚\r\nbackend-api  | â”‚   582 â”‚   â”‚   except StopIteration as exc:                                                       â”‚\r\nbackend-api  | â”‚   583 â”‚   â”‚   â”‚   # StopIteration can't be set on an asyncio.Future                              â”‚\r\nbackend-api  | â”‚   584 â”‚   â”‚   â”‚   # it raises a TypeError and leaves the Future pending forever                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /app/core/quivr_core/quivr_rag_langgraph.py:274 in generate_chat_llm                             â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   271 â”‚   â”‚   rag_chain = custom_prompts.CHAT_LLM_PROMPT | llm                                   â”‚\r\nbackend-api  | â”‚   272 â”‚   â”‚                                                                                      â”‚\r\nbackend-api  | â”‚   273 â”‚   â”‚   # Run                                                                              â”‚\r\nbackend-api  | â”‚ â± 274 â”‚   â”‚   response = rag_chain.invoke(final_inputs)                                          â”‚\r\nbackend-api  | â”‚   275 â”‚   â”‚   formatted_response = {                                                             â”‚\r\nbackend-api  | â”‚   276 â”‚   â”‚   â”‚   \"answer\": response,  # Assuming the last message contains the final answer     â”‚\r\nbackend-api  | â”‚   277 â”‚   â”‚   }                                                                                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:2879 in invoke          â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   2876 â”‚   â”‚   â”‚   â”‚   if i == 0:                                                                â”‚\r\nbackend-api  | â”‚   2877 â”‚   â”‚   â”‚   â”‚   â”‚   input = context.run(step.invoke, input, config, **kwargs)             â”‚\r\nbackend-api  | â”‚   2878 â”‚   â”‚   â”‚   â”‚   else:                                                                     â”‚\r\nbackend-api  | â”‚ â± 2879 â”‚   â”‚   â”‚   â”‚   â”‚   input = context.run(step.invoke, input, config)                       â”‚\r\nbackend-api  | â”‚   2880 â”‚   â”‚   # finish the root run                                                             â”‚\r\nbackend-api  | â”‚   2881 â”‚   â”‚   except BaseException as e:                                                        â”‚\r\nbackend-api  | â”‚   2882 â”‚   â”‚   â”‚   run_manager.on_chain_error(e)                                                 â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:277 in     â”‚\r\nbackend-api  | â”‚ invoke                                                                                           â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    274 â”‚   â”‚   config = ensure_config(config)                                                    â”‚\r\nbackend-api  | â”‚    275 â”‚   â”‚   return cast(                                                                      â”‚\r\nbackend-api  | â”‚    276 â”‚   â”‚   â”‚   ChatGeneration,                                                               â”‚\r\nbackend-api  | â”‚ â±  277 â”‚   â”‚   â”‚   self.generate_prompt(                                                         â”‚\r\nbackend-api  | â”‚    278 â”‚   â”‚   â”‚   â”‚   [self._convert_input(input)],                                             â”‚\r\nbackend-api  | â”‚    279 â”‚   â”‚   â”‚   â”‚   stop=stop,                                                                â”‚\r\nbackend-api  | â”‚    280 â”‚   â”‚   â”‚   â”‚   callbacks=config.get(\"callbacks\"),                                        â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:777 in     â”‚\r\nbackend-api  | â”‚ generate_prompt                                                                                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    774 â”‚   â”‚   **kwargs: Any,                                                                    â”‚\r\nbackend-api  | â”‚    775 â”‚   ) -> LLMResult:                                                                       â”‚\r\nbackend-api  | â”‚    776 â”‚   â”‚   prompt_messages = [p.to_messages() for p in prompts]                              â”‚\r\nbackend-api  | â”‚ â±  777 â”‚   â”‚   return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)   â”‚\r\nbackend-api  | â”‚    778 â”‚                                                                                         â”‚\r\nbackend-api  | â”‚    779 â”‚   async def agenerate_prompt(                                                           â”‚\r\nbackend-api  | â”‚    780 â”‚   â”‚   self,                                                                             â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:634 in     â”‚\r\nbackend-api  | â”‚ generate                                                                                         â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    631 â”‚   â”‚   â”‚   except BaseException as e:                                                    â”‚\r\nbackend-api  | â”‚    632 â”‚   â”‚   â”‚   â”‚   if run_managers:                                                          â”‚\r\nbackend-api  | â”‚    633 â”‚   â”‚   â”‚   â”‚   â”‚   run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))   â”‚\r\nbackend-api  | â”‚ â±  634 â”‚   â”‚   â”‚   â”‚   raise e                                                                   â”‚\r\nbackend-api  | â”‚    635 â”‚   â”‚   flattened_outputs = [                                                             â”‚\r\nbackend-api  | â”‚    636 â”‚   â”‚   â”‚   LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type:  â”‚\r\nbackend-api  | â”‚    637 â”‚   â”‚   â”‚   for res in results                                                            â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:624 in     â”‚\r\nbackend-api  | â”‚ generate                                                                                         â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    621 â”‚   â”‚   for i, m in enumerate(messages):                                                  â”‚\r\nbackend-api  | â”‚    622 â”‚   â”‚   â”‚   try:                                                                          â”‚\r\nbackend-api  | â”‚    623 â”‚   â”‚   â”‚   â”‚   results.append(                                                           â”‚\r\nbackend-api  | â”‚ â±  624 â”‚   â”‚   â”‚   â”‚   â”‚   self._generate_with_cache(                                            â”‚\r\nbackend-api  | â”‚    625 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   m,                                                                â”‚\r\nbackend-api  | â”‚    626 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   stop=stop,                                                        â”‚\r\nbackend-api  | â”‚    627 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   run_manager=run_managers[i] if run_managers else None,            â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:834 in     â”‚\r\nbackend-api  | â”‚ _generate_with_cache                                                                             â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    831 â”‚   â”‚   â”‚   **kwargs,                                                                     â”‚\r\nbackend-api  | â”‚    832 â”‚   â”‚   ):                                                                                â”‚\r\nbackend-api  | â”‚    833 â”‚   â”‚   â”‚   chunks: List[ChatGenerationChunk] = []                                        â”‚\r\nbackend-api  | â”‚ â±  834 â”‚   â”‚   â”‚   for chunk in self._stream(messages, stop=stop, **kwargs):                     â”‚\r\nbackend-api  | â”‚    835 â”‚   â”‚   â”‚   â”‚   chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)       â”‚\r\nbackend-api  | â”‚    836 â”‚   â”‚   â”‚   â”‚   if run_manager:                                                           â”‚\r\nbackend-api  | â”‚    837 â”‚   â”‚   â”‚   â”‚   â”‚   if chunk.message.id is None:                                          â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:637 in _stream      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    634 â”‚   â”‚   â”‚   response = raw_response.parse()                                               â”‚\r\nbackend-api  | â”‚    635 â”‚   â”‚   â”‚   base_generation_info = {\"headers\": dict(raw_response.headers)}                â”‚\r\nbackend-api  | â”‚    636 â”‚   â”‚   else:                                                                             â”‚\r\nbackend-api  | â”‚ â±  637 â”‚   â”‚   â”‚   response = self.client.create(**payload)                                      â”‚\r\nbackend-api  | â”‚    638 â”‚   â”‚   with response:                                                                    â”‚\r\nbackend-api  | â”‚    639 â”‚   â”‚   â”‚   is_first_chunk = True                                                         â”‚\r\nbackend-api  | â”‚    640 â”‚   â”‚   â”‚   for chunk in response:                                                        â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py:274 in wrapper                   â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   271 â”‚   â”‚   â”‚   â”‚   â”‚   else:                                                                  â”‚\r\nbackend-api  | â”‚   272 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   msg = f\"Missing required argument: {quote(missing[0])}\"            â”‚\r\nbackend-api  | â”‚   273 â”‚   â”‚   â”‚   â”‚   raise TypeError(msg)                                                       â”‚\r\nbackend-api  | â”‚ â± 274 â”‚   â”‚   â”‚   return func(*args, **kwargs)                                                   â”‚\r\nbackend-api  | â”‚   275 â”‚   â”‚                                                                                      â”‚\r\nbackend-api  | â”‚   276 â”‚   â”‚   return wrapper  # type: ignore                                                     â”‚\r\nbackend-api  | â”‚   277                                                                                            â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/openai/resources/chat/completions.py:704 in create       â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    701 â”‚   â”‚   timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,                     â”‚\r\nbackend-api  | â”‚    702 â”‚   ) -> ChatCompletion | Stream[ChatCompletionChunk]:                                    â”‚\r\nbackend-api  | â”‚    703 â”‚   â”‚   validate_response_format(response_format)                                         â”‚\r\nbackend-api  | â”‚ â±  704 â”‚   â”‚   return self._post(                                                                â”‚\r\nbackend-api  | â”‚    705 â”‚   â”‚   â”‚   \"/chat/completions\",                                                          â”‚\r\nbackend-api  | â”‚    706 â”‚   â”‚   â”‚   body=maybe_transform(                                                         â”‚\r\nbackend-api  | â”‚    707 â”‚   â”‚   â”‚   â”‚   {                                                                         â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/openai/_base_client.py:1268 in post                      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   1265 â”‚   â”‚   opts = FinalRequestOptions.construct(                                             â”‚\r\nbackend-api  | â”‚   1266 â”‚   â”‚   â”‚   method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **opti  â”‚\r\nbackend-api  | â”‚   1267 â”‚   â”‚   )                                                                                 â”‚\r\nbackend-api  | â”‚ â± 1268 â”‚   â”‚   return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=str  â”‚\r\nbackend-api  | â”‚   1269 â”‚                                                                                         â”‚\r\nbackend-api  | â”‚   1270 â”‚   def patch(                                                                            â”‚\r\nbackend-api  | â”‚   1271 â”‚   â”‚   self,                                                                             â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/openai/_base_client.py:945 in request                    â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    942 â”‚   â”‚   else:                                                                             â”‚\r\nbackend-api  | â”‚    943 â”‚   â”‚   â”‚   retries_taken = 0                                                             â”‚\r\nbackend-api  | â”‚    944 â”‚   â”‚                                                                                     â”‚\r\nbackend-api  | â”‚ â±  945 â”‚   â”‚   return self._request(                                                             â”‚\r\nbackend-api  | â”‚    946 â”‚   â”‚   â”‚   cast_to=cast_to,                                                              â”‚\r\nbackend-api  | â”‚    947 â”‚   â”‚   â”‚   options=options,                                                              â”‚\r\nbackend-api  | â”‚    948 â”‚   â”‚   â”‚   stream=stream,                                                                â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/openai/_base_client.py:1005 in _request                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   1002 â”‚   â”‚   â”‚   log.debug(\"Encountered Exception\", exc_info=True)                             â”‚\r\nbackend-api  | â”‚   1003 â”‚   â”‚   â”‚                                                                                 â”‚\r\nbackend-api  | â”‚   1004 â”‚   â”‚   â”‚   if remaining_retries > 0:                                                     â”‚\r\nbackend-api  | â”‚ â± 1005 â”‚   â”‚   â”‚   â”‚   return self._retry_request(                                               â”‚\r\nbackend-api  | â”‚   1006 â”‚   â”‚   â”‚   â”‚   â”‚   input_options,                                                        â”‚\r\nbackend-api  | â”‚   1007 â”‚   â”‚   â”‚   â”‚   â”‚   cast_to,                                                              â”‚\r\nbackend-api  | â”‚   1008 â”‚   â”‚   â”‚   â”‚   â”‚   retries_taken=retries_taken,                                          â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/openai/_base_client.py:1083 in _retry_request            â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   1080 â”‚   â”‚   # different thread if necessary.                                                  â”‚\r\nbackend-api  | â”‚   1081 â”‚   â”‚   time.sleep(timeout)                                                               â”‚\r\nbackend-api  | â”‚   1082 â”‚   â”‚                                                                                     â”‚\r\nbackend-api  | â”‚ â± 1083 â”‚   â”‚   return self._request(                                                             â”‚\r\nbackend-api  | â”‚   1084 â”‚   â”‚   â”‚   options=options,                                                              â”‚\r\nbackend-api  | â”‚   1085 â”‚   â”‚   â”‚   cast_to=cast_to,                                                              â”‚\r\nbackend-api  | â”‚   1086 â”‚   â”‚   â”‚   retries_taken=retries_taken + 1,                                              â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/openai/_base_client.py:1005 in _request                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   1002 â”‚   â”‚   â”‚   log.debug(\"Encountered Exception\", exc_info=True)                             â”‚\r\nbackend-api  | â”‚   1003 â”‚   â”‚   â”‚                                                                                 â”‚\r\nbackend-api  | â”‚   1004 â”‚   â”‚   â”‚   if remaining_retries > 0:                                                     â”‚\r\nbackend-api  | â”‚ â± 1005 â”‚   â”‚   â”‚   â”‚   return self._retry_request(                                               â”‚\r\nbackend-api  | â”‚   1006 â”‚   â”‚   â”‚   â”‚   â”‚   input_options,                                                        â”‚\r\nbackend-api  | â”‚   1007 â”‚   â”‚   â”‚   â”‚   â”‚   cast_to,                                                              â”‚\r\nbackend-api  | â”‚   1008 â”‚   â”‚   â”‚   â”‚   â”‚   retries_taken=retries_taken,                                          â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/openai/_base_client.py:1083 in _retry_request            â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   1080 â”‚   â”‚   # different thread if necessary.                                                  â”‚\r\nbackend-api  | â”‚   1081 â”‚   â”‚   time.sleep(timeout)                                                               â”‚\r\nbackend-api  | â”‚   1082 â”‚   â”‚                                                                                     â”‚\r\nbackend-api  | â”‚ â± 1083 â”‚   â”‚   return self._request(                                                             â”‚\r\nbackend-api  | â”‚   1084 â”‚   â”‚   â”‚   options=options,                                                              â”‚\r\nbackend-api  | â”‚   1085 â”‚   â”‚   â”‚   cast_to=cast_to,                                                              â”‚\r\nbackend-api  | â”‚   1086 â”‚   â”‚   â”‚   retries_taken=retries_taken + 1,                                              â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/openai/_base_client.py:1015 in _request                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   1012 â”‚   â”‚   â”‚   â”‚   )                                                                         â”‚\r\nbackend-api  | â”‚   1013 â”‚   â”‚   â”‚                                                                                 â”‚\r\nbackend-api  | â”‚   1014 â”‚   â”‚   â”‚   log.debug(\"Raising connection error\")                                         â”‚\r\nbackend-api  | â”‚ â± 1015 â”‚   â”‚   â”‚   raise APIConnectionError(request=request) from err                            â”‚\r\nbackend-api  | â”‚   1016 â”‚   â”‚                                                                                     â”‚\r\nbackend-api  | â”‚   1017 â”‚   â”‚   log.debug(                                                                        â”‚\r\nbackend-api  | â”‚   1018 â”‚   â”‚   â”‚   'HTTP Response: %s %s \"%i %s\" %s',                                            â”‚\r\nbackend-api  | â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯`"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nThe code is running into OpenAI because the `ImageGeneratorTool` class explicitly uses the `OpenAI` client to generate images. The `_run` and `_arun` methods create an instance of the `OpenAI` client and call its `images.generate` method, which is why the code interacts with OpenAI, regardless of the network configuration with Ollama <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/tools/image_generator.py)</sup>.\n\nTo resolve this, you need to replace the `OpenAI` client with the appropriate client or API call for the Ollama model. Ensure that the code paths you are using are correctly configured to utilize the Ollama API instead of defaulting to OpenAI.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu but in the log, the code connect to opanai's some network service and then report error, not image function, and I have not configure openai key"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nIf the code is connecting to OpenAI's services despite not having configured an OpenAI API key, it might be due to a fallback mechanism or default configuration that defaults to OpenAI when no specific supplier is set. The `set_llm_model` method in the `LLMEndpointConfig` class uses `LLMModelConfig.get_supplier_by_model_name(model)` to determine the supplier. If no supplier is found, it defaults to OpenAI, as indicated by the default supplier setting: `supplier: DefaultModelSuppliers = DefaultModelSuppliers.OPENAI` <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/core/quivr_core/config.py)</sup>.\n\nTo prevent this, ensure that your model is correctly configured to use Ollama and that the supplier is explicitly set to Ollama in your configuration. Double-check the `_model_defaults` dictionary to ensure your Ollama model is correctly listed under the appropriate supplier. Additionally, verify that any fallback logic in the code does not default to OpenAI when an API key is missing or when a supplier is not explicitly set.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu i see in the log the chat code use rag_chain.invoke(final_inputs) to get the chat response, so how to print the rag_chain and  final_inputs details?\r\n`backend-api  | â”‚ /app/core/quivr_core/quivr_rag_langgraph.py:274 in generate_chat_llm                             â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   271 â”‚   â”‚   rag_chain = custom_prompts.CHAT_LLM_PROMPT | llm                                   â”‚\r\nbackend-api  | â”‚   272 â”‚   â”‚                                                                                      â”‚\r\nbackend-api  | â”‚   273 â”‚   â”‚   # Run                                                                              â”‚\r\nbackend-api  | â”‚ â± 274 â”‚   â”‚   response = rag_chain.invoke(final_inputs)                                          â”‚\r\nbackend-api  | â”‚   275 â”‚   â”‚   formatted_response = {                                                             â”‚\r\nbackend-api  | â”‚   276 â”‚   â”‚   â”‚   \"answer\": response,  # Assuming the last message contains the final answer     â”‚\r\nbackend-api  | â”‚   277 â”‚   â”‚   }                                                                                  â”‚`"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo print the details of `rag_chain` and `final_inputs`, you can add some logging or print statements in the `generate_chat_llm` function. Here's how you can do it:\n\n1. **Locate the `generate_chat_llm` function** in your codebase, specifically in the `quivr_rag_langgraph.py` file.\n\n2. **Add print statements** before the `rag_chain.invoke(final_inputs)` line to output the details:\n\n```python\n# Print the rag_chain details\nprint(\"RAG Chain Details:\", rag_chain)\n\n# Print the final_inputs details\nprint(\"Final Inputs:\", final_inputs)\n\n# Run\nresponse = rag_chain.invoke(final_inputs)\n```\n\n3. **Rebuild and restart your Docker container** to apply these changes, following the steps you used previously to update the Docker image.\n\nThis will output the details of `rag_chain` and `final_inputs` to the console, allowing you to inspect their contents and verify that they are configured as expected.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu Do I need to run litellm when use quivr with ollama?"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nYes, you need to run LiteLLM when using Quivr with Ollama. The `ChatLiteLLM` is instantiated in the code, indicating its use in the integration with Ollama <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/brain/integrations/Big/Brain.py)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/brain/rags/quivr_rag.py)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu So how to run litellm and ollama? please use ollama mistral for example."
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo run LiteLLM with Ollama, specifically using the Ollama Mistral model, follow these steps:\n\n1. **Install Dependencies**: Ensure you have the necessary dependencies installed. You need `litellm` and other related packages:\n\n   ```bash\n   pip install litellm==1.40.4 llama-cpp-python==0.2.67\n   ```\n\n2. **Configure the Model**: Add the Ollama Mistral model to your `_model_defaults` in the configuration:\n\n   ```python\n   _model_defaults: Dict[DefaultModelSuppliers, Dict[str, LLMConfig]] = {\n       DefaultModelSuppliers.OLLAMA: {\n           \"ollama-mistral\": LLMConfig(context=128000, tokenizer_hub=\"Xenova/mistral-tokenizer-v3\"),\n       },\n   }\n   ```\n\n3. **Set Environment Variables**: Ensure your `.env` file or environment variables are correctly set, especially `ollama_api_base_url`.\n\n4. **Run the Application**: Start your application, ensuring that the Docker container or local environment is configured to use the Ollama API. You can use the `make build` command to rebuild your Docker image if needed.\n\n5. **Verify Configuration**: Check that the `ChatLiteLLM` is correctly instantiated with the Ollama model and API base URL:\n\n   ```python\n   api_base = self.brain_settings.ollama_api_base_url\n   llm = ChatLiteLLM(\n       temperature=0,\n       model=\"ollama-mistral\",\n       api_base=api_base,\n       max_tokens=128000,\n   )\n   ```\n\nBy following these steps, you should be able to run LiteLLM with the Ollama Mistral model in Quivr.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "@dosu I mean how to run litellm and ollama outside for quivr?"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo run LiteLLM with Ollama outside of Quivr, follow these steps:\n\n1. **Install Dependencies**: Ensure you have `litellm` and any other necessary packages installed:\n\n   ```bash\n   pip install litellm==1.40.4 llama-cpp-python==0.2.67\n   ```\n\n2. **Set Up Configuration**: Create a configuration for your Ollama model. For example, for the Ollama Mistral model:\n\n   ```python\n   from litellm import ChatLiteLLM\n\n   api_base = \"your_ollama_api_base_url\"  # Replace with your actual API base URL\n\n   llm = ChatLiteLLM(\n       temperature=0.7,\n       model=\"ollama-mistral\",\n       api_base=api_base,\n       max_tokens=128000,\n   )\n   ```\n\n3. **Environment Variables**: Ensure that any required environment variables, such as `ollama_api_base_url`, are set in your environment.\n\n4. **Run Your Code**: Use the `ChatLiteLLM` instance to interact with the Ollama model:\n\n   ```python\n   response = llm.invoke(\"Your input text here\")\n   print(\"Response:\", response)\n   ```\n\nBy setting up the configuration and environment correctly, you can run LiteLLM with Ollama models independently of Quivr.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3328,
    "title": "[Bug]: invalid api token",
    "author": "adithep",
    "state": "closed",
    "created_at": "2024-10-07T08:02:06Z",
    "updated_at": "2025-01-10T12:09:37Z",
    "labels": [
      "bug",
      "Stale",
      "area: backend"
    ],
    "body": "### What happened?\n\nOnce trying to chat with newly create brain got:\r\n\r\n`UnauthorizedError: status_code: 401, body: {'message': 'invalid api token'}`\r\n\r\nThis bug only happens when trying to chat with a brain with document.\r\n\r\nChatting directly with the model is fine. So, likely not a problem with open ai api key?\n\n### Relevant log output\n\n```bash\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\r\nbackend-api  | OperationalError: server closed the connection unexpectedly\r\nbackend-api  |         This probably means the server terminated abnormally\r\nbackend-api  |         before or while processing the request.\r\nbackend-api  |\r\nbackend-api  |\r\nbackend-api  | 2024-10-07T07:45:39.920715Z [error    ] [sqlalchemy.pool.impl.AsyncAdaptedQueuePool] msg=The garbage collector is trying to clean up non-checked-in connection <AdaptedConnection <asyncpg.connection.Connection object at 0xffff07afd3f0>>, which will be terminated.  Please ensure that SQLAlchemy pooled connections are returned to the pool explicitly, either by calling ``close()`` or by using appropriate context managers to manage their lifecycle.\r\nbackend-api  | /usr/local/lib/python3.11/site-packages/rich/text.py:1138: SAWarning: The garbage collector is trying to clean up non-checked-in connection <AdaptedConnection <asyncpg.connection.Connection object at 0xffff07afd3f0>>, which will be terminated.  Please ensure that SQLAlchemy pooled connections are returned to the pool explicitly, either by calling ``close()`` or by using appropriate context managers to manage their lifecycle.\r\nbackend-api  |   _line_appends = [line._spans.append for line in new_lines._lines]\r\nbackend-api  | 2024-10-07T07:45:39.921032Z [error    ] [sqlalchemy.pool.impl.QueuePool] msg=Exception during reset or similar\r\nbackend-api  | â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py:986 in _finalize_fairy           â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    983 â”‚   â”‚   â”‚   â”‚   )                                                                         â”‚\r\nbackend-api  | â”‚    984 â”‚   â”‚   â”‚   assert fairy.dbapi_connection is dbapi_connection                             â”‚\r\nbackend-api  | â”‚    985 â”‚   â”‚   â”‚                                                                                 â”‚\r\nbackend-api  | â”‚ â±  986 â”‚   â”‚   â”‚   fairy._reset(                                                                 â”‚\r\nbackend-api  | â”‚    987 â”‚   â”‚   â”‚   â”‚   pool,                                                                     â”‚\r\nbackend-api  | â”‚    988 â”‚   â”‚   â”‚   â”‚   transaction_was_reset=transaction_was_reset,                              â”‚\r\nbackend-api  | â”‚    989 â”‚   â”‚   â”‚   â”‚   terminate_only=detach,                                                    â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py:1432 in _reset                   â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   1429 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   \"Connection %s rollback-on-return\",                               â”‚\r\nbackend-api  | â”‚   1430 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   self.dbapi_connection,                                            â”‚\r\nbackend-api  | â”‚   1431 â”‚   â”‚   â”‚   â”‚   â”‚   )                                                                     â”‚\r\nbackend-api  | â”‚ â± 1432 â”‚   â”‚   â”‚   â”‚   pool._dialect.do_rollback(self)                                           â”‚\r\nbackend-api  | â”‚   1433 â”‚   â”‚   elif pool._reset_on_return is reset_commit:                                       â”‚\r\nbackend-api  | â”‚   1434 â”‚   â”‚   â”‚   if self._echo:                                                                â”‚\r\nbackend-api  | â”‚   1435 â”‚   â”‚   â”‚   â”‚   pool.logger.debug(                                                        â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/sqlalchemy/engine/default.py:699 in do_rollback          â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    696 â”‚   â”‚   pass                                                                              â”‚\r\nbackend-api  | â”‚    697 â”‚                                                                                         â”‚\r\nbackend-api  | â”‚    698 â”‚   def do_rollback(self, dbapi_connection):                                              â”‚\r\nbackend-api  | â”‚ â±  699 â”‚   â”‚   dbapi_connection.rollback()                                                       â”‚\r\nbackend-api  | â”‚    700 â”‚                                                                                         â”‚\r\nbackend-api  | â”‚    701 â”‚   def do_commit(self, dbapi_connection):                                                â”‚\r\nbackend-api  | â”‚    702 â”‚   â”‚   dbapi_connection.commit()                                                         â”‚\r\nbackend-api  | â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\r\nbackend-api  | OperationalError: server closed the connection unexpectedly\r\nbackend-api  |         This probably means the server terminated abnormally\r\nbackend-api  |         before or while processing the request.\r\nbackend-api  |\r\nbackend-api  |\r\nbackend-api  | 2024-10-07T07:45:39.992070Z [error    ] [sqlalchemy.pool.impl.AsyncAdaptedQueuePool] msg=The garbage collector is trying to clean up non-checked-in connection <AdaptedConnection <asyncpg.connection.Connection object at 0xffff0533a020>>, which will be terminated.  Please ensure that SQLAlchemy pooled connections are returned to the pool explicitly, either by calling ``close()`` or by using appropriate context managers to manage their lifecycle.\r\nbackend-api  | /usr/local/lib/python3.11/site-packages/rich/text.py:1138: SAWarning: The garbage collector is trying to clean up non-checked-in connection <AdaptedConnection <asyncpg.connection.Connection object at 0xffff0533a020>>, which will be terminated.  Please ensure that SQLAlchemy pooled connections are returned to the pool explicitly, either by calling ``close()`` or by using appropriate context managers to manage their lifecycle.\r\nbackend-api  |   _line_appends = [line._spans.append for line in new_lines._lines]\r\nbackend-api  | 2024-10-07T07:45:39.992308Z [error    ] [sqlalchemy.pool.impl.AsyncAdaptedQueuePool] msg=The garbage collector is trying to clean up non-checked-in connection <AdaptedConnection <asyncpg.connection.Connection object at 0xfffefdb38f40>>, which will be terminated.  Please ensure that SQLAlchemy pooled connections are returned to the pool explicitly, either by calling ``close()`` or by using appropriate context managers to manage their lifecycle.\r\nbackend-api  | /usr/local/lib/python3.11/site-packages/rich/text.py:1138: SAWarning: The garbage collector is trying to clean up non-checked-in connection <AdaptedConnection <asyncpg.connection.Connection object at 0xfffefdb38f40>>, which will be terminated.  Please ensure that SQLAlchemy pooled connections are returned to the pool explicitly, either by calling ``close()`` or by using appropriate context managers to manage their lifecycle.\r\nbackend-api  |   _line_appends = [line._spans.append for line in new_lines._lines]\r\nbackend-api  | 2024-10-07T07:45:39.285815Z [error    ] [asyncio] msg=Task exception was never retrieved\r\nbackend-api  | future: <Task finished name='Task-2226' coro=<<async_generator_athrow without __name__>()> exception=UnauthorizedError({'message': 'invalid api token'})>\r\nbackend-api  | â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py:661 in              â”‚\r\nbackend-api  | â”‚ _astream_log_implementation                                                                      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   658 â”‚   â”‚   # Yield each chunk from the output stream                                          â”‚\r\nbackend-api  | â”‚   659 â”‚   â”‚   if diff:                                                                           â”‚\r\nbackend-api  | â”‚   660 â”‚   â”‚   â”‚   async for log in stream:                                                       â”‚\r\nbackend-api  | â”‚ â± 661 â”‚   â”‚   â”‚   â”‚   yield log                                                                  â”‚\r\nbackend-api  | â”‚   662 â”‚   â”‚   else:                                                                              â”‚\r\nbackend-api  | â”‚   663 â”‚   â”‚   â”‚   state = RunLog(state=None)  # type: ignore[arg-type]                           â”‚\r\nbackend-api  | â”‚   664 â”‚   â”‚   â”‚   async for log in stream:                                                       â”‚\r\nbackend-api  | â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\r\nbackend-api  | GeneratorExit\r\nbackend-api  |\r\nbackend-api  | During handling of the above exception, another exception occurred:\r\nbackend-api  |\r\nbackend-api  | â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py:670 in              â”‚\r\nbackend-api  | â”‚ _astream_log_implementation                                                                      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   667 â”‚   finally:                                                                               â”‚\r\nbackend-api  | â”‚   668 â”‚   â”‚   # Wait for the runnable to finish, if not cancelled (eg. by break)                 â”‚\r\nbackend-api  | â”‚   669 â”‚   â”‚   try:                                                                               â”‚\r\nbackend-api  | â”‚ â± 670 â”‚   â”‚   â”‚   await task                                                                     â”‚\r\nbackend-api  | â”‚   671 â”‚   â”‚   except asyncio.CancelledError:                                                     â”‚\r\nbackend-api  | â”‚   672 â”‚   â”‚   â”‚   pass                                                                           â”‚\r\nbackend-api  | â”‚   673                                                                                            â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py:624 in              â”‚\r\nbackend-api  | â”‚ consume_astream                                                                                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   621 â”‚   â”‚   â”‚   prev_final_output: Optional[Output] = None                                     â”‚\r\nbackend-api  | â”‚   622 â”‚   â”‚   â”‚   final_output: Optional[Output] = None                                          â”‚\r\nbackend-api  | â”‚   623 â”‚   â”‚   â”‚                                                                                  â”‚\r\nbackend-api  | â”‚ â± 624 â”‚   â”‚   â”‚   async for chunk in runnable.astream(input, config, **kwargs):                  â”‚\r\nbackend-api  | â”‚   625 â”‚   â”‚   â”‚   â”‚   prev_final_output = final_output                                           â”‚\r\nbackend-api  | â”‚   626 â”‚   â”‚   â”‚   â”‚   if final_output is None:                                                   â”‚\r\nbackend-api  | â”‚   627 â”‚   â”‚   â”‚   â”‚   â”‚   final_output = chunk                                                   â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1411 in astream             â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   1408 â”‚   â”‚   â”‚   â”‚   â”‚   interrupt_after=interrupt_after,                                      â”‚\r\nbackend-api  | â”‚   1409 â”‚   â”‚   â”‚   â”‚   â”‚   manager=run_manager,                                                  â”‚\r\nbackend-api  | â”‚   1410 â”‚   â”‚   â”‚   â”‚   ):                                                                        â”‚\r\nbackend-api  | â”‚ â± 1411 â”‚   â”‚   â”‚   â”‚   â”‚   async for _ in runner.atick(                                          â”‚\r\nbackend-api  | â”‚   1412 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   loop.tasks.values(),                                              â”‚\r\nbackend-api  | â”‚   1413 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   timeout=self.step_timeout,                                        â”‚\r\nbackend-api  | â”‚   1414 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   retry_policy=self.retry_policy,                                   â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langgraph/pregel/runner.py:145 in atick                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   142 â”‚   â”‚   â”‚   # give control back to the caller                                              â”‚\r\nbackend-api  | â”‚   143 â”‚   â”‚   â”‚   yield                                                                          â”‚\r\nbackend-api  | â”‚   144 â”‚   â”‚   # panic on failure or timeout                                                      â”‚\r\nbackend-api  | â”‚ â± 145 â”‚   â”‚   _panic_or_proceed(all_futures, asyncio.TimeoutError)                               â”‚\r\nbackend-api  | â”‚   146                                                                                            â”‚\r\nbackend-api  | â”‚   147                                                                                            â”‚\r\nbackend-api  | â”‚   148 def _should_stop_others(                                                                   â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langgraph/pregel/runner.py:190 in _panic_or_proceed      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   187 â”‚   â”‚   â”‚   while inflight:                                                                â”‚\r\nbackend-api  | â”‚   188 â”‚   â”‚   â”‚   â”‚   inflight.pop().cancel()                                                    â”‚\r\nbackend-api  | â”‚   189 â”‚   â”‚   â”‚   # raise the exception                                                          â”‚\r\nbackend-api  | â”‚ â± 190 â”‚   â”‚   â”‚   raise exc                                                                      â”‚\r\nbackend-api  | â”‚   191 â”‚   if inflight:                                                                           â”‚\r\nbackend-api  | â”‚   192 â”‚   â”‚   # if we got here means we timed out                                                â”‚\r\nbackend-api  | â”‚   193 â”‚   â”‚   while inflight:                                                                    â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langgraph/pregel/retry.py:76 in arun_with_retry          â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    73 â”‚   â”‚   â”‚   task.writes.clear()                                                            â”‚\r\nbackend-api  | â”‚    74 â”‚   â”‚   â”‚   # run the task                                                                 â”‚\r\nbackend-api  | â”‚    75 â”‚   â”‚   â”‚   if stream:                                                                     â”‚\r\nbackend-api  | â”‚ â±  76 â”‚   â”‚   â”‚   â”‚   async for _ in task.proc.astream(task.input, task.config):                 â”‚\r\nbackend-api  | â”‚    77 â”‚   â”‚   â”‚   â”‚   â”‚   pass                                                                   â”‚\r\nbackend-api  | â”‚    78 â”‚   â”‚   â”‚   else:                                                                          â”‚\r\nbackend-api  | â”‚    79 â”‚   â”‚   â”‚   â”‚   await task.proc.ainvoke(task.input, task.config)                           â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langgraph/utils/runnable.py:504 in astream               â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   501 â”‚   â”‚   â”‚   â”‚   â”‚   )                                                                      â”‚\r\nbackend-api  | â”‚   502 â”‚   â”‚   â”‚   â”‚   output: Output = None                                                      â”‚\r\nbackend-api  | â”‚   503 â”‚   â”‚   â”‚   â”‚   add_supported = False                                                      â”‚\r\nbackend-api  | â”‚ â± 504 â”‚   â”‚   â”‚   â”‚   async for chunk in aiterator:                                              â”‚\r\nbackend-api  | â”‚   505 â”‚   â”‚   â”‚   â”‚   â”‚   yield chunk                                                            â”‚\r\nbackend-api  | â”‚   506 â”‚   â”‚   â”‚   â”‚   â”‚   # collect final output                                                 â”‚\r\nbackend-api  | â”‚   507 â”‚   â”‚   â”‚   â”‚   â”‚   if add_supported:                                                      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py:258 in              â”‚\r\nbackend-api  | â”‚ tap_output_aiter                                                                                 â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   255 â”‚   â”‚   Yields:                                                                            â”‚\r\nbackend-api  | â”‚   256 â”‚   â”‚   â”‚   T: The output value.                                                           â”‚\r\nbackend-api  | â”‚   257 â”‚   â”‚   \"\"\"                                                                                â”‚\r\nbackend-api  | â”‚ â± 258 â”‚   â”‚   async for chunk in output:                                                         â”‚\r\nbackend-api  | â”‚   259 â”‚   â”‚   â”‚   # root run is handled in .astream_log()                                        â”‚\r\nbackend-api  | â”‚   260 â”‚   â”‚   â”‚   if run_id != self.root_id:                                                     â”‚\r\nbackend-api  | â”‚   261 â”‚   â”‚   â”‚   â”‚   # if we can't find the run silently ignore                                 â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:1314 in atransform      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   1311 â”‚   â”‚   final: Input                                                                      â”‚\r\nbackend-api  | â”‚   1312 â”‚   â”‚   got_first_val = False                                                             â”‚\r\nbackend-api  | â”‚   1313 â”‚   â”‚                                                                                     â”‚\r\nbackend-api  | â”‚ â± 1314 â”‚   â”‚   async for ichunk in input:                                                        â”‚\r\nbackend-api  | â”‚   1315 â”‚   â”‚   â”‚   # The default implementation of transform is to buffer input and              â”‚\r\nbackend-api  | â”‚   1316 â”‚   â”‚   â”‚   # then call stream.                                                           â”‚\r\nbackend-api  | â”‚   1317 â”‚   â”‚   â”‚   # It'll attempt to gather all input into a single chunk using                 â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:875 in astream          â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    872 â”‚   â”‚   Yields:                                                                           â”‚\r\nbackend-api  | â”‚ run_in_executor                                                                                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   587 â”‚                                                                                          â”‚\r\nbackend-api  | â”‚   588 â”‚   if executor_or_config is None or isinstance(executor_or_config, dict):                 â”‚\r\nbackend-api  | â”‚   589 â”‚   â”‚   # Use default executor with context copied from current context                    â”‚\r\nbackend-api  | â”‚ â± 590 â”‚   â”‚   return await asyncio.get_running_loop().run_in_executor(                           â”‚\r\nbackend-api  | â”‚   591 â”‚   â”‚   â”‚   None,                                                                          â”‚\r\nbackend-api  | â”‚   592 â”‚   â”‚   â”‚   cast(Callable[..., T], partial(copy_context().run, wrapper)),                  â”‚\r\nbackend-api  | â”‚   593 â”‚   â”‚   )                                                                                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/concurrent/futures/thread.py:58 in run                                 â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    55 â”‚   â”‚   â”‚   return                                                                         â”‚\r\nbackend-api  | â”‚    56 â”‚   â”‚                                                                                      â”‚\r\nbackend-api  | â”‚    57 â”‚   â”‚   try:                                                                               â”‚\r\nbackend-api  | â”‚ â±  58 â”‚   â”‚   â”‚   result = self.fn(*self.args, **self.kwargs)                                    â”‚\r\nbackend-api  | â”‚    59 â”‚   â”‚   except BaseException as exc:                                                       â”‚\r\nbackend-api  | â”‚    60 â”‚   â”‚   â”‚   self.future.set_exception(exc)                                                 â”‚\r\nbackend-api  | â”‚    61 â”‚   â”‚   â”‚   # Break a reference cycle with the exception 'exc'                             â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py:581 in wrapper        â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   578 â”‚                                                                                          â”‚\r\nbackend-api  | â”‚   579 â”‚   def wrapper() -> T:                                                                    â”‚\r\nbackend-api  | â”‚   580 â”‚   â”‚   try:                                                                               â”‚\r\nbackend-api  | â”‚ â± 581 â”‚   â”‚   â”‚   return func(*args, **kwargs)                                                   â”‚\r\nbackend-api  | â”‚   582 â”‚   â”‚   except StopIteration as exc:                                                       â”‚\r\nbackend-api  | â”‚   583 â”‚   â”‚   â”‚   # StopIteration can't be set on an asyncio.Future                              â”‚\r\nbackend-api  | â”‚   584 â”‚   â”‚   â”‚   # it raises a TypeError and leaves the Future pending forever                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /app/core/quivr_core/quivr_rag_langgraph.py:199 in retrieve                                      â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   196 â”‚   â”‚   â”‚   dict: The retrieved chunks                                                     â”‚\r\nbackend-api  | â”‚   197 â”‚   â”‚   \"\"\"                                                                                â”‚\r\nbackend-api  | â”‚   198 â”‚   â”‚   question = state[\"messages\"][-1].content                                           â”‚\r\nbackend-api  | â”‚ â± 199 â”‚   â”‚   docs = self.compression_retriever.invoke(question)                                 â”‚\r\nbackend-api  | â”‚   200 â”‚   â”‚   return {\"docs\": docs}                                                              â”‚\r\nbackend-api  | â”‚   201 â”‚                                                                                          â”‚\r\nbackend-api  | â”‚   202 â”‚   def generate_rag(self, state):                                                         â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/retrievers.py:252 in invoke               â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   249 â”‚   â”‚   â”‚   â”‚   result = self._get_relevant_documents(input, **_kwargs)                    â”‚\r\nbackend-api  | â”‚   250 â”‚   â”‚   except Exception as e:                                                             â”‚\r\nbackend-api  | â”‚   251 â”‚   â”‚   â”‚   run_manager.on_retriever_error(e)                                              â”‚\r\nbackend-api  | â”‚ â± 252 â”‚   â”‚   â”‚   raise e                                                                        â”‚\r\nbackend-api  | â”‚   253 â”‚   â”‚   else:                                                                              â”‚\r\nbackend-api  | â”‚   254 â”‚   â”‚   â”‚   run_manager.on_retriever_end(                                                  â”‚\r\nbackend-api  | â”‚   255 â”‚   â”‚   â”‚   â”‚   result,                                                                    â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_core/retrievers.py:245 in invoke               â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   242 â”‚   â”‚   try:                                                                               â”‚\r\nbackend-api  | â”‚   243 â”‚   â”‚   â”‚   _kwargs = kwargs if self._expects_other_args else {}                           â”‚\r\nbackend-api  | â”‚   244 â”‚   â”‚   â”‚   if self._new_arg_supported:                                                    â”‚\r\nbackend-api  | â”‚ â± 245 â”‚   â”‚   â”‚   â”‚   result = self._get_relevant_documents(                                     â”‚\r\nbackend-api  | â”‚   246 â”‚   â”‚   â”‚   â”‚   â”‚   input, run_manager=run_manager, **_kwargs                              â”‚\r\nbackend-api  | â”‚   247 â”‚   â”‚   â”‚   â”‚   )                                                                          â”‚\r\nbackend-api  | â”‚   248 â”‚   â”‚   â”‚   else:                                                                          â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain/retrievers/contextual_compression.py:46 in     â”‚\r\nbackend-api  | â”‚ _get_relevant_documents                                                                          â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   43 â”‚   â”‚   â”‚   query, config={\"callbacks\": run_manager.get_child()}, **kwargs                  â”‚\r\nbackend-api  | â”‚   44 â”‚   â”‚   )                                                                                   â”‚\r\nbackend-api  | â”‚   45 â”‚   â”‚   if docs:                                                                            â”‚\r\nbackend-api  | â”‚ â± 46 â”‚   â”‚   â”‚   compressed_docs = self.base_compressor.compress_documents(                      â”‚\r\nbackend-api  | â”‚   47 â”‚   â”‚   â”‚   â”‚   docs, query, callbacks=run_manager.get_child()                              â”‚\r\nbackend-api  | â”‚   48 â”‚   â”‚   â”‚   )                                                                               â”‚\r\nbackend-api  | â”‚   49 â”‚   â”‚   â”‚   return list(compressed_docs)                                                    â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_cohere/rerank.py:121 in compress_documents     â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   118 â”‚   â”‚   â”‚   A sequence of compressed documents.                                            â”‚\r\nbackend-api  | â”‚   119 â”‚   â”‚   \"\"\"                                                                                â”‚\r\nbackend-api  | â”‚   120 â”‚   â”‚   compressed = []                                                                    â”‚\r\nbackend-api  | â”‚ â± 121 â”‚   â”‚   for res in self.rerank(documents, query):                                          â”‚\r\nbackend-api  | â”‚   122 â”‚   â”‚   â”‚   doc = documents[res[\"index\"]]                                                  â”‚\r\nbackend-api  | â”‚   123 â”‚   â”‚   â”‚   doc_copy = Document(doc.page_content, metadata=deepcopy(doc.metadata))         â”‚\r\nbackend-api  | â”‚   124 â”‚   â”‚   â”‚   doc_copy.metadata[\"relevance_score\"] = res[\"relevance_score\"]                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/langchain_cohere/rerank.py:88 in rerank                  â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚    85 â”‚   â”‚   ]                                                                                  â”‚\r\nbackend-api  | â”‚    86 â”‚   â”‚   model = model or self.model                                                        â”‚\r\nbackend-api  | â”‚    87 â”‚   â”‚   top_n = top_n if (top_n is None or top_n > 0) else self.top_n                      â”‚\r\nbackend-api  | â”‚ â±  88 â”‚   â”‚   results = self.client.rerank(                                                      â”‚\r\nbackend-api  | â”‚    89 â”‚   â”‚   â”‚   query=query,                                                                   â”‚\r\nbackend-api  | â”‚    90 â”‚   â”‚   â”‚   documents=docs,                                                                â”‚\r\nbackend-api  | â”‚    91 â”‚   â”‚   â”‚   model=model,                                                                   â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚ /usr/local/lib/python3.11/site-packages/cohere/base_client.py:1599 in rerank                     â”‚\r\nbackend-api  | â”‚                                                                                                  â”‚\r\nbackend-api  | â”‚   1596 â”‚   â”‚   â”‚   â”‚   â”‚   typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_re  â”‚\r\nbackend-api  | â”‚   1597 â”‚   â”‚   â”‚   â”‚   )                                                                         â”‚\r\nbackend-api  | â”‚   1598 â”‚   â”‚   â”‚   if _response.status_code == 401:                                              â”‚\r\nbackend-api  | â”‚ â± 1599 â”‚   â”‚   â”‚   â”‚   raise UnauthorizedError(                                                  â”‚\r\nbackend-api  | â”‚   1600 â”‚   â”‚   â”‚   â”‚   â”‚   typing.cast(typing.Any, construct_type(type_=typing.Any, object_=_re  â”‚\r\nbackend-api  | â”‚   1601 â”‚   â”‚   â”‚   â”‚   )                                                                         â”‚\r\nbackend-api  | â”‚   1602 â”‚   â”‚   â”‚   if _response.status_code == 403:                                              â”‚\r\nbackend-api  | â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\r\nbackend-api  | UnauthorizedError: status_code: 401, body: {'message': 'invalid api token'}\r\nba\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-225/[bug]-invalid-api-token\">CORE-225 [Bug]: invalid api token</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey there, @adithep! I'm here to help you with any bugs, questions, or contributions. Let's tackle this issue together!\n\n<!-- Answer -->\nThe \"UnauthorizedError: status_code: 401, body: {'message': 'invalid api token'}\" occurs when trying to chat with a brain with a document because the authorization process involves validating the user's access to the specific brain using the `validate_brain_authorization` function. This function checks if the user has the required roles (Viewer, Editor, Owner) for the brain. If the API token is invalid or the user does not have the necessary permissions, the authorization fails, resulting in a 401 error.\n\nIn contrast, when chatting directly with the model, this specific authorization step related to the brain is bypassed, allowing the interaction to proceed without triggering the error <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/chat/controller/chat_routes.py)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "adithep",
        "body": "I am using admin user that created the brain. So I do not think that could be the problem"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3545,
    "title": "Sync: limit update",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-01-06T17:04:25Z",
    "updated_at": "2025-01-09T08:37:01Z",
    "labels": [
      "backend"
    ],
    "body": "Fetch syncs if : \n\n* users is premium\n* user is connected in the last week",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-327/sync-limit-update\">CORE-327 Sync: limit update</a></p>"
      }
    ]
  },
  {
    "issue_number": 3436,
    "title": "Improve test in quivr-core for megaparse",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-31T10:57:13Z",
    "updated_at": "2025-01-08T11:20:51Z",
    "labels": [
      "improvement"
    ],
    "body": "We could have a mock of the API using `httpx.Transport` to mock responses and errors from megaparse API. So we can tests how quivr_core respond to each scenario. (TODO: create a ticket for this) â€” Amine",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-267/improve-test-in-quivr-core-for-megaparse\">CORE-267 Improve test in quivr-core for megaparse</a></p>"
      }
    ]
  },
  {
    "issue_number": 3461,
    "title": "Add Modules Checker",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-11-07T09:53:34Z",
    "updated_at": "2025-01-08T11:20:21Z",
    "labels": [],
    "body": "At instantiation of MegaParse, verify that all the modules imbrics well (output type == next input type)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-289/add-modules-checker\">CORE-289 Add Modules Checker</a></p>"
      }
    ]
  },
  {
    "issue_number": 3546,
    "title": "Build MP on GPU CI",
    "author": "AmineDiro",
    "state": "closed",
    "created_at": "2025-01-08T09:43:36Z",
    "updated_at": "2025-01-08T11:16:24Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-328/build-mp-on-gpu-ci\">CORE-328 Build MP on GPU CI</a></p>"
      }
    ]
  },
  {
    "issue_number": 3295,
    "title": "[Bug]: Missing Input in Brainentity",
    "author": "zdkderpphb",
    "state": "closed",
    "created_at": "2024-09-30T23:41:15Z",
    "updated_at": "2025-01-05T20:05:33Z",
    "labels": [
      "bug",
      "Stale",
      "area: backend"
    ],
    "body": "### What happened?\n\nI was unable to edit a brain because of an input error. I added the line\r\n    quivr_assistant: Optional[bool] = None\r\nat the end of class BrainEntity and then it worked.\r\n\n\n### Relevant log output\n\n```bash\nbackend-api  | pydantic_core._pydantic_core.ValidationError: 1 validation error for BrainEntity\r\nbackend-api  | quivr_assistant\r\nbackend-api  |   Extra inputs are not permitted [type=extra_forbidden, input_value=False, input_type=bool]\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-219/[bug]-missing-input-in-brainentity\">CORE-219 [Bug]: Missing Input in Brainentity</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey there, @zdkderpphb! I'm here to help you with any bugs, questions, or contributions you have. Let's tackle this together!\n\n<!-- Answer -->\nA similar issue was not directly found, but it seems like the error is due to an extra input not being permitted in the `BrainEntity` class. Adding `quivr_assistant: Optional[bool] = None` to the class definition resolved the issue for you by allowing this additional input.\n\nTo properly handle this, ensure that any new attributes you add to `BrainEntity` are defined in the class to avoid validation errors. This aligns with how optional attributes are typically managed in the class <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/brain/entity/brain_entity.py)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "StanGirard",
        "body": "Hey, can you try to do a `supabase db reset` from the backend folder please ? "
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3304,
    "title": "[Bug]: Connect Google Drive doesn't work (self-hosted Quivr)",
    "author": "hex",
    "state": "closed",
    "created_at": "2024-10-02T13:14:24Z",
    "updated_at": "2025-01-05T20:05:32Z",
    "labels": [
      "bug",
      "Stale",
      "area: backend"
    ],
    "body": "### What happened?\n\nTrying to connect to Google Drive on a self hosted Quivr returns `Internal Server Error`.\n\n### Relevant log output\n\n```bash\nbackend-api  | INFO:     172.19.0.1:51808 - \"GET /healthz HTTP/1.1\" 200 OK\r\nbackend-api  | INFO:     172.19.0.1:51818 - \"OPTIONS /sync/google/authorize?name=2kgyd6ez9ykme68vej7m3 HTTP/1.1\" 200 OK\r\nbackend-api  | [DEBUG] quivr_api.modules.sync.controller.google_sync_routes [google_sync_routes.py:82]: Authorizing Google Drive sync for user: 06bc76d6-e496-488d-83cf-490628cd02ab, name : 2kgyd6ez9ykme68vej7m3\r\nbackend-api  | [INFO] quivr_api.modules.sync.controller.google_sync_routes [google_sync_routes.py:98]: Generated authorization URL: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=622670270568-l8erggglra0ngg77mkflitchih1m0eo1.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fquivr-api.erepubliklabs.com%2Fsync%2Fgoogle%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.metadata.readonly+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.readonly+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+openid&state=user_id%3D06bc76d6-e496-488d-83cf-490628cd02ab%2C+name%3D2kgyd6ez9ykme68vej7m3&access_type=offline&include_granted_scopes=true&prompt=consent for user: 06bc76d6-e496-488d-83cf-490628cd02ab\r\nbackend-api  | [INFO] quivr_api.modules.sync.repository.sync_user [sync_user.py:47]: Creating sync user with input: user_id='06bc76d6-e496-488d-83cf-490628cd02ab' name='2kgyd6ez9ykme68vej7m3' email=None provider='Google' credentials={} state={'state': 'user_id=06bc76d6-e496-488d-83cf-490628cd02ab, name=2kgyd6ez9ykme68vej7m3'} additional_data={} status='SYNCED'\r\nbackend-api  | [INFO] quivr_api.modules.sync.repository.sync_user [sync_user.py:55]: Sync user created successfully: {'id': 34, 'name': '2kgyd6ez9ykme68vej7m3', 'provider': 'Google', 'state': {'state': 'user_id=06bc76d6-e496-488d-83cf-490628cd02ab, name=2kgyd6ez9ykme68vej7m3'}, 'credentials': {}, 'user_id': '06bc76d6-e496-488d-83cf-490628cd02ab', 'email': None, 'additional_data': {}, 'status': 'SYNCED'}\r\nbackend-api  | INFO:     172.19.0.1:51818 - \"POST /sync/google/authorize?name=2kgyd6ez9ykme68vej7m3 HTTP/1.1\" 200 OK\r\nbackend-api  | [INFO] quivr_api.modules.sync.controller.google_sync_routes [google_sync_routes.py:127]: State: user_id=06bc76d6-e496-488d-83cf-490628cd02ab, name=2kgyd6ez9ykme68vej7m3\r\nbackend-api  | [DEBUG] quivr_api.modules.sync.controller.google_sync_routes [google_sync_routes.py:131]: Handling OAuth2 callback for user: 06bc76d6-e496-488d-83cf-490628cd02ab with state: user_id=06bc76d6-e496-488d-83cf-490628cd02ab, name=2kgyd6ez9ykme68vej7m3\r\nbackend-api  | [INFO] quivr_api.modules.sync.repository.sync_user [sync_user.py:124]: Getting sync user by state: {'state': 'user_id=06bc76d6-e496-488d-83cf-490628cd02ab, name=2kgyd6ez9ykme68vej7m3'}\r\nbackend-api  | [INFO] quivr_api.modules.sync.repository.sync_user [sync_user.py:131]: Sync user found by state: {'id': 34, 'name': '2kgyd6ez9ykme68vej7m3', 'provider': 'Google', 'state': {'state': 'user_id=06bc76d6-e496-488d-83cf-490628cd02ab, name=2kgyd6ez9ykme68vej7m3'}, 'credentials': {}, 'user_id': '06bc76d6-e496-488d-83cf-490628cd02ab', 'email': None, 'additional_data': {}, 'status': 'SYNCED'}\r\nbackend-api  | [INFO] quivr_api.modules.sync.controller.google_sync_routes [google_sync_routes.py:135]: Retrieved sync user state: id=34 user_id=UUID('06bc76d6-e496-488d-83cf-490628cd02ab') name='2kgyd6ez9ykme68vej7m3' email=None provider='Google' credentials={} state={'state': 'user_id=06bc76d6-e496-488d-83cf-490628cd02ab, name=2kgyd6ez9ykme68vej7m3'} additional_data={} status='SYNCED'\r\nbackend-api  | INFO:     172.19.0.1:51820 - \"GET /sync/google/oauth2callback?state=user_id%3D06bc76d6-e496-488d-83cf-490628cd02ab,%20name%3D2kgyd6ez9ykme68vej7m3&code=4/0AVG7fiTUpXwXEQBFNXv1D_tP9Y8bO_w2Fta9CWk-kNhR2xgjC-iL8NSGBSqu75YOv9r41Q&scope=email%20https://www.googleapis.com/auth/drive.metadata.readonly%20https://www.googleapis.com/auth/drive.readonly%20https://www.googleapis.com/auth/userinfo.email%20openid%20https://www.googleapis.com/auth/drive%20https://www.googleapis.com/auth/drive.metadata%20https://www.googleapis.com/auth/drive.photos.readonly%20https://www.googleapis.com/auth/drive.apps.readonly%20https://www.googleapis.com/auth/drive.appdata%20https://www.googleapis.com/auth/drive.scripts%20https://www.googleapis.com/auth/drive.file&authuser=1&hd=erepubliklabs.com&prompt=consent HTTP/1.1\" 500 Internal Server Error\r\nbackend-api  | ERROR:    Exception in ASGI application\r\nbackend-api  | Traceback (most recent call last):\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 406, in run_asgi\r\nbackend-api  |     result = await app(  # type: ignore[func-returns-value]\r\nbackend-api  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\nbackend-api  |     return await self.app(scope, receive, send)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\nbackend-api  |     await super().__call__(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\r\nbackend-api  |     await self.middleware_stack(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\nbackend-api  |     raise exc\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\nbackend-api  |     await self.app(scope, receive, _send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\nbackend-api  |     await self.app(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\nbackend-api  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\nbackend-api  |     raise exc\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\nbackend-api  |     await app(scope, receive, sender)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 754, in __call__\r\nbackend-api  |     await self.middleware_stack(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 774, in app\r\nbackend-api  |     await route.handle(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 295, in handle\r\nbackend-api  |     await self.app(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 77, in app\r\nbackend-api  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\nbackend-api  |     raise exc\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\nbackend-api  |     await app(scope, receive, sender)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\r\nbackend-api  |     response = await f(request)\r\nbackend-api  |                ^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 278, in app\r\nbackend-api  |     raw_response = await run_endpoint_function(\r\nbackend-api  |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 193, in run_endpoint_function\r\nbackend-api  |     return await run_in_threadpool(dependant.call, **values)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/concurrency.py\", line 42, in run_in_threadpool\r\nbackend-api  |     return await anyio.to_thread.run_sync(func, *args)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\nbackend-api  |     return await get_async_backend().run_sync_in_worker_thread(\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\r\nbackend-api  |     return await future\r\nbackend-api  |            ^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\r\nbackend-api  |     result = context.run(func, *args)\r\nbackend-api  |              ^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/app/api/quivr_api/modules/sync/controller/google_sync_routes.py\", line 152, in oauth2callback_google\r\nbackend-api  |     flow.fetch_token(authorization_response=str(request.url))\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/google_auth_oauthlib/flow.py\", line 285, in fetch_token\r\nbackend-api  |     return self.oauth2session.fetch_token(self.client_config[\"token_uri\"], **kwargs)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/requests_oauthlib/oauth2_session.py\", line 406, in fetch_token\r\nbackend-api  |     self._client.parse_request_body_response(r.text, scope=self.scope)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/oauthlib/oauth2/rfc6749/clients/base.py\", line 427, in parse_request_body_response\r\nbackend-api  |     self.token = parse_token_response(body, scope=scope)\r\nbackend-api  |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/oauthlib/oauth2/rfc6749/parameters.py\", line 441, in parse_token_response\r\nbackend-api  |     validate_token_parameters(params)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/oauthlib/oauth2/rfc6749/parameters.py\", line 471, in validate_token_parameters\r\nbackend-api  |     raise w\r\nbackend-api  | Warning: Scope has changed from \"https://www.googleapis.com/auth/drive.metadata.readonly https://www.googleapis.com/auth/userinfo.email openid https://www.googleapis.com/auth/drive.readonly\" to \"https://www.googleapis.com/auth/drive.metadata.readonly https://www.googleapis.com/auth/userinfo.email openid https://www.googleapis.com/auth/drive https://www.googleapis.com/auth/drive.photos.readonly https://www.googleapis.com/auth/drive.scripts https://www.googleapis.com/auth/drive.file https://www.googleapis.com/auth/drive.readonly https://www.googleapis.com/auth/drive.apps.readonly https://www.googleapis.com/auth/drive.metadata https://www.googleapis.com/auth/drive.appdata\".\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-222/[bug]-connect-google-drive-doesnt-work-self-hosted-quivr\">CORE-222 [Bug]: Connect Google Drive doesn't work (self-hosted Quivr)</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey @hex! I'm here to help you with any bugs, questions, or becoming a contributor. Let's squash those bugs together! ğŸ¤–\n\n<!-- Answer -->\nThe \"Internal Server Error\" when connecting Google Drive on a self-hosted Quivr could be due to several reasons:\n\n1. **Environment Variables**: Ensure that all necessary environment variables for Google OAuth, such as `GOOGLE_CLIENT_ID`, `GOOGLE_CLIENT_SECRET`, and others, are correctly set.\n\n2. **State Parameter**: The state parameter must be correctly managed and passed between requests. Ensure that it is valid and matches the expected state.\n\n3. **User Validation**: Check that the `user_id` from the state matches the `current_user`. Any mismatch will cause an error.\n\n4. **Token Fetching**: The `flow.fetch_token` method requires a valid authorization response. Ensure that the authorization response is correct and that the token fetching process is successful.\n\n5. **Service Initialization**: Verify that the credentials are valid and that the service can be initialized without issues.\n\nAdditionally, the log indicates a warning about the scope change. Ensure that the requested scopes are correctly configured in the Google Cloud Console and match the scopes used in your application <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/sync/controller/google_sync_routes.py)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3133,
    "title": "Enable filtering on extracted document metadata",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-03T09:16:24Z",
    "updated_at": "2025-01-03T16:06:41Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-184/enable-filtering-on-extracted-document-metadata\">CORE-184 Enable filtering on extracted document metadata</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3135,
    "title": "Improving user experience in long conversations",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-03T09:20:56Z",
    "updated_at": "2025-01-03T16:06:40Z",
    "labels": [
      "Stale"
    ],
    "body": "See [GEN-11](https://linear.app/getquivr/issue/GEN-11/follow-up-questions-lose-context-of-the-thread)\n\nExisting solutions (memory):\n\n* [https://github.com/mem0ai/mem0](https://github.com/mem0ai/mem0) (can be [integrated with LangGraph](https://docs.mem0.ai/integrations/langgraph))\n* [https://github.com/kingjulio8238/Memary](https://github.com/kingjulio8238/Memary)\n* [https://github.com/OSU-NLP-Group/HippoRAG](https://github.com/OSU-NLP-Group/HippoRAG)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-186/improving-user-experience-in-long-conversations\">CORE-186 Improving user experience in long conversations</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3287,
    "title": "[Bug]: Can't add url to brain",
    "author": "wonderu",
    "state": "closed",
    "created_at": "2024-09-30T14:07:59Z",
    "updated_at": "2025-01-03T16:06:39Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### What happened?\n\n1. Open brain settings\r\n2. Add https://microsoft.com\r\n3. Press \"Feed Brain\" button.\r\nGot an red error in the right bottom corner\r\n\n\n### Relevant log output\n\n```bash\nFailed to crawl website: {\"message\":\"Network Error\",\"name\":\"AxiosError\",\"stack\":\"AxiosError: Network Error\\n    at c.onerror (https://chat.quivr.app/_next/static/chunks/511-3735812c78bc81f6.js:6:101022)\\n    at tU.request (https://chat.quivr.app/_next/static/chunks/511-3735812c78bc81f6.js:6:105947)\\n    at async https://chat.quivr.app/_next/static/chunks/app/studio/%5BbrainId%5D/page-ae50d52db736d04d.js:1:57688\",\"config\":{\"transitional\":{\"silentJSONParsing\":true,\"forcedJSONParsing\":true,\"clarifyTimeoutError\":false},\"adapter\":[\"xhr\",\"http\"],\"transformRequest\":[null],\"transformResponse\":[null],\"timeout\":0,\"xsrfCookieName\":\"XSRF-TOKEN\",\"xsrfHeaderName\":\"X-XSRF-TOKEN\",\"maxContentLength\":-1,\"maxBodyLength\":-1,\"env\":{},\"headers\":{\"Accept\":\"application/json, text/plain, */*\",\"Content-Type\":\"application/json\",\"Authorization\":\"Bearer XXXX\"},\"baseURL\":\"https://api.quivr.app\",\"method\":\"post\",\"url\":\"/crawl?bulk_id=XXX&brain_id=XXX\",\"data\":\"{\\\"url\\\":\\\"https://microsoft.com\\\",\\\"js\\\":false,\\\"depth\\\":1,\\\"max_pages\\\":100,\\\"max_time\\\":60}\"},\"code\":\"ERR_NETWORK\",\"status\":null}\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-214/[bug]-cant-add-url-to-brain\">CORE-214 [Bug]: Can't add url to brain</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nI found some similar issues that might help resolve your problem:\n\n1. **[Bug]:** <sup>[[1]](https://github.com/QuivrHQ/quivr/issues/1413)</sup> - This issue was related to network errors and was resolved by updating the frontend `.env` file to use the local IP instead of `localhost`. Additionally, ensuring that port 5050 is open in the firewall was suggested. More details can be found [here](https://github.com/StanGirard/quivr/issues/961).\n\n2. **Network Error:** <sup>[[2]](https://github.com/QuivrHQ/quivr/issues/282)</sup> - This issue was resolved by changing the `NEXT_PUBLIC_BACKEND_URL` to the user's own IP address and re-running `docker compose`.\n\n3. **Failed to upload file:** <sup>[[3]](https://github.com/QuivrHQ/quivr/issues/202)</sup> - This issue was due to a network error on the user's side and was resolved by retrying after some time.\n\nTo resolve the \"Network Error\" when trying to add a URL to the brain in the Quivr app, you should ensure that the URL extraction process is functioning correctly. The `process_url_func` function in `process_url.py` handles the URL processing. Here are some steps to troubleshoot and resolve the issue:\n\n1. **Check URL Validity**: Ensure the URL you are trying to add is valid and accessible.\n2. **Network Connectivity**: Verify that there are no network connectivity issues that might be causing the error.\n3. **CORS Configuration**: Ensure that the CORS (Cross-Origin Resource Sharing) settings are correctly configured. The `add_cors_middleware` function in `cors.py` sets up the CORS middleware, allowing specific origins. Make sure the origin of your request is included in the `origins` list.\n\nHere is the relevant part of the code that processes the URL:\n\n```python\nasync def process_url_func(\n    url: str,\n    brain_id: UUID,\n    knowledge_id: UUID,\n    brain_service: BrainService,\n    vector_service: VectorService,\n):\n    crawl_website = URL(url=url)\n    extracted_content = await extract_from_url(crawl_website)\n    extracted_content_bytes = extracted_content.encode(\"utf-8\")\n    file_name = slugify(crawl_website.url) + \".txt\"\n\n    brain = brain_service.get_brain_by_id(brain_id)\n    if brain is None:\n        logger.error(\"It seems like you're uploading knowledge to an unknown brain.\")\n        return 1\n\n    with build_file(extracted_content_bytes, knowledge_id, file_name) as file_instance:\n        await process_file(\n            file_instance=file_instance,\n            brain=brain,\n            brain_service=brain_service,\n            integration=None,\n            integration_link=None,\n            vector_service=vector_service,\n        )\n```\n\nEnsure that the `extract_from_url` function is able to fetch and extract content from the provided URL. If there are issues with this function, it could lead to a \"Network Error\" <sup>[[4]](https://github.com/QuivrHQ/quivr/blob/main/backend/worker/quivr_worker/process/process_url.py)</sup>.\n\nAdditionally, here is the CORS middleware configuration:\n\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\n\norigins = [\n    \"http://localhost\",\n    \"http://localhost:3000\",\n    \"http://localhost:3001\",\n    \"https://quivr.app\",\n    \"https://www.quivr.app\",\n    \"http://quivr.app\",\n    \"http://www.quivr.app\",\n    \"https://chat.quivr.app\",\n    \"*\",\n]\n\ndef add_cors_middleware(app):\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=origins,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n```\n\nMake sure that the origin of your request is included in the `origins` list to avoid CORS-related issues <sup>[[5]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/middlewares/cors.py)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3288,
    "title": "Stalled queries to backend",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-30T15:29:13Z",
    "updated_at": "2025-01-03T16:06:38Z",
    "labels": [
      "backend",
      "Stale",
      "area: backend"
    ],
    "body": "<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/da9000da-0cae-454c-aa88-392e34ff3d81/9d85202d-ac64-44a6-95eb-e0d5f502ba06?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC9kYTkwMDBkYS0wY2FlLTQ1NGMtYWE4OC0zOTJlMzRmZjNkODEvOWQ4NTIwMmQtYWM2NC00NGE2LTk1ZWItZTBkNWY1MDJiYTA2IiwiaWF0IjoxNzI3NzEwMTUyLCJleHAiOjMzMjk4MjcwMTUyfQ.IpDCVjI20FonDZW0IVS94zEphoOmp2lN6-jPJ0aoVm8 \" alt=\"image.png\" width=\"1908\" height=\"754\" />",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-215/stalled-queries-to-backend\">CORE-215 Stalled queries to backend</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3189,
    "title": "[Backend] Preview File",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-11T09:12:23Z",
    "updated_at": "2025-01-03T12:09:24Z",
    "labels": [
      "Stale",
      "area: backend"
    ],
    "body": "* Send generate url in get knowledge response ? -> Check if you can send a thumbnail ? \n* Syncs already have url send it to frontend",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-74/[backend]-preview-file\">ENT-74 [Backend] Preview File</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3222,
    "title": "Drop tables & Migrations",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-18T14:17:49Z",
    "updated_at": "2025-01-03T12:09:23Z",
    "labels": [
      "Stale",
      "area: scripts"
    ],
    "body": "Drop tables\n\n* sync_file\n* sync_active\n* brain_vectors ? \n\nMigrate knowledges: \n\n* Migrate storage\n* Update knowledge table\n\nMigrate syncs  @stan ? ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-83/drop-tables\">ENT-83 Drop tables</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3369,
    "title": "Production and Preview performance",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-10-14T14:03:53Z",
    "updated_at": "2024-12-31T08:37:37Z",
    "labels": [],
    "body": "* API very slow\n\n\\-",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-235/production-performance\">CORE-235 Production performance</a></p>"
      }
    ]
  },
  {
    "issue_number": 3270,
    "title": "Serialization performance get knowledge brain",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-26T20:07:35Z",
    "updated_at": "2024-12-31T00:24:55Z",
    "labels": [
      "enhancement",
      "Stale",
      "area: backend"
    ],
    "body": "<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/e2653651-4b66-4944-b79e-1dc2bd17a8d6/33474928-8cb0-465e-9de9-71c16861c229?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC9lMjY1MzY1MS00YjY2LTQ5NDQtYjc5ZS0xZGMyYmQxN2E4ZDYvMzM0NzQ5MjgtOGNiMC00NjVlLTlkZTktNzFjMTY4NjFjMjI5IiwiaWF0IjoxNzI3MzgxMjU0LCJleHAiOjMzMjk3OTQxMjU0fQ.eFAsw3X5MX1wfaH9RElqIJE2j9qnHcdvoug0XDT6Jow \" alt=\"image.png\" width=\"1419\" height=\"593\" />\n\ntest with 406 knowledges takes \\~2s to load. this is too slow\n\nLooking in profiling, serialization takes a lot of time. We are using pydantic v2 I think. I dont see how iti is slow",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-105/serialization-performance\">ENT-105 Serialization performance</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3440,
    "title": "Removing Pydantic V1 dependencies",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-10-31T14:59:39Z",
    "updated_at": "2024-12-30T10:37:36Z",
    "labels": [],
    "body": "Now that LangChain moved to Pydantic V2, we can clean up our codebase from any remaining Pydantic V1 dependency",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-271/removing-pydantic-v1-dependencies\">CORE-271 Removing Pydantic V1 dependencies</a></p>"
      }
    ]
  },
  {
    "issue_number": 3192,
    "title": "Remove knowledge from brain",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-11T12:53:40Z",
    "updated_at": "2024-12-30T00:26:22Z",
    "labels": [
      "Stale",
      "area: backend"
    ],
    "body": "* shouldn't remove knowledge from the KMS\n* if KM is folder remove all subsequent children from brain\n* Check what rules apply for the syncs vs local knowledge",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-77/[backend]-remove-knowledge-from-brain\">ENT-77 [backend] Remove knowledge from brain</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3233,
    "title": "Optimize get_knowledge_tree",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-20T13:44:15Z",
    "updated_at": "2024-12-29T20:05:30Z",
    "labels": [
      "Stale",
      "area: backend",
      "improvement"
    ],
    "body": "Current generate query: \n\n```\nEXPLAIN ANALYZE\nWITH RECURSIVE knowledge_tree(id, file_name, url, extension, status, SOURCE, source_link, file_size, file_sha1, created_at, updated_at, metadata, is_folder, user_id, parent_id, sync_id, sync_file_id) AS\n  (SELECT knowledge_1.id AS id,\n          knowledge_1.file_name AS file_name,\n          knowledge_1.url AS url,\n          knowledge_1.extension AS extension,\n          knowledge_1.status AS status,\n          knowledge_1.source AS SOURCE,\n          knowledge_1.source_link AS source_link,\n          knowledge_1.file_size AS file_size,\n          knowledge_1.file_sha1 AS file_sha1,\n          knowledge_1.created_at AS created_at,\n          knowledge_1.updated_at AS updated_at,\n          knowledge_1.metadata AS metadata,\n          knowledge_1.is_folder AS is_folder,\n          knowledge_1.user_id AS user_id,\n          knowledge_1.parent_id AS parent_id,\n          knowledge_1.sync_id AS sync_id,\n          knowledge_1.sync_file_id AS sync_file_id\n   FROM knowledge AS knowledge_1\n   WHERE knowledge_1.parent_id = '40ba47d7-51b2-4b2a-9247-89e29619efb0'::UUID\n   UNION ALL SELECT knowledge_2.id AS id,\n                    knowledge_2.file_name AS file_name,\n                    knowledge_2.url AS url,\n                    knowledge_2.extension AS extension,\n                    knowledge_2.status AS status,\n                    knowledge_2.source AS SOURCE,\n                    knowledge_2.source_link AS source_link,\n                    knowledge_2.file_size AS file_size,\n                    knowledge_2.file_sha1 AS file_sha1,\n                    knowledge_2.created_at AS created_at,\n                    knowledge_2.updated_at AS updated_at,\n                    knowledge_2.metadata AS metadata,\n                    knowledge_2.is_folder AS is_folder,\n                    knowledge_2.user_id AS user_id,\n                    knowledge_2.parent_id AS parent_id,\n                    knowledge_2.sync_id AS sync_id,\n                    knowledge_2.sync_file_id AS sync_file_id\n   FROM knowledge AS knowledge_2\n   JOIN knowledge_tree ON knowledge_2.parent_id = knowledge_tree.id)\nSELECT knowledge.id,\n       knowledge.file_name,\n       knowledge.url,\n       knowledge.extension,\n       knowledge.status,\n       knowledge.source,\n       knowledge.source_link,\n       knowledge.file_size,\n       knowledge.file_sha1,\n       knowledge.created_at,\n       knowledge.updated_at,\n       knowledge.metadata,\n       knowledge.is_folder,\n       knowledge.user_id,\n       knowledge.parent_id,\n       knowledge.sync_id,\n       knowledge.sync_file_id,\n       prompts_1.id AS id_1,\n       prompts_1.content,\n       prompts_1.title,\n       prompts_1.status AS status_1,\n       brains_1.brain_id,\n       brains_1.name,\n       brains_1.description,\n       brains_1.status AS status_2,\n       brains_1.model,\n       brains_1.max_tokens,\n       brains_1.temperature,\n       brains_1.last_update,\n       brains_1.brain_type,\n       brains_1.prompt_id,\n       syncs_1.id AS id_2,\n       syncs_1.name AS name_1,\n       syncs_1.provider,\n       syncs_1.email,\n       syncs_1.user_id AS user_id_1,\n       syncs_1.credentials,\n       syncs_1.state,\n       syncs_1.created_at AS created_at_1,\n       syncs_1.updated_at AS updated_at_1,\n       syncs_1.last_synced_at,\n       syncs_1.additional_data\nFROM knowledge\nJOIN knowledge_tree ON knowledge.id = knowledge_tree.id\nLEFT OUTER JOIN (knowledge_brain AS knowledge_brain_1\n                 JOIN brains AS brains_1 ON brains_1.brain_id = knowledge_brain_1.brain_id) ON knowledge.id = knowledge_brain_1.knowledge_id\nLEFT OUTER JOIN prompts AS prompts_1 ON prompts_1.id = brains_1.prompt_id\nLEFT OUTER JOIN syncs AS syncs_1 ON syncs_1.id = knowledge.sync_id\n```\n\nShould be simpler: \n\n* Remove additional join btw knowledge & knowledge_tree \n\nPrevious implementation\n\n```\nWITH RECURSIVE knowledge_tree AS (\n    SELECT *\n    FROM knowledge\n    WHERE parent_id = :parent_id\n    UNION ALL\n    SELECT k.*\n    FROM knowledge k\n    JOIN knowledge_tree kt ON k.parent_id = kt.id\n)\nSELECT * FROM knowledge_tree\n```",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-88/optimize-get-knowledge-tree\">ENT-88 Optimize get_knowledge_tree</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3250,
    "title": "Enable flexible way to pass different prompts to different workflow nodes",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-09-24T09:18:03Z",
    "updated_at": "2024-12-28T16:05:39Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "We need to be able to easily pass different prompts / prompt IDs to our LangGraph nodes, for instance via a metadata dictionary, then use the prompt service to collect them",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-209/enable-flexible-way-to-pass-different-prompts-to-different-workflow\">CORE-209 Enable flexible way to pass different prompts to different workflow nodes</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3234,
    "title": "Rewrite celery process_file_task",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-20T14:19:26Z",
    "updated_at": "2024-12-28T12:08:17Z",
    "labels": [
      "Stale",
      "area: backend"
    ],
    "body": "* Should process a knowledge instance (syncs or local)\n* Takes a KnowledgeDTO Object\n  * If Local \n    * Build file -> downloads from S3 using SupabaseS3Storage(knowledge_id) \n    * Skip \n  * If Sjync:\n    * Recursively get children\n    * Build file -> Uses SyncUtils to download the file\n    * Insert children to DB using KnowledgeService\n    * Return Vec<ProcessableKnowledge\n* For file in Vec<ProcessableKnowledge>:\n  * Process the files \n  * Save chunks\n  * Update knowledge status to PROCESSED",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-89/refactor-celery-process-file-task\">ENT-89 Refactor celery process_file_task</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3010,
    "title": "[Feature]: JSON Document loader",
    "author": "atulgrover",
    "state": "closed",
    "created_at": "2024-08-15T06:57:11Z",
    "updated_at": "2024-12-27T20:06:06Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "### The Feature\r\n\r\nPlease add JSON document parsing and uploading into SUPABASE feature... JSON enables metadata to be attached to each chunk of text (as compared to each document). \r\n\r\nhttps://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/json/\r\n\r\n### Motivation, pitch\r\n\r\nAdding JSON document support to QUIVR could be a game-changer and enable\r\n\r\n1. **Versatile Data Integration:** JSON is increasingly becoming the preferred format for data interchange across various platforms and APIs. By adding JSON support, your RAG application can seamlessly integrate with a broader range of data sources, from web APIs to internal data pipelines, enabling richer, more diverse information retrieval.\r\n\r\n2. **Granular Metadata Control:** JSON's hierarchical structure allows for attaching metadata not just to entire documents but to individual data chunks. This granular control enables more precise indexing and retrieval, enhancing the relevance and accuracy of the generated content.\r\n\r\n3. **Enhanced Search and Filtering:** With JSON, one could store complex, nested data that can be easily parsed and queried. This means that QUIVR can support more sophisticated search and filtering capabilities, allowing users to retrieve exactly what they need, faster and with greater precision. \r\n\r\n4. **Future-Proofing:** As data structures evolve, JSON's flexibility ensures your application remains adaptable. Adding JSON support today sets the stage for easily accommodating future data formats and structures, keeping QUIVR ahead of the curve.\r\n\r\n5. **Improved User Experience:** Users benefit from more accurate, contextually relevant results, and the ability to interact with data in a more intuitive way. JSON's structure supports this by enabling a more organized and efficient handling of complex datasets.\r\n\r\n### Twitter / LinkedIn details\r\n\r\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-149/[feature]-json-document-loader\">CORE-149 [Feature]: JSON Document loader</a></p>"
      },
      {
        "user": "remoteblue",
        "body": "+1. Is there any temporary measure to digest JSON while this feature is not available?"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3240,
    "title": "Slow brain creation",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-21T17:59:45Z",
    "updated_at": "2024-12-26T00:24:49Z",
    "labels": [
      "Stale",
      "area: backend"
    ],
    "body": "[https://quivr-brain.sentry.io/performance/trace/f1030ff6332442f6955c8656ba2eac49/?environment=production&fov=165.7060546875%2C415.5721664428711&node=span-bc2495be04ccfb79&node=txn-75fb16b04606492f8dfa60c5989c3b3e&project=4505455607808000&query=http.method%3APOST&referrer=performance-transaction-summary&source=performance_transaction_summary&statsPeriod=7d&timestamp=1726635285&transaction=%2Fbrains%2F&unselectedSeries=p100%28%29&unselectedSeries=avg%28%29](https://quivr-brain.sentry.io/performance/trace/f1030ff6332442f6955c8656ba2eac49/?environment=production&fov=165.7060546875%2C415.5721664428711&node=span-bc2495be04ccfb79&node=txn-75fb16b04606492f8dfa60c5989c3b3e&project=4505455607808000&query=http.method%3APOST&referrer=performance-transaction-summary&source=performance_transaction_summary&statsPeriod=7d&timestamp=1726635285&transaction=%2Fbrains%2F&unselectedSeries=p100%28%29&unselectedSeries=avg%28%29)\n\n* Move openai meaning embedding to asynchronous operation or remove it",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-93/slow-brain-creation\">ENT-93 Slow brain creation</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3236,
    "title": "[Feature]: Uploaded knowledge table to have date/time as another column along with name/status/action",
    "author": "ChatIBC",
    "state": "closed",
    "created_at": "2024-09-20T17:56:59Z",
    "updated_at": "2024-12-25T00:24:49Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "### The Feature\n\n![image](https://github.com/user-attachments/assets/a6e387e7-d926-4f4a-bb1c-71fdfaafcf6b)\r\n\n\n### Motivation, pitch\n\nIt is easy to loose sight of what all has been uploaded and what remains to be uploaded (the counter stops at 1000) ....if there is a date/time column that can show the last file uploaded will help keep track of the files..... \n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-206/[feature]-uploaded-knowledge-table-to-have-datetime-as-another-column\">CORE-206 [Feature]: Uploaded knowledge table to have date/time as another column along with name/status/action</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3344,
    "title": "Add ColPali on Diff Assistant",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-08T08:48:10Z",
    "updated_at": "2024-12-23T10:27:50Z",
    "labels": [],
    "body": "Investigate and implementation of the usage of ColPali for Diff Assistant : <br>[https://danielvanstrien.xyz/posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html](https://danielvanstrien.xyz/posts/post-with-code/colpali-qdrant/2024-10-02_using_colpali_with_qdrant.html)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-229/add-colipali-on-diff-assitant\">CORE-229 Add ColiPali on Diff assitant</a></p>"
      }
    ]
  },
  {
    "issue_number": 2691,
    "title": "[Bug]: supabase start fails with 502 error",
    "author": "chtanch",
    "state": "closed",
    "created_at": "2024-06-19T15:36:59Z",
    "updated_at": "2024-12-21T16:05:37Z",
    "labels": [
      "bug",
      "Stale",
      "area: scripts"
    ],
    "body": "### What happened?\n\nI am trying to set up Quivr to run locally with ollama. The .env file is updated with a fake OPENAI_API_KEY and OLLAMA_API_BASE_URL.\r\n\r\nAfter running `supabase start`, I get an Error Status 502.\r\n\r\nI am running behind a corporate firewall. The environment variables `http_proxy` and `https_proxy` have been set, and docker daemon+client have been configured to use the proxies.\n\n### Relevant log output\n\n```bash\nSkipping migration local_20240107152745_ollama.sql... (file name must match pattern \"<timestamp>_name.sql\")\r\n                                                                                                           Applying migration 20240103173626_init.sql...\r\nApplying migration 20240103175048_prod.sql...\r\n                                             Applying migration 20240103181249_premium.sql...\r\n                                                                                             Applying migration 20240103181925_cleanup.sql...\r\nApplying migration 20240103193921_stripe_customers.sql...\r\n                                                         Applying migration 20240103194255_api.sql...\r\n                                                                                                     Applying migration 20240103204741_product_to_features.sql...\r\n                                                                                                                                                                 Applying migration 20240103231656_product.sql...\r\n                            Applying migration 20240103234423_models.sql...\r\n                                                                           Applying migration 20240107231636_policies.sql...\r\n                                                                                                                            Applying migration 20240119070124_search.sql...\r\n                                                                                                                                                                           Applying migration 20240119222036_metadata.sql...\r\n                                       Applying migration 20240120004107_tags.sql...\r\n                                                                                    Applying migration 20240121195523_fix-public.sql...\r\n                                                                                                                                       Applying migration 20240122194117_monthly-credit.sql...\r\n         Applying migration 20240125230346_raw.sql...\r\nApplying migration 20240206040636_notion.sql...\r\n                                               Applying migration 20240207034043_related.sql...\r\n                                                                                               Applying migration 20240207071108_chunk.sql...\r\n                                                                                                                                             Applying migration 20240216192826_integration.sql...\r\n            Applying migration 20240228182948_notion.sql...\r\n                                                           Applying migration 20240304223646_integrations-all.sql...\r\nApplying migration 20240305225452_tags-integration.sql...\r\n                                                         Applying migration 20240306013910_allow_model_change.sql...\r\n                                                                                                                    Applying migration 20240306205133_integration_display_name.sql...\r\n                                                                                                                                                                                   AApplying migration 20240313024244_onboarding-user.sql...\r\n                                                       Applying migration 20240314005817_user_identity_company_info.sql...\r\n                                                                                                                          Applying migration 20240316195514_usage_purpose.sql...\r\n                                                                                                                                                                                Applying migration 20240318024425_rename_feedback_to_thumbs.sql...\r\n                                                             Applying migration 20240320215813_fix_match_vector_function.sql...\r\n                                                                                                                               Applying migration 20240329212126_onboarding_brain.sql...\r\n   Applying migration 20240330233534_ingestion.sql...\r\n                                                     Applying migration 20240402005455_ingestion.sql...\r\n                                                                                                       Applying migration 20240402013303_ingestion_url.sql...\r\n                                                                                                                                                             Applying migration 20240402015128_seed-ingestions.sql...\r\n                                Applying migration 20240410112108_assistant.sql...\r\n                                                                                  Applying migration 20240501180719_notifications.sql...\r\nApplying migration 20240506150059_timestampz.sql...\r\n                                                   Applying migration 20240514080520_rls_optim.sql...\r\n                                                                                                     Applying migration 20240516143634_syncs.sql...\r\n                                                                                                                                                   Applying migration 20240521144817_syncs-files.sql...\r\n                  Applying migration 20240529132812_syncs-emails.sql...\r\n                                                                       Applying migration 20240606170930_sync-reduce-time.sql...\r\n                                                                                                                                Applying migration 20240608095352_supported-sync.sql...\r\n  Applying migration 20240610141546_force-sync.sql...\r\n                                                     Seeding data supabase/seed.sql...\r\nsupabase_edge_runtime_secondbrain container logs:\r\n                                                 Download https://deno.land/std/http/status.ts\r\n                                                                                              Download https://deno.land/x/jose@v4.13.1/index.ts\r\n                                                                                                                                                Stopping containers...\r\nError status 502:\r\nTry rerunning the command with --debug to troubleshoot the error.\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "chtanch",
        "body": "On running `supabase start --debug`\r\n\r\nError log (only the last parts, since the whole log is very long)\r\n```\r\n2024/06/19 23:36:14 Sent Header: User-Agent [Go-http-client/1.1]\r\n                                                                                                                       2024/06/19 23:36:14 Sent Header: Apikey [eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0]\r\n                                                                                                                                      2024/06/19 23:36:14 Send Done\r\n                                                                                                                                                                   2024/06/19 23:36:14 Recv First Byte\r\n2024/06/19 23:36:15 Sent Header: Host [127.0.0.1:54321]\r\n                                                       2024/06/19 23:36:15 Sent Header: User-Agent [Go-http-client/1.1]\r\n                                                                                                                       2024/06/19 23:36:15 Sent Header: Apikey [eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0]\r\n                                                                                                                                      2024/06/19 23:36:15 Send Done\r\n                                                                                                                                                                   2024/06/19 23:36:15 Recv First Byte\r\n2024/06/19 23:36:16 Sent Header: Host [127.0.0.1:54321]\r\n                                                       2024/06/19 23:36:16 Sent Header: User-Agent [Go-http-client/1.1]\r\n                                                                                                                       2024/06/19 23:36:16 Sent Header: Apikey [eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0]\r\n                                                                                                                                      2024/06/19 23:36:16 Send Done\r\n                                                                                                                                                                   2024/06/19 23:36:16 Recv First Byte\r\n2024/06/19 23:36:17 Sent Header: Host [127.0.0.1:54321]\r\n                                                       2024/06/19 23:36:17 Sent Header: User-Agent [Go-http-client/1.1]\r\n                                                                                                                       2024/06/19 23:36:17 Sent Header: Apikey [eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0]\r\n                                                                                                                                      2024/06/19 23:36:17 Send Done\r\n                                                                                                                                                                   2024/06/19 23:36:17 Recv First Byte\r\n2024/06/19 23:36:18 Sent Header: Host [127.0.0.1:54321]\r\n                                                       2024/06/19 23:36:18 Sent Header: User-Agent [Go-http-client/1.1]\r\n                                                                                                                       2024/06/19 23:36:18 Sent Header: Apikey [eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0]\r\n                                                                                                                                      2024/06/19 23:36:18 Send Done\r\n                                                                                                                                                                   2024/06/19 23:36:18 Recv First Byte\r\n2024/06/19 23:36:19 Sent Header: Host [127.0.0.1:54321]\r\n                                                       2024/06/19 23:36:19 Sent Header: User-Agent [Go-http-client/1.1]\r\n                                                                                                                       2024/06/19 23:36:19 Sent Header: Apikey [eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0]\r\n                                                                                                                                      2024/06/19 23:36:19 Send Done\r\n                                                                                                                                                                   2024/06/19 23:36:19 Recv First Byte\r\n2024/06/19 23:36:20 Sent Header: Host [127.0.0.1:54321]\r\n                                                       2024/06/19 23:36:20 Sent Header: User-Agent [Go-http-client/1.1]\r\n                                                                                                                       2024/06/19 23:36:20 Sent Header: Apikey [eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0]\r\n                                                                                                                                      2024/06/19 23:36:20 Send Done\r\n                                                                                                                                                                   2024/06/19 23:36:20 Recv First Byte\r\n2024/06/19 23:36:21 Sent Header: Host [127.0.0.1:54321]\r\n                                                       2024/06/19 23:36:21 Sent Header: User-Agent [Go-http-client/1.1]\r\n                                                                                                                       2024/06/19 23:36:21 Sent Header: Apikey [eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0]\r\n                                                                                                                                      2024/06/19 23:36:21 Send Done\r\n                                                                                                                                                                   2024/06/19 23:36:21 Recv First Byte\r\n                  supabase_edge_runtime_secondbrain container logs:\r\n                                                                   DEBUG Opening cache /root/.cache/deno/dep_analysis_cache_v1...\r\n                                                                                                                                 DEBUG Opening cache /root/.cache/deno/node_analysis_cache_v1...\r\n            DEBUG FileFetcher::fetch() - specifier: file:///home/deno/index.ts\r\n                                                                              DEBUG FileFetcher::fetch() - specifier: https://deno.land/std/http/status.ts\r\n                                                                                                                                                          DEBUG FileFetcher::fetch_remote() - specifier: https://deno.land/std/http/status.ts\r\n                                                         DEBUG FileFetcher::fetch_cached - specifier: https://deno.land/std/http/status.ts\r\n                                                                                                                                          Download https://deno.land/std/http/status.ts\r\n   DEBUG starting new connection: https://deno.land/\r\n                                                    DEBUG FileFetcher::fetch() - specifier: https://deno.land/x/jose@v4.13.1/index.ts\r\n                                                                                                                                     DEBUG FileFetcher::fetch_remote() - specifier: https://deno.land/x/jose@v4.13.1/index.ts\r\n                                         DEBUG FileFetcher::fetch_cached - specifier: https://deno.land/x/jose@v4.13.1/index.ts\r\n                                                                                                                               Download https://deno.land/x/jose@v4.13.1/index.ts\r\n                                                                                                                                                                                 DEBUG starting new connection: https://deno.land/\r\nStopping containers...\r\nPruned containers: [34616962f4e6539a602da042c8cbf79679eb90e4533fd49943123c8695266d57 de64e846806decf3cf64ca1871fc25728227cf6c5c49ef5e0b3d14d683dc7395 29a37ad83bf6f249b52d42f852c4e4e75372661ec56951edec9f8191a4ac5378 572a8f1233d982f78e6263f09ea730abad2179ba2754648f9c63350793debd3b 9a8c5b7d420a4a45996792709cc0dc93c43caed9946182af80955ac5b6313cb5 1d5ee6490905f80bcd8335d56b8f02bd75b698bf1e24e6db33a0decde44bc0ee e93df671b0eed00e9412fbe6a5c961d0a7ee8a5a31faaeb9535a2e16aff68837 e2398a996747bc56ab75b3359cd08ac8f7fc8b1b5dacec2b0faf47bb825f6b0b c129cd6fb979e610d91d1d40072b11a0b99be0b40da83c700f8eb71fd8e46075 a3a5717e70457df5d6a02c88fc76e3a0c1da504ba889de8c1796adf32c2b029f]\r\nPruned volumes: [supabase_db_secondbrain supabase_config_secondbrain supabase_inbucket_secondbrain supabase_storage_secondbrain supabase_edge_runtime_secondbrain]\r\nPruned network: [supabase_network_secondbrain]\r\nError status 502:\r\n```"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey @chtanch! I'm here to help you with any issues or questions you have. I'm a bot that can assist with bug fixes, answer questions, and guide you to become a contributor. Let me know how I can assist you!\n\n<!-- Answer -->\nThe 502 Bad Gateway error you're encountering when running `supabase start` behind a corporate firewall is likely due to network configuration issues, particularly with the proxy settings. Here are some steps to troubleshoot and resolve this issue:\n\n1. **Verify Proxy Settings**:\n   Ensure that the `http_proxy` and `https_proxy` environment variables are correctly set. You can check this by running:\n   ```bash\n   echo $http_proxy\n   echo $https_proxy\n   ```\n\n2. **Docker Proxy Configuration**:\n   Make sure Docker is configured to use the proxy settings. You can configure Docker to use a proxy by creating or editing the Docker daemon configuration file (`/etc/systemd/system/docker.service.d/http-proxy.conf` for systemd-based systems):\n   ```plaintext\n   [Service]\n   Environment=\"HTTP_PROXY=http://proxy.example.com:80/\"\n   Environment=\"HTTPS_PROXY=https://proxy.example.com:443/\"\n   Environment=\"NO_PROXY=localhost,127.0.0.1\"\n   ```\n   After making changes, restart Docker:\n   ```bash\n   sudo systemctl daemon-reload\n   sudo systemctl restart docker\n   ```\n\n3. **Supabase CLI Proxy Configuration**:\n   Ensure that the Supabase CLI is aware of the proxy settings. You can set the proxy environment variables before running the Supabase commands:\n   ```bash\n   export http_proxy=http://proxy.example.com:80/\n   export https_proxy=https://proxy.example.com:443/\n   supabase start\n   ```\n\n4. **Network Configuration**:\n   Check if there are any network restrictions or firewall rules that might be blocking the necessary ports for Supabase. Ensure that the following ports are open:\n   - 54321 (PostgreSQL)\n   - 54322 (Kong)\n   - 54323 (Studio)\n   - 54324 (Realtime)\n   - 54325 (Storage)\n   - 54326 (Edge Functions)\n\n5. **Debugging**:\n   Run the Supabase start command with the `--debug` flag to get more detailed logs:\n   ```bash\n   supabase start --debug\n   ```\n   Analyze the debug logs to identify any specific network-related errors.\n\nIf the issue persists, you might need to consult your network administrator to ensure that all necessary traffic is allowed through the corporate firewall.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "chtanch",
        "body": "I have checked all the solution steps, and they do not solve my problem"
      },
      {
        "user": "fxgeoffrey",
        "body": "I have the same issue too. "
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3537,
    "title": "Update NextJS to version 15",
    "author": "StanGirard",
    "state": "closed",
    "created_at": "2024-12-21T15:56:52Z",
    "updated_at": "2024-12-21T15:57:33Z",
    "labels": [],
    "body": "Update nextjs to version 15 and add turbopack\r\n\r\nHere is a guide on how to -> https://nextjs.org/docs/app/building-your-application/upgrading/version-15",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-321/update-nextjs-to-version-15\">CORE-321 Update NextJS to version 15</a></p>"
      }
    ]
  },
  {
    "issue_number": 3207,
    "title": "Check for circular references",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-16T08:36:53Z",
    "updated_at": "2024-12-20T12:09:18Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "Adding knowledge with parent_id should check for circular reference",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-81/check-for-circular-references\">ENT-81 Check for circular references</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3211,
    "title": "Show user that a connection is deleting",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-16T11:36:05Z",
    "updated_at": "2024-12-20T12:09:17Z",
    "labels": [
      "Stale"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-82/show-user-that-a-connection-is-deleting\">ENT-82 Show user that a connection is deleting</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3206,
    "title": "[Bug]: supabase_vector_secondbrain container is not ready: unhealthy",
    "author": "windowshopr",
    "state": "closed",
    "created_at": "2024-09-16T06:47:05Z",
    "updated_at": "2024-12-20T08:07:49Z",
    "labels": [
      "bug",
      "Stale",
      "area: backend"
    ],
    "body": "### What happened?\n\nWindows 10, Python 3.11, Docker Desktop.\r\n\r\nTrying to follow the documentation step by step.\r\n\r\n```\r\ncd backend && supabase start\r\n```\r\n\r\n```\r\nsupabase_vector_secondbrain container is not ready: unhealthy\r\nTry rerunning the command with --debug to troubleshoot the error.\r\n```\r\n\r\n```\r\nsupabase start --debug\r\n```\r\n\r\n```\r\nPruned containers: [c0808a31c4edefd350ab4453a12a4c9c311f3be4a350cb898e90b461c0490a07 875ad8a524b0a4e8e8ceaef129008da8302d5bf2c67c2382264992e8c94cb655]\r\nPruned volumes: [supabase_db_secondbrain supabase_config_secondbrain]\r\nPruned network: [supabase_network_secondbrain]\r\nfailed to create docker container: Error response from daemon: Conflict. The container name \"/supabase_vector_secondbrain\" is already in use by container \"a1ff8452be7d314a0eed6d054981708a1d66181d5e904b34e710715aa0946888\". You have to remove (or rename) that container to be able to reuse that name.\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/3de92202-25c2-48f1-b1bf-ea64473b3882)\r\n\r\nI noticed that the supabase vector container is stuck on restarting, so I delete that container, and re-reun:\r\n\r\n```\r\nsupabase start --debug\r\n```\r\n\r\nSame output as above:\r\n\r\n```\r\nsupabase_vector_secondbrain container is not ready: unhealthy\r\n```\r\n\r\nSomething about vector_secondbrain not passing the health checks?\r\n\r\n![image](https://github.com/user-attachments/assets/93da14a4-6bed-46d7-b179-8732c320df40)\r\n\n\n### Relevant log output\n\n```bash\nLooking at the Docker Desktop log of the supabase_vector_secondbrain output, there's this, not sure if it helps debug?\r\n\r\n\r\n...\r\n2024-09-16 00:40:20 2024-09-16T06:40:20.054112Z ERROR source{component_kind=\"source\" component_id=docker_host component_type=docker_logs component_name=docker_host}: vector::sources::docker_logs: Listing currently running containers failed. error=error trying to connect: tcp connect error: Network unreachable (os error 101)\r\n2024-09-16 00:40:20 2024-09-16T06:40:20.056385Z  INFO vector::internal_events::api: API server running. address=0.0.0.0:9001 playground=http://0.0.0.0:9001/playground\r\n2024-09-16 00:40:20 2024-09-16T06:40:20.056545Z  INFO vector::app: All sources have finished.\r\n2024-09-16 00:40:20 2024-09-16T06:40:20.056645Z  INFO vector: Vector has stopped.\r\n2024-09-16 00:40:20 2024-09-16T06:40:20.069990Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"rest_logs, logflare_storage, logflare_db, project_logs, logflare_realtime, kong_logs, logflare_kong, auth_logs, router, db_logs, docker_host, realtime_logs, storage_logs, logflare_rest, logflare_functions, kong_err, logflare_auth\" time_remaining=\"59 seconds left\"\r\n2024-09-16 00:40:26 2024-09-16T06:40:26.845553Z  INFO vector::app: Internal log rate limit configured. internal_log_rate_secs=10\r\n2024-09-16 00:40:26 2024-09-16T06:40:26.845651Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,lapin=info,kube=info\"\r\n2024-09-16 00:40:26 2024-09-16T06:40:26.845706Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.yaml\"]\r\n2024-09-16 00:40:26 2024-09-16T06:40:26.872608Z  WARN vector::config::loading: Transform \"router._unmatched\" has no consumers\r\n2024-09-16 00:40:26 2024-09-16T06:40:26.873113Z  INFO source{component_kind=\"source\" component_id=docker_host component_type=docker_logs component_name=docker_host}: vector::sources::docker_logs: Capturing logs from now on. now=2024-09-16T06:40:26.873062835+00:00\r\n2024-09-16 00:40:26 2024-09-16T06:40:26.873193Z  INFO source{component_kind=\"source\" component_id=docker_host component_type=docker_logs component_name=docker_host}: vector::sources::docker_logs: Listening to docker log events.\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.030058Z  INFO vector::topology::running: Running healthchecks.\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.030427Z  INFO vector: Vector has started. debug=\"false\" version=\"0.28.1\" arch=\"x86_64\" revision=\"ff15924 2023-03-06\"\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.030556Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.030681Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.030749Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.030818Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.030926Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.031079Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.031146Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.035574Z ERROR source{component_kind=\"source\" component_id=docker_host component_type=docker_logs component_name=docker_host}: vector::sources::docker_logs: Listing currently running containers failed. error=error trying to connect: tcp connect error: Network unreachable (os error 101)\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.039825Z  INFO vector::internal_events::api: API server running. address=0.0.0.0:9001 playground=http://0.0.0.0:9001/playground\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.039863Z  INFO vector::app: All sources have finished.\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.039866Z  INFO vector: Vector has stopped.\r\n2024-09-16 00:40:27 2024-09-16T06:40:27.050768Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"logflare_kong, logflare_rest\" time_remaining=\"59 seconds left\"\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.186820Z  INFO vector::app: Internal log rate limit configured. internal_log_rate_secs=10\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.186894Z  INFO vector::app: Log level is enabled. level=\"vector=info,codec=info,vrl=info,file_source=info,tower_limit=trace,rdkafka=info,buffers=info,lapin=info,kube=info\"\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.187112Z  INFO vector::app: Loading configs. paths=[\"/etc/vector/vector.yaml\"]\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.214866Z  WARN vector::config::loading: Transform \"router._unmatched\" has no consumers\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.215238Z  INFO source{component_kind=\"source\" component_id=docker_host component_type=docker_logs component_name=docker_host}: vector::sources::docker_logs: Capturing logs from now on. now=2024-09-16T06:40:40.215191780+00:00\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.215288Z  INFO source{component_kind=\"source\" component_id=docker_host component_type=docker_logs component_name=docker_host}: vector::sources::docker_logs: Listening to docker log events.\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.382769Z  INFO vector::topology::running: Running healthchecks.\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.382984Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.383056Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.383063Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.383065Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.383067Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.383068Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.383070Z  INFO vector::topology::builder: Healthcheck passed.\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.383033Z  INFO vector: Vector has started. debug=\"false\" version=\"0.28.1\" arch=\"x86_64\" revision=\"ff15924 2023-03-06\"\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.388487Z ERROR source{component_kind=\"source\" component_id=docker_host component_type=docker_logs component_name=docker_host}: vector::sources::docker_logs: Listing currently running containers failed. error=error trying to connect: tcp connect error: Network unreachable (os error 101)\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.393704Z  INFO vector::internal_events::api: API server running. address=0.0.0.0:9001 playground=http://0.0.0.0:9001/playground\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.393806Z  INFO vector::app: All sources have finished.\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.393831Z  INFO vector: Vector has stopped.\r\n2024-09-16 00:40:40 2024-09-16T06:40:40.405276Z  INFO vector::topology::running: Shutting down... Waiting on running components. remaining_components=\"logflare_kong, logflare_realtime, storage_logs, logflare_storage\" time_remaining=\"59 seconds left\"\r\n```\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-202/[bug]-supabase-vector-secondbrain-container-is-not-ready-unhealthy\">CORE-202 [Bug]: supabase_vector_secondbrain container is not ready: unhealthy</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey there, @windowshopr! ğŸ‘‹ I'm here to help you with any bugs, questions, or contributions you might have. Let's squash those bugs and get things running smoothly!\n\n<!-- Answer -->\nI found a similar issue that might be relevant to your problem: [[Bug]: dependency failed to start: container supabase-analytics is unhealthy](https://github.com/QuivrHQ/quivr/issues/1783). This issue is closed and was last updated on August 09, 2024 <sup>[[1]](https://github.com/QuivrHQ/quivr/issues/1783)</sup>.\n\nFor the `supabase_vector_secondbrain` container, ensure that the necessary environment variables are correctly set. These include:\n\n- `openai_api_key`\n- `azure_openai_embeddings_url`\n- `supabase_url`\n- `supabase_service_key`\n- `pg_database_url`\n- `pg_database_async_url`\n- `embedding_dim`\n\nAdditionally, the `backend/ci-migration.sh` script indicates the need for:\n- `PROJECT_ID`\n- `SUPABASE_ACCESS_TOKEN`\n- `SUPABASE_DB_PASSWORD` <sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/models/settings.py)</sup><sup>[[3]](https://github.com/QuivrHQ/quivr/blob/main/backend/ci-migration.sh)</sup>.\n\nMake sure these variables are correctly configured in your environment.\n\nIf the issue persists, you might want to check the Docker logs for more specific error messages related to the health checks. The logs you provided indicate a network issue (`Network unreachable (os error 101)`). This could be related to Docker's network configuration or a firewall setting on your Windows machine.\n\nTry the following steps:\n1. Ensure Docker Desktop is up to date.\n2. Restart Docker Desktop.\n3. Check your firewall settings to ensure Docker has the necessary permissions.\n4. Verify that no other containers are conflicting with the `supabase_vector_secondbrain` container.\n\nIf these steps do not resolve the issue, you may need to investigate further into the Docker network settings or consult the Docker documentation for more detailed troubleshooting steps.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "windowshopr",
        "body": "The similar issue you found didn't provide an answer.\r\n\r\nThe .env file is left completely default, except for the uncommenting of the line:\r\n\r\n```\r\nOLLAMA_API_BASE_URL=http://host.docker.internal:11434 # Uncomment to activate ollama. This is the local url for the ollama api\r\n```\r\n\r\n...as I plan to use Ollama models for my applications.\r\n\r\nThe documentation should be updated if other changes are to be made for the supabase instructions.\r\n\r\nAs a second attempt watching the installation video on Youtube doesn't show using supabase at all, just:\r\n\r\n```\r\ndocker compose pull\r\n```\r\n\r\n```\r\ndocker compose up\r\n```\r\n\r\n...Which results in the following output, where I see two questionable sections\r\n\r\n```\r\ntime=\"2024-09-16T00:57:53-06:00\" level=warning msg=\"I:\\\\nasty\\\\Python_Projects\\\\LLM\\\\quivr\\\\docker-compose.yml: `version` is obsolete\"\r\n[+] Running 7/0\r\n âœ” Container redis        Created                                                                                  0.0s\r\n âœ” Container backend-api  Created                                                                                  0.0s\r\n âœ” Container web          Created                                                                                  0.0s\r\n âœ” Container beat         Created                                                                                  0.0s\r\n âœ” Container worker       Created                                                                                  0.0s\r\n âœ” Container flower       Created                                                                                  0.0s\r\n âœ” Container notifier     Created                                                                                  0.0s\r\nAttaching to backend-api, beat, flower, notifier, redis, web, worker\r\nredis        | 1:C 16 Sep 2024 06:57:54.279 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\nredis        | 1:C 16 Sep 2024 06:57:54.279 * Redis version=7.2.3, bits=64, commit=00000000, modified=0, pid=1, just started\r\nredis        | 1:C 16 Sep 2024 06:57:54.279 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\r\nredis        | 1:M 16 Sep 2024 06:57:54.280 * monotonic clock: POSIX clock_gettime\r\nredis        | 1:M 16 Sep 2024 06:57:54.284 * Running mode=standalone, port=6379.\r\nredis        | 1:M 16 Sep 2024 06:57:54.289 * Server initialized\r\nredis        | 1:M 16 Sep 2024 06:57:54.289 * Loading RDB produced by version 7.2.3\r\nredis        | 1:M 16 Sep 2024 06:57:54.289 * RDB age 47 seconds\r\nredis        | 1:M 16 Sep 2024 06:57:54.289 * RDB memory usage when created 1.44 Mb\r\nredis        | 1:M 16 Sep 2024 06:57:54.290 * Done loading RDB, keys loaded: 12, keys expired: 0.\r\nredis        | 1:M 16 Sep 2024 06:57:54.290 * DB loaded from disk: 0.001 seconds\r\nredis        | 1:M 16 Sep 2024 06:57:54.290 * Ready to accept connections tcp\r\nbackend-api  | INFO:     Will watch for changes in these directories: ['/app']\r\nbackend-api  | INFO:     Uvicorn running on http://0.0.0.0:5050 (Press CTRL+C to quit)\r\nbackend-api  | INFO:     Started reloader process [1] using StatReload\r\nweb          |    â–² Next.js 14.1.0\r\nweb          |    - Local:        http://localhost:3000\r\nweb          |    - Network:      http://0.0.0.0:3000\r\nweb          |\r\nweb          |  âœ“ Ready in 379ms\r\nnotifier     | [INFO] quivr_api.modules.dependencies [dependencies.py:179]: Creating Supabase client\r\nnotifier     | [INFO] notifier_service [celery_monitor.py:150]: Started  quivr-notifier service...\r\nnotifier     | [INFO] notifier_service [celery_monitor.py:51]: Initialized knowledge_service. Listening to task event...\r\nflower       | [INFO] quivr_api.modules.dependencies [dependencies.py:179]: Creating Supabase client\r\nbeat         | [INFO] quivr_api.modules.dependencies [dependencies.py:179]: Creating Supabase client\r\nworker       | [INFO] quivr_api.modules.dependencies [dependencies.py:179]: Creating Supabase client\r\nflower       | [INFO] quivr_api.modules.dependencies [dependencies.py:153]: Creating Postgres DB engine\r\nflower       | [I 240916 06:58:03 command:168] Visit me at http://0.0.0.0:5555\r\nflower       | [I 240916 06:58:03 command:176] Broker: redis://redis:6379/0\r\nflower       | [I 240916 06:58:03 command:177] Registered tasks:\r\nflower       |     ['NotionConnectorLoad',\r\nflower       |      'celery.accumulate',\r\nflower       |      'celery.backend_cleanup',\r\nflower       |      'celery.chain',\r\nflower       |      'celery.chord',\r\nflower       |      'celery.chord_unlock',\r\nflower       |      'celery.chunks',\r\nflower       |      'celery.group',\r\nflower       |      'celery.map',\r\nflower       |      'celery.starmap',\r\nflower       |      'check_is_premium_task',\r\nflower       |      'fetch_and_store_notion_files_task',\r\nworker       | [INFO] quivr_api.modules.dependencies [dependencies.py:153]: Creating Postgres DB engine\r\nflower       |      'process_active_syncs_task',\r\nbeat         | [INFO] quivr_api.modules.dependencies [dependencies.py:153]: Creating Postgres DB engine\r\nflower       |      'process_crawl_task',\r\nflower       |      'process_file_task',\r\nflower       |      'process_notion_sync_task',\r\nflower       |      'process_sync_task',\r\nworker       | /usr/local/lib/python3.11/site-packages/celery/platforms.py:829: SecurityWarning: You're running the worker with superuser privileges: this is\r\nflower       |      'quivr_worker.celery_worker.ping_telemetry',\r\nworker       | absolutely not recommended!\r\nbeat         | [2024-09-16 06:58:03,672: INFO/MainProcess] beat: Starting...\r\nflower       |      'quivr_worker.celery_worker.process_integration_brain_sync_user_brain']\r\nworker       |\r\nflower       | [I 240916 06:58:03 mixins:229] Connected to redis://redis:6379/0\r\nworker       | Please specify a different user using the --uid option.\r\nworker       |\r\nworker       | User information: uid=0 euid=0 gid=0 egid=0\r\nworker       |\r\nworker       |   warnings.warn(SecurityWarning(ROOT_DISCOURAGED.format(\r\nworker       |\r\nworker       |  -------------- celery@f5355532b0d8 v5.4.0 (opalescent)\r\nworker       | --- ***** -----\r\nworker       | -- ******* ---- Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.31 2024-09-16 06:58:03\r\nworker       | - *** --- * ---\r\nbeat         | [2024-09-16 06:58:03,774: INFO/MainProcess] Scheduler: Sending due task process_premium_users (check_is_premium_task)\r\nworker       | - ** ---------- [config]\r\nworker       | - ** ---------- .> app:         quivr_api.celery_config:0x7fc4242c4790\r\nbeat         | [2024-09-16 06:58:03,793: INFO/MainProcess] Scheduler: Sending due task process_active_syncs (process_active_syncs_task)\r\nworker       | - ** ---------- .> transport:   redis://redis:6379/0\r\nworker       | - ** ---------- .> results:     redis://redis:6379/0\r\nworker       | - *** --- * --- .> concurrency: 2 (prefork)\r\nworker       | -- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)\r\nworker       | --- ***** -----\r\nworker       |  -------------- [queues]\r\nworker       |                 .> celery           exchange=celery(direct) key=celery\r\nworker       |\r\nworker       |\r\nworker       | [tasks]\r\nworker       |   . NotionConnectorLoad\r\nworker       |   . check_is_premium_task\r\nworker       |   . fetch_and_store_notion_files_task\r\nworker       |   . process_active_syncs_task\r\nworker       |   . process_crawl_task\r\nworker       |   . process_file_task\r\nworker       |   . process_notion_sync_task\r\nworker       |   . process_sync_task\r\nworker       |   . quivr_worker.celery_worker.ping_telemetry\r\nworker       |   . quivr_worker.celery_worker.process_integration_brain_sync_user_brain\r\nworker       |\r\nworker       | [2024-09-16 06:58:04,032: WARNING/MainProcess] /usr/local/lib/python3.11/site-packages/celery/worker/consumer/consumer.py:508: CPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine\r\nworker       | whether broker connection retries are made during startup in Celery 6.0 and above.\r\nworker       | If you wish to retain the existing behavior for retrying connections on startup,\r\nworker       | you should set broker_connection_retry_on_startup to True.\r\nworker       |   warnings.warn(\r\nworker       |\r\nworker       | [2024-09-16 06:58:04,061: INFO/MainProcess] Connected to redis://redis:6379/0\r\nworker       | [2024-09-16 06:58:04,064: WARNING/MainProcess] /usr/local/lib/python3.11/site-packages/celery/worker/consumer/consumer.py:508: CPendingDeprecationWarning: The broker_connection_retry configuration setting will no longer determine\r\nworker       | whether broker connection retries are made during startup in Celery 6.0 and above.\r\nworker       | If you wish to retain the existing behavior for retrying connections on startup,\r\nworker       | you should set broker_connection_retry_on_startup to True.\r\nworker       |   warnings.warn(\r\nworker       |\r\nworker       | [2024-09-16 06:58:04,070: INFO/MainProcess] mingle: searching for neighbors\r\nbackend-api  | [INFO] quivr_api.modules.dependencies [dependencies.py:179]: Creating Supabase client\r\nbackend-api  | [INFO] quivr_api.modules.dependencies [dependencies.py:153]: Creating Postgres DB engine\r\nflower       | [W 240916 06:58:04 inspector:44] Inspect method stats failed\r\nflower       | [W 240916 06:58:04 inspector:44] Inspect method registered failed\r\nflower       | [W 240916 06:58:04 inspector:44] Inspect method active_queues failed\r\nflower       | [W 240916 06:58:04 inspector:44] Inspect method active failed\r\nflower       | [W 240916 06:58:04 inspector:44] Inspect method reserved failed\r\nflower       | [W 240916 06:58:04 inspector:44] Inspect method scheduled failed\r\nworker       | [2024-09-16 06:58:05,078: INFO/MainProcess] mingle: all alone\r\nworker       | [2024-09-16 06:58:05,091: INFO/MainProcess] celery@f5355532b0d8 ready.\r\nworker       | [2024-09-16 06:58:05,095: INFO/MainProcess] Task check_is_premium_task[7f027681-655a-4596-9a42-c411da568b29] received\r\nworker       | [2024-09-16 06:58:05,097: INFO/MainProcess] Task process_active_syncs_task[f857b54f-b2ec-49d3-ba84-7a34dc0cd7a9] received\r\nworker       | [INFO] celery_worker [check_premium.py:16]: Stripe deactivated, skipping check for premium users\r\nworker       | [2024-09-16 06:58:05,103: INFO/ForkPoolWorker-2] Task check_is_premium_task[7f027681-655a-4596-9a42-c411da568b29] succeeded in 0.004031250000025466s: None\r\nworker       | [2024-09-16 06:58:05,146: ERROR/ForkPoolWorker-1] Task process_active_syncs_task[f857b54f-b2ec-49d3-ba84-7a34dc0cd7a9] raised unexpected: ConnectionRefusedError(111, \"Connect call failed ('192.168.65.254', 54322)\")\r\nworker       | Traceback (most recent call last):\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/celery/app/trace.py\", line 453, in trace_task\r\nworker       |     R = retval = fun(*args, **kwargs)\r\nworker       |                  ^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/celery/app/trace.py\", line 736, in __protected_call__\r\nworker       |     return self.run(*args, **kwargs)\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/app/worker/quivr_worker/celery_worker.py\", line 270, in process_active_syncs_task\r\nworker       |     loop.run_until_complete(\r\nworker       |   File \"/usr/local/lib/python3.11/asyncio/base_events.py\", line 653, in run_until_complete\r\nworker       |     return future.result()\r\nworker       |            ^^^^^^^^^^^^^^^\r\nworker       |   File \"/app/worker/quivr_worker/syncs/process_active_syncs.py\", line 62, in process_all_active_syncs\r\nworker       |     async with build_syncs_utils(sync_services) as mapping_sync_utils:\r\nworker       |   File \"/usr/local/lib/python3.11/contextlib.py\", line 204, in __aenter__\r\nworker       |     return await anext(self.gen)\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/app/worker/quivr_worker/syncs/utils.py\", line 90, in build_syncs_utils\r\nworker       |     raise e\r\nworker       |   File \"/app/worker/quivr_worker/syncs/utils.py\", line 56, in build_syncs_utils\r\nworker       |     await session.execute(\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlmodel/ext/asyncio/session.py\", line 143, in execute\r\nworker       |     return await super().execute(\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/ext/asyncio/session.py\", line 461, in execute\r\nworker       |     result = await greenlet_spawn(\r\nworker       |              ^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 201, in greenlet_spawn\r\nworker       |     result = context.throw(*sys.exc_info())\r\nworker       |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlmodel/orm/session.py\", line 127, in execute\r\nworker       |     return super().execute(\r\nworker       |            ^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2362, in execute\r\nworker       |     return self._execute_internal(\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2237, in _execute_internal\r\nworker       |     conn = self._connection_for_bind(bind)\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2106, in _connection_for_bind\r\nworker       |     return trans._connection_for_bind(engine, execution_options)\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"<string>\", line 2, in _connection_for_bind\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/state_changes.py\", line 139, in _go\r\nworker       |     ret_value = fn(self, *arg, **kw)\r\nworker       |                 ^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 1189, in _connection_for_bind\r\nworker       |     conn = bind.connect()\r\nworker       |            ^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 3278, in connect\r\nworker       |     return self._connection_cls(self)\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 146, in __init__\r\nworker       |     self._dbapi_connection = engine.raw_connection()\r\nworker       |                              ^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 3302, in raw_connection\r\nworker       |     return self.pool.connect()\r\nworker       |            ^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 449, in connect\r\nworker       |     return _ConnectionFairy._checkout(self)\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 1263, in _checkout\r\nworker       |     fairy = _ConnectionRecord.checkout(pool)\r\nworker       |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 712, in checkout\r\nworker       |     rec = pool._do_get()\r\nworker       |           ^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/impl.py\", line 179, in _do_get\r\nworker       |     with util.safe_reraise():\r\nflower       | [W 240916 06:58:05 inspector:44] Inspect method revoked failed\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py\", line 146, in __exit__\r\nflower       | [W 240916 06:58:05 inspector:44] Inspect method conf failed\r\nworker       |     raise exc_value.with_traceback(exc_tb)\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/impl.py\", line 177, in _do_get\r\nworker       |     return self._create_connection()\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 390, in _create_connection\r\nworker       |     return _ConnectionRecord(self)\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 674, in __init__\r\nworker       |     self.__connect()\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 900, in __connect\r\nworker       |     with util.safe_reraise():\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py\", line 146, in __exit__\r\nworker       |     raise exc_value.with_traceback(exc_tb)\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 896, in __connect\r\nworker       |     self.dbapi_connection = connection = pool._invoke_creator(self)\r\nworker       |                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/create.py\", line 643, in connect\r\nworker       |     return dialect.connect(*cargs, **cparams)\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/default.py\", line 621, in connect\r\nworker       |     return self.loaded_dbapi.connect(*cargs, **cparams)\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 932, in connect\r\nworker       |     await_only(creator_fn(*arg, **kw)),\r\nworker       |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\r\nworker       |     return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\r\nworker       |     value = await result\r\nworker       |             ^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connection.py\", line 2329, in connect\r\nworker       |     return await connect_utils._connect(\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connect_utils.py\", line 1017, in _connect\r\nworker       |     raise last_error or exceptions.TargetServerAttributeNotMatched(\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connect_utils.py\", line 991, in _connect\r\nworker       |     conn = await _connect_addr(\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connect_utils.py\", line 828, in _connect_addr\r\nworker       |     return await __connect_addr(params, True, *args)\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connect_utils.py\", line 873, in __connect_addr\r\nworker       |     tr, pr = await connector\r\nworker       |              ^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connect_utils.py\", line 744, in _create_ssl_connection\r\nworker       |     tr, pr = await loop.create_connection(\r\nworker       |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/asyncio/base_events.py\", line 1085, in create_connection\r\nworker       |     raise exceptions[0]\r\nworker       |   File \"/usr/local/lib/python3.11/asyncio/base_events.py\", line 1069, in create_connection\r\nworker       |     sock = await self._connect_sock(\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/asyncio/base_events.py\", line 973, in _connect_sock\r\nworker       |     await self.sock_connect(sock, address)\r\nworker       |   File \"/usr/local/lib/python3.11/asyncio/selector_events.py\", line 634, in sock_connect\r\nworker       |     return await fut\r\nworker       |            ^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/asyncio/selector_events.py\", line 674, in _sock_connect_cb\r\nworker       |     raise OSError(err, f'Connect call failed {address}')\r\nworker       | ConnectionRefusedError: [Errno 111] Connect call failed ('192.168.65.254', 54322)\r\nbackend-api  | [INFO] quivr_api.main [main.py:120]: Telemetry enabled, we use telemetry to collect anonymous usage data.\r\nbackend-api  | [INFO] quivr_api.main [main.py:121]: To disable telemetry, set the TELEMETRY_ENABLED environment variable to false.\r\nworker       | [2024-09-16 06:58:08,540: INFO/MainProcess] Events of group {task} enabled by remote.\r\nbackend-api  | INFO:     Started server process [8]\r\nbackend-api  | INFO:     Waiting for application startup.\r\nbackend-api  | INFO:     Application startup complete.\r\nbackend-api  | INFO:     127.0.0.1:57108 - \"GET /healthz HTTP/1.1\" 500 Internal Server Error\r\nbackend-api  | ERROR:    Exception in ASGI application\r\nbackend-api  | Traceback (most recent call last):\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 406, in run_asgi\r\nbackend-api  |     result = await app(  # type: ignore[func-returns-value]\r\nbackend-api  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\nbackend-api  |     return await self.app(scope, receive, send)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\nbackend-api  |     await super().__call__(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\r\nbackend-api  |     await self.middleware_stack(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\nbackend-api  |     raise exc\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\nbackend-api  |     await self.app(scope, receive, _send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\nbackend-api  |     await self.app(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\nbackend-api  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\nbackend-api  |     raise exc\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\nbackend-api  |     await app(scope, receive, sender)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 754, in __call__\r\nbackend-api  |     await self.middleware_stack(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 774, in app\r\nbackend-api  |     await route.handle(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 295, in handle\r\nbackend-api  |     await self.app(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 77, in app\r\nbackend-api  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\nbackend-api  |     raise exc\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\nbackend-api  |     await app(scope, receive, sender)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 74, in app\r\nbackend-api  |     response = await f(request)\r\nbackend-api  |                ^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/routing.py\", line 269, in app\r\nbackend-api  |     solved_result = await solve_dependencies(\r\nbackend-api  |                     ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/dependencies/utils.py\", line 596, in solve_dependencies\r\nbackend-api  |     solved = await solve_generator(\r\nbackend-api  |              ^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/fastapi/dependencies/utils.py\", line 521, in solve_generator\r\nbackend-api  |     return await stack.enter_async_context(cm)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/contextlib.py\", line 638, in enter_async_context\r\nbackend-api  |     result = await _enter(cm)\r\nbackend-api  |              ^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/contextlib.py\", line 204, in __aenter__\r\nbackend-api  |     return await anext(self.gen)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/app/api/quivr_api/modules/dependencies.py\", line 108, in get_async_session\r\nbackend-api  |     raise e\r\nbackend-api  |   File \"/app/api/quivr_api/modules/dependencies.py\", line 101, in get_async_session\r\nbackend-api  |     await session.execute(\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlmodel/ext/asyncio/session.py\", line 143, in execute\r\nbackend-api  |     return await super().execute(\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/ext/asyncio/session.py\", line 461, in execute\r\nbackend-api  |     result = await greenlet_spawn(\r\nbackend-api  |              ^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 201, in greenlet_spawn\r\nbackend-api  |     result = context.throw(*sys.exc_info())\r\nbackend-api  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlmodel/orm/session.py\", line 127, in execute\r\nbackend-api  |     return super().execute(\r\nbackend-api  |            ^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2362, in execute\r\nbackend-api  |     return self._execute_internal(\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2237, in _execute_internal\r\nbackend-api  |     conn = self._connection_for_bind(bind)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 2106, in _connection_for_bind\r\nbackend-api  |     return trans._connection_for_bind(engine, execution_options)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"<string>\", line 2, in _connection_for_bind\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/state_changes.py\", line 139, in _go\r\nbackend-api  |     ret_value = fn(self, *arg, **kw)\r\nbackend-api  |                 ^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/session.py\", line 1189, in _connection_for_bind\r\nbackend-api  |     conn = bind.connect()\r\nbackend-api  |            ^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 3278, in connect\r\nbackend-api  |     return self._connection_cls(self)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 146, in __init__\r\nbackend-api  |     self._dbapi_connection = engine.raw_connection()\r\nbackend-api  |                              ^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/base.py\", line 3302, in raw_connection\r\nbackend-api  |     return self.pool.connect()\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 449, in connect\r\nbackend-api  |     return _ConnectionFairy._checkout(self)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 1263, in _checkout\r\nbackend-api  |     fairy = _ConnectionRecord.checkout(pool)\r\nbackend-api  |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 712, in checkout\r\nbackend-api  |     rec = pool._do_get()\r\nbackend-api  |           ^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/impl.py\", line 179, in _do_get\r\nbackend-api  |     with util.safe_reraise():\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py\", line 146, in __exit__\r\nbackend-api  |     raise exc_value.with_traceback(exc_tb)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/impl.py\", line 177, in _do_get\r\nbackend-api  |     return self._create_connection()\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 390, in _create_connection\r\nbackend-api  |     return _ConnectionRecord(self)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 674, in __init__\r\nbackend-api  |     self.__connect()\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 900, in __connect\r\nbackend-api  |     with util.safe_reraise():\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py\", line 146, in __exit__\r\nbackend-api  |     raise exc_value.with_traceback(exc_tb)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/pool/base.py\", line 896, in __connect\r\nbackend-api  |     self.dbapi_connection = connection = pool._invoke_creator(self)\r\nbackend-api  |                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/create.py\", line 643, in connect\r\nbackend-api  |     return dialect.connect(*cargs, **cparams)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/default.py\", line 621, in connect\r\nbackend-api  |     return self.loaded_dbapi.connect(*cargs, **cparams)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/dialects/postgresql/asyncpg.py\", line 932, in connect\r\nbackend-api  |     await_only(creator_fn(*arg, **kw)),\r\nbackend-api  |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 132, in await_only\r\nbackend-api  |     return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/sqlalchemy/util/_concurrency_py3k.py\", line 196, in greenlet_spawn\r\nbackend-api  |     value = await result\r\nbackend-api  |             ^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connection.py\", line 2329, in connect\r\nbackend-api  |     return await connect_utils._connect(\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connect_utils.py\", line 1017, in _connect\r\nbackend-api  |     raise last_error or exceptions.TargetServerAttributeNotMatched(\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connect_utils.py\", line 991, in _connect\r\nbackend-api  |     conn = await _connect_addr(\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connect_utils.py\", line 828, in _connect_addr\r\nbackend-api  |     return await __connect_addr(params, True, *args)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connect_utils.py\", line 873, in __connect_addr\r\nbackend-api  |     tr, pr = await connector\r\nbackend-api  |              ^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/asyncpg/connect_utils.py\", line 744, in _create_ssl_connection\r\nbackend-api  |     tr, pr = await loop.create_connection(\r\nbackend-api  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"uvloop/loop.pyx\", line 2039, in create_connection\r\nbackend-api  |   File \"uvloop/loop.pyx\", line 2016, in uvloop.loop.Loop.create_connection\r\nbackend-api  | ConnectionRefusedError: [Errno 111] Connection refused\r\n```\r\n\r\nTrying to ignore these issues, I navigate to the webpage anyway and try to login:\r\n\r\n![image](https://github.com/user-attachments/assets/a3983d56-d54b-4d06-ba03-718108dbbd28)\r\n\r\n\r\nNo dice, connection refused."
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3522,
    "title": "Add Language in knowledge and chunk tables",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-12-16T09:30:41Z",
    "updated_at": "2024-12-19T16:17:17Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-315/add-language-in-knowledge-and-chunk-tables\">CORE-315 Add Language in knowledge and chunk tables</a></p>"
      }
    ]
  },
  {
    "issue_number": 3523,
    "title": "Create a language extractor module in Quivr Core",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-12-16T09:31:50Z",
    "updated_at": "2024-12-19T16:17:15Z",
    "labels": [],
    "body": "\\-> Extract language of the question, the chunks and the overall document.",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-316/create-a-language-extractor-module-in-quivr-core\">CORE-316 Create a language extractor module in Quivr Core</a></p>"
      }
    ]
  },
  {
    "issue_number": 3524,
    "title": "Extract question language",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-12-16T09:32:21Z",
    "updated_at": "2024-12-19T16:17:13Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-317/add-a-optional-rag-parameter-with-lang\">CORE-317 Add a Optional RAG Parameter with lang</a></p>"
      }
    ]
  },
  {
    "issue_number": 3437,
    "title": "MegaParse SDK - Add a pool of connection with httpx",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-31T11:04:25Z",
    "updated_at": "2024-12-17T15:20:13Z",
    "labels": [],
    "body": "* This instanciates a http conn with the API on each `process_file`. This could be problematic. the httpx.AsyncClient actually holds a pool of connections. The better way would be to hold a `self.client` that is closed when the object is gc'ed but we can keep it for now",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-268/megaparse-sdk-add-a-pool-of-connection-with-httpx\">CORE-268 MegaParse SDK - Add a pool of connection with httpx</a></p>"
      }
    ]
  },
  {
    "issue_number": 3435,
    "title": "Add new tests in Megaparse",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-31T09:42:17Z",
    "updated_at": "2024-12-16T16:42:30Z",
    "labels": [],
    "body": "for test_processing\n\n```\nExample of listing all files in directory and parametrizing a fixture with the documents1\n\ndef list_files_in_data_directory(): \n data_dir = Path(\"./tests/data\") \n return [file for file in data_dir.iterdir() if file.is_file()] \n\n\ndef file_id(file_path): \n return file_path.name \n\n\n@pytest.fixture(params=list_files_in_data_directory(), ids=file_id) \ndef qfile_placeholder(request) -> QuivrFile: \n \"\"\"Fixture to create a QuivrFile for each file in data/ directory.\"\"\" \n file_path = request.param\n```\n\nSee Amine Comments [here](https://github.com/QuivrHQ/MegaParse/pull/93) !!!\n\n* Add a test \\_check_free_memory() diff memoire virtuel et memoire utilisÃ©",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-266/add-new-tests-in-megaparse\">CORE-266 Add new tests in Megaparse</a></p>"
      }
    ]
  },
  {
    "issue_number": 2777,
    "title": "How do I run the Ollama [Bug]: ",
    "author": "FaizanMunsaf",
    "state": "closed",
    "created_at": "2024-06-28T13:04:15Z",
    "updated_at": "2024-12-16T12:10:42Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### What happened?\n\nI am trying to run Ollama there's no specific command I found that run Ollama private model.\r\nThere's any further guidance available with current repo?\n\n### Relevant log output\n\n```bash\nJust looking forward to Ollama show into my bot, I will generate my answer using Ollama llama3-8b model!\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/TECH-61/how-do-i-run-the-ollama-[bug]\">TECH-61 How do I run the Ollama [Bug]:</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey @FaizanMunsaf! I'm here to help you with the Ollama issue you're facing. I'm a bot designed to assist with bugs, questions, and contributions. Let's work on resolving this together.\n\n<!-- Answer -->\nTo run the Ollama private model using the current repository, follow these steps:\n\n1. **Install Ollama**:\n   Install Ollama from their [website](https://ollama.ai/).\n\n2. **Run Ollama in the background**:\n   Execute the following command to run Ollama:\n   ```bash\n   ollama run llama2\n   ```\n\n3. **Run Migrations**:\n   Move the migration file and reset the database:\n   ```bash\n   mv supabase/migrations/local_20240107152745_ollama.sql supabase/migrations/20240107152745_ollama.sql\n   supabase db reset\n   ```\n\n4. **Add Ollama Model to Quivr**:\n   Update the `user_settings` table in Supabase to include the Ollama models:\n   ```json\n   [\n     \"ollama/llama2\",\n     \"ollama/mistral\"\n   ]\n   ```\n   If you want to add it to your user only, you can add it to the `models` column in the `user_settings` table. If you want to set it as default for all new users, drop the entire table with the following command:\n   ```sql\n   DELETE TABLE user_settings;\n   ```\n\n5. **Configure .env File**:\n   Update your `.env` file to activate the Ollama API by uncommenting and setting the `OLLAMA_API_BASE_URL`:\n   ```env\n   # LOCAL\n   OLLAMA_API_BASE_URL=http://host.docker.internal:11434\n   ```\n\n6. **Integrate with Quivr**:\n   Ensure that the `QuivrRAG` class in `quivr_rag.py` is properly configured to use the Ollama model. The `ChatLiteLLM` class should be instantiated with the `api_base` parameter set to the `OLLAMA_API_BASE_URL` if the model starts with \"ollama\". This is handled in the `_create_llm` method of the `QuivrRAG` class:\n   ```python\n   def _create_llm(\n       self,\n       callbacks,\n       model,\n       streaming=False,\n       temperature=0,\n   ) -> BaseLLM:\n       if streaming and callbacks is None:\n           raise ValueError(\n               \"Callbacks must be provided when using streaming language models\"\n           )\n\n       api_base = None\n       if self.brain_settings.ollama_api_base_url and model.startswith(\"ollama\"):\n           api_base = self.brain_settings.ollama_api_base_url\n\n       return ChatLiteLLM(\n           temperature=temperature,\n           max_tokens=self.max_tokens,\n           model=model,\n           streaming=streaming,\n           verbose=False,\n           callbacks=callbacks,\n           api_base=api_base,\n       )\n   ```\n\nBy following these steps, you will have the Ollama model running and integrated with Quivr <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/docs/developers/contribution/llm/ollama.mdx)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/.env.example)</sup><sup>[[3]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/brain/rags/quivr_rag.py)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "roaidev",
        "body": "real problem is getting it to work with llama3. how do we do that?\r\n"
      },
      {
        "user": "tomgeorgekap",
        "body": "how to run with llama3\r\n"
      },
      {
        "user": "c4801725870",
        "body": "If you look at the \"brains\" table there is a \"model\" column and they don't give you a way to change the model initially - the code will default to gpt-3.5-turbo-0125 if this value is NULL. so put \"ollama/llama2\" in the row for your brain under the model column. unfortunately i get a new error, openai.NotFoundError: 404 page not found... it looks like it is still trying to use openAI for some reason. \r\n\r\nedit: the reason was the API base url is different for actually chatting with the bot. look at my post below for the solution. i don't know if this was a coincidence because ollama2 API is now compatible with openai format. And at first glance it looks like the code is still hardcoded to use ChatOpenAI, see backend/core/quivr_core/llm/llm_endpoint.py\r\n`\r\n_llm = ChatOpenAI(\r\n                model=config.model,\r\n                api_key=SecretStr(config.llm_api_key) if config.llm_api_key else None,\r\n                base_url=config.llm_base_url,\r\n            )\r\n`\r\n\r\nit works anyways, hopefully the authors don't intentionally keep breaking the ollama functionality."
      },
      {
        "user": "c4801725870",
        "body": "I got further along, the 404 error is from quivr_api/modules/rag_service/rag_service.py, change line 92 and add + \"/v1\" to ollama_url in the llm_base_url assignment.\r\n\r\n                llm_base_url=ollama_url + \"/v1\",\r\n\r\nnow I get a different error, it just says the model is not found\r\n\r\n\r\n | openai.NotFoundError: Error code: 404 - {'error': {'message': 'model \"ollama/llama2\" not found, try pulling it first', 'type': 'api_error', 'param': None, 'code': None}}\r\n\r\nso the API working, we just sent it a bad model. it probably expects \"llama2\"\r\n\r\nso change line 91 \r\n\r\n                model=\"llama2\",\r\n\r\nhardcode it, who cares for now.\r\n\r\nthus, now, is working. \r\n\r\nit is complete\r\n\r\n![image](https://github.com/user-attachments/assets/344560a2-a079-44c6-8867-73ed52fc3314)\r\n\r\n\r\n"
      },
      {
        "user": "c4801725870",
        "body": "> real problem is getting it to work with llama3. how do we do that?\r\n\r\nas long as the API works the same you literally just type in llama3 wherever you put llama2 before. "
      },
      {
        "user": "PFLigthart",
        "body": "@c4801725870 I have tried and still cannot seem to get it to work. Here is my complete process I just followed:\r\n\r\n- Already have ollama installed and have llama3.1 locally which works and is running in a terminal\r\n\r\n- Download docker desktop (I got 4.32.0)\r\n- git clone https://github.com/quivrhq/quivr.git && cd quivr\r\n- cp .env.example .env\r\n- vim .env\r\n- Comment OPENAI_API_KEY, uncomment Ollama\r\n- vim docker-compose.yml\r\n- Change line 53 from backend-base:latest -->stangirard/quivr-backend-prebuilt:latest\r\n- cd backend && supabase start\r\n- cd ../\r\n- docker compose pull\r\n- docker compose up\r\n- Go to localhost:3000\r\n- Sign in with admin@quivr.app -> password: admin\r\n- Create username and fill in the details on the Welcome to quivr popup\r\n- Go to localhost:54323\r\n- Update user_settings table models column to read [\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\", \"ollama/llama3.1\"]\r\n- Go to quivr/api/quivr_api/modules/rag_service/rag_service.py, change line 92 and add + \"/v1\" to ollama_url in the llm_base_url assignment. (as indicated above)\r\n- Also change line 91 to read model=\"llama3.1\"\r\n- Create a new brain using the auto-popup after the first sign-in\r\n- Brain seems to be created correctly with a green popup confirmation (did change the brain to use ollama/llama3.1 from drop down)\r\n- Data upload also seems to work correctly with a green popup\r\n- Asking a question results in a Red popup \"TypeError: network error\"\r\n- Start of terminal error:\r\nbackend-core  | [INFO] quivr_api.modules.chat.controller.chat_routes [chat_routes.py:212]: Creating question for chat 9c1eb*******814b with brain 926c******6a70 of type <class 'uuid.UUID'>\r\nbackend-core  | [INFO] quivr_api.modules.chat.controller.chat.utils [utils.py:79]: Model to use: name='gpt-3.5-turbo-0125' price=1 max_input=4000 max_output=1000\r\nbackend-core  | [INFO] quivr_api.modules.user.service.user_usage [user_usage.py:97]: User admin@quivr.app request count updated to 5\r\n\r\nSeems to still want to use gpt-3.5. Any help is appreciated.\r\nCheers,"
      },
      {
        "user": "c4801725870",
        "body": "> @c4801725870 I have tried and still cannot seem to get it to work. Here is my complete process I just followed:\r\n> \r\n> * Already have ollama installed and have llama3.1 locally which works and is running in a terminal\r\n> * Download docker desktop (I got 4.32.0)\r\n> * git clone https://github.com/quivrhq/quivr.git && cd quivr\r\n> * cp .env.example .env\r\n> * vim .env\r\n> * Comment OPENAI_API_KEY, uncomment Ollama\r\n> * vim docker-compose.yml\r\n> * Change line 53 from backend-base:latest -->stangirard/quivr-backend-prebuilt:latest\r\n> * cd backend && supabase start\r\n> * cd ../\r\n> * docker compose pull\r\n> * docker compose up\r\n> * Go to localhost:3000\r\n> * Sign in with [admin@quivr.app](mailto:admin@quivr.app) -> password: admin\r\n> * Create username and fill in the details on the Welcome to quivr popup\r\n> * Go to localhost:54323\r\n> * Update user_settings table models column to read [\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\", \"ollama/llama3.1\"]\r\n> * Go to quivr/api/quivr_api/modules/rag_service/rag_service.py, change line 92 and add + \"/v1\" to ollama_url in the llm_base_url assignment. (as indicated above)\r\n> * Also change line 91 to read model=\"llama3.1\"\r\n> * Create a new brain using the auto-popup after the first sign-in\r\n> * Brain seems to be created correctly with a green popup confirmation (did change the brain to use ollama/llama3.1 from drop down)\r\n> * Data upload also seems to work correctly with a green popup\r\n> * Asking a question results in a Red popup \"TypeError: network error\"\r\n> * Start of terminal error:\r\n>   backend-core  | [INFO] quivr_api.modules.chat.controller.chat_routes [chat_routes.py:212]: Creating question for chat 9c1eb***814b with brain 926c**6a70 of type <class 'uuid.UUID'>\r\n>   backend-core  | [INFO] quivr_api.modules.chat.controller.chat.utils [utils.py:79]: Model to use: name='gpt-3.5-turbo-0125' price=1 max_input=4000 max_output=1000\r\n>   backend-core  | [INFO] quivr_api.modules.user.service.user_usage [user_usage.py:97]: User [admin@quivr.app](mailto:admin@quivr.app) request count updated to 5\r\n> \r\n> Seems to still want to use gpt-3.5. Any help is appreciated. Cheers,\r\n\r\nmy user_settings only includes ollama\r\n\r\n![image](https://github.com/user-attachments/assets/59b1572f-d0ab-483d-9d37-9bb222373fb4)\r\n\r\nalso, in quivr_api/modules/chat/controller/chat/utils.py change line 45\r\n\r\n`\r\n    default_model = \"ollama/llama2\"\r\n`\r\nor whatever you want. \r\n\r\nas you can see they hardcoded it to the gpt-3.5 and there is no consideration for other models. "
      },
      {
        "user": "c4801725870",
        "body": "@PFLigthart i took another look to see if i can make it so i can switch models. Unfortunately we run into the issue that the \"default model\" referenced in quivr_api/modules/chat/controller/chat/utils.py is called upon brain creation, because the row in the Brains table defaults \"model\" to NULL. Also I thought it would be worth mentioning, you need to add the model to the models table, just adding a row for the new model.\r\n\r\nI proved this by changing the code to use \"brain_model\" rather than \"default_model\". \r\nI created a new brain, but it submitted the data to the wrong model(it used the default model)\r\n\r\n`# If brain.model is None, set it to the default_model\r\n    if brain_model is None:\r\n        brain_model = default_model\r\n        \r\n    model_to_use = LLMModel(  # TODO Implement default models in database\r\n        name=brain_model, price=1, max_input=4000, max_output=1000\r\n    )\r\n\r\n    logger.debug(\"Brain model: %s\", brain_model)\r\n\r\n\r\n    is_brain_model_available = any(\r\n        brain_model == model_dict.get(\"name\") for model_dict in models_settings\r\n    )\r\n\r\n    is_user_allowed_model = brain_model in user_settings.get(\r\n        \"models\", [brain_model]\r\n    )  # Checks if the model is available in the list of models\r\n\r\n    logger.debug(f\"Brain model: {brain_model}\")\r\n    logger.debug(f\"User models: {user_settings.get('models', [])}\")\r\n    logger.debug(f\"Model available: {is_brain_model_available}\")\r\n    logger.debug(f\"User allowed model: {is_user_allowed_model}\")\r\n\r\n    if is_brain_model_available and is_user_allowed_model:\r\n        # Use the model from the brain\r\n        model_to_use.name = brain_model\r\n        for model_dict in models_settings:\r\n            if model_dict.get(\"name\") == model_to_use.name:\r\n                model_to_use.price = model_dict.get(\"price\")\r\n                model_to_use.max_input = model_dict.get(\"max_input\")\r\n                model_to_use.max_output = model_dict.get(\"max_output\")\r\n                break\r\n\r\n    logger.info(f\"Model to use: {model_to_use}\")\r\n\r\n    return model_to_use`\r\n\r\nfor the quivr_api/modules/rag_service/rag_service.py change line 91 to \r\n`\r\n                model=self.model_to_use.name.split('/')[1],\r\n`\r\nthis last part will eliminate having to modify this code in the future. Now you just need to edit the default model when creating a new brain. \r\n\r\nThe bad news I have so far is the new code seems really bad at tokenizing the data input when the brain is created. This project needs a lot of work. "
      },
      {
        "user": "PFLigthart",
        "body": "@c4801725870 this worked. Thank you so much. I was also able to get it to work with llama3 and llama3.1 by hardcoding the relevant entries. Appreciate it.\nCheers."
      },
      {
        "user": "caishanli",
        "body": "> > @c4801725870 I have tried and still cannot seem to get it to work. Here is my complete process I just followed:\r\n> > \r\n> > * Already have ollama installed and have llama3.1 locally which works and is running in a terminal\r\n> > * Download docker desktop (I got 4.32.0)\r\n> > * git clone https://github.com/quivrhq/quivr.git && cd quivr\r\n> > * cp .env.example .env\r\n> > * vim .env\r\n> > * Comment OPENAI_API_KEY, uncomment Ollama\r\n> > * vim docker-compose.yml\r\n> > * Change line 53 from backend-base:latest -->stangirard/quivr-backend-prebuilt:latest\r\n> > * cd backend && supabase start\r\n> > * cd ../\r\n> > * docker compose pull\r\n> > * docker compose up\r\n> > * Go to localhost:3000\r\n> > * Sign in with [admin@quivr.app](mailto:admin@quivr.app) -> password: admin\r\n> > * Create username and fill in the details on the Welcome to quivr popup\r\n> > * Go to localhost:54323\r\n> > * Update user_settings table models column to read [\"gpt-3.5-turbo-0125\", \"gpt-4-0125-preview\", \"ollama/llama3.1\"]\r\n> > * Go to quivr/api/quivr_api/modules/rag_service/rag_service.py, change line 92 and add + \"/v1\" to ollama_url in the llm_base_url assignment. (as indicated above)\r\n> > * Also change line 91 to read model=\"llama3.1\"\r\n> > * Create a new brain using the auto-popup after the first sign-in\r\n> > * Brain seems to be created correctly with a green popup confirmation (did change the brain to use ollama/llama3.1 from drop down)\r\n> > * Data upload also seems to work correctly with a green popup\r\n> > * Asking a question results in a Red popup \"TypeError: network error\"\r\n> > * Start of terminal error:\r\n> >   backend-core  | [INFO] quivr_api.modules.chat.controller.chat_routes [chat_routes.py:212]: Creating question for chat 9c1eb***814b with brain 926c**6a70 of type <class 'uuid.UUID'>\r\n> >   backend-core  | [INFO] quivr_api.modules.chat.controller.chat.utils [utils.py:79]: Model to use: name='gpt-3.5-turbo-0125' price=1 max_input=4000 max_output=1000\r\n> >   backend-core  | [INFO] quivr_api.modules.user.service.user_usage [user_usage.py:97]: User [admin@quivr.app](mailto:admin@quivr.app) request count updated to 5\r\n> > \r\n> > Seems to still want to use gpt-3.5. Any help is appreciated. Cheers,\r\n> \r\n> my user_settings only includes ollama\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/129828180/353254807-59b1572f-d0ab-483d-9d37-9bb222373fb4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjQxNzI1MDEsIm5iZiI6MTcyNDE3MjIwMSwicGF0aCI6Ii8xMjk4MjgxODAvMzUzMjU0ODA3LTU5YjE1NzJmLWQwYWItNDgzZC05ZDM3LTliYjIyMjM3M2ZiNC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwODIwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDgyMFQxNjQzMjFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03YTFlMDdlYzAxYjYyNzM5ZDI3NDIwOTQ2NmQyNWYzNDgwZjNhMDQzNzVjMDdjNmMwNDE5MzI5MmVlZmEzNmZjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.wL3oaBs8aK_pR0b9rnjCjQTFL9-GFgSJAIt-Urc9cjg)\r\n> \r\n> also, in quivr_api/modules/chat/controller/chat/utils.py change line 45\r\n> \r\n> `default_model = \"ollama/llama2\"` or whatever you want.\r\n> \r\n> as you can see they hardcoded it to the gpt-3.5 and there is no consideration for other models.\r\n\r\nthe lastest version has no models column in user_settings table, but has a new table named models. So I add ollama mistral model to this table, but when I send ask a question in quivr, the error is \"TypeError : network error\", and in ollama log, it received a post request with uri of \"/chat/completions\", post data is ok with model name \"mistra\", but as [https://github.com/ollama/ollama?tab=readme-ov-file#chat-with-a-model](ollama chat-with-a-model) says, quivr should post to uri of \"/api/chat\". So is the uri \"/chat/completions\" correct?"
      },
      {
        "user": "c4801725870",
        "body": "@caishanli need to make sure the model in models table has ollama/ as part of the string, literally ollama/mistral for example. While reading the code i notice it only detects ollama if it actually finds ollama as part of the model string. Simply putting mistral for model will detect as GPT and probably use wrong API which you experienced. This is why line 91 need to use string.split method to extract model type. If I just changed to llama3 instead of ollama/llama3 it would not hit that branch of code... Again this project needs a lot of work and you need to carefully examine the code to get it functioning. It is not in the author's interest to produce a repo that allows full offline functionality out of the box for free. "
      },
      {
        "user": "caishanli",
        "body": "> @caishanli need to make sure the model in models table has ollama/ as part of the string, literally ollama/mistral for example. While reading the code i notice it only detects ollama if it actually finds ollama as part of the model string. Simply putting mistral for model will detect as GPT and probably use wrong API which you experienced. This is why line 91 need to use string.split method to extract model type. If I just changed to llama3 instead of ollama/llama3 it would not hit that branch of code... Again this project needs a lot of work and you need to carefully examine the code to get it functioning. It is not in the author's interest to produce a repo that allows full offline functionality out of the box for free.\r\n\r\nThank you for your reply! I tested ollama/mistral, the result is the same. The main problem is that quivr send wrong post data to ollama, shortly quivr use the wrong ollama rest api, so ollama can't reply correctly. \r\nAnd code is not same as the , so how did the pull requester test this code?"
      },
      {
        "user": "yukha-dw",
        "body": "> edit: the reason was the API base url is different for actually chatting with the bot. look at my post below for the solution. i don't know if this was a coincidence because ollama2 API is now compatible with openai format. And at first glance it looks like the code is still hardcoded to use ChatOpenAI, see backend/core/quivr_core/llm/llm_endpoint.py\r\n_llm = ChatOpenAI( model=config.model, api_key=SecretStr(config.llm_api_key) if config.llm_api_key else None, base_url=config.llm_base_url, )\r\n\r\nmine does not working so I have to import `ChatOllama from langchain_community.chat_models` then replace `_llm = ChatOpenAI(model=config.model` with `_llm = ChatOllama(model=\"llama2\"`. I haven't tested RAG functionality, it could be broken too\r\n\r\n> Go to quivr/api/quivr_api/modules/rag_service/rag_service.py, change line 92 and add + \"/v1\" to ollama_url in the llm_base_url assignment. (as indicated above)\r\n\r\nthis change breaks brain\r\n\r\nollama: 0.3.9\r\nmodel: llama2"
      },
      {
        "user": "aishock",
        "body": "The Supabase has completely new interface now and I failed to configure llamma3.1 using the new Supabase. The column of \"models\" are removed from user settings and I have to add in Ollama to \"models\" table, but I can't manage to get it to work. Does anyone succeed to do that using the new Supabase?"
      },
      {
        "user": "yukha-dw",
        "body": "@aishock I believe I'm using same version as you. I also update the model on \"models\" table instead. But some of the codes need to be changed because they are hardcoded to OpenAI. \r\n\r\n### Changes:\r\n**.env.example**\r\nUpdate Embedding dimension (llama2 is 4096) from:\r\n```\r\nEMBEDDING_DIM=1536\r\n```\r\nto:\r\n```\r\nEMBEDDING_DIM=4096\r\n```\r\n\r\n**backend/api/quivr_api/modules/rag_service/rag_service.py**\r\nUpdate `LLMEndpointConfig` from:\r\n```PY\r\nllm_config=LLMEndpointConfig(\r\n    model=self.model_to_use,  # type: ignore\r\n    llm_base_url=model.endpoint_url,\r\n    llm_api_key=api_key,\r\n```\r\nto\r\n```PY\r\nllm_config=LLMEndpointConfig(\r\n    model=\"llama2\", # self.model_to_use,  # type: ignore\r\n    llm_base_url= \"http://host.docker.internal:11434\", # model.endpoint_url,\r\n    llm_api_key=api_key,\r\n```\r\n\r\n**backend/core/quivr_core/llm/llm_endpoint.py**\r\nAdd Import `ChatOllama`, from:\r\n```PY\r\nfrom langchain_openai import AzureChatOpenAI, ChatOpenAI\r\n```\r\nto\r\n```PY\r\nfrom langchain_openai import AzureChatOpenAI, ChatOpenAI\r\nfrom langchain_community.chat_models import ChatOllama\r\n```\r\n\r\nUpdate `ChatOpenAI` from:\r\n```PY\r\n_llm = ChatOpenAI(\r\n    model=config.model,\r\n    api_key=SecretStr(config.llm_api_key)\r\n```\r\nto:\r\n```PY\r\n_llm = ChatOllama(\r\n    model=\"llama2\", #config.model,\r\n    api_key=SecretStr(config.llm_api_key)\r\n```\r\n\r\n**backend/supabase/config.toml**\r\nDisable analytics (somehow it does not working for me), from:\r\n```\r\n[analytics]\r\nenabled = true\r\n```\r\nto:\r\n```\r\n[analytics]\r\nenabled = false\r\n```\r\n\r\n**backend/supabase/seed.sql**\r\nDo not insert data to `public.brains` and `public.brain_users` (I'm too lazy to update accordingly), delete these lines:\r\n```SQL\r\nINSERT INTO \"public\".\"brains\"...\r\nINSERT INTO \"public\".\"brains_users\"...\r\n```\r\n\r\nUpdate `public.models` from:\r\n```SQL\r\nINSERT INTO \"public\".\"models\" (\"name\", \"price\", \"max_input\", \"max_output\", \"description\", \"display_name\", \"image_url\", \"default\", \"endpoint_url\", \"env_variable_name\") VALUES\r\n\t('gpt-4-0125-preview', 1, 4000, 4000, 'Default Description', 'GPT4', 'https://quivr-cms.s3.eu-west-3.amazonaws.com/logo_quivr_white_7e3c72620f.png', false, 'https://api.openai.com/v1', 'OPENAI_API_KEY'),\r\n\t('gpt-3.5-turbo-0125', 1, 10000, 1000, 'Default Description', 'GPT-3.5', 'https://quivr-cms.s3.eu-west-3.amazonaws.com/logo_quivr_white_7e3c72620f.png', true, 'https://api.openai.com/v1', 'OPENAI_API_KEY');\r\n```\r\nto:\r\n```SQL\r\nINSERT INTO \"public\".\"models\" (\"name\", \"price\", \"max_input\", \"max_output\", \"description\", \"display_name\", \"image_url\", \"default\", \"endpoint_url\", \"env_variable_name\") VALUES\r\n\t('ollama/llama2', 1, 10000, 1000, 'Default Description', 'ollama/llama2', 'https://quivr-cms.s3.eu-west-3.amazonaws.com/logo_quivr_white_7e3c72620f.png', true, 'http://host.docker.internal:11434', 'OLLAMA_API_BASE_URL');\r\n```\r\n\r\n**backend/supabase/migrations/20240103173626_init.sql** and **backend/supabase/migrations/20240103175048_prod.sql**\r\nUpdate all `vector(1536)` or `VECTOR(1536)` to `vector(4096)` to `VECTOR(4096)`\r\n\r\nAfter changing the code, you need to rebuild the image and supabase"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3059,
    "title": "[Backend] - Possibility to copy / paster file in multiple folders.",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-08-22T14:43:21Z",
    "updated_at": "2024-12-16T08:09:05Z",
    "labels": [
      "Stale",
      "area: backend",
      "Feature"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-59/[backend]-possibility-to-paste-copy-a-file-to-add-it-to-multiple\">ENT-59 [Backend] - Possibility to paste / copy a file to add it to multiple folders. Possibility to change folderId of a file</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3186,
    "title": "[Backend] Get Sync file source_link",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-11T08:33:29Z",
    "updated_at": "2024-12-15T20:06:19Z",
    "labels": [
      "Stale",
      "area: backend"
    ],
    "body": "Listing sync file returned from backend should have a source and source_link associated. This is useful for frontend to display them",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-71/[backend]-get-sync-file-routes-add-source\">ENT-71 [Backend] Get Sync file routes add source</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3196,
    "title": "[Bug]: windows11 wsl2 docker compose up failed",
    "author": "caishanli",
    "state": "closed",
    "created_at": "2024-09-11T15:42:36Z",
    "updated_at": "2024-12-15T20:06:18Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### What happened?\n\ndocker compose up failed, only redis and web started, beat worker flower notifier backend-api start failed.\n\n### Relevant log output\n\n```bash\ntest@test-pc:~/quivr$ docker compose up\r\nWARN[0000] /home/test/quivr/docker-compose.yml: `version` is obsolete\r\n[+] Running 7/0\r\n âœ” Container backend-api  Created                                                                                             0.0s\r\n âœ” Container redis        Created                                                                                             0.0s\r\n âœ” Container beat         Created                                                                                             0.0s\r\n âœ” Container worker       Created                                                                                             0.0s\r\n âœ” Container web          Created                                                                                             0.0s\r\n âœ” Container flower       Created                                                                                             0.0s\r\n âœ” Container notifier     Created                                                                                             0.0s\r\nAttaching to backend-api, beat, flower, notifier, redis, web, worker\r\nredis        | 1:C 11 Sep 2024 15:38:41.581 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\nredis        | 1:C 11 Sep 2024 15:38:41.581 * Redis version=7.2.3, bits=64, commit=00000000, modified=0, pid=1, just started\r\nredis        | 1:C 11 Sep 2024 15:38:41.581 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\r\nredis        | 1:M 11 Sep 2024 15:38:41.581 * monotonic clock: POSIX clock_gettime\r\nredis        | 1:M 11 Sep 2024 15:38:41.582 * Running mode=standalone, port=6379.\r\nredis        | 1:M 11 Sep 2024 15:38:41.583 * Server initialized\r\nredis        | 1:M 11 Sep 2024 15:38:41.583 * Loading RDB produced by version 7.2.3\r\nredis        | 1:M 11 Sep 2024 15:38:41.583 * RDB age 1664 seconds\r\nredis        | 1:M 11 Sep 2024 15:38:41.583 * RDB memory usage when created 0.83 Mb\r\nredis        | 1:M 11 Sep 2024 15:38:41.583 * Done loading RDB, keys loaded: 0, keys expired: 0.\r\nredis        | 1:M 11 Sep 2024 15:38:41.583 * DB loaded from disk: 0.000 seconds\r\nredis        | 1:M 11 Sep 2024 15:38:41.583 * Ready to accept connections tcp\r\nbackend-api  | INFO:     Will watch for changes in these directories: ['/app']\r\nbackend-api  | INFO:     Uvicorn running on http://0.0.0.0:5050 (Press CTRL+C to quit)\r\nbackend-api  | INFO:     Started reloader process [1] using StatReload\r\nweb          |    â–² Next.js 14.1.0\r\nweb          |    - Local:        http://localhost:3000\r\nweb          |    - Network:      http://0.0.0.0:3000\r\nweb          |\r\nweb          |  âœ“ Ready in 80ms\r\nbeat         | Usage: python -m celery [OPTIONS] COMMAND [ARGS]...\r\nbeat         | Try 'python -m celery --help' for help.\r\nbeat         |\r\nbeat         | Error: Invalid value for '-A' / '--app':\r\nbeat         | Unable to load celery application.\r\nbeat         | While trying to load the module quivr_worker.celery_worker the following error occurred:\r\nbeat         | Traceback (most recent call last):\r\nbeat         |   File \"/usr/local/lib/python3.11/site-packages/celery/bin/celery.py\", line 58, in convert\r\nbeat         |     return find_app(value)\r\nbeat         |            ^^^^^^^^^^^^^^^\r\nbeat         |   File \"/usr/local/lib/python3.11/site-packages/celery/app/utils.py\", line 386, in find_app\r\nbeat         |     sym = imp(app)\r\nbeat         |           ^^^^^^^^\r\nbeat         |   File \"/usr/local/lib/python3.11/site-packages/celery/utils/imports.py\", line 109, in import_from_cwd\r\nbeat         |     return imp(module, package=package)\r\nbeat         |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbeat         |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbeat         |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbeat         |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbeat         |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbeat         |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbeat         |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbeat         |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbeat         |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbeat         |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbeat         |   File \"/app/worker/quivr_worker/celery_worker.py\", line 10, in <module>\r\nbeat         |     from quivr_api.models.settings import settings\r\nbeat         |   File \"/app/api/quivr_api/models/settings.py\", line 144, in <module>\r\nbeat         |     settings = BrainSettings()  # type: ignore\r\nbeat         |                ^^^^^^^^^^^^^^^\r\nbeat         |   File \"/usr/local/lib/python3.11/site-packages/pydantic_settings/main.py\", line 144, in __init__\r\nbeat         |     super().__init__(\r\nbeat         |   File \"/usr/local/lib/python3.11/site-packages/pydantic/main.py\", line 193, in __init__\r\nbeat         |     self.__pydantic_validator__.validate_python(data, self_instance=self)\r\nbeat         | pydantic_core._pydantic_core.ValidationError: 1 validation error for BrainSettings\r\nbeat         | embedding_dim\r\nbeat         |   Field required [type=missing, input_value={'openai_api_key': 'CHANG...nternal:54322/postgres'}, input_type=dict]\r\nbeat         |     For further information visit https://errors.pydantic.dev/2.8/v/missing\r\nbeat         |\r\nworker       | Usage: python -m celery [OPTIONS] COMMAND [ARGS]...\r\nworker       | Try 'python -m celery --help' for help.\r\nworker       |\r\nworker       | Error: Invalid value for '-A' / '--app':\r\nworker       | Unable to load celery application.\r\nworker       | While trying to load the module quivr_worker.celery_worker the following error occurred:\r\nworker       | Traceback (most recent call last):\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/celery/bin/celery.py\", line 58, in convert\r\nworker       |     return find_app(value)\r\nworker       |            ^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/celery/app/utils.py\", line 386, in find_app\r\nworker       |     sym = imp(app)\r\nworker       |           ^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/celery/utils/imports.py\", line 109, in import_from_cwd\r\nworker       |     return imp(module, package=package)\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nworker       |     return _bootstrap._gcd_import(name[level:], package, level)\r\nworker       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker       |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nworker       |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nworker       |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nworker       |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nworker       |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nworker       |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nworker       |   File \"/app/worker/quivr_worker/celery_worker.py\", line 10, in <module>\r\nworker       |     from quivr_api.models.settings import settings\r\nworker       |   File \"/app/api/quivr_api/models/settings.py\", line 144, in <module>\r\nworker       |     settings = BrainSettings()  # type: ignore\r\nworker       |                ^^^^^^^^^^^^^^^\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/pydantic_settings/main.py\", line 144, in __init__\r\nworker       |     super().__init__(\r\nworker       |   File \"/usr/local/lib/python3.11/site-packages/pydantic/main.py\", line 193, in __init__\r\nworker       |     self.__pydantic_validator__.validate_python(data, self_instance=self)\r\nworker       | pydantic_core._pydantic_core.ValidationError: 1 validation error for BrainSettings\r\nworker       | embedding_dim\r\nworker       |   Field required [type=missing, input_value={'openai_api_key': 'CHANG...nternal:54322/postgres'}, input_type=dict]\r\nworker       |     For further information visit https://errors.pydantic.dev/2.8/v/missing\r\nworker       |\r\nflower       | Usage: python -m celery [OPTIONS] COMMAND [ARGS]...\r\nflower       | Try 'python -m celery --help' for help.\r\nflower       |\r\nflower       | Error: Invalid value for '-A' / '--app':\r\nflower       | Unable to load celery application.\r\nflower       | While trying to load the module quivr_worker.celery_worker the following error occurred:\r\nflower       | Traceback (most recent call last):\r\nflower       |   File \"/usr/local/lib/python3.11/site-packages/celery/bin/celery.py\", line 58, in convert\r\nflower       |     return find_app(value)\r\nflower       |            ^^^^^^^^^^^^^^^\r\nflower       |   File \"/usr/local/lib/python3.11/site-packages/celery/app/utils.py\", line 386, in find_app\r\nflower       |     sym = imp(app)\r\nflower       |           ^^^^^^^^\r\nflower       |   File \"/usr/local/lib/python3.11/site-packages/celery/utils/imports.py\", line 109, in import_from_cwd\r\nflower       |     return imp(module, package=package)\r\nflower       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nflower       |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nflower       |     return _bootstrap._gcd_import(name[level:], package, level)\r\nflower       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nflower       |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nflower       |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nflower       |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nflower       |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nflower       |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nflower       |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nflower       |   File \"/app/worker/quivr_worker/celery_worker.py\", line 10, in <module>\r\nflower       |     from quivr_api.models.settings import settings\r\nflower       |   File \"/app/api/quivr_api/models/settings.py\", line 144, in <module>\r\nflower       |     settings = BrainSettings()  # type: ignore\r\nflower       |                ^^^^^^^^^^^^^^^\r\nflower       |   File \"/usr/local/lib/python3.11/site-packages/pydantic_settings/main.py\", line 144, in __init__\r\nflower       |     super().__init__(\r\nflower       |   File \"/usr/local/lib/python3.11/site-packages/pydantic/main.py\", line 193, in __init__\r\nflower       |     self.__pydantic_validator__.validate_python(data, self_instance=self)\r\nflower       | pydantic_core._pydantic_core.ValidationError: 1 validation error for BrainSettings\r\nflower       | embedding_dim\r\nflower       |   Field required [type=missing, input_value={'openai_api_key': 'CHANG...nternal:54322/postgres'}, input_type=dict]\r\nflower       |     For further information visit https://errors.pydantic.dev/2.8/v/missing\r\nflower       |\r\nnotifier     | Traceback (most recent call last):\r\nnotifier     |   File \"/app/worker/quivr_worker/celery_monitor.py\", line 11, in <module>\r\nnotifier     |     from quivr_api.modules.dependencies import async_engine\r\nnotifier     |   File \"/app/api/quivr_api/modules/dependencies.py\", line 22, in <module>\r\nnotifier     |     from quivr_api.models.settings import BrainSettings\r\nnotifier     |   File \"/app/api/quivr_api/models/settings.py\", line 144, in <module>\r\nnotifier     |     settings = BrainSettings()  # type: ignore\r\nnotifier     |                ^^^^^^^^^^^^^^^\r\nnotifier     |   File \"/usr/local/lib/python3.11/site-packages/pydantic_settings/main.py\", line 144, in __init__\r\nnotifier     |     super().__init__(\r\nnotifier     |   File \"/usr/local/lib/python3.11/site-packages/pydantic/main.py\", line 193, in __init__\r\nnotifier     |     self.__pydantic_validator__.validate_python(data, self_instance=self)\r\nnotifier     | pydantic_core._pydantic_core.ValidationError: 1 validation error for BrainSettings\r\nnotifier     | embedding_dim\r\nnotifier     |   Field required [type=missing, input_value={'openai_api_key': 'CHANG...nternal:54322/postgres'}, input_type=dict]\r\nnotifier     |     For further information visit https://errors.pydantic.dev/2.8/v/missing\r\nbackend-api  | Process SpawnProcess-1:\r\nbackend-api  | Traceback (most recent call last):\r\nbackend-api  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-api  |     self.run()\r\nbackend-api  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-api  |     self._target(*self._args, **self._kwargs)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-api  |     target(sockets=sockets)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-api  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-api  |     return runner.run(main)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-api  |     return self._loop.run_until_complete(task)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-api  |     await self._serve(sockets)\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-api  |     config.load()\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-api  |     self.loaded_app = import_from_string(self.app)\r\nbackend-api  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-api  |     module = importlib.import_module(module_str)\r\nbackend-api  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-api  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-api  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-api  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-api  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-api  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-api  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-api  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-api  |   File \"/app/api/quivr_api/main.py\", line 15, in <module>\r\nbackend-api  |     from quivr_api.modules.analytics.controller.analytics_routes import analytics_router\r\nbackend-api  |   File \"/app/api/quivr_api/modules/analytics/controller/analytics_routes.py\", line 4, in <module>\r\nbackend-api  |     from quivr_api.middlewares.auth.auth_bearer import AuthBearer, get_current_user\r\nbackend-api  |   File \"/app/api/quivr_api/middlewares/auth/__init__.py\", line 1, in <module>\r\nbackend-api  |     from .auth_bearer import AuthBearer, get_current_user\r\nbackend-api  |   File \"/app/api/quivr_api/middlewares/auth/auth_bearer.py\", line 10, in <module>\r\nbackend-api  |     from quivr_api.modules.api_key.service.api_key_service import ApiKeyService\r\nbackend-api  |   File \"/app/api/quivr_api/modules/api_key/service/api_key_service.py\", line 6, in <module>\r\nbackend-api  |     from quivr_api.modules.api_key.repository.api_keys import ApiKeys\r\nbackend-api  |   File \"/app/api/quivr_api/modules/api_key/repository/api_keys.py\", line 7, in <module>\r\nbackend-api  |     from quivr_api.modules.dependencies import get_supabase_client\r\nbackend-api  |   File \"/app/api/quivr_api/modules/dependencies.py\", line 22, in <module>\r\nbackend-api  |     from quivr_api.models.settings import BrainSettings\r\nbackend-api  |   File \"/app/api/quivr_api/models/settings.py\", line 144, in <module>\r\nbackend-api  |     settings = BrainSettings()  # type: ignore\r\nbackend-api  |                ^^^^^^^^^^^^^^^\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/pydantic_settings/main.py\", line 144, in __init__\r\nbackend-api  |     super().__init__(\r\nbackend-api  |   File \"/usr/local/lib/python3.11/site-packages/pydantic/main.py\", line 193, in __init__\r\nbackend-api  |     self.__pydantic_validator__.validate_python(data, self_instance=self)\r\nbackend-api  | pydantic_core._pydantic_core.ValidationError: 1 validation error for BrainSettings\r\nbackend-api  | embedding_dim\r\nbackend-api  |   Field required [type=missing, input_value={'openai_api_key': 'CHANG...nternal:54322/postgres'}, input_type=dict]\r\nbackend-api  |     For further information visit https://errors.pydantic.dev/2.8/v/missing\r\nbeat exited with code 0\r\nworker exited with code 0\r\nflower exited with code 0\r\nnotifier exited with code 0\r\n^CGracefully stopping... (press Ctrl+C again to force)\r\n[+] Stopping 7/7\r\n âœ” Container web          Stopped                                                                                             0.5s\r\n âœ” Container notifier     Stopped                                                                                             2.9s\r\n âœ” Container flower       Stopped                                                                                             2.7s\r\n âœ” Container backend-api  Stopped                                                                                             0.5s\r\n âœ” Container beat         Stopped                                                                                             4.0s\r\n âœ” Container worker       Stopped                                                                                             3.6s\r\n âœ” Container redis        Stopped                                                                                             0.5s\r\ncanceled\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-200/[bug]-windows11-wsl2-docker-compose-up-failed\">CORE-200 [Bug]: windows11 wsl2 docker compose up failed</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\n@caishanli Hello! I'm here to help you with bugs, questions, and becoming a contributor. If you need any assistance, feel free to ask.\n\n<!-- Answer -->\nThe `docker compose up` command failed to start the `beat`, `worker`, `flower`, `notifier`, and `backend-api` containers because the `embedding_dim` field is missing in the environment variables. This field is required for the `BrainSettings` model to initialize correctly.\n\nTo resolve this issue, ensure that the `embedding_dim` environment variable is set. Here are the required environment variables for the `BrainSettings` model:\n\n- `openai_api_key`\n- `azure_openai_embeddings_url`\n- `supabase_url`\n- `supabase_service_key`\n- `resend_api_key`\n- `resend_email_address`\n- `pg_database_url`\n- `pg_database_async_url`\n- `embedding_dim`\n\nMake sure all these variables are defined in your `.env` file or environment before running `docker compose up` again <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/models/settings.py)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "caishanli",
        "body": "> @caishanli Hello! I'm here to help you with bugs, questions, and becoming a contributor. If you need any assistance, feel free to ask.\r\n> \r\n> The `docker compose up` command failed to start the `beat`, `worker`, `flower`, `notifier`, and `backend-api` containers because the `embedding_dim` field is missing in the environment variables. This field is required for the `BrainSettings` model to initialize correctly.\r\n> \r\n> To resolve this issue, ensure that the `embedding_dim` environment variable is set. Here are the required environment variables for the `BrainSettings` model:\r\n> \r\n> * `openai_api_key`\r\n> * `azure_openai_embeddings_url`\r\n> * `supabase_url`\r\n> * `supabase_service_key`\r\n> * `resend_api_key`\r\n> * `resend_email_address`\r\n> * `pg_database_url`\r\n> * `pg_database_async_url`\r\n> * `embedding_dim`\r\n> \r\n> Make sure all these variables are defined in your `.env` file or environment before running `docker compose up` again [[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/models/settings.py).\r\n> \r\n> _To continue talking to [Dosu](https://dosu.dev), mention @dosu._\r\n\r\n.env file is copy from .env.example with nothing changed"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3185,
    "title": "Handle sync status in Front + limit to one account Notion integration",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-10T17:22:50Z",
    "updated_at": "2024-12-15T00:30:00Z",
    "labels": [
      "Stale",
      "area: frontend"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-70/handle-sync-status-in-front-limit-to-one-account-notion-integration\">ENT-70 Handle sync status in Front + limit to one account Notion integration</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3178,
    "title": "...",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-09-10T13:39:41Z",
    "updated_at": "2024-12-14T20:06:01Z",
    "labels": [
      "Stale"
    ],
    "body": "test",
    "comments": [
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3176,
    "title": "[Feature]: Enable Editing of Previous Messages in Chat Interface",
    "author": "TorstenC",
    "state": "closed",
    "created_at": "2024-09-09T18:53:26Z",
    "updated_at": "2024-12-13T20:08:21Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "### The Feature\n\nI propose that the Quivr web interface should provide the capability to edit previous messages from both the user and the agent before submitting (round-trip). This functionality should be similar to the feature available at `console.anthropic.com/workbench` and `platform.openai.com/playground/chat`.\n\n### Motivation, pitch\n\nThis feature significantly simplifies the chat dialogue and enhances collaboration efficiency with the language model (LLM). Here are two typical use cases that illustrate the advantages:\r\n\r\n1. **Clarification and Context Addition:** If I forget to specify that my question relates to Linux, I can update my original query with â€œin Linuxâ€ and remove the irrelevant part of the agentâ€™s response (e.g., related to Windows) before asking my follow-up question.\r\n   \r\n2. **Specific Adjustments in Responses:** If the response includes a placeholder for a file path such as \"your/path/to/config.yaml,\" I can replace this placeholder with the correct path. This ensures that all further code adjustments include the correct path.\r\n\r\nBy allowing the editing of previous messages, interactions become clearer and more precise, minimizing misunderstandings and making the workflow smoother.\r\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-198/[feature]-enable-editing-of-previous-messages-in-chat-interface\">CORE-198 [Feature]: Enable Editing of Previous Messages in Chat Interface</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey @TorstenC! I'm here to help you with any bugs, questions, or contributions. Let me know what you need assistance with!\n\n<!-- Answer -->\nTo enable the editing of previous messages in the Quivr web interface, you can leverage the existing `update_message_by_id` method in the `ChatRepository` class, which allows for updating a message by its ID <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/chat/repository/chats.py)</sup>. Hereâ€™s a high-level approach to implement this feature:\n\n### Backend Changes\n\n1. **Update Message Endpoint**:\n   - Create an API endpoint to handle the update requests for chat messages.\n   - Use the `update_message_by_id` method to update the message content in the database.\n\n```python\nfrom fastapi import APIRouter, HTTPException\nfrom uuid import UUID\nfrom pydantic import BaseModel\n\nclass UpdateMessageRequest(BaseModel):\n    message_id: UUID\n    new_content: str\n\nrouter = APIRouter()\n\n@router.put(\"/chat/{chat_id}/message\")\nasync def update_message(chat_id: UUID, request: UpdateMessageRequest):\n    try:\n        updated_message = chat_service.update_message_by_id(request.message_id, {\"content\": request.new_content})\n        return updated_message\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n```\n\n### Frontend Changes\n\n1. **UI for Editing Messages**:\n   - Add an edit button next to each message in the chat interface.\n   - When the edit button is clicked, replace the message text with a text input field pre-filled with the current message content.\n\n```tsx\nimport { useState } from \"react\";\n\nconst MessageRow = ({ message, onUpdate }) => {\n  const [isEditing, setIsEditing] = useState(false);\n  const [newContent, setNewContent] = useState(message.content);\n\n  const handleEdit = () => {\n    setIsEditing(true);\n  };\n\n  const handleSave = () => {\n    onUpdate(message.id, newContent);\n    setIsEditing(false);\n  };\n\n  return (\n    <div>\n      {isEditing ? (\n        <input\n          type=\"text\"\n          value={newContent}\n          onChange={(e) => setNewContent(e.target.value)}\n        />\n      ) : (\n        <span>{message.content}</span>\n      )}\n      <button onClick={handleEdit}>Edit</button>\n      {isEditing && <button onClick={handleSave}>Save</button>}\n    </div>\n  );\n};\n```\n\n2. **Handling Update Requests**:\n   - Implement a function to send the updated message content to the backend.\n\n```tsx\nimport { useState } from \"react\";\nimport axios from \"axios\";\n\nconst Chat = ({ chatId, messages }) => {\n  const [chatMessages, setChatMessages] = useState(messages);\n\n  const handleUpdateMessage = async (messageId, newContent) => {\n    try {\n      const response = await axios.put(`/chat/${chatId}/message`, {\n        message_id: messageId,\n        new_content: newContent,\n      });\n      setChatMessages((prevMessages) =>\n        prevMessages.map((msg) =>\n          msg.id === messageId ? { ...msg, content: newContent } : msg\n        )\n      );\n    } catch (error) {\n      console.error(\"Failed to update message\", error);\n    }\n  };\n\n  return (\n    <div>\n      {chatMessages.map((message) => (\n        <MessageRow key={message.id} message={message} onUpdate={handleUpdateMessage} />\n      ))}\n    </div>\n  );\n};\n```\n\n### Summary\n\nBy implementing these changes, you can enable the editing of previous messages in the Quivr web interface, enhancing the flexibility and accuracy of interactions with the language model. This feature will allow users to clarify and adjust their queries and responses, making the chat dialogue more efficient and effective.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3174,
    "title": "handle Notion specific integration in Frontend",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-09T12:17:18Z",
    "updated_at": "2024-12-13T16:06:46Z",
    "labels": [
      "Stale",
      "area: frontend"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-69/handle-notion-specific-integration-in-frontend\">ENT-69 handle Notion specific integration in Frontend</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3169,
    "title": "[Bug]: New install cannot log in over local network and frontend seems incomplete.",
    "author": "root-reindeer-flotilla",
    "state": "closed",
    "created_at": "2024-09-06T21:34:54Z",
    "updated_at": "2024-12-13T04:07:20Z",
    "labels": [
      "bug",
      "Stale",
      "area: frontend"
    ],
    "body": "### What happened?\n\nI installed quivr on my local windows machine with docker desktop. I got everything to work yesterday after a few hours. I decided to move my install to my server so I can access it over my network. I'm using proxmox to which I installed a debian container that has docker installed on it. I set up everything mostly the same but I'm getting this very weird frontend bug. I try to enter admin@quivr.app admin and login and nothing happens at all. \r\n\r\nThe supabase install works well, and even 10.10.10.101:5050/docs#/ works as I can visit them like normal.\r\n\r\nDoes anyone have any ideas?\r\n\r\n![image](https://github.com/user-attachments/assets/28971da9-4b6f-4032-bf32-ea694efcda9b)\r\n\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-197/[bug]-new-install-cannot-log-in-over-local-network-and-frontend-seems\">CORE-197 [Bug]: New install cannot log in over local network and frontend seems incomplete.</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHello @root-reindeer-flotilla! I'm here to assist you with any bugs or questions you have about Quivr. Let's get this sorted out together!\n\n<!-- Answer -->\nI found a similar open issue that might help you resolve your problem: [[Bug]: Not able to login. Dead home page](https://github.com/QuivrHQ/quivr/issues/2468). The suggested solution involves several steps:\n\n1. **Environment Configuration**: Ensure that the `NEXT_PUBLIC_FRONTEND_URL` in your environment configuration matches your application's public URL, including the correct protocol and port. For your setup, this means including `http://` and specifying port `3000` if that's what you're using.\n\n2. **Browser Settings**: Make sure JavaScript is enabled in your browser. Check the browser's console for any JavaScript errors that might reveal more about the issue.\n\n3. **Network Requests**: Use your browser's developer tools to monitor network requests when attempting to log in. Look for any requests that fail or return error responses.\n\n4. **Service Status**: Confirm that all related services, including Docker containers and Supabase, are running correctly. This involves checking that Docker containers are up and that Supabase services are accessible.\n\nAdditionally, another issue might be related to the configuration of your environment variables in the `.env` file. Specifically, the `NEXT_PUBLIC_BACKEND_URL` and `NEXT_PUBLIC_SUPABASE_URL` should be set to the IP address of your server instead of `localhost`. Here is the corrected configuration:\n\n```env\nNEXT_PUBLIC_BACKEND_URL=http://10.10.10.101:5050\nNEXT_PUBLIC_SUPABASE_URL=http://10.10.10.101:54321\n```\n\nMake sure to update these values in your `.env` file and restart the services for the changes to take effect <sup>[[1]](https://github.com/QuivrHQ/quivr/issues/2468)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/.env.example)</sup>.\n\nFor the frontend appearing incomplete, ensure that all necessary environment variables are correctly set and that your Docker configuration is accurate. Verify the `dockerfile` path and build context in the `docker-compose.yml` file. Also, check the deployment logs for any errors during the build and deployment process <sup>[[3]](https://github.com/QuivrHQ/quivr/blob/main/docs/deployment/porter.mdx)</sup>.\n\nBy following these steps, you should be able to identify and resolve the issues preventing login and causing the frontend to appear incomplete.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "root-reindeer-flotilla",
        "body": "@dosu (I am aware this is an AI)\r\n\r\nChanging those did not do anything.\r\n\r\nI think you misunderstood. It's still on my local network and the internal IP address is 10.10.10.101. The backend/supabase IP is still local to the frontend. eg http://10.10.10.101:5050 aka http://localhost:5050.\r\n\r\nMy working theory is that docker isn't properly forwarding ports from the internal containers but I could be completely wrong. Any other ideas to try?"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nThe issue with Docker not properly forwarding ports from the internal containers, preventing login and causing the frontend to appear incomplete, could be due to several reasons. Here are some potential causes and solutions:\n\n1. **Port Configuration**: Ensure that the ports are correctly mapped and not being blocked by any firewall or network settings. The `docker-compose.yml` file shows the following port mappings:\n    - Frontend: `3000:3000`\n    - Backend API: `5050:5050`\n    - Flower: `5555:5555`\n    - Redis: `6379:6379`\n    - Debug Port: `5678:5678` (from `docker-compose.dev.yml`)\n\n2. **Environment Variables**: The frontend service relies on several environment variables (`NEXT_PUBLIC_BACKEND_URL`, `NEXT_PUBLIC_SUPABASE_URL`, etc.). Ensure these variables are correctly set and accessible.\n\n3. **Network Configuration**: The services are connected via a custom bridge network (`quivr-network`). Ensure that this network is properly created and that there are no conflicts with other networks.\n\n4. **Service Dependencies**: The frontend service depends on the backend API service. Ensure that the backend API service is running and healthy. The health check for the backend API is configured to check `http://localhost:5050/healthz`.\n\n5. **Docker Host Configuration**: The `extra_hosts` configuration is used to map `host.docker.internal` to `host-gateway`. Ensure that this mapping is correct and that the host gateway is accessible.\n\n6. **Volume Mounts**: Ensure that the volume mounts are correctly set up, especially for services like `backend-api`, `notifier`, `worker`, `beat`, and `flower`, which have volumes mounted from the host to the container. This ensures that the latest code changes are reflected inside the containers.\n\nHere is the relevant part of the `docker-compose.yml` and `docker-compose.dev.yml` files for reference:\n\n```yml\nservices:\n  frontend:\n    image: quivr-frontend-prebuilt\n    pull_policy: never\n    build:\n      context: frontend\n      dockerfile: Dockerfile\n      args:\n        - NEXT_PUBLIC_ENV=local\n        - NEXT_PUBLIC_BACKEND_URL=${NEXT_PUBLIC_BACKEND_URL}\n        - NEXT_PUBLIC_SUPABASE_URL=${NEXT_PUBLIC_SUPABASE_URL}\n        - NEXT_PUBLIC_SUPABASE_ANON_KEY=${NEXT_PUBLIC_SUPABASE_ANON_KEY}\n        - NEXT_PUBLIC_CMS_URL=${NEXT_PUBLIC_CMS_URL}\n        - NEXT_PUBLIC_FRONTEND_URL=${NEXT_PUBLIC_FRONTEND_URL}\n        - NEXT_PUBLIC_AUTH_MODES=${NEXT_PUBLIC_AUTH_MODES}\n    container_name: web\n    depends_on:\n      - backend-api\n    restart: always\n    ports:\n      - 3000:3000\n\n  backend-api:\n    image: quivr-backend-api:latest\n    pull_policy: if_not_present\n    env_file:\n      - .env\n    build:\n      context: backend\n      dockerfile: Dockerfile\n    container_name: backend-api\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"http://localhost:5050/healthz\" ]\n    command: >\n      /bin/bash -c \"python -m uvicorn quivr_api.main:app --host 0.0.0.0 --log-level info --reload --port 5050 --loop uvloop\"\n    restart: always\n    ports:\n      - 5050:5050\n\n  notifier:\n    image: backend-api:latest\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    env_file:\n      - .env\n    container_name: notifier\n    volumes:\n      - ./backend/:/app/\n    command: >\n      /bin/bash -c \"python /app/worker/quivr_worker/celery_monitor.py\"\n    restart: always\n    depends_on:\n      - redis\n      - worker\n\n  worker:\n    image: backend-api:latest\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    env_file:\n      - .env\n    container_name: worker\n    volumes:\n      - ./backend/:/app/\n    command: >\n      /bin/bash -c \"python -m celery -A quivr_worker.celery_worker worker -l info -E\"\n    restart: always\n    depends_on:\n      - redis\n\n  redis:\n    image: redis:latest\n    container_name: redis\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    restart: always\n    ports:\n      - 6379:6379\n\n  beat:\n    image: backend-api:latest\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    env_file:\n      - .env\n    container_name: beat\n    command: >\n      /bin/bash -c \"python -m celery -A quivr_worker.celery_worker beat -l info\"\n    volumes:\n      - ./backend/:/app/\n    restart: always\n    depends_on:\n      - redis\n\n  flower:\n    image: backend-api:latest\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    env_file:\n      - .env\n    container_name: flower\n    command: >\n      /bin/bash -c \"python -m celery -A quivr_worker.celery_worker flower -l info --port=5555\"\n    volumes:\n      - ./backend/:/app/\n    restart: always\n    depends_on:\n      - redis\n      - worker\n      - beat\n    ports:\n      - 5555:5555\n```\n\nCheck these configurations and ensure that all services are running correctly and that the ports are accessible from your local network setup with IP `10.10.10.101` <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.yml)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.dev.yml)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "aishock",
        "body": "Hi, you mentioned that you once got everything worked on your local machine, do you mind sharing more details of what models you used (such as ChatGPT, Ollama) and how you add in the models to Quivr?"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3363,
    "title": "Add examples prompts in docs quivr",
    "author": "StanGirard",
    "state": "closed",
    "created_at": "2024-10-10T11:02:54Z",
    "updated_at": "2024-12-11T13:51:33Z",
    "labels": [
      "documentation",
      "area: docs"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-114/add-examples-prompts-in-docs-quivr\">ENT-114 Add examples prompts in docs quivr</a></p>"
      }
    ]
  },
  {
    "issue_number": 3364,
    "title": "Add example Quivr with pgvector",
    "author": "StanGirard",
    "state": "closed",
    "created_at": "2024-10-10T11:03:11Z",
    "updated_at": "2024-12-11T13:51:30Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-233/add-example-quivr-with-pgvector\">CORE-233 Add example Quivr with pgvector</a></p>"
      }
    ]
  },
  {
    "issue_number": 3459,
    "title": "Enable storage of retrieval configuration on PG",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-11-06T14:16:27Z",
    "updated_at": "2024-12-10T13:52:41Z",
    "labels": [],
    "body": "Currently, a single YAML file, present in the repository, contains the retrieval configuration used by all brains.\n\nThis will enable the storage of different retrieval configurations on PG.",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-287/enable-storage-of-retrieval-configuration-on-pg\">CORE-287 Enable storage of retrieval configuration on PG</a></p>"
      }
    ]
  },
  {
    "issue_number": 3160,
    "title": "[Bug]: Can't Add Ollama Using Supabase",
    "author": "aishock",
    "state": "closed",
    "created_at": "2024-09-05T09:56:27Z",
    "updated_at": "2024-12-10T08:08:29Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### What happened?\n\n**Background:**\r\nI'm installing Quivr locally on Ubuntu and I want to use llama3.1 in Ollama. \r\nI changed the URL in .env file using the internal IP address of the host machine,  \"OLLAMA_API_BASE_URL=http://xx.xx.xx.xxx:11434\" and I have checked that \"curl http://xx.xx.xx.xxx:11434/v1/models/llama3.1\" works fine in my terminal. \r\n\r\n**Problem:**\r\nI can't configure llama3.1 in Supabase. The browser always gives me \"TypeError: Network error when attempting to fetch resource\" when I talk to llama3.1. \r\nWhat I have done are:\r\n1. Go to Supabase table editor\r\n2. Find models\r\n3. Insert a row to fill in information of llama3.1 as follow:\r\nendpoint_url: http://xx.xx.xx.xxx:11434/v1/models/llama3.1\r\nenv_variable_name: empty\r\n\r\nPlease let me know how to configure Supabase and any insights will be appreciated. \n\n### Relevant log output\n\n```bash\naishock@aishock-Z790-GAMING-X-AX:~/quivr$ curl http://10.69.46.188:11434/v1/models/llama3.1\r\n{\"id\":\"llama3.1\",\"object\":\"model\",\"created\":1725504891,\"owned_by\":\"library\"}\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-194/[bug]-cant-add-ollama-using-supabase\">CORE-194 [Bug]: Can't Add Ollama Using Supabase</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey there, @aishock! I'm here to assist you with any bugs, questions, or contributions you have for Quivr. Let's tackle this issue together!\n\n<!-- Answer -->\nTo configure Ollama with llama3.1 in Supabase, ensure the following steps are correctly followed:\n\n1. **Update the `.env` file**:\n   Make sure the `OLLAMA_API_BASE_URL` is correctly set in your `.env` file. It should look something like this:\n   ```env\n   OLLAMA_API_BASE_URL=http://xx.xx.xx.xxx:11434\n   ```\n\n2. **Configure the Supabase `models` table**:\n   When adding a new row in the `models` table, ensure the `endpoint_url` is set to the base URL of the Ollama API, not the specific model endpoint. The `env_variable_name` should be set to the environment variable that holds the API key, if applicable. Here is an example configuration:\n   - `endpoint_url`: `http://xx.xx.xx.xxx:11434`\n   - `env_variable_name`: `OLLAMA_API_BASE_URL`\n\n3. **Check Network Configuration**:\n   Ensure that the network settings allow your local machine to communicate with the Ollama API. Sometimes, network errors can be caused by firewall settings or network restrictions.\n\n4. **Verify API Key**:\n   If the Ollama API requires an API key, ensure that the key is correctly set in your environment variables and that the `env_variable_name` in the Supabase `models` table points to this variable.\n\nHere is an example of how the row in the `models` table should look:\n```sql\nINSERT INTO models (endpoint_url, env_variable_name) VALUES ('http://xx.xx.xx.xxx:11434', 'OLLAMA_API_BASE_URL');\n```\n\nBy following these steps, you should be able to configure and use llama3.1 with Ollama in Supabase. If the issue persists, double-check the network settings and ensure that the API endpoint is accessible from your local machine.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "aishock",
        "body": "> Hey there, @aishock! I'm here to assist you with any bugs, questions, or contributions you have for Quivr. Let's tackle this issue together!\r\n> \r\n> To configure Ollama with llama3.1 in Supabase, ensure the following steps are correctly followed:\r\n> \r\n> 1. **Update the `.env` file**:\r\n>    Make sure the `OLLAMA_API_BASE_URL` is correctly set in your `.env` file. It should look something like this:\r\n>    ```dotenv\r\n>    OLLAMA_API_BASE_URL=http://xx.xx.xx.xxx:11434\r\n>    ```\r\n> 2. **Configure the Supabase `models` table**:\r\n>    When adding a new row in the `models` table, ensure the `endpoint_url` is set to the base URL of the Ollama API, not the specific model endpoint. The `env_variable_name` should be set to the environment variable that holds the API key, if applicable. Here is an example configuration:\r\n>    \r\n>    * `endpoint_url`: `http://xx.xx.xx.xxx:11434`\r\n>    * `env_variable_name`: `OLLAMA_API_BASE_URL`\r\n> 3. **Check Network Configuration**:\r\n>    Ensure that the network settings allow your local machine to communicate with the Ollama API. Sometimes, network errors can be caused by firewall settings or network restrictions.\r\n> 4. **Verify API Key**:\r\n>    If the Ollama API requires an API key, ensure that the key is correctly set in your environment variables and that the `env_variable_name` in the Supabase `models` table points to this variable.\r\n> \r\n> Here is an example of how the row in the `models` table should look:\r\n> \r\n> ```sql\r\n> INSERT INTO models (endpoint_url, env_variable_name) VALUES ('http://xx.xx.xx.xxx:11434', 'OLLAMA_API_BASE_URL');\r\n> ```\r\n> \r\n> By following these steps, you should be able to configure and use llama3.1 with Ollama in Supabase. If the issue persists, double-check the network settings and ensure that the API endpoint is accessible from your local machine.\r\n> \r\n> _To continue talking to [Dosu](https://dosu.dev), mention @dosu._\r\n\r\nI have tried the combination of 'http://xx.xx.xx.xxx:11434' and 'OLLAMA_API_BASE_URL', but it doesn't work.\r\n"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo troubleshoot further, let's ensure a few more details are correctly configured:\n\n1. **Supabase `models` Table Structure**:\n   Ensure the `models` table in Supabase has the correct structure. Here is the expected format:\n   ```sql\n   create table \"public\".\"models\" (\n       \"name\" text not null,\n       \"price\" integer default 1,\n       \"max_input\" integer default 2000,\n       \"max_output\" integer default 1000,\n       \"description\" text not null default 'Default Description'::text,\n       \"display_name\" text not null default gen_random_uuid(),\n       \"image_url\" text not null default 'https://quivr-cms.s3.eu-west-3.amazonaws.com/logo_quivr_white_7e3c72620f.png'::text,\n       \"default\" boolean default false,\n       \"endpoint_url\" text not null default 'https://api.openai.com/v1',\n       \"env_variable_name\" text not null default 'OPENAI_API_KEY'\n   );\n\n   CREATE UNIQUE INDEX models_pkey ON public.models USING btree (name);\n\n   alter table \"public\".\"models\" add constraint \"models_pkey\" PRIMARY KEY using index \"models_pkey\";\n   ```\n\n2. **Correct Row Insertion**:\n   When inserting the row, ensure all required fields are filled. Here is an example:\n   ```sql\n   INSERT INTO models (name, price, max_input, max_output, description, display_name, image_url, \"default\", endpoint_url, env_variable_name)\n   VALUES ('llama3.1', 1, 2000, 1000, 'Llama 3.1 Model', 'Llama 3.1', 'https://quivr-cms.s3.eu-west-3.amazonaws.com/logo_quivr_white_7e3c72620f.png', false, 'http://xx.xx.xx.xxx:11434', 'OLLAMA_API_BASE_URL');\n   ```\n\n3. **Network Configuration**:\n   Ensure there are no firewall or network restrictions blocking the connection between your local machine and the Ollama API.\n\n4. **Environment Variables**:\n   Double-check that the `OLLAMA_API_BASE_URL` is correctly set in your environment variables and that your application is loading these variables correctly.\n\n5. **Error Handling**:\n   Check the Quivr codebase for any specific error handling mechanisms that might provide more insight into the network error. For example, the code might be logging more detailed error messages that could help diagnose the issue <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/sync/utils/sync.py)</sup>.\n\nBy ensuring these configurations, you should be able to resolve the network error and successfully configure llama3.1 with Ollama in Supabase.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3498,
    "title": "Sample question & generate question/intent",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-11-25T10:21:17Z",
    "updated_at": "2024-12-09T14:03:28Z",
    "labels": [],
    "body": "* Sample question from data -> LLM to generate intent\n* 8 type of question / use cases of rag",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-302/sample-question-and-generate-questionintent\">CORE-302 Sample question &amp; generate question/intent</a></p>"
      }
    ]
  },
  {
    "issue_number": 3497,
    "title": "EDA Question",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-11-25T10:20:13Z",
    "updated_at": "2024-12-09T14:03:14Z",
    "labels": [],
    "body": "*  EDA Question \n  * Sample question from data -> LLM to generate intent\n  * Build intent classifier\n  * 8 type of question / use cases of rag\n  * Question correlation in chat \n* EDA Chat:\n  * Chat length stats\n  * Stats on number of sources returned",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-301/eda-question\">CORE-301 EDA Question</a></p>"
      }
    ]
  },
  {
    "issue_number": 3391,
    "title": "Remove all Megaparse dependencies from Worker/API",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-10-18T09:39:39Z",
    "updated_at": "2024-12-09T14:02:26Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-240/remove-all-megaparse-dependencies-from-workerapi\">CORE-240 Remove all Megaparse dependencies from Worker/API</a></p>"
      }
    ]
  },
  {
    "issue_number": 3151,
    "title": "[Feature]: csv file uploaded in the knowledge base should not be downloadable (via sources)",
    "author": "atulgrover",
    "state": "closed",
    "created_at": "2024-09-05T05:42:07Z",
    "updated_at": "2024-12-09T08:08:50Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "### The Feature\n\nthe csv file had tabular information and should not be downloadable by default...  there must be an option to tuen off the sources for csv files.... \r\n\r\n![image](https://github.com/user-attachments/assets/e9dc8183-d003-40c0-b029-84beb06561f7)\n\n### Motivation, pitch\n\nwhere in the code can I do it myself?\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-193/[feature]-csv-file-uploaded-in-the-knowledge-base-should-not-be\">CORE-193 [Feature]: csv file uploaded in the knowledge base should not be downloadable (via sources)</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo ensure that CSV files uploaded in the knowledge base are not downloadable by default and provide an option to turn off the sources for CSV files, you can modify the `getOptions` function to conditionally disable the \"Download\" option for CSV files. Additionally, you can add a new option to turn off the sources for CSV files.\n\nHere's how you can modify the code:\n\n1. Add a check to disable the \"Download\" option for CSV files.\n2. Add a new option to turn off the sources for CSV files.\n\n```tsx\nconst getOptions = (): Option[] => [\n    {\n      label: \"Delete\",\n      onClick: () => void onDeleteKnowledge(knowledge),\n      iconName: \"delete\",\n      iconColor: \"dangerous\",\n      disabled: brain?.role !== \"Owner\",\n    },\n    {\n      label: \"Download\",\n      onClick: () => void downloadFile(),\n      iconName: \"download\",\n      iconColor: \"primary\",\n      disabled: brain?.role !== \"Owner\" || !isUploadedKnowledge(knowledge) || knowledge.extension === \".csv\",\n    },\n    {\n      label: \"Turn Off Source\",\n      onClick: () => void turnOffSource(),\n      iconName: \"toggle_off\",\n      iconColor: \"warning\",\n      disabled: brain?.role !== \"Owner\" || knowledge.extension !== \".csv\",\n    },\n  ];\n\nconst turnOffSource = () => {\n  // Implement the logic to turn off the source for CSV files\n  console.log(`Turning off source for knowledge ID: ${knowledge.id}`);\n  setOptionsOpened(false);\n};\n```\n\nIn this modification:\n- The \"Download\" option is disabled if the file extension is `.csv`.\n- A new \"Turn Off Source\" option is added, which is enabled only for CSV files. The `turnOffSource` function should contain the logic to turn off the source for CSV files <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/frontend/app/studio/[brainId]/BrainManagementTabs/components/KnowledgeTab/KnowledgeTable/KnowledgeItem/KnowledgeItem.tsx)</sup>.\n\nThis change should be made in the `KnowledgeItem.tsx` file located at `frontend/app/studio/[brainId]/BrainManagementTabs/components/KnowledgeTab/KnowledgeTable/KnowledgeItem/KnowledgeItem.tsx`.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3150,
    "title": "[Bug]:  âœ˜ worker Error  pull access denied for quivr-backend-api, repository does not exist or may require 'docker login'",
    "author": "yodaphone",
    "state": "closed",
    "created_at": "2024-09-04T15:20:59Z",
    "updated_at": "2024-12-09T00:28:25Z",
    "labels": [
      "bug",
      "Stale",
      "area: backend"
    ],
    "body": "### What happened?\n\nA bug happened!\r\n\r\nFollow the instructions and got this error when i want to set up quivr locally. I also followed this [#2950](https://github.com/QuivrHQ/quivr/issues/2950) but it didnt work for me.\r\n\r\n\n\n### Relevant log output\n\n```bash\ndocker compose pull\r\n[+] Pulling 7/7\r\n âœ” backend-core Skipped - No image to be pulled                                                                                                                                                                                         0.0s\r\n âœ” notifier Skipped                                                                                                                                                                                                                     0.0s\r\n âœ” beat Skipped - Image is already being pulled by worker                                                                                                                                                                               0.0s\r\n âœ” flower Skipped - Image is already being pulled by worker                                                                                                                                                                             0.0s\r\n âœ” frontend Skipped                                                                                                                                                                                                                     0.0s\r\n âœ” redis Pulled                                                                                                                                                                                                                         0.2s\r\n âœ˜ worker Error                                             pull access denied for quivr-backend-api, repository does not exist or may require 'docker login': denied: requested access to the resource is denied                       0.4s\r\nError response from daemon: pull access denied for quivr-backend-api, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-192/[bug]-âœ˜-worker-error-pull-access-denied-for-quivr-backend-api\">CORE-192 [Bug]: âœ˜ worker Error pull access denied for quivr-backend-api, repository does not exist or may require 'docker login'</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey @yodaphone! I'm here to help you with any bugs or questions you have. I'm a bot that can assist with problem-solving and providing guidance for becoming a contributor.\n\n<!-- Answer -->\nI found a similar closed issue that might help you resolve the \"pull access denied for quivr-backend-api\" error:\n\n1. **[Bug]: Pull failure** <sup>[[1]](https://github.com/QuivrHQ/quivr/issues/2009)</sup>: This issue was related to Docker's inability to find a specific repository. The suggested solution was to ensure the image exists and that you have the necessary access rights. Additionally, it was recommended to check other Docker-related files or commands in your setup.\n\nTo resolve your specific issue, follow these steps:\n\n1. **Build the `quivr-backend-api` image locally**: The `quivr-backend-api` image needs to be built locally since it is not available for pulling from a remote repository. The `docker-compose.dev.yml` file indicates that the `backend-api` service should be built from the local `backend` directory.\n\nHere is the relevant section from the `docker-compose.dev.yml` file:\n\n```yml\nbackend-api:\n  image: backend-api:latest\n  extra_hosts:\n    - \"host.docker.internal:host-gateway\"\n  pull_policy: never\n  env_file:\n    - .env\n  build:\n    context: backend\n    dockerfile: Dockerfile.dev\n    cache_from:\n      - backend-api:latest\n    args:\n      - DEV_MODE=true\n  container_name: backend-api\n  volumes:\n    - ./backend/:/app/\n  command: >\n    /bin/bash -c \"python -m uvicorn quivr_api.main:app --host 0.0.0.0 --log-level info --reload --port 5050\"\n  restart: always\n  ports:\n    - 5050:5050\n    - 5678:5678 # debug port\n```\n\n2. **Build the image**: Ensure you have the `backend` directory with the necessary Dockerfile and source code. Run the following command to build the `backend-api` image locally:\n\n```sh\ndocker-compose -f docker-compose.dev.yml build\n```\n\n3. **Start the services**: After the build completes, start the services using:\n\n```sh\ndocker-compose -f docker-compose.dev.yml up\n```\n\nAdditionally, ensure you have followed the prerequisites and initial setup steps as outlined in the Quivr README:\n\n1. Clone the repository:\n\n```sh\ngit clone https://github.com/quivrhq/quivr.git && cd quivr\n```\n\n2. Copy the `.env.example` file to `.env`:\n\n```sh\ncp .env.example .env\n```\n\n3. Update the `.env` file with your `OPENAI_API_KEY`:\n\n```sh\nvim .env # or use any text editor\n```\n\n4. Launch the project:\n\n```sh\ncd backend && supabase start\ncd ../\ndocker compose pull\ndocker compose up\n```\n\nBy following these steps, you will build the `backend-api` image locally and use it for the `backend-api` service, resolving the \"pull access denied\" error <sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.yml)</sup><sup>[[3]](https://github.com/QuivrHQ/quivr/blob/main/README.md)</sup><sup>[[4]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.dev.yml)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "yodaphone",
        "body": "@dosu i tried these and got the 1st part done \r\n\r\ndocker-compose -f docker-compose.dev.yml up & have the containers running. when i access http://ipaddress:5050, the page is blank.\r\n\r\ndo i have to modify the docker-compose.yml file thats in the quivr folder to point to the local builds?"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nYes, you need to modify the `docker-compose.yml` file in the Quivr folder to point to the local builds if the page is blank when accessing the service at http://ipaddress:5050. Specifically, you should ensure that the `image` fields for the `frontend` and `backend-api` services are set to use the local builds instead of prebuilt images.\n\nHere is the modified `docker-compose.yml` file:\n\n```yml\nversion: \"3.8\"\n\nservices:\n  frontend:\n    build:\n      context: frontend\n      dockerfile: Dockerfile\n      args:\n        - NEXT_PUBLIC_ENV=local\n        - NEXT_PUBLIC_BACKEND_URL=${NEXT_PUBLIC_BACKEND_URL}\n        - NEXT_PUBLIC_SUPABASE_URL=${NEXT_PUBLIC_SUPABASE_URL}\n        - NEXT_PUBLIC_SUPABASE_ANON_KEY=${NEXT_PUBLIC_SUPABASE_ANON_KEY}\n        - NEXT_PUBLIC_CMS_URL=${NEXT_PUBLIC_CMS_URL}\n        - NEXT_PUBLIC_FRONTEND_URL=${NEXT_PUBLIC_FRONTEND_URL}\n        - NEXT_PUBLIC_AUTH_MODES=${NEXT_PUBLIC_AUTH_MODES}\n    container_name: web\n    depends_on:\n      - backend-api\n    restart: always\n    ports:\n      - 3000:3000\n\n  backend-api:\n    image: backend-api:latest\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    pull_policy: never\n    env_file:\n      - .env\n    build:\n      context: backend\n      dockerfile: Dockerfile.dev\n      cache_from:\n        - backend-api:latest\n      args:\n        - DEV_MODE=true\n    container_name: backend-api\n    volumes:\n      - ./backend/:/app/\n    command: >\n      /bin/bash -c \"python -m uvicorn quivr_api.main:app --host 0.0.0.0 --log-level info --reload --port 5050\"\n    restart: always\n    ports:\n      - 5050:5050\n      - 5678:5678 # debug port\n\n  worker:\n    pull_policy: never\n    image: backend-api:latest\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    env_file:\n      - .env\n    container_name: worker\n    volumes:\n      - ./backend/:/app/\n    command: >\n      /bin/bash -c \"python -m celery -A quivr_worker.celery_worker worker -l info -E\"\n    restart: always\n    depends_on:\n      - redis\n\n  notifier:\n    pull_policy: never\n    image: backend-api:latest\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    env_file:\n      - .env\n    container_name: notifier\n    volumes:\n      - ./backend/:/app/\n    command: >\n      /bin/bash -c \"python /app/worker/quivr_worker/celery_monitor.py\"\n    restart: always\n    depends_on:\n      - redis\n      - worker\n\n  beat:\n    image: backend-api:latest\n    pull_policy: never\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    env_file:\n      - .env\n    container_name: beat\n    command: >\n      /bin/bash -c \"python -m celery -A quivr_worker.celery_worker beat -l info\"\n    volumes:\n      - ./backend/:/app/\n    restart: always\n    depends_on:\n      - redis\n\n  flower:\n    image: backend-api:latest\n    pull_policy: never\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    env_file:\n      - .env\n    container_name: flower\n    command: >\n      /bin/bash -c \"python -m celery -A quivr_worker.celery_worker flower -l info --port=5555\"\n    volumes:\n      - ./backend/:/app/\n    restart: always\n    depends_on:\n      - redis\n      - worker\n      - beat\n    ports:\n      - 5555:5555\n\n  redis:\n    image: redis:latest\n    container_name: redis\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    restart: always\n    ports:\n      - 6379:6379\n\nnetworks:\n  quivr-network:\n    driver: bridge\n```\n\nThis configuration ensures that the `frontend` and `backend-api` services are built from the local Dockerfiles instead of using prebuilt images <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.yml)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "yodaphone",
        "body": "@dosu i'm a bit lost with this step\r\n\r\ndocker-compose -f docker-compose.dev.yml up\r\n\r\nafter i do this & keep the containers running, do I also have to run\r\n\r\ndocker compose up with the modified docker-compose.yml file?\r\n\r\nthats what I did and I'm getting this error\r\n\r\n```\r\ndocker compose up -d\r\n[+] Building 135.4s (13/21)                                                                                                                                                                                                   docker:default\r\n => [frontend internal] load build definition from Dockerfile                                                                                                                                                                           0.1s\r\n => => transferring dockerfile: 3.23kB                                                                                                                                                                                                  0.0s\r\n => [frontend internal] load metadata for docker.io/library/node:18.19.0-alpine                                                                                                                                                         0.2s\r\n => [frontend internal] load .dockerignore                                                                                                                                                                                              0.0s\r\n => => transferring context: 340B                                                                                                                                                                                                       0.0s\r\n => [frontend internal] load build context                                                                                                                                                                                              0.2s\r\n => => transferring context: 72.28kB                                                                                                                                                                                                    0.1s\r\n => [frontend base 1/1] FROM docker.io/library/node:18.19.0-alpine@sha256:0085670310d2879621f96a4216c893f92e2ded827e9e6ef8437672e1bd72f437                                                                                              0.0s\r\n => CACHED [frontend builder 1/4] WORKDIR /app                                                                                                                                                                                          0.0s\r\n => CACHED [frontend runner 2/8] RUN addgroup --system --gid 1001 nodejs                                                                                                                                                                0.0s\r\n => CACHED [frontend runner 3/8] RUN adduser --system --uid 1001 nextjs                                                                                                                                                                 0.0s\r\n => CACHED [frontend deps 1/5] RUN apk add --no-cache libc6-compat python3 make g++                                                                                                                                                     0.0s\r\n => CACHED [frontend deps 2/5] RUN yarn global add node-gyp                                                                                                                                                                             0.0s\r\n => CACHED [frontend deps 3/5] WORKDIR /app                                                                                                                                                                                             0.0s\r\n => CACHED [frontend deps 4/5] COPY package.json yarn.lock* package-lock.json* pnpm-lock.yaml* ./                                                                                                                                       0.0s\r\n => ERROR [frontend deps 5/5] RUN     if [ -f yarn.lock ]; then yarn --frozen-lockfile;     elif [ -f package-lock.json ]; then npm ci;     elif [ -f pnpm-lock.yaml ]; then yarn global add pnpm && pnpm i --frozen-lockfile;     e  134.7s\r\n------\r\n > [frontend deps 5/5] RUN     if [ -f yarn.lock ]; then yarn --frozen-lockfile;     elif [ -f package-lock.json ]; then npm ci;     elif [ -f pnpm-lock.yaml ]; then yarn global add pnpm && pnpm i --frozen-lockfile;     else echo \"Lockfile not found.\" && exit 1;     fi:\r\n0.946 yarn install v1.22.19\r\n1.166 [1/4] Resolving packages...\r\n2.640 [2/4] Fetching packages...\r\n94.25 warning chart.js@4.4.2: The engine \"pnpm\" appears to be invalid.\r\n94.31 [3/4] Linking dependencies...\r\n94.34 warning \" > @testing-library/user-event@14.5.1\" has unmet peer dependency \"@testing-library/dom@>=7.21.4\".\r\n94.36 warning \" > @typescript-eslint/eslint-plugin@5.62.0\" has unmet peer dependency \"@typescript-eslint/parser@^5.0.0\".\r\n94.37 warning \" > react-katex@3.0.1\" has unmet peer dependency \"prop-types@^15.8.1\".\r\n94.37 warning \" > @vitejs/plugin-react@4.0.4\" has unmet peer dependency \"vite@^4.2.0\".\r\n130.7 [4/4] Building fresh packages...\r\n131.7 error /app/node_modules/@sentry/cli: Command failed.\r\n131.7 Exit code: 1\r\n131.7 Command: node ./scripts/install.js\r\n131.7 Arguments:\r\n131.7 Directory: /app/node_modules/@sentry/cli\r\n131.7 Output:\r\n131.7 [sentry-cli] Downloading from https://downloads.sentry-cdn.com/sentry-cli/1.77.3/sentry-cli-Linux-x86_64\r\n131.7 Error: Unable to download sentry-cli binary from https://downloads.sentry-cdn.com/sentry-cli/1.77.3/sentry-cli-Linux-x86_64.\r\n131.7 Error code: ECONNREFUSED\r\n131.7 info Visit https://yarnpkg.com/en/docs/cli/install for documentation about this command.\r\n------\r\nfailed to solve: process \"/bin/sh -c if [ -f yarn.lock ]; then yarn --frozen-lockfile;     elif [ -f package-lock.json ]; then npm ci;     elif [ -f pnpm-lock.yaml ]; then yarn global add pnpm && pnpm i --frozen-lockfile;     else echo \\\"Lockfile not found.\\\" && exit 1;     fi\" did not complete successfully: exit code: 1\r\n\r\n```"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3141,
    "title": "fix celery notifier update status exception",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-03T14:43:51Z",
    "updated_at": "2024-12-07T20:06:15Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "update occurs because knowledge doesnt exist this is because knowledge was deleted in task before. Sucess/failure ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-190/fix-celery-notifier-update-status-exception\">CORE-190 fix celery notifier update status exception</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3131,
    "title": "Use documents metadata to improve the retrieval",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-03T09:15:36Z",
    "updated_at": "2024-12-07T16:06:12Z",
    "labels": [
      "Stale"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-182/use-documents-metadata-to-improve-the-retrieval\">CORE-182 Use documents metadata to improve the retrieval</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3137,
    "title": "[Bug]: the knowledge counter is stuck at 1000",
    "author": "atulgrover",
    "state": "closed",
    "created_at": "2024-09-03T10:10:06Z",
    "updated_at": "2024-12-07T16:06:11Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### What happened?\n\nA bug happened!\r\n![image](https://github.com/user-attachments/assets/b8a47523-36d3-4460-aef6-cc41f0ec6e88)\r\n[Bug]: the knowledge counter is stuck at 1000\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-187/[bug]-the-knowledge-counter-is-stuck-at-1000\">CORE-187 [Bug]: the knowledge counter is stuck at 1000</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nThe knowledge counter is limited to a maximum value of 1000 for the model \"mistral/mistral-small\" due to a condition in the code:\n\n```ts\ncase \"mistral/mistral-small\":\n  return 1000;\n```\n\nYou can find this condition in the `defineMaxTokens.ts` file <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/frontend/lib/helpers/defineMaxTokens.ts)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "StanGirard",
        "body": "Hey Mate, <br><br>You can go to `backend/supabase/config.toml` and increase the max rows to 2000. Be careful it has some performance issues -> \n\n<img src=\"https://uploads.linear.app/51e2032d-a488-42cf-9483-a30479d3e2d0/71fba78b-1c26-45c5-bb42-1382c65b4e2f/99b3e4fa-575d-4448-865e-84c78928abd5?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiLzUxZTIwMzJkLWE0ODgtNDJjZi05NDgzLWEzMDQ3OWQzZTJkMC83MWZiYTc4Yi0xYzI2LTQ1YzUtYmI0Mi0xMzgyYzY1YjRlMmYvOTliM2U0ZmEtNTc1ZC00NDQ4LTg2NWUtODRjNzg5MjhhYmQ1IiwiaWF0IjoxNzI1MzU4OTI5LCJleHAiOjMzMjk1OTE4OTI5fQ.bJbAK__RVmvPM9u-bP5LNykVlVO64WRDz9GilHEtWco \" alt=\"image.png\" width=\"1024\" height=\"327\" />\n\nThen simply `supabase stop && supabase start` and you'll be good to go"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3138,
    "title": "[Bug]: Document ...once loaded (by mistake) cannot be unloaded....and has to be embedded",
    "author": "atulgrover",
    "state": "closed",
    "created_at": "2024-09-03T10:44:47Z",
    "updated_at": "2024-12-07T16:06:10Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### What happened?\n\nA bug happened!\r\n![image](https://github.com/user-attachments/assets/77380355-8f81-481e-8341-4fe670cf7834)\r\nsubsequent delation of the document does not delete the related embeddings...\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-188/[bug]-document-once-loaded-by-mistake-cannot-be-unloadedand-has-to-be\">CORE-188 [Bug]: Document ...once loaded (by mistake) cannot be unloaded....and has to be embedded</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo fix the bug where a document, once loaded, cannot be unloaded and its subsequent deletion does not remove the related embeddings, you need to ensure that the deletion process in your `VectorService` or `VectorRepository` properly handles the removal of both the document and its associated embeddings.\n\nHere is a potential solution to address this issue:\n\n1. **Ensure Proper Deletion in the Repository:**\n   Modify the `VectorRepository` to include a method that deletes vectors by `knowledge_id`.\n\n2. **Update the Service to Use the Deletion Method:**\n   Ensure that the `VectorService` calls this method when a document is unloaded or deleted.\n\nHere is an example of how you can implement these changes:\n\n### Step 1: Update `VectorRepository` to Include Deletion Method\n\n```python\nclass VectorRepository:\n    def __init__(self, session: Session):\n        self.session = session\n\n    # Existing methods...\n\n    def delete_vectors_by_knowledge_id(self, knowledge_id: int):\n        self.session.execute(\n            sqlalchemy.delete(Vector).where(Vector.knowledge_id == knowledge_id)\n        )\n        self.session.commit()\n```\n\n### Step 2: Update `VectorService` to Use the Deletion Method\n\n```python\nclass VectorService:\n    def __init__(self, repository: VectorRepository):\n        self.repository = repository\n        self._embedding = None\n\n    # Existing methods...\n\n    def unload_document(self, knowledge_id: int):\n        # Ensure vectors related to the knowledge_id are deleted\n        self.repository.delete_vectors_by_knowledge_id(knowledge_id)\n```\n\n### Step 3: Write a Test to Verify the Deletion\n\nAdd a test to ensure that vectors are properly deleted when a document is unloaded.\n\n```python\ndef test_unload_document(session: Session, test_data: TestData):\n    vectors, knowledge, _ = test_data\n    assert knowledge.id\n\n    repo = VectorRepository(session)\n    service = VectorService(repo)\n\n    # Unload the document\n    service.unload_document(knowledge.id)\n\n    # Verify that vectors are deleted\n    results = repo.get_vectors_by_knowledge_id(knowledge.id)\n    assert len(results) == 0, f\"Expected 0 vectors, got {len(results)}\"\n```\n\nBy implementing these changes, you ensure that when a document is unloaded or deleted, its associated embeddings are also removed from the database, thus fixing the bug <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/vector/tests/test_vectors.py)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3139,
    "title": "[Bug]: pnly one file show up as loaded for ingestion even when multiple files are dragged for addition",
    "author": "atulgrover",
    "state": "closed",
    "created_at": "2024-09-03T10:52:15Z",
    "updated_at": "2024-12-07T16:06:09Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### What happened?\n\nA bug happened!\r\n![image](https://github.com/user-attachments/assets/4433b467-d00d-498a-9662-81279956f172)\r\n\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-189/[bug]-pnly-one-file-show-up-as-loaded-for-ingestion-even-when-multiple\">CORE-189 [Bug]: pnly one file show up as loaded for ingestion even when multiple files are dragged for addition</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nThe issue of only one file showing up as loaded for ingestion even when multiple files are dragged for addition in the \"Add Knowledge\" UI modal could be due to the following code logic in the `onDrop` function:\n\n```ts\nconst files: File[] = (\n  knowledgeToFeed.filter((c) => c.source === \"upload\") as FeedItemUploadType[]\n).map((c) => c.file);\n\nconst onDrop = (acceptedFiles: File[], fileRejections: FileRejection[]) => {\n  if (!isBrainCreationModalOpened) {\n    setShouldDisplayFeedCard(true);\n  }\n  if (fileRejections.length > 0) {\n    const firstRejection = fileRejections[0];\n\n    if (firstRejection.errors[0].code === \"file-invalid-type\") {\n      publish({ variant: \"danger\", text: t(\"invalidFileType\") });\n    } else {\n      publish({\n        variant: \"danger\",\n        text: t(\"maxSizeError\", { ns: \"upload\" }),\n      });\n    }\n\n    return;\n  }\n  for (const file of acceptedFiles) {\n    const isAlreadyInFiles =\n      files.filter((f) => f.name === file.name && f.size === file.size).length > 0;\n    if (isAlreadyInFiles) {\n      publish({\n        variant: \"warning\",\n        text: t(\"alreadyAdded\", { fileName: file.name, ns: \"upload\" }),\n      });\n    } else {\n      if (isOnboarding) {\n        void trackOnboardingEvent(\"FILE_UPLOADED\");\n      } else {\n        void track(\"FILE_UPLOADED\");\n      }\n\n      addKnowledgeToFeed({\n        source: \"upload\",\n        file: cloneFileWithSanitizedName(file),\n      });\n    }\n  }\n};\n```\n\nThe `onDrop` function processes each file in the `acceptedFiles` array. However, if a file is already present in the `files` array (checked by name and size), it will not be added again, and a warning will be published instead. This could be why only one file appears to be loaded if the other files are considered duplicates based on their name and size <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/frontend/lib/hooks/useDropzone.ts)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3109,
    "title": "Remplacer LangChain LCEL par LangGraph",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-08-28T08:50:45Z",
    "updated_at": "2024-12-07T12:09:01Z",
    "labels": [
      "backend",
      "Stale",
      "area: backend"
    ],
    "body": "Aujourd'hui, nous utilisons [LangChain Expression Language](https://python.langchain.com/v0.2/docs/introduction/) (LCEL) pour construire notre pipeline de RAG dans [backend/core/quivr_core/quivr_rag.py](https://github.com/QuivrHQ/quivr/blob/b7a081e94f73a181d7b425b4194af1b44f9df6fd/backend/core/quivr_core/quivr_rag.py).\n\nLCEL ne permet de faire que des pipelines de type DAG (sans cycle) et, de plus, est peu intuitive et difficile Ã  debugger. Nous allons donc utiliser [LangGraph](https://langchain-ai.github.io/langgraph/), qui est une librairie plus flexible et intuitive.",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-175/remplacer-langchain-lcel-par-langgraph\">CORE-175 Remplacer LangChain LCEL par LangGraph</a></p>"
      },
      {
        "user": "StanGirard",
        "body": "Sounds good to me, fix the conflict first"
      },
      {
        "user": "StanGirard",
        "body": "#3128 "
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3479,
    "title": "Restructure MegaParse",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-11-15T08:28:37Z",
    "updated_at": "2024-12-06T10:06:18Z",
    "labels": [],
    "body": "* Mettre le sdk au meme niveau que megaparse\n* MegaParser -> BaseParser\n* parser\\\\type.py not good reserve pour autre chose ->Â trouver un autre nom : model.py ou config.py\n* FakeParserBuilder in convtest.py\n* Remove fake parser dict and just monkey patch parser_dict or add in init of ParserBuilder a p_registryÂ \n* Change upload_url to parse_url (api)\n* ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-294/restructure-megaparse\">CORE-294 Restructure MegaParse</a></p>"
      }
    ]
  },
  {
    "issue_number": 3480,
    "title": "Type SDK",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-11-15T08:29:14Z",
    "updated_at": "2024-12-06T10:06:06Z",
    "labels": [],
    "body": "Add more typing in megaparse\n\n* Type unstructured result -> Everything must be typedÂ \n* See quivr_api/module routes ->Â typer le retour dâ€™api avec ApiResponse ou un truc comme caÂ \n* Typer le retour SDK",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-295/type-sdk\">CORE-295 Type SDK</a></p>"
      }
    ]
  },
  {
    "issue_number": 3481,
    "title": "Remove Langchain PlayWright",
    "author": "chloedia",
    "state": "closed",
    "created_at": "2024-11-15T08:30:08Z",
    "updated_at": "2024-12-06T10:05:53Z",
    "labels": [],
    "body": "* Not use Langchain Playwright -> directly playwright needs to be closed at some point so yield (browser set see url_playwright.py of langchain",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-296/remove-langchain-playwright\">CORE-296 Remove Langchain PlayWright</a></p>"
      }
    ]
  },
  {
    "issue_number": 3368,
    "title": "OCR outside of celery",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-10-14T14:03:35Z",
    "updated_at": "2024-12-06T10:05:43Z",
    "labels": [],
    "body": "* Document parsing outside celery ( unstructured / megaparse) ??\n* API service",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-234/ocr-outside-of-celery\">CORE-234 OCR outside of celery</a></p>"
      }
    ]
  },
  {
    "issue_number": 3120,
    "title": "BUG task crawler",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-02T07:32:23Z",
    "updated_at": "2024-12-06T08:08:10Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-179/bug-task-crawler\">CORE-179 BUG task crawler</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3134,
    "title": "Enabling configurable workflow in the ingestion pipeline",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-09-03T09:18:06Z",
    "updated_at": "2024-12-04T17:10:47Z",
    "labels": [],
    "body": "As for the retrieval pipeline, we will use LangGraph and an extearn;",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-185/switching-to-langgraph-in-the-ingestion-pipeline\">CORE-185 Switching to LangGraph in the ingestion pipeline</a></p>"
      }
    ]
  },
  {
    "issue_number": 3458,
    "title": "Use the workflow service",
    "author": "jacopo-chevallard",
    "state": "closed",
    "created_at": "2024-11-06T10:46:36Z",
    "updated_at": "2024-12-04T17:10:34Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-286/use-the-workflow-service\">CORE-286 Use the workflow service</a></p>"
      }
    ]
  },
  {
    "issue_number": 3119,
    "title": "[Feature]: i18n support",
    "author": "thinker007",
    "state": "closed",
    "created_at": "2024-08-31T08:33:53Z",
    "updated_at": "2024-12-04T12:10:19Z",
    "labels": [
      "enhancement",
      "Stale"
    ],
    "body": "### The Feature\n\nAny plan support i18n for example Chinese Japanese etc.\n\n### Motivation, pitch\n\nmore i18n users\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-178/[feature]-i18n-support\">CORE-178 [Feature]: i18n support</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3115,
    "title": "Notion pages upload bug",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-08-30T08:25:58Z",
    "updated_at": "2024-12-03T16:06:58Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "should trim naming to append extension",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-177/notion-pages-upload-bug\">CORE-177 Notion pages upload bug</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 757,
    "title": "[Issue] Can't run SQL scripts in Supabase while installing Quivr for first time",
    "author": "moayadeldin",
    "state": "closed",
    "created_at": "2023-07-24T18:46:08Z",
    "updated_at": "2024-12-02T02:59:41Z",
    "labels": [
      "Stale"
    ],
    "body": "Hello,\r\n\r\n\r\nI am trying to follow the readme file instructions in order to install Quivr on my local machine for the first time but I am stuck at step 4.\r\n\r\nHow may I run the following code in SQL given it is a shell command? \r\n```\r\nchmod +x migration.sh\r\n./migration.sh\r\n```\r\n\r\nI tried to run it in Supabase SQL snippet and of course it gave me invalid SQL query and I tried to run it in my terminal and it keeps saying command not found for each .sql file in the migrations.sh.\r\n\r\nthen the rest of the guidelines in Step 4 mentions choosing create_scripts if it is my first time and i can't find any files named as such\r\n\r\nI opened the video tutorial and the scripts used in it are removed and not found in this repo, the author says that the tutorial is a little bit outdated and the readme is to be followed but It doesn't seem obvious for me.\r\n\r\nIs there something I am getting wrong here? How can I perform step 4 properly and run the SQL queries in Supabase?\r\n",
    "comments": [
      {
        "user": "flake9",
        "body": "+1"
      },
      {
        "user": "jchapmanCR",
        "body": "You need to copy what is in scripts/tables.sql and paste it into the SQL query window in Supabase. Execute that query and you'll have the latest table schemas (no need for migrations)."
      },
      {
        "user": "maxsvetlik",
        "body": "The README is underspecified for this step. You can simply run\r\n```\r\nchmod +x migration.sh\r\n./migration.sh\r\n```\r\nIt will ask you for your database `host` `name` `port` and `password`. These can be found in Supabase via `Project Settings > Database`\r\n_then_ it will give you the option of creating new tables (if this is your first time, this is the option you should choose) or migrating an existing database.\r\n\r\nThe script will use your credentials to create the database schemas for you. "
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      },
      {
        "user": "usmbok",
        "body": "Far from stale - I have same issue - and its the script we needâ€¦\r\n"
      }
    ]
  },
  {
    "issue_number": 3098,
    "title": "Rethink and redo file check logic",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-08-26T14:45:29Z",
    "updated_at": "2024-11-29T16:06:44Z",
    "labels": [
      "Stale"
    ],
    "body": "* In syncfiles\n* In filesha1",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-171/rethink-and-redo-file-check-logic\">CORE-171 Rethink and redo file check logic</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3085,
    "title": "Opacity in icon color picker",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-08-26T07:42:58Z",
    "updated_at": "2024-11-29T08:08:06Z",
    "labels": [
      "Stale"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-166/opacity-in-icon-color-picker\">CORE-166 Opacity in icon color picker</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3499,
    "title": "EDA knowledge",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-11-25T10:21:45Z",
    "updated_at": "2024-11-28T11:29:42Z",
    "labels": [],
    "body": "* Focus on pdf only\n* stats of  knowledges per brain\n* stats on length of knowledge : nb of chunks \n* t-sne knowledge + kmeans -> knowledge  per cluster ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-303/eda-knowledge\">CORE-303 EDA knowledge</a></p>"
      }
    ]
  },
  {
    "issue_number": 3082,
    "title": "[Bug]: JSONDecodeError using api, endpoint: \"{base_url}/chat/{chat_id}/question?brain_id={brain_id}\"",
    "author": "HELIOPOTELICKI",
    "state": "closed",
    "created_at": "2024-08-23T17:40:17Z",
    "updated_at": "2024-11-26T20:06:33Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### What happened?\n\nI uploaded the containers locally, and using Python, I implemented the functions to interact with Quivr.\r\nI have used this class that I implemented a few months ago, and it worked well (before having the notifier, by the way).\r\n\r\nI updated Quivr, and now I encounter this JSONDecode error.\r\n\r\nThe error occurs using my implementation and through the Swagger UI as well.\r\n\r\n\r\n```python\r\nclass QuivrAPI:\r\n    def __init__(self, base_url: Optional[str] = None, api_key: Optional[str] = None) -> None:\r\n        \"\"\"\r\n        Initialize the QuivrAPI class with base URL and API key.\r\n\r\n        Args:\r\n            base_url (Optional[str]): The base URL for the API. Defaults to environment variable QUIVR_API_URL.\r\n            api_key (Optional[str]): The API key for authentication. Defaults to environment variable QUIVR_API_KEY.\r\n        \"\"\"\r\n        self.base_url: str = base_url or os.getenv(\"QUIVR_API_URL\")\r\n        self.headers: Dict[str, str] = {\r\n            \"Authorization\": f\"Bearer {api_key or os.getenv('QUIVR_API_KEY')}\",\r\n            \"accept\": \"application/json\"\r\n        }\r\n\r\n    def ask_question(self, question: str, brain_id: str, prompt_id: str, chat_id: Optional[str] = None, model: Optional[str] = \"gpt-4o\", temperature: Optional[int] = 0, max_tokens: Optional[int] = 4000) -> Dict[str, Any]:\r\n            \"\"\"\r\n            Ask a question to a specific brain.\r\n    \r\n            Args:\r\n                question (str): The question to ask.\r\n                brain_id (str): The ID of the brain.\r\n                prompt_id (str): The ID of the prompt.\r\n                chat_id (Optional[str]): The ID of the chat. Defaults to None.\r\n                model (Optional[str]): The model to use. Defaults to \"gpt-4o\".\r\n                temperature (Optional[int]): The temperature setting for the model. Defaults to 0.\r\n                max_tokens (Optional[int]): The maximum number of tokens to generate. Defaults to 4000.\r\n    \r\n            Returns:\r\n                Dict[str, Any]: The response from the API as a dictionary.\r\n            \"\"\"\r\n            if chat_id is None:\r\n                chat = self.create_chat()\r\n                chat_id = chat[\"chat_id\"]\r\n    \r\n            payload: Dict[str, Any] = {\r\n                \"question\": question,\r\n                \"model\": model,\r\n                \"temperature\": temperature,\r\n                \"max_tokens\": max_tokens,\r\n                \"brain_id\": brain_id,\r\n                \"prompt_id\": prompt_id\r\n            }\r\n            response = requests.post(\r\n                f\"{self.base_url}/chat/{chat_id}/question?brain_id={brain_id}\",\r\n                json=payload,\r\n                headers=self.headers\r\n            )\r\n            return response.json()\r\n```\r\n\r\nError:\r\n\r\n```bash\r\n---------------------------------------------------------------------------\r\nJSONDecodeError                           Traceback (most recent call last)\r\nFile c:\\Users\\helio\\anaconda3\\envs\\moderator\\Lib\\site-packages\\requests\\models.py:974, in Response.json(self, **kwargs)\r\n    [973](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/site-packages/requests/models.py:973) try:\r\n--> [974](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/site-packages/requests/models.py:974)     return complexjson.loads(self.text, **kwargs)\r\n    [975](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/site-packages/requests/models.py:975) except JSONDecodeError as e:\r\n    [976](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/site-packages/requests/models.py:976)     # Catch JSON-related errors and raise as requests.JSONDecodeError\r\n    [977](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/site-packages/requests/models.py:977)     # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\r\n\r\nFile c:\\Users\\helio\\anaconda3\\envs\\moderator\\Lib\\json\\__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\r\n    [343](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/__init__.py:343) if (cls is None and object_hook is None and\r\n    [344](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/__init__.py:344)         parse_int is None and parse_float is None and\r\n    [345](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/__init__.py:345)         parse_constant is None and object_pairs_hook is None and not kw):\r\n--> [346](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/__init__.py:346)     return _default_decoder.decode(s)\r\n    [347](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/__init__.py:347) if cls is None:\r\n\r\nFile c:\\Users\\helio\\anaconda3\\envs\\moderator\\Lib\\json\\decoder.py:337, in JSONDecoder.decode(self, s, _w)\r\n    [333](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/decoder.py:333) \"\"\"Return the Python representation of ``s`` (a ``str`` instance\r\n    [334](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/decoder.py:334) containing a JSON document).\r\n    [335](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/decoder.py:335) \r\n    [336](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/decoder.py:336) \"\"\"\r\n--> [337](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/decoder.py:337) obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n    [338](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/decoder.py:338) end = _w(s, end).end()\r\n\r\nFile c:\\Users\\helio\\anaconda3\\envs\\moderator\\Lib\\json\\decoder.py:355, in JSONDecoder.raw_decode(self, s, idx)\r\n    [354](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/decoder.py:354) except StopIteration as err:\r\n--> [355](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/decoder.py:355)     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\n    [356](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/json/decoder.py:356) return obj, end\r\n\r\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nJSONDecodeError                           Traceback (most recent call last)\r\nCell In[10], [line 1](vscode-notebook-cell:?execution_count=10&line=1)\r\n----> [1](vscode-notebook-cell:?execution_count=10&line=1) quivr.ask_question(question=\"Qual Ã© o atual presidente do Brasil?\", \r\n      [2](vscode-notebook-cell:?execution_count=10&line=2)                    brain_id=\"40ba47d7-51b2-4b2a-9247-89e29619efb0\", \r\n      [3](vscode-notebook-cell:?execution_count=10&line=3)                    prompt_id=\"b813c6a2-8364-4f4f-8b0[4](vscode-notebook-cell:?execution_count=10&line=4)-6cf24d1985cc\",\r\n      4                    )\r\n\r\nFile f:\\COGMO\\COGMO-AGENTS\\CogmoAgents-Moderador\\app\\services\\quivr_utils.py:194, in QuivrAPI.ask_question(self, question, brain_id, prompt_id, chat_id, model, temperature, max_tokens)\r\n    [181](file:///F:/COGMO/COGMO-AGENTS/CogmoAgents-Moderador/app/services/quivr_utils.py:181) payload: Dict[str, Any] = {\r\n    [182](file:///F:/COGMO/COGMO-AGENTS/CogmoAgents-Moderador/app/services/quivr_utils.py:182)     \"question\": question,\r\n    [183](file:///F:/COGMO/COGMO-AGENTS/CogmoAgents-Moderador/app/services/quivr_utils.py:183)     \"model\": model,\r\n   (...)\r\n    [187](file:///F:/COGMO/COGMO-AGENTS/CogmoAgents-Moderador/app/services/quivr_utils.py:187)     \"prompt_id\": prompt_id\r\n    [188](file:///F:/COGMO/COGMO-AGENTS/CogmoAgents-Moderador/app/services/quivr_utils.py:188) }\r\n    [189](file:///F:/COGMO/COGMO-AGENTS/CogmoAgents-Moderador/app/services/quivr_utils.py:189) response = requests.post(\r\n    [190](file:///F:/COGMO/COGMO-AGENTS/CogmoAgents-Moderador/app/services/quivr_utils.py:190)     f\"{self.base_url}/chat/{chat_id}/question?brain_id={brain_id}\",\r\n    [191](file:///F:/COGMO/COGMO-AGENTS/CogmoAgents-Moderador/app/services/quivr_utils.py:191)     json=payload,\r\n    [192](file:///F:/COGMO/COGMO-AGENTS/CogmoAgents-Moderador/app/services/quivr_utils.py:192)     headers=self.headers\r\n    [193](file:///F:/COGMO/COGMO-AGENTS/CogmoAgents-Moderador/app/services/quivr_utils.py:193) )\r\n--> [194](file:///F:/COGMO/COGMO-AGENTS/CogmoAgents-Moderador/app/services/quivr_utils.py:194) return response.json()\r\n\r\nFile c:\\Users\\helio\\anaconda3\\envs\\moderator\\Lib\\site-packages\\requests\\models.py:978, in Response.json(self, **kwargs)\r\n    [974](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/site-packages/requests/models.py:974)     return complexjson.loads(self.text, **kwargs)\r\n    [975](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/site-packages/requests/models.py:975) except JSONDecodeError as e:\r\n    [976](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/site-packages/requests/models.py:976)     # Catch JSON-related errors and raise as requests.JSONDecodeError\r\n    [977](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/site-packages/requests/models.py:977)     # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\r\n--> [978](file:///C:/Users/helio/anaconda3/envs/moderator/Lib/site-packages/requests/models.py:978)     raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\r\n\r\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\r\n\r\nOn swagger:\r\n![image](https://github.com/user-attachments/assets/8dbdca80-5d8f-48af-b9be-3e52acccaf13)\r\n\r\n\r\n\r\nOS: Windows 11 Pro\r\nPython: 3.11.9\r\nQuivr: 0.0.300\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-165/[bug]-jsondecodeerror-using-api-endpoint-base-urlchatchat\">CORE-165 [Bug]: JSONDecodeError using api, endpoint: \"{base_url}/chat/{chat_id}/question?brain_id={brain_id}\"</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey there, @HELIOPOTELICKI! I'm here to help you with your bug report. I'm a bot that can assist with solving bugs, answering questions, and becoming a contributor. Let's get to the bottom of this together!\n\n<!-- Answer -->\nYou are encountering a `JSONDecodeError` when using the `QuivrAPI` class to interact with the endpoint `{base_url}/chat/{chat_id}/question?brain_id={brain_id}` because the code attempts to parse a string that is not valid JSON. Specifically, the error occurs in the `make_recursive_tool_calls` method when it tries to parse the `function.arguments` field of a `tool_call` object.\n\nHere is the relevant code snippet:\n\n```python\nfunction_args = json.loads(tool_call.function.arguments)\n```\n\nIf `tool_call.function.arguments` is not a valid JSON string, `json.loads` will raise a `JSONDecodeError`.\n\nTo fix this issue, ensure that `tool_call.function.arguments` is a valid JSON string before attempting to parse it. You can add error handling to catch and log the error, or validate the string before parsing.\n\nHere is an example of how you can add error handling:\n\n```python\nimport json\n\ndef safe_json_loads(json_string):\n    try:\n        return json.loads(json_string)\n    except json.JSONDecodeError as e:\n        logger.error(f\"JSONDecodeError: {e}\")\n        return None\n\n# Usage in make_recursive_tool_calls\nfunction_args = safe_json_loads(tool_call.function.arguments)\nif function_args is None:\n    # Handle the error appropriately, e.g., skip this tool call or return an error response\n    continue\n```\n\nBy adding this error handling, you can avoid the `JSONDecodeError` and handle invalid JSON strings more gracefully <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/brain/composite_brain_qa.py)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3080,
    "title": "Graceful shutdown notifier",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-08-23T15:35:32Z",
    "updated_at": "2024-11-26T16:07:08Z",
    "labels": [
      "enhancement",
      "backend",
      "Stale"
    ],
    "body": "Notifier should have graceful shutdown with cleanup of pending tasks in queue etc..",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-164/graceful-shutdown-notifier\">CORE-164 Graceful shutdown notifier</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 2950,
    "title": "Issue: Installation of Quivr Failing Due to Missing backend-base:latest Image",
    "author": "sNor4pro",
    "state": "closed",
    "created_at": "2024-08-06T19:25:18Z",
    "updated_at": "2024-11-25T13:38:22Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\r\n\r\n## Issue: Installation of Quivr Failing Due to Missing `backend-base:latest` Image\r\n\r\nHello,\r\n\r\nI encountered an issue while trying to install Quivr on a fresh installation of POP!_OS and Ubuntu. Both attempts failed at the same point: Docker could not find the `backend-base:latest` image.\r\n\r\n### Steps Taken\r\n1. Attempted to install Quivr using the standard installation procedure.\r\n2. Encountered the following error: `No such image: backend-base:latest`.\r\n3. Tried manually installing the image, but it appears to have been deleted or is otherwise unavailable.\r\n\r\n### Question\r\nIs there an alternative way to install Quivr or a workaround for this issue? Any guidance or updated instructions would be greatly appreciated.\r\n\r\nThank you!\r\n\r\n\r\n### Relevant log output\r\n\r\n```bash\r\nanon@anon:~/quivr$ docker compose up\r\nWARN[0000] /home/mike/quivr/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion \r\n[+] Building 104.4s (23/23) FINISHED                                                                                                                          docker:default\r\n => [frontend internal] load build definition from Dockerfile                                                                                                           0.2s\r\n => => transferring dockerfile: 2.90kB                                                                                                                                  0.0s\r\n => [frontend internal] load metadata for docker.io/library/node:18.19.0-alpine                                                                                         1.6s\r\n => [frontend internal] load .dockerignore                                                                                                                              0.1s\r\n => => transferring context: 340B                                                                                                                                       0.0s\r\n => [frontend base 1/1] FROM docker.io/library/node:18.19.0-alpine@sha256:0085670310d2879621f96a4216c893f92e2ded827e9e6ef8437672e1bd72f437                              2.7s\r\n => => resolve docker.io/library/node:18.19.0-alpine@sha256:0085670310d2879621f96a4216c893f92e2ded827e9e6ef8437672e1bd72f437                                            0.1s\r\n => => sha256:0085670310d2879621f96a4216c893f92e2ded827e9e6ef8437672e1bd72f437 1.43kB / 1.43kB                                                                          0.0s\r\n => => sha256:aacbcec05180c1dd8c33dba8a9c42b75dbfdd659aa57617497f1ce2c5d83d889 1.16kB / 1.16kB                                                                          0.0s\r\n => => sha256:c8eb770fbfacf54104162cc9035c478ddb7d8dc15dca5298af028257f1dbdb3f 7.14kB / 7.14kB                                                                          0.0s\r\n => => sha256:eb6c7c29ba4d368f2428cacd291f7821b750fac3b1fb65b937ef855c573cdf97 40.24MB / 40.24MB                                                                        1.1s\r\n => => sha256:3d4a65156edf0208c8421995310d9e662e7ee63e2bcae660efb02f6c4ddef6a9 2.34MB / 2.34MB                                                                          0.5s\r\n => => sha256:5bdb6c27eb32087b71a9dde411c1f1eeb87563c0445f89db4eb7639d2cf50f45 450B / 450B                                                                              0.6s\r\n => => extracting sha256:eb6c7c29ba4d368f2428cacd291f7821b750fac3b1fb65b937ef855c573cdf97                                                                               0.8s\r\n => => extracting sha256:3d4a65156edf0208c8421995310d9e662e7ee63e2bcae660efb02f6c4ddef6a9                                                                               0.1s\r\n => => extracting sha256:5bdb6c27eb32087b71a9dde411c1f1eeb87563c0445f89db4eb7639d2cf50f45                                                                               0.0s\r\n => [frontend internal] load build context                                                                                                                              0.2s\r\n => => transferring context: 5.12MB                                                                                                                                     0.1s\r\n => [frontend deps 1/5] RUN apk add --no-cache libc6-compat python3 make g++                                                                                            4.0s\r\n => [frontend builder 1/4] WORKDIR /app                                                                                                                                 0.4s\r\n => [frontend runner 2/8] RUN addgroup --system --gid 1001 nodejs                                                                                                       0.4s\r\n => [frontend runner 3/8] RUN adduser --system --uid 1001 nextjs                                                                                                        0.4s\r\n => [frontend deps 2/5] RUN yarn global add node-gyp                                                                                                                    3.1s \r\n => [frontend deps 3/5] WORKDIR /app                                                                                                                                    0.2s \r\n => [frontend deps 4/5] COPY package.json yarn.lock* package-lock.json* pnpm-lock.yaml* ./                                                                              0.2s \r\n => [frontend deps 5/5] RUN     if [ -f yarn.lock ]; then yarn --frozen-lockfile;     elif [ -f package-lock.json ]; then npm ci;     elif [ -f pnpm-lock.yaml ]; the  35.7s \r\n => [frontend builder 2/4] COPY --from=deps /app/node_modules ./node_modules                                                                                            2.8s \r\n => [frontend builder 3/4] COPY . .                                                                                                                                     0.2s \r\n => [frontend builder 4/4] RUN yarn build                                                                                                                              48.9s \r\n => [frontend runner 4/8] COPY --from=builder /app/public ./public                                                                                                      0.2s \r\n => [frontend runner 5/8] RUN mkdir .next                                                                                                                               0.5s \r\n => [frontend runner 6/8] RUN chown nextjs:nodejs .next                                                                                                                 0.5s \r\n => [frontend runner 7/8] COPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./                                                                            0.3s \r\n => [frontend runner 8/8] COPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static                                                                    0.2s \r\n => [frontend] exporting to image                                                                                                                                       0.5s \r\n => => exporting layers                                                                                                                                                 0.4s\r\n => => writing image sha256:e830f1cb24af97621d40c7ea74378e9df1d108157ad256caf43d42a2ea065fbb                                                                            0.0s\r\n => => naming to docker.io/library/quivr-frontend-prebuilt                                                                                                              0.0s\r\n => [frontend] resolving provenance for metadata file                                                                                                                   0.0s\r\n[+] Running 6/6\r\n âœ” Network quivr_default   Created                                                                                                                                      0.1s \r\n âœ” Container redis         Created                                                                                                                                      0.3s \r\n âœ” Container backend-core  Created                                                                                                                                      0.2s \r\n âœ” Container web           Created                                                                                                                                      0.2s \r\n âœ” Container worker        Created                                                                                                                                      0.2s \r\n âœ” Container beat          Created                                                                                                                                      0.2s \r\n â ‹ Container notifier      Creating                                                                                                                                     0.0s \r\n â ‹ Container flower        Creating                                                                                                                                     0.0s \r\nError response from daemon: No such image: backend-base:latest\r\n\r\n\r\n\r\n----------------\r\nanon@anon:~/quivr$ docker pull backend-base:latest\r\nError response from daemon: pull access denied for backend-base, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\r\n```\r\n\r\n\r\n### Twitter / LinkedIn details\r\n\r\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/TICK-39/issue-installation-of-quivr-failing-due-to-missing-backend-baselatest\">TICK-39 Issue: Installation of Quivr Failing Due to Missing backend-base:latest Image</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHey there, @sNor4pro! I'm here to help with any bugs, questions, or contributions you have regarding the Quivr project. Let's get that issue sorted out for you.\n\n<!-- Answer -->\nTo address the issue where Docker cannot find the `backend-base:latest` image, you can modify the `docker-compose.yml` file to build the image locally instead of pulling it from a remote repository. Here is an alternative way to install Quivr by ensuring the backend image is built locally:\n\n```yml\nversion: \"3.8\"\n\nservices:\n  frontend:\n    image: quivr-frontend-prebuilt\n    pull_policy: never\n    build:\n      context: frontend\n      dockerfile: Dockerfile\n      args:\n        - NEXT_PUBLIC_ENV=local\n        - NEXT_PUBLIC_BACKEND_URL=${NEXT_PUBLIC_BACKEND_URL}\n        - NEXT_PUBLIC_SUPABASE_URL=${NEXT_PUBLIC_SUPABASE_URL}\n        - NEXT_PUBLIC_SUPABASE_ANON_KEY=${NEXT_PUBLIC_SUPABASE_ANON_KEY}\n        - NEXT_PUBLIC_CMS_URL=${NEXT_PUBLIC_CMS_URL}\n        - NEXT_PUBLIC_FRONTEND_URL=${NEXT_PUBLIC_FRONTEND_URL}\n        - NEXT_PUBLIC_AUTH_MODES=${NEXT_PUBLIC_AUTH_MODES}\n    container_name: web\n    depends_on:\n      - backend-core\n    restart: always\n    ports:\n      - 3000:3000\n\n  backend-core:\n    build:\n      context: backend\n      dockerfile: Dockerfile\n    container_name: backend-core\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"http://localhost:5050/healthz\" ]\n    command:\n      - \"uvicorn\"\n      - \"main:app\"\n      - \"--host\"\n      - \"0.0.0.0\"\n      - \"--port\"\n      - \"5050\"\n      - \"--workers\"\n      - \"6\"\n    restart: always\n    volumes:\n      - ./backend/:/code/\n    ports:\n      - 5050:5050\n\n  redis:\n    image: redis:latest@sha256:a7cee7c8178ff9b5297cb109e6240f5072cdaaafd775ce6b586c3c704b06458e\n    container_name: redis\n    restart: always\n    ports:\n      - 6379:6379\n\n  worker:\n    build:\n      context: backend\n      dockerfile: Dockerfile\n    container_name: worker\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    command: celery -A celery_worker worker -l info\n    restart: always\n    depends_on:\n      - redis\n\n  beat:\n    build:\n      context: backend\n      dockerfile: Dockerfile\n    container_name: beat\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    command: celery -A celery_worker beat -l info\n    restart: always\n    depends_on:\n      - redis\n\n  flower:\n    build:\n      context: backend\n      dockerfile: Dockerfile\n    container_name: flower\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    command: celery -A celery_worker flower -l info --port=5555\n    restart: always\n    depends_on:\n      - redis\n      - worker\n      - beat\n    ports:\n      - 5555:5555\n\nnetworks:\n  quivr-network:\n    driver: bridge\n```\n\nIn this configuration, the `backend-core`, `worker`, `beat`, and `flower` services are set to build the Docker image locally using the `Dockerfile` in the `backend` directory. This should resolve the issue of Docker not finding the `backend-base:latest` image <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.yml)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "sNor4pro",
        "body": "it now spams the whole time, is this correct?: @dosu\r\nWebinterface from Quivr works, create brain doesn't.\r\n\r\n```\r\nAttaching to backend-core, beat, flower, redis, web, worker\r\nredis | 1:C 06 Aug 2024 19:42:47.576 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\r\nredis | 1:C 06 Aug 2024 19:42:47.577 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\nredis | 1:C 06 Aug 2024 19:42:47.577 * Redis version=7.2.3, bits=64, commit=00000000, modified=0, pid=1, just started\r\nredis | 1:C 06 Aug 2024 19:42:47.577 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\r\nredis | 1:M 06 Aug 2024 19:42:47.577 * monotonic clock: POSIX clock_gettime\r\nredis | 1:M 06 Aug 2024 19:42:47.577 * Running mode=standalone, port=6379.\r\nredis | 1:M 06 Aug 2024 19:42:47.577 * Server initialized\r\nredis | 1:M 06 Aug 2024 19:42:47.577 * Loading RDB produced by version 7.2.3\r\nredis | 1:M 06 Aug 2024 19:42:47.577 * RDB age 4 seconds\r\nredis | 1:M 06 Aug 2024 19:42:47.577 * RDB memory usage when created 0.83 Mb\r\nredis | 1:M 06 Aug 2024 19:42:47.577 * Done loading RDB, keys loaded: 0, keys expired: 0.\r\nredis | 1:M 06 Aug 2024 19:42:47.577 * DB loaded from disk: 0.000 seconds\r\nredis | 1:M 06 Aug 2024 19:42:47.577 * Ready to accept connections tcp\r\nbackend-core | INFO: Uvicorn running on http://0.0.0.0:5050/ (Press CTRL+C to quit)\r\nbackend-core | INFO: Started parent process [1]\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nweb | â–² Next.js 14.1.0\r\nweb | - Local: http://localhost:3000/\r\nweb | - Network: http://0.0.0.0:3000/\r\nweb |\r\nweb | âœ“ Ready in 43ms\r\nbackend-core | INFO: Waiting for child process [8]\r\nbackend-core | INFO: Child process [8] died\r\nbackend-core | INFO: Waiting for child process [9]\r\nbackend-core | INFO: Child process [9] died\r\nbackend-core | INFO: Waiting for child process [10]\r\nbackend-core | INFO: Child process [10] died\r\nbackend-core | INFO: Waiting for child process [11]\r\nbackend-core | INFO: Child process [11] died\r\nbackend-core | INFO: Waiting for child process [12]\r\nbackend-core | INFO: Child process [12] died\r\nbackend-core | INFO: Waiting for child process [13]\r\nbackend-core | INFO: Child process [13] died\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbeat | Usage: celery [OPTIONS] COMMAND [ARGS]...\r\nbeat | Try 'celery --help' for help.\r\nbeat |\r\nbeat | Error: Invalid value for '-A' / '--app':\r\nbeat | Unable to load celery application.\r\nbeat | The module celery_worker was not found.\r\nworker | Usage: celery [OPTIONS] COMMAND [ARGS]...\r\nworker | Try 'celery --help' for help.\r\nworker |\r\nworker | Error: Invalid value for '-A' / '--app':\r\nworker | Unable to load celery application.\r\nworker | The module celery_worker was not found.\r\nbackend-core | INFO: Waiting for child process [9]\r\nbackend-core | INFO: Child process [9] died\r\nbackend-core | INFO: Waiting for child process [11]\r\nbackend-core | INFO: Child process [11] died\r\nbackend-core | INFO: Waiting for child process [13]\r\nbackend-core | INFO: Child process [13] died\r\nbackend-core | INFO: Waiting for child process [21]\r\nbackend-core | INFO: Child process [21] died\r\nbackend-core | INFO: Waiting for child process [23]\r\nbackend-core | INFO: Child process [23] died\r\nbackend-core | INFO: Waiting for child process [25]\r\nbackend-core | INFO: Child process [25] died\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nflower | Usage: celery [OPTIONS] COMMAND [ARGS]...\r\nflower | Try 'celery --help' for help.\r\nflower |\r\nflower | Error: Invalid value for '-A' / '--app':\r\nflower | Unable to load celery application.\r\nflower | The module celery_worker was not found.\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbackend-core | ERROR: Error loading ASGI app. Could not import module \"main\".\r\nbeat exited with code 0\r\nworker exited with code 0\r\nbackend-core | INFO: Waiting for child process [11]\r\n```"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nThe errors you're encountering indicate that the `backend-core` service is unable to import the \"main\" module, and the `worker`, `beat`, and `flower` services are unable to load the `celery_worker` module. Here are steps to resolve these issues:\n\n1. **Check Directory Structure**:\n   Ensure that the `backend` directory contains the `main.py` and `celery_worker.py` files. The structure should look like this:\n   ```\n   backend/\n   â”œâ”€â”€ main.py\n   â””â”€â”€ celery_worker.py\n   ```\n\n2. **Verify Module Paths**:\n   Ensure that the `main.py` file contains the `app` object, and the `celery_worker.py` file is correctly set up for Celery.\n\n3. **Check Volume Mounts**:\n   Ensure that the code is correctly mounted and accessible within the container. The `volumes` directive in the `docker-compose.yml` file should mount the `./backend/` directory to `/code/` in the container.\n\n4. **Inspect Docker Logs**:\n   Check the logs of the failing services to get more details about the import errors. You can use the following commands:\n   ```sh\n   docker-compose logs backend-core\n   docker-compose logs worker\n   docker-compose logs beat\n   docker-compose logs flower\n   ```\n\n5. **Example `main.py`**:\n   Here is an example of what the `main.py` file might look like:\n   ```python\n   from fastapi import FastAPI\n\n   app = FastAPI()\n\n   @app.get(\"/\")\n   def read_root():\n       return {\"Hello\": \"World\"}\n   ```\n\n6. **Example `celery_worker.py`**:\n   Here is an example of what the `celery_worker.py` file might look like:\n   ```python\n   from celery import Celery\n\n   celery = Celery('celery_worker', broker='pyamqp://guest@localhost//')\n\n   @celery.task\n   def add(x, y):\n       return x + y\n   ```\n\nBy ensuring the directory structure is correct, the module paths are properly set up, and using the `manage_services.sh` script to manage your services, you should be able to resolve the import issues for the `backend-core`, `worker`, `beat`, and `flower` services <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.dev.yml)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "sNor4pro",
        "body": "I don't see these files in the folder. \r\n\r\nSee below:\r\n\r\n```\r\nmike@pc:~/quivr/backend$ ls\r\napi              core        Dockerfile.dev  poetry.toml     supabase\r\nci-migration.sh  Dockerfile  poetry.lock     pyproject.toml\r\n```\r\nsnipped from docker-compose logs backend-core\r\n\r\n```backend-core  | INFO:     Waiting for child process [796]\r\nbackend-core  | INFO:     Child process [796] died\r\nbackend-core  | INFO:     Waiting for child process [808]\r\nbackend-core  | INFO:     Child process [808] died\r\nbackend-core  | INFO:     Waiting for child process [812]\r\nbackend-core  | INFO:     Child process [812] died\r\nbackend-core  | INFO:     Waiting for child process [820]\r\nbackend-core  | INFO:     Child process [820] died\r\nbackend-core  | INFO:     Waiting for child process [822]\r\nbackend-core  | INFO:     Child process [822] died\r\nbackend-core  | INFO:     Waiting for child process [824]\r\nbackend-core  | INFO:     Child process [824] died\r\nbackend-core  | ERROR:    Error loading ASGI app. Could not import module \"main\".\r\nbackend-core  | ERROR:    Error loading ASGI app. Could not import module \"main\".\r\nbackend-core  | ERROR:    Error loading ASGI app. Could not import module \"main\".\r\nbackend-core  | ERROR:    Error loading ASGI app. Could not import module \"main\".\r\nbackend-core  | ERROR:    Error loading ASGI app. Could not import module \"main\".\r\nbackend-core  | ERROR:    Error loading ASGI app. Could not import module \"main\".\r\nbackend-core  | INFO:     Received SIGTERM, exiting.\r\nbackend-core  | INFO:     Waiting for child process [808]\r\nbackend-core  | INFO:     Waiting for child process [820]\r\nbackend-core  | INFO:     Waiting for child process [824]\r\nbackend-core  | INFO:     Waiting for child process [832]\r\nbackend-core  | INFO:     Waiting for child process [834]\r\nbackend-core  | INFO:     Waiting for child process [836]\r\nbackend-core  | INFO:     Stopping parent process [1]\r\nmike@pc:~/quivr/backend$ \r\n```\r\nsnipped from worker log: \r\n```worker  | \r\nworker  | Error: Invalid value for '-A' / '--app': \r\nworker  | Unable to load celery application.\r\nworker  | The module celery_worker was not found.\r\nworker  | Usage: celery [OPTIONS] COMMAND [ARGS]...\r\nworker  | Try 'celery --help' for help.\r\nworker  | \r\nworker  | Error: Invalid value for '-A' / '--app': \r\nworker  | Unable to load celery application.\r\nworker  | The module celery_worker was not found.\r\nworker  | Usage: celery [OPTIONS] COMMAND [ARGS]...\r\nworker  | Try 'celery --help' for help.\r\nworker  | \r\nworker  | Error: Invalid value for '-A' / '--app': \r\nworker  | Unable to load celery application.\r\nworker  | The module celery_worker was not found.\r\n```\r\nsnipped from beat log:\r\n```beat  | Error: Invalid value for '-A' / '--app': \r\nbeat  | Unable to load celery application.\r\nbeat  | The module celery_worker was not found.\r\nbeat  | Usage: celery [OPTIONS] COMMAND [ARGS]...\r\nbeat  | Try 'celery --help' for help.\r\nbeat  | \r\nbeat  | Error: Invalid value for '-A' / '--app': \r\nbeat  | Unable to load celery application.\r\nbeat  | The module celery_worker was not found.\r\nbeat  | Usage: celery [OPTIONS] COMMAND [ARGS]...\r\nbeat  | Try 'celery --help' for help.\r\nbeat  | \r\nbeat  | Error: Invalid value for '-A' / '--app': \r\nbeat  | Unable to load celery application.\r\nbeat  | The module celery_worker was not found.\r\nbeat  | Usage: celery [OPTIONS] COMMAND [ARGS]...\r\nbeat  | Try 'celery --help' for help.\r\nbeat  | \r\nbeat  | Error: Invalid value for '-A' / '--app': \r\nbeat  | Unable to load celery application.\r\nbeat  | The module celery_worker was not found.\r\n```\r\nsnipped from flower log:\r\n```flower  | Error: Invalid value for '-A' / '--app': \r\nflower  | Unable to load celery application.\r\nflower  | The module celery_worker was not found.\r\nflower  | Usage: celery [OPTIONS] COMMAND [ARGS]...\r\nflower  | Try 'celery --help' for help.\r\nflower  | \r\nflower  | Error: Invalid value for '-A' / '--app': \r\nflower  | Unable to load celery application.\r\nflower  | The module celery_worker was not found.\r\nflower  | Usage: celery [OPTIONS] COMMAND [ARGS]...\r\nflower  | Try 'celery --help' for help.\r\nflower  | \r\nflower  | Error: Invalid value for '-A' / '--app': \r\nflower  | Unable to load celery application.\r\nflower  | The module celery_worker was not found.\r\n```\r\n\r\n\r\nCan I simply create them? I used your docker-compose. @dosu"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nThe errors indicate that the `main.py` and `celery_worker.py` files are missing from the `backend` directory. You can create these files with the following content:\n\n### `main.py`\nThis file sets up a basic FastAPI application.\n```python\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/healthz\")\ndef read_healthz():\n    return {\"status\": \"ok\"}\n```\n\n### `celery_worker.py`\nThis file sets up a basic Celery worker.\n```python\nfrom celery import Celery\n\ncelery = Celery('celery_worker', broker='redis://redis:6379/0')\n\n@celery.task\ndef add(x, y):\n    return x + y\n```\n\n### Directory Structure\nEnsure your `backend` directory looks like this:\n```\nbackend/\nâ”œâ”€â”€ api\nâ”œâ”€â”€ core\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ Dockerfile.dev\nâ”œâ”€â”€ poetry.lock\nâ”œâ”€â”€ poetry.toml\nâ”œâ”€â”€ pyproject.toml\nâ”œâ”€â”€ supabase\nâ”œâ”€â”€ main.py\nâ””â”€â”€ celery_worker.py\n```\n\n### Dockerfile\nEnsure your `Dockerfile` in the `backend` directory copies these files correctly:\n```dockerfile\nFROM python:3.9-slim\n\nWORKDIR /code\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"5050\", \"--workers\", \"6\"]\n```\n\n### docker-compose.yml\nEnsure your `docker-compose.yml` is correctly configured. Here is an example configuration:\n```yml\nversion: \"3.8\"\n\nservices:\n  backend-core:\n    build:\n      context: backend\n      dockerfile: Dockerfile\n    container_name: backend-core\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"http://localhost:5050/healthz\" ]\n    command:\n      - \"uvicorn\"\n      - \"main:app\"\n      - \"--host\"\n      - \"0.0.0.0\"\n      - \"--port\"\n      - \"5050\"\n      - \"--workers\"\n      - \"6\"\n    restart: always\n    volumes:\n      - ./backend/:/code/\n    ports:\n      - 5050:5050\n\n  redis:\n    image: redis:latest\n    container_name: redis\n    restart: always\n    ports:\n      - 6379:6379\n\n  worker:\n    build:\n      context: backend\n      dockerfile: Dockerfile\n    container_name: worker\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    command: celery -A celery_worker worker -l info\n    restart: always\n    depends_on:\n      - redis\n\n  beat:\n    build:\n      context: backend\n      dockerfile: Dockerfile\n    container_name: beat\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    command: celery -A celery_worker beat -l info\n    restart: always\n    depends_on:\n      - redis\n\n  flower:\n    build:\n      context: backend\n      dockerfile: Dockerfile\n    container_name: flower\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    command: celery -A celery_worker flower -l info --port=5555\n    restart: always\n    depends_on:\n      - redis\n      - worker\n      - beat\n    ports:\n      - 5555:5555\n```\n\nBy creating these files and ensuring the correct configuration, the `backend-core`, `worker`, `beat`, and `flower` services should function correctly <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.yml)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/celery_worker.py)</sup><sup>[[3]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.dev.yml)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "sNor4pro",
        "body": "this is my docker, looks like something is missing: @dosu\r\n```\r\n# Using a slim version for a smaller base image\r\nFROM python:3.11.6-slim-bullseye\r\n\r\nARG DEV_MODE\r\nENV DEV_MODE=$DEV_MODE\r\n\r\n# Install GEOS library, Rust, and other dependencies, then clean up\r\nRUN apt-get clean && apt-get update && apt-get install -y \\\r\n    libgeos-dev \\\r\n    libcurl4-openssl-dev \\\r\n    libssl-dev \\\r\n    binutils \\\r\n    pandoc \\\r\n    curl \\\r\n    git \\\r\n    poppler-utils \\\r\n    tesseract-ocr \\\r\n    autoconf \\\r\n    automake \\\r\n    build-essential \\\r\n    libtool \\\r\n    python-dev \\\r\n    wget \\\r\n    # Additional dependencies for document handling\r\n    libmagic-dev \\\r\n    poppler-utils \\\r\n    tesseract-ocr \\\r\n    libreoffice \\\r\n    libpq-dev \\\r\n    gcc \\\r\n    pandoc && \\\r\n    rm -rf /var/lib/apt/lists/*\r\n\r\n\r\nRUN ARCHITECTURE=$(uname -m) && \\\r\n    if [ \"$ARCHITECTURE\" = \"x86_64\" ]; then \\\r\n    wget https://github.com/supabase/cli/releases/download/v1.163.6/supabase_1.163.6_linux_amd64.deb && \\\r\n    dpkg -i supabase_1.163.6_linux_amd64.deb && \\\r\n    rm supabase_1.163.6_linux_amd64.deb; \\\r\n    elif [ \"$ARCHITECTURE\" = \"aarch64\" ]; then \\\r\n    wget https://github.com/supabase/cli/releases/download/v1.163.6/supabase_1.163.6_linux_arm64.deb && \\\r\n    dpkg -i supabase_1.163.6_linux_arm64.deb && \\\r\n    rm supabase_1.163.6_linux_arm64.deb; \\\r\n    fi && \\\r\n    rm -rf /var/lib/apt/lists/*\r\n\r\nRUN curl -sSL https://install.python-poetry.org | POETRY_HOME=/opt/poetry python && \\\r\n    cd /usr/local/bin && \\\r\n    ln -s /opt/poetry/bin/poetry && \\\r\n    poetry config virtualenvs.create false\r\n\r\n\r\n# Add Rust binaries to the PATH\r\nENV PATH=\"/root/.cargo/bin:${PATH}\" \\\r\n    POETRY_CACHE_DIR=/tmp/poetry_cache \\\r\n    PYTHONDONTWRITEBYTECODE=1\r\n\r\nWORKDIR /code\r\n\r\n# Copy monorepo dependencies\r\n# CORE\r\nCOPY core/pyproject.toml core/README.md core/poetry.lock  /code/core/\r\nCOPY core/quivr_core /code/core/quivr_core\r\n# API\r\nCOPY api/pyproject.toml  api/poetry.lock api/README.md /code/api/\r\nCOPY api/quivr_api /code/api/quivr_api\r\n\r\nCOPY ./pyproject.toml ./poetry.lock* /code/\r\n\r\nCOPY ./ci-migration.sh /code/\r\n\r\nCOPY supabase /code/supabase/\r\n\r\n# Run install\r\nRUN poetry install --no-root --with dev,test && \\\r\n    playwright install --with-deps && \\\r\n    rm -rf $POETRY_CACHE_DIR\r\n\r\n\r\nENV PYTHONPATH=/code\r\n\r\n\r\nEXPOSE 5050 \r\n```"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nIt looks like your `Dockerfile` is missing the steps to copy and set up the `main.py` and `celery_worker.py` files. Here are the steps to update your `Dockerfile` and ensure these files are included:\n\n### Updated `Dockerfile`\nAdd the following lines to copy the `main.py` and `celery_worker.py` files and set the entry point for the application:\n\n```dockerfile\n# Using a slim version for a smaller base image\nFROM python:3.11.6-slim-bullseye\n\nARG DEV_MODE\nENV DEV_MODE=$DEV_MODE\n\n# Install GEOS library, Rust, and other dependencies, then clean up\nRUN apt-get clean && apt-get update && apt-get install -y \\\n    libgeos-dev \\\n    libcurl4-openssl-dev \\\n    libssl-dev \\\n    binutils \\\n    pandoc \\\n    curl \\\n    git \\\n    poppler-utils \\\n    tesseract-ocr \\\n    autoconf \\\n    automake \\\n    build-essential \\\n    libtool \\\n    python-dev \\\n    wget \\\n    # Additional dependencies for document handling\n    libmagic-dev \\\n    poppler-utils \\\n    tesseract-ocr \\\n    libreoffice \\\n    libpq-dev \\\n    gcc \\\n    pandoc && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN ARCHITECTURE=$(uname -m) && \\\n    if [ \"$ARCHITECTURE\" = \"x86_64\" ]; then \\\n    wget https://github.com/supabase/cli/releases/download/v1.163.6/supabase_1.163.6_linux_amd64.deb && \\\n    dpkg -i supabase_1.163.6_linux_amd64.deb && \\\n    rm supabase_1.163.6_linux_amd64.deb; \\\n    elif [ \"$ARCHITECTURE\" = \"aarch64\" ]; then \\\n    wget https://github.com/supabase/cli/releases/download/v1.163.6/supabase_1.163.6_linux_arm64.deb && \\\n    dpkg -i supabase_1.163.6_linux_arm64.deb && \\\n    rm supabase_1.163.6_linux_arm64.deb; \\\n    fi && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN curl -sSL https://install.python-poetry.org | POETRY_HOME=/opt/poetry python && \\\n    cd /usr/local/bin && \\\n    ln -s /opt/poetry/bin/poetry && \\\n    poetry config virtualenvs.create false\n\n# Add Rust binaries to the PATH\nENV PATH=\"/root/.cargo/bin:${PATH}\" \\\n    POETRY_CACHE_DIR=/tmp/poetry_cache \\\n    PYTHONDONTWRITEBYTECODE=1\n\nWORKDIR /code\n\n# Copy monorepo dependencies\n# CORE\nCOPY core/pyproject.toml core/README.md core/poetry.lock  /code/core/\nCOPY core/quivr_core /code/core/quivr_core\n# API\nCOPY api/pyproject.toml  api/poetry.lock api/README.md /code/api/\nCOPY api/quivr_api /code/api/quivr_api\n\nCOPY ./pyproject.toml ./poetry.lock* /code/\n\nCOPY ./ci-migration.sh /code/\n\nCOPY supabase /code/supabase/\n\n# Copy main.py and celery_worker.py\nCOPY main.py /code/\nCOPY celery_worker.py /code/\n\n# Run install\nRUN poetry install --no-root --with dev,test && \\\n    playwright install --with-deps && \\\n    rm -rf $POETRY_CACHE_DIR\n\nENV PYTHONPATH=/code\n\nEXPOSE 5050\n\n# Set the entry point for the application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"5050\"]\n```\n\n### `main.py`\nCreate a `main.py` file with the following content:\n```python\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/healthz\")\ndef read_healthz():\n    return {\"status\": \"ok\"}\n```\n\n### `celery_worker.py`\nCreate a `celery_worker.py` file with the following content:\n```python\nfrom celery import Celery\n\ncelery = Celery('celery_worker', broker='redis://redis:6379/0')\n\n@celery.task\ndef add(x, y):\n    return x + y\n```\n\n### Directory Structure\nEnsure your `backend` directory looks like this:\n```\nbackend/\nâ”œâ”€â”€ api\nâ”œâ”€â”€ core\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ Dockerfile.dev\nâ”œâ”€â”€ poetry.lock\nâ”œâ”€â”€ poetry.toml\nâ”œâ”€â”€ pyproject.toml\nâ”œâ”€â”€ supabase\nâ”œâ”€â”€ main.py\nâ””â”€â”€ celery_worker.py\n```\n\nBy updating your `Dockerfile` to include the `main.py` and `celery_worker.py` files and ensuring the correct directory structure, the `backend-core`, `worker`, `beat`, and `flower` services should function correctly <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/celery_worker.py)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/main.py)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "sNor4pro",
        "body": "No error spams anymore, this is good, but now i can't create a brain or make go into my profile see: @dosu\r\n\r\n```\r\nbackend-core  | INFO:     172.19.0.1:38630 - \"OPTIONS /user/identity HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:38632 - \"OPTIONS /user/credits HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:33406 - \"OPTIONS /user HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:38630 - \"OPTIONS /user/identity HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:38632 - \"OPTIONS /onboarding HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:33406 - \"OPTIONS /user HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:38630 - \"OPTIONS /user/identity HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:38632 - \"OPTIONS /user/credits HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:33406 - \"OPTIONS /onboarding HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:38630 - \"OPTIONS /onboarding HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:38632 - \"OPTIONS /onboarding HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:33406 - \"OPTIONS /user HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:38630 - \"OPTIONS /user/identity HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:38632 - \"OPTIONS /onboarding HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     127.0.0.1:56878 - \"GET /healthz HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /user HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /user/identity HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /user/credits HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /onboarding HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /user/credits HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /user HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /user/identity HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /onboarding HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /user/credits HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /user/credits HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /user/credits HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /user HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59802 - \"OPTIONS /user/identity HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:59806 - \"OPTIONS /onboarding HTTP/1.1\" 404 Not Found\r\n```\r\n"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nThe 404 errors indicate that the endpoints for `/user`, `/user/identity`, `/user/credits`, and `/onboarding` are not defined in your FastAPI application. You need to ensure that these endpoints are correctly implemented in your `main.py` or other relevant files.\n\nHere is an example of how you can define these endpoints in your `main.py`:\n\n### `main.py`\n```python\nfrom fastapi import FastAPI, HTTPException\n\napp = FastAPI()\n\n@app.get(\"/healthz\")\ndef read_healthz():\n    return {\"status\": \"ok\"}\n\n@app.options(\"/user\")\ndef options_user():\n    return {\"status\": \"ok\"}\n\n@app.options(\"/user/identity\")\ndef options_user_identity():\n    return {\"status\": \"ok\"}\n\n@app.options(\"/user/credits\")\ndef options_user_credits():\n    return {\"status\": \"ok\"}\n\n@app.options(\"/onboarding\")\ndef options_onboarding():\n    return {\"status\": \"ok\"}\n\n@app.get(\"/user/identity\")\ndef get_user_identity():\n    # Implement your logic here\n    return {\"identity\": \"user_identity\"}\n\n@app.get(\"/user/credits\")\ndef get_user_credits():\n    # Implement your logic here\n    return {\"credits\": \"user_credits\"}\n\n@app.get(\"/onboarding\")\ndef get_onboarding():\n    # Implement your logic here\n    return {\"onboarding\": \"onboarding_info\"}\n```\n\n### Directory Structure\nEnsure your `backend` directory looks like this:\n```\nbackend/\nâ”œâ”€â”€ api\nâ”œâ”€â”€ core\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ Dockerfile.dev\nâ”œâ”€â”€ poetry.lock\nâ”œâ”€â”€ poetry.toml\nâ”œâ”€â”€ pyproject.toml\nâ”œâ”€â”€ supabase\nâ”œâ”€â”€ main.py\nâ””â”€â”€ celery_worker.py\n```\n\n### Dockerfile\nEnsure your `Dockerfile` in the `backend` directory copies these files correctly:\n```dockerfile\n# Using a slim version for a smaller base image\nFROM python:3.11.6-slim-bullseye\n\nARG DEV_MODE\nENV DEV_MODE=$DEV_MODE\n\n# Install GEOS library, Rust, and other dependencies, then clean up\nRUN apt-get clean && apt-get update && apt-get install -y \\\n    libgeos-dev \\\n    libcurl4-openssl-dev \\\n    libssl-dev \\\n    binutils \\\n    pandoc \\\n    curl \\\n    git \\\n    poppler-utils \\\n    tesseract-ocr \\\n    autoconf \\\n    automake \\\n    build-essential \\\n    libtool \\\n    python-dev \\\n    wget \\\n    # Additional dependencies for document handling\n    libmagic-dev \\\n    poppler-utils \\\n    tesseract-ocr \\\n    libreoffice \\\n    libpq-dev \\\n    gcc \\\n    pandoc && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN ARCHITECTURE=$(uname -m) && \\\n    if [ \"$ARCHITECTURE\" = \"x86_64\" ]; then \\\n    wget https://github.com/supabase/cli/releases/download/v1.163.6/supabase_1.163.6_linux_amd64.deb && \\\n    dpkg -i supabase_1.163.6_linux_amd64.deb && \\\n    rm supabase_1.163.6_linux_amd64.deb; \\\n    elif [ \"$ARCHITECTURE\" = \"aarch64\" ]; then \\\n    wget https://github.com/supabase/cli/releases/download/v1.163.6/supabase_1.163.6_linux_arm64.deb && \\\n    dpkg -i supabase_1.163.6_linux_arm64.deb && \\\n    rm supabase_1.163.6_linux_arm64.deb; \\\n    fi && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN curl -sSL https://install.python-poetry.org | POETRY_HOME=/opt/poetry python && \\\n    cd /usr/local/bin && \\\n    ln -s /opt/poetry/bin/poetry && \\\n    poetry config virtualenvs.create false\n\n# Add Rust binaries to the PATH\nENV PATH=\"/root/.cargo/bin:${PATH}\" \\\n    POETRY_CACHE_DIR=/tmp/poetry_cache \\\n    PYTHONDONTWRITEBYTECODE=1\n\nWORKDIR /code\n\n# Copy monorepo dependencies\n# CORE\nCOPY core/pyproject.toml core/README.md core/poetry.lock  /code/core/\nCOPY core/quivr_core /code/core/quivr_core\n# API\nCOPY api/pyproject.toml  api/poetry.lock api/README.md /code/api/\nCOPY api/quivr_api /code/api/quivr_api\n\nCOPY ./pyproject.toml ./poetry.lock* /code/\n\nCOPY ./ci-migration.sh /code/\n\nCOPY supabase /code/supabase/\n\n# Copy main.py and celery_worker.py\nCOPY main.py /code/\nCOPY celery_worker.py /code/\n\n# Run install\nRUN poetry install --no-root --with dev,test && \\\n    playwright install --with-deps && \\\n    rm -rf $POETRY_CACHE_DIR\n\nENV PYTHONPATH=/code\n\nEXPOSE 5050\n\n# Set the entry point for the application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"5050\"]\n```\n\nBy defining these endpoints and ensuring the correct directory structure and Dockerfile configuration, the backend service should be able to handle the requests to `/user`, `/user/identity`, `/user/credits`, and `/onboarding` without returning 404 errors <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/brain/controller/brain_routes.py)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/brain/api_brain_qa.py)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "sNor4pro",
        "body": "I edited the main.py and dockerfile with your data, stil the same issue: @dosu\r\n```\r\n\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /chat HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:49054 - \"OPTIONS /user HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49080 - \"OPTIONS /onboarding HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49062 - \"OPTIONS /user/identity HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /brains/ HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:49054 - \"OPTIONS /onboarding HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49080 - \"OPTIONS /user/identity HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /user HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /onboarding HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49062 - \"OPTIONS /user/identity HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /user HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /user/credits HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /user/credits HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /onboarding HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49062 - \"OPTIONS /user/identity HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /user HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /onboarding HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /user/identity HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49062 - \"OPTIONS /user HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /onboarding HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /user/identity HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /user HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /chat HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /onboarding HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49062 - \"OPTIONS /user HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:45650 - \"OPTIONS /user/identity HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /brains/ HTTP/1.1\" 404 Not Found\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /onboarding HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:45650 - \"OPTIONS /user/identity HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:49066 - \"OPTIONS /user HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     127.0.0.1:45450 - \"GET /healthz HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:45840 - \"OPTIONS /user/identity HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:45840 - \"OPTIONS /user/identity HTTP/1.1\" 200 OK\r\nbackend-core  | INFO:     172.19.0.1:45840 - \"OPTIONS /user/identity HTTP/1.1\" 200 OK\r\n```\r\n"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nIt looks like the 404 errors for the `/chat` and `/brains/` endpoints indicate that these endpoints are not defined in your FastAPI application. You need to ensure that these endpoints are correctly implemented and registered in your FastAPI router.\n\n### Define and Register Endpoints\n\n1. **Create `chat_routes.py` and `brain_routes.py`**:\n   Ensure you have the following files in your `backend/api/quivr_api/modules` directory:\n\n   - `chat_routes.py`\n   - `brain_routes.py`\n\n2. **Implement Endpoints in `chat_routes.py`**:\n   Here is an example of how you can define the `/chat` endpoints:\n\n   ```python\n   from fastapi import APIRouter, Depends\n   from some_auth_module import AuthBearer  # Replace with your actual auth module\n   from some_user_module import get_current_user, UserIdentity  # Replace with your actual user module\n   from some_chat_service import chat_service  # Replace with your actual chat service\n\n   chat_router = APIRouter()\n\n   @chat_router.post(\"/chat\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"])\n   async def create_chat(name: str, current_user: UserIdentity = Depends(get_current_user)):\n       \"\"\"Create a new chat.\"\"\"\n       created_chat = chat_service.create_chat(name, current_user.id)\n       return {\"chat\": created_chat}\n\n   @chat_router.get(\"/chat\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"])\n   async def get_chats(current_user: UserIdentity = Depends(get_current_user)):\n       \"\"\"Retrieve all chats for the current user.\"\"\"\n       chats = chat_service.get_chats(current_user.id)\n       return {\"chats\": chats}\n\n   @chat_router.delete(\"/chat/{chat_id}\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"])\n   async def delete_chat(chat_id: str, current_user: UserIdentity = Depends(get_current_user)):\n       \"\"\"Delete a specific chat.\"\"\"\n       chat_service.delete_chat(chat_id, current_user.id)\n       return {\"message\": \"Chat deleted\"}\n\n   @chat_router.post(\"/chat/{chat_id}/question\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"])\n   async def add_question(chat_id: str, brain_id: str, chat_question: ChatQuestion, current_user: UserIdentity = Depends(get_current_user)):\n       \"\"\"Add a question to a specific chat.\"\"\"\n       response = chat_service.add_question(chat_id, brain_id, chat_question, current_user.id)\n       return {\"response\": response}\n\n   @chat_router.get(\"/chat/{chat_id}/history\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"])\n   async def get_chat_items(chat_id: str, current_user: UserIdentity = Depends(get_current_user)):\n       \"\"\"Retrieve chat history for a specific chat.\"\"\"\n       chat_items = chat_service.get_chat_items(chat_id, current_user.id)\n       return {\"chat_items\": chat_items}\n\n   @chat_router.put(\"/chat/{chat_id}/metadata\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"])\n   async def update_chat(chat_id: str, chat: ChatUpdatableProperties, current_user: UserIdentity = Depends(get_current_user)):\n       \"\"\"Update chat metadata.\"\"\"\n       updated_chat = chat_service.update_chat(chat_id, chat, current_user.id)\n       return {\"chat\": updated_chat}\n\n   @chat_router.put(\"/chat/{chat_id}/{message_id}\", dependencies=[Depends(AuthBearer())], tags=[\"Chat\"])\n   async def update_chat_message(chat_id: str, message_id: str, chat_message_updatable_properties: ChatMessageUpdatableProperties, current_user: UserIdentity = Depends(get_current_user)):\n       \"\"\"Update a specific chat message.\"\"\"\n       updated_message = chat_service.update_chat_message(chat_id, message_id, chat_message_updatable_properties, current_user.id)\n       return {\"message\": updated_message}\n   ```\n\n3. **Implement Endpoints in `brain_routes.py`**:\n   Here is an example of how you can define the `/brains/` endpoints:\n\n   ```python\n   from fastapi import APIRouter, Depends\n   from some_auth_module import AuthBearer  # Replace with your actual auth module\n   from some_user_module import get_current_user, UserIdentity  # Replace with your actual user module\n   from some_brain_service import brain_user_service  # Replace with your actual brain service\n\n   brain_router = APIRouter()\n\n   @brain_router.get(\"/brains/\", dependencies=[Depends(AuthBearer())], tags=[\"Brain\"])\n   async def retrieve_all_brains_for_user(current_user: UserIdentity = Depends(get_current_user)):\n       \"\"\"Retrieve all brains for the current user.\"\"\"\n       brains = brain_user_service.get_user_brains(current_user.id)\n       return {\"brains\": brains}\n   ```\n\n4. **Register Routers in `main.py`**:\n   Ensure you register these routers in your `main.py`:\n\n   ```python\n   from fastapi import FastAPI\n   from backend.api.quivr_api.modules.chat_routes import chat_router\n   from backend.api.quivr_api.modules.brain_routes import brain_router\n\n   app = FastAPI()\n\n   app.include_router(chat_router)\n   app.include_router(brain_router)\n\n   @app.get(\"/healthz\")\n   def read_healthz():\n       return {\"status\": \"ok\"}\n   ```\n\n5. **Update Dockerfile**:\n   Ensure your `Dockerfile` is correctly set up to copy all necessary files:\n\n   ```dockerfile\n   # Using a slim version for a smaller base image\n   FROM python:3.11.6-slim-bullseye\n\n   ARG DEV_MODE\n   ENV DEV_MODE=$DEV_MODE\n\n   # Install dependencies\n   RUN apt-get clean && apt-get update && apt-get install -y \\\n       libgeos-dev \\\n       libcurl4-openssl-dev \\\n       libssl-dev \\\n       binutils \\\n       pandoc \\\n       curl \\\n       git \\\n       poppler-utils \\\n       tesseract-ocr \\\n       autoconf \\\n       automake \\\n       build-essential \\\n       libtool \\\n       python-dev \\\n       wget \\\n       libmagic-dev \\\n       poppler-utils \\\n       tesseract-ocr \\\n       libreoffice \\\n       libpq-dev \\\n       gcc \\\n       pandoc && \\\n       rm -rf /var/lib/apt/lists/*\n\n   RUN ARCHITECTURE=$(uname -m) && \\\n       if [ \"$ARCHITECTURE\" = \"x86_64\" ]; then \\\n       wget https://github.com/supabase/cli/releases/download/v1.163.6/supabase_1.163.6_linux_amd64.deb && \\\n       dpkg -i supabase_1.163.6_linux_amd64.deb && \\\n       rm supabase_1.163.6_linux_amd64.deb; \\\n       elif [ \"$ARCHITECTURE\" = \"aarch64\" ]; then \\\n       wget https://github.com/supabase/cli/releases/download/v1.163.6/supabase_1.163.6_linux_arm64.deb && \\\n       dpkg -i supabase_1.163.6_linux_arm64.deb && \\\n       rm supabase_1.163.6_linux_arm64.deb; \\\n       fi && \\\n       rm -rf /var/lib/apt/lists/*\n\n   RUN curl -sSL https://install.python-poetry.org | POETRY_HOME=/opt/poetry python && \\\n       cd /usr/local/bin && \\\n       ln -s /opt/poetry/bin/poetry && \\\n       poetry config virtualenvs.create false\n\n   # Add Rust binaries to the PATH\n   ENV PATH=\"/root/.cargo/bin:${PATH}\" \\\n       POETRY_CACHE_DIR=/tmp/poetry_cache \\\n       PYTHONDONTWRITEBYTECODE=1\n\n   WORKDIR /code\n\n   # Copy monorepo dependencies\n   # CORE\n   COPY core/pyproject.toml core/README.md core/poetry.lock  /code/core/\n   COPY core/quivr_core /code/core/quivr_core\n   # API\n   COPY api/pyproject.toml  api/poetry.lock api/README.md /code/api/\n   COPY api/quivr_api /code/api/quivr_api\n\n   COPY ./pyproject.toml ./poetry.lock* /code/\n\n   COPY ./ci-migration.sh /code/\n\n   COPY supabase /code/supabase/\n\n   # Copy main.py and celery_worker.py\n   COPY main.py /code/\n   COPY celery_worker.py /code/\n\n   # Run install\n   RUN poetry install --no-root --with dev,test && \\\n       playwright install --with-deps && \\\n       rm -rf $POETRY_CACHE_DIR\n\n   ENV PYTHONPATH=/code\n\n   EXPOSE 5050\n\n   # Set the entry point for the application\n   CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"5050\"]\n   ```\n\nBy defining and registering these endpoints, the backend service should be able to handle the requests to `/chat` and `/brains/` without returning 404 errors <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/brain/controller/brain_routes.py)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/api/quivr_api/modules/chat/controller/chat_routes.py)</sup>.\n\nIf you still encounter issues, it might be helpful to loop in a maintainer for further assistance. This is a tough nut to crack, could really use your help on this, @StanGirard.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "sNor4pro",
        "body": "Other errors now: @dosu\r\n\r\n```\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\nbackend-core  | Process SpawnProcess-38:\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/supervisors/multiprocess.py\", line 63, in target\r\nbackend-core  |     return self.real_target(sockets)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-core  |     await self._serve(sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 22, in import_from_string\r\nbackend-core  |     raise exc from None\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\nbackend-core  | Process SpawnProcess-37:\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/supervisors/multiprocess.py\", line 63, in target\r\nbackend-core  |     return self.real_target(sockets)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-core  |     await self._serve(sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 22, in import_from_string\r\nbackend-core  |     raise exc from None\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\nbackend-core  | INFO:     Waiting for child process [51]\r\nbackend-core  | INFO:     Child process [51] died\r\nbackend-core  | INFO:     Waiting for child process [63]\r\nbackend-core  | INFO:     Child process [63] died\r\nbackend-core  | INFO:     Waiting for child process [72]\r\nbackend-core  | INFO:     Child process [72] died\r\nbackend-core  | Process SpawnProcess-39:\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/supervisors/multiprocess.py\", line 63, in target\r\nbackend-core  |     return self.real_target(sockets)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-core  |     await self._serve(sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 22, in import_from_string\r\nbackend-core  |     raise exc from None\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\nbackend-core  | Process SpawnProcess-41:\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/supervisors/multiprocess.py\", line 63, in target\r\nbackend-core  |     return self.real_target(sockets)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-core  |     await self._serve(sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 22, in import_from_string\r\nbackend-core  |     raise exc from None\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\nbackend-core  | Process SpawnProcess-40:\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/supervisors/multiprocess.py\", line 63, in target\r\nbackend-core  |     return self.real_target(sockets)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-core  |     await self._serve(sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 22, in import_from_string\r\nbackend-core  |     raise exc from None\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\nbackend-core  | INFO:     Waiting for child process [75]\r\nbackend-core  | INFO:     Child process [75] died\r\nbackend-core  | INFO:     Waiting for child process [77]\r\nbackend-core  | INFO:     Child process [77] died\r\nbackend-core  | INFO:     Waiting for child process [78]\r\nbackend-core  | INFO:     Child process [78] died\r\nbackend-core  | Process SpawnProcess-44:\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/supervisors/multiprocess.py\", line 63, in target\r\nbackend-core  |     return self.real_target(sockets)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-core  |     await self._serve(sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 22, in import_from_string\r\nbackend-core  |     raise exc from None\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\nbackend-core  | Process SpawnProcess-43:\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/supervisors/multiprocess.py\", line 63, in target\r\nbackend-core  |     return self.real_target(sockets)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-core  |     await self._serve(sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 22, in import_from_string\r\nbackend-core  |     raise exc from None\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\nbackend-core  | Process SpawnProcess-42:\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/supervisors/multiprocess.py\", line 63, in target\r\nbackend-core  |     return self.real_target(sockets)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-core  |     await self._serve(sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 22, in import_from_string\r\nbackend-core  |     raise exc from None\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\nbackend-core  | INFO:     Waiting for child process [63]\r\nbackend-core  | INFO:     Child process [63] died\r\nbackend-core  | INFO:     Waiting for child process [75]\r\nbackend-core  | INFO:     Child process [75] died\r\nbackend-core  | INFO:     Waiting for child process [78]\r\nbackend-core  | INFO:     Child process [78] died\r\nbackend-core  | INFO:     Waiting for child process [85]\r\nbackend-core  | INFO:     Child process [85] died\r\nbackend-core  | Process SpawnProcess-47:\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/supervisors/multiprocess.py\", line 63, in target\r\nbackend-core  |     return self.real_target(sockets)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-core  |     await self._serve(sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 22, in import_from_string\r\nbackend-core  |     raise exc from None\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\nbackend-core  | Process SpawnProcess-48:\r\nbackend-core  | Process SpawnProcess-46:\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/supervisors/multiprocess.py\", line 63, in target\r\nbackend-core  |     return self.real_target(sockets)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-core  |     await self._serve(sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 22, in import_from_string\r\nbackend-core  |     raise exc from None\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/supervisors/multiprocess.py\", line 63, in target\r\nbackend-core  |     return self.real_target(sockets)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-core  |     await self._serve(sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 22, in import_from_string\r\nbackend-core  |     raise exc from None\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\nbackend-core  | Process SpawnProcess-45:\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 80, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/supervisors/multiprocess.py\", line 63, in target\r\nbackend-core  |     return self.real_target(sockets)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\r\nbackend-core  |     await self._serve(sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 22, in import_from_string\r\nbackend-core  |     raise exc from None\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 2, in <module>\r\nbackend-core  |     from backend.api.quivr_api.modules.chat_routes import chat_router\r\nbackend-core  | ModuleNotFoundError: No module named 'backend'\r\n```\r\n\r\n\r\n"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nThe `ModuleNotFoundError: No module named 'backend'` error indicates that the `backend` module is not being found in the Python path. This is likely due to the way the Docker container is set up and how the Python path is configured.\n\nHere are the steps to resolve this issue:\n\n### 1. Ensure Correct Directory Structure\nMake sure your project directory structure is correct and includes all necessary files:\n\n```\nquivr/\nâ”œâ”€â”€ backend/\nâ”‚   â”œâ”€â”€ api/\nâ”‚   â”‚   â”œâ”€â”€ quivr_api/\nâ”‚   â”‚   â”‚   â”œâ”€â”€ modules/\nâ”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ chat_routes.py\nâ”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ brain_routes.py\nâ”‚   â”‚   â”‚   â””â”€â”€ __init__.py\nâ”‚   â”‚   â””â”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ core/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ Dockerfile.dev\nâ”‚   â”œâ”€â”€ poetry.lock\nâ”‚   â”œâ”€â”€ poetry.toml\nâ”‚   â”œâ”€â”€ pyproject.toml\nâ”‚   â”œâ”€â”€ supabase/\nâ”‚   â”œâ”€â”€ main.py\nâ”‚   â””â”€â”€ celery_worker.py\nâ”œâ”€â”€ docker-compose.yml\nâ””â”€â”€ other_files...\n```\n\n### 2. Update `main.py`\nEnsure your `main.py` correctly imports the modules:\n\n```python\nfrom fastapi import FastAPI\nfrom backend.api.quivr_api.modules.chat_routes import chat_router\nfrom backend.api.quivr_api.modules.brain_routes import brain_router\n\napp = FastAPI()\n\napp.include_router(chat_router)\napp.include_router(brain_router)\n\n@app.get(\"/healthz\")\ndef read_healthz():\n    return {\"status\": \"ok\"}\n```\n\n### 3. Set the `PYTHONPATH` Environment Variable\nEnsure that the `PYTHONPATH` environment variable is set correctly in your Docker container. You can do this by adding it to your `docker-compose.yml` file or directly in the Dockerfile.\n\n#### Option 1: Set `PYTHONPATH` in `docker-compose.yml`\nAdd the `PYTHONPATH` environment variable to the `backend-core` service in your `docker-compose.yml`:\n\n```yaml\nversion: \"3.8\"\n\nservices:\n  backend-core:\n    build:\n      context: backend\n      dockerfile: Dockerfile\n    container_name: backend-core\n    environment:\n      - PYTHONPATH=/code\n    extra_hosts:\n      - \"host.docker.internal:host-gateway\"\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"http://localhost:5050/healthz\" ]\n    command:\n      - \"uvicorn\"\n      - \"main:app\"\n      - \"--host\"\n      - \"0.0.0.0\"\n      - \"--port\"\n      - \"5050\"\n      - \"--workers\"\n      - \"6\"\n    restart: always\n    volumes:\n      - ./backend/:/code/\n    ports:\n      - 5050:5050\n\n  # Other services...\n```\n\n#### Option 2: Set `PYTHONPATH` in Dockerfile\nAdd the `PYTHONPATH` environment variable to your Dockerfile:\n\n```dockerfile\n# Using a slim version for a smaller base image\nFROM python:3.11.6-slim-bullseye\n\nARG DEV_MODE\nENV DEV_MODE=$DEV_MODE\n\n# Install dependencies\nRUN apt-get clean && apt-get update && apt-get install -y \\\n    libgeos-dev \\\n    libcurl4-openssl-dev \\\n    libssl-dev \\\n    binutils \\\n    pandoc \\\n    curl \\\n    git \\\n    poppler-utils \\\n    tesseract-ocr \\\n    autoconf \\\n    automake \\\n    build-essential \\\n    libtool \\\n    python-dev \\\n    wget \\\n    libmagic-dev \\\n    poppler-utils \\\n    tesseract-ocr \\\n    libreoffice \\\n    libpq-dev \\\n    gcc \\\n    pandoc && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN ARCHITECTURE=$(uname -m) && \\\n    if [ \"$ARCHITECTURE\" = \"x86_64\" ]; then \\\n    wget https://github.com/supabase/cli/releases/download/v1.163.6/supabase_1.163.6_linux_amd64.deb && \\\n    dpkg -i supabase_1.163.6_linux_amd64.deb && \\\n    rm supabase_1.163.6_linux_amd64.deb; \\\n    elif [ \"$ARCHITECTURE\" = \"aarch64\" ]; then \\\n    wget https://github.com/supabase/cli/releases/download/v1.163.6/supabase_1.163.6_linux_arm64.deb && \\\n    dpkg -i supabase_1.163.6_linux_arm64.deb && \\\n    rm supabase_1.163.6_linux_arm64.deb; \\\n    fi && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN curl -sSL https://install.python-poetry.org | POETRY_HOME=/opt/poetry python && \\\n    cd /usr/local/bin && \\\n    ln -s /opt/poetry/bin/poetry && \\\n    poetry config virtualenvs.create false\n\n# Add Rust binaries to the PATH\nENV PATH=\"/root/.cargo/bin:${PATH}\" \\\n    POETRY_CACHE_DIR=/tmp/poetry_cache \\\n    PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONPATH=/code\n\nWORKDIR /code\n\n# Copy monorepo dependencies\n# CORE\nCOPY core/pyproject.toml core/README.md core/poetry.lock  /code/core/\nCOPY core/quivr_core /code/core/quivr_core\n# API\nCOPY api/pyproject.toml  api/poetry.lock api/README.md /code/api/\nCOPY api/quivr_api /code/api/quivr_api\n\nCOPY ./pyproject.toml ./poetry.lock* /code/\n\nCOPY ./ci-migration.sh /code/\n\nCOPY supabase /code/supabase/\n\n# Copy main.py and celery_worker.py\nCOPY main.py /code/\nCOPY celery_worker.py /code/\n\n# Run install\nRUN poetry install --no-root --with dev,test && \\\n    playwright install --with-deps && \\\n    rm -rf $POETRY_CACHE_DIR\n\nEXPOSE 5050\n\n# Set the entry point for the application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"5050\"]\n```\n\n### 4. Rebuild and Restart Docker Containers\nAfter making these changes, rebuild your Docker images and restart the containers:\n\n```sh\ndocker-compose down\ndocker-compose up --build\n```\n\nBy ensuring the `PYTHONPATH` is correctly set and the directory structure is correct, the `backend-core` service should be able to find the `backend` module and start without errors <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.dev.yml)</sup>.\n\nIf you still encounter issues, it might be helpful to loop in a maintainer for further assistance. This is a tough nut to crack, could really use your help on this, @StanGirard.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "sNor4pro",
        "body": "I don't think is gonna work, tried all what you said.\r\nCann you give me step by step all commands with dependencies from scratch ubuntu to build Quivr? @dosu"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo build and run Quivr from scratch on a fresh Ubuntu installation, follow these detailed steps:\n\n### Step-by-Step Guide\n\n#### Step 0: Install Supabase CLI\n\n1. **Install Supabase CLI**:\n   Follow the instructions [here](https://supabase.com/docs/guides/cli/getting-started) to install the Supabase CLI.\n\n   ```bash\n   supabase -v # Check that the installation worked\n   ```\n\n#### Step 1: Install Docker and Docker Compose\n\n1. **Update your package list**:\n\n   ```bash\n   sudo apt-get update\n   ```\n\n2. **Install Docker**:\n\n   ```bash\n   sudo apt-get install -y docker.io\n   ```\n\n3. **Install Docker Compose**:\n\n   ```bash\n   sudo apt-get install -y docker-compose\n   ```\n\n4. **Start and enable Docker service**:\n\n   ```bash\n   sudo systemctl start docker\n   sudo systemctl enable docker\n   ```\n\n5. **Add your user to the Docker group** (to avoid using `sudo` with Docker commands):\n\n   ```bash\n   sudo usermod -aG docker $USER\n   newgrp docker\n   ```\n\n#### Step 2: Clone the Quivr Repository\n\n1. **Clone the repository**:\n\n   ```bash\n   git clone https://github.com/quivrhq/quivr.git && cd quivr\n   ```\n\n#### Step 3: Configure Environment Variables\n\n1. **Copy the `.env.example` file to `.env`**:\n\n   ```bash\n   cp .env.example .env\n   ```\n\n2. **Update the `.env` file**:\n\n   ```bash\n   vim .env # or use nano, emacs, or vscode\n   ```\n\n   Update the `OPENAI_API_KEY` variable in the `.env` file. You can get your API key [here](https://platform.openai.com/api-keys).\n\n   > Want to use [Ollama.ai](https://ollama.ai) instead?\n   > Uncomment the following lines in the `.env` file:\n   > OLLAMA_API_BASE_URL\n   > Run the following command to start Ollama: `ollama run llama2`\n   > You can find more information about Ollama [here](https://ollama.ai/).\n\n#### Step 4: Install Additional Dependencies\n\n1. **Install additional dependencies required for document handling**:\n\n   ```bash\n   sudo apt-get install -y libgeos-dev libcurl4-openssl-dev libssl-dev binutils curl git autoconf automake build-essential libtool python-dev libmagic-dev poppler-utils tesseract-ocr libreoffice libpq-dev gcc pandoc\n   ```\n\n#### Step 5: Launch the Project\n\n1. **Start Supabase**:\n\n   ```bash\n   cd backend && supabase start\n   ```\n\n2. **Pull and start Docker containers**:\n\n   ```bash\n   cd ../\n   docker compose pull\n   docker compose up\n   ```\n\n   If you are a developer, you can run the project in development mode with the following command:\n\n   ```bash\n   docker compose -f docker-compose.dev.yml up --build\n   ```\n\n#### Step 6: Access the Application\n\n1. **Login to the app**:\n   You can now sign in to the app with `admin@quivr.app` & `admin`. Access the app at [http://localhost:3000/login](http://localhost:3000/login).\n\n2. **Access Quivr backend API**:\n   [http://localhost:5050/docs](http://localhost:5050/docs)\n\n3. **Access Supabase**:\n   [http://localhost:54323](http://localhost:54323)\n\nBy following these steps, you should have Quivr up and running on your fresh Ubuntu installation. For more detailed information, you can refer to the [documentation](https://docs.quivr.app/) <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/README.md)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/Dockerfile.dev)</sup><sup>[[3]](https://github.com/QuivrHQ/quivr/blob/main/docs/install.mdx)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "saitho",
        "body": "I also had that issue. After running `docker compose -f docker-compose.dev.yml up --build` it worked..."
      },
      {
        "user": "sNor4pro",
        "body": "I tried this command too, but i had no luck. "
      }
    ]
  },
  {
    "issue_number": 2941,
    "title": "[Bug]: update error",
    "author": "johnfelipe",
    "state": "closed",
    "created_at": "2024-08-04T17:56:09Z",
    "updated_at": "2024-11-25T13:38:22Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\nA bug happened!\r\n\r\n    git pull\r\n\r\n\r\nStep 2: Update the migration\r\n\r\n    supabase migration up\r\n\r\nand something happen\r\n\r\n    cannot read config in /home/felipe/Quivr: open supabase/config.toml: no such file or directory\r\n    Have you set up the project with supabase init?\r\n\r\nhow can solve this?\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/TICK-37/[bug]-update-error\">TICK-37 [Bug]: update error</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "Hey @johnfelipe! I'm here to help you with any bugs or questions you may have. I can assist with solving issues, answering questions, and becoming a contributor. Let me know how I can assist you!\n\nTo resolve the issue of the missing `supabase/config.toml` file, you need to create the `config.toml` file in the `supabase` directory with the appropriate configuration. Here is an example of what the `config.toml` file should look like:\n\n```toml\n# A string used to distinguish different Supabase projects on the same host. Defaults to the\n# working directory name when running `supabase init`.\nproject_id = \"secondbrain\"\n\n[api]\nenabled = true\n# Port to use for the API URL.\nport = 54321\n# Schemas to expose in your API. Tables, views and stored procedures in this schema will get API\n# endpoints. public and storage are always included.\nschemas = [\"public\", \"storage\", \"graphql_public\", \"vault\"]\n# Extra schemas to add to the search_path of every request. public is always included.\nextra_search_path = [\"public\", \"extensions\", \"stripe\", \"vault\"]\n# The maximum number of rows returns from a view, table, or stored procedure. Limits payload size\n# for accidental or malicious requests.\nmax_rows = 1000\n\n[db]\n# Port to use for the local database URL.\nport = 54322\n# Port used by db diff command to initialize the shadow database.\nshadow_port = 54320\n# The database major version to use. This has to be the same as your remote database's. Run `SHOW\n# server_version;` on the remote database to check.\nmajor_version = 15\n\n[db.pooler]\nenabled = false\n# Port to use for the local connection pooler.\nport = 54329\n# Specifies when a server connection can be reused by other clients.\n# Configure one of the supported pooler modes: `transaction`, `session`.\npool_mode = \"transaction\"\n# How many server connections to allow per user/database pair.\ndefault_pool_size = 20\n# Maximum number of client connections allowed.\nmax_client_conn = 100\n\n[realtime]\nenabled = true\n# Bind realtime via either IPv4 or IPv6. (default: IPv6)\n# ip_version = \"IPv6\"\n# The maximum length in bytes of HTTP request headers. (default: 4096)\n# max_header_length = 4096\n\n[studio]\nenabled = true\n# Port to use for Supabase Studio.\nport = 54323\n# External URL of the API server that frontend connects to.\napi_url = \"http://localhost\"\n# OpenAI API Key to use for Supabase AI in the Supabase Studio.\nopenai_api_key = \"env(OPENAI_API_KEY)\"\n\n# Email testing server. Emails sent with the local dev setup are not actually sent - rather, they\n# are monitored, and you can view the emails that would have been sent from the web interface.\n[inbucket]\nenabled = true\n# Port to use for the email testing server web interface.\nport = 54324\n# Uncomment to expose additional ports for testing user applications that send emails.\n# smtp_port = 54325\n# pop3_port = 54326\n\n[storage]\nenabled = true\n# The maximum file size allowed (e.g. \"5MB\", \"500KB\").\nfile_size_limit = \"50MiB\"\n\n[auth]\nenabled = true\n# The base URL of your website. Used as an allow-list for redirects and for constructing URLs used\n# in emails.\nsite_url = \"http://localhost:3000\"\n# A list of *exact* URLs that auth providers are permitted to redirect to post authentication.\nadditional_redirect_urls = [\"https://localhost:3000\"]\n# How long tokens are valid for, in seconds. Defaults to 3600 (1 hour), maximum 604,800 (1 week).\njwt_expiry = 3600\n# If disabled, the refresh token will never expire.\nenable_refresh_token_rotation = true\n# Allows refresh tokens to be reused after expiry, up to the specified interval in seconds.\n# Requires enable_refresh_token_rotation = true.\nrefresh_token_reuse_interval = 10\n# Allow/disallow new user signups to your project.\nenable_signup = true\n# Allow/disallow testing manual linking of accounts\nenable_manual_linking = false\n\n[auth.email]\n# Allow/disallow new user signups via email to your project.\nenable_signup = true\n# If enabled, a user will be required to confirm any email change on both the old, and new email\n# addresses. If disabled, only the new email is required to confirm.\ndouble_confirm_changes = true\n# If enabled, users need to confirm their email address before signing in.\nenable_confirmations = false\n\n# Uncomment to customize email template\n# [auth.email.template.invite]\n# subject = \"You have been invited\"\n# content_path = \"./supabase/templates/invite.html\"\n```\n\nCreate this file in the `supabase` directory of your project, and then try running `supabase migration up` again <sup>[\\[1\\]](https://github.com/QuivrHQ/quivr/blob/main/backend/supabase/config.toml)</sup>.\n\n*To continue talking to *[*Dosu*](https://dosu.dev)*, mention @dosu.*"
      },
      {
        "user": "johnfelipe",
        "body": "file is exactly same, pls give me more option for solve this issue:\n\n```\nfelipe@template:~$ cd Quivr/\nfelipe@template:~/Quivr$ supabase migration up\ncannot read config in /home/felipe/Quivr: open supabase/config.toml: no such file or directory\nHave you set up the project with supabase init?\n```"
      },
      {
        "user": "dosubot[bot]",
        "body": "To resolve the issue where `supabase migration up` cannot read the `supabase/config.toml` file, try the following steps:\n\n1. **Verify the File Path**: Ensure that the `supabase/config.toml` file is indeed located at `/home/felipe/Quivr/supabase/config.toml`.\n2. **Check Directory Structure**: The directory structure should look like this:\n\n   ```\n   /home/felipe/Quivr/\n   â”œâ”€â”€ supabase/\n   â”‚   â””â”€â”€ config.toml\n   ```\n3. **Run** `supabase init`: The error message suggests running `supabase init` to set up the project. This command initializes the Supabase project and creates the necessary configuration files. Run the following command in the `Quivr` directory:\n\n   ```sh\n   supabase init\n   ```\n4. **Permissions**: Ensure that the file has the correct permissions and is readable by the user running the command. You can set the appropriate permissions using:\n\n   ```sh\n   chmod 644 /home/felipe/Quivr/supabase/config.toml\n   ```\n5. **Environment Variables**: Ensure that the necessary environment variables are set. Copy the `.env.example` file to `.env` and update it with the correct values:\n\n   ```sh\n   cp .env.example .env\n   ```\n6. **Supabase CLI Version**: Ensure you are using the correct version of the Supabase CLI. The `Quivr` repository specifies the version in its `.devcontainer/devcontainer.json` file as `ghcr.io/devcontainers-contrib/features/supabase-cli:1`. Check your current version with:\n\n   ```sh\n   supabase -v\n   ```\n7. **Run in Debug Mode**: If the issue persists, try running the command in debug mode to get more detailed information:\n\n   ```sh\n   supabase migration up --debug\n   ```\n\nBy following these steps, you should be able to resolve the issue and successfully run the `supabase migration up` command <sup>[\\[1\\]](https://github.com/QuivrHQ/quivr/blob/main/backend/supabase/config.toml)</sup><sup>[\\[2\\]](https://github.com/QuivrHQ/quivr/blob/main/README.md)</sup><sup>[\\[3\\]](https://github.com/QuivrHQ/quivr/blob/main/.env.example)</sup>.\n\n*To continue talking to *[*Dosu*](https://dosu.dev)*, mention @dosu.*"
      },
      {
        "user": "StanGirard",
        "body": "Hey my friend, please make sure you are in the `backend` folder when doing so ğŸ™‚ "
      }
    ]
  },
  {
    "issue_number": 2937,
    "title": "[Feature]: Prompt default",
    "author": "MakeTooRRSS",
    "state": "closed",
    "created_at": "2024-08-02T13:25:55Z",
    "updated_at": "2024-11-25T13:38:22Z",
    "labels": [],
    "body": "### The Feature\n\nHow can I put a custom prompt in the default brain? But at the same time the prompt is hidden from the user?\n\n### Motivation, pitch\n\nThis function would be interesting so that the default brain I have a prompt set for all users and also can not read it so that it is protected and can not edit or copy the prompt. Thank you very much\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/TICK-35/[feature]-prompt-default\">TICK-35 [Feature]: Prompt default</a></p>"
      }
    ]
  },
  {
    "issue_number": 2833,
    "title": "[Feature]: graph rag",
    "author": "mikestaub",
    "state": "closed",
    "created_at": "2024-07-11T07:18:10Z",
    "updated_at": "2024-11-25T13:38:22Z",
    "labels": [],
    "body": "### The Feature\n\nI would like to be able to import my graph data and use it in my brains: https://github.com/microsoft/graphragit\n\n### Motivation, pitch\n\nusing graph data has been proven to improve results\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/TICK-16/[feature]-graph-rag\">TICK-16 [Feature]: graph rag</a></p>"
      },
      {
        "user": "StanGirard",
        "body": "Hey my friend, this is planned. Give us a couple of months  ğŸ™‚"
      }
    ]
  },
  {
    "issue_number": 2885,
    "title": "[Bug]: Error response from daemon: No such image: backend-base:latest",
    "author": "Techno-Trek",
    "state": "closed",
    "created_at": "2024-07-20T03:32:26Z",
    "updated_at": "2024-11-25T13:38:19Z",
    "labels": [
      "bug"
    ],
    "body": "### What happened?\n\nAfter the last update I can no longer start building the quivr docker images.\r\nThe error is presented when executing the \"docker compose up\" command\r\n\r\nSystem: Ubuntu Server 24.04 LTS\n\n### Relevant log output\n\n```bash\n=> [frontend runner 7/8] COPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./                                              0.5s\r\n => [frontend runner 8/8] COPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static                                      0.0s\r\n => [frontend] exporting to image                                                                                                         0.3s\r\n => => exporting layers                                                                                                                   0.3s\r\n => => writing image sha256:fb07fbe90db961d2f09f593f666debf18b5eb03e7793b0728a2c6d5e1271d7bc                                              0.0s\r\n => => naming to docker.io/library/quivr-frontend-prebuilt                                                                                0.0s\r\n[+] Running 6/3\r\n âœ” Network quivr_default   Created                                                                                                        0.1s\r\n âœ” Container redis         Created                                                                                                        0.0s\r\n âœ” Container backend-core  Created                                                                                                        0.0s\r\n âœ” Container web           Created                                                                                                        0.0s\r\n âœ” Container worker        Created                                                                                                        0.0s\r\n âœ” Container beat          Created                                                                                                        0.0s\r\n â ‹ Container notifier      Creating                                                                                                       0.0s\r\n â ‹ Container flower        Creating                                                                                                       0.0s\r\nError response from daemon: No such image: backend-base:latest\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/TICK-23/[bug]-error-response-from-daemon-no-such-image-backend-baselatest\">TICK-23 [Bug]: Error response from daemon: No such image: backend-base:latest</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo resolve the issue where the \"docker compose up\" command fails with the error \"No such image: backend-base:latest\" on your Ubuntu Server 24.04 LTS system, you need to ensure that the `backend-base:latest` image is available locally or can be built from the provided Dockerfile.\n\nYou can force the build process by running the following command:\n\n```sh\ndocker compose up --build\n```\n\nThis command will build the `backend-base:latest` image from the Dockerfile specified in the `backend-core` service configuration and then start the services.\n\nHere is the relevant part of the `docker-compose.dev.yml` file that specifies the build context and Dockerfile:\n\n```yml\nservices:\n  backend-core:\n    image: backend-base:latest\n    build:\n      context: backend\n      dockerfile: Dockerfile.dev\n```\n\nBy running `docker compose up --build`, Docker will use the `backend/Dockerfile.dev` to build the `backend-base:latest` image, ensuring that the image is available for all services that depend on it <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/docker-compose.dev.yml)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/Dockerfile)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "Techno-Trek",
        "body": "@dosu\r\n\r\nrunning the command \"docker compose up --build\"\r\nanother error occurs:\r\n\r\n=> CACHED [worker 12/13] COPY supabase /code/supabase/                                                                                   0.0s\r\n => ERROR [worker 13/13] RUN poetry install --no-root --with dev,test &&     playwright install --with-deps &&     rm -rf /tmp/poetry_ca  1.1s\r\n------\r\n > [worker 13/13] RUN poetry install --no-root --with dev,test &&     playwright install --with-deps &&     rm -rf /tmp/poetry_cache:\r\n0.555 Skipping virtualenv creation, as specified in config file.\r\n0.655 Installing dependencies from lock file\r\n1.019\r\n1.019 pyproject.toml changed significantly since poetry.lock was last generated. Run `poetry lock [--no-update]` to fix the lock file.\r\n------\r\nfailed to solve: process \"/bin/sh -c poetry install --no-root --with dev,test &&     playwright install --with-deps &&     rm -rf $POETRY_CACHE_DIR\" did not complete successfully: exit code: 1\r\n\r\n\r\n"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo resolve the issue where the `poetry install --no-root --with dev,test` command fails due to a significant change in the `pyproject.toml` file since the `poetry.lock` file was last generated, you should regenerate the `poetry.lock` file. This can be done by running the following command:\n\n```sh\npoetry lock\n```\n\nAfter regenerating the `poetry.lock` file, you can proceed with the installation:\n\n```sh\npoetry install --no-root --with dev,test\n```\n\nThis will ensure that the `poetry.lock` file is in sync with the `pyproject.toml` file, resolving any discrepancies that might have caused the installation to fail <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/backend/Dockerfile)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/backend/pyproject.toml)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "Techno-Trek",
        "body": "@dosu It didn't work, the error persists.\r\n\r\n\r\n => => exporting layers                                                                                                     0.0s\r\n => => writing image sha256:adc6af4a6bf70f6b39509eee94856ad2b84bc9fd96eaec8f98d2e8df4a52d2d6                                0.0s\r\n => => naming to docker.io/library/quivr-frontend-prebuilt                                                                  0.0s\r\n[+] Running 5/6\r\n âœ” Network quivr_default   Created                                                                                          0.3s\r\n âœ” Container redis         Created                                                                                         10.3s\r\n âœ” Container backend-core  Created                                                                                         10.3s\r\n â ‹ Container web           Creating                                                                                         0.1s\r\n âœ” Container beat          Created                                                                                          0.1s\r\n âœ” Container worker        Created                                                                                          0.1s\r\n â ‹ Container flower        Creating                                                                                         0.0s\r\n â ‹ Container notifier      Creating                                                                                         0.0s\r\nError response from daemon: No such image: backend-base:latest\r\n"
      },
      {
        "user": "lanyur",
        "body": "So can the author be more rigorous? According to the README, it's impossible to achieve successful build."
      },
      {
        "user": "zdkderpphb",
        "body": "@lanyur I also ran in this problem. After Codecheck i changed the Line with the image. Maybe a quick fix for you.\r\n\r\nbackend-base:latest to stangirard/quivr-backend-prebuilt:latest"
      },
      {
        "user": "Techno-Trek",
        "body": "@zdkderpphb Thank you very much. It apparently worked, I'm going to redo everything from the beginning to see if it will work correctly."
      },
      {
        "user": "lanyur",
        "body": "> @lanyur I also ran in this problem. After Codecheck i changed the Line with the image. Maybe a quick fix for you.\r\n> \r\n> backend-base:latest to stangirard/quivr-backend-prebuilt:latest\r\n\r\nthank you,it's worked"
      },
      {
        "user": "PFLigthart",
        "body": "I also have the same error which is partially resolved by following @zdkderpphb recommendation. However, I am unable to get llama3 or 2 to work with ollama. I followed the instructions on the README exactly. I also checked issue #2777 but this does not solve my problems. Tried to get it working today and yesterday."
      },
      {
        "user": "c4801725870",
        "body": "> I also have the same error which is partially resolved by following @zdkderpphb recommendation. However, I am unable to get llama3 or 2 to work with ollama. I followed the instructions on the README exactly. I also checked issue #2777 but this does not solve my problems. Tried to get it working today and yesterday.\r\n\r\nhi, sorry about your troubles. see https://github.com/QuivrHQ/quivr/issues/2777 once again, i got it working"
      },
      {
        "user": "atulgrover",
        "body": "the docker-compose.yaml file should be updated with all \"images:\" directed to \"stangirard/quivr-backend-prebuilt:latest\"\r\n\r\n**image: stangirard/quivr-backend-prebuilt:latest**"
      }
    ]
  },
  {
    "issue_number": 2597,
    "title": "[Epic]: Where does the user change the language?",
    "author": "ghost",
    "state": "closed",
    "created_at": "2024-05-19T17:38:53Z",
    "updated_at": "2024-11-24T16:05:52Z",
    "labels": [
      "epic",
      "Stale"
    ],
    "body": "### Why do we need this functionality?\n\nHello, I am not able to change the language from English to another language. I need help.\n\n### What are we developing?\n\n#\n\n### What are the validation criteria to validate this feature?\n\n#\n\n### What's out of scope for this feature?\n\n#\n\n### Should this feature be feature flagged?\n\n- [ ] Feature Flagged\n\n### How are we going to code this feature?\n\n#\n\n### Are there any technical decisions made that should be shared?\n\n#\n\n### What is preventing us from delivering this feature?\n\n#\n\n### What can the feature potentially break?\n\n#",
    "comments": [
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo change the language in the application, follow these steps:\n\n1. Use the `useLanguageHook` to access the `change` method. This allows you to switch languages by passing the desired language object to it.\n2. Each language is represented by an object with properties like `label`, `flag`, and `shortName`. For example, to switch to Spanish, find the language object with `shortName` \"es\".\n3. The `change` method updates the application's UI to the selected language and saves this preference in `localStorage` under \"selectedLanguage\".\n4. Here's a code snippet for a language switcher component:\n\n```tsx\nimport React from 'react';\nimport { useLanguageHook, languages } from './path/to/useLanguageHook';\n\nconst LanguageSwitcher = () => {\n  const { change, currentLanguage } = useLanguageHook();\n\n  const handleLanguageChange = (newLanguageShortName) => {\n    const newLanguage = languages.find(lang => lang.shortName === newLanguageShortName);\n    if (newLanguage) {\n      change(newLanguage);\n    }\n  };\n\n  return (\n    <div>\n      <p>Current Language: {currentLanguage?.label}</p>\n      <button onClick={() => handleLanguageChange('es')}>Switch to EspaÃ±ol</button>\n      {/* Add more buttons for other languages as needed */}\n    </div>\n  );\n};\n\nexport default LanguageSwitcher;\n```\n\nThis component allows users to switch languages, automatically updating the UI and saving the preference for future sessions. Ensure you have the `languages` array and `useLanguageHook` properly imported from their respective locations in your project.\n\n\n<!-- Dosu Comment Footer -->\n<details>\n<summary>\n\n#### Details\n</summary>\n\n**Sources**\n- [frontend/public/locales/es/home.json](https://github.com/QuivrHQ/quivr/blob/main/frontend/public/locales/es/home.json)\n- [frontend/app/user/components/LanguageSelect/hooks/useLanguageHook.ts](https://github.com/QuivrHQ/quivr/blob/main/frontend/app/user/components/LanguageSelect/hooks/useLanguageHook.ts)\n\n<br/>\n\n**About Dosu**\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n</details>\n\n*To continue the conversation, mention @dosu.*\n"
      },
      {
        "user": "ghost",
        "body": "@dosu no I mean when im on the website, the website is already running. but how do I change the language from English to Spanish? "
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo change the language from English to Spanish on the website, you can interact with a language switcher component if it's available. This component should be visible on the website, often found in the header, footer, or settings menu. Look for an icon representing a globe, flags, or simply a dropdown menu labeled with the current language or \"Language.\"\n\nHere's a quick guide on how to use such a component:\n\n1. **Locate the Language Switcher**: It could be labeled with \"English\", \"ES\", a flag icon, or even just a globe icon.\n2. **Select Spanish**: Click on the component to open the list of available languages and select \"EspaÃ±ol\" or the flag representing Spain or Spanish-speaking countries.\n3. **Confirm the Change**: The website should automatically update to display content in Spanish.\n\nIf the website uses a `useLanguageHook` or similar functionality, the language preference might be saved in your browser's `localStorage`, ensuring that your preference is remembered for future visits.\n\nIn case you can't find a language switcher on the website, it's possible that the feature might not be implemented or is not easily visible. In such scenarios, reaching out to the website's support or feedback channel might be helpful to request this feature or get assistance on how to access it.\n\n\n<!-- Dosu Comment Footer -->\n<details>\n<summary>\n\n#### Details\n</summary>\n\n**Sources**\n- [frontend/app/user/components/LanguageSelect/hooks/useLanguageHook.ts](https://github.com/QuivrHQ/quivr/blob/main/frontend/app/user/components/LanguageSelect/hooks/useLanguageHook.ts)\n- [frontend/app/user/components/LanguageSelect/LanguageSelect.tsx](https://github.com/QuivrHQ/quivr/blob/main/frontend/app/user/components/LanguageSelect/LanguageSelect.tsx)\n\n<br/>\n\n**About Dosu**\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n</details>\n\n*To continue the conversation, mention @dosu.*\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      },
      {
        "user": "MakeTooRRSS",
        "body": "Hello could you finally put the language selector? I have the same problem, there are different languages but you can not put another language on the site. Can someone help me? Thanks"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 2464,
    "title": "[Bug]: Unable to log in",
    "author": "tc0715888",
    "state": "closed",
    "created_at": "2024-04-22T00:14:13Z",
    "updated_at": "2024-11-24T12:09:33Z",
    "labels": [
      "bug",
      "Stale"
    ],
    "body": "### What happened?\n\nStep 5: Login to the app\r\n\r\nYou can now sign in to the app with admin@quivr.app & admin. You can access the app at http://localhost:3000/login.\r\n\r\n\r\nI logged in using the username and password provided on your website, but it showed \"Failed to retrieve\".\n\n### Relevant log output\n\n_No response_\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "dosubot[bot]",
        "body": "<!-- Answer -->\nTo address the login issue you're experiencing, let's start by ensuring the application is correctly running and that all configurations are properly set. Here are a few steps to troubleshoot the problem:\n\n1. **Verify Application Status**: Make sure the application is correctly built and served. If you're deploying, ensure the deployment process completed successfully.\n\n2. **Check Port Configurations**: Ensure that the Dockerfile's exposed port matches the container port in your deployment configuration. This is crucial for the application to communicate correctly.\n\n3. **Review Recent Changes**: Look into the recent changes in the codebase, especially those related to login functionality and data fetching restrictions for non-logged-in users. A change that prevents data fetching when a user is not logged in could impact the login process.\n\n4. **Error Handling in Google Login**: If you're using Google login, check the error handling part for any issues. The application should display specific error messages if the login fails due to missing credentials.\n\n5. **Examine Error Logs**: More detailed information from error logs or console outputs during the login attempt can be very helpful. Look for any error messages or codes that could indicate the nature of the problem, such as 401 (Unauthorized), which suggests issues with authentication credentials.\n\nBy following these steps, you should be able to narrow down the cause of the login issue. If the problem persists, providing more detailed information about any error messages or logs you encounter will be essential for further assistance.\n\n\n<!-- Dosu Comment Footer -->\n<details>\n<summary>\n\n#### Details\n</summary>\n\n**Sources**\n- [[Bug]: There is no response after clicking the login button](https://github.com/QuivrHQ/quivr/issues/1714)\n- [frontend/app/(auth)/login/page.tsx](https://github.com/QuivrHQ/quivr/blob/main/frontend/app/(auth)/login/page.tsx)\n- [frontend/public/locales/fr/login.json](https://github.com/QuivrHQ/quivr/blob/main/frontend/public/locales/fr/login.json)\n- [frontend/public/locales/en/login.json](https://github.com/QuivrHQ/quivr/blob/main/frontend/public/locales/en/login.json)\n- [docs/developers/useQuivr/error_handling.mdx](https://github.com/QuivrHQ/quivr/blob/main/docs/developers/useQuivr/error_handling.mdx)\n\n<br/>\n\n**About Dosu**\nThis response is meant to be useful and save you time. It is **not meant to be a precise solution**, but rather a starting point for your own research.\n\nHelp me be more useful! Please leave a ğŸ‘ if this is helpful and ğŸ‘ if it is irrelevant.\n</details>\n\n*To continue the conversation, mention @dosu.*\n"
      },
      {
        "user": "JerryKwan",
        "body": "Encountered the same problem.\r\nWhen accessing http://localhost:3000/login, I can see the form, but there is no response when clicking \"Login\" Button \r\nIt seems like some scripts and stylesheets are not loaded properly\r\n"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      },
      {
        "user": "mz0in",
        "body": "Same Issue ,  followed the doc Unable to login "
      },
      {
        "user": "mz0in",
        "body": "> Same Issue , followed the doc Unable to login\r\n\r\nthe problem is when deploying on remote VPS  , on local machine it's working "
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3033,
    "title": "[Bug]: ERROR [frontend deps 1/5] RUN apk add --no-cache libc6-compat python3 make g++",
    "author": "jasonyang-ytl",
    "state": "closed",
    "created_at": "2024-08-21T06:46:44Z",
    "updated_at": "2024-11-24T12:09:32Z",
    "labels": [
      "bug",
      "Stale",
      "area: frontend"
    ],
    "body": "### What happened?\n\nError while spawning docker image\r\n\r\n\n\n### Relevant log output\n\n```bash\nWARN[0000] /home/jasonyang/quivr/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion\r\n[+] Building 11.9s (9/21)                                                                                                                                                                                                    docker:default\r\n => [frontend internal] load build definition from Dockerfile                                                                                                                                                                          0.0s\r\n => => transferring dockerfile: 2.90kB                                                                                                                                                                                                 0.0s\r\n => [frontend internal] load metadata for docker.io/library/node:18.19.0-alpine                                                                                                                                                        1.6s\r\n => [frontend internal] load .dockerignore                                                                                                                                                                                             0.0s\r\n => => transferring context: 340B                                                                                                                                                                                                      0.0s\r\n => CACHED [frontend base 1/1] FROM docker.io/library/node:18.19.0-alpine@sha256:0085670310d2879621f96a4216c893f92e2ded827e9e6ef8437672e1bd72f437                                                                                      0.0s\r\n => [frontend internal] load build context                                                                                                                                                                                             0.1s\r\n => => transferring context: 71.83kB                                                                                                                                                                                                   0.0s\r\n => CACHED [frontend builder 1/4] WORKDIR /app                                                                                                                                                                                         0.0s\r\n => CACHED [frontend runner 2/8] RUN addgroup --system --gid 1001 nodejs                                                                                                                                                               0.0s\r\n => CACHED [frontend runner 3/8] RUN adduser --system --uid 1001 nextjs                                                                                                                                                                0.0s\r\n => ERROR [frontend deps 1/5] RUN apk add --no-cache libc6-compat python3 make g++                                                                                                                                                    10.3s\r\n------\r\n > [frontend deps 1/5] RUN apk add --no-cache libc6-compat python3 make g++:\r\n0.248 fetch https://dl-cdn.alpinelinux.org/alpine/v3.19/main/x86_64/APKINDEX.tar.gz\r\n5.254 WARNING: fetching https://dl-cdn.alpinelinux.org/alpine/v3.19/main: temporary error (try again later)\r\n5.254 fetch https://dl-cdn.alpinelinux.org/alpine/v3.19/community/x86_64/APKINDEX.tar.gz\r\n10.26 WARNING: fetching https://dl-cdn.alpinelinux.org/alpine/v3.19/community: temporary error (try again later)\r\n10.26 ERROR: unable to select packages:\r\n10.26   g++ (no such package):\r\n10.26     required by: world[g++]\r\n10.26   libc6-compat (no such package):\r\n10.26     required by: world[libc6-compat]\r\n10.26   make (no such package):\r\n10.26     required by: world[make]\r\n10.26   python3 (no such package):\r\n10.26     required by: world[python3]\r\n------\r\nfailed to solve: process \"/bin/sh -c apk add --no-cache libc6-compat python3 make g++\" did not complete successfully: exit code: 4\n```\n\n\n### Twitter / LinkedIn details\n\n_No response_",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-156/[bug]-error-[frontend-deps-15]-run-apk-add-no-cache-libc6-compat\">CORE-156 [Bug]: ERROR [frontend deps 1/5] RUN apk add --no-cache libc6-compat python3 make g++</a></p>"
      },
      {
        "user": "dosubot[bot]",
        "body": "<!-- Greeting -->\nHello, @jasonyang-ytl! I'm here to help you with any bugs, questions, or contributions you may have while waiting for a human maintainer. Let's tackle this together!\n\n<!-- Answer -->\nTo resolve the error when running `apk add --no-cache libc6-compat python3 make g++` in the Docker build process, you can ensure that the `libc6-compat` package is available in the Alpine repositories. The provided Dockerfile already includes the necessary command to add these packages:\n\n```dockerfile\nRUN apk add --no-cache libc6-compat python3 make g++\n```\n\nIf you are encountering errors, it might be due to a temporary issue with the Alpine package repositories or a network issue. Here are a few steps you can take to troubleshoot and resolve the issue:\n\n1. **Update the package index**: Ensure that the package index is up-to-date before installing the packages.\n2. **Check network connectivity**: Ensure that your Docker build process has network access to the Alpine package repositories.\n3. **Specify a different Alpine version**: Sometimes, using a different version of Alpine can resolve package availability issues.\n\nHere is an updated snippet of the Dockerfile with the package index update and additional steps to ensure Python is properly set up:\n\n```dockerfile\nFROM node:18.13.0-alpine\n\n# Install Python and essential build tools\nRUN apk add --update --no-cache libc6-compat python3 make g++ && ln -sf python3 /usr/bin/python\nRUN python3 -m ensurepip\nRUN pip3 install --no-cache --upgrade pip setuptools\n\n# Create the directory on the node image where our Next.js app will live\nRUN mkdir -p /app\n\n# Set /app as the working directory\nWORKDIR /app\n\n# Copy package.json and yarn.lock to the /app working directory\nCOPY package*.json yarn.lock ./\n\n# Install dependencies in /app\nRUN yarn install --network-timeout 1000000\n\n# Copy the rest of our Next.js folder into /app\nCOPY . .\n\n# Ensure port 3000 is accessible to our system\nEXPOSE 3000\n\n# Run yarn dev, as we would via the command line \nCMD [\"yarn\", \"dev\"]\n```\n\nIf the issue persists, you may need to check the specific error message for more details or try using a different base image that includes the required dependencies <sup>[[1]](https://github.com/QuivrHQ/quivr/blob/main/frontend/Dockerfile)</sup><sup>[[2]](https://github.com/QuivrHQ/quivr/blob/main/frontend/Dockerfile.dev)</sup>.\n\n\n<!-- Dosu Comment Footer -->\n*To continue talking to [Dosu](https://dosu.dev), mention @dosu.*\n"
      },
      {
        "user": "StanGirard",
        "body": "Hello, this seems like a network issue. Could you please retry ?"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3034,
    "title": "[WIP] Airtable Integration",
    "author": "StanGirard",
    "state": "closed",
    "created_at": "2024-08-21T07:44:34Z",
    "updated_at": "2024-11-24T12:09:31Z",
    "labels": [
      "Stale"
    ],
    "body": "* Strat37\n* Bullpen cap",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-51/[wip]-airtable-integration\">ENT-51 [WIP] Airtable Integration</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3025,
    "title": "Emoji Selector Component",
    "author": "linear[bot]",
    "state": "closed",
    "created_at": "2024-08-19T15:42:38Z",
    "updated_at": "2024-11-22T16:06:41Z",
    "labels": [
      "Stale",
      "Feature"
    ],
    "body": null,
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/ENT-46/emoji-selector-component\">ENT-46 Emoji Selector Component</a></p>"
      },
      {
        "user": "github-actions[bot]",
        "body": "Thanks for your contributions, we'll be closing this issue as it has gone stale. Feel free to reopen if you'd like to continue the discussion."
      }
    ]
  },
  {
    "issue_number": 3486,
    "title": "Sources not in demo",
    "author": "StanGirard",
    "state": "closed",
    "created_at": "2024-11-18T13:52:22Z",
    "updated_at": "2024-11-22T12:49:09Z",
    "labels": [
      "bug"
    ],
    "body": "Run the chatbot examples, there is no sources in the metadata. <br><br>I'm using the latest version of quivr-core. Running the example as is. No source are provided in the answer. <br><br>I didn't change a single line of code. <br><br><br><br>[https://github.com/QuivrHQ/quivr/tree/main/examples/chatbot](https://github.com/QuivrHQ/quivr/tree/main/examples/chatbot)",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-299/sources-not-in-demo\">CORE-299 Sources not in demo</a></p>"
      }
    ]
  },
  {
    "issue_number": 3422,
    "title": "Missing Tavily in doc web search",
    "author": "StanGirard",
    "state": "closed",
    "created_at": "2024-10-23T14:46:54Z",
    "updated_at": "2024-11-22T12:49:09Z",
    "labels": [
      "bug",
      "area: docs"
    ],
    "body": "[https://core.quivr.com/en/latest/workflows/examples/rag_with_web_search/](https://core.quivr.com/en/latest/workflows/examples/rag_with_web_search/)<br><br><br>Is missing the tavily API key ",
    "comments": [
      {
        "user": "linear[bot]",
        "body": "<p><a href=\"https://linear.app/getquivr/issue/CORE-255/missing-tavily-in-doc-web-search\">CORE-255 Missing Tavily in doc web search</a></p>"
      }
    ]
  },
  {
    "issue_number": 1176,
    "title": "[Issue] - Brand new install error during docker compose up",
    "author": "sanjayws",
    "state": "closed",
    "created_at": "2023-09-15T06:24:12Z",
    "updated_at": "2024-11-21T15:36:13Z",
    "labels": [],
    "body": "root@jay-quivr-dev-01:~/Quivr# docker compose up\r\n[+] Running 5/0\r\n âœ” Container redis         Created                                                                                                                                                                                                        0.0s \r\n âœ” Container web           Created                                                                                                                                                                                                        0.0s \r\n âœ” Container worker        Created                                                                                                                                                                                                        0.0s \r\n âœ” Container backend-core  Created                                                                                                                                                                                                        0.0s \r\n âœ” Container flower        Created                                                                                                                                                                                                        0.0s \r\nAttaching to backend-core, flower, redis, web, worker\r\nredis         | 1:C 15 Sep 2023 06:24:59.785 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\r\nredis         | 1:C 15 Sep 2023 06:24:59.786 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\r\nredis         | 1:C 15 Sep 2023 06:24:59.786 * Redis version=7.2.1, bits=64, commit=00000000, modified=0, pid=1, just started\r\nredis         | 1:C 15 Sep 2023 06:24:59.786 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\r\nredis         | 1:M 15 Sep 2023 06:24:59.786 * monotonic clock: POSIX clock_gettime\r\nredis         | 1:M 15 Sep 2023 06:24:59.787 * Running mode=standalone, port=6379.\r\nredis         | 1:M 15 Sep 2023 06:24:59.790 # Warning: Could not create server TCP listening socket ::*:6379: unable to bind socket, errno: 97\r\nredis         | 1:M 15 Sep 2023 06:24:59.791 * Server initialized\r\nredis         | 1:M 15 Sep 2023 06:24:59.791 * Loading RDB produced by version 7.2.1\r\nredis         | 1:M 15 Sep 2023 06:24:59.791 * RDB age 85 seconds\r\nredis         | 1:M 15 Sep 2023 06:24:59.791 * RDB memory usage when created 0.83 Mb\r\nredis         | 1:M 15 Sep 2023 06:24:59.791 * Done loading RDB, keys loaded: 0, keys expired: 0.\r\nredis         | 1:M 15 Sep 2023 06:24:59.791 * DB loaded from disk: 0.000 seconds\r\nredis         | 1:M 15 Sep 2023 06:24:59.791 * Ready to accept connections tcp\r\nweb           | yarn run v1.22.19\r\nweb           | $ next start\r\nweb           | - ready started server on 0.0.0.0:3000, url: http://localhost:3000\r\nbackend-core  | INFO:     Will watch for changes in these directories: ['/code']\r\nbackend-core  | INFO:     Uvicorn running on http://0.0.0.0:5050 (Press CTRL+C to quit)\r\nbackend-core  | INFO:     Started reloader process [1] using StatReload\r\nweb           | - info Loaded env from /app/.env\r\nweb           | - info Loaded env from /app/.env\r\nworker        | Usage: celery [OPTIONS] COMMAND [ARGS]...\r\nworker        | Try 'celery --help' for help.\r\nworker        | \r\nworker        | Error: Invalid value for '-A' / '--app': \r\nworker        | Unable to load celery application.\r\nworker        | While trying to load the module celery_worker the following error occurred:\r\nworker        | Traceback (most recent call last):\r\nworker        |   File \"/usr/local/lib/python3.11/site-packages/kombu/utils/imports.py\", line 59, in symbol_by_name\r\nworker        |     module = imp(module_name, package=package, **kwargs)\r\nworker        |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker        |   File \"/usr/local/lib/python3.11/site-packages/celery/utils/imports.py\", line 105, in import_from_cwd\r\nworker        |     return imp(module, package=package)\r\nworker        |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker        |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nworker        |     return _bootstrap._gcd_import(name[level:], package, level)\r\nworker        |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker        |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nworker        |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nworker        |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nworker        |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nworker        |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nworker        |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nworker        |   File \"/code/celery_worker.py\", line 42, in <module>\r\nworker        |     raise ValueError(f\"Unsupported broker URL: {CELERY_BROKER_URL}\")\r\nworker        | ValueError: Unsupported broker URL: \r\nworker        | \r\nworker        | During handling of the above exception, another exception occurred:\r\nworker        | \r\nworker        | Traceback (most recent call last):\r\nworker        |   File \"/usr/local/lib/python3.11/site-packages/celery/bin/celery.py\", line 57, in convert\r\nworker        |     return find_app(value)\r\nworker        |            ^^^^^^^^^^^^^^^\r\nworker        |   File \"/usr/local/lib/python3.11/site-packages/celery/app/utils.py\", line 384, in find_app\r\nworker        |     sym = symbol_by_name(app, imp=imp)\r\nworker        |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker        |   File \"/usr/local/lib/python3.11/site-packages/kombu/utils/imports.py\", line 61, in symbol_by_name\r\nworker        |     reraise(ValueError,\r\nworker        |   File \"/usr/local/lib/python3.11/site-packages/kombu/exceptions.py\", line 34, in reraise\r\nworker        |     raise value.with_traceback(tb)\r\nworker        |   File \"/usr/local/lib/python3.11/site-packages/kombu/utils/imports.py\", line 59, in symbol_by_name\r\nworker        |     module = imp(module_name, package=package, **kwargs)\r\nworker        |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker        |   File \"/usr/local/lib/python3.11/site-packages/celery/utils/imports.py\", line 105, in import_from_cwd\r\nworker        |     return imp(module, package=package)\r\nworker        |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker        |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nworker        |     return _bootstrap._gcd_import(name[level:], package, level)\r\nworker        |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nworker        |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nworker        |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nworker        |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nworker        |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nworker        |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nworker        |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nworker        |   File \"/code/celery_worker.py\", line 42, in <module>\r\nworker        |     raise ValueError(f\"Unsupported broker URL: {CELERY_BROKER_URL}\")\r\nworker        | ValueError: Couldn't import 'celery_worker': Unsupported broker URL: \r\nworker        | \r\nbackend-core  | Process SpawnProcess-1:\r\nbackend-core  | Traceback (most recent call last):\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\nbackend-core  |     self.run()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\nbackend-core  |     self._target(*self._args, **self._kwargs)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/_subprocess.py\", line 76, in subprocess_started\r\nbackend-core  |     target(sockets=sockets)\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 61, in run\r\nbackend-core  |     return asyncio.run(self.serve(sockets=sockets))\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\r\nbackend-core  |     return runner.run(main)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\r\nbackend-core  |     return self._loop.run_until_complete(task)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/asyncio/base_events.py\", line 653, in run_until_complete\r\nbackend-core  |     return future.result()\r\nbackend-core  |            ^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 68, in serve\r\nbackend-core  |     config.load()\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 473, in load\r\nbackend-core  |     self.loaded_app = import_from_string(self.app)\r\nbackend-core  |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 21, in import_from_string\r\nbackend-core  |     module = importlib.import_module(module_str)\r\nbackend-core  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nbackend-core  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nbackend-core  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nbackend-core  |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nbackend-core  |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nbackend-core  |   File \"/code/main.py\", line 19, in <module>\r\nbackend-core  |     from routes.crawl_routes import crawl_router\r\nbackend-core  |   File \"/code/routes/crawl_routes.py\", line 5, in <module>\r\nbackend-core  |     from celery_worker import process_crawl_and_notify\r\nbackend-core  |   File \"/code/celery_worker.py\", line 42, in <module>\r\nbackend-core  |     raise ValueError(f\"Unsupported broker URL: {CELERY_BROKER_URL}\")\r\nbackend-core  | ValueError: Unsupported broker URL: \r\nflower        | Usage: celery [OPTIONS] COMMAND [ARGS]...\r\nflower        | Try 'celery --help' for help.\r\nflower        | \r\nflower        | Error: Invalid value for '-A' / '--app': \r\nflower        | Unable to load celery application.\r\nflower        | While trying to load the module celery_worker the following error occurred:\r\nflower        | Traceback (most recent call last):\r\nflower        |   File \"/usr/local/lib/python3.11/site-packages/kombu/utils/imports.py\", line 59, in symbol_by_name\r\nflower        |     module = imp(module_name, package=package, **kwargs)\r\nflower        |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nflower        |   File \"/usr/local/lib/python3.11/site-packages/celery/utils/imports.py\", line 105, in import_from_cwd\r\nflower        |     return imp(module, package=package)\r\nflower        |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nflower        |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nflower        |     return _bootstrap._gcd_import(name[level:], package, level)\r\nflower        |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nflower        |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nflower        |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nflower        |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nflower        |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nflower        |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nflower        |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nflower        |   File \"/code/celery_worker.py\", line 42, in <module>\r\nflower        |     raise ValueError(f\"Unsupported broker URL: {CELERY_BROKER_URL}\")\r\nflower        | ValueError: Unsupported broker URL: \r\nflower        | \r\nflower        | During handling of the above exception, another exception occurred:\r\nflower        | \r\nflower        | Traceback (most recent call last):\r\nflower        |   File \"/usr/local/lib/python3.11/site-packages/celery/bin/celery.py\", line 57, in convert\r\nflower        |     return find_app(value)\r\nflower        |            ^^^^^^^^^^^^^^^\r\nflower        |   File \"/usr/local/lib/python3.11/site-packages/celery/app/utils.py\", line 384, in find_app\r\nflower        |     sym = symbol_by_name(app, imp=imp)\r\nflower        |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nflower        |   File \"/usr/local/lib/python3.11/site-packages/kombu/utils/imports.py\", line 61, in symbol_by_name\r\nflower        |     reraise(ValueError,\r\nflower        |   File \"/usr/local/lib/python3.11/site-packages/kombu/exceptions.py\", line 34, in reraise\r\nflower        |     raise value.with_traceback(tb)\r\nflower        |   File \"/usr/local/lib/python3.11/site-packages/kombu/utils/imports.py\", line 59, in symbol_by_name\r\nflower        |     module = imp(module_name, package=package, **kwargs)\r\nflower        |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nflower        |   File \"/usr/local/lib/python3.11/site-packages/celery/utils/imports.py\", line 105, in import_from_cwd\r\nflower        |     return imp(module, package=package)\r\nflower        |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nflower        |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\r\nflower        |     return _bootstrap._gcd_import(name[level:], package, level)\r\nflower        |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nflower        |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\nflower        |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\nflower        |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\r\nflower        |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\r\nflower        |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\r\nflower        |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\nflower        |   File \"/code/celery_worker.py\", line 42, in <module>\r\nflower        |     raise ValueError(f\"Unsupported broker URL: {CELERY_BROKER_URL}\")\r\nflower        | ValueError: Couldn't import 'celery_worker': Unsupported broker URL: \r\nflower        | ",
    "comments": [
      {
        "user": "StanGirard",
        "body": "Get the new env variable from the example for the backend ;)\r\n\r\n\r\nCELERY_BROKER_URL isn't set in your context."
      },
      {
        "user": "sanjayws",
        "body": "> Get the new env variable from the example for the backend ;)\r\n> \r\n> CELERY_BROKER_URL isn't set in your context.\r\n\r\nThat worked, thanks mate."
      },
      {
        "user": "dwehrmann",
        "body": "@StanGirard , @sanjayws Can you elaborate, please? I have the exact same error but CELERY_BROKER_URL is set in my .env file. Was there a change?"
      }
    ]
  }
]