[
  {
    "issue_number": 1950,
    "title": "Ubuntu 22.04.5 LTS, et al. h2ogpt install script fail",
    "author": "ob83",
    "state": "closed",
    "created_at": "2025-05-11T23:26:01Z",
    "updated_at": "2025-05-14T11:25:35Z",
    "labels": [],
    "body": "This all worked back in Nov.2024 on Linux Mint, but now fails at the exact same point on Mint and Ubuntu regardless of OS versions I've tried, on fresh OS installs.\n\nFailing installer: Linux Quick Install (..h2ogpt/blob/main/docs/README_LINUX.md)\n\nFailing script: curl -fsSL https://h2o-release.s3.amazonaws.com/h2ogpt/linux_install_full.sh | bash\n\nError message (always the same & at the same exact location): \n\t\t\t...\n  Downloading posthog-3.22.0-py2.py3-none-any.whl.metadata (3.0 kB)\n  Downloading posthog-3.21.0-py2.py3-none-any.whl.metadata (2.9 kB)\n  Downloading posthog-3.20.0-py2.py3-none-any.whl.metadata (2.9 kB)\n  Downloading posthog-3.19.1-py2.py3-none-any.whl.metadata (2.9 kB)\nerror: resolution-too-deep\n\n× Dependency resolution exceeded maximum depth\n╰─> Pip cannot resolve the current dependencies as the dependency graph is too complex for pip to solve efficiently.\n\t\t\t...\n\t\t\t\nLet me know anything you'd like me to try. Many thanks!",
    "comments": [
      {
        "user": "ob83",
        "body": "I fixed it locally & it works good. There were several bugs I will investigate how they got in. Thanks for making this code available. Salute, H2o.ai!"
      }
    ]
  },
  {
    "issue_number": 1949,
    "title": "Unable to Upload Documents",
    "author": "pseudomushi",
    "state": "open",
    "created_at": "2025-05-11T05:20:47Z",
    "updated_at": "2025-05-11T11:46:38Z",
    "labels": [],
    "body": "Really appreciate for making H2O.ai and making it available for the public. Been using H2O.ai for the last 2 weeks however, recently I have encountered an issue towards uploading documents. To be precise on my issue, when I click on 'Add Documents' a new dialogue box opens asking to browse the files to upload and thereafter, when I upload the required documents, the 'Add' button is not working. \n\nPlease see the below picture for reference.\n\n![Image](https://github.com/user-attachments/assets/9cd3cbde-39bd-433a-845b-729d1cb635d1)\n\nSimilarly, the 'Add' button within the 'Import from file system' is also not working. Please see the below screenshot for reference.\n\n![Image](https://github.com/user-attachments/assets/2c300d6c-bd67-4747-8e09-96d29e80c5f5)\n\nPlease help me out understand the issue, Thank you.",
    "comments": []
  },
  {
    "issue_number": 1944,
    "title": "Use gocloud.dev as a drop in replacement for *sql.DB",
    "author": "avnerv",
    "state": "open",
    "created_at": "2025-04-29T06:54:14Z",
    "updated_at": "2025-04-29T06:54:14Z",
    "labels": [],
    "body": "details in: https://github.com/h2oai/cloud-platform/issues/107",
    "comments": []
  },
  {
    "issue_number": 1943,
    "title": "User-Created Shared Collections Not Visible to Other Users",
    "author": "Abhishek577",
    "state": "open",
    "created_at": "2025-04-26T10:41:48Z",
    "updated_at": "2025-04-26T10:41:48Z",
    "labels": [],
    "body": "I have developed an application using H2O GPT as the back-end.\nWhen I create a shared collection through the CLI, it works correctly — all users are able to view and access the collection after logging in.\n\nHowever, when a user creates a collection via the UI (with type set to shared), the collection becomes visible only to that specific user. Other users are unable to see or access it, even though it is marked as shared.\n\nExpected Behavior:\nShared collections created by any user should be accessible and visible to all users, just like collections created through the CLI.\n\nCurrent Behavior:\nCollections created via the UI in shared mode are restricted only to the creating user.\n\nAdditional Info:\n\nCLI-created shared collections behave correctly.\n\nUI-created shared collections are incorrectly restricted.\n\nCould you please help clarify if this is a bug or if some additional configuration is needed?\n\n",
    "comments": []
  },
  {
    "issue_number": 1932,
    "title": "Are the Gradio  and  OpenWebUI Demos cancelled?",
    "author": "DorinDXN",
    "state": "closed",
    "created_at": "2025-02-21T17:15:59Z",
    "updated_at": "2025-04-16T17:17:21Z",
    "labels": [],
    "body": "Thanks for your work.\n\nAre the Gradio Demo (https://gpt.h2o.ai/) and  OpenWebUI Demo (https://gpt-docs.h2o.ai/)  cancelled from now on?\n\ncheers,\nDorin",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Yes, you can still use fremium h2ogpte that they point to."
      },
      {
        "user": "DorinDXN",
        "body": "Thanks,\nI guess that is\nhttps://h2ogpte.genai.h2o.ai/\nI see the costs\nhttps://h2ogpte.genai.h2o.ai/models\nbut is not clear how and when they are applied\nin the video https://www.youtube.com/watch?v=R5geRo1ijm8\nthere is an 'Usage' tab that is not present in the new variant\nhttps://h2ogpte.genai.h2o.ai/usage\ngives 404 error.\n\nPlease give some more details.\n\ncheers,\nDorin "
      },
      {
        "user": "pseudotensor",
        "body": "The usage is in the models page too:\n\n![Image](https://github.com/user-attachments/assets/854bdbe1-0c26-4fdd-9c40-53896be2d1a3)"
      },
      {
        "user": "DorinDXN",
        "body": "You're right, I was looking for it on the left side as in the older layout. Thanks!\n\ncheers,\nDorin"
      },
      {
        "user": "DorinDXN",
        "body": "Are there some limits beside the daily $20? it worked very well for a while, now no model are responding to chat (infinite wait, as it seems) the self test runs ok and the calls for that tests appears at usage and stats though.\nI used\nhttps://h2ogpte.genai.h2o.ai/"
      },
      {
        "user": "DorinDXN",
        "body": "It seems to be back now in good working status\n\ncheers,\nDorin "
      },
      {
        "user": "DorinDXN",
        "body": "It looks like it happens again, 7 days later, perhaps, there is some weekly limit. "
      }
    ]
  },
  {
    "issue_number": 950,
    "title": "Add option to disable ssl verify check ",
    "author": "kalaiselvan263",
    "state": "closed",
    "created_at": "2023-10-12T16:52:27Z",
    "updated_at": "2025-04-08T09:41:56Z",
    "labels": [
      "reporter/support",
      "type/feature",
      "cust-citi"
    ],
    "body": "https://support.h2o.ai/a/tickets/106627\r\n\r\nNeed option to disable ssl verify in code \r\n\r\n```\r\nfrom gradio_client import Client\r\nimport ast\r\nHOST_URL =\"[https://eas-aimlservices-gtdcu.nam.nsroot.net:21425/\"](https://eas-aimlservices-gtdcu.nam.nsroot.net:21425/%22)\r\nclient = Client(HOST_URL)\r\n # string of dict for input\r\nkwargs = dict(instruction_nochat='Who are you?')\r\nres = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\r\n # string of dict for output\r\nresponse = ast.literal_eval(res)['response']\r\nprint(response)\r\n```",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "This was done earlier.\r\n\r\nThese options from gradio are exposed:\r\n\r\n```\r\n        ssl_verify: bool = True,\r\n        ssl_keyfile: str | None = None,\r\n        ssl_certfile: str | None = None,\r\n        ssl_keyfile_password: str | None = None,\r\n\r\n```"
      },
      {
        "user": "kalaiselvan263",
        "body": "Customer have added the ssl_verify=False , However still they are getting the same error.\r\n\r\n```\r\n(gradioclient) -bash-4.2$ python generate.py --ssl_verify=False\r\nLoaded as API: https://eas-aimlservices-gtdcu.nam.nsroot.net:21425/ ✔\r\nTraceback (most recent call last):\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\r\n    httplib_response = self._make_request(\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 386, in _make_request\r\n    self._validate_conn(conn)\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1042, in _validate_conn\r\n    conn.connect()\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/connection.py\", line 414, in connect\r\n    self.sock = ssl_wrap_socket(\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\r\n    ssl_sock = _ssl_wrap_socket_impl(\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\r\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/ssl.py\", line 512, in wrap_socket\r\n    return self.sslsocket_class._create(\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/ssl.py\", line 1070, in _create\r\n    self.do_handshake()\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/ssl.py\", line 1341, in do_handshake\r\n    self._sslobj.do_handshake()\r\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)\r\n \r\nDuring handling of the above exception, another exception occurred:\r\n \r\nTraceback (most recent call last):\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/requests/adapters.py\", line 489, in send\r\n    resp = conn.urlopen(\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\r\n    retries = retries.increment(\r\n  File \"/home/sr70157/.conda/envs/gradioclient/lib/python3.10/site-packages/urllib3/util/retry.py\", line 592, in increment\r\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='eas-aimlservices-gtdcu.nam.nsroot.net', port=21425): Max retries exceeded with url: /config (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))\r\n\r\n(base) [root@aingtu4689 h2ogpt]# curl -LIk https://eas-aimlservices-gtdcu.nam.nsroot.net:21234/\r\nHTTP/1.1 200 OK\r\ndate: Tue, 17 Oct 2023 07:48:53 GMT\r\nserver: uvicorn\r\ncontent-length: 320578\r\ncontent-type: text/html; charset=utf-8\r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "@kalaiselvan263 Did they do something like this?  I don't know SSL much, but maybe even if don't verify, still need to set the other things:\r\n\r\nhttps://stackoverflow.com/a/76405442\r\n\r\nBut unclear if really solved or solvable:\r\nhttps://github.com/gradio-app/gradio/issues/3846#issuecomment-1653460562\r\nhttps://github.com/gradio-app/gradio/issues/5497"
      },
      {
        "user": "pseudotensor",
        "body": "solved and FAQ item added"
      },
      {
        "user": "shodanx2",
        "body": "Thank you that fixed my problem with SSL self-signed for \r\nhttps://github.com/Vincentqyw/youtube-dl-webui.git\r\nunrelated to your project but this tread is top google link for \r\ngradio httpcore.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate"
      },
      {
        "user": "kschmid23",
        "body": "what is the solution?"
      }
    ]
  },
  {
    "issue_number": 1929,
    "title": "Container Registry?",
    "author": "devinrouthuzh",
    "state": "open",
    "created_at": "2025-02-05T12:28:12Z",
    "updated_at": "2025-04-01T10:54:23Z",
    "labels": [],
    "body": "Greetings,\n\nI noticed that the [container registry](https://console.cloud.google.com/gcr/images/vorvan/global/h2oai/h2ogpt-runtime) linked in the [Prebuild Docker section](https://github.com/h2oai/h2ogpt/blob/main/docs/README_DOCKER.md#prebuild-docker-for-windowslinux-x86) of the docs isn't available. Is there an issue here, or am I doing/referencing something incorrectly?\n\nThanks!\n\nBest wishes,\nDevin Routh",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "https://github.com/h2oai/h2ogpt/issues/1924#issuecomment-2606492121"
      },
      {
        "user": "devinrouthuzh",
        "body": "I'm still seeing an error when loading the linked container registry. Is this just me?\n\n<img width=\"788\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b82f753c-c5f8-4b75-90f3-348c06f2b721\" />"
      },
      {
        "user": "devinrouthuzh",
        "body": "I wanted to followup as I'm unable to view the container registry. Am I missing something, or is the registry not available for others as well?"
      },
      {
        "user": "pseudotensor",
        "body": "Yes see that too.  @lakinduakash where is it located now?"
      },
      {
        "user": "devinrouthuzh",
        "body": "Just wanted to follow-up once more as the original container registry link remains down. Is there any timeline on when a new link will become available? Thanks again!"
      },
      {
        "user": "Jack-Byte",
        "body": "Hi @devinrouthuzh there is a [pinned issue](https://github.com/h2oai/h2ogpt/issues/1940 ) showing the new registry https://hub.docker.com/r/h2oairelease/h2oai-h2ogpt-runtime/tags \n\nHowever, the last push is 5 months old. Which doesn't really match with what was on GCR before. See for instances pseudotensors screenshot [here](https://github.com/h2oai/h2ogpt/issues/1924#issuecomment-2606492121)"
      }
    ]
  },
  {
    "issue_number": 1925,
    "title": "Best Way to Enable Multi-Database Selection in H2O GPT?",
    "author": "BhoomikaMuralidhara",
    "state": "open",
    "created_at": "2025-01-21T10:57:22Z",
    "updated_at": "2025-03-31T04:51:21Z",
    "labels": [],
    "body": "Hello,\nI’m trying to implement a feature in H2O GPT where users can select multiple databases in real-time using checkboxes, and the system queries all selected databases dynamically as the selection changes. My goals are:\n\nAllow users to select or deselect databases (via checkboxes) dynamically.\nPerform queries in real-time based on the current selection.\nCombine results from all selected databases into a single response.\nI’m working with the evaluate_nochat function and want to modify:\n\nlangchain_mode to support multi-selection (as a list).\nQuery logic to dynamically loop through the selected databases and update results in real-time.\nIf anyone has experience with real-time query updates or can suggest the best approach to achieve this, I’d really appreciate your insights!\n\nThanks in advance!",
    "comments": [
      {
        "user": "NKM999",
        "body": "> Hello, I’m trying to implement a feature in H2O GPT where users can select multiple databases in real-time using checkboxes, and the system queries all selected databases dynamically as the selection changes. My goals are:\n> \n> Allow users to select or deselect databases (via checkboxes) dynamically. Perform queries in real-time based on the current selection. Combine results from all selected databases into a single response. I’m working with the evaluate_nochat function and want to modify:\n> \n> langchain_mode to support multi-selection (as a list). Query logic to dynamically loop through the selected databases and update results in real-time. If anyone has experience with real-time query updates or can suggest the best approach to achieve this, I’d really appreciate your insights!\n> \n> Thanks in advance!\n\nHi, could you get any solution??? @pseudotensor  Please help and share!!!"
      }
    ]
  },
  {
    "issue_number": 1941,
    "title": "Add \"All\" Collection Tab to Query Data from All Collections",
    "author": "intelligenceabhii",
    "state": "open",
    "created_at": "2025-03-30T17:26:18Z",
    "updated_at": "2025-03-31T04:46:54Z",
    "labels": [],
    "body": "Hi @pseudotensor I would like to request a feature to add an \"All\" collection tab that allows users to query data across all available collections at once. Selecting the \"All\" tab should fetch and display data from every collection, simplifying the process of querying and viewing data across collections without switching between them.\n\n**If this feature is already available, please provide guidance on how to enable it.**",
    "comments": [
      {
        "user": "NKM999",
        "body": "> Hi [@pseudotensor](https://github.com/pseudotensor) I would like to request a feature to add an \"All\" collection tab that allows users to query data across all available collections at once. Selecting the \"All\" tab should fetch and display data from every collection, simplifying the process of querying and viewing data across collections without switching between them.\n> \n> **If this feature is already available, please provide guidance on how to enable it.**\n\nI am also having the same issue. I tried harder but could not get it.  @pseudotensor Please save me!!!"
      }
    ]
  },
  {
    "issue_number": 1940,
    "title": "GCR is Depricated, unable to pull images",
    "author": "z0rx0r",
    "state": "closed",
    "created_at": "2025-03-26T17:42:28Z",
    "updated_at": "2025-03-26T22:21:52Z",
    "labels": [],
    "body": "As of March 18th GCR is no longer available as a container registry - this makes pulling gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 not possible. \n\nPlease update where you are storing the base images. ",
    "comments": [
      {
        "user": "lakinduakash",
        "body": "The new repository is at docker hub,\nwe can use `docker pull h2oairelease/h2oai-h2ogpt-runtime:v0.2.1-1245`"
      }
    ]
  },
  {
    "issue_number": 1934,
    "title": "MyData is not deleting docs on page load or re-load with authentication.",
    "author": "intelligenceabhii",
    "state": "open",
    "created_at": "2025-03-11T04:49:52Z",
    "updated_at": "2025-03-11T04:49:52Z",
    "labels": [],
    "body": "It has been observed that when accessed without authentication, the MyData PDF is deleted as expected. However, the MyData collection is not deleted upon page load / re-load on the Document tab when accessed with authentication. How to achieve the docs deletion with authentication? Please help @pseudotensor !!!",
    "comments": []
  },
  {
    "issue_number": 369,
    "title": "About UserData and MyData",
    "author": "joaonevesvisualnuts",
    "state": "closed",
    "created_at": "2023-06-30T19:02:32Z",
    "updated_at": "2025-03-10T09:58:33Z",
    "labels": [],
    "body": "When I upload anything to these databases, are they public?\r\nFrom what I understood, \"MyData\" is for me only, \"UserData\" is shared, but shared with whom?\r\nIs it with everyone connected to my private server or is it to a public server?\r\n\r\nThank you in advance.\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "UserData: Shared with anyone who is on your server.  Persisted across sessions in single location for entire server.  Control upload via allow_upload_to_user_data option.  Useful for collaboration.\r\n\r\nMyData: Scratch space that is inaccessible if one goes into a new browser session.  Useful for public demonstrations so that every instance is independent.  Or useful  if user is not allowed to upload to shared UserData and wants to do Q/A."
      },
      {
        "user": "intelligenceabhii",
        "body": "It has been observed that when accessed without authentication, the MyData PDF is deleted as expected. However,  the MyData collection is not deleted upon page load / re-load on the Document tab when accessed with authentication. How to achieve the docs deletion with authentication? Please help @pseudotensor !!!"
      }
    ]
  },
  {
    "issue_number": 1933,
    "title": "Fresh install fails on Ubuntu 24.04 LTS",
    "author": "dawnarius",
    "state": "open",
    "created_at": "2025-02-27T08:57:43Z",
    "updated_at": "2025-02-27T10:43:00Z",
    "labels": [],
    "body": "Hello\nSince this project is obviously no longer maintained I can't manage to install it anymore (on a fresh Ubuntu 24.04 LTS with python 3.10 and following the official guides)\nToo much errors and conflicts since some packages are actively maintained but not h2ogpt code\nCan anyone provide me with list of the correct versions of the used packages? or a clear and working guide to install it on a fresh Ubuntu 24.04 LTS with python 3.10?\nAlso if anyone has a working version, please do \"pip env list\" and give me the resulting list\nIt could really help\nThanks ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I haven't tried Ubuntu 24 yet, just 22.  Maybe someone else has tried it.\n\nI don't expect any conflicts etc. because most packages are fixed in version that ever had had issues in past.\n\nE.g. you could try docker build but change to Ubuntu 24.  I expect it would work fine.\n\nA separate internal version is maintained and i haven't noticed any issues in any builds."
      }
    ]
  },
  {
    "issue_number": 1818,
    "title": "I encountered an error while trying to use the tool. This was the error: 1 validation error for SerperDevToolSchema search_query   str type expected (type=type_error.str).  Tool Search the internet accepts these inputs: Search the internet(search_query: 'string') - A tool that can be used to search the internet with a search_query. search_query: 'Mandatory search query you want to use to search the internet'",
    "author": "RaghavMangla",
    "state": "open",
    "created_at": "2024-08-28T11:26:01Z",
    "updated_at": "2025-02-09T09:22:10Z",
    "labels": [],
    "body": "how to fix this error, occurs while running a crewai agent",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I've seen that sometimes, I think the serp stuff maybe fails or does something odd sometimes.\r\n\r\nDo you see this sometimes or all the time?"
      },
      {
        "user": "Ansumanbhujabal",
        "body": "I am seeing that frequently these days , Any solutions?"
      },
      {
        "user": "pseudotensor",
        "body": "Can you give more details?  Full back trace?  When it occurs?  What system?  Thanks!"
      }
    ]
  },
  {
    "issue_number": 1930,
    "title": "GoogleSerperAPIWrapper gives error     \" Input should be a valid string [type=string_type, input_value={'description': 'Dr. Mihi...n field', 'type': 'str'}, input_type=dict]     For further information visit https://errors.pydantic.dev/2.10/v/string_type.\"",
    "author": "Ansumanbhujabal",
    "state": "open",
    "created_at": "2025-02-06T15:24:49Z",
    "updated_at": "2025-02-06T15:24:49Z",
    "labels": [],
    "body": "### **Sample Serper use**\n```\nsearch = GoogleSerperAPIWrapper()\n@tool(\"GoogleSearchTool\")\ndef Google_search_tool(search_query: str):\n    \"\"\"Performs a search using the GoogleSearchTool.\"\"\"\n    return search().run(search_query)\n## Defining Agents\n\nfamous_personality_reviewer_agent = Agent(role=\n                          \"Senior Content Validator and fact checker\",\n                          goal=\"Validate the content with facts and get the most correct content\", \n                          backstory=\n                          \"\"\"\n                          You are a Senior Content Validator and fact checker  who has been assigned to validate the task \n                          for the {career_name} field if {famous_personalities_name} You will give best fact based \n                          \"\"\",\n                          llm=llm,\n                          tools = [Google_search_tool],\n                          verbose=True)\n```\n\n### **Error is** \n\n```\nI encountered an error while trying to use the tool. This was the error: Arguments validation failed: 1 validation error for Googlesearchtool\nsearch_query\n  Input should be a valid string [type=string_type, input_value={'description': 'Dr. Mihi...n field', 'type': 'str'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type.\n Tool GoogleSearchTool accepts these inputs: Tool Name: GoogleSearchTool\nTool Arguments: {'search_query': {'description': None, 'type': 'str'}}\nTool Description: Performs a search using the GoogleSearchTool.\n```\n\n\n### **Dependecies**\n\n```\nagentneo==1.2.3\ncachetools==5.5.0\ncertifi==2024.12.14\ncrewai==0.95.0\ncrewai-tools==0.32.0\nGitPython==3.1.44\ngoogle-api-core==2.24.0\ngoogle-auth==2.37.0\ngoogle-cloud-aiplatform==1.77.0\ngoogle-cloud-bigquery==3.27.0\ngoogle-cloud-core==2.4.1\ngoogle-cloud-resource-manager==1.14.0\ngoogle-cloud-storage==2.19.0\ngoogle-crc32c==1.6.0\ngoogle-resumable-media==2.7.2\ngoogleapis-common-protos==1.66.0\ngroq==0.13.1\ngrpc-google-iam-v1==0.14.0\nlangchain==0.3.17\nlangchain-cohere==0.3.4\nlangchain-community==0.3.16\nlangchain-core==0.3.33\nlangchain-exa==0.2.1\nlangchain-experimental==0.3.4\nlangchain-groq==0.2.2\nlangchain-openai==0.2.14\nlangchain-pinecone==0.2.0\nlangchain-text-splitters==0.3.5\npydantic==2.10.4\npydantic-settings==2.7.1\npydantic_core==2.27.2\nscrapegraph_py==1.10.0\nselenium==4.27.1\nserpapi==0.1.5\nsetuptools==75.8.0\n\n```\n",
    "comments": []
  },
  {
    "issue_number": 1928,
    "title": "[Errno 24] https://gpt.h2o.ai/",
    "author": "DorinDXN",
    "state": "closed",
    "created_at": "2025-01-28T18:42:24Z",
    "updated_at": "2025-01-28T19:10:58Z",
    "labels": [],
    "body": "In https://gpt.h2o.ai/ \nthis error occurs, then multiple errors about connections\n[Errno 24] Too many open files: 'sources_dir/sources_LLM_53720439-1770-44b7-a5e5-0ac3ac94738f'\n\nPlease investigate,\ncheers,\nDorin",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Is it ok after restart?  I seem to have to restart once a month or so."
      }
    ]
  },
  {
    "issue_number": 1926,
    "title": "Issue with Large-Scale Document Embedding in H2O GPT",
    "author": "BhoomikaMuralidhara",
    "state": "open",
    "created_at": "2025-01-22T11:57:49Z",
    "updated_at": "2025-01-22T11:57:49Z",
    "labels": [],
    "body": "Hi everyone,\n\nA mode was created specifically for an email folder in H2O GPT, where all documents are .docx. An issue has been observed when embedding a large number of documents into this mode.\n\nHere’s what happens:\n\nWhen embedding fewer documents (e.g., around 100 or less), everything works fine—all documents are successfully added to the database, and new ones can be added without any problems.\nHowever, when embedding a large number of documents (e.g., around 13,000 .docx files), only a portion of the documents (approximately 4,000) appears in the database. After that, adding new documents becomes impossible.\nThis issue seems specific to the email folder mode. Since all documents are .docx, it doesn’t appear to be related to missing libraries.\n\nCould this behavior be related to:\n\nA database size limit?\nMemory constraints?\nA misconfiguration in the mode or embedding setup?\nAny insights or suggestions for resolving this would be greatly appreciated.\n\nThanks in advance!",
    "comments": []
  },
  {
    "issue_number": 1924,
    "title": "Status of project? No new versions since 24.06.04; docker images on soon to be deprecated container registry",
    "author": "agm-eratosth",
    "state": "open",
    "created_at": "2025-01-20T22:27:34Z",
    "updated_at": "2025-01-22T07:37:30Z",
    "labels": [],
    "body": "Hi,\n\nCould you please comment on the status and roadmap of this project? I'm seeing some code commits but version bumps have ended since June. \n\nAdditionally Docker images used to get a commit specific build at https://gcr.io/vorvan/h2oai/h2ogpt-runtime but that hasn't happened since August. Finally, there is a deprecation warning when visiting that URL that the google cloud container registry will be deprecated on March 18th 2025 in favor of the artifact registry.\n\nThanks.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "The pypi version is somewhat incomplete compared to the full version for windows and linux via docker or the installer, so it hasn't been much of a focus to frequently update releases.  There are just alot of non-pypi package things that are required for the full thing.\n\n@EshamAaqib Perhaps the link to the OSS h2oGPT vorvan is different at some point?  Aug is too early a cut-off, so it must have moved."
      },
      {
        "user": "pseudotensor",
        "body": "The current location on vorvan is https://console.cloud.google.com/gcr/images/vorvan/global/h2oai/h2oai-h2ogpt-runtime\n\n![Image](https://github.com/user-attachments/assets/ebcf797a-e51f-4a7f-9d89-1a55ad6a43ca)"
      }
    ]
  },
  {
    "issue_number": 1921,
    "title": "`pydantic.errors.PydanticUserError` on CPU",
    "author": "denmuslimov",
    "state": "open",
    "created_at": "2025-01-15T03:39:45Z",
    "updated_at": "2025-01-15T03:39:45Z",
    "labels": [],
    "body": "Trying to run a CPU-based model on Windows 11.\nA clean installation following this instruction (changed the name of conda environment to avoid collisions):\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md\n\nTried running any model from the instruction below:\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_CPU.md\n\nFirst, I got errors for missing libraries:\n```\npip install fire\npip install langchain\npip install -U langchain-anthropic\npip install langchain-community\npip install langchain-experimental\npip install langchain-google-genai\n```\n\nAfter I got just this message:\n```\nC:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_config.py:345: UserWarning: Valid config keys have changed in V2:\n* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n  warnings.warn(message, UserWarning)\nTraceback (most recent call last):\n  File \"C:\\Users\\Worker\\Documents\\AI\\CPU_based\\h2ogpt\\generate.py\", line 20, in <module>\n    entrypoint_main()\n  File \"C:\\Users\\Worker\\Documents\\AI\\CPU_based\\h2ogpt\\generate.py\", line 16, in entrypoint_main\n    H2O_Fire(main)\n  File \"C:\\Users\\Worker\\Documents\\AI\\CPU_based\\h2ogpt\\src\\utils.py\", line 79, in H2O_Fire\n    fire.Fire(component=component, command=args)\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\fire\\core.py\", line 135, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\fire\\core.py\", line 468, in _Fire\n    component, remaining_args = _CallAndUpdateTrace(\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\fire\\core.py\", line 684, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n  File \"C:\\Users\\Worker\\Documents\\AI\\CPU_based\\h2ogpt\\src\\gen.py\", line 2058, in main\n    from gpt_langchain import get_embedding\n  File \"C:\\Users\\Worker\\Documents\\AI\\CPU_based\\h2ogpt\\src\\gpt_langchain.py\", line 60, in <module>\n    from langchain_mistralai.chat_models import ChatMistralAI\n  File \"C:\\Users\\Worker\\Documents\\AI\\CPU_based\\h2ogpt\\src\\langchain_mistralai\\chat_models.py\", line 317, in <module>\n    class ChatMistralAI(BaseChatModel):\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_model_construction.py\", line 224, in __new__\n    complete_model_class(\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_model_construction.py\", line 602, in complete_model_class\n    schema = cls.__get_pydantic_core_schema__(cls, handler)\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\main.py\", line 702, in __get_pydantic_core_schema__\n    return handler(source)\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 84, in __call__\n    schema = self._handler(source_type)\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 610, in generate_schema\n    schema = self._generate_schema_inner(obj)\n  ...\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 612, in generate_schema\n    metadata_js_function = _extract_get_pydantic_json_schema(obj, schema)\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt_cpu\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 2395, in _extract_get_pydantic_json_schema\n    raise PydanticUserError(\npydantic.errors.PydanticUserError: The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n```\n",
    "comments": []
  },
  {
    "issue_number": 1778,
    "title": "AutoGen Steps",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-08-01T21:38:29Z",
    "updated_at": "2025-01-11T07:11:55Z",
    "labels": [],
    "body": "Example prompts:\r\n* `Make a matplotlib plot and save as titanic.png, for the titanic surivors vs. other parameters in interesting way.`\r\n* `Today is August 31, 2024.  Write Python code to plot TSLA's and META's stock price gains YTD vs. time per week, and save the plot to a file named 'stock_gains.png'`\r\n\r\n\r\n- [x] code executation agent\r\n- [x] Add OpenAI Files API\r\n- [x] Return files in some way supported by OpenAI API for files so give back file objects instead of disk locations. use usage to return file_ids\r\n- [x] Prevent autogen from going on OpenAI server\r\n- [x] handle pip install issues like AgentZero\r\n- [x] Avoid newline in streaming of iostream\r\n- [x] Remove empty vs. None messages\r\n- [x] Collapse assistants\r\n- [ ] Check how continue works, seems to get stuck in loop (i.e. max_tokens small)\r\n- [x] Control docker life time externally, too slow (10s) for each chat to stop it then\r\n- [x] If no pyautogen, then don't allow autogen_server True\r\n- [x] DinD needs `--privileged` for docker in docker run.  But better is Docker out of Docker: https://microsoft.github.io/autogen/docs/topics/code-execution/cli-code-executor/#combining-autogen-in-docker-with-a-docker-based-executor with `-v /var/run/docker.sock:/var/run/docker.sock` to base `docker run`\r\n\r\n\r\nhttps://microsoft.github.io/autogen/docs/tutorial/conversation-patterns/\r\nhttps://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment\r\nhttps://microsoft.github.io/autogen/docs/tutorial/code-executors/\r\n\r\n\r\nSome discussions:\r\nhttps://www.reddit.com/r/LangChain/comments/1db6evc/best_production_agent_framework_langraph_vs/\r\nhttps://www.reddit.com/r/LangChain/comments/1b7q44y/autogen_vs_langgraph/\r\nlangchain tools with autogen: https://github.com/microsoft/autogen/blob/main/notebook/agentchat_langchain.ipynb",
    "comments": []
  },
  {
    "issue_number": 1919,
    "title": "Windows 11: Can't run LLama",
    "author": "denmuslimov",
    "state": "open",
    "created_at": "2025-01-10T04:09:58Z",
    "updated_at": "2025-01-10T04:09:58Z",
    "labels": [],
    "body": "I'm experimenting with different models.\r\nSome refuse to run.\r\n\r\nWindows 11, RTX 4070 12Gb\r\n\r\nThe following commands: \r\n`python generate.py --base_model=TheBloke/Llama-2-7b-Chat-GPTQ --load_gptq=\"model\" --use_safetensors=True --prompt_type=llama2 --save_dir='save'`\r\n`python generate.py --base_model=TheBloke/Nous-Hermes-13B-GPTQ --score_model=None --load_gptq=model --use_safetensors=True --prompt_type=instruct --langchain_mode=UserData`\r\n\r\nReturn the this error:\r\n```\r\n...\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\utils.py\", line 79, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 135, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 468, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 684, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\gen.py\", line 2329, in main\r\n    model_state_trial = model_lock_to_state(model_dict, cache_model_state=False, **kwargs_model_lock_to_state)\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\model_utils.py\", line 1689, in model_lock_to_state\r\n    return __model_lock_to_state(model_dict1, **kwargs)\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\model_utils.py\", line 1767, in __model_lock_to_state\r\n    model0, tokenizer0, device = get_model_retry(reward_type=False,\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\model_utils.py\", line 425, in get_model_retry\r\n    model1, tokenizer1, device1 = get_model(**kwargs)\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\model_utils.py\", line 1208, in get_model\r\n    return get_hf_model(load_8bit=load_8bit,\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\model_utils.py\", line 1534, in get_hf_model\r\n    model = exllama_set_max_input_length(model, tokenizer.model_max_length)\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\auto_gptq\\utils\\exllama_utils.py\", line 15, in exllama_set_max_input_length\r\n    from exllama_kernels import prepare_buffers, cleanup_buffers_cuda\r\nImportError: DLL load failed while importing exllama_kernels: The specified module could not be found.\r\n```",
    "comments": []
  },
  {
    "issue_number": 1918,
    "title": "Windows Install -- getting error when running.",
    "author": "denmuslimov",
    "state": "closed",
    "created_at": "2025-01-06T02:15:18Z",
    "updated_at": "2025-01-08T05:01:16Z",
    "labels": [],
    "body": "After installing it, I can run the H2O GPT on `http://localhost:7860/`.\r\nBut every question results in an error.\r\n\r\nI have a Windows 11 with RTX 4070.\r\nFollowed this link for installation:\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md\r\nInstalled LLaMa simply with `pip install llama-cpp-python`.\r\n\r\nError:\r\n\r\n```\r\nINFO:     127.0.0.1:56398 - \"POST /queue/join HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:56398 - \"GET /queue/data?session_hash=00eo3wbgbcmln HTTP/1.1\" 200 OK\r\nevaluate_nochat exception: LlamaTokenizerFast has no attribute _pad_token: ('', '', '', True, False, 'zephyr'\r\n  ... \r\n, 'host2': '127.0.0.1', 'picture': 'None'}, {}, [['Hello', '']])\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\queueing.py\", line 575, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\r\n    result = await self.call_function(\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\blocks.py\", line 1532, in call_function\r\n    prediction = await utils.async_iteration(iterator)\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\utils.py\", line 671, in async_iteration\r\n    return await iterator.__anext__()\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\utils.py\", line 664, in __anext__\r\n    return await anyio.to_thread.run_sync(\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\r\n    result = context.run(func, *args)\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\utils.py\", line 647, in run_sync_iterator_async\r\n    return next(iterator)\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\utils.py\", line 809, in gen_wrapper\r\n    response = next(iterator)\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\gradio_funcs.py\", line 1215, in bot\r\n    for res in get_response(fun1, history, chatbot_role1, speaker1, tts_language1, roles_state1,\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\gradio_funcs.py\", line 625, in get_response\r\n    yield from _get_response(fun1, history, chatbot_role1, speaker1, tts_language1, roles_state1, tts_speed1,\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\gradio_funcs.py\", line 870, in _get_response\r\n    for output_fun in fun1():\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\gen.py\", line 4165, in evaluate\r\n    stopping_criteria = get_stopping(prompt_type, prompt_dict, tokenizer, device, base_model,\r\n  File \"C:\\Users\\Worker\\Documents\\AI\\h2ogpt\\src\\stopping.py\", line 183, in get_stopping\r\n    if tokenizer._pad_token:  # use hidden variable to avoid annoying properly logger bug\r\n  File \"C:\\Users\\Worker\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 1104, in __getattr__\r\n    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\r\nAttributeError: LlamaTokenizerFast has no attribute _pad_token. Did you mean: '_add_tokens'?\r\n```",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "07ad82bc498e127a22381d98dbffbf1b845ef4e9"
      },
      {
        "user": "denmuslimov",
        "body": "Thank you!"
      }
    ]
  },
  {
    "issue_number": 1917,
    "title": "Throwing:  AttributeError: PreTrainedTokenizerFast has no attribute _pad_token. Did you mean: '_add_tokens' at runtime",
    "author": "l0r3zz",
    "state": "closed",
    "created_at": "2025-01-05T19:21:20Z",
    "updated_at": "2025-01-08T01:21:03Z",
    "labels": [],
    "body": "Hello all,\r\nRunning on an \r\n\r\n- Core™ i7-11800H @ 2.30GHz × 16\r\n- NVIDIA GeForce RTX 3070 Laptop GPU/PCIe/SSE2 / NVIDIA Corporation GA104M \r\n- Memory :64GiB\r\n- OS: Pop!_OS 22.04 LTS\r\n\r\nUsing startup...\r\npython generate.py \\\r\n        --base_model=h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \\\r\n        --score_model=None \\\r\n        --prompt_type=human_bot \\\r\n        --cli=True \\\r\n        --gradio_offline_level=1 \\\r\n        --load4bit=True\r\n\r\ncurrent repo version: \r\nbase) l0r3zz@tarnover:[2025-01-05 11:18:40]-$git log -1\r\ncommit a0fcc3344d53a834fe3cb5b26265aaeb84993b77 (HEAD -> main, origin/main, origin/HEAD)\r\nAuthor: Jonathan C. McKinney <pseudotensor@gmail.com>\r\nDate:   Tue Dec 3 23:58:28 2024 -0800\r\n\r\n\r\n(Got here after watching): https://youtu.be/Coj72EzmX20?si=ofBAsNACnB7JAKe7\r\n\r\ngot through all the build issues, but after startup, and the printing of:\r\n_Enter an instruction:_ \r\n\r\nIt blows up no matter what I enter...\r\n\r\n\r\n(base) l0r3zz@tarnover:[2025-01-04 07:52:56]-$./model.sh \r\nMust install langchain for transcription, disabling\r\nUsing Model h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\r\nMust install langchain for preloading embedding model, disabling\r\nMust install DocTR and LangChain installed if enabled DocTR, disabling\r\nStarting get_model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \r\nCould not determine --max_seq_len, setting to 4096.  Pass if not correct\r\n/home/l0r3zz/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nCould not determine --max_seq_len, setting to 4096.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 4096.  Pass if not correct\r\ndevice_map: {'': 0}\r\nStarting get_model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \r\nCould not determine --max_seq_len, setting to 4096.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 4096.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 4096.  Pass if not correct\r\ndevice_map: {'': 0}\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.62s/it]\r\n\r\nEnter an instruction: Hello World\r\nTraceback (most recent call last):\r\n  File \"/home/l0r3zz/github/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/l0r3zz/github/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/l0r3zz/github/h2ogpt/src/utils.py\", line 79, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/l0r3zz/.local/lib/python3.12/site-packages/fire/core.py\", line 135, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l0r3zz/.local/lib/python3.12/site-packages/fire/core.py\", line 468, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l0r3zz/.local/lib/python3.12/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l0r3zz/github/h2ogpt/src/gen.py\", line 2430, in main\r\n    return run_cli(**get_kwargs(run_cli, **local_kwargs))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l0r3zz/github/h2ogpt/src/cli.py\", line 226, in run_cli\r\n    for gen_output in gener:\r\n                      ^^^^^\r\n  File \"/home/l0r3zz/github/h2ogpt/src/gen.py\", line 4165, in evaluate\r\n    stopping_criteria = get_stopping(prompt_type, prompt_dict, tokenizer, device, base_model,\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l0r3zz/github/h2ogpt/src/stopping.py\", line 183, in get_stopping\r\n    if tokenizer._pad_token:  # use hidden variable to avoid annoying properly logger bug\r\n       ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l0r3zz/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 1104, in __getattr__\r\n    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\r\nAttributeError: PreTrainedTokenizerFast has no attribute _pad_token. Did you mean: '_add_tokens'?\r\n\r\n\r\nI tried some troubleshooting but can't get anywhere...\r\n\r\n\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "07ad82bc498e127a22381d98dbffbf1b845ef4e9"
      }
    ]
  },
  {
    "issue_number": 1915,
    "title": "HTML format",
    "author": "Abhishek577",
    "state": "open",
    "created_at": "2025-01-01T09:39:52Z",
    "updated_at": "2025-01-01T09:39:52Z",
    "labels": [],
    "body": "While using own collection for querying the **auth.db** is storing the meta data, HTML tags and glimpse of source instead of the replay. All these are visible from the text_output of data column in auth.db. How to store the response only",
    "comments": []
  },
  {
    "issue_number": 1912,
    "title": "Docker container not running in offline mode despite following documentation",
    "author": "llmIntruder",
    "state": "open",
    "created_at": "2024-12-26T15:15:59Z",
    "updated_at": "2024-12-26T15:15:59Z",
    "labels": [],
    "body": "Hi,\r\nI'm trying to run a Docker container offline, but I am encountering issues where the container still tries to download models and dependencies from the internet during runtime. I've followed the official documentation for Docker offline setup and have manually copied the .huggingface_cache and all necessary files, but the container is still connecting to the internet.",
    "comments": []
  },
  {
    "issue_number": 1911,
    "title": "Files Uploaded via API Stored in Temp Folder Instead of Specific Path",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-12-24T06:17:49Z",
    "updated_at": "2024-12-24T06:17:49Z",
    "labels": [],
    "body": "## Description\r\nWhen I upload documents using the UI upload button, the files are correctly stored in the specified path associated with the collection (e.g., test_path/Introduction - Ladakh Expedition.pdf). However, when uploading the same files using the /upload_api and /add_file_api endpoints, the files are being stored in a temporary folder (e.g., /tmp/gradio/<random_hash>/<filename>).\r\nThis discrepancy in behavior between the UI and API uploads causes inconsistency in file storage, making it difficult to manage uploaded files.\r\n\r\nSteps to Reproduce\r\nUpload a file through the UI upload button.\r\nResult: File is stored in the correct path (e.g., test_path/).\r\nUpload the same file using the /upload_api and /add_file_api endpoints.\r\nResult: File is stored in a temp folder (e.g., /tmp/gradio/<random_hash>/).\r\n\r\nExpected Behavior\r\nFiles uploaded via the API should be stored in the same path as those uploaded through the UI (e.g., test_path/).\r\nActual Behavior\r\nFiles uploaded via the API are stored in a temporary folder instead of the intended path.\r\n\r\n```\r\n@app.post(\"/upload_add_document\")\r\nasync def upload_add_document(\r\n    langchain_mode: str = Form(...),\r\n    file: UploadFile = File(...)\r\n):\r\n    try:\r\n        allowed_extensions = {\".pdf\", \".txt\", \".docx\"}\r\n        _, ext = os.path.splitext(file.filename)\r\n        if ext.lower() not in allowed_extensions:\r\n            raise HTTPException(status_code=400, detail=\"Unsupported file type. Allowed types: .pdf, .txt, .docx\")\r\n\r\n        # Save the uploaded file locally\r\n        local_file_path = f\"{file.filename}\"\r\n        with open(local_file_path, \"wb\") as f:\r\n            f.write(file.file.read())\r\n\r\n        # Step 1: Upload the document\r\n        try:\r\n            with tqdm(total=100, desc=f\"Uploading {file.filename}\", unit='%', ncols=80) as pbar:\r\n                x, server_file_path = client.predict(local_file_path, api_name='/upload_api')\r\n                pbar.update(100)\r\n        except Exception as e:\r\n            raise HTTPException(status_code=500, detail=f\"Failed to upload document: {str(e)}\")\r\n\r\n        # Step 2: Add the document with OCR processing\r\n        try:\r\n            loaders = tuple([None, None, None, None, None, None])  # Adjust loaders as needed\r\n            with tqdm(total=100, desc=f\"Adding {file.filename} to {langchain_mode}\", unit='%', ncols=80) as pbar:\r\n                response = client.predict(\r\n                    server_file_path, langchain_mode, True, 512, True, *loaders, api_name='/add_file_api'\r\n                )\r\n                pbar.update(100)\r\n        except Exception as e:\r\n            raise HTTPException(status_code=500, detail=f\"Failed to add document: {str(e)}\")\r\n\r\n        # Cleanup: Delete the local file after processing\r\n        if os.path.exists(local_file_path):\r\n            os.remove(local_file_path)\r\n\r\n        return {\r\n            \"status\": \"success\",\r\n            \"langchain_mode\": langchain_mode,\r\n            \"filename\": file.filename,\r\n            \"server_file_path\": server_file_path,\r\n        }\r\n\r\n    except Exception as e:\r\n        if os.path.exists(local_file_path):\r\n            os.remove(local_file_path)\r\n        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\r\n```     \r\n       \r\n### Observed File Paths\r\nUI Uploads: test_path/Introduction - Ladakh Expedition.pdf\r\nAPI Uploads: /tmp/gradio/<random_hash>/Rahul_text.txt\r\n\r\nIts urgent please help me or suggest a way to resolve this issue on priority basis.\r\n@pseudotensor",
    "comments": []
  },
  {
    "issue_number": 1910,
    "title": "Unable to Locate Login Page File for Adding Download Button",
    "author": "Abhishek577",
    "state": "open",
    "created_at": "2024-12-20T11:59:16Z",
    "updated_at": "2024-12-20T11:59:16Z",
    "labels": [],
    "body": "I am looking to add a download button to the login page to allow users to access an instruction document easily. However, I am unable to locate the specific file that handles the login page within the project. I’ve searched through the project directories but couldn’t identify where the login page is defined or rendered.\r\n\r\nCould someone please guide me on how to find the correct file or component for the login page? Additionally, any suggestions on the best way to implement a download button in this context would be greatly appreciated.\r\n\r\nThank you for your help!",
    "comments": []
  },
  {
    "issue_number": 1907,
    "title": "Installing clean environment fails after execution of \"bash docs/linux_install_full.sh\"",
    "author": "juerware",
    "state": "open",
    "created_at": "2024-12-13T10:19:07Z",
    "updated_at": "2024-12-18T10:22:01Z",
    "labels": [],
    "body": "From clear environment, installing full environment is wanted executing:\r\n```bash docs/linux_install_full.sh```\r\nCurrent checkout of the respository ```a0fcc334``` (latest one at the moment of execution)\r\nError shown:\r\n```\r\n+ bash ./docs/run_patches.sh\r\n++ python3.10 -c 'import site; print(site.getsitepackages()[0])'\r\n+ sp=/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages\r\n+ sed -i 's/with HiddenPrints():/if True:/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/utilities/serpapi.py\r\n+ sed -i 's/client='\\''ANDROID_MUSIC'\\''/client='\\''ANDROID'\\''/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/17.31.35/19.08.35/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/17.33.2/19.08.35/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/17.31.35/19.08.35/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/17.33.2/19.08.35/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/5.16.51/6.40.52/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/5.21/6.41/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pytube/innertube.py\r\n+ sed -i s/pytubefixfix/pytubefix/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fiftyone/utils/youtube.py\r\n+ sed -i s/Pytube/PytubeFix/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fiftyone/utils/youtube.py\r\n+ sed -i s/PytubeFixFix/PytubeFix/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fiftyone/utils/youtube.py\r\n+ sed -i s/Pytube/Pytubefix/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fiftyone/utils/youtube.py\r\n+ sed -i 's/pytube>=15/pytube>=6/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fiftyone/utils/youtube.py\r\n+ sed -i s/pytube/pytubefix/g /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fiftyone/utils/youtube.py\r\n+ sed -i 's/except OSError:/except (OSError, RuntimeError):/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\r\n+ sed -i 's/while True:/while True:\\n            time.sleep(0.001)\\n/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/gradio_client/client.py\r\n+ patch /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py docs/trans.patch\r\npatching file /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py\r\nHunk #1 FAILED at 3412.\r\n1 out of 1 hunk FAILED -- saving rejects to file /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py.rej\r\n```\r\n\r\nIn the same commit but following the install documentation with following error:\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md#install\r\n```\r\n+ bash ./docs/run_patches.sh\r\n++ python3.10 -c 'import site; print(site.getsitepackages()[0])'\r\n+ sp=/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages\r\n+ sed -i 's/with HiddenPrints():/if True:/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/utilities/serpapi.py\r\n+ sed -i 's/except OSError:/except (OSError, RuntimeError):/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\r\n+ sed -i 's/while True:/while True:\\n            time.sleep(0.001)\\n/g' /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/gradio_client/client.py\r\n+ patch /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py docs/trans.patch\r\npatching file /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py\r\nHunk #1 FAILED at 3412.\r\n1 out of 1 hunk FAILED -- saving rejects to file /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py.rej\r\n```",
    "comments": []
  },
  {
    "issue_number": 1908,
    "title": "[Errno 28] No space left on device",
    "author": "DorinDXN",
    "state": "closed",
    "created_at": "2024-12-13T19:01:55Z",
    "updated_at": "2024-12-14T21:15:32Z",
    "labels": [],
    "body": "Hi there, hope you're well :)\r\n\r\nI'm getting this error \r\n'[Errno 28] No space left on device'\r\n in the browser. \r\nI think it started 2-3 days ago. I thought the issue was on my machine, but maybe it's a server issue. \r\nPlease check.\r\n\r\nbest regards,\r\nDorin",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "You mean gpt.h2o.ai or gpt-docs.h2o.ai\r\n\r\nYes, I see that, I cleared some space and restarted the underlying gpt.h2o.ai server.  Both seem ok now."
      }
    ]
  },
  {
    "issue_number": 1906,
    "title": "Llama 3.2 vision and 3.3",
    "author": "chengchu88",
    "state": "open",
    "created_at": "2024-12-12T13:29:23Z",
    "updated_at": "2024-12-12T13:29:23Z",
    "labels": [],
    "body": "Hello sir,\r\nI am running h2ogpt 0.2.0, and I can deploy llama 3 with no issue, but got error when trying to deploy llama. 3.2 vision and llama 3.3. is there any solution to it? Does anyone have any success with h2ogpt 0.2.1?\r\n\r\nThanks\r\n",
    "comments": []
  },
  {
    "issue_number": 1905,
    "title": "Collections Disappear",
    "author": "InesBenAmor99",
    "state": "closed",
    "created_at": "2024-11-28T16:17:49Z",
    "updated_at": "2024-12-03T07:12:37Z",
    "labels": [],
    "body": "Hello, I'm facing an issue with collections. When I create collections and then log out and log back in, the collections disappear. The same happens when I refresh the page. What could be the problem?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Ensure auth is enabled so that you can login as a specific user, else you have to login in the login page."
      }
    ]
  },
  {
    "issue_number": 1600,
    "title": "random assertion errors due to evaluate_nochat",
    "author": "Blacksuan19",
    "state": "closed",
    "created_at": "2024-05-06T16:01:02Z",
    "updated_at": "2024-12-03T00:39:14Z",
    "labels": [],
    "body": "when using the docker image, I randomly get assertion errors when making a request from the gradio UI, sometimes it works and sometimes it does not, here is the raised error.\r\n\r\nthis occurs with the latest two docker images tagged `4059a2c9` and `7297519c`.\r\n\r\n<details>\r\n\r\n<summary>Full error</summary>\r\n\r\n```python\r\nthread exception: Traceback (most recent call last):                                                                                                                                                                                         File \"/workspace/src/utils.py\", line 502, in run                                                                                                                                                                                             self._return = self._target(*self._args, **self._kwargs)                                                                                                                                                                                 File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\r\n    return self.invoke(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\r\n    raise e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py\", line 137, in _call\r\n    output, extra_return_dict = self.combine_docs(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py\", line 244, in combine_docs\r\n    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 293, in predict\r\n    return self(kwargs, callbacks=callbacks)[self.output_key]\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\r\n    return self.invoke(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\r\n    raise e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 103, in _call\r\n    response = self.generate([inputs], run_manager=run_manager)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 115, in generate\r\n    return self.llm.generate_prompt(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 633, in generate_prompt\r\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 803, in generate\r\n    output = self._generate_helper(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 670, in _generate_helper\r\n    raise e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 657, in _generate_helper\r\n    self._generate(\r\n  File \"/workspace/src/gpt_langchain.py\", line 2339, in _generate\r\n    rets = super()._generate(prompts, stop=stop, run_manager=run_manager, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 267, in _generate\r\n    responses = self.pipeline(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 240, in __call__\r\n    return super().__call__(text_inputs, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1223, in __call__\r\n    outputs = list(final_iterator)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\r\n    item = next(self.iterator)\r\n File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\r\n    processed = self.infer(item, **self.params)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1149, in forward\r\n    model_outputs = self._forward(model_inputs, **forward_params)\r\n  File \"/workspace/src/h2oai_pipeline.py\", line 271, in _forward\r\n    return self.__forward(model_inputs, **generate_kwargs)\r\n  File \"/workspace/src/h2oai_pipeline.py\", line 309, in __forward\r\n    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1576, in generate\r\n    result = self._greedy_search(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2494, in _greedy_search\r\n    outputs = self(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1158, in forward\r\n    outputs = self.model(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/model.py\", line 127, in forward\r\n    h, _, _ = layer(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/block.py\", line 123, in forward\r\n    attn_output, _, past_key_value = self.attn.forward(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 235, in forward\r\n    xq, xk = self.rope.forward(xq, xk, self.start_pos, seqlen)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 60, in forward\r\n    freqs_cis = self.reshape_for_broadcast(freqs_cis, xq_).to(xq_.device)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 48, in reshape_for_broadcast\r\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\r\nAssertionError\r\n\r\nmake stop: Traceback (most recent call last):\r\n  File \"/workspace/src/utils.py\", line 502, in run\r\n    self._return = self._target(*self._args, **self._kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\r\n    return self.invoke(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\r\n    raise e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py\", line 137, in _call\r\n    output, extra_return_dict = self.combine_docs(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py\", line 244, in combine_docs\r\n    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 293, in predict\r\n    return self(kwargs, callbacks=callbacks)[self.output_key]\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\r\n    return self.invoke(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\r\n    raise e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 103, in _call\r\n    response = self.generate([inputs], run_manager=run_manager)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 115, in generate\r\n    return self.llm.generate_prompt(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 633, in generate_prompt\r\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 803, in generate\r\n    output = self._generate_helper(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 670, in _generate_helper\r\n    raise e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 657, in _generate_helper\r\n    self._generate(\r\n  File \"/workspace/src/gpt_langchain.py\", line 2339, in _generate\r\n    rets = super()._generate(prompts, stop=stop, run_manager=run_manager, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 267, in _generate\r\n    responses = self.pipeline(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 240, in __call__\r\n    return super().__call__(text_inputs, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1223, in __call__\r\n    outputs = list(final_iterator)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\r\n    item = next(self.iterator)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\r\n    processed = self.infer(item, **self.params)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1149, in forward\r\n    model_outputs = self._forward(model_inputs, **forward_params)\r\n  File \"/workspace/src/h2oai_pipeline.py\", line 271, in _forward\r\n    return self.__forward(model_inputs, **generate_kwargs)\r\n  File \"/workspace/src/h2oai_pipeline.py\", line 309, in __forward\r\n    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1576, in generate\r\n    result = self._greedy_search(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2494, in _greedy_search\r\n    outputs = self(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1158, in forward\r\n    outputs = self.model(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/model.py\", line 127, in forward\r\n    h, _, _ = layer(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/block.py\", line 123, in forward\r\n    attn_output, _, past_key_value = self.attn.forward(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 235, in forward\r\n    xq, xk = self.rope.forward(xq, xk, self.start_pos, seqlen)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 60, in forward\r\n    freqs_cis = self.reshape_for_broadcast(freqs_cis, xq_).to(xq_.device)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 48, in reshape_for_broadcast\r\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\r\nAssertionError\r\n\r\nhit stop\r\nevaluate_nochat exception: : ('', '', '', True, 'open_chat', \"{   'PreInput': None,\\n    'PreInstruct': 'GPT4 User: ',\\n    'PreResponse': 'GPT4 Assistant:',\\n    'botstr': 'GPT4 Assistant:',\\n    'can_handle_system_prompt': False,\\n\r\n  'chat_sep': '<|end_of_turn|>',\\n    'chat_turn_sep': '<|end_of_turn|>',\\n    'generates_leading_space': False,\\n    'humanstr': 'GPT4 User: ',\\n    'promptA': '',\\n    'promptB': '',\\n    'system_prompt': '',\\n    'terminate_response\r\n': ['GPT4 Assistant:', '<|end_of_turn|>']}\", 0, 1, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0.0, True, '', '', 'UserData', True, 'Query', [], 10, True, 512, 'Relevant', ['/workspace/user_path/9b999f43-2ade-4148-97cf-d2448125168c/r\r\nes/e6a9ce98_user_upload_protocols.pdf'], [], [], [], [], 'Pay attention and remember the information below, which will help to answer the question or imperative after the context ends.', 'According to only the information in the docume\r\nnt sources provided within the context above, write an insightful and well-structured response to: ', 'In order to write a concise single-paragraph or bulleted list summary, pay attention to the following text.', 'Using only the inform\r\nation in the document sources above, write a condensed and concise summary of key results (preferably as about 10 bullet points).', 'Answer this question with vibrant details in order for some NLP embedding model to use that answer as\r\nbetter query than original question: ', 'Who are you and what do you do?', 'Ensure your entire response is outputted as a single piece of strict valid JSON text.', 'Ensure your response is strictly valid JSON text.', 'Ensure your entir\r\ne response is outputted as strict valid JSON text inside a Markdown code block with the json language identifier.   Ensure all JSON keys are less than 64 characters, and ensure JSON key names are made of only alphanumerics, underscores\r\n, or hyphens.', 'Ensure you follow this JSON schema:\\n```json\\n{properties_schema}\\n```', 'auto', ['OCR', 'DocTR', 'Caption', 'ASR'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', [], [], '', False, '[]', '[]', 'best_near_prompt', 51\r\n2, -1.0, -1.0, 'split_or_merge', '\\n\\n', 0, 'auto', False, False, '[]', 'None', None, [], 1.0, None, None, 'text', '', '', '', '', {'model': 'model', 'tokenizer': 'tokenizer', 'device': 'cuda', 'base_model': 'TheBloke/openchat_3.5-16k-\r\nAWQ', 'tokenizer_base_model': '', 'lora_weights': '[]', 'inference_server': '[]', 'prompt_type': 'open_chat', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': 'GPT4 User: ', 'PreInput': None, 'PreResponse': 'GPT4 Assistant:\r\n', 'terminate_response': ['GPT4 Assistant:', '<|end_of_turn|>'], 'chat_sep': '<|end_of_turn|>', 'chat_turn_sep': '<|end_of_turn|>', 'humanstr': 'GPT4 User: ', 'botstr': 'GPT4 Assistant:', 'generates_leading_space': False, 'system_promp\r\nt': '', 'can_handle_system_prompt': False}, 'visible_models': 0, 'h2ogpt_key': None}, {'MyData': [None, '90711427-650d-458e-ac69-bc1629b452be', 'test']}, {'langchain_modes': ['Disabled', 'LLM', 'UserData'], 'langchain_mode_paths': {'Us\r\nerData': '/workspace/user_path/'}, 'langchain_mode_types': {'UserData': 'shared', 'github h2oGPT': 'shared', 'DriverlessAI docs': 'shared', 'wiki': 'shared', 'wiki_full': '', 'LLM': 'personal', 'Disabled': 'personal'}}, {'headers': '',\r\n 'host': '0.0.0.0:7850', 'username': 'test', 'connection': 'Upgrade', 'pragma': 'no-cache', 'cache-control': 'no-cache', 'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/\r\n537.36 Edg/124.0.0.0', 'upgrade': 'websocket', 'origin': 'http://0.0.0.0:7850', 'sec-websocket-version': '13', 'accept-encoding': 'gzip, deflate', 'accept-language': 'en-US,en;q=0.9,ar;q=0.8', 'cookie': 'access-token-unsecure-hhN8p\r\ny5JLVRfL-0OTPND8TGcb3qhs2GvSJQ8qV1LI50=vrLRNuXKqoKCZDSCqo1OHg; access-token-unsecure-s-dRx26Pws-xf2TfvaYIjqwWsGjiH9960S06PrlT6tg=AnrezJi1hR1NjfFx29n_bg; access-token-unsecure-SF0CZ7POfi6Imk0jDfN44qO9W9VB0hu3nUcGevVPMYw=SU1SQYZL79hpAN43\r\nhEDgIQ; access-token-unsecure-9LIDZewsE4If1yY7ixHa-yOZJO20M-PQVSDjJtfYQYA=o8YMAhHGtoLQDjMVZVITsQ; access-token-unsecure-qS0zsQdPdQYJsrMX4RXh3HQwEDeknaNz0RppngdPvGY=AGmuVQm8_KVKkMg8HdQtqg; access-token-unsecure--qfFGcbj-JQc0O0MamjIfNGlf\r\ngUrb6t7xyB3hRUL1I8=NVbKjP5O7Q3xJxHYvaiUfw; access-token-unsecure-YeY4iDfE2-hlA1izGtL7vBNbLbCosRLpSAJFo-j6_e0=xkWJTIiCTZGbhG1H60OTBg; access-token-unsecure-BwVTmtTwIzOYqtTpvsZkHQvnjr8N60WJaX_V6njwUAw=8uPW51j557W7S8ZO_e5iSQ', 'sec-websoc\r\nket-key': 'CS4lXFJi7AM2jwkdWyhKyQ==', 'sec-websocket-extensions': 'permessage-deflate; client_max_window_bits', 'host2': '14.1.206.49', 'picture': 'None'}, {}, [['summarize the given document', '']])\r\n\r\n``` \r\n\r\n\r\n</details>\r\n\r\n\r\n<details>\r\n\r\n<summary>Docker command</summary>\r\n\r\n\r\nused command to run h2ogpt\r\n\r\n```bash\r\n\r\nexport CONTEXT_LENGTH=16384\r\nexport IMAGE_TAG=7297519c\r\n\r\ndocker run \\\r\n --init \\\r\n--gpus all \\\r\n--runtime=nvidia \\\r\n--shm-size=2g \\\r\n-p 7850:7860 \\\r\n-v /etc/passwd:/etc/passwd:ro \\\r\n-v /etc/group:/etc/group:ro \\\r\n-u $(id -u):$(id -g) \\\r\ngcr.io/vorvan/h2oai/h2ogpt-runtime:$IMAGE_TAG /workspace/generate.py \\\r\n--page_title=\"GenNet AI\" \\\r\n--favicon_path=\"/workspace/assets/gennet_logo.svg\" \\\r\n--height=700 \\\r\n--gradio_size=\"medium\" \\\r\n--enable_heap_analytics=False \\\r\n--document_choice_in_sidebar=True \\\r\n--actions_in_sidebar=True \\\r\n--openai_server=False \\\r\n--use_gpu_id=False \\\r\n--score_model=None \\\r\n--prompt_type=open_chat \\\r\n--base_model=TheBloke/openchat_3.5-16k-AWQ \\\r\n--compile_model=True \\\r\n--use_cache=True \\\r\n--use_flash_attention_2=True \\\r\n--attention_sinks=True \\\r\n--sink_dict=\"{'num_sink_tokens': 4, 'window_length': $CONTEXT_LENGTH }\" \\\r\n--save_dir='/workspace/save/' \\\r\n--user_path='/workspace/user_path/' \\\r\n--langchain_mode=\"UserData\" \\\r\n--langchain_modes=\"['UserData', 'LLM']\" \\\r\n--visible_langchain_actions=\"['Query']\" \\\r\n--visible_langchain_agents=\"[]\" \\\r\n--use_llm_if_no_docs=True \\\r\n--max_seq_len=$CONTEXT_LENGTH \\\r\n--enable_ocr=True \\\r\n--enable_tts=False \\\r\n--enable_stt=False\r\n\r\n```\r\n\r\n</details>",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, I see the issue is from awq:\r\n```\r\nFile \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 48, in reshape_for_broadcast\r\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\r\n```\r\n\r\nIt's likely a bug in awq, perhaps when combined with attention sinks, flash attention, or compile of model.  While we expose those options from transformers, I cannot be sure arbitrary combinations work.\r\n\r\n\r\nIf I run:\r\n```\r\npython generate.py \\\r\n--enable_heap_analytics=False \\\r\n--document_choice_in_sidebar=True \\\r\n--actions_in_sidebar=True \\\r\n--openai_server=False \\\r\n--use_gpu_id=False \\\r\n--score_model=None \\\r\n--prompt_type=open_chat \\\r\n--base_model=TheBloke/openchat_3.5-16k-AWQ \\\r\n--compile_model=True \\\r\n--use_cache=True \\\r\n--use_flash_attention_2=True \\\r\n--attention_sinks=True \\\r\n--sink_dict=\"{'num_sink_tokens': 4, 'window_length': 16384 }\" \\\r\n--use_llm_if_no_docs=True \\\r\n--max_seq_len=16384 \\\r\n--enable_ocr=True\r\n```\r\n\r\nI don't have a generic issue running.  I removed things that shouldn't be relevant to the awq issue.\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/a14a70d2-eacb-4345-96be-c23ee2377d8b)\r\n\r\nHowever, when I upload some text and then ask a question, I get the same issue:\r\n\r\n```\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\r\n  warn_deprecated(\r\nthread exception: Traceback (most recent call last):\r\n  File \"/home/jon/h2ogpt/src/utils.py\", line 502, in run\r\n    self._return = self._target(*self._args, **self._kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\r\n    return self.invoke(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\r\n    raise e\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py\", line 137, in _call\r\n    output, extra_return_dict = self.combine_docs(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py\", line 244, in combine_docs\r\n    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 293, in predict\r\n    return self(kwargs, callbacks=callbacks)[self.output_key]\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\r\n    return wrapped(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 378, in __call__\r\n    return self.invoke(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 163, in invoke\r\n    raise e\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/base.py\", line 153, in invoke\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 103, in _call\r\n    response = self.generate([inputs], run_manager=run_manager)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain/chains/llm.py\", line 115, in generate\r\n    return self.llm.generate_prompt(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 633, in generate_prompt\r\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 803, in generate\r\n    output = self._generate_helper(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 670, in _generate_helper\r\n    raise e\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 657, in _generate_helper\r\n    self._generate(\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 2339, in _generate\r\n    rets = super()._generate(prompts, stop=stop, run_manager=run_manager, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 267, in _generate\r\n    responses = self.pipeline(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 240, in __call__\r\n    return super().__call__(text_inputs, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1223, in __call__\r\n    outputs = list(final_iterator)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\r\n    item = next(self.iterator)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\r\n    processed = self.infer(item, **self.params)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1149, in forward\r\n    model_outputs = self._forward(model_inputs, **forward_params)\r\n  File \"/home/jon/h2ogpt/src/h2oai_pipeline.py\", line 271, in _forward\r\n    return self.__forward(model_inputs, **generate_kwargs)\r\n  File \"/home/jon/h2ogpt/src/h2oai_pipeline.py\", line 309, in __forward\r\n    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1576, in generate\r\n    result = self._greedy_search(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2494, in _greedy_search\r\n    outputs = self(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1158, in forward\r\n    outputs = self.model(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/model.py\", line 119, in forward\r\n    h, _, past_key_value = layer(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/block.py\", line 113, in forward\r\n    attn_output, _, past_key_value = self.attn.forward(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 210, in forward\r\n    xq, xk = self.rope.forward(xq, xk, self.start_pos, seqlen)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 62, in forward\r\n    freqs_cis = self.reshape_for_broadcast(freqs_cis, xq_).to(xq_.device)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/awq/modules/fused/attn.py\", line 50, in reshape_for_broadcast\r\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\r\nAssertionError\r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "This does the same thing:\r\n```\r\npython generate.py \\\r\n--enable_heap_analytics=False \\\r\n--document_choice_in_sidebar=True \\\r\n--actions_in_sidebar=True \\\r\n--openai_server=False \\\r\n--use_gpu_id=False \\\r\n--score_model=None \\\r\n--prompt_type=open_chat \\\r\n--base_model=TheBloke/openchat_3.5-16k-AWQ \\\r\n--attention_sinks=True \\\r\n--sink_dict=\"{'num_sink_tokens': 4, 'window_length': 16384 }\" \\\r\n--use_llm_if_no_docs=True \\\r\n--max_seq_len=16384 \\\r\n--enable_ocr=True\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "As does this:\r\n```\r\npython generate.py \\\r\n--enable_heap_analytics=False \\\r\n--document_choice_in_sidebar=True \\\r\n--actions_in_sidebar=True \\\r\n--openai_server=False \\\r\n--use_gpu_id=False \\\r\n--score_model=None \\\r\n--prompt_type=open_chat \\\r\n--base_model=TheBloke/openchat_3.5-16k-AWQ \\\r\n--use_llm_if_no_docs=True \\\r\n--max_seq_len=16384 \\\r\n--enable_ocr=True\r\n```\r\n\r\nSo it seems to be a pure awq issue.\r\n\r\nThe latest 0.2.5 does the same thing.  Reducing to (say) 15000 does same thing."
      },
      {
        "user": "pseudotensor",
        "body": "A small script does the same thing, so it's not related to h2oGPT itself."
      },
      {
        "user": "pseudotensor",
        "body": "https://github.com/casper-hansen/AutoAWQ/issues/472"
      },
      {
        "user": "Blacksuan19",
        "body": "I'm getting a similar error with LLAMA-3 GGUF as well (same model mentioned in the FAQ), it only includes the evaluate_nochat exception from the log above.\r\n\r\n<details>\r\n<summary>error running LLAMA-3</summary>\r\n\r\n```python\r\nevaluate_nochat exception: : ('', '', '', True, 'unknown', \"{   'PreInput': None,\\n    'PreInstruct': None,\\n    'PreResponse': None,\\n    'botstr': None,\\n    'can_handle_system_prompt': False,\\n    'chat_sep': '\\\\n',\\n    'chat_turn_\r\nsep': '\\\\n',\\n    'generates_leading_space': False,\\n    'humanstr': None,\\n    'promptA': None,\\n    'promptB': None,\\n    'system_prompt': '',\\n    'terminate_response': []}\", 0, 1, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, Tr\r\nue, '', '', 'LLM', True, 'Query', None, 10, True, 512, 'Relevant', ['All'], None, None, None, None, 'Pay attention and remember the information below, which will help to answer the question or imperative after the context ends.', 'Acco\r\nrding to only the information in the document sources provided within the context above, write an insightful and well-structured response to: ', 'In order to write a concise single-paragraph or bulleted list summary, pay attention to t\r\nhe following text.', 'Using only the information in the document sources above, write a condensed and concise summary of key results (preferably as about 10 bullet points).', 'Answer this question with vibrant details in order for some\r\n NLP embedding model to use that answer as better query than original question: ', 'Who are you and what do you do?', 'Ensure your entire response is outputted as a single piece of strict valid JSON text.', 'Ensure your response is str\r\nictly valid JSON text.', 'Ensure your entire response is outputted as strict valid JSON text inside a Markdown code block with the json language identifier.   Ensure all JSON keys are less than 64 characters, and ensure JSON key names are made of only alphanumerics, underscores, or hyphens.', 'Ensure you follow this JSON schema:\\n```json\\n{properties_schema}\\n```', 'auto', ['DocTR', 'Caption', 'ASR'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', [], None, '', False, '[]', '[]', 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, 'auto', False, False, '[]', 'None', None, None, 1, None, None, 'text', '', '', '', '', {'model': 'model', 'tokenizer': 'tokenizer', 'device': 'cpu', 'base_model': 'llama', 'tokenizer_base_model': '', 'lora_weights': '[]', 'inference_server': '[]', 'prompt_type': 'unknown', 'prompt_dict': {'promptA': None, 'promptB': None, 'PreInstruct': None, 'PreInput': None, 'PreResponse': None, 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': False}, 'visible_models': 0, 'h2ogpt_key': None}, {'MyData': [None, '6164574a-865b-4659-8fda-d35faa6a1d09', 'test']}, {'langchain_modes': ['Disabled', 'LLM', 'MyData', 'UserData'], 'langchain_mode_paths': {'UserData': None}, 'langchain_mode_types': {'UserData': 'shared', 'github h2oGPT': 'shared', 'DriverlessAI docs': 'shared', 'wiki': 'shared', 'wiki_full': '', 'MyData': 'personal', 'LLM': 'personal', 'Disabled': 'personal'}}, {'headers': '', 'host': '0.0.0.0:7850', 'username': 'test', 'connection': 'keep-alive', 'content-length': '117', 'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0', 'dnt': '1', 'content-type': 'application/json', 'accept': '*/*', 'origin': 'http://0.0.0.0:7850', 'referer': 'http://0.0.0.0:7850/', 'accept-encoding': 'gzip, deflate', 'accept-language': 'en-US,en;q=0.9,ar;q=0.8', 'cookie': 'access-token-unsecure-hhN8py5JLVRfL-0OTPND8TGcb3qhs2GvSJQ8qV1LI50=vrLRNuXKqoKCZDSCqo1OHg; access-token-unsecure-s-dRx26Pws-xf2TfvaYIjqwWsGjiH9960S06PrlT6tg=AnrezJi1hR1NjfFx29n_bg; access-token-unsecure-SF0CZ7POfi6Imk0jDfN44qO9W9VB0hu3nUcGevVPMYw=SU1SQYZL79hpAN43hEDgIQ; access-token-unsecure-9LIDZewsE4If1yY7ixHa-yOZJO20M-PQVSDjJtfYQYA=o8YMAhHGtoLQDjMVZVITsQ; access-token-unsecure-qS0zsQdPdQYJsrMX4RXh3HQwEDeknaNz0RppngdPvGY=AGmuVQm8_KVKkMg8HdQtqg; access-token-unsecure--qfFGcbj-JQc0O0MamjIfNGlfgUrb6t7xyB3hRUL1I8=NVbKjP5O7Q3xJxHYvaiUfw; access-token-unsecure-YeY4iDfE2-hlA1izGtL7vBNbLbCosRLpSAJFo-j6_e0=xkWJTIiCTZGbhG1H60OTBg; access-token-unsecure-BwVTmtTwIzOYqtTpvsZkHQvnjr8N60WJaX_V6njwUAw=8uPW51j557W7S8ZO_e5iSQ; access-token-unsecure-JSzZdmZ5Fn4S9ekIB_5lXnXTrnwvQu1X7IyivtmRjuk=mm2CzLGIw9b3H9xFfS1KpQ; access-token-unsecure-IxTC1FBXOKLvW0SXsNRzMYxrHxvTPTIwwB4y69dHG9A=fYRfZinU_x99RK1k11fvIA; access-token-unsecure-eJjGwBq3ju0P30aflFl-P8uUU2QqEgAIsxgw-FsdJgU=fDM1Je-16fNS5ndkdNRL6g; access-token-unsecure-uiZ2ybGZZJgrCTxTV22rFbgZVNssg73T8oL2xiUqp1I=VrTDb6L_l-gQy27srKSq0Q; access-token-unsecure-U2bTOnaNSeVnj1LFMlrf1Mtm6lWYkZALH_MqD44PYNU=lt481IiB7f9YGqWkkOinIA; access-token-unsecure-RxkNbfFX2paq-I07CliUNsj55vZP1qWOWwp2u-TDNCc=mmNdzgtQmfvlDlQe6i4PvA; access-token-unsecure-zfJ9zz3Jn9sTfeKgGIdQFP7gIY4kjYQ8rW5jEkOaylQ=IsBrbZibgh9mmFvTDmEOAg; access-token-unsecure-L9Xt7VY0kFHSJqZAK8-I90p4rp6XNVtxCYJp3nAmxfs=LZu9e7Ggvl2vYBE1AuZbLQ', 'host2': '14.1.246.124', 'picture': 'None'}, {}, [['hello', '']])\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/queueing.py\", line 566, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/route_utils.py\", line 261, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/blocks.py\", line 1788, in process_api\r\n    result = await self.call_function(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/blocks.py\", line 1352, in call_function\r\n    prediction = await utils.async_iteration(iterator)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/utils.py\", line 595, in async_iteration\r\n    return await iterator.__anext__()\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/utils.py\", line 588, in __anext__\r\n    return await anyio.to_thread.run_sync(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\r\n    result = context.run(func, *args)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/utils.py\", line 571, in run_sync_iterator_async\r\n    return next(iterator)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/utils.py\", line 754, in gen_wrapper\r\n    response = next(iterator)\r\n  File \"/workspace/src/gradio_runner.py\", line 5053, in bot\r\n    for res in get_response(fun1, history, chatbot_role1, speaker1, tts_language1, roles_state1,\r\n  File \"/workspace/src/gradio_runner.py\", line 4948, in get_response\r\n    for output_fun in fun1():\r\n  File \"/workspace/src/gen.py\", line 4402, in evaluate\r\n    prompt_basic = prompter.generate_prompt(data_point, context_from_history=False)\r\n  File \"/workspace/src/prompter.py\", line 1729, in generate_prompt\r\n    assert self.use_chat_template\r\nAssertionError\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>run command</summary>\r\n\r\n```bash\r\nexport IMAGE_TAG=4059a2c9\r\nexport HF_TOKEN=hf_xxx\r\n\r\ndocker run \\\r\n --init \\\r\n--gpus all \\\r\n--runtime=nvidia \\\r\n--shm-size=2g \\\r\n-p 7850:7860 \\\r\n-v /etc/passwd:/etc/passwd:ro \\\r\n-v /etc/group:/etc/group:ro \\\r\n-u $(id -u):$(id -g) \\\r\ngcr.io/vorvan/h2oai/h2ogpt-runtime:$IMAGE_TAG /workspace/generate.py \\\r\n--openai_server=False \\\r\n--auth=\"/workspace/auth/users.json\" \\\r\n--h2ogpt_api_keys=\"/workspace/auth/api_keys.json\" \\\r\n--use_gpu_id=False \\\r\n--score_model=None \\\r\n--base_model=llama \\\r\n--model_path_llama=https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf?download=true\r\n--tokenizer_base_model=meta-llama/Meta-Llama-3-8B-Instruct \\\r\n--save_dir='/workspace/save/' \\\r\n--user_path='/workspace/user_path/' \\\r\n--langchain_mode=\"UserData\" \\\r\n--langchain_modes=\"['UserData', 'LLM']\" \\\r\n--visible_langchain_actions=\"['Query']\" \\\r\n--visible_langchain_agents=\"[]\" \\\r\n--use_llm_if_no_docs=True \\\r\n--enable_ocr=True \\\r\n--enable_tts=False \\\r\n--enable_stt=False\r\n\r\n```\r\n</details>"
      },
      {
        "user": "pseudotensor",
        "body": "It should fail and make you pass:\r\n```\r\n--max_seq_len=8192\r\n```\r\nas well.\r\n\r\ne.g.\r\n```\r\npython generate.py --openai_server=False --score_model=None --base_model=llama --model_path_llama=https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf?download=true --tokenizer_base_model=meta-llama/Meta-Llama-3-8B-Instruct --use_llm_if_no_docs=True --max_seq_len=8192\r\n```\r\ngives:\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/1f954ef9-8f16-4b61-b0d8-be6c82efc194)\r\n\r\n\r\nI don't see the error you see.  And when I debug the code with the above command on latest h2oGPT, I see that chat_template is True.\r\n\r\nPerhaps you are using older docker image or older h2oGPT or something?"
      },
      {
        "user": "Blacksuan19",
        "body": "there was a missing slash in the command after `--model_path_llama`, fixed now, however I'm still unable to run LLAMA-3 with the docker image due to it being a gated model, I tried passing `--use_auth_token=hf_xxx` and setting environment variables `HUGGING_FACE_HUB_TOKEN` and `HF_TOKEN` and `HUGGINGFACE_TOKEN` but still cannot access.\r\n\r\nthis occurs on both latest and previous docker images with tags `c25144e9` and `4059a2c9`"
      },
      {
        "user": "pseudotensor",
        "body": "Can you share a stack trace of where it's failing?"
      },
      {
        "user": "Blacksuan19",
        "body": "here is the full stack trace, I'm authenticated on the huggingface-cli, have exported `HUGGING_FACE_HUB_TOKEN` in the environment and passing `--use_auth_token` to `docker run`\r\n\r\nthe error is not raised while starting the container, only after sending a query is the error raised.\r\n\r\n<details>\r\n\r\n\r\n```python\r\nevaluate_nochat exception: You are trying to access a gated repo.\r\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\r\n401 Client Error. (Request ID: Root=1-6639f802-164d6a07579d391a189244e2;12303a0c-d620-412a-b17f-58372bd127d5)\r\n\r\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/generation_config.json.\r\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.: ('', '', '', True, 'unknown', \"{   'PreInput': None,\\n    'PreInstruct': None,\\n    'PreResponse': None,\\n    'botstr': None,\\n\r\n    'can_handle_system_prompt': False,\\n    'chat_sep': '\\\\n',\\n    'chat_turn_sep': '\\\\n',\\n    'generates_leading_space': False,\\n    'humanstr': None,\\n    'promptA': None,\\n    'promptB': None,\\n    'system_prompt': '',\\n    'termi\r\nnate_response': []}\", 0, 1, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'UserData', True, 'Query', None, 10, True, 512, 'Relevant', ['/workspace/user_path/9b999f43-2ade-4148-97cf-d2448125168c/res/b1a173b2_user_upload\r\n_Abubakar-Yusif-March-2024-Progress-Report.pdf'], None, None, None, None, 'Pay attention and remember the information below, which will help to answer the question or imperative after the context ends.', 'According to only the informat\r\nion in the document sources provided within the context above, write an insightful and well-structured response to: ', 'In order to write a concise single-paragraph or bulleted list summary, pay attention to the following text.', 'Usin\r\ng only the information in the document sources above, write a condensed and concise summary of key results (preferably as about 10 bullet points).', 'Answer this question with vibrant details in order for some NLP embedding model to us\r\ne that answer as better query than original question: ', 'Who are you and what do you do?', 'Ensure your entire response is outputted as a single piece of strict valid JSON text.', 'Ensure your response is strictly valid JSON text.', '\r\nEnsure your entire response is outputted as strict valid JSON text inside a Markdown code block with the json language identifier.   Ensure all JSON keys are less than 64 characters, and ensure JSON key names are made of only alphanume\r\nrics, underscores, or hyphens.', 'Ensure you follow this JSON schema:\\n```json\\n{properties_schema}\\n```', 'auto', ['OCR', 'DocTR', 'Caption', 'ASR'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', [], None, '', False, '[]', '[]', 'be\r\nst_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, 'auto', False, False, '[]', 'None', None, None, 1, None, None, 'text', '', '', '', '', {'model': 'model', 'tokenizer': 'tokenizer', 'device': 'cpu', 'base_model': 'llama', 'tok\r\nenizer_base_model': 'meta-llama/Meta-Llama-3-8B-Instruct', 'lora_weights': '[]', 'inference_server': '[]', 'prompt_type': 'unknown', 'prompt_dict': {'promptA': None, 'promptB': None, 'PreInstruct': None, 'PreInput': None, 'PreResponse'\r\n: None, 'terminate_response': [], 'chat_sep': '\\n', 'chat_turn_sep': '\\n', 'humanstr': None, 'botstr': None, 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': False}, 'visible_models': 0, 'h2ogpt_key':\r\nNone}, {'MyData': [None, '2db742f5-7880-4708-bfcc-061f995f6c51', 'test']}, {'langchain_modes': ['Disabled', 'LLM', 'UserData'], 'langchain_mode_paths': {'UserData': '/workspace/user_path/'}, 'langchain_mode_types': {'UserData': 'shared\r\n', 'github h2oGPT': 'shared', 'DriverlessAI docs': 'shared', 'wiki': 'shared', 'wiki_full': '', 'LLM': 'personal', 'Disabled': 'personal'}}, {'headers': '', 'host': '0.0.0.0:7850', 'username': 'test', 'connection': 'keep-alive', 'c\r\nontent-length': '155', 'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0', 'dnt': '1', 'content-type': 'application/json', 'accept': '*/*', 'origin': 'htt\r\np://0.0.0.0:7850', 'referer': 'http://0.0.0.0:7850/', 'accept-encoding': 'gzip, deflate', 'accept-language': 'en-US,en;q=0.9,ar;q=0.8', 'cookie': 'access-token-unsecure-hhN8py5JLVRfL-0OTPND8TGcb3qhs2GvSJQ8qV1LI50=vrLRNuXKqoKCZD\r\nSCqo1OHg; access-token-unsecure-s-dRx26Pws-xf2TfvaYIjqwWsGjiH9960S06PrlT6tg=AnrezJi1hR1NjfFx29n_bg; access-token-unsecure-SF0CZ7POfi6Imk0jDfN44qO9W9VB0hu3nUcGevVPMYw=SU1SQYZL79hpAN43hEDgIQ; access-token-unsecure-9LIDZewsE4If1yY7ixHa-yO\r\nZJO20M-PQVSDjJtfYQYA=o8YMAhHGtoLQDjMVZVITsQ; access-token-unsecure-qS0zsQdPdQYJsrMX4RXh3HQwEDeknaNz0RppngdPvGY=AGmuVQm8_KVKkMg8HdQtqg; access-token-unsecure--qfFGcbj-JQc0O0MamjIfNGlfgUrb6t7xyB3hRUL1I8=NVbKjP5O7Q3xJxHYvaiUfw; access-tok\r\nen-unsecure-YeY4iDfE2-hlA1izGtL7vBNbLbCosRLpSAJFo-j6_e0=xkWJTIiCTZGbhG1H60OTBg; access-token-unsecure-BwVTmtTwIzOYqtTpvsZkHQvnjr8N60WJaX_V6njwUAw=8uPW51j557W7S8ZO_e5iSQ; access-token-unsecure-JSzZdmZ5Fn4S9ekIB_5lXnXTrnwvQu1X7IyivtmRjuk\r\n=mm2CzLGIw9b3H9xFfS1KpQ; access-token-unsecure-IxTC1FBXOKLvW0SXsNRzMYxrHxvTPTIwwB4y69dHG9A=fYRfZinU_x99RK1k11fvIA; access-token-unsecure-eJjGwBq3ju0P30aflFl-P8uUU2QqEgAIsxgw-FsdJgU=fDM1Je-16fNS5ndkdNRL6g; access-token-unsecure-uiZ2ybGZ\r\nZJgrCTxTV22rFbgZVNssg73T8oL2xiUqp1I=VrTDb6L_l-gQy27srKSq0Q; access-token-unsecure-U2bTOnaNSeVnj1LFMlrf1Mtm6lWYkZALH_MqD44PYNU=lt481IiB7f9YGqWkkOinIA; access-token-unsecure-RxkNbfFX2paq-I07CliUNsj55vZP1qWOWwp2u-TDNCc=mmNdzgtQmfvlDlQe6i4\r\nPvA; access-token-unsecure-zfJ9zz3Jn9sTfeKgGIdQFP7gIY4kjYQ8rW5jEkOaylQ=IsBrbZibgh9mmFvTDmEOAg; access-token-unsecure-L9Xt7VY0kFHSJqZAK8-I90p4rp6XNVtxCYJp3nAmxfs=LZu9e7Ggvl2vYBE1AuZbLQ; access-token-unsecure-Q1siuNCSLCiNRxIYSoa5j3shaosW\r\nMURmotg6HLCC7_U=GnFkZ58wi1tNt8jvtK_UXw; access-token-unsecure-18dc1X-LAJmB7lwYCfNUZLG0jDJ14hzU62wssf0wc68=sl8WTC1xPw4lEirkcWQTwg; access-token-unsecure-EjGzYB6qR9D1LPBN82KeJcwOVChluLaElinAd1OgeSk=jzCYJhyTI3Aka1vIsd-b8g; access-token-un\r\nsecure-X7gaKAyAK8MUU18su8lDmyVdbuDVILxO9xngYdFnvgo=1JT2Rzr5QHd5rLGbEgqASg; access-token-unsecure-SBEZ_P_Ugyx9zNJcGUYmCbOTE7jSx0BgJFTIyH3OkSg=_X_Xbu1W23MfAgZgdLlWZg; access-token-unsecure-wxxoptBvH1YNkOHaad2CST68Cg5SrC23tRQMsGy2TOc=Vl3F\r\nHsOce3Q6zMWHJq79tA; access-token-unsecure-6WqVOYaEXoF6RktDUq-aghEiDdfSuCY2tkPQHCqGjck=N4QVkzJs0_AgRFQRdx5Y-Q; access-token-unsecure-PX6qZPvMN8mpgv3RwObslqNOspATAkytwLj-5mrO12Q=CxUwwnlESuufP4AeTnOh3Q; access-token-unsecure-a8SnDigFzjafo\r\nM84LMb3tCUsrk44E9VoXdpmv36kpNo=DjNp22jvVqPfWo9k6fWxUg; access-token-unsecure-xy6fU_AFL6Tp1nllr3w_QAOlrvJRFgRn0bTPJnCrTAo=Nqt1rhRGgIiZD_m8zaERrg; access-token-unsecure-9JU3qRVH6hK4ZC54rM0J035t62b8-m26tT3T5I-wEWA=ESa5PlhZ1RUHvb1Klzxujg',\r\n 'host2': '14.1.246.126', 'picture': 'None'}, {}, [['summarize the given data', '']])\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\r\n    response.raise_for_status()\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/generation_config.json\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\r\n    resolved_file = hf_hub_download(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\r\n    return _hf_hub_download_to_cache_dir(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1325, in _hf_hub_download_to_cache_dir\r\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1823, in _raise_on_head_call_error\r\n    raise head_call_error\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1722, in _get_metadata_or_catch_error\r\n    metadata = get_hf_file_metadata(url=url, proxies=proxies, timeout=etag_timeout, headers=headers)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1645, in get_hf_file_metadata\r\n    r = _request_wrapper(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 372, in _request_wrapper\r\n    response = _request_wrapper(\r\nFile \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 396, in _request_wrapper\r\n    hf_raise_for_status(response)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 321, in hf_raise_for_status\r\n    raise GatedRepoError(message, response) from e\r\nhuggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-6639f802-164d6a07579d391a189244e2;12303a0c-d620-412a-b17f-58372bd127d5)\r\n\r\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/generation_config.json.\r\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/queueing.py\", line 566, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/route_utils.py\", line 261, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/blocks.py\", line 1788, in process_api\r\n    result = await self.call_function(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/blocks.py\", line 1352, in call_function\r\n    prediction = await utils.async_iteration(iterator)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/utils.py\", line 595, in async_iteration\r\n    return await iterator.__anext__()\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/utils.py\", line 588, in __anext__\r\n    return await anyio.to_thread.run_sync(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\r\n    result = context.run(func, *args)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/utils.py\", line 571, in run_sync_iterator_async\r\n    return next(iterator)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/utils.py\", line 754, in gen_wrapper\r\n    response = next(iterator)\r\n  File \"/workspace/src/gradio_runner.py\", line 5053, in bot\r\n    for res in get_response(fun1, history, chatbot_role1, speaker1, tts_language1, roles_state1,\r\n  File \"/workspace/src/gradio_runner.py\", line 4948, in get_response\r\n    for output_fun in fun1():\r\n  File \"/workspace/src/gen.py\", line 4278, in evaluate\r\n    prompter = Prompter(prompt_type, prompt_dict, debug=debug, stream_output=stream_output,\r\n  File \"/workspace/src/prompter.py\", line 1706, in __init__\r\n    self.terminate_response = update_terminate_responses(self.terminate_response,\r\n  File \"/workspace/src/stopping.py\", line 25, in update_terminate_responses\r\n    generate_eos_token_id = GenerationConfig.from_pretrained(tokenizer.name_or_path).eos_token_id\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 843, in from_pretrained\r\n    resolved_config_file = cached_file(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 416, in cached_file\r\n    raise EnvironmentError(\r\nOSError: You are trying to access a gated repo.\r\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\r\n401 Client Error. (Request ID: Root=1-6639f802-164d6a07579d391a189244e2;12303a0c-d620-412a-b17f-58372bd127d5)\r\n\r\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/generation_config.json.\r\nAccess to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must be authenticated to access it.\r\n```\r\n\r\n</details>"
      },
      {
        "user": "pseudotensor",
        "body": "I see.  But if you pass the env `HUGGING_FACE_HUB_TOKEN` through it should still work here.\r\n\r\ne.g. the docker line would add:\r\n```\r\n-e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "But, I made some changes for that particular piece of code."
      },
      {
        "user": "Blacksuan19",
        "body": "I can confirm passing the environment variable to the docker image with `-e` or passing `--use_auth_token` also works on the latest image."
      }
    ]
  },
  {
    "issue_number": 1904,
    "title": "RAG ",
    "author": "CommitAndPray",
    "state": "open",
    "created_at": "2024-11-27T16:04:01Z",
    "updated_at": "2024-11-27T16:04:01Z",
    "labels": [],
    "body": "Hello, could someone please guide me to where I can find the exact implementation of the RAG part?",
    "comments": []
  },
  {
    "issue_number": 1903,
    "title": "Multiple collections simultaneously",
    "author": "oumaymajr",
    "state": "open",
    "created_at": "2024-11-26T10:27:24Z",
    "updated_at": "2024-11-26T10:27:24Z",
    "labels": [],
    "body": "Hello , i was looking if there are any update about this suggestion ? i need to test if it works with multiple collections simultaneously .",
    "comments": []
  },
  {
    "issue_number": 1867,
    "title": "Newly created Collection available for all the users when authentication is enabled",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-10-03T11:58:52Z",
    "updated_at": "2024-11-16T03:27:35Z",
    "labels": [],
    "body": "When authentication is enabled, I want to create a shared collection, such as `UserData`, that remains accessible to all users at all times, regardless of who created it.\r\n\r\nI encountered an issue when attempting to create a new collection (`UserData2`). Here’s the process I followed:\r\n\r\n1. I first created the collection using the UI.\r\n2. I then ran the following command:\r\n\r\n`python generate.py --base_model=meta-llama/Llama-2-13b-chat-hf --score_model=None --langchain_modes=\"['UserData',UserData2,LLM,MyData]\" --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048 --max_new_tokens=2048 --min_new_tokens=128 --prompt_type=llama2 --enable_stt=False --enable_tts=False --auth_filename=$auth_filename --auth_access=open --guest_name=avser --auth=\"[(admin, admin)]\"`\r\n\r\nThe issue is that the newly created collection (`UserData2`) is only accessible to the user who created it, similar to how `MyData` functions. However, I would like this collection to behave like `UserData`, where it is available to all users globally, even when authentication is enabled.\r\n\r\nHowever When authentication is disabled during collection creation, the collection is accessible to all users as expected.\r\n\r\nso Could you please provide guidance on how to create a shared collection like `UserData` that remains accessible to all users, even when authentication is enabled?\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "How did you create it in the UI?  The box takes a few args, including the collection type (assumed to be personal if no passed)."
      },
      {
        "user": "llmwesee",
        "body": "Document Selection >> Add Collection then type `UserData2, shared, userpath`  then put `UserData2` in the\r\n `--langchain_modes=\"['UserData',UserData2,LLM,MyData]\"`"
      },
      {
        "user": "llmwesee",
        "body": "However i also created the collection with **src/make_db.py:**  by adding all the files in the folder **user_path3** then \r\n`python src/make_db.py --user_path=user_path3 --collection_name=UserData3 --langchain_type=shared ` \r\n\r\n`python generate.py --base_model=meta-llama/Llama-2-13b-chat-hf --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048 --batch_size=16 --prompt_type=llama2 --langchain_modes=['UserData','UserData3','MyData','LLM'] --auth_filename=$auth_filename --auth_access=open --guest_name=avser --auth=\"[(admin, admin)]\"\r\n`\r\n\r\nthen still didn't showing UserData3 in the collections for all the users although the embedding are stored in **db_dir_UserData3** folder\r\n\r\nAnd when adding `--langchain_modes=['UserData','UserData3'] --langchain_mode_paths={'UserData':'user_path','UserData3':'user_path3'} --langchain_mode_types={'UserData':'shared','UserData3':'shared'}` \r\n\r\nto the command like:\r\n\r\n`python generate.py --base_model=meta-llama/Llama-2-13b-chat-hf --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048 --batch_size=16 --prompt_type=llama2 --langchain_modes=['UserData','UserData3'] --langchain_mode_paths={'UserData':'user_path','UserData3':'user_path3'} --langchain_mode_types={'UserData':'shared','UserData3':'shared'} --auth_filename=$auth_filename --auth_access=open --guest_name=avser --auth=\"[(admin, admin)]\"`\r\n\r\nthen it showing the following error:\r\n\r\n`File \"/home/xxxx/src/gen.py\", line 1383, in main\r\n    langchain_mode_paths = str_to_dict(langchain_mode_paths)\r\n  File \"/home/xxxx/src/utils.py\", line 1863, in str_to_dict\r\n    raise ValueError(\"Invalid str_to_dict for %s\" % x)\r\nValueError: Invalid str_to_dict for UserData3:user_path3`\r\n\r\nNote:  I created the authentication server through the **LDAP**\r\n@pseudotensor please help me regarding this!"
      },
      {
        "user": "hartysoly",
        "body": "This worked well for me hopefully that helps, i am a beginner so good luck :)\r\n\r\npython generate.py --base_model=XYZ-brrrr --prompt_type=brrrr--max_max_new_tokens=2048\t--max_new_tokens=1024 --max_seq_len=8094 --max_quality=True --langchain_modes=\"['UserData','brrrData']\" --user_path=user_path --langchain_mode_types=\"{'UserData':'shared','brrrData':'shared'}\" --function_server=True --function_server_workers=10 --multiple_workers_gunicorn=True --async_output=True --num_async=10 --auth_access=closed --admin_pass=meadmin --auth=\"[('brrryaa','winkwink')]\" --auth_freeze\r\n"
      }
    ]
  },
  {
    "issue_number": 1896,
    "title": "taking long time to give response (around 2 min)",
    "author": "mbbutt",
    "state": "open",
    "created_at": "2024-11-07T12:31:59Z",
    "updated_at": "2024-11-12T21:48:14Z",
    "labels": [],
    "body": "Hello\r\n\r\nI am running in the following machine.\r\n\r\nCPU: 12th Gen Intel(R) Core(TM) i7-12700\r\nRAM: 32GB, speed: 4400MT/s\r\nNVIDIA RTX A2000 12GB\r\n\r\nmodel is:\r\nllama-2-7b-chat.Q6_K.gguf\r\n\r\nAnd it takes around 2 min to start giving a response.\r\nis it reasonable or it should be faster?\r\n\r\n\r\nbat command to start the bot\r\n\r\n```\r\n\"C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\Scripts\\python.exe\"^\r\n \"generate.py\"^\r\n --share=False ^\r\n --auth=[('jon','password')] ^\r\n --auth_access=closed ^\r\n --gradio_offline_level=1 ^\r\n --base_model=\"llama\" ^\r\n --prompt_type=llama2 ^\r\n --model_path_llama=C:\\Users\\Public\\git\\h2ogpt\\llama-2-7b-chat.Q6_K.gguf^\r\n --score_model=None ^\r\n --langchain_mode=\"LLLM\" ^\r\n --user_path=user_path ^\r\n --load_4bit=True ^\r\n --llamacpp_dict=\"{'n_gpu_layers':5}\"\r\n\r\n```\r\n\r\nWhile running idle \r\nit is taking 7GB GPU memory (remains same when running the query)\r\n24.4GB RAM (remains same when running the query)\r\nCPU utilization stays 2 to 3%\r\n\r\n\r\nWhen running the query CPU utilization goes closer to 100%\r\nGPU remains 1% to 2%\r\n\r\nand it takes around 2 min to start giving a response.\r\n\r\nIt seems it is not utilizing GPU at all.\r\ncould you please see what i am doing wrong here?\r\nI want to get faster response \r\n\r\ncuda version is\r\n\r\n```\r\nC:\\Windows\\System32>nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2022 NVIDIA Corporation\r\nBuilt on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\r\nCuda compilation tools, release 11.8, V11.8.89\r\nBuild cuda_11.8.r11.8/compiler.31833905_0\r\n```\r\n\r\n\r\n\r\n\r\nbelow is my pip list\r\n\r\n```\r\nPackage                                  Version\r\n---------------------------------------- ---------------\r\nabsl-py                                  2.1.0\r\naccelerate                               0.32.1\r\naiofiles                                 23.2.1\r\naiohappyeyeballs                         2.4.3\r\naiohttp                                  3.10.9\r\naiosignal                                1.3.1\r\naltair                                   5.4.1\r\nannotated-types                          0.7.0\r\nanthropic                                0.8.1\r\nantlr4-python3-runtime                   4.9.3\r\nanyio                                    4.6.0\r\nappdirs                                  1.4.4\r\nAPScheduler                              3.10.4\r\nargcomplete                              3.5.1\r\narxiv                                    1.4.8\r\nasgiref                                  3.8.1\r\nasync-timeout                            4.0.3\r\nattributedict                            0.3.0\r\nattrs                                    24.2.0\r\naudioread                                3.0.1\r\nAuthlib                                  1.3.1\r\nauto-gptq                                0.6.0\r\nautoawq                                  0.1.8+cu118\r\nautoawq_kernels                          0.0.3+cu118\r\nbabel                                    2.16.0\r\nbackoff                                  2.2.1\r\nbackports.tarfile                        1.2.0\r\nbcrypt                                   4.2.0\r\nbeautifulsoup4                           4.12.3\r\nbioc                                     2.1\r\nbitsandbytes                             0.41.1\r\nblessings                                1.7\r\nboto3                                    1.35.35\r\nbotocore                                 1.35.35\r\nBrotli                                   1.1.0\r\nbs4                                      0.0.2\r\nbuild                                    1.2.2.post1\r\ncachetools                               5.5.0\r\ncertifi                                  2024.8.30\r\ncffi                                     1.17.1\r\nchardet                                  5.2.0\r\ncharset-normalizer                       3.3.2\r\nchroma-bullet                            2.2.0\r\nchroma-hnswlib                           0.7.3\r\nchroma-migrate                           0.0.7\r\nchromadb                                 0.4.23\r\nchromamigdb                              0.3.26\r\nclick                                    8.1.7\r\nclickhouse-connect                       0.6.6\r\ncodecov                                  2.1.13\r\ncolorama                                 0.4.6\r\ncoloredlogs                              15.0.1\r\ncolour-runner                            0.1.1\r\ncontourpy                                1.3.0\r\ncoverage                                 7.6.1\r\ncryptography                             43.0.1\r\ncssselect2                               0.7.0\r\ncutlet                                   0.3.0\r\ncycler                                   0.12.1\r\ndacite                                   1.7.0\r\ndataclasses-json                         0.6.7\r\nDataProperty                             1.0.1\r\ndatasets                                 2.16.1\r\ndateparser                               1.1.8\r\ndecorator                                5.1.1\r\ndeepdiff                                 8.0.1\r\ndefusedxml                               0.7.1\r\nDeprecated                               1.2.14\r\ndiffusers                                0.24.0\r\ndill                                     0.3.7\r\ndiskcache                                5.6.3\r\ndistlib                                  0.3.8\r\ndistro                                   1.9.0\r\ndnspython                                2.7.0\r\ndocopt                                   0.6.2\r\ndocutils                                 0.20.1\r\nduckdb                                   0.7.1\r\nduckduckgo_search                        6.3.0\r\ndurationpy                               0.9\r\neffdet                                   0.4.1\r\neinops                                   0.8.0\r\nemoji                                    2.14.0\r\net-xmlfile                               1.1.0\r\neval_type_backport                       0.2.0\r\nevaluate                                 0.4.0\r\nexceptiongroup                           1.2.2\r\nexecnet                                  2.1.1\r\nexllama                                  0.0.18+cu118\r\nfastapi                                  0.115.0\r\nfeedparser                               6.0.11\r\nffmpeg                                   1.4\r\nffmpy                                    0.4.0\r\nfiftyone                                 1.0.0\r\nfiftyone-brain                           0.17.0\r\nfiftyone_db                              1.1.6\r\nfilelock                                 3.16.1\r\nfiletype                                 1.2.0\r\nfire                                     0.5.0\r\nflatbuffers                              24.3.25\r\nfonttools                                4.54.1\r\nfrozenlist                               1.4.1\r\nfsspec                                   2023.10.0\r\nftfy                                     6.2.3\r\nfugashi                                  1.3.2\r\nfuture                                   1.0.0\r\ng2pkk                                    0.1.2\r\ngekko                                    1.2.1\r\nglob2                                    0.7\r\ngoogle-ai-generativelanguage             0.4.0\r\ngoogle-api-core                          2.20.0\r\ngoogle-auth                              2.35.0\r\ngoogle-generativeai                      0.3.2\r\ngoogle_search_results                    2.4.2\r\ngoogleapis-common-protos                 1.65.0\r\ngpt4all                                  1.0.5\r\ngradio                                   3.50.2\r\ngradio_client                            0.6.1\r\ngradio_pdf                               0.0.15\r\ngradio_tools                             0.0.9\r\ngraphql-core                             3.2.4\r\ngreenlet                                 3.0.3\r\ngrpcio                                   1.66.2\r\ngrpcio-health-checking                   1.62.3\r\ngrpcio-status                            1.62.3\r\ngrpcio-tools                             1.62.3\r\ngruut                                    2.2.3\r\ngruut-ipa                                0.13.0\r\ngruut-lang-de                            2.0.1\r\ngruut-lang-en                            2.0.1\r\ngruut-lang-es                            2.0.1\r\ngruut_lang_fr                            2.0.2\r\nh11                                      0.14.0\r\nh2                                       4.1.0\r\nh5py                                     3.12.1\r\nhf_transfer                              0.1.8\r\nhnswlib                                  0.8.0\r\nhnswmiglib                               0.7.0\r\nhpack                                    4.0.0\r\nhtml2text                                2024.2.26\r\nhtml5lib                                 1.1\r\nhttpcore                                 1.0.6\r\nhttptools                                0.6.1\r\nhttpx                                    0.27.0\r\nhuggingface-hub                          0.25.1\r\nhumanfriendly                            10.0\r\nhumanize                                 4.11.0\r\nHypercorn                                0.17.3\r\nhyperframe                               6.0.1\r\nidna                                     3.10\r\nimageio                                  2.35.1\r\nimportlib_metadata                       8.4.0\r\nimportlib_resources                      6.4.5\r\nimutils                                  0.5.4\r\ninflate64                                1.0.0\r\niniconfig                                2.0.0\r\ninspecta                                 0.1.3\r\nInstructorEmbedding                      1.0.1\r\nintervaltree                             3.1.0\r\niopath                                   0.1.10\r\njaconv                                   0.4.0\r\njamo                                     0.4.1\r\njaraco.context                           6.0.1\r\njieba                                    0.42.1\r\nJinja2                                   3.1.4\r\njiter                                    0.6.1\r\njmespath                                 1.0.1\r\njoblib                                   1.4.2\r\njsonlines                                1.2.0\r\njsonpatch                                1.33\r\njsonpath-python                          1.0.6\r\njsonpointer                              3.0.0\r\njsonschema                               4.23.0\r\njsonschema-specifications                2024.10.1\r\nkaleido                                  0.2.1\r\nkiwisolver                               1.4.7\r\nkubernetes                               31.0.0\r\nlangchain                                0.0.354\r\nlangchain-community                      0.0.8\r\nlangchain-core                           0.1.6\r\nlangchain-experimental                   0.0.47\r\nlangchain-google-genai                   0.0.6\r\nlangchain-mistralai                      0.0.2\r\nlangdetect                               1.0.9\r\nlangid                                   1.1.6\r\nlangsmith                                0.0.77\r\nlayoutparser                             0.3.4\r\nlazy_loader                              0.4\r\nlibrosa                                  0.10.1\r\nllama_cpp_python                         0.2.26+cpuavx2\r\nllama_cpp_python_cuda                    0.2.26+cu121avx\r\nllvmlite                                 0.43.0\r\nlm-dataformat                            0.0.20\r\nlm_eval                                  0.4.4\r\nloralib                                  0.1.2\r\nlxml                                     5.3.0\r\nlz4                                      4.3.3\r\nMarkdown                                 3.7\r\nmarkdown-it-py                           3.0.0\r\nMarkupSafe                               2.1.5\r\nmarshmallow                              3.22.0\r\nmatplotlib                               3.9.2\r\nmbstrdecoder                             1.1.3\r\nmdurl                                    0.1.2\r\nmistralai                                0.0.8\r\nmmh3                                     5.0.1\r\nmojimoji                                 0.0.13\r\nmongoengine                              0.24.2\r\nmonotonic                                1.6\r\nmore-itertools                           10.5.0\r\nmotor                                    3.5.3\r\nmplcursors                               0.5.3\r\nmpmath                                   1.3.0\r\nmsg-parser                               1.2.0\r\nmsgpack                                  1.1.0\r\nmultidict                                6.1.0\r\nmultiprocess                             0.70.15\r\nmultivolumefile                          0.2.3\r\nmutagen                                  1.47.0\r\nmypy-extensions                          1.0.0\r\nnarwhals                                 1.9.1\r\nnest-asyncio                             1.6.0\r\nnetworkx                                 2.8.8\r\nnltk                                     3.9.1\r\nnum2words                                0.5.13\r\nnumba                                    0.60.0\r\nnumexpr                                  2.10.1\r\nnumpy                                    1.23.4\r\noauthlib                                 3.2.2\r\nolefile                                  0.47\r\nomegaconf                                2.3.0\r\nonnx                                     1.17.0\r\nonnxruntime                              1.15.1\r\nonnxruntime-gpu                          1.15.0\r\nopenai                                   1.51.2\r\nopencv-python                            4.10.0.84\r\nopencv-python-headless                   4.10.0.84\r\nopenpyxl                                 3.1.5\r\nopentelemetry-api                        1.27.0\r\nopentelemetry-exporter-otlp-proto-common 1.27.0\r\nopentelemetry-exporter-otlp-proto-grpc   1.27.0\r\nopentelemetry-instrumentation            0.48b0\r\nopentelemetry-instrumentation-asgi       0.48b0\r\nopentelemetry-instrumentation-fastapi    0.48b0\r\nopentelemetry-proto                      1.27.0\r\nopentelemetry-sdk                        1.27.0\r\nopentelemetry-semantic-conventions       0.48b0\r\nopentelemetry-util-http                  0.48b0\r\nopenvino                                 2022.3.0\r\noptimum                                  1.16.1\r\norderly-set                              5.2.2\r\norjson                                   3.10.7\r\noutcome                                  1.3.0.post0\r\noverrides                                7.7.0\r\npackaging                                24.1\r\npandas                                   2.0.2\r\npathvalidate                             3.2.1\r\npdf2image                                1.17.0\r\npdfminer.six                             20221105\r\npdfplumber                               0.10.4\r\npeft                                     0.13.1\r\npikepdf                                  9.3.0\r\npillow                                   10.4.0\r\npillow_heif                              0.18.0\r\npip                                      23.0.1\r\npip-licenses                             5.0.0\r\nplatformdirs                             4.3.6\r\nplaywright                               1.47.0\r\nplotly                                   5.24.1\r\npluggy                                   1.5.0\r\npooch                                    1.8.2\r\nportalocker                              2.10.1\r\nposthog                                  3.7.0\r\npprintpp                                 0.4.0\r\nprettytable                              3.11.0\r\nprimp                                    0.6.3\r\npriority                                 2.0.0\r\npropcache                                0.2.0\r\nproto-plus                               1.24.0\r\nprotobuf                                 4.25.5\r\npsutil                                   6.0.0\r\npulsar-client                            3.5.0\r\npy7zr                                    0.22.0\r\npyarrow                                  17.0.0\r\npyarrow-hotfix                           0.6\r\npyasn1                                   0.6.1\r\npyasn1_modules                           0.4.1\r\npybcj                                    1.0.2\r\npybind11                                 2.13.6\r\npyclipper                                1.3.0.post5\r\npycocotools                              2.0.8\r\npycparser                                2.22\r\npycryptodomex                            3.21.0\r\npydantic                                 2.9.2\r\npydantic_core                            2.23.4\r\npydantic-settings                        2.1.0\r\npydash                                   8.0.3\r\npydub                                    0.25.1\r\npydyf                                    0.11.0\r\npyee                                     12.0.0\r\nPygments                                 2.18.0\r\npymongo                                  4.8.0\r\nPyMuPDF                                  1.24.11\r\npynvml                                   11.5.3\r\npypandoc                                 1.14\r\npypandoc_binary                          1.14\r\npyparsing                                3.1.4\r\npypdf                                    5.0.1\r\npypdfium2                                4.30.0\r\npyphen                                   0.16.0\r\nPyPika                                   0.48.9\r\npyppmd                                   1.1.0\r\npyproject-api                            1.8.0\r\npyproject_hooks                          1.2.0\r\npyreadline3                              3.5.4\r\nPySocks                                  1.7.1\r\npytablewriter                            1.2.0\r\npytesseract                              0.3.13\r\npytest                                   8.3.3\r\npytest-xdist                             3.6.1\r\npython-crfsuite                          0.9.11\r\npython-dateutil                          2.8.2\r\npython-doctr                             0.5.4a0\r\npython-docx                              1.1.2\r\npython-dotenv                            1.0.1\r\npython-iso639                            2024.4.27\r\npython-magic                             0.4.27\r\npython-magic-bin                         0.4.14\r\npython-multipart                         0.0.12\r\npython-pptx                              0.6.23\r\npytube                                   15.0.0\r\npytz                                     2024.2\r\npywin32                                  307\r\nPyYAML                                   6.0.2\r\npyzstd                                   0.16.1\r\nRapidFuzz                                3.10.0\r\nrarfile                                  4.2\r\nreferencing                              0.35.1\r\nregex                                    2024.9.11\r\nreplicate                                0.20.0\r\nrequests                                 2.32.3\r\nrequests-file                            2.1.0\r\nrequests-oauthlib                        2.0.0\r\nrequests-toolbelt                        1.0.0\r\nresponses                                0.18.0\r\nretrying                                 1.3.4\r\nrich                                     13.9.2\r\nrootpath                                 0.1.1\r\nrouge                                    1.0.1\r\nrouge_score                              0.1.2\r\nrpds-py                                  0.20.0\r\nrsa                                      4.9\r\nruff                                     0.6.9\r\ns3transfer                               0.10.2\r\nsacrebleu                                2.3.1\r\nsafetensors                              0.4.5\r\nscikit-image                             0.24.0\r\nscikit-learn                             1.2.2\r\nscipy                                    1.13.1\r\nselenium                                 4.25.0\r\nsemantic-version                         2.10.0\r\nsemanticscholar                          0.8.4\r\nsentence-transformers                    2.2.2\r\nsentencepiece                            0.1.99\r\nsetuptools                               65.5.0\r\nsgmllib3k                                1.0.0\r\nShapely                                  1.8.5.post1\r\nshellingham                              1.5.4\r\nsix                                      1.16.0\r\nsniffio                                  1.3.1\r\nsortedcontainers                         2.4.0\r\nsoundfile                                0.12.1\r\nsoupsieve                                2.6\r\nsoxr                                     0.5.0.post1\r\nSQLAlchemy                               2.0.35\r\nsqlitedict                               2.1.0\r\nsse-starlette                            0.10.3\r\nsseclient-py                             1.8.0\r\nstarlette                                0.38.6\r\nstrawberry-graphql                       0.246.0\r\nsympy                                    1.13.3\r\ntabledata                                1.3.3\r\ntabulate                                 0.9.0\r\ntaskgroup                                0.0.0a4\r\ntcolorpy                                 0.1.6\r\ntenacity                                 8.5.0\r\ntermcolor                                2.5.0\r\ntext-generation                          0.7.0\r\ntextstat                                 0.7.4\r\ntexttable                                1.7.0\r\nthreadpoolctl                            3.5.0\r\ntifffile                                 2024.9.20\r\ntiktoken                                 0.8.0\r\ntimm                                     1.0.9\r\ntinycss2                                 1.3.0\r\ntokenizers                               0.19.1\r\ntoml                                     0.10.2\r\ntomli                                    2.0.2\r\ntomlkit                                  0.12.0\r\ntorch                                    2.1.2+cu118\r\ntorchvision                              0.16.2+cu118\r\ntox                                      4.21.2\r\ntqdm                                     4.66.5\r\ntqdm-multiprocess                        0.0.11\r\ntransformers                             4.40.2\r\ntrio                                     0.26.2\r\ntrio-websocket                           0.11.1\r\ntypepy                                   1.3.2\r\ntyper                                    0.12.5\r\ntyping_extensions                        4.12.2\r\ntyping-inspect                           0.9.0\r\ntzdata                                   2024.2\r\ntzlocal                                  5.2\r\nujson                                    5.10.0\r\nUnidecode                                1.3.8\r\nuniversal-analytics-python3              1.1.1\r\nunstructured                             0.12.5\r\nunstructured-client                      0.26.0\r\nunstructured-inference                   0.7.23\r\nunstructured.pytesseract                 0.3.13\r\nurllib3                                  2.2.3\r\nuvicorn                                  0.31.0\r\nvalidators                               0.34.0\r\nvirtualenv                               20.26.6\r\nvoxel51-eta                              0.13.0\r\nwatchfiles                               0.24.0\r\nwavio                                    0.0.8\r\nwcwidth                                  0.2.13\r\nweasyprint                               62.3\r\nweaviate-client                          4.8.1\r\nwebencodings                             0.5.1\r\nwebsocket-client                         1.8.0\r\nwebsockets                               11.0.3\r\nwikipedia                                1.4.0\r\nwolframalpha                             5.1.3\r\nword2number                              1.1\r\nwrapt                                    1.16.0\r\nwsproto                                  1.2.0\r\nxlrd                                     2.0.1\r\nXlsxWriter                               3.2.0\r\nxmltodict                                0.13.0\r\nxxhash                                   3.5.0\r\nyarl                                     1.14.0\r\nyt-dlp                                   2023.10.13\r\nzipp                                     3.20.2\r\nzopfli                                   0.2.3\r\nzstandard                                0.23.0\r\n```",
    "comments": [
      {
        "user": "mbbutt",
        "body": "below is the log if it may help.\r\n\r\n```\r\n\r\n-----\r\n\r\nWeasyPrint could not import some external libraries. Please carefully follow the installation steps before reporting an issue:\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#installation\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#troubleshooting\r\n\r\n-----\r\n\r\n\r\n-----\r\n\r\nWeasyPrint could not import some external libraries. Please carefully follow the installation steps before reporting an issue:\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#installation\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#troubleshooting\r\n\r\n-----\r\n\r\nSTT enabled, may use more GPU, set --enable_stt=False for low-memory systems\r\nTTS enabled, may use more GPU, set --enable_tts=False for low-memory systems\r\nUsing Model llama\r\nload INSTRUCTOR_Transformer\r\nmax_seq_length  512\r\nMust install DocTR and LangChain installed if enabled DocTR, disabling\r\nC:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nC:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nStarting get_model: llama\r\nFailed to listen to n_gpus: Failed to load shared library 'C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\llama_cpp_cuda\\llama.dll': Could not find module 'C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\llama_cpp_cuda\\llama.dll' (or one of its dependencies). Try using the full path with constructor syntax.\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q6_K.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 18\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q6_K:  226 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q6_K\r\nllm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 5.15 GiB (6.56 BPW)\r\nllm_load_print_meta: general.name     = LLaMA v2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size       =    0.11 MiB\r\nllm_load_tensors: system memory used  = 5272.45 MiB\r\n....................................................................................................\r\nllama_new_context_with_model: n_ctx      = 2048\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\r\nllama_build_graph: non-view tensors processed: 676/676\r\nllama_new_context_with_model: compute buffer total size = 42.19 MiB\r\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |\r\nAuto-detected LLaMa n_ctx=4096, will unload then reload with this setting.\r\nwarning: failed to VirtualUnlock buffer: The segment is already unlocked.\r\n\r\nFailed to listen to n_gpus: Failed to load shared library 'C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\llama_cpp_cuda\\llama.dll': Could not find module 'C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\llama_cpp_cuda\\llama.dll' (or one of its dependencies). Try using the full path with constructor syntax.\r\nllama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q6_K.gguf (version GGUF V2)\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  10:                          general.file_type u32              = 18\r\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q6_K:  226 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 4096\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 32\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 1\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 11008\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 4096\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q6_K\r\nllm_load_print_meta: model params     = 6.74 B\r\nllm_load_print_meta: model size       = 5.15 GiB (6.56 BPW)\r\nllm_load_print_meta: general.name     = LLaMA v2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size       =    0.11 MiB\r\nllm_load_tensors: system memory used  = 5272.45 MiB\r\n....................................................................................................\r\nllama_new_context_with_model: n_ctx      = 4096\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\r\nllama_build_graph: non-view tensors processed: 676/676\r\nllama_new_context_with_model: compute buffer total size = 75.19 MiB\r\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |\r\nModel {'base_model': 'llama', 'base_model0': 'llama', 'tokenizer_base_model': '', 'lora_weights': '', 'inference_server': '', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'visible_models': None, 'h2ogpt_key': None, 'load_8bit': 'pause', 'load_4bit': True, 'low_bit_mode': 1, 'load_half': True, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': True, 'gpu_id': 0, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 5, 'n_gqa': 0, 'model_path_llama': 'C:\\\\Users\\\\Public\\\\git\\\\h2ogpt\\\\llamacpp_path\\\\llama-2-7b-chat.Q6_K.gguf', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': '', 'n_batch': 128}, 'rope_scaling': {}, 'max_seq_len': 4096, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}}\r\nBegin auto-detect HF cache text generation models\r\nC:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nNo loading model openai/whisper-base.en because is_encoder_decoder=True\r\nNo loading model microsoft/speecht5_hifigan because The checkpoint you are trying to load has model type `hifigan` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\r\nNo loading model microsoft/speecht5_tts because is_encoder_decoder=True\r\nEnd auto-detect HF cache text generation models\r\nBegin auto-detect llama.cpp models\r\nEnd auto-detect llama.cpp models\r\nC:\\Users\\Public\\git\\h2ogpt\\gradio_utils\\prompt_form.py:211: GradioUnusedKwargWarning: You have unused kwarg parameters in Chatbot, please remove them: {'likeable': True}\r\n  text_output = gr.Chatbot(label=output_label0,\r\nC:\\Users\\Public\\git\\h2ogpt\\gradio_utils\\prompt_form.py:216: GradioUnusedKwargWarning: You have unused kwarg parameters in Chatbot, please remove them: {'likeable': True}\r\n  text_output2 = gr.Chatbot(label=output_label0_model2,\r\nRunning on local URL:  http://0.0.0.0:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStarted Gradio Server and/or GUI: server_name: localhost port: None\r\nUse local URL: http://localhost:7860/\r\nC:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in ModelInfoResponse has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nC:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_names\" in ModelListResponse has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nOpenAI API URL: http://0.0.0.0:5000\r\nOpenAI API key: EMPTY\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "The logs don't mention using the GPU, which is probably why it's slow.  Something wrong with the llamacpp_python installation."
      },
      {
        "user": "mbbutt",
        "body": "Hello \r\nThanks for the response.\r\ni can see in pip list, both lama_cpp (for CPU) and llama_cpp (for cuda) are installed\r\ncould this be the reason?  \r\n\r\n```\r\nllama_cpp_python                         0.2.26+cpuavx2\r\nllama_cpp_python_cuda                    0.2.26+cu121avx\r\n\r\n```\r\n\r\nshould i uninstall or get some different version?\r\n\r\n\r\nthis is the issue\r\n\r\n```\r\nFailed to listen to n_gpus: Failed to load shared library\r\n'C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\llama_cpp_cuda\\llama.dll': Could not find module\r\n'C:\\Users\\Public\\pyenv-win\\pyenv-win\\bin\\.h2o\\lib\\site-packages\\llama_cpp_cuda\\llama.dll' (or one of its dependencies). Try using the full path with constructor syntax.\r\n\r\n\r\n```\r\n"
      },
      {
        "user": "mbbutt",
        "body": "now I did the new installation\r\nbut it failed on loading the model\r\n\r\nwith \r\n\r\n```\r\nUsing Model h2oai/h2ogpt-4096-llama2-7b-chat\r\ngit failed to run: [WinError 2] The system cannot find the file specified\r\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\r\nWindows fatal exception: code 0xc0000139\r\n\r\n```\r\n\r\n\r\nbelow is the complete log error\r\n\r\n\r\n```\r\n(.myh2o) C:\\Users\\Public\\h2ogpt_Nov24>python generate.py --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --prompt_type=human_bot --cli=True\r\n\r\n-----\r\n\r\nWeasyPrint could not import some external libraries. Please carefully follow the installation steps before reporting an issue:\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#installation\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#troubleshooting\r\n\r\n-----\r\n\r\nC:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\n\r\n-----\r\n\r\nWeasyPrint could not import some external libraries. Please carefully follow the installation steps before reporting an issue:\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#installation\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#troubleshooting\r\n\r\n-----\r\n\r\nUsing Model h2oai/h2ogpt-4096-llama2-7b-chat\r\ngit failed to run: [WinError 2] The system cannot find the file specified\r\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\r\nWindows fatal exception: code 0xc0000139\r\n\r\nCurrent thread 0x00002ea8 (most recent call first):\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 1176 in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 571 in module_from_spec\r\n  File \"<frozen importlib._bootstrap>\", line 674 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\awq\\modules\\linear.py\", line 4 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\awq\\models\\base.py\", line 16 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\awq\\models\\mpt.py\", line 1 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\awq\\models\\__init__.py\", line 1 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\awq\\__init__.py\", line 2 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\tuners\\lora\\awq.py\", line 26 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\tuners\\lora\\model.py\", line 50 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py\", line 20 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\tuners\\__init__.py\", line 21 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\mapping.py\", line 22 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\auto.py\", line 32 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\__init__.py\", line 22 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\transformers\\trainer.py\", line 226 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1050 in _gcd_import\r\n  ...\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1778, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\transformers\\trainer.py\", line 226, in <module>\r\n    from peft import PeftModel\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\__init__.py\", line 22, in <module>\r\n    from .auto import (\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\auto.py\", line 32, in <module>\r\n    from .mapping import MODEL_TYPE_TO_PEFT_MODEL_MAPPING\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\mapping.py\", line 22, in <module>\r\n    from peft.tuners.xlora.model import XLoraModel\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\tuners\\__init__.py\", line 21, in <module>\r\n    from .lora import LoraConfig, LoraModel, LoftQConfig, LoraRuntimeConfig\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py\", line 20, in <module>\r\n    from .model import LoraModel\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\tuners\\lora\\model.py\", line 50, in <module>\r\n    from .awq import dispatch_awq\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\peft\\tuners\\lora\\awq.py\", line 26, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\awq\\__init__.py\", line 2, in <module>\r\n    from awq.models.auto import AutoAWQForCausalLM\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\awq\\models\\__init__.py\", line 1, in <module>\r\n    from .mpt import MptAWQForCausalLM\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\awq\\models\\mpt.py\", line 1, in <module>\r\n    from .base import BaseAWQForCausalLM\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\awq\\models\\base.py\", line 16, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\awq\\modules\\linear.py\", line 4, in <module>\r\n    import awq_inference_engine  # with CUDA kernels\r\nImportError: DLL load failed while importing awq_inference_engine: The specified procedure could not be found.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\src\\utils.py\", line 79, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\fire\\core.py\", line 135, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\fire\\core.py\", line 468, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\fire\\core.py\", line 684, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\src\\gen.py\", line 2060, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\src\\gpt_langchain.py\", line 552, in get_embedding\r\n    embedding = HuggingFaceBgeEmbeddings(model_name=hf_embedding_model,\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py\", line 287, in __init__\r\n    import sentence_transformers\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\sentence_transformers\\__init__.py\", line 18, in <module>\r\n    from sentence_transformers.trainer import SentenceTransformerTrainer\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\sentence_transformers\\trainer.py\", line 12, in <module>\r\n    from transformers import EvalPrediction, PreTrainedTokenizerBase, Trainer, TrainerCallback\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1766, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"C:\\Users\\Public\\h2ogpt_Nov24\\.myh2o\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1780, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\r\nDLL load failed while importing awq_inference_engine: The specified procedure could not be found.\r\n\r\n```\r\n(.myh2o) C:\\Users\\Public\\h2ogpt_Nov24>"
      },
      {
        "user": "mbbutt",
        "body": "some how it is working when reinstall VS builder tool."
      }
    ]
  },
  {
    "issue_number": 1899,
    "title": "ModuleNotFoundError: No module named 'langchain_openai.chat_models'; 'langchain_openai' is not a package",
    "author": "ricvolpi",
    "state": "closed",
    "created_at": "2024-11-12T10:33:57Z",
    "updated_at": "2024-11-12T18:58:16Z",
    "labels": [],
    "body": "Running `python src/make_db.py --download_some=True` I get the following error\r\n\r\n```unix\r\nTraceback (most recent call last):\r\n  File \"/home/jupyter/Desktop/h2ogpt/src/make_db.py\", line 9, in <module>\r\n    from gpt_langchain import path_to_docs, get_some_dbs_from_hf, all_db_zips, some_db_zips, create_or_update_db, \\\r\n  File \"/home/jupyter/Desktop/h2ogpt/src/gpt_langchain.py\", line 2077, in <module>\r\n    from langchain_together import ChatTogether\r\n  File \"/home/jupyter/conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_together/__init__.py\", line 1, in <module>\r\n    from langchain_together.chat_models import ChatTogether\r\n  File \"/home/jupyter/conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_together/chat_models.py\", line 18, in <module>\r\n    from langchain_openai.chat_models.base import BaseChatOpenAI\r\nModuleNotFoundError: No module named 'langchain_openai.chat_models'; 'langchain_openai' is not a package\r\n```\r\n\r\nthe reason is the langchain_openai wrapper `src/langchain_openai.py` (see a similar issue [here](https://github.com/langchain-ai/langchain/issues/2079#issuecomment-1487416187)). I fixed it by renaming `src/langchain_openai.py` into `src/langchain_openai_wrapper.py` and updating related imports. Things work now, but maybe we can think about a cleaner fix for this? I wonder if the other wrappers cause other issues here and there.\r\n\r\nOS Linux / Debian 11\r\nPython 10.3",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It's already been renamed last week to langchain_openai_local.py .  You must be out of date on repo. Thanks!"
      }
    ]
  },
  {
    "issue_number": 1898,
    "title": "pydantic.errors.PydanticUserError: A non-annotated attribute was detected: `allow_map_1 = True`",
    "author": "ricvolpi",
    "state": "closed",
    "created_at": "2024-11-12T08:42:03Z",
    "updated_at": "2024-11-12T09:50:03Z",
    "labels": [],
    "body": "I have installed packages in `requirements.txt` and `reqs_optional/requirements_optional_gpu_only.txt`.\r\n\r\nWhen I run `python src/make_db.py --download_some=True`, I get\r\n\r\n```unix\r\nTraceback (most recent call last):\r\n  File \"/home/jupyter/Desktop/h2ogpt/src/make_db.py\", line 9, in <module>\r\n    from gpt_langchain import path_to_docs, get_some_dbs_from_hf, all_db_zips, some_db_zips, create_or_update_db, \\\r\n  File \"/home/jupyter/Desktop/h2ogpt/src/gpt_langchain.py\", line 97, in <module>\r\n    from h2o_serpapi import H2OSerpAPIWrapper\r\n  File \"/home/jupyter/Desktop/h2ogpt/src/h2o_serpapi.py\", line 8, in <module>\r\n    from utils_langchain import _chunk_sources, add_parser, _add_meta\r\n  File \"/home/jupyter/Desktop/h2ogpt/src/utils_langchain.py\", line 396, in <module>\r\n    class H2OMapReduceDocumentsChain(MapReduceDocumentsChain):\r\n  File \"/home/jupyter/conda/envs/h2o/lib/python3.10/site-packages/pydantic/_internal/_model_construction.py\", line 115, in __new__\r\n    private_attributes = inspect_namespace(\r\n  File \"/home/jupyter/conda/envs/h2o/lib/python3.10/site-packages/pydantic/_internal/_model_construction.py\", line 428, in inspect_namespace\r\n    raise PydanticUserError(\r\npydantic.errors.PydanticUserError: A non-annotated attribute was detected: `allow_map_1 = True`. All model fields require a type annotation; if `allow_map_1` is not meant to be a field, you may be able to resolve this error by annotating it as a `ClassVar` or updating `model_config['ignored_types']`.\r\n```\r\n\r\nAm I missing any dependency?\r\n\r\nOS: Debian GNU/Linux 11.\r\nPython version: 3.10\r\nPackages:\r\n\r\n```unix\r\nabsl-py                                  2.1.0\r\naccelerate                               1.1.1\r\naiofiles                                 23.2.1\r\naiohappyeyeballs                         2.4.3\r\naiohttp                                  3.10.10\r\naiosignal                                1.3.1\r\nannotated-types                          0.7.0\r\nanthropic                                0.39.0\r\nanyio                                    4.6.2.post1\r\nappdirs                                  1.4.4\r\nAPScheduler                              3.10.4\r\nasgiref                                  3.8.1\r\nasync-timeout                            4.0.3\r\nattrs                                    24.2.0\r\nauto_gptq                                0.7.1\r\nautoawq                                  0.2.6\r\nautoawq_kernels                          0.0.7\r\nbackoff                                  2.2.1\r\nbcrypt                                   4.2.0\r\nbeautifulsoup4                           4.12.3\r\nbioc                                     2.1\r\nbitsandbytes                             0.44.1\r\nblis                                     0.7.11\r\nboto3                                    1.35.58\r\nbotocore                                 1.35.58\r\nbuild                                    1.2.2.post1\r\ncachetools                               5.5.0\r\ncatalogue                                2.0.10\r\ncertifi                                  2024.8.30\r\ncharset-normalizer                       3.4.0\r\nchroma-hnswlib                           0.7.6\r\nchromadb                                 0.5.18\r\nclick                                    8.1.7\r\ncloudpathlib                             0.20.0\r\ncolorama                                 0.4.6\r\ncoloredlogs                              15.0.1\r\nconfection                               0.1.5\r\ncontourpy                                1.3.0\r\ncycler                                   0.12.1\r\ncymem                                    2.0.8\r\ndataclasses-json                         0.6.7\r\ndatasets                                 3.1.0\r\ndefusedxml                               0.7.1\r\nDeprecated                               1.2.14\r\ndill                                     0.3.8\r\ndistro                                   1.9.0\r\ndocopt                                   0.6.2\r\ndocutils                                 0.21.2\r\ndurationpy                               0.9\r\neinops                                   0.8.0\r\net_xmlfile                               2.0.0\r\nevaluate                                 0.4.3\r\nexceptiongroup                           1.2.2\r\nexecnet                                  2.1.1\r\nexllama                                  0.0.18+cu121\r\nfaiss-gpu                                1.7.2\r\nfastapi                                  0.115.4\r\nfastapi-utils                            0.8.0\r\nffmpy                                    0.4.0\r\nfilelock                                 3.16.1\r\nfire                                     0.7.0\r\nflatbuffers                              24.3.25\r\nfonttools                                4.54.1\r\nfrozenlist                               1.5.0\r\nfsspec                                   2024.9.0\r\ngekko                                    1.2.1\r\ngoogle-ai-generativelanguage             0.6.10\r\ngoogle-api-core                          2.23.0\r\ngoogle-api-python-client                 2.151.0\r\ngoogle-auth                              2.36.0\r\ngoogle-auth-httplib2                     0.2.0\r\ngoogle-generativeai                      0.8.3\r\ngoogleapis-common-protos                 1.65.0\r\ngradio                                   4.44.0\r\ngradio_client                            1.3.0\r\ngreenlet                                 3.1.1\r\ngrpcio                                   1.67.1\r\ngrpcio-status                            1.67.1\r\ngunicorn                                 23.0.0\r\nh11                                      0.14.0\r\nhf_transfer                              0.1.8\r\nhttpcore                                 1.0.6\r\nhttplib2                                 0.22.0\r\nhttptools                                0.6.4\r\nhttpx                                    0.27.2\r\nhttpx-sse                                0.4.0\r\nhuggingface-hub                          0.26.2\r\nhumanfriendly                            10.0\r\nidna                                     3.10\r\nimportlib_metadata                       8.5.0\r\nimportlib_resources                      6.4.5\r\niniconfig                                2.0.0\r\nInstructorEmbedding                      1.0.1\r\nintervaltree                             3.1.0\r\nJinja2                                   3.1.4\r\njiter                                    0.7.0\r\njmespath                                 1.0.1\r\njoblib                                   1.4.2\r\njson_repair                              0.30.1\r\njsonlines                                4.0.0\r\njsonpatch                                1.33\r\njsonpointer                              3.0.0\r\njsonschema                               4.23.0\r\njsonschema-specifications                2024.10.1\r\nkiwisolver                               1.4.7\r\nkubernetes                               31.0.0\r\nlangchain                                0.3.4\r\nlangchain-anthropic                      0.2.4\r\nlangchain-chroma                         0.1.4\r\nlangchain-community                      0.3.3\r\nlangchain-core                           0.3.13\r\nlangchain-experimental                   0.3.3\r\nlangchain-google-genai                   2.0.4\r\nlangchain-mistralai                      0.2.0\r\nlangchain-ollama                         0.2.0\r\nlangchain-text-splitters                 0.3.0\r\nlangcodes                                3.4.1\r\nlangsmith                                0.1.137\r\nlanguage_data                            1.2.0\r\nlimits                                   3.13.0\r\nlm-dataformat                            0.0.20\r\nloralib                                  0.1.2\r\nlxml                                     5.3.0\r\nmarisa-trie                              1.2.1\r\nMarkdown                                 3.7\r\nmarkdown-it-py                           3.0.0\r\nMarkupSafe                               2.1.5\r\nmarshmallow                              3.23.1\r\nmatplotlib                               3.9.2\r\nmdurl                                    0.1.2\r\nmmh3                                     5.0.1\r\nmonotonic                                1.6\r\nmpmath                                   1.3.0\r\nmultidict                                6.1.0\r\nmultiprocess                             0.70.16\r\nmurmurhash                               1.0.10\r\nmypy-extensions                          1.0.0\r\nnetworkx                                 3.4.2\r\nnltk                                     3.9.1\r\nnumpy                                    1.26.4\r\nnvidia-cublas-cu12                       12.1.3.1\r\nnvidia-cuda-cupti-cu12                   12.1.105\r\nnvidia-cuda-nvrtc-cu12                   12.1.105\r\nnvidia-cuda-runtime-cu12                 12.1.105\r\nnvidia-cudnn-cu12                        8.9.2.26\r\nnvidia-cufft-cu12                        11.0.2.54\r\nnvidia-curand-cu12                       10.3.2.106\r\nnvidia-cusolver-cu12                     11.4.5.107\r\nnvidia-cusparse-cu12                     12.1.0.106\r\nnvidia-nccl-cu12                         2.20.5\r\nnvidia-nvjitlink-cu12                    12.6.77\r\nnvidia-nvtx-cu12                         12.1.105\r\noauthlib                                 3.2.2\r\nollama                                   0.3.3\r\nonnxruntime                              1.20.0\r\nonnxruntime-gpu                          1.15.0\r\nopenai                                   1.54.3\r\nopenpyxl                                 3.1.5\r\nopentelemetry-api                        1.28.1\r\nopentelemetry-exporter-otlp-proto-common 1.28.1\r\nopentelemetry-exporter-otlp-proto-grpc   1.28.1\r\nopentelemetry-instrumentation            0.49b1\r\nopentelemetry-instrumentation-asgi       0.49b1\r\nopentelemetry-instrumentation-fastapi    0.49b1\r\nopentelemetry-proto                      1.28.1\r\nopentelemetry-sdk                        1.28.1\r\nopentelemetry-semantic-conventions       0.49b1\r\nopentelemetry-util-http                  0.49b1\r\norjson                                   3.10.11\r\noverrides                                7.7.0\r\npackaging                                24.2\r\npandas                                   2.2.3\r\npeft                                     0.13.2\r\npillow                                   10.4.0\r\npip                                      24.3.1\r\npluggy                                   1.5.0\r\nportalocker                              2.10.1\r\nposthog                                  3.7.0\r\npreshed                                  3.0.9\r\npropcache                                0.2.0\r\nproto-plus                               1.25.0\r\nprotobuf                                 5.28.3\r\npsutil                                   5.9.8\r\npyarrow                                  18.0.0\r\npyasn1                                   0.6.1\r\npyasn1_modules                           0.4.1\r\npydantic                                 2.9.2\r\npydantic_core                            2.23.4\r\npydantic-settings                        2.6.1\r\npydub                                    0.25.1\r\npyexiv2                                  2.15.3\r\nPygments                                 2.18.0\r\npynvml                                   11.5.3\r\npypandoc_binary                          1.14\r\npyparsing                                3.2.0\r\npyphen                                   0.17.0\r\nPyPika                                   0.48.9\r\npyproject_hooks                          1.2.0\r\npytest                                   8.3.3\r\npytest-xdist                             3.6.1\r\npython-dateutil                          2.9.0.post0\r\npython-dotenv                            1.0.1\r\npython-multipart                         0.0.17\r\npytz                                     2024.2\r\nPyYAML                                   6.0.2\r\nreferencing                              0.35.1\r\nregex                                    2024.11.6\r\nrequests                                 2.32.3\r\nrequests-oauthlib                        2.0.0\r\nrequests-toolbelt                        1.0.0\r\nrich                                     13.9.4\r\nrouge                                    1.0.1\r\nrouge_score                              0.1.2\r\nrpds-py                                  0.21.0\r\nrsa                                      4.9\r\nruff                                     0.7.3\r\ns3transfer                               0.10.3\r\nsacrebleu                                2.4.3\r\nsafetensors                              0.4.5\r\nscikit-learn                             1.5.2\r\nscipy                                    1.14.1\r\nsemantic-version                         2.10.0\r\nsentence-transformers                    3.3.0\r\nsentence-transformers-old                2.2.2\r\nsentencepiece                            0.2.0\r\nsetuptools                               75.3.0\r\nshellingham                              1.5.4\r\nsix                                      1.16.0\r\nslowapi                                  0.1.9\r\nsmart-open                               7.0.5\r\nsniffio                                  1.3.1\r\nsortedcontainers                         2.4.0\r\nsoupsieve                                2.6\r\nspacy                                    3.7.5\r\nspacy-legacy                             3.0.12\r\nspacy-loggers                            1.0.5\r\nSQLAlchemy                               2.0.35\r\nsrsly                                    2.4.8\r\nsse-starlette                            2.1.3\r\nstarlette                                0.41.2\r\nsympy                                    1.13.3\r\ntabulate                                 0.9.0\r\ntenacity                                 9.0.0\r\ntermcolor                                2.5.0\r\ntext-generation                          0.7.0\r\ntextstat                                 0.7.4\r\nthinc                                    8.2.5\r\nthreadpoolctl                            3.5.0\r\ntiktoken                                 0.8.0\r\ntokenizers                               0.20.3\r\ntomli                                    2.1.0\r\ntomlkit                                  0.12.0\r\ntorch                                    2.3.1\r\ntorchvision                              0.18.1\r\ntqdm                                     4.67.0\r\ntransformers                             4.46.2\r\ntriton                                   2.3.1\r\ntyper                                    0.13.0\r\ntyping_extensions                        4.12.2\r\ntyping-inspect                           0.9.0\r\ntzdata                                   2024.2\r\ntzlocal                                  5.2\r\nujson                                    5.10.0\r\nuritemplate                              4.1.1\r\nurllib3                                  2.2.3\r\nuvicorn                                  0.32.0\r\nuvloop                                   0.21.0\r\nwasabi                                   1.1.3\r\nwatchfiles                               0.24.0\r\nweasel                                   0.4.1\r\nwebsocket-client                         1.8.0\r\nwebsockets                               12.0\r\nwheel                                    0.45.0\r\nwrapt                                    1.16.0\r\nxxhash                                   3.5.0\r\nyarl                                     1.17.1\r\nzipp                                     3.21.0\r\nzstandard                                0.23.0\r\n```\r\n\r\n",
    "comments": [
      {
        "user": "ricvolpi",
        "body": "My bad, just found proper install instructions [here](https://github.com/h2oai/h2ogpt/blob/main/docs/linux_install.sh)."
      }
    ]
  },
  {
    "issue_number": 1889,
    "title": "Role-Based Login for collections addone or removal Management:",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-10-29T12:07:58Z",
    "updated_at": "2024-11-10T20:40:53Z",
    "labels": [],
    "body": "Hi,\r\nRequest for Assistance in the following important aspect:\r\n\r\n1. I need help with the  **Role-Based Login for collections addone or removal Management:**\r\nI’d like to implement a role-based login system to restrict the Add/Delete functionality for specific folders to admin users only. Could you provide guidance on how to set this up?\r\n2. I just simply want only admin can control the **Document Selection** tab. Except chat all tabs should be hidden for the users. Only admin have access all the tabs and add or remove collection functionality.\r\n\r\nThe above issue affecting the implementation.  Request specific links/ suggestions please.\r\n\r\nThanks and Warm regards",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It's certainly possible, but one would have to dig into the code and see what the best way is for exactly what you want to do.  It could be quite simple or more complex."
      }
    ]
  },
  {
    "issue_number": 1897,
    "title": "Automation of file upload and embedding generation for MyData collection through api",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-11-08T12:06:22Z",
    "updated_at": "2024-11-08T12:06:22Z",
    "labels": [],
    "body": "When I'm uploading files through the '/upload_api' and '/add_file_api' then they upload the files in the user_path and embedding are automatically generated in the vector store. However, the same i'm not able to replicate in for **MyData** collection. \r\n\r\nBelow is the sample code for uploading files in **UserData** through api:\r\n\r\n`import os\r\nimport time\r\nfrom tqdm import tqdm\r\nfrom gradio_client import Client\r\n\r\nclass DocumentUploader:\r\n    def __init__(self, host_url: str, api_key: str = 'EMPTY'):\r\n        self.client = Client(host_url)\r\n        self.api_key = api_key\r\n\r\n    def upload_document(self, local_file_path: str) -> str:\r\n        \"\"\"Uploads a document to the server and returns the server file path.\"\"\"\r\n        with tqdm(total=100, desc=f\"Uploading {os.path.basename(local_file_path)}\", unit='%', ncols=80) as pbar:\r\n            _, server_file_path = self.client.predict(local_file_path, api_name='/upload_api')\r\n            pbar.update(100)\r\n        return server_file_path\r\n\r\n    def add_document_and_ocr(self, server_file_path: str, loaders: list) -> dict:\r\n        \"\"\"Adds the document to the server with OCR processing.\"\"\"\r\n        with tqdm(total=100, desc=f\"Processing {os.path.basename(server_file_path)}\", unit='%', ncols=80) as pbar:\r\n            res = self.client.predict(\r\n                server_file_path, \"UserData\", True, 512, True, *loaders, api_name='/add_file_api'\r\n            )\r\n            pbar.update(100)\r\n        print(\"Document processing completed and is ready for querying.\")\r\n        return res\r\n\r\n    def process_all_files_in_folder(self, folder_path: str):\r\n        \"\"\"Process all PDF files in the specified folder.\"\"\"\r\n        pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\r\n        if not pdf_files:\r\n            print(\"No PDF files found in the folder.\")\r\n            return\r\n\r\n        print(f\"Found {len(pdf_files)} PDF files. Starting upload and processing...\")\r\n\r\n        loaders = [None] * 6  # Adjust loaders as needed\r\n        for pdf_file in pdf_files:\r\n            file_path = os.path.join(folder_path, pdf_file)\r\n            server_file_path = self.upload_document(file_path)\r\n            self.add_document_and_ocr(server_file_path, loaders)`\r\n\r\nPlease help me for automate the same in the **MyData** collection! ",
    "comments": []
  },
  {
    "issue_number": 1895,
    "title": "ImportError: cannot import name 'cached_download' from 'huggingface_hub'",
    "author": "juerware",
    "state": "closed",
    "created_at": "2024-11-06T13:48:55Z",
    "updated_at": "2024-11-06T20:47:11Z",
    "labels": [],
    "body": "Commit: f7971ea1 --> lastest at the time to write this issue in the main branch\r\nThe environment was created launching the script: ```bash docs/linux_install_full.sh```\r\nThe error was: ```cannot import name 'cached_download'```\r\nSolution found in this issue: ```https://github.com/FunAudioLLM/CosyVoice/issues/516```\r\nTo solve execute the following command line: ```pip install huggingface-hub==0.25.2``` in the conda environment for downgrading\r\n",
    "comments": []
  },
  {
    "issue_number": 1894,
    "title": "ModuleNotFoundError: No module named 'langchain_openai.chat_models'; 'langchain_openai' is not a package",
    "author": "Vidminas",
    "state": "closed",
    "created_at": "2024-11-04T15:02:12Z",
    "updated_at": "2024-11-05T19:37:33Z",
    "labels": [],
    "body": "I installed h2ogpt locally on Windows following the guide at <https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md>.\r\nFollowing the docs in <https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md>, I was trying to set up a user database by running `python src/make_db.py --add_if_exists=True --user_path=userdata` (where `userdata` is a directory I created).\r\nIt results in the error:\r\n```shell\r\n$ python src/make_db.py --add_if_exists=True --user_path=userdata\r\nFontconfig error: Cannot load default config file: No such file: (null)\r\nWARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Vidminas\\GitHub\\h2ogpt\\src\\make_db.py\", line 9, in <module>\r\n    from gpt_langchain import path_to_docs, get_some_dbs_from_hf, all_db_zips, some_db_zips, create_or_update_db, \\\r\n  File \"C:\\Users\\Vidminas\\GitHub\\h2ogpt\\src\\gpt_langchain.py\", line 2076, in <module>\r\n    from langchain_together import ChatTogether\r\n  File \"C:\\dev\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_together\\__init__.py\", line 3, in <module>\r\n    from langchain_together.chat_models import ChatTogether\r\n  File \"C:\\dev\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_together\\chat_models.py\", line 18, in <module>\r\n    from langchain_openai.chat_models.base import BaseChatOpenAI\r\nModuleNotFoundError: No module named 'langchain_openai.chat_models'; 'langchain_openai' is not a package\r\n```\r\n\r\nDigging into this, I found that it's caused by the file `src/langchain_openai.py` which overrides the `langchain-openai` package visibility. This file should be renamed to something else.",
    "comments": []
  },
  {
    "issue_number": 1883,
    "title": "Getting functional server error",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-10-22T10:22:38Z",
    "updated_at": "2024-10-24T23:12:04Z",
    "labels": [],
    "body": "### Description\r\nHi, I'm encountering an issue while attempting to use a functional server to upload files through an API. Specifically, when I run the server with the following command, I receive an error related to connection establishment. However, if I run the server without enabling the functional server, it works as expected.\r\n\r\nPlease help me to solve the issue.\r\n\r\nThank You\r\n\r\n### Command\r\n\r\n```\r\n python generate.py --base_model=meta-llama/Meta-Llama-3.1-8B-Instruct  --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --visible_visible_models=False --max_seq_len=8192 --max_max_new_tokens=4096 --max_new_tokens=4096 --min_new_tokens=256 --api_open=True --allow_api=True --max_quality=True  --function_server=True --function_server_workers=5 --multiple_workers_gunicorn=True --function_server_port=5002 --function_api_key=API_KEY\r\n ```\r\n \r\n \r\n### Issue Details\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connection.py\", line 196, in _new_conn\r\n    sock = connection.create_connection(\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/util/connection.py\", line 85, in create_connection\r\n    raise err\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/util/connection.py\", line 73, in create_connection\r\n    sock.connect(sa)\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\r\n    response = self._make_request(\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 495, in _make_request\r\n    conn.request(\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connection.py\", line 398, in request\r\n    self.endheaders()\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/http/client.py\", line 1278, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/http/client.py\", line 1038, in _send_output\r\n    self.send(msg)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/http/client.py\", line 976, in send\r\n    self.connect()\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connection.py\", line 236, in connect\r\n    self.sock = self._new_conn()\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connection.py\", line 211, in _new_conn\r\n    raise NewConnectionError(\r\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7de388ea7010>: Failed to establish a new connection: [Errno 111] Connection refused\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\r\n    resp = conn.urlopen(\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\r\n    retries = retries.increment(\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/urllib3/util/retry.py\", line 519, in increment\r\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\r\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='0.0.0.0', port=5002): Max retries exceeded with url: /execute_function/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7de388ea7010>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/versha/Documents/narad050824_pmdemo_mobile_lake_apitest/weseecore/src/gpt_langchain.py\", line 9383, in update_user_db\r\n    return _update_user_db(file, db1s=db1s,\r\n  File \"/home/versha/Documents/narad050824_pmdemo_mobile_lake_apitest/weseecore/src/gpt_langchain.py\", line 9664, in _update_user_db\r\n    sources = call_function_server('0.0.0.0', function_server_port, 'path_to_docs', (file,), simple_kwargs,\r\n  File \"/home/versha/Documents/narad050824_pmdemo_mobile_lake_apitest/weseecore/src/function_client.py\", line 50, in call_function_server\r\n    execute_result = execute_function_on_server(host, port, function_name, args, kwargs, use_disk, use_pickle,\r\n  File \"/home/versha/Documents/narad050824_pmdemo_mobile_lake_apitest/weseecore/src/function_client.py\", line 21, in execute_function_on_server\r\n    response = requests.post(url, json=payload, headers=headers)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/requests/api.py\", line 115, in post\r\n    return request(\"post\", url, data=data, json=json, **kwargs)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/requests/api.py\", line 59, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/versha/miniconda3/envs/narad_050824_pmdemomobile_lake/lib/python3.10/site-packages/requests/adapters.py\", line 700, in send\r\n    raise ConnectionError(e, request=request)\r\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='0.0.0.0', port=5002): Max retries exceeded with url: /execute_function/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7de388ea7010>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\n```\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "That means the client side can't reach the function server.  You can check the start-up shows the 5002 port started, and you can look at the openai_logs/* function server related files to see if present\r\n\r\ni.e. they may look like:\r\n\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ ls -alrt openai_logs/\r\ntotal 224\r\n-rw-------  1 jon jon     0 Oct 20 10:00 gunicorn_OpenAI_d9ffa2da-638d-4d78-a66b-2ddf28748f4a_stdout.log\r\n-rw-------  1 jon jon     0 Oct 20 10:00 gunicorn_Function_c8db57d7-aa31-494f-b4a6-01cc810663ed_stdout.log\r\n-rw-------  1 jon jon  1154 Oct 20 10:00 gunicorn_OpenAI_d9ffa2da-638d-4d78-a66b-2ddf28748f4a_stderr.log\r\n-rw-------  1 jon jon  1154 Oct 20 10:00 gunicorn_Function_c8db57d7-aa31-494f-b4a6-01cc810663ed_stderr.log\r\n-rw-------  1 jon jon   299 Oct 20 15:48 gunicorn_OpenAI_110a8b60-9f67-4045-ad93-4fa75b21ff44_stdout.log\r\n-rw-------  1 jon jon 11486 Oct 20 15:48 gunicorn_Agent_f900440d-2f0f-4b81-a2b0-0b44dfd6231c_stdout.log\r\n-rw-------  1 jon jon 21009 Oct 20 15:56 gunicorn_OpenAI_110a8b60-9f67-4045-ad93-4fa75b21ff44_stderr.log\r\n-rw-------  1 jon jon 20392 Oct 20 15:56 gunicorn_Agent_f900440d-2f0f-4b81-a2b0-0b44dfd6231c_stderr.log\r\n-rw-------  1 jon jon   167 Oct 20 15:58 gunicorn_OpenAI_2a853fe3-bc2b-4069-a3a0-7ab4f3346568_stdout.log\r\n-rw-------  1 jon jon 11486 Oct 20 15:58 gunicorn_Agent_a3da0ca9-6eef-4f01-b5d6-0cff2d36c92c_stdout.log\r\n-rw-------  1 jon jon 19283 Oct 20 16:00 gunicorn_OpenAI_2a853fe3-bc2b-4069-a3a0-7ab4f3346568_stderr.log\r\n-rw-------  1 jon jon 20073 Oct 20 16:00 gunicorn_Agent_a3da0ca9-6eef-4f01-b5d6-0cff2d36c92c_stderr.log\r\n-rw-------  1 jon jon     0 Oct 20 16:01 gunicorn_OpenAI_96b3c872-136c-45f8-a0f2-c9a228887872_stdout.log\r\n-rw-------  1 jon jon     0 Oct 20 16:01 gunicorn_Agent_5e81accd-e122-4890-902f-dddccbcc6edb_stdout.log\r\ndrwx------  2 jon jon  4096 Oct 20 16:01 ./\r\n-rw-------  1 jon jon 15287 Oct 20 16:02 gunicorn_OpenAI_96b3c872-136c-45f8-a0f2-c9a228887872_stderr.log\r\n-rw-------  1 jon jon 15923 Oct 20 16:02 gunicorn_Agent_5e81accd-e122-4890-902f-dddccbcc6edb_stderr.log\r\ndrwxrwxr-x 83 jon jon 61440 Oct 23 12:51 ../\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ \r\n```\r\nover multiple runs (new file each run).  \r\n\r\ne.g. this would be a bad startup:\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ cat openai_logs/gunicorn_Function_c8db57d7-aa31-494f-b4a6-01cc810663ed_stderr.log\r\n[2024-10-20 10:00:37 -0700] [907130] [INFO] Starting gunicorn 23.0.0\r\n[2024-10-20 10:00:37 -0700] [907130] [ERROR] Connection in use: ('0.0.0.0', 5002)\r\n[2024-10-20 10:00:37 -0700] [907130] [ERROR] connection to ('0.0.0.0', 5002) failed: [Errno 98] Address already in use\r\n[2024-10-20 10:00:38 -0700] [907130] [ERROR] Connection in use: ('0.0.0.0', 5002)\r\n[2024-10-20 10:00:38 -0700] [907130] [ERROR] connection to ('0.0.0.0', 5002) failed: [Errno 98] Address already in use\r\n[2024-10-20 10:00:39 -0700] [907130] [ERROR] Connection in use: ('0.0.0.0', 5002)\r\n[2024-10-20 10:00:39 -0700] [907130] [ERROR] connection to ('0.0.0.0', 5002) failed: [Errno 98] Address already in use\r\n[2024-10-20 10:00:40 -0700] [907130] [ERROR] Connection in use: ('0.0.0.0', 5002)\r\n[2024-10-20 10:00:40 -0700] [907130] [ERROR] connection to ('0.0.0.0', 5002) failed: [Errno 98] Address already in use\r\n[2024-10-20 10:00:41 -0700] [907130] [ERROR] Connection in use: ('0.0.0.0', 5002)\r\n[2024-10-20 10:00:41 -0700] [907130] [ERROR] connection to ('0.0.0.0', 5002) failed: [Errno 98] Address already in use\r\n[2024-10-20 10:00:42 -0700] [907130] [ERROR] Can't connect to ('0.0.0.0', 5002)\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ \r\n```\r\n\r\nWhile a good output would be:\r\n\r\n```\r\n(h2ogpte) jon@pseudotensor:~/h2ogpte/docker_data/h2ogpt/docker_logs/h2ogpt_openai$ tail -10 gunicorn_Function_296f27f4-e7ae-4284-85e2-5fd7a741b25c_stdout.log\r\ngit_hash: cf74d576ecfca0e24cac27588b52a4701dd7cb1d\r\nvisible_models: ['meta-llama/Meta-Llama-3.1-8B-Instruct']\r\nvisible_vision_models: ['mistralai/Pixtral-12B-2409']\r\nCommand: /usr/bin/gunicorn -w 5 -k uvicorn.workers.UvicornWorker --timeout 60 -b 0.0.0.0:5002 openai_server.function_server:app\r\nHash: cf74d576ecfca0e24cac27588b52a4701dd7cb1d\r\nPrep: persist_directory=db_dir_UserData exists, using\r\nPrep: persist_directory=db_dir_UserData exists, using\r\nPrep: persist_directory=db_dir_UserData exists, using\r\nPrep: persist_directory=db_dir_UserData exists, using\r\nPrep: persist_directory=db_dir_UserData exists, using\r\n(h2ogpte) jon@pseudotensor:~/h2ogpte/docker_data/h2ogpt/docker_logs/h2ogpt_openai$ tail -10 gunicorn_Function_296f27f4-e7ae-4284-85e2-5fd7a741b25c_stderr.log \r\n[2024-10-23 22:37:58 +0000] [198] [INFO] Application startup complete.\r\n[2024-10-23 22:37:58 +0000] [200] [INFO] Started server process [200]\r\n[2024-10-23 22:37:58 +0000] [200] [INFO] Waiting for application startup.\r\n[2024-10-23 22:37:58 +0000] [200] [INFO] Application startup complete.\r\n[2024-10-23 22:37:58 +0000] [201] [INFO] Started server process [201]\r\n[2024-10-23 22:37:58 +0000] [201] [INFO] Waiting for application startup.\r\n[2024-10-23 22:37:58 +0000] [201] [INFO] Application startup complete.\r\n[2024-10-23 22:37:58 +0000] [197] [INFO] Started server process [197]\r\n[2024-10-23 22:37:58 +0000] [197] [INFO] Waiting for application startup.\r\n[2024-10-23 22:37:58 +0000] [197] [INFO] Application startup complete.\r\n(h2ogpte) jon@pseudotensor:~/h2ogpte/docker_data/h2ogpt/docker_logs/h2ogpt_openai$ \r\n\r\n```\r\n "
      }
    ]
  },
  {
    "issue_number": 1873,
    "title": "request for combining multiple chromadb  ",
    "author": "mbbutt",
    "state": "open",
    "created_at": "2024-10-09T09:44:25Z",
    "updated_at": "2024-10-24T23:11:55Z",
    "labels": [],
    "body": "I would like to request an enhancement to that allows users to select multiple chromadb databases simultaneously (in the Resources section), rather than being restricted to one database at a time. Currently, the software only supports single database selection, which limits efficiency when users need to query or access documents across multiple datasets. The proposed enhancement includes updating the user interface to allow multi-database selection via checkbox list (or similar)",
    "comments": []
  },
  {
    "issue_number": 1871,
    "title": "[HELM] Refactor Chart and include isolated agents",
    "author": "EshamAaqib",
    "state": "open",
    "created_at": "2024-10-07T09:39:24Z",
    "updated_at": "2024-10-24T23:11:46Z",
    "labels": [],
    "body": "# Description\r\n\r\nIt would be better to refactor the h2oGPT Helm chart as we did with the Enterprise version. Also we need to include isolated agents in the chart, so agents can run independently from h2oGPT OSS ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Should be optional as to whether agent pod is separate or not."
      }
    ]
  },
  {
    "issue_number": 1864,
    "title": "Feature request: Include User ID, input prompt, and elapsed time in reviews.csv",
    "author": "yokomizo-tech",
    "state": "open",
    "created_at": "2024-10-03T02:11:51Z",
    "updated_at": "2024-10-24T23:11:25Z",
    "labels": [],
    "body": "The reviews.csv file is helpful, but it would be more useful if it included additional columns for User ID, input prompt, and elapsed time to generate a response, rather than just generated answers, reviews, and timestamps.",
    "comments": []
  },
  {
    "issue_number": 1853,
    "title": "Upload API takes longer time than uploading through the UI.",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-09-25T08:46:49Z",
    "updated_at": "2024-10-24T23:11:17Z",
    "labels": [],
    "body": "The documentation provides sample code for uploading a file from the client side through the `upload_api`. However, I have observed that uploading files through this API takes longer than uploading through the UI. I would like to know the reason for this discrepancy and how I can make the process faster. Also, if multiple clients send files through this API, how is this handled?\r\n\r\n```\r\nimport os\r\nimport ast\r\nimport time\r\nfrom tqdm import tqdm\r\nfrom watchdog.observers import Observer\r\nfrom watchdog.events import FileSystemEventHandler\r\nfrom gradio_client import Client\r\n\r\nclass DocumentUploader:\r\n    def __init__(self, host_url: str, api_key: str = 'EMPTY'):\r\n        self.client = Client(host_url)\r\n        self.api_key = api_key\r\n\r\n    def upload_document(self, local_file_path: str) -> str:\r\n        \"\"\"Uploads a document to the server and returns the server file path with a progress bar.\"\"\"\r\n        with tqdm(total=100, desc=f\"Uploading {os.path.basename(local_file_path)}\", unit='%', ncols=80) as pbar:\r\n            _, server_file_path = self.client.predict(local_file_path, api_name='/upload_api')\r\n            pbar.update(100)\r\n        return server_file_path\r\n\r\n    def add_document_and_ocr(self, server_file_path: str, loaders: list) -> dict:\r\n        \"\"\"Adds the document to the server with OCR processing and shows progress.\"\"\"\r\n        with tqdm(total=100, desc=f\"Processing {os.path.basename(server_file_path)}\", unit='%', ncols=80) as pbar:\r\n            res = self.client.predict(\r\n                server_file_path, \"UserData\", True, 512, True, *loaders, api_name='/add_file_api'\r\n            )\r\n            pbar.update(100)\r\n        return res\r\n\r\n    def query_document(self, langchain_mode: str, instruction: str) -> str:\r\n        \"\"\"Queries the document based on given instructions and returns the response.\"\"\"\r\n        kwargs = dict(langchain_mode=langchain_mode, instruction=instruction)\r\n        res = self.client.predict(str(kwargs), api_name='/submit_nochat_api')\r\n        return ast.literal_eval(res)['response']\r\n\r\n    def process_file(self, file_path: str):\r\n        \"\"\"Processes a single file with progress display.\"\"\"\r\n        loaders = [\r\n            ['Caption', 'CaptionBlip2', 'Pix2Struct', 'OCR', 'DocTR'],\r\n            ['PyMuPDF', 'Unstructured', 'PyPDF', 'TryHTML', 'OCR'],\r\n            None, None\r\n        ]\r\n        print(f\"Processing single file: {file_path}\")\r\n        server_file_path = self.upload_document(file_path)\r\n        self.add_document_and_ocr(server_file_path, loaders)\r\n\r\n    def query_uploaded_documents(self, instruction: str):\r\n        \"\"\"Queries the already uploaded documents.\"\"\"\r\n        response = self.query_document(\"UserData\", instruction)\r\n        print(\"Query response:\", response)\r\n\r\n\r\nclass NewFileHandler(FileSystemEventHandler):\r\n    \"\"\"Event handler that triggers when a new file is added to the folder.\"\"\"\r\n    def __init__(self, uploader: DocumentUploader):\r\n        self.uploader = uploader\r\n\r\n    def on_created(self, event):\r\n        \"\"\"Triggered when a new file is created in the watched folder.\"\"\"\r\n        if not event.is_directory and event.src_path.endswith(\".pdf\"):\r\n            print(f\"New file detected: {event.src_path}\")\r\n            self.uploader.process_file(event.src_path)\r\n\r\n\r\ndef monitor_folder(folder_path: str, uploader: DocumentUploader):\r\n    \"\"\"Monitors the folder and triggers the uploader when new files are added.\"\"\"\r\n    event_handler = NewFileHandler(uploader)\r\n    observer = Observer()\r\n    observer.schedule(event_handler, folder_path, recursive=False)\r\n    observer.start()\r\n    print(f\"Monitoring folder: {folder_path}\")\r\n\r\n    try:\r\n        while True:\r\n            time.sleep(1)  # Keep the script running\r\n    except KeyboardInterrupt:\r\n        observer.stop()\r\n\r\n    observer.join()\r\n\r\n\r\n# Usage example\r\nhost_url = \"http://xx.xx.x.xx:7860/\" \r\nfolder_path = \"data\"\r\n\r\nuploader = DocumentUploader(host_url)\r\n\r\n# Start monitoring the folder\r\nmonitor_folder(folder_path, uploader)\r\n```",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, maybe the API and UI are using different options by default for which handlers (e.g. doctr, unstructured, OCR, vision, etc.) are used.  Good to compare logs for each.\r\n\r\nNote that the API and UI use the same code and use gradio's unified way of generating API from UI itself."
      }
    ]
  },
  {
    "issue_number": 1851,
    "title": "Clean install, nothing boots when i do 'python generate.py'",
    "author": "Domsmasher1",
    "state": "open",
    "created_at": "2024-09-23T10:40:11Z",
    "updated_at": "2024-10-24T23:11:07Z",
    "labels": [],
    "body": "When i try to run the start script i get this error message\r\n\r\n```\r\n(h2ogpt) C:\\Users\\domin\\Documents\\aiGen\\h2ogpt>python generate.py\r\nFontconfig error: Cannot load default config file: No such file: (null)\r\nC:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nWARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\nWindows fatal exception: code 0xc0000139\r\n\r\nCurrent thread 0x000007e4 (most recent call first):\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 1176 in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 571 in module_from_spec\r\n  File \"<frozen importlib._bootstrap>\", line 674 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\modules\\linear.py\", line 4 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\base.py\", line 16 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\mpt.py\", line 1 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\__init__.py\", line 1 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\__init__.py\", line 2 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\awq.py\", line 26 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\model.py\", line 50 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py\", line 20 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\__init__.py\", line 21 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\mapping.py\", line 22 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\auto.py\", line 32 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\__init__.py\", line 22 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\trainer.py\", line 215 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1050 in _gcd_import\r\n  ...\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1603, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\trainer.py\", line 215, in <module>\r\n    from peft import PeftModel\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\__init__.py\", line 22, in <module>\r\n    from .auto import (\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\auto.py\", line 32, in <module>\r\n    from .mapping import MODEL_TYPE_TO_PEFT_MODEL_MAPPING\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\mapping.py\", line 22, in <module>\r\n    from peft.tuners.xlora.model import XLoraModel\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\__init__.py\", line 21, in <module>\r\n    from .lora import LoraConfig, LoraModel, LoftQConfig, LoraRuntimeConfig\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py\", line 20, in <module>\r\n    from .model import LoraModel\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\model.py\", line 50, in <module>\r\n    from .awq import dispatch_awq\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\awq.py\", line 26, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\__init__.py\", line 2, in <module>\r\n    from awq.models.auto import AutoAWQForCausalLM\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\__init__.py\", line 1, in <module>\r\n    from .mpt import MptAWQForCausalLM\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\mpt.py\", line 1, in <module>\r\n    from .base import BaseAWQForCausalLM\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\base.py\", line 16, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\modules\\linear.py\", line 4, in <module>\r\n    import awq_inference_engine  # with CUDA kernels\r\nImportError: DLL load failed while importing awq_inference_engine: The specified procedure could not be found.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\domin\\Documents\\aiGen\\h2ogpt\\generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"C:\\Users\\domin\\Documents\\aiGen\\h2ogpt\\generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"C:\\Users\\domin\\Documents\\aiGen\\h2ogpt\\src\\utils.py\", line 79, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"C:\\Users\\domin\\Documents\\aiGen\\h2ogpt\\src\\gen.py\", line 2055, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"C:\\Users\\domin\\Documents\\aiGen\\h2ogpt\\src\\gpt_langchain.py\", line 550, in get_embedding\r\n    embedding = HuggingFaceBgeEmbeddings(model_name=hf_embedding_model,\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py\", line 287, in __init__\r\n    import sentence_transformers\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\sentence_transformers\\__init__.py\", line 17, in <module>\r\n    from sentence_transformers.trainer import SentenceTransformerTrainer\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\sentence_transformers\\trainer.py\", line 13, in <module>\r\n    from transformers import EvalPrediction, PreTrainedTokenizerBase, Trainer, TrainerCallback\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1593, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"C:\\Users\\domin\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1605, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\r\nDLL load failed while importing awq_inference_engine: The specified procedure could not be found.\r\n```\r\n\r\nI have gone though and followed eveything here https://github.com/h2oai/h2ogpt/blob/main/docs/README_WINDOWS.md, and eveything else ran without issue. \r\n\r\nAny ideas?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Same issues as here:\r\n\r\nhttps://github.com/h2oai/h2ogpt/issues/1835\r\nhttps://github.com/h2oai/h2ogpt/issues/1561\r\n\r\nMaybe try loading model from UI, the other person said.\r\n\r\nIf anyone with windows can help figure it out would be great."
      },
      {
        "user": "Domsmasher1",
        "body": "The issue is i dont even get to a stage where the ui boots, it crashes before that stage, unlike the other two which seem to get to the ui stage"
      },
      {
        "user": "pseudotensor",
        "body": "Try what was done for ooba:\r\n\r\nhttps://github.com/oobabooga/text-generation-webui/issues/4253#issuecomment-1932400867\r\n\r\n```\r\npip uninstall autoawq\r\ngit clone https://github.com/casper-hansen/AutoAWQ\r\ncd AutoAWQ\r\npip install -e .\r\n```"
      }
    ]
  },
  {
    "issue_number": 1848,
    "title": "Issue with Concurrent Query Processing and Document Upload",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-09-18T05:50:05Z",
    "updated_at": "2024-10-24T23:09:51Z",
    "labels": [],
    "body": "I have implemented a solution using vLLM on an A100 server to support multiple users. However, I have encountered an issue:\r\n\r\nWhile one user's query is being processed, other users are unable to upload documents into the `UserData` or MyData collections. The document upload process gets stuck at the processing stage without any errors appearing in the terminal or UI. Additionally, the document is not uploaded successfully.\r\n\r\nCan you suggest ways to decouple the query processing, document upload, and user interface programs so they can run independently of each other?\r\n\r\nAlternatively, can we build or use prebuilt separate APIs to manage program in the backend?\r\nPlease provide suggestions or potential solutions.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "They should all be independent unless you changed CONCURRENCY_COUNT to be 1.  This is tested normally.  The backend has no issues with this at all."
      },
      {
        "user": "pseudotensor",
        "body": "Once you have that working, I can explain how to make it even more efficient using the function_server."
      },
      {
        "user": "llmwesee",
        "body": "this is the command for running h2ogpt with login.\r\n`python generate.py --base_model=meta-llama/Meta-Llama-3.1-8B-Instruct  --score_model=None --langchain_mode='UserData' --user_path=user_path --auth='' --use_auth_token=True --visible_visible_models=False --max_seq_len=8192 --max_max_new_tokens=4096 --max_new_tokens=4096 --min_new_tokens=256\r\n`\r\ncan you show me some examples for having h2ogpt as fully backend server running with full functionality from query processing to document uploading for multiple users concurrently & independently . I want to integrated it's backend with react or next.js as frontend with having full functionality like as h2ogpt and having a datalake for all related document things "
      },
      {
        "user": "pseudotensor",
        "body": "I'd guess I'd need to ask how you see things blocked.  E.g. if you had a pytest test code that you are running that shows how things are blocking each other (e.g. long add of dock and then chat is blocked in another test you ran with -n 2) or you just show video of the UI and what you are doing, I can mimic it and see if I can see what you are seeing."
      },
      {
        "user": "pseudotensor",
        "body": "As for the function server, you can try it.  Just add to CLI:\r\n\r\n```\r\n --function_server=True --function_server_workers=5 --multiple_workers_gunicorn=True --function_server_port=5002 --function_api_key=API_KEY\r\n```"
      },
      {
        "user": "llmwesee",
        "body": "the function server has issue when hitting through `upload_api `and `add_file_api`  \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/abc/Documents/xxxx/xxxx/src/gpt_langchain.py\", line 9383, in update_user_db\r\n    return _update_user_db(file, db1s=db1s,\r\n  File \"/home/xxxx/src/gpt_langchain.py\", line 9664, in _update_user_db\r\n    sources = call_function_server('0.0.0.0', function_server_port, 'path_to_docs', (file,), simple_kwargs,\r\n  File \"/home/xxxx/src/function_client.py\", line 50, in call_function_server\r\n    execute_result = execute_function_on_server(host, port, function_name, args, kwargs, use_disk, use_pickle,\r\n  File \"/home/xxxx/src/function_client.py\", line 21, in execute_function_on_server\r\n    response = requests.post(url, json=payload, headers=headers)\r\n  File \"/home/xxxx/lib/python3.10/site-packages/requests/api.py\", line 115, in post\r\n    return request(\"post\", url, data=data, json=json, **kwargs)\r\n  File \"/home/xxxx/lib/python3.10/site-packages/requests/api.py\", line 59, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/home/xxxx/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/xxxx/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/xxxx/lib/python3.10/site-packages/requests/adapters.py\", line 700, in send\r\n    raise ConnectionError(e, request=request)\r\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='0.0.0.0', port=5002): Max retries exceeded with url: /execute_function/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1deb5867a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "It just looks like the function server isn't even up.  Perhaps you have something else on that port etc.  Check startup logs."
      },
      {
        "user": "llmwesee",
        "body": "> They should all be independent unless you changed CONCURRENCY_COUNT to be 1. This is tested normally. The backend has no issues with this at all.\r\n\r\nwhen setting concurrency count to be 64:\r\n\r\n`python generate.py --base_model=meta-llama/Meta-Llama-3.1-8B-Instruct  --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --visible_visible_models=False --max_seq_len=8192 --max_max_new_tokens=4096 --max_new_tokens=4096 --min_new_tokens=256 --api_open=True --allow_api=True --max_quality=True  --function_server=True --function_server_workers=5 --multiple_workers_gunicorn=True --function_server_port=5002 --function_api_key=API_KEY --concurrency_count=64`\r\n\r\nthen the following error is shown:\r\n\r\n```\r\nFile \"/home/xxxx/src/gen.py\", line 1736, in main\r\n    raise ValueError(\r\nValueError: Concurrency count > 1 will lead to mixup in cache use for local LLMs, disable this raise at own risk.\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "Correct, I recommend vLLM for handling concurrency well, transformers is not itself thread safe."
      }
    ]
  },
  {
    "issue_number": 1678,
    "title": "Source Link opened in the same tab",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-06-11T06:24:53Z",
    "updated_at": "2024-10-22T04:53:58Z",
    "labels": [],
    "body": "After got the answer and go to the source and hit the source link then it opened in the same tab instead of new tab. By looking the code, The link should open in the new tab always instead of same tab because we are targeting the blank. Then is it a Bug or something else?\r\n\r\n```\r\ndef get_url(x, from_str=False, short_name=False, font_size=2):\r\n    if not from_str:\r\n        source = x.metadata['source']\r\n    else:\r\n        source = x\r\n    if short_name:\r\n        source_name = get_short_name(source)\r\n    else:\r\n        source_name = source\r\n    if source.startswith('http://') or source.startswith('https://'):\r\n        return \"\"\"<font size=\"%s\"><a href=\"%s\" target=\"_blank\"  rel=\"noopener noreferrer\">%s</a></font>\"\"\" % (\r\n            font_size, source, source_name)\r\n    elif '<a href=' not in source:\r\n        return \"\"\"<font size=\"%s\"><a href=\"file/%s\" target=\"_blank\"  rel=\"noopener noreferrer\">%s</a></font>\"\"\" % (\r\n            font_size, source, source_name)\r\n    else:\r\n        # already filled\r\n        return source\r\n```",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Sorry for delay.\r\n\r\nI've never seen it open in same tab.  For me a PDF link leads to downloading of the PDF.\r\n\r\nFor images, it opens in new tab always for me.\r\n\r\nJust tried it again, seems all ok.\r\n\r\nCan you give a specific document and question where there is an issue?"
      },
      {
        "user": "llmwesee",
        "body": "I'm attaching the both PDF and also the screenshot for the reference. \r\n\r\n[Screencast from 28-06-24 09:45:42 AM IST.webm](https://github.com/h2oai/h2ogpt/assets/137979399/256c5ded-3f4d-4f68-a2dd-ef19b7cd1d31)\r\n\r\n[2210.14699v2.pdf](https://github.com/user-attachments/files/16024081/2210.14699v2.pdf)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "I see.  It must be because for me it downloads the PDF in Ubuntu, but for you it is instead opening the PDF up.  And you are using some modified code to get the correct page number, which is great."
      },
      {
        "user": "llmwesee",
        "body": "Yep, I want to open the PDF up. And for get the correct page number it doesn't work on cases where PDFs are the scanned copies."
      },
      {
        "user": "Sivakajan-tech",
        "body": "I would like to contribute to this issue. \r\n\r\nWindows often requires file paths to be formatted differently. The path used is formatted correctly for Window. Using file:/// for better cross-platform support:\r\n\r\n```\r\nreturn \"\"\"<font size=\"%s\"><a href=\"file:///%s\" target=\"_blank\"  rel=\"noopener noreferrer\">%s</a></font>\"\"\" % (\r\n```\r\n[Fix#1882](https://github.com/h2oai/h2ogpt/pull/1882)\r\nNote the three slashes (file:///) in the link. This makes the link an absolute path which can be more reliable across different platforms.\r\n\r\nRef: https://stackoverflow.com/questions/25354048/difference-between-file-and-file"
      }
    ]
  },
  {
    "issue_number": 1876,
    "title": "[Question] RAG retrieval method used in H2OGPT",
    "author": "chengchu88",
    "state": "closed",
    "created_at": "2024-10-16T18:40:09Z",
    "updated_at": "2024-10-16T20:59:20Z",
    "labels": [],
    "body": "Hello sir,\r\nI am assuming the H2O GPT uses langchain for RAG (document Q/A), is that correct?\r\nIf you could, would you kindly point me to the section/code where the retrieval is implemented, with the instruction input at the 'Ask or Ingest' as query? I have been looking at the gradio_runner and gpt_langchain but could not figure out where it is.\r\nThank you!",
    "comments": []
  },
  {
    "issue_number": 1862,
    "title": "Where is the thumbs up/down results?",
    "author": "yokomizo-tech",
    "state": "closed",
    "created_at": "2024-10-01T05:45:21Z",
    "updated_at": "2024-10-01T05:57:35Z",
    "labels": [
      "type/question"
    ],
    "body": "I find the results from the thumbs up/down buttons in chat tab useful; however, auth.json does not seem to store them. Are they stored somewhere else?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "https://github.com/h2oai/h2ogpt/blob/70ef4f859762bac1543b8557611bf28c1629176e/gradio_utils/prompt_form.py#L141\r\n\r\nSo reviews.csv if reviews_file is not set on CLI."
      }
    ]
  },
  {
    "issue_number": 1849,
    "title": "H2ogpt openai endpoints",
    "author": "CommitAndPray",
    "state": "closed",
    "created_at": "2024-09-18T10:25:19Z",
    "updated_at": "2024-09-30T15:38:09Z",
    "labels": [],
    "body": "Hi, can someone explain how I can test these endpoints? I keep getting the same error as shown in the screenshot. If I need to get a key, how can I obtain it? \r\n![image](https://github.com/user-attachments/assets/4cd7cc00-0406-46d4-95d2-e52a94dfe336)\r\n![image](https://github.com/user-attachments/assets/af26629a-4896-4ec3-ac1f-1028136da639)\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "OpenAI endpoints are on port 5000 by default."
      },
      {
        "user": "CommitAndPray",
        "body": "Thank you for responding. Do I need an API key to test the endpoints, or can I just set the api_key to empty"
      },
      {
        "user": "pseudotensor",
        "body": "EMPTY is default.   You can set the key, like I showed in the FAQ for how to setup gpt.h2o.ai\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/0e1597644dfee5a2b368d753d84086d966647bd4/docs/FAQ.md#L408"
      },
      {
        "user": "CommitAndPray",
        "body": "Thank you, I can now test H2O GPT on Open Web UI. My question is: will it use the documents I have already uploaded to H2O GPT, or will it follow the RAG process and parameters of Open Web UI? Does it interact with the files in my vector database?"
      },
      {
        "user": "pseudotensor",
        "body": "The database is separate.  On the backend operations would use h2oGPT except for the database."
      }
    ]
  },
  {
    "issue_number": 1845,
    "title": "Accessing and Uploading Files in MyData Using upload_api and add_file_api",
    "author": "llmwesee",
    "state": "closed",
    "created_at": "2024-09-13T11:23:16Z",
    "updated_at": "2024-09-30T06:26:20Z",
    "labels": [],
    "body": "**Problem Description:**\r\nI am uploading files to the `MyData `folder through the UI. The **sources_dir** already contains text files related to the LLM, `UserData`, and `MyData` which has information related to the files name. However, I am facing difficulty accessing the `MyData` folder and its files similarly to how I can access the` user_path` that contains documents uploaded to `UserData` through the UI.\r\n\r\n**Expected Behavior:**\r\nI would like to:\r\nAccess the` MyData` folder and retrieve the files stored in it, just like I can with the `user_path` for `UserData`.\r\nUpload files to both `MyData` and `UserData` using the existing `upload_api` and` add_file_api`.\r\n\r\n**Current Behavior:**\r\nI am able to upload files to `UserData `via the UI, but I cannot access or interact with `MyData `in a similar manner.\r\nUnclear on how to use the APIs to upload files directly to `MyData` and `UserData`.\r\n\r\n**Request:**\r\nCould you provide a solution or example code demonstrating how to:\r\n\r\nAccess and retrieve files from the `MyData `folder.\r\nUpload files to both `MyData` and `UserData` using `upload_api` and `add_file_api`.\r\n\r\n**Sample Code Request:**\r\nI would appreciate a code example illustrating the following use cases:\r\n\r\nUploading files to `MyData` and `UserData` using `upload_api` and `add_file_api`.\r\nAccessing and listing files in the `MyData` directory similarly to how it’s done with the `user_path`.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Personal scratch collections like MyData are not persisted unless you use enable the auth db so that users can login.\r\n\r\nI have a FAQ for the item you are asking about I think: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#per-user-database"
      }
    ]
  },
  {
    "issue_number": 1819,
    "title": "upload document via API",
    "author": "vitalyshalumov",
    "state": "closed",
    "created_at": "2024-08-28T11:54:21Z",
    "updated_at": "2024-09-30T06:23:41Z",
    "labels": [
      "type/question"
    ],
    "body": "Hello,\r\nHow can I upload a document via API and what formats does it support?\r\n\r\nThank you.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "See run_client_chat_stream_langchain_steps3 in tests/test_client_calls.py for example client code that does many things including upload using `upload_api` endpoint"
      }
    ]
  },
  {
    "issue_number": 1835,
    "title": "Get a lot of Errors in Webinterface",
    "author": "DrNokkel",
    "state": "closed",
    "created_at": "2024-09-10T09:42:56Z",
    "updated_at": "2024-09-28T22:51:08Z",
    "labels": [],
    "body": "I got these Error Signs and cant do anything on the webinterface.\r\n\r\nBefore I started to ge along with the webinterface i tested it in the cli which worked nicely.\r\npython generate.py --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --prompt_type=human_bot --cli=True\r\n\r\nIt`s my first time to work with such project I hope someone can help me\r\n\r\n![image](https://github.com/user-attachments/assets/1c35bc6e-ed36-4d71-ab73-e79510ba8a15)\r\n\r\n\r\n(h2ogpt) C:\\Windows\\System32\\h2ogpt>python generate.py --share=False --gradio_offline_level=1 --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --prompt_type=human_bot --load_8bit=True\r\nFontconfig error: Cannot load default config file: No such file: (null)\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nMust install langchain for transcription, disabling\r\nUsing Model h2oai/h2ogpt-4096-llama2-7b-chat\r\ngit failed to run: [WinError 2] The system cannot find the file specified\r\nMust install langchain for preloading embedding model, disabling\r\nMust install DocTR and LangChain installed if enabled DocTR, disabling\r\nStarting get_model: h2oai/h2ogpt-4096-llama2-7b-chat\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\ndevice_map: {'': 0}\r\nLoading checkpoint shards: 100%|██████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.81s/it]\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nBegin auto-detect HF cache text generation models\r\nEnd auto-detect HF cache text generation models\r\nBegin auto-detect llama.cpp models\r\nEnd auto-detect llama.cpp models\r\nRunning on local URL:  http://0.0.0.0:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStarted Gradio Server and/or GUI: server_name: localhost port: None\r\nUse local URL: http://localhost:7860/\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_lock\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_info\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_names\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nverbose disabled\r\nOpenAI API URL: http://0.0.0.0:5000\r\nOpenAI API key: EMPTY\r\nINFO:     127.0.0.1:49761 - \"GET / HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:49761 - \"GET /info HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:49761 - \"GET /theme.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:49762 - \"GET /heartbeat/po69u1hn4b8 HTTP/1.1\" 200 OK\r\nClient Connected: po69u1hn4b8\r\nINFO:     127.0.0.1:49761 - \"POST /queue/join HTTP/1.1\" 500 Internal Server Error\r\nException in ASGI application\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 210, in __init__\r\n    core_schema = _getattr_no_parents(type, '__pydantic_core_schema__')\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 98, in _getattr_no_parents\r\n    raise AttributeError(attribute)\r\nAttributeError: __pydantic_core_schema__\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 401, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\gradio\\route_utils.py\", line 695, in __call__\r\n    await self.simple_response(scope, receive, send, request_headers=headers)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\gradio\\route_utils.py\", line 711, in simple_response\r\n    await self.app(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 73, in app\r\n    response = await f(request)\r\n               ^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\routing.py\", line 291, in app\r\n    solved_result = await solve_dependencies(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 639, in solve_dependencies\r\n    ) = await request_body_to_args(  # body_params checked above\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 813, in request_body_to_args\r\n    fields_to_extract = get_model_fields(first_field.type_)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\_compat.py\", line 284, in get_model_fields\r\n    ModelField(field_info=field_info, name=name)\r\n  File \"<string>\", line 6, in __init__\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\_compat.py\", line 109, in __post_init__\r\n    self._type_adapter: TypeAdapter[Any] = TypeAdapter(\r\n                                           ^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 212, in __init__\r\n    core_schema = _get_schema(type, config_wrapper, parent_depth=_parent_depth + 1)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 81, in _get_schema\r\n    schema = gen.generate_schema(type_)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 499, in generate_schema\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 734, in _generate_schema_inner\r\n    return self._annotated_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1749, in _annotated_schema\r\n    schema = self._apply_annotations(source_type, annotations)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1817, in _apply_annotations\r\n    schema = get_inner_schema(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 82, in __call__\r\n    schema = self._handler(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1899, in new_handler\r\n    schema = metadata_get_schema(source, get_inner_schema)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1895, in <lambda>\r\n    lambda source, handler: handler(source)\r\n                            ^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 82, in __call__\r\n    schema = self._handler(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1798, in inner_handler\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 755, in _generate_schema_inner\r\n    return self.match_type(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 837, in match_type\r\n    return self._match_generic_type(obj, origin)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 861, in _match_generic_type\r\n    return self._union_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1149, in _union_schema\r\n    choices.append(self.generate_schema(arg))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 499, in generate_schema\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 755, in _generate_schema_inner\r\n    return self.match_type(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 841, in match_type\r\n    return self._unknown_type_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 402, in _unknown_type_schema\r\n    raise PydanticSchemaGenerationError(\r\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'starlette.requests.Request'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\r\n\r\nIf you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\r\n\r\nFor further information visit https://errors.pydantic.dev/2.7/u/schema-for-unknown-type\r\nINFO:     127.0.0.1:49763 - \"POST /queue/join HTTP/1.1\" 500 Internal Server Error\r\nException in ASGI application\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 210, in __init__\r\n    core_schema = _getattr_no_parents(type, '__pydantic_core_schema__')\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 98, in _getattr_no_parents\r\n    raise AttributeError(attribute)\r\nAttributeError: __pydantic_core_schema__\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 401, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\gradio\\route_utils.py\", line 695, in __call__\r\n    await self.simple_response(scope, receive, send, request_headers=headers)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\gradio\\route_utils.py\", line 711, in simple_response\r\n    await self.app(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\starlette\\routing.py\", line 73, in app\r\n    response = await f(request)\r\n               ^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\routing.py\", line 291, in app\r\n    solved_result = await solve_dependencies(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 639, in solve_dependencies\r\n    ) = await request_body_to_args(  # body_params checked above\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\dependencies\\utils.py\", line 813, in request_body_to_args\r\n    fields_to_extract = get_model_fields(first_field.type_)\r\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\_compat.py\", line 284, in get_model_fields\r\n    ModelField(field_info=field_info, name=name)\r\n  File \"<string>\", line 6, in __init__\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\fastapi\\_compat.py\", line 109, in __post_init__\r\n    self._type_adapter: TypeAdapter[Any] = TypeAdapter(\r\n                                           ^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 212, in __init__\r\n    core_schema = _get_schema(type, config_wrapper, parent_depth=_parent_depth + 1)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\type_adapter.py\", line 81, in _get_schema\r\n    schema = gen.generate_schema(type_)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 499, in generate_schema\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 734, in _generate_schema_inner\r\n    return self._annotated_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1749, in _annotated_schema\r\n    schema = self._apply_annotations(source_type, annotations)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1817, in _apply_annotations\r\n    schema = get_inner_schema(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 82, in __call__\r\n    schema = self._handler(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1899, in new_handler\r\n    schema = metadata_get_schema(source, get_inner_schema)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1895, in <lambda>\r\n    lambda source, handler: handler(source)\r\n                            ^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_schema_generation_shared.py\", line 82, in __call__\r\n    schema = self._handler(source_type)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1798, in inner_handler\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 755, in _generate_schema_inner\r\n    return self.match_type(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 837, in match_type\r\n    return self._match_generic_type(obj, origin)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 861, in _match_generic_type\r\n    return self._union_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 1149, in _union_schema\r\n    choices.append(self.generate_schema(arg))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 499, in generate_schema\r\n    schema = self._generate_schema_inner(obj)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 755, in _generate_schema_inner\r\n    return self.match_type(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 841, in match_type\r\n    return self._unknown_type_schema(obj)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py\", line 402, in _unknown_type_schema\r\n    raise PydanticSchemaGenerationError(\r\npydantic.errors.PydanticSchemaGenerationError: Unable to generate pydantic-core schema for <class 'starlette.requests.Request'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\r\n\r\nIf you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\r\n\r\nFor further information visit https://errors.pydantic.dev/2.7/u/schema-for-unknown-type",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Sounds like the installation is messed up.  How did you install?  Did you use the predefined script in a clean env?"
      },
      {
        "user": "DrNokkel",
        "body": "Yes i followed step by step the windows installation. After that i tried to fix it myself . I will try a new installation. "
      },
      {
        "user": "DrNokkel",
        "body": "(h2ogpt) C:\\Windows\\System32\\h2ogpt>python generate.py --share=False --gradio_offline_level=1 --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --prompt_type=human_bot --load_8bit=True\r\nFontconfig error: Cannot load default config file: No such file: (null)\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nNo GPUs detected\r\nUsing Model h2oai/h2ogpt-4096-llama2-7b-chat\r\nWARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\r\n  warn_deprecated(\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1603, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\trainer.py\", line 215, in <module>\r\n    from peft import PeftModel\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\__init__.py\", line 22, in <module>\r\n    from .auto import (\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\auto.py\", line 32, in <module>\r\n    from .mapping import MODEL_TYPE_TO_PEFT_MODEL_MAPPING\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\mapping.py\", line 22, in <module>\r\n    from peft.tuners.xlora.model import XLoraModel\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\__init__.py\", line 21, in <module>\r\n    from .lora import LoraConfig, LoraModel, LoftQConfig, LoraRuntimeConfig\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py\", line 20, in <module>\r\n    from .model import LoraModel\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\model.py\", line 50, in <module>\r\n    from .awq import dispatch_awq\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\peft\\tuners\\lora\\awq.py\", line 26, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\__init__.py\", line 2, in <module>\r\n    from awq.models.auto import AutoAWQForCausalLM\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\__init__.py\", line 1, in <module>\r\n    from .mpt import MptAWQForCausalLM\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\mpt.py\", line 1, in <module>\r\n    from .base import BaseAWQForCausalLM\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\models\\base.py\", line 16, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\awq\\modules\\linear.py\", line 4, in <module>\r\n    import awq_inference_engine  # with CUDA kernels\r\nImportError: DLL load failed while importing awq_inference_engine: The specified module could not be found.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Windows\\System32\\h2ogpt\\generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"C:\\Windows\\System32\\h2ogpt\\generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\utils.py\", line 78, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\fire\\core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gen.py\", line 2044, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 562, in get_embedding\r\n    embedding = HuggingFaceEmbeddings(model_name=hf_embedding_model, model_kwargs=model_kwargs)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 203, in warn_if_direct_instance\r\n    return wrapped(self, *args, **kwargs)\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py\", line 71, in __init__\r\n    import sentence_transformers\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\sentence_transformers\\__init__.py\", line 15, in <module>\r\n    from sentence_transformers.trainer import SentenceTransformerTrainer\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\sentence_transformers\\trainer.py\", line 10, in <module>\r\n    from transformers import EvalPrediction, PreTrainedTokenizerBase, Trainer, TrainerCallback\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1593, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"C:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1605, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\r\nDLL load failed while importing awq_inference_engine: The specified module could not be found.\r\n\r\nAfter a fresh installation i got this error.\r\nLast time i tried to fix it by myself until the Webinterface errors"
      },
      {
        "user": "pseudotensor",
        "body": "For the awq issue, does this help if you install this?\r\n```\r\npip install autoawq-kernels -c reqs_optional/reqs_constraints.txt\r\n```\r\n\r\nIn your original post, I'm guessing that you were further along, but things failed during use of the UI?"
      },
      {
        "user": "DrNokkel",
        "body": "(h2ogpt) C:\\Users\\thies\\h2ogpt>python generate.py --share=False --gradio_offline_level=1 --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --prompt_type=human_bot --load_8bit=True\r\nFontconfig error: Cannot load default config file: No such file: (null)\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nNo GPUs detected\r\nsoundfile, librosa, and wavio not installed, disabling STT\r\nsoundfile, librosa, and wavio not installed, disabling TTS\r\nUsing Model h2oai/h2ogpt-4096-llama2-7b-chat\r\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\r\n  warn_deprecated(\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\r\n  warnings.warn(\r\nStarting get_model: h2oai/h2ogpt-4096-llama2-7b-chat\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nLoading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:14<00:00,  7.25s/it]\r\nBegin auto-detect HF cache text generation models\r\nEnd auto-detect HF cache text generation models\r\nBegin auto-detect llama.cpp models\r\nEnd auto-detect llama.cpp models\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\gradio\\components\\dropdown.py:179: UserWarning: The value passed into gr.Dropdown() is not in the list of choices. Please update the list of choices to include: None or set allow_custom_value=True.\r\n  warnings.warn(\r\nRunning on local URL:  http://0.0.0.0:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStarted Gradio Server and/or GUI: server_name: localhost port: None\r\nUse local URL: http://localhost:7860/\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_lock\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_info\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_names\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nverbose disabled\r\nOpenAI API URL: http://0.0.0.0:5000\r\nOpenAI API key: EMPTY\r\nINFO:     127.0.0.1:51867 - \"GET / HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51867 - \"GET /info HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51867 - \"GET /favicon.ico HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51867 - \"GET /theme.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51867 - \"GET /heartbeat/rz68hpe3k2k HTTP/1.1\" 200 OK\r\nClient Connected: rz68hpe3k2k\r\nINFO:     127.0.0.1:51875 - \"GET / HTTP/1.1\" 200 OK\r\nClient disconnected: rz68hpe3k2k\r\nINFO:     127.0.0.1:51875 - \"GET /info HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51875 - \"GET /theme.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51876 - \"GET /heartbeat/ec6wxy0tk2o HTTP/1.1\" 200 OK\r\nClient Connected: ec6wxy0tk2o\r\nINFO:     127.0.0.1:51888 - \"GET / HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51888 - \"GET /assets/index-380480d1.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51888 - \"GET /assets/index-4ea145fa.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51888 - \"GET /assets/svelte/svelte.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51888 - \"GET /assets/Index-3a0c450d.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51888 - \"GET /assets/Index-c88eb5f1.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51888 - \"GET /info HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51888 - \"GET /theme.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51888 - \"GET /assets/SelectSource-ffeae268.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51888 - \"GET /assets/Blocks-a37e4ec0.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51888 - \"GET /assets/Blocks-c34a3b8c.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51888 - \"GET /assets/SelectSource.svelte_svelte_type_style_lang-adcf3c44.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51888 - \"GET /heartbeat/91e1jxkuq56 HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/api-logo-5346f193.svg HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/logo-3707f936.svg HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-79eb3848.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Example-09c7df8b.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-edf307d2.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-93c91554.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-2853eb31.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-8f1feca1.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Example-98fc2b2c.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-7f696158.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-7a433ab1.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-76c3ee3f.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-3812b7f1.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Textbox-dde6f8cc.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Example-6ded08d8.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/ModifyUpload-13b13e32.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/FileUpload-a4fc0425.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Example-1cda6415.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-51c40da3.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Image-c2f962bb.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/ImageUploader-b33972d3.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Example-bcfbe567.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Video-a80c372b.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Example-9429a5cb.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/index-5522f595.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Tabs-a57f85ff.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-d43fcb36.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-329f8260.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-efd1eb27.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Index-8af74d56.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Example-da1a7264.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Index-83656a6f.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-b658ebcd.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /custom_component/98a833ceec073ac17ed81e096eb6a8cf/component/style.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/index-da1c5d77.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Index-8f24ac6d.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-8c6a6015.css HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Index-a00e552c.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index.svelte_svelte_type_style_lang-0afb02c2.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/utils-572af92b.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Example.svelte_svelte_type_style_lang-2c410615.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/prism-python-73fafd54.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-b2fbe4c6.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Example-b7e49832.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Index-2ea49b90.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-2d00126c.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Index-b88bd069.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-667acb08.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/BlockTitle-341cd4bd.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Info-9c6ce410.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Index-4519e1de.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Example-36c18249.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/DropdownArrow-c1184342.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Example-e7bf77ec.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Index-4d98cf33.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-46e5737c.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Index-073ecbe4.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Textbox-105527d7.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Check-965babbe.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Copy-b365948f.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Example-d6a316ec.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-3bc00d54.js HTTP/1.1\" 200 OK\r\nClient Connected: 91e1jxkuq56\r\nINFO:     127.0.0.1:51903 - \"GET /assets/FileUpload-45ee7e35.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Empty-e94c8b62.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/BlockLabel-2524f0c6.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/File-d0b52941.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/ModifyUpload-75c48d83.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/ModifyUpload.svelte_svelte_type_style_lang-5d7cfa29.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/IconButton-98904c3d.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/DownloadLink-2061f7f7.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Clear-2c7bae91.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/file-url-65353b28.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Undo-b088de14.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/UploadText-b6444409.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Upload-351cc897.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Example-e82c91a3.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Index-36d4562e.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/ImageUploader-f5fe978e.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-693f6e84.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/ShareButton-b22e3a12.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Image-eaba773f.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Image-c454d182.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/SelectSource-b0a5278e.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Example-487e4071.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/index-d5419efa.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Video-99f28ea4.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Video-8670328f.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Trim-1b343e72.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Example-490940c1.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-2d295f17.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Tabs-33cd00ce.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Index-2d3c42f0.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-be7c0f16.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Index-36d2ae00.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/Index-36a0cae0.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/index-2f00b72c.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Index-5edcd718.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /assets/dsv-576afacd.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Example-4e86bea7.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Index-0c1d215c.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/index-c455d978.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51902 - \"GET /custom_component/98a833ceec073ac17ed81e096eb6a8cf/component/index.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Music-755043aa.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Example-8632de63.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51903 - \"GET /assets/Index-aa84147a.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /assets/Index-73a5f22f.js HTTP/1.1\" 200 OK\r\nINFO:     127.0.0.1:51901 - \"GET /favicon.ico HTTP/1.1\" 200 OK\r\nClient disconnected: ec6wxy0tk2o\r\n\r\nIt started now but these parts looking nit right for me \r\n\r\n\r\nFontconfig error: Cannot load default config file: No such file: (null)\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nNo GPUs detected\r\nsoundfile, librosa, and wavio not installed, disabling STT\r\nsoundfile, librosa, and wavio not installed, disabling TTS\r\n\r\n\r\n\r\nI have an GPU installed \r\n+-----------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 552.23                 Driver Version: 552.23         CUDA Version: 12.4     |\r\n|-----------------------------------------+------------------------+----------------------+\r\n| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                        |               MIG M. |\r\n|=========================================+========================+======================|\r\n|   0  NVIDIA RTX 2000 Ada Gene...  WDDM  |   00000000:01:00.0  On |                  Off |\r\n| 30%   26C    P8              4W /   70W |     349MiB /  16380MiB |      2%      Default |\r\n|                                         |                        |                  N/A |\r\n+-----------------------------------------+------------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------------------+\r\n| Processes:                                                                              |\r\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n|        ID   ID                                                               Usage      |\r\n|=========================================================================================|\r\n|    0   N/A  N/A      2716    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\r\n|    0   N/A  N/A      5212    C+G   ..._8wekyb3d8bbwe\\Microsoft.Photos.exe      N/A      |\r\n|    0   N/A  N/A      6880    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\r\n|    0   N/A  N/A      7036    C+G   C:\\Windows\\explorer.exe                     N/A      |\r\n|    0   N/A  N/A      7656    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\r\n|    0   N/A  N/A      7680    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\r\n|    0   N/A  N/A      9848    C+G   ...on\\128.0.2739.67\\msedgewebview2.exe      N/A      |\r\n|    0   N/A  N/A      9948    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\r\n+-----------------------------------------------------------------------------------------+\r\n\r\nI can open the localhost link. But its stuck in this screen\r\n![image](https://github.com/user-attachments/assets/15c0da91-60ac-417c-8bce-20a1f370612e)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Interesting on the stuck screen.  Same as https://github.com/h2oai/h2ogpt/issues/1834\r\n\r\n"
      },
      {
        "user": "DrNokkel",
        "body": "> For the awq issue, does this help if you install this?\r\n> \r\n> ```\r\n> pip install autoawq-kernels -c reqs_optional/reqs_constraints.txt\r\n> ```\r\n> \r\n> In your original post, I'm guessing that you were further along, but things failed during use of the UI?\r\n\r\nYes i couldnt press anything in the ui , everytime I pressed  a button more error messages popping up\r\n"
      },
      {
        "user": "DrNokkel",
        "body": "> Interesting on the stuck screen. Same as #1834\r\nThanks for the support ill try it this way\r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Could you try doing:\r\n```\r\npip uninstall gradio gradio_client -y\r\npip install gradio==3.50.2 # also installs client\r\n```\r\n\r\nand see if anything changed?"
      },
      {
        "user": "DrNokkel",
        "body": "Yes its helping thanks\r\n"
      },
      {
        "user": "DrNokkel",
        "body": "Its just a bit slow. I think ive got another problem with no gpu detecting.\r\n\r\nBut really nice support here\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "I recommend using GGUF of some modern model.  The `h2oai/h2ogpt-4096-llama2-7b-chat` you chose is super old and without GGUF requires beefier GPU."
      },
      {
        "user": "DrNokkel",
        "body": "Okay thanks.\r\n\r\n\r\n(h2ogpt) C:\\Windows\\System32\\h2ogpt>python generate.py --share=False --gradio_offline_level=1 --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --prompt_type=human_bot --load_8bit=True\r\nFontconfig error: Cannot load default config file: No such file: (null)\r\nC:\\ProgramData\\miniconda3\\envs\\h2ogpt\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\n*No GPUs detected*\r\n\r\nIm just suprised why theres no gpu detected"
      },
      {
        "user": "DrNokkel",
        "body": "Getting this now \r\n\r\n(h2ogpt2) C:\\Windows\\System32\\h2ogpt>python generate.py --share=False --gradio_offline_level=1 --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --prompt_type=human_bot --load_8bit=True\r\nFontconfig error: Cannot load default config file: No such file: (null)\r\nC:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nUsing Model h2oai/h2ogpt-4096-llama2-7b-chat\r\nWARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\nWindows fatal exception: code 0xc0000139\r\n\r\nCurrent thread 0x00001e14 (most recent call first):\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 1176 in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 571 in module_from_spec\r\n  File \"<frozen importlib._bootstrap>\", line 674 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\awq\\modules\\linear.py\", line 4 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\awq\\models\\base.py\", line 16 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\awq\\models\\mpt.py\", line 1 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\awq\\models\\__init__.py\", line 1 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\awq\\__init__.py\", line 2 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\tuners\\lora\\awq.py\", line 26 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\tuners\\lora\\model.py\", line 50 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py\", line 20 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\tuners\\__init__.py\", line 21 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 992 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\mapping.py\", line 22 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\auto.py\", line 32 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\__init__.py\", line 22 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\transformers\\trainer.py\", line 215 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1050 in _gcd_import\r\n  ...\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1603, in _get_module\r\n    return importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\transformers\\trainer.py\", line 215, in <module>\r\n    from peft import PeftModel\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\__init__.py\", line 22, in <module>\r\n    from .auto import (\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\auto.py\", line 32, in <module>\r\n    from .mapping import MODEL_TYPE_TO_PEFT_MODEL_MAPPING\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\mapping.py\", line 22, in <module>\r\n    from peft.tuners.xlora.model import XLoraModel\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\tuners\\__init__.py\", line 21, in <module>\r\n    from .lora import LoraConfig, LoraModel, LoftQConfig, LoraRuntimeConfig\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py\", line 20, in <module>\r\n    from .model import LoraModel\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\tuners\\lora\\model.py\", line 50, in <module>\r\n    from .awq import dispatch_awq\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\peft\\tuners\\lora\\awq.py\", line 26, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\awq\\__init__.py\", line 2, in <module>\r\n    from awq.models.auto import AutoAWQForCausalLM\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\awq\\models\\__init__.py\", line 1, in <module>\r\n    from .mpt import MptAWQForCausalLM\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\awq\\models\\mpt.py\", line 1, in <module>\r\n    from .base import BaseAWQForCausalLM\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\awq\\models\\base.py\", line 16, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\awq\\modules\\linear.py\", line 4, in <module>\r\n    import awq_inference_engine  # with CUDA kernels\r\nImportError: DLL load failed while importing awq_inference_engine: The specified procedure could not be found.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Windows\\System32\\h2ogpt\\generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"C:\\Windows\\System32\\h2ogpt\\generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\utils.py\", line 79, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\fire\\core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\fire\\core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\fire\\core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gen.py\", line 2044, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 550, in get_embedding\r\n    embedding = HuggingFaceBgeEmbeddings(model_name=hf_embedding_model,\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\langchain_community\\embeddings\\huggingface.py\", line 287, in __init__\r\n    import sentence_transformers\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\sentence_transformers\\__init__.py\", line 17, in <module>\r\n    from sentence_transformers.trainer import SentenceTransformerTrainer\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\sentence_transformers\\trainer.py\", line 13, in <module>\r\n    from transformers import EvalPrediction, PreTrainedTokenizerBase, Trainer, TrainerCallback\r\n  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1593, in __getattr__\r\n    module = self._get_module(self._class_to_module[name])\r\n  File \"C:\\Users\\das\\miniconda3\\envs\\h2ogpt2\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1605, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.trainer because of the following error (look up to see its traceback):\r\nDLL load failed while importing awq_inference_engine: The specified procedure could not be found.\r\n\r\n\r\n\r\nI try this but it doesn't helped\r\n\r\n\r\n> For the awq issue, does this help if you install this?\r\n> \r\n> ```\r\n> pip install autoawq-kernels -c reqs_optional/reqs_constraints.txt\r\n> ```\r\n> \r\n> In your original post, I'm guessing that you were further along, but things failed during use of the UI?\r\n\r\nAnything what im doing wrong?\r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "I'm unsure, seems like peft needs awq and that fails on your system.  I would avoid 8bit loading and just use GGUF."
      },
      {
        "user": "pseudotensor",
        "body": "The gradio pydantic thing is fixed: https://github.com/h2oai/h2ogpt/commit/be5f37ef283ebd4321bc5f40980accf5d9822e6c"
      }
    ]
  },
  {
    "issue_number": 1834,
    "title": "Unable to get the login screen",
    "author": "rohitnanda1443",
    "state": "closed",
    "created_at": "2024-09-10T05:56:52Z",
    "updated_at": "2024-09-28T22:49:50Z",
    "labels": [],
    "body": "Hi I am running a fresh install of H2O-GPT: \r\n\r\nRunning script is: python generate.py --inference_server=\"vllm:0.0.0.0:5001\" --guest_name=\"\"  --enable_tts=False --enable_stt=False --base_model=mistralai/Mistral-7B-Instruct-v0.3 --enable_transcriptions=False & \r\n\r\nEven if we run without an inference server we get the same issue. Has there been any change in the gradio server files? \r\n\r\n![error](https://github.com/user-attachments/assets/c858df92-24de-44f5-9edc-777db9682b82)\r\n\r\n\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, nothing seems wrong when I run a similar command.  I recommend checking the dev console in chrome browser for errors and posting or fixing."
      },
      {
        "user": "pseudotensor",
        "body": "In some cases in past, I've noticed that the browser gets messed up, and one has to reset its cache.\r\n\r\n![image](https://github.com/user-attachments/assets/f58c6117-8b46-442c-9bae-ce9ba38cfd86)\r\n"
      },
      {
        "user": "rohitnanda1443",
        "body": "Tried clearing the cache out and retried. Got the same issue.  \r\n\r\nInstallation steps:\r\n\r\n1. Used: Vast.ai to get a server (Ubuntu with Cuda 12.1)\r\n2. Installed H2O-GPT fallowing the Linux install steps\r\n3. Installed Ngrok \r\n4. started H20 gradio server \r\n5. started Ngrok and got a web URL. \r\n\r\nThis used to work previously. (Has there been any changes in the Gradio libraries?) \r\n"
      },
      {
        "user": "rohitnanda1443",
        "body": "![image](https://github.com/user-attachments/assets/c602848f-1c9d-4844-88a4-ae3904888aa0)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Nothing comes to mind except that perhaps the env is not installed correctly and you have (e.g.) too new a gradio or something."
      },
      {
        "user": "rohitnanda1443",
        "body": "Did a fresh install couple of days back. Should i give you the ngrok link.\r\n\r\nOn Tue, 10 Sept 2024, 12:59 pseudotensor, ***@***.***> wrote:\r\n\r\n> Nothing comes to mind except that perhaps the env is not installed\r\n> correctly and you have (e.g.) too new a gradio or something.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/h2oai/h2ogpt/issues/1834#issuecomment-2339882518>, or\r\n> unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BGADUMGVJT6RLWIWNPZDFCDZV2NXFAVCNFSM6AAAAABN56WL4CVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGMZZHA4DENJRHA>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "To be clear, alot of work in h2oGPT repo in last month has been on agents backend in openai_server that wouldn't be affecting anything for normal users.  So I'd guess something must have changed or broken for installation.\r\n\r\nCan you do `pip freeze` and share the results?"
      },
      {
        "user": "rohitnanda1443",
        "body": "The gradio is old one 4.26.0 \r\n\r\n`absl-py==2.1.0\r\naccelerate==0.34.2\r\naiofiles==23.2.1\r\naiohappyeyeballs==2.4.0\r\naiohttp==3.10.5\r\naiosignal==1.3.1\r\naltair==5.4.1\r\nannotated-types==0.7.0\r\nanthropic==0.25.8\r\nantlr4-python3-runtime==4.9.3\r\nanyascii==0.3.2\r\nanyio==4.4.0\r\nappdirs==1.4.4\r\nAPScheduler==3.10.4\r\nargcomplete==3.5.0\r\narxiv==1.4.8\r\nasgiref==3.8.1\r\nasync-timeout==4.0.3\r\nattrs==24.2.0\r\naudioread==3.0.1\r\nAuthlib==1.3.2\r\nauto_gptq==0.7.1\r\nautoawq==0.2.5\r\nautoawq_kernels==0.0.6\r\nbabel==2.16.0\r\nbackoff==2.2.1\r\nbackports.tarfile==1.2.0\r\nbangla==0.0.2\r\nbcrypt==4.2.0\r\nbeautifulsoup4==4.12.3\r\nbioc==2.1\r\nbitsandbytes==0.43.3\r\nblinker==1.8.2\r\nblis==0.7.11\r\nbnnumerizer==0.0.2\r\nbnunicodenormalizer==0.1.7\r\nboto3==1.35.14\r\nbotocore==1.35.14\r\nBrotli==1.1.0\r\nbs4==0.0.2\r\nbuild==1.2.2\r\ncachetools==5.5.0\r\ncatalogue==2.0.10\r\ncertifi==2024.8.30\r\ncffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1725560520483/work\r\nchardet==5.2.0\r\ncharset-normalizer==3.3.2\r\nchroma-hnswlib==0.7.3\r\nchromadb==0.4.23\r\nclick==8.1.7\r\ncloudpathlib==0.19.0\r\ncolorama==0.4.6\r\ncoloredlogs==15.0.1\r\nconfection==0.1.5\r\ncontourpy==1.2.1\r\ncoqpit==0.0.17\r\ncramjam==2.8.3\r\ncryptography==43.0.1\r\ncssselect2==0.2.1\r\ncutlet==0.3.0\r\ncycler==0.12.1\r\ncymem==2.0.8\r\nCython==3.0.11\r\ndacite==1.7.0\r\ndataclasses-json==0.6.7\r\ndatasets==2.21.0\r\ndateparser==1.1.8\r\ndecorator==5.1.1\r\ndeepdiff==8.0.1\r\ndeepspeed==0.15.1\r\ndefusedxml==0.7.1\r\nDeprecated==1.2.14\r\ndiffusers==0.24.0\r\ndill==0.3.8\r\ndiskcache==5.6.3\r\ndistro==1.9.0\r\ndnspython==2.6.1\r\ndocopt==0.6.2\r\ndocutils==0.20.1\r\ndocx2txt==0.8\r\nduckdb @ https://h2o-release.s3.amazonaws.com/h2ogpt/duckdb-0.8.2.dev4025%2Bg9698e9e6a8.d20230907-cp310-cp310-linux_x86_64.whl#sha256=0908cf14585903aae039ef36d769420fe0265d17de6440bb4eabab27a1bdde0f\r\nduckduckgo_search==6.2.11\r\neffdet==0.4.1\r\neinops==0.8.0\r\nemoji==2.12.1\r\nencodec==0.1.1\r\net-xmlfile==1.1.0\r\neval_type_backport==0.2.0\r\nevaluate==0.4.0\r\nexceptiongroup==1.2.2\r\nexecnet==2.1.1\r\nexllama @ https://github.com/jllllll/exllama/releases/download/0.0.18/exllama-0.0.18+cu121-cp310-cp310-linux_x86_64.whl#sha256=b4e4a8cb699757880cc2c96f8d3a35b6dd9f1fba73208c59f2f9dab9d49c4fae\r\nexllamav2==0.0.16\r\nfaiss-gpu==1.7.2\r\nfastapi==0.114.0\r\nfastparquet==2024.5.0\r\nfeedparser==6.0.11\r\nffmpeg==1.4\r\nffmpeg-python==0.2.0\r\nffmpy==0.4.0\r\nfiftyone==0.25.1\r\nfiftyone-brain==0.17.0\r\nfiftyone_db==1.1.5\r\nfilelock==3.16.0\r\nfiletype==1.2.0\r\nfire==0.5.0\r\nflash-attn==2.6.3\r\nFlask==3.0.3\r\nflatbuffers==24.3.25\r\nfonttools @ file:///home/conda/feedstock_root/build_artifacts/fonttools_1725391240115/work\r\nfrozenlist==1.4.1\r\nfsspec==2024.6.1\r\nftfy==6.2.3\r\nfugashi==1.3.2\r\nfuture==1.0.0\r\ng2pkk==0.1.2\r\ngekko==1.2.1\r\nglob2==0.7\r\ngoogle-ai-generativelanguage==0.6.2\r\ngoogle-api-core==2.19.2\r\ngoogle-api-python-client==2.144.0\r\ngoogle-auth==2.34.0\r\ngoogle-auth-httplib2==0.2.0\r\ngoogle-generativeai==0.5.2\r\ngoogle_search_results==2.4.2\r\ngoogleapis-common-protos==1.65.0\r\ngpt4all==1.0.5\r\ngradio @ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl#sha256=922798c98b4954bd76a78633f85c2a91c9c8df3018a0237928b193033a78c0a5\r\ngradio_client @ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio_client-0.15.1-py3-none-any.whl#sha256=38d8f877aa2fcb9b56ddee352085f3ca58977f35b4da0853758053fbd6084ea1\r\ngradio_pdf==0.0.15\r\ngradio_tools==0.0.9\r\ngraphql-core==3.2.4\r\ngreenlet==3.0.3\r\ngroq==0.5.0\r\ngrpcio==1.66.1\r\ngrpcio-status==1.62.3\r\ngrpcio-tools==1.62.3\r\ngruut==2.2.3\r\ngruut-ipa==0.13.0\r\ngruut_lang_de==2.0.1\r\ngruut_lang_en==2.0.1\r\ngruut_lang_es==2.0.1\r\ngruut_lang_fr==2.0.2\r\nh11==0.14.0\r\nh2==4.1.0\r\nh5py==3.11.0\r\nhangul-romanize==0.1.0\r\nhf_transfer==0.1.8\r\nhjson==3.1.0\r\nhpack==4.0.0\r\nhtml2text==2024.2.26\r\nhtml5lib @ file:///home/conda/feedstock_root/build_artifacts/html5lib_1592930327044/work\r\nhttpcore==1.0.5\r\nhttplib2==0.22.0\r\nhttptools==0.6.1\r\nhttpx==0.25.2\r\nhttpx-sse==0.4.0\r\nhuggingface-hub==0.24.6\r\nhumanfriendly==10.0\r\nhumanize==4.10.0\r\nHypercorn==0.17.3\r\nhyperframe==6.0.1\r\nidna==3.8\r\nimageio==2.35.1\r\nimportlib_metadata==8.4.0\r\nimportlib_resources==6.4.4\r\nimutils==0.5.4\r\ninflate64==1.0.0\r\ninflect==7.4.0\r\niniconfig==2.0.0\r\nInstructorEmbedding==1.0.1\r\nintervaltree==3.1.0\r\niopath==0.1.10\r\nitsdangerous==2.2.0\r\njaconv==0.4.0\r\njamo==0.4.1\r\njaraco.context==6.0.1\r\njieba==0.42.1\r\nJinja2==3.1.4\r\njiter==0.5.0\r\njmespath==1.0.1\r\njoblib==1.4.2\r\njq==1.8.0\r\njson_repair==0.29.1\r\njsonlines==1.2.0\r\njsonpatch==1.33\r\njsonpath-python==1.0.6\r\njsonpointer==3.0.0\r\njsonschema==4.23.0\r\njsonschema-specifications==2023.12.1\r\nkaleido==0.2.1\r\nkiwisolver==1.4.7\r\nkubernetes==30.1.0\r\nlangchain==0.2.16\r\nlangchain-anthropic==0.1.12\r\nlangchain-community==0.2.16\r\nlangchain-core==0.2.38\r\nlangchain-experimental==0.0.65\r\nlangchain-google-genai==1.0.3\r\nlangchain-groq==0.1.3\r\nlangchain-openai==0.1.23\r\nlangchain-text-splitters==0.2.4\r\nlangchain-together==0.1.1\r\nlangcodes==3.4.0\r\nlangdetect==1.0.9\r\nlanggraph==0.2.19\r\nlanggraph-checkpoint==1.0.9\r\nlangid==1.1.6\r\nlangsmith==0.1.116\r\nlanguage_data==1.2.0\r\nlayoutparser==0.3.4\r\nlazy_loader==0.4\r\nlibrosa==0.10.1\r\nllama_cpp_python==0.2.56\r\nllvmlite==0.43.0\r\nlm-dataformat==0.0.20\r\nloralib==0.1.2\r\nlxml==5.3.0\r\nmarisa-trie==1.2.0\r\nMarkdown==3.7\r\nmarkdown-it-py==3.0.0\r\nMarkupSafe==2.1.5\r\nmarshmallow==3.22.0\r\nmatplotlib==3.9.2\r\nmdurl==0.1.2\r\nmistralai==0.1.8\r\nmmh3==4.1.0\r\nmojimoji==0.0.13\r\nmongoengine==0.24.2\r\nmonotonic==1.6\r\nmore-itertools==10.5.0\r\nmotor==3.5.1\r\nmplcursors==0.5.3\r\nmpmath==1.3.0\r\nmsg-parser==1.2.0\r\nmsgpack==1.0.8\r\nmultidict==6.0.5\r\nmultiprocess==0.70.16\r\nmultivolumefile==0.2.3\r\nmunkres==1.1.4\r\nmurmurhash==1.0.10\r\nmutagen==1.47.0\r\nmypy-extensions==1.0.0\r\nnarwhals==1.6.3\r\nnest-asyncio==1.6.0\r\nnetworkx==2.8.8\r\nninja==1.11.1.1\r\nnltk==3.9.1\r\nnoisereduce==3.0.2\r\nnum2words==0.5.13\r\nnumba==0.60.0\r\nnumpy==1.23.0\r\nnvidia-cublas-cu12==12.1.3.1\r\nnvidia-cuda-cupti-cu12==12.1.105\r\nnvidia-cuda-nvrtc-cu12==12.1.105\r\nnvidia-cuda-runtime-cu12==12.1.105\r\nnvidia-cudnn-cu12==8.9.2.26\r\nnvidia-cufft-cu12==11.0.2.54\r\nnvidia-curand-cu12==10.3.2.106\r\nnvidia-cusolver-cu12==11.4.5.107\r\nnvidia-cusparse-cu12==12.1.0.106\r\nnvidia-ml-py==12.560.30\r\nnvidia-nccl-cu12==2.19.3\r\nnvidia-nvjitlink-cu12==12.6.68\r\nnvidia-nvtx-cu12==12.1.105\r\noauthlib==3.2.2\r\nolefile==0.47\r\nomegaconf==2.3.0\r\nonnx==1.16.2\r\nonnxruntime==1.15.0\r\nonnxruntime-gpu==1.15.0\r\nopenai==1.44.0\r\nopencv-python==4.10.0.84\r\nopencv-python-headless==4.10.0.84\r\nopenpyxl==3.1.5\r\nopentelemetry-api==1.27.0\r\nopentelemetry-exporter-otlp-proto-common==1.27.0\r\nopentelemetry-exporter-otlp-proto-grpc==1.27.0\r\nopentelemetry-instrumentation==0.48b0\r\nopentelemetry-instrumentation-asgi==0.48b0\r\nopentelemetry-instrumentation-fastapi==0.48b0\r\nopentelemetry-proto==1.27.0\r\nopentelemetry-sdk==1.27.0\r\nopentelemetry-semantic-conventions==0.48b0\r\nopentelemetry-util-http==0.48b0\r\nopenvino==2022.3.0\r\noptimum==1.17.1\r\norderly-set==5.2.2\r\norjson==3.10.7\r\noutcome==1.3.0.post0\r\noverrides==7.7.0\r\npackaging==23.2\r\npandas==2.2.2\r\npdf2image==1.17.0\r\npdfminer.six==20231228\r\npdfplumber==0.11.4\r\npeft==0.12.0\r\npikepdf==9.2.1\r\npillow @ file:///home/conda/feedstock_root/build_artifacts/pillow_1719903544255/work\r\npillow_heif==0.18.0\r\npip-licenses==5.0.0\r\nplatformdirs==4.3.2\r\nplaysound==1.3.0\r\nplaywright==1.46.0\r\nplotly==5.24.0\r\npluggy==1.5.0\r\npooch==1.8.2\r\nportalocker==2.10.1\r\nposthog==3.6.3\r\npprintpp==0.4.0\r\npreshed==3.0.9\r\nprettytable==3.11.0\r\nprimp==0.6.1\r\npriority==2.0.0\r\nproto-plus==1.24.0\r\nprotobuf==4.25.4\r\npsutil==6.0.0\r\npulsar-client==3.5.0\r\npy-cpuinfo==9.0.0\r\npy7zr==0.22.0\r\npyarrow==17.0.0\r\npyasn1==0.6.0\r\npyasn1_modules==0.4.0\r\npybcj==1.0.2\r\npycairo==1.26.1\r\npyclipper==1.3.0.post5\r\npycocotools==2.0.8\r\npycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1711811537435/work\r\npycryptodomex==3.20.0\r\npydantic==2.9.0\r\npydantic-settings==2.4.0\r\npydantic_core==2.23.2\r\npydash==8.0.3\r\npydub==0.25.1\r\npydyf @ file:///home/conda/feedstock_root/build_artifacts/pydyf_1720856584155/work\r\npyee==11.1.0\r\nPygments==2.18.0\r\nPyGObject==3.48.2\r\npymongo==4.8.0\r\nPyMuPDF==1.24.10\r\nPyMuPDFb==1.24.10\r\npynndescent==0.5.13\r\npynvml==11.5.3\r\npypandoc==1.13\r\npypandoc_binary==1.13\r\npyparsing==3.1.4\r\npypdf==4.3.1\r\npypdfium2==4.30.0\r\npyphen @ file:///home/conda/feedstock_root/build_artifacts/pyphen_1722357615489/work\r\nPyPika==0.48.9\r\npypinyin==0.52.0\r\npyppmd==1.1.0\r\npyproject_hooks==1.1.0\r\npyrubberband==0.3.0\r\npysbd==0.3.4\r\nPySocks==1.7.1\r\npytesseract==0.3.13\r\npytest==8.3.2\r\npytest-xdist==3.6.1\r\npython-crfsuite==0.9.10\r\npython-dateutil==2.9.0.post0\r\npython-doctr @ git+https://github.com/h2oai/doctr.git@aee9b1c369e37af9e18265660935bce2c4447d65\r\npython-docx==1.1.0\r\npython-dotenv==1.0.1\r\npython-iso639==2024.4.27\r\npython-magic==0.4.27\r\npython-multipart==0.0.9\r\npython-pptx==0.6.23\r\npytube==15.0.0\r\npytz==2024.1\r\nPyYAML==6.0.2\r\npyzstd==0.16.1\r\nqdrant-client==1.11.1\r\nrapidfuzz==3.9.7\r\nrarfile==4.2\r\nreferencing==0.35.1\r\nregex==2024.7.24\r\nreplicate==0.25.2\r\nrequests==2.32.3\r\nrequests-file==2.1.0\r\nrequests-oauthlib==2.0.0\r\nresponses==0.18.0\r\nretrying==1.3.4\r\nrich==13.8.0\r\nrouge==1.0.1\r\nrouge_score==0.1.2\r\nrpds-py==0.20.0\r\nrsa==4.9\r\nruff==0.6.4\r\ns3transfer==0.10.2\r\nsacrebleu==2.3.1\r\nsafetensors==0.4.5\r\nscikit-image==0.24.0\r\nscikit-learn==1.5.1\r\nscipy==1.11.4\r\nselenium==4.24.0\r\nsemantic-version==2.10.0\r\nsemanticscholar==0.8.4\r\nsentence-transformers==2.2.2\r\nsentencepiece==0.2.0\r\nsgmllib3k==1.0.0\r\nShapely==1.8.5.post1\r\nshellingham==1.5.4\r\nsix @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work\r\nsmart-open==7.0.4\r\nsniffio==1.3.1\r\nsortedcontainers==2.4.0\r\nsoundfile==0.12.1\r\nsoupsieve==2.6\r\nsoxr==0.5.0.post1\r\nspacy==3.7.6\r\nspacy-legacy==3.0.12\r\nspacy-loggers==1.0.5\r\nSQLAlchemy==2.0.34\r\nsrsly==2.4.8\r\nsse-starlette==0.10.3\r\nsseclient-py==1.8.0\r\nstarlette==0.38.5\r\nstrawberry-graphql==0.138.1\r\nSudachiDict-core==20240716\r\nSudachiPy==0.6.8\r\nsympy==1.13.2\r\ntabulate==0.9.0\r\ntaskgroup==0.0.0a4\r\ntenacity==8.5.0\r\ntensorboard==2.17.1\r\ntensorboard-data-server==0.7.2\r\ntermcolor==2.4.0\r\ntext-generation==0.7.0\r\ntextstat==0.7.4\r\ntexttable==1.7.0\r\nthinc==8.2.5\r\nthreadpoolctl==3.5.0\r\ntifffile==2024.8.30\r\ntiktoken==0.7.0\r\ntimm==1.0.9\r\ntinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1713974937325/work\r\ntogether==1.1.5\r\ntokenizers==0.19.1\r\ntomli==2.0.1\r\ntomlkit==0.12.0\r\ntorch==2.2.1+cu121\r\ntorchaudio==2.2.1+cu121\r\ntorchvision==0.17.1+cu121\r\ntqdm==4.66.5\r\ntrainer==0.0.36\r\ntransformers==4.40.0\r\ntrio==0.26.2\r\ntrio-websocket==0.11.1\r\ntriton==2.2.0\r\nTTS==0.22.0\r\ntypeguard==4.3.0\r\ntyper==0.12.5\r\ntyping-inspect==0.9.0\r\ntyping_extensions==4.12.2\r\ntzdata==2024.1\r\ntzlocal==5.2\r\nujson==5.10.0\r\numap-learn==0.5.6\r\nunicodedata2 @ file:///home/conda/feedstock_root/build_artifacts/unicodedata2_1695847980273/work\r\nUnidecode==1.3.8\r\nuniversal-analytics-python3==1.1.1\r\nunstructured==0.12.5\r\nunstructured-client==0.22.0\r\nunstructured-inference==0.7.23\r\nunstructured.pytesseract==0.3.13\r\nuritemplate==4.1.1\r\nurllib3==2.2.2\r\nuvicorn==0.30.6\r\nuvloop==0.20.0\r\nvalidators==0.34.0\r\nvoxel51-eta==0.12.7\r\nwasabi==1.1.3\r\nwatchfiles==0.24.0\r\nwavio==0.0.8\r\nwcwidth==0.2.13\r\nweasel==0.4.1\r\nweasyprint @ file:///home/conda/feedstock_root/build_artifacts/weasyprint_1719054467811/work/weasyprint-62.3-py3-none-any.whl#sha256=d31048646ce15084e135b33e334a61f526aa68d2f679fcc109ed0e0f5edaed21\r\nweaviate-client==3.26.2\r\nwebencodings @ file:///home/conda/feedstock_root/build_artifacts/webencodings_1694681268211/work\r\nwebsocket-client==1.8.0\r\nwebsockets==11.0.3\r\nWerkzeug==3.0.4\r\nwikipedia==1.4.0\r\nwolframalpha==5.1.3\r\nwrapt==1.16.0\r\nwsproto==1.2.0\r\nxlrd==2.0.1\r\nXlsxWriter==3.2.0\r\nxmltodict==0.13.0\r\nxxhash==3.5.0\r\nyarl==1.10.0\r\nyt-dlp==2023.10.13\r\nzipp==3.20.1\r\nzopfli==0.2.3\r\nzstandard==0.23.0\r\n`\r\n\r\n[pip-output.txt](https://github.com/user-attachments/files/16941390/pip-output.txt)\r\n"
      },
      {
        "user": "rohitnanda1443",
        "body": "I think I could zero on the issue. It could be npm. Tried from scratch again and during installation got an error \"npm not found\". If one uses apt-get to install npm and then again \"GPLOK=1 bash docs/linux_install.sh\" then the error is as below. \r\n\r\nError Logs attached. \r\n[NPM-Error.log](https://github.com/user-attachments/files/16945089/NPM-Error.log)\r\n\r\nInstalled node 20 and npm and reinstalled linux_install.sh. The script exited without any errors. \r\n\r\nIssue still persists and we have no login screen\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "The npm at the end is only needed for agents.  And it comes last after all other steps, so it shouldn't matter."
      },
      {
        "user": "pseudotensor",
        "body": "We build docker images for main.  Have you tried that?  It would help eliminate installation issues.\r\n\r\nWe use the h2ogpte branch's docker build all the time (uses older gradio due to kubernetes issues)"
      },
      {
        "user": "rohitnanda1443",
        "body": "Noted, but Vast.ai loads a docker image and gives us a ssh / jupyter login into the image. Cannot run another docker image inside it. \r\n\r\nWe can however load other docker images from https://hub.docker.com/search?q=h2ogpt , but the problem is that these images donot have the H2O directory from where I can fire the generate.py command and if one tries to clone the H2O-GPT git and then run again it does not function as conda etc is not installed. Reinstalling using the manual method again gives the same Login screen issue. "
      },
      {
        "user": "pseudotensor",
        "body": "What if you go back to an older version (i.e. older hash) like you used to use.  Does it work for you?"
      },
      {
        "user": "rohitnanda1443",
        "body": "On vast.ai I was using the same docker image \"cuda12.1-Ubuntu22.04 -dev\" all throughout. \r\n\r\nWhat all I have tried: \r\n1) Had a couple of months old H2O-GPT repo on PC. Uploaded that on vast and used the manual install. Got the same issue (maybe some versions of packages it is pulling have changed)\r\n2) Tried installing with latest repo: Same issue \r\n3) Tried to change the docker image to Ubuntu 24.04 - No luck either\r\n4) Tried the Quick Install - Errors out on llama_cpp_python \r\n\r\nIt was quite simple a couple of weeks back to get a vast instance and just install and go without any errors in 20 mins. (There could be some package clash that has now come up with the dependency updates). \r\n\r\nQuestion: How can one check the eror logs of Gradio (I could not find it anywhere). That will throw some idea.  "
      },
      {
        "user": "pseudotensor",
        "body": "The errors just go to the console.  I focused on browser cache issue since I've seen that before when nothing loads.  But your error in the console suggests it's something bad.\r\n\r\nWe constrain versions of things that caused issues at any point in time, but yes many are relaxed.  But note that I'm not seeing the same issue, so it's not a generic problem."
      },
      {
        "user": "rohitnanda1443",
        "body": "Request if you could try out installing on the GPU sharing sites. You may be able to zero on the issue faster. \r\n\r\nI had taken 4-5 fresh servers from different data centres to try it and all ended up with the same screen. (Tried with different ubuntu images too).\r\n\r\nDo you suggest trying with a Cuda > 12.1 image? "
      },
      {
        "user": "pseudotensor",
        "body": "Maybe vast.ai changed something if the old version also fails to work.\r\n\r\nWhat are a list of GPU sharing services that you recommend might be reasonable?\r\n\r\ncuda == 12.1 should be good.\r\n\r\nYou can also check if the client works, e.g. the gradio client or openai client.  Can it reach the server?  Would isolate if issue with your browser connecting to the host or not."
      },
      {
        "user": "rohitnanda1443",
        "body": "Nope, some dependency conflicts on the main branch currently. The issue is with other servers also. I tried on runpod.io. Used the image pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04. On Vast.ai (which is a marketplace) used various servers accross the world with different images. \r\n\r\nBelow are the 2 errors during install on runpod.io. These also came on Vast.ai. \r\n\r\n![image](https://github.com/user-attachments/assets/4775364d-ffeb-407e-8d47-18f6209a9908)\r\n\r\n![image](https://github.com/user-attachments/assets/eda2bb88-c9c6-40a6-ac18-c116f3c15cb7)\r\n\r\n\r\nThe same screen I am getting on ngrok: \r\n![image](https://github.com/user-attachments/assets/4288af1b-e992-49e1-9aef-370db92cf6fb)\r\n\r\nTerminal screenshot while running Gradio - cannot see an error: \r\n\r\n![image](https://github.com/user-attachments/assets/38421d7b-c09e-4b56-9dce-fd5d0c0c08cb)\r\n\r\n\r\nPod Screenshot: \r\n![image](https://github.com/user-attachments/assets/4904490a-61a1-4846-acd4-0b590fd21a9d)\r\n\r\n\r\n**One way of resolution can be using the pip freeze command on your system and see which package versions have changed and then retry by installing the old packages** "
      },
      {
        "user": "CodyNault",
        "body": "I just did a manual installation on Windows today and am getting the same exact error. Tried clearing cache and different browsers and that didnt work either.\r\n\r\n![image](https://github.com/user-attachments/assets/7d336600-0da5-4a54-956b-7b9b21ae6cf1)\r\n![image](https://github.com/user-attachments/assets/8fc4be22-8c18-4813-b2eb-74f67964d02b)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Could you try doing:\r\n```\r\npip uninstall gradio gradio_client -y\r\npip install gradio==3.50.2 # also installs client\r\n```\r\n\r\nand see if anything changed?"
      },
      {
        "user": "CodyNault",
        "body": "3.50.2 did the trick! Thank you! \r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Ok, 3.50.2 is perfectly fine, just no PDF viewer and a few other minor things.\r\n\r\n[freeze.log.zip](https://github.com/user-attachments/files/16975061/freeze.log.zip)\r\n\r\nAbove is a list of my current install that I haven't updated in maybe a few weeks.  Maybe one of these packages is updated and now breaking things.  I'll reinstall the env.\r\n\r\nHowever, docker works, so probably not package issue."
      },
      {
        "user": "rohitnanda1443",
        "body": "Update: Did some testing and may have zeroes on the issue for the Devs:\r\n\r\n1) Further to above after a full Linux install, I thought to resolve Gradio I should manually install all its dependencies: From - https://deps.dev/pypi/gradio/4.26.0/dependencies got the dependencies list and installed: pip install aiofiles==23.2.1 altair==5.4.1 fastapi==0.114.1 ffmpy==0.4.0 gradio-client==0.15.1 httpx==0.27.2 huggingface-hub==0.24.6 importlib-resources==6.4.5 jinja2==3.1.4 markupsafe==2.1.5 matplotlib==3.9.2 numpy==1.26.4 orjson==3.10.7 packaging==24.1.0 pandas==2.2.2 pillow==10.4.0 pydantic==2.9.1 pydub==0.25.1 python-multipart==0.0.9 pyyaml==6.0.2 ruff==0.6.4 semantic-version==2.10.0 tomlkit==0.12.0 typer==0.12.5 typing-extensions==4.12.2 uvicorn==0.30.6 annotated-types==0.7.0 anyio==4.4.0 attrs==24.2.0 certifi==2024.8.30 charset-normalizer==3.3.2 click==8.1.7 contourpy==1.3.0 cycler==0.12.1 exceptiongroup==1.2.2 filelock==3.16.0 fonttools==4.53.1 fsspec==2024.9.0 h11==0.14.0 httpcore==1.0.5 idna==3.8.0 jsonschema==4.23.0 jsonschema-specifications==2023.12.1 kiwisolver==1.4.7 markdown-it-py==3.0.0 mdurl==0.1.2 narwhals==1.7.0 pydantic-core==2.23.3 pygments==2.18.0 pyparsing==3.1.4 python-dateutil==2.9.0.post0 pytz==2024.2.0 referencing==0.35.1 requests==2.32.3 rich==13.8.1 rpds-py==0.20.0 shellingham==1.5.4 six==1.16.0 sniffio==1.3.1 starlette==0.38.5 tqdm==4.66.5 tzdata==2024.1.0 urllib3==2.2.2 websockets==11.0.3 zipp==3.20.1\r\n\r\n**It did not work**\r\n\r\n2) Then I was told in an old PC the installation was not full and only requirements.txt was installed. Fired up a new server and instead of GPLOK=1 bash docs/linux_install.sh I did `pip install -r requirements.txt`. Fired up Gradio using GRADIO_SERVER_PORT=7860 python generate.py --base_model=togethercomputer/RedPajama-INCITE-Chat-3B-v1 --prompt_type=human_bot --score_model=None --langchain_mode=ChatLLM --visible_langchain_modes=\"['ChatLLM', 'UserData', 'MyData']\" --user_path=user_path --share=False --hf_embedding_model=sentence-transformers/all-MiniLM-L6-v2\r\n\r\n**Success, I got the Gradio screen**. But getting errors \r\n![WhatsApp Image 2024-09-12 at 12 47 18_68e8dfe0](https://github.com/user-attachments/assets/98726e58-9c04-4e36-832a-e9c9e85aaa47)\r\n\r\n\r\nThe Terminal shows: \r\n![WhatsApp Image 2024-09-12 at 12 40 39_dc7b40d0](https://github.com/user-attachments/assets/9faff49d-ef98-4f10-84b3-156ecf68544b)\r\n \r\n\r\n**Issue:**\r\n1) The other optional scripts that are being installed are creating some dependency issue \r\n2) During a full install, the optional scripts uninstalls and keeps reinstalling numpy and pandas (Both which are dependencies of Gradio)\r\n\r\n**Resolution Requested**\r\n1) Please help in isolating the package in optional install creating the dependency conflict and we can fix its version (Like in the requirements.txt script) "
      },
      {
        "user": "DrNokkel",
        "body": "> Could you try doing:\r\n> \r\n> ```\r\n> pip uninstall gradio gradio_client -y\r\n> pip install gradio==3.50.2 # also installs client\r\n> ```\r\n> \r\n> and see if anything changed?\r\n\r\nThanks for your support this helped \r\n"
      },
      {
        "user": "rohitnanda1443",
        "body": "Installing the gradio 3.5 works but with lots of errors. Trying out of Gradio 4.2 etc.. gives the same screen error as the first screen \r\n\r\n![image](https://github.com/user-attachments/assets/08f8c7af-d450-4ceb-8485-b4fd39396a02)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Hi, I'm not sure what is going on.  The above seems like an issue with the model-server not gradio itself."
      }
    ]
  },
  {
    "issue_number": 1812,
    "title": "H2O-GPT on AMD GPUs (ROCm)",
    "author": "rohitnanda1443",
    "state": "open",
    "created_at": "2024-08-24T20:12:03Z",
    "updated_at": "2024-09-23T14:51:28Z",
    "labels": [],
    "body": "Hi, How can we run H20-GPT on AMD-GPUs using the AMD ROCm libraries.\r\n\r\nOne can easily run an inference server on Ollama using ROCm thereby H2O-GPT needs to use this Ollama server for inferencing. \r\n\r\nProblem: H2o-GPT install fails as it keeps finding CUDA during install. Some guidance here on editing the install script for ROCm would be helpful,\r\n\r\nMethod: \r\n1) LLM runs on an inference server using ROCm\r\n2) H2o-GPT sends LLM requests to the inference server ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Can you share what you mean by it finds CUDA during install and fails?  Maybe logs etc.?\r\n\r\nI adjusted one block In docs/linux_install.sh CUDA is mentioned.\r\n"
      },
      {
        "user": "rohitnanda1443",
        "body": "**It should not be uninstalling ROCm-Torch** \r\n\r\n\r\n`` Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n      /tmp/pip-install-jav98t1i/flash-attn_c0c8ed92b3c147bfa04d7e6ab7c98f49/setup.py:95: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\r\n        warnings.warn(\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 2, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"/tmp/pip-install-jav98t1i/flash-attn_c0c8ed92b3c147bfa04d7e6ab7c98f49/setup.py\", line 179, in <module>\r\n          CUDAExtension(\r\n        File \"/home/rohit/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1074, in CUDAExtension\r\n          library_dirs += library_paths(cuda=True)\r\n        File \"/home/rohit/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1201, in library_paths\r\n          if (not os.path.exists(_join_cuda_home(lib_dir)) and\r\n        File \"/home/rohit/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2407, in _join_cuda_home\r\n          raise OSError('CUDA_HOME environment variable is not set. '\r\n      OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root. \r\n      \r\n      \r\n      torch.__version__  = 2.2.1+cu121\r\n      \r\n      \r\n      [end of output]\r\n  \r\n  \r\nnote: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n× Encountered error while generating package metadata.\r\n╰─> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details. \r\n\r\nAttempting uninstall: torch\r\n    Found existing installation: torch 2.5.0.dev20240822+rocm6.1\r\n    Uninstalling torch-2.5.0.dev20240822+rocm6.1:\r\n      Successfully uninstalled torch-2.5.0.dev20240822+rocm6.1\r\n  Attempting uninstall: sse_starlette\r\n    Found existing installation: sse-starlette 0.10.3\r\n    Uninstalling sse-starlette-0.10.3:\r\n      Successfully uninstalled sse-starlette-0.10.3\r\n  Attempting uninstall: torchvision\r\n    Found existing installation: torchvision 0.20.0.dev20240823+rocm6.1\r\n    Uninstalling torchvision-0.20.0.dev20240823+rocm6.1:\r\n      Successfully uninstalled torchvision-0.20.0.dev20240823+rocm6.1\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ntts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 1.26.4 which is incompatible.\r\ntts 0.22.0 requires pandas<2.0,>=1.4, but you have pandas 2.2.2 which is incompatible.\r\nawscli 1.34.5 requires docutils<0.17,>=0.10, but you have docutils 0.21.2 which is incompatible.\r\nfiftyone 0.25.0 requires sse-starlette<1,>=0.10.3, but you have sse-starlette 2.1.3 which is incompatible.\r\ntorchaudio 2.4.0.dev20240823+rocm6.1 requires torch==2.5.0.dev20240822, but you have torch 2.2.1 which is incompatible.\r\nvllm 0.5.5+rocm614 requires pydantic>=2.8, but you have pydantic 2.7.0 which is incompatible.\r\nSuccessfully installed docutils-0.21.2 pandas-2.2.2 pydantic-2.7.0 pydantic-core-2.18.1 pypandoc_binary-1.13 sse_starlette-2.1.3 torch-2.2.1 `torchvision-0.17.1` ``\r\n"
      },
      {
        "user": "rohitnanda1443",
        "body": "Do we have an ROCm Docker image? \r\n"
      },
      {
        "user": "pseudotensor",
        "body": "We don't build one, but you can build one."
      }
    ]
  },
  {
    "issue_number": 1844,
    "title": "h2ogpt with open WebUI",
    "author": "InesBenAmor99",
    "state": "closed",
    "created_at": "2024-09-13T10:11:45Z",
    "updated_at": "2024-09-16T19:37:26Z",
    "labels": [
      "type/question"
    ],
    "body": "Hello, I would like to integrate H2O GPT with Open WebUI. Will it be possible to chat with all the documents that are already uploaded in collections created from the H2O GPT interface and stored in the vector database, or will it just be a simple chat without accessing the documents already uploaded in the background?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "The documents won't work across.\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/FAQ.md#open-web-ui\r\n\r\nWe use it here: https://gpt-docs.h2o.ai\r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "FYI I recommend the non-docker way so it can use h2oGPT OSS changes to the package so it works optimally."
      },
      {
        "user": "InesBenAmor99",
        "body": "I don't actually understand the need for [https://gpt-docs.h2o.ai/](https://gpt-docs.h2o.ai/ ) when it's just an implementation of the LLM model, not H2O GPT functionalities. Can you explain the difference between these two options?"
      },
      {
        "user": "pseudotensor",
        "body": "As in the FAQ, open web ui is using h2oGPT for *all* backend activities (embedding, STT, TTS, image generation, document ingestion, etc.).  openweb ui is then just a UI front end."
      }
    ]
  },
  {
    "issue_number": 1842,
    "title": "BUG: Broken agent flow",
    "author": "fatihozturkh2o",
    "state": "closed",
    "created_at": "2024-09-13T09:00:43Z",
    "updated_at": "2024-09-13T09:23:48Z",
    "labels": [
      "agents/bug"
    ],
    "body": "For agents, running into the following error now:\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/fatih/h2ogpt/openai_server/autogen_streaming.py\", line 50, in run_autogen_in_proc\r\n    ret_dict = func(**kwargs)\r\n  File \"/home/fatih/h2ogpt/openai_server/agent_utils.py\", line 113, in run_agent\r\n    ret_dict = run_agent_func(**kwargs)\r\n  File \"/home/fatih/h2ogpt/openai_server/autogen_2agent_backend.py\", line 145, in run_autogen_2agent\r\n    chat_result = code_executor_agent.initiate_chat(**chat_kwargs)\r\n  File \"/home/fatih/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py\", line 1095, in initiate_chat\r\n    self.send(msg2send, recipient, request_reply=True, silent=silent)\r\n  File \"/home/fatih/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py\", line 733, in send\r\n    message = self._process_message_before_send(message, recipient, ConversableAgent._is_silent(self, silent))\r\n  File \"/home/fatih/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py\", line 278, in _is_silent\r\n    return agent.silent if agent.silent is not None else silent\r\nAttributeError: 'H2OConversableAgent' object has no attribute 'silent'\r\n```\r\n\r\nseems related to this commit: https://github.com/h2oai/h2ogpt/commit/e3114a1ccc6b60d8b1e60c16855abedabfa2c533\r\n\r\nRepro:\r\nprompt = any prompt\r\nagent_type = auto",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Must be due to version differences.  My version of ConversableAgent has no silent parameter.\r\n\r\n\r\na9b97158d1d5f3a8fbb47246e28327e7609bc903"
      },
      {
        "user": "fatihozturkh2o",
        "body": "That explains, I have 0.2.35 (because of > 2.33.0) . Let me also downgrade to that then."
      }
    ]
  },
  {
    "issue_number": 1841,
    "title": "BUG: Broken # execution: false logic for agents",
    "author": "fatihozturkh2o",
    "state": "closed",
    "created_at": "2024-09-12T13:32:30Z",
    "updated_at": "2024-09-13T02:46:19Z",
    "labels": [],
    "body": "It seems when `# execution: false` is in the codeblock, code executor tries to execute the code and returns the output: \r\n```\r\nexitcode: -2 (execution failed)\r\nCode output:\r\n```\r\nand since this is not an empty string, there is no logic in `terminate_message_func` function to terminate the chat either, hence the conversation goes on as a loop.\r\n\r\nI think we need to be able to skip execution of such code blocks in H2OLocalCommandLineCodeExecutor.\r\n\r\nrepro prompt: `Write a code to show an image in matplotlib. Do not execute any code`\r\nmodel: claude-3-5-sonnet-20240620\r\nagent_type: auto\r\n#### How it looks  (since it was a loop, forcefully killed the process before hitting the conversation limit)\r\nhttps://github.com/user-attachments/assets/88a6c31a-0c27-4e78-a459-c03e9acb415a\r\n\r\n\r\n\r\n",
    "comments": []
  },
  {
    "issue_number": 1838,
    "title": "Fix cost for multi-agent",
    "author": "pseudotensor",
    "state": "closed",
    "created_at": "2024-09-11T23:19:17Z",
    "updated_at": "2024-09-13T00:24:21Z",
    "labels": [],
    "body": "https://github.com/h2oai/h2ogpt/pull/1836",
    "comments": []
  },
  {
    "issue_number": 1715,
    "title": "Missing instructions for make_db in docker",
    "author": "tomerjr",
    "state": "closed",
    "created_at": "2024-06-30T11:27:29Z",
    "updated_at": "2024-09-10T20:18:15Z",
    "labels": [
      "type/question"
    ],
    "body": "Hi, would like a clarification as to how to use make_db with specific path as data source and with the ability to create a collection name.\r\n\r\nI've tried to use it like the normal instructions with --collection_name   and   --user_path   but it did not work and i did not find an answer to this in other issues or the doc files.\r\n\r\nfor example i tried:\r\n\r\nexport GRADIO_SERVER_PORT=7860\r\nexport OPENAI_SERVER_PORT=5000\r\nsudo docker run \\\r\n       --gpus all \\\r\n       --runtime=nvidia \\\r\n       --shm-size=1g \\\r\n       -p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT \\\r\n       -p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT \\\r\n       --rm --init \\\r\n       --network host \\\r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u `id -u`:`id -g` \\\r\n       -v \"${HOME}\"/.cache/huggingface/hub/:/workspace/.cache/huggingface/hub \\\r\n       -v \"${HOME}\"/.config:/workspace/.config/ \\\r\n       -v \"${HOME}\"/.triton:/workspace/.triton/  \\\r\n       -v \"${HOME}\"/save:/workspace/save \\\r\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\r\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\r\n       -v \"${HOME}\"/users:/workspace/users \\\r\n       -v \"${HOME}\"/db_nonusers:/workspace/db_nonusers \\\r\n       -v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path \\\r\n       -v \"${HOME}\"/h2ogpt_auth:/workspace/h2ogpt_auth \\\r\n       -e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT \\\r\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 /workspace/src/make_db.py \\\r\n       --collection_name=Duck\r\n\r\nThank you and have a good day.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It's already documented here near bottom of this section:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#database-creation"
      },
      {
        "user": "tomerjr",
        "body": "so when using docker i must use weaviate in order to change the user_path and the collection_name?\r\nfrom the doc its not stated how to run it locally with these arguments and with docker otherwise. @pseudotensor "
      },
      {
        "user": "pseudotensor",
        "body": "The bottom of the section has this:\r\n```\r\nmkdir -p ~/.cache\r\nmkdir -p ~/save\r\nmkdir -p ~/user_path\r\nmkdir -p ~/db_dir_UserData\r\ndocker run \\\r\n       --gpus all \\\r\n       --runtime=nvidia \\\r\n       --shm-size=2g \\\r\n       --rm --init \\\r\n       --network host \\\r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u `id -u`:`id -g` \\\r\n       -v \"${HOME}\"/.cache:/workspace/.cache \\\r\n       -v \"${HOME}\"/save:/workspace/save \\\r\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\r\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\r\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 /workspace/src/make_db.py --verbose --use_unstructured_pdf=False --enable_pdf_ocr=False --hf_embedding_model=BAAI/bge-small-en-v1.5 --cut_distance=10000\r\n```\r\n\r\nIt doesn't use weaviate.  Can you explain what you mean?"
      },
      {
        "user": "tomerjr",
        "body": "For example, i ran something like this:\r\n\r\nexport GRADIO_SERVER_PORT=7860\r\nexport OPENAI_SERVER_PORT=5000\r\nsudo docker run \\\r\n       --gpus all \\\r\n       --runtime=nvidia \\\r\n       --shm-size=1g \\\r\n       -p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT \\\r\n       -p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT \\\r\n       --rm --init \\\r\n       --network host \\\r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u `id -u`:`id -g` \\\r\n       -v \"${HOME}\"/.cache/huggingface/hub/:/workspace/.cache/huggingface/hub \\\r\n       -v \"${HOME}\"/.config:/workspace/.config/ \\\r\n       -v \"${HOME}\"/.triton:/workspace/.triton/  \\\r\n       -v \"${HOME}\"/save:/workspace/save \\\r\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\r\n       -v \"${HOME}\"/db_dir_UserData:/workspace/ducks \\\r\n       -v \"${HOME}\"/users:/workspace/users \\\r\n       -v \"${HOME}\"/db_nonusers:/workspace/db_nonusers \\\r\n       -v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path \\\r\n       -v \"${HOME}\"/h2ogpt_auth:/workspace/h2ogpt_auth \\\r\n       -e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT \\\r\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 /workspace/src/make_db.py \\\r\n       --collection_name=Ducks --user_path=:/workspace/duck --langchain_type=personal --persist_directory=users/tomer/db_dir_duck\r\n       \r\nAnd then:\r\n\r\nexport GRADIO_SERVER_PORT=7860\r\nexport OPENAI_SERVER_PORT=5000\r\nsudo docker run \\\r\n       --gpus all \\\r\n       --runtime=nvidia \\\r\n       --shm-size=1g \\\r\n       -p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT \\\r\n       -p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT \\\r\n       --rm --init \\\r\n       --network host \\\r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u `id -u`:`id -g` \\\r\n       -v \"${HOME}\"/.cache/huggingface/hub/:/workspace/.cache/huggingface/hub \\\r\n       -v \"${HOME}\"/.config:/workspace/.config/ \\\r\n       -v \"${HOME}\"/.triton:/workspace/.triton/  \\\r\n       -v \"${HOME}\"/save:/workspace/save \\\r\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\r\n       -v \"${HOME}\"/db_dir_UserData:/workspace/users/tomer/db_dir_duck \\\r\n       -v \"${HOME}\"/users:/workspace/users \\\r\n       -v \"${HOME}\"/db_nonusers:/workspace/db_nonusers \\\r\n       -v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path \\\r\n       -v \"${HOME}\"/h2ogpt_auth:/workspace/h2ogpt_auth \\\r\n       -e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT \\\r\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 /workspace/generate.py \\         --base_model=https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q2_K.gguf \\\r\n          --use_safetensors=True \\\r\n          --prompt_type=zephyr \\\r\n          --save_dir='/workspace/save/' \\\r\n          --use_gpu_id=False \\\r\n          --user_path=/workspace/user_path \\\r\n          --langchain_mode=\"LLM\" \\\r\n          --langchain_modes=\"['UserData', 'LLM','tomer']\" \\\r\n          --score_model=None \\   \r\n          --max_max_new_tokens=2048 \\\r\n          --max_new_tokens=1024 \\\r\n          --openai_port=$OPENAI_SERVER_PORT\r\n\r\n       \r\nAnd it does add the collection and I can see it but I cant see the documents in it when i run gen.py.\r\nI can only see the documents in the default user.\r\nWhat I want is to run it on my chosen folder, for my chosen user/db and then to be able to load it with gen.py.\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/13628115/7250693c-18a2-45fd-802c-b22066fca13c)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Few things:\r\n\r\n1.\r\n\r\nIf I try roughly what you did, I notice `--user_path=:/workspace/duck` with odd `:` inside.\r\n\r\n2. Your collection name should match the expected name, e.g. db_dir_duck\r\n\r\nThis is explained in the make_db docs:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/8dcf590df1158b86bc37f619b881c86272ceb767/src/make_db.py#L200-L202\r\n\r\n3. You are not using a persistent user, so why you have a hash in your Directory.\r\n\r\nThis is also understood by the same make_db docs lines as above, where <user> should be in the name of the path.\r\n\r\n4.   You need to login and/or use auth or some kind. Login is simplest first thing.\r\n\r\n5. Here's what I try, just to follow along:\r\n\r\n```\r\npython src/make_db.py --collection_name=Ducks --user_path=user_path_test --langchain_type=personal --persist_directory=users/tomer/db_dir_duck\r\n```\r\n\r\n```\r\npython generate.py --base_model=https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q2_K.gguf --use_safetensors=True --prompt_type=zephyr --save_dir='save2' --use_gpu_id=False --user_path=user_path_test --langchain_mode=\"LLM\" --langchain_modes=\"['UserData', 'LLM','tomer']\" --score_model=None --add_disk_models_to_ui=False\r\n```\r\n\r\nWhat I see is that you have (as you showed) a directory that is not the same as the database you created.  i.e. it has a hash inside it.  That's because by default personal directories are hashed like that.  So when you just set langchain_mode as \"tomer\" it doesn't know where that db is located.\r\n\r\nI don't currently have a way to specify the database path for personal databases for such \"temporary\" users.\r\n\r\nE.g. so you should run:\r\n\r\n1. \r\n\r\n```\r\npython src/make_db.py --collection_name=duck --user_path=user_path_test --langchain_type=personal --persist_directory=users/tomer/db_dir_duck/\r\n```\r\n\r\n2. Then run without \"tomer\" since that is only for a single user, above named \"tomer\".\r\n```\r\npython generate.py --base_model=https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q2_K.gguf --use_safetensors=True --prompt_type=zephyr --save_dir='save2' --use_gpu_id=False --user_path=user_path_test --langchain_mode=\"LLM\" --langchain_modes=\"['UserData', 'LLM']\" --score_model=None --add_disk_models_to_ui=False\r\n```\r\n\r\n3. Then login as user \"tomer\"\r\n\r\n![image](https://github.com/user-attachments/assets/51241c90-f262-421c-87f9-c7f8c09d48e3)\r\n\r\n4. Then add the collection:\r\n\r\n![image](https://github.com/user-attachments/assets/8b78fc2e-6375-47d6-8836-143a8f3b907e)\r\n\r\nThen you'll see the \"Directory\" be correct:\r\n\r\n![image](https://github.com/user-attachments/assets/f36281cd-6237-4027-a250-362ecb7ef59f)\r\n\r\nand you'll see your docs when choosing the duck collection:\r\n\r\n![image](https://github.com/user-attachments/assets/f1720238-ec2c-4db8-971b-2e1b4ef03195)\r\n\r\n\r\n\r\n"
      },
      {
        "user": "tomerjr",
        "body": "@pseudotensor  how will this work for docker?\r\nWhenever i try to change the user_path i get an error message saying \"user_path != user_path\" or something similar.\r\nI need instructions regarding docker specifically please.\r\nThank you for all your help."
      },
      {
        "user": "pseudotensor",
        "body": "I'm not aware of any issue that should occur specifically for docker.  The instructions I added should work.  If you can share your error I'm happy to look."
      },
      {
        "user": "tomerjr",
        "body": "@pseudotensor \r\n![image](https://github.com/user-attachments/assets/08eeffcb-331d-4379-a711-e968299c42e5)\r\nIn addition, I just realized that you were referring to uploading documents and then creating the db for them but that is not what I want.\r\nI want to make the DB separately and then for the user to have it loaded upon logging in.\r\nAgain, thank you so much for all your help so far."
      },
      {
        "user": "pseudotensor",
        "body": "I'd guess \"user_path\" has wrong permissions or is a file.  Needs to be fixed."
      },
      {
        "user": "tomerjr",
        "body": "> I'd guess \"user_path\" has wrong permissions or is a file. Needs to be fixed.\r\n\r\n@pseudotensor  I checked and it is not a file. also, the permissions for the \"duck\" directory are the same for \"user_path\" directory. it seems like there is an assertion that the path should be \"user_path\" directory for some reason."
      },
      {
        "user": "pseudotensor",
        "body": "When you are mapping maps, you are mapping /home/user_path -> /workspace/duck inside docker.  So h2oGPT inside docker won't be able to find that path unless you set --user_path=/workspace/duck"
      }
    ]
  },
  {
    "issue_number": 1796,
    "title": "upload docs",
    "author": "InesBenAmor99",
    "state": "open",
    "created_at": "2024-08-12T15:50:01Z",
    "updated_at": "2024-09-10T20:16:04Z",
    "labels": [],
    "body": "What could be the problem if, when uploading a document (simple unscanned PDF), the process gets stuck at the processing stage, without displaying any errors, and without the document being uploaded? I've also tested uploading to the online version, but that doesn't work either. ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Are you able to provide an example document that has issues?  I expect it's getting stuck in using unstructured OCR or something that is very slow."
      },
      {
        "user": "llmwesee",
        "body": "I'm also facing the same issue. I already uploaded around 720 documents in the UserData. But now whenever I trying to upload documents in collections like 'UserData'  or 'MyData'  the process gets stuck at the processing stage, without displaying any errors, and without the document being uploaded?"
      },
      {
        "user": "pseudotensor",
        "body": "Can you see what the processes are by looking at `ps -auxwf` and `top` (e.g. hit c and show in wide view)?  Any processes still going that seem stuck?\r\n\r\nI have over 3000 tests that run over a 24 hour period, and not noticing anything wrong."
      }
    ]
  },
  {
    "issue_number": 1028,
    "title": "include grclient.py in the OpenAI compliant API to make easy access to basic doc operations",
    "author": "pseudotensor",
    "state": "closed",
    "created_at": "2023-11-01T04:57:19Z",
    "updated_at": "2024-09-04T03:07:10Z",
    "labels": [],
    "body": "https://h2oai.slack.com/archives/C04ND1ZCXGD/p1698811222904429\r\n\r\n\r\n```\r\nExample LLM, QA, Summarization, Extraction using open-source client code from url.\r\nIn bash:\r\nconda create -n gradioclient -y\r\nconda activate gradioclient\r\nconda install python=3.10 -y\r\npip install gradio_client==0.6.1\r\n\r\n# Download Gradio Wrapper code if GradioClient class used, not needed for native Gradio Client\r\n# No wheel for now\r\nwget https://raw.githubusercontent.com/h2oai/h2ogpt/main/gradio_utils/grclient.py\r\nmkdir -p gradio_utils\r\nmv grclient.py gradio_utils\r\n```\r\nThen in python:\r\n```\r\nimport os\r\nfrom gradio_utils.grclient import GradioClient\r\n\r\nh2ogpt_key = os.getenv('H2OGPT_KEY') or os.getenv('H2OGPT_H2OGPT_KEY')\r\n# if you have API key for public instance:\r\nclient = GradioClient(\"https://gpt.h2o.ai\", h2ogpt_key=h2ogpt_key)\r\n\r\n# LLM\r\nprint(client.question(\"Who are you?\"))\r\n\r\nurl = \"https://cdn.openai.com/papers/whisper.pdf\"\r\n\r\n# Q/A\r\nprint(client.query(\"What is whisper?\", url=url))\r\n# summarization (map_reduce over all pages if top_k_docs=-1)\r\nprint(client.summarize(\"What is whisper?\", url=url, top_k_docs=3))\r\n# extraction (map per page)\r\nprint(client.extract(\"Give bullet for all key points\", url=url, top_k_docs=3))\r\nWill output stuff like this: https://github.com/h2oai/h2ogpt/blob/main/tests/test_client_readme.py#L34-L57\r\n```",
    "comments": [
      {
        "user": "this",
        "body": "Now we use the `openai` Python package for DAI H2OGPT integration  https://github.com/h2oai/h2oai/pull/33328. Hence this issue is no longer valid."
      }
    ]
  },
  {
    "issue_number": 1809,
    "title": "Please tell me how to summarise local document using local API",
    "author": "anushaharish538",
    "state": "closed",
    "created_at": "2024-08-20T14:03:47Z",
    "updated_at": "2024-08-27T05:20:28Z",
    "labels": [
      "type/question"
    ],
    "body": "Please tell me how to summarise local document using local API",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Check out the client code for the tests in tests/test_client_calls.py::test_client_summarization_from_url or test_client_summarization_from_text\r\n\r\nIt's very simple."
      },
      {
        "user": "anushaharish538",
        "body": "This is for urls correct ?. tests/test_client_calls.py::test_client_summarization_from_url or test_client_summarization_from_text. I need for document summarisation.\r\n\r\nRegards,\r\nAnusha Harish"
      },
      {
        "user": "pseudotensor",
        "body": "There are 3 test codes, the other one is test_client_summarization that takes files like PDFs.  Is that what you want?"
      }
    ]
  },
  {
    "issue_number": 1810,
    "title": "TypeError Can only concatenate str (not \"bool\") to str",
    "author": "btriw",
    "state": "closed",
    "created_at": "2024-08-23T10:21:29Z",
    "updated_at": "2024-08-26T16:39:47Z",
    "labels": [],
    "body": "Hello i tried to connect h2ogpt with gradio so i can get the function from h2opgt, like this:\r\n\r\n![image](https://github.com/user-attachments/assets/e16e1e43-6e74-4ed9-93b2-3c4c6e5e253b)\r\n\r\nthis is the code that use the vicuna_client:\r\n\r\n![image](https://github.com/user-attachments/assets/781241f3-b2d4-4fd4-b440-c6786ae7a552)\r\n\r\nit can be used as expected, but when i try to ask throught h2opgt it getting error like in the Title above, TypeError: can only concatenate str (not \"bool\") to str\r\n\r\n![image](https://github.com/user-attachments/assets/779a154c-74f4-44f9-9381-f4e47048c858)\r\n\r\ndoes anyone know what happen? i try to re-clone the repo but the probelm still exist\r\nhope anyone can answer this :)",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "What version of gradio do you have?  h2oGPT works with gradio 3.50.2 or gradio from requirements.txt:\r\n```\r\ngradio @ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl\r\ngradio_client @ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio_client-0.15.1-py3-none-any.whl\r\n```\r\n\r\nIt appears you have some other version."
      },
      {
        "user": "btriw",
        "body": "Hello @pseudotensor sorry for my late reply and thanks for the reply :) \r\nafter installed the recommended version its worked again, thanks :D"
      }
    ]
  },
  {
    "issue_number": 1808,
    "title": "Relocate \"Delete Selected Sources from DB\" Button for Safety",
    "author": "yokomizo-tech",
    "state": "closed",
    "created_at": "2024-08-20T05:26:20Z",
    "updated_at": "2024-08-25T21:40:19Z",
    "labels": [],
    "body": "The \"Delete Selected Sources from DB\" button on the Document Selection tab appears to be risky. Could the functionality for document deletion be moved to a separate tab, such as a \"Document Deletion\" tab, to prevent accidental deletions and enhance user safety?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It's a good idea, to at least make it an option."
      }
    ]
  },
  {
    "issue_number": 1799,
    "title": "API Support",
    "author": "anushaharish538",
    "state": "closed",
    "created_at": "2024-08-13T16:48:52Z",
    "updated_at": "2024-08-20T12:38:36Z",
    "labels": [
      "type/question"
    ],
    "body": "Hi,\r\n\r\nCan I chat with files using api by giving file url or website url.\r\n\r\nRegards,\r\nAnusha",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Ya the full gradio-based API exists for uploading, managing, querying docs.  And the openAI API can be used for querying docs.\r\n\r\nE.g. https://github.com/h2oai/h2ogpt/blob/e7c04e57b21dd412f4babd10aed8d3a7b686e446/tests/test_client_calls.py#L2972\r\n\r\nThis test shows many client calls for various tasks that are tested."
      },
      {
        "user": "anushaharish538",
        "body": "I need for summarise the docs"
      }
    ]
  },
  {
    "issue_number": 1667,
    "title": "Is there a plan to incorporate a Knowledge Graph RAG Query Engine?",
    "author": "vitalyshalumov",
    "state": "closed",
    "created_at": "2024-06-05T12:29:22Z",
    "updated_at": "2024-08-16T09:20:47Z",
    "labels": [
      "type/question"
    ],
    "body": null,
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Yes"
      },
      {
        "user": "FellowTraveler",
        "body": "What's the status on this?"
      }
    ]
  },
  {
    "issue_number": 1766,
    "title": "Integrating H2OGPT in multitenance application.",
    "author": "Eliyas",
    "state": "closed",
    "created_at": "2024-07-23T21:09:59Z",
    "updated_at": "2024-08-15T23:30:09Z",
    "labels": [
      "type/question"
    ],
    "body": "We have a multi-tenant application and are integrating H2OGPT to serve various clients. Each client may have hundreds of users.\r\n\r\n**1. Our goal is to store and ingest user, client, and permission-based documents efficiently within H2OGPT. Is there a method to manage and search only in documents permitted for each user without duplicating them?**\r\nI have explored using collections in H2OGPT, where each user would have a separate collection. However, duplicating documents across multiple collections is not ideal for us.\r\nAdministrators of each client should have access to all documents, while other users should only access documents based on their permissions.\r\n\r\n**2. How to add custom metadata to a document?**\r\nI am thinking of filtering documents based on custom metadata like roles, permission, and client names. I saw that chromaDB has a metadata filter available in the query.\r\n```\r\nresults = collection.query(\r\n   query_texts=[\"This is a query document\"],\r\n   n_results=2,\r\n   where={\"metadata_field\": \"is_equal_to_this\"}\r\n)\r\n\r\n```\r\nBut not sure how to add custom metadata into a document and pass metadata filter in summary or query API in H2OGPT.\r\n\r\n**3. Do we need to restart the AI server to create a new collection and DB?**\r\n\r\n**For creating user collection now I am doing this.**\r\n`python src/make_db.py --user_path=clients/user1 --collection_name=user1 --langchain_type=personal --hf_embedding_model=hkunlp/instructor-large --persist_directory=users/test/db_dir_user1`\r\n\r\n**After**\r\n`python generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --tokenizer_base_model=zephyr/zephyr-7B-beta --prompt_type=zephyr --max_seq_len=4096 --langchain_mode='UserData' --langchain_modes=['UserData', 'img', 'live', 'user1'] --langchain_mode_paths=\"{'UserData':'user_path','img':'clients/img','live':'clients/live','user1':'clients/user1'}\"  --langchain_mode_types='{'UserData':'personal','img':'personal','live':'personal','user1':'personal'}' --save_dir=saveDir --verbose=True --system_prompt='auto'`\r\n\r\nDo we need to add and execute the script to define user collections in **langchain_modes**, **langchain_mode_paths**, and **langchain_mode_types** every time we restart the server, or is it a one-time setup? If we have 100 users, do we need to include details for each user's collection in the script?\r\n\r\n",
    "comments": [
      {
        "user": "llmwesee",
        "body": "I'm also curious about the implementation details?"
      },
      {
        "user": "pseudotensor",
        "body": "@Eliyas Sorry for the delay.\r\n\r\n1a. The simplest way to implement an efficient document ingestion is via caching of the embedding for a given input.  That is relatively easy.\r\n\r\n1b. Permission based access is automatic if using personal collections and auth access to the user.\r\n\r\n2. h2oGPT automatically adds meta data to this and its searchable as AND or OR logical operation, but there is no way to add additional meta data.  However, if the document was parsed and had metadata (e.g. PDF) and you add metadata_in_context='all' or pass a list of keys to metadata_in_context, then we will use that.  So the issue just passes to the PDF having the correct metadata and you choosing which to use.\r\n\r\n3. No need to restart.  If created outside server with make_db, then any user can add that collection by name and see it.  E.g. for specific users, one would follow this: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#personal-collections-with-make_db"
      }
    ]
  },
  {
    "issue_number": 1768,
    "title": "Error when using AWQ model with 'use_gpu_id=False'",
    "author": "bw-Deejee",
    "state": "closed",
    "created_at": "2024-07-24T13:38:14Z",
    "updated_at": "2024-08-15T23:21:13Z",
    "labels": [],
    "body": "Using the newest version of the repo.\r\nFollowing command works:\r\n\r\n`python generate.py --base_model=casperhansen/llama-3-70b-instruct-awq --load_awq=model`\r\n\r\nfollowing command doesn't:\r\n\r\n`python generate.py --base_model=casperhansen/llama-3-70b-instruct-awq --load_awq=model --use_gpu_id=False`\r\n\r\nerror message:\r\n`... TypeError: functools.partial(<bound method AutoAWQForCausalLM.from_quantized of <class 'awq.models.auto.AutoAWQForCausalLM'>>, fuse_layers=True) got multiple values for keyword argument 'use_safetensors'`\r\n\r\nSetting `--use_safetensors=True` doesnt help.\r\n\r\nIs there any way to solve this?\r\n",
    "comments": []
  },
  {
    "issue_number": 1779,
    "title": "embedding model Cohere/Cohere-embed-multilingual-v3.0 not supported.",
    "author": "juerware",
    "state": "closed",
    "created_at": "2024-08-02T08:15:41Z",
    "updated_at": "2024-08-15T22:41:34Z",
    "labels": [
      "resolution/wontfix"
    ],
    "body": "Command line executed:\r\n```shell\r\nexport max_input_tokens=8192;\r\nexport max_total_input_tokens=16384;\r\nexport chunk_size=2048;\r\nTOKENIZERS_PARALLELISM=true python generate.py \\\r\n    --base_model=meta-llama/Meta-Llama-3.1-8B-Instruct --prompt_type=llama2 \\\r\n    --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192 \\\r\n    --user_path=/opt/myDocuments/all/ --langchain_mode='UserData' --max_quality=True \\\r\n    --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True \\\r\n    --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True \\\r\n    --hf_embedding_model=\"Cohere/Cohere-embed-multilingual-v3.0\" \\\r\n    --memory_restriction_level=0 --score_model=None --verbose=True --debug=True \\\r\n    --show_examples=True --compile_model=True \\\r\n    --share=True \\\r\n    --max_input_tokens=$max_input_tokens --max_total_input_tokens=$max_total_input_tokens --chunk_size=$chunk_size;\r\n ```\r\nError output:\r\n```shell\r\nWARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name Cohere/Cohere-embed-multilingual-v3.0. Creating a new one with mean pooling.\r\nTraceback (most recent call last):\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/src/utils.py\", line 75, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/src/gen.py\", line 2015, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/src/gpt_langchain.py\", line 559, in get_embedding\r\n    embedding = HuggingFaceEmbeddings(model_name=hf_embedding_model, model_kwargs=model_kwargs)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 203, in warn_if_direct_instance\r\n    return wrapped(self, *args, **kwargs)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 79, in __init__\r\n    self.client = sentence_transformers.SentenceTransformer(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 299, in __init__\r\n    modules = self._load_auto_model(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 1324, in _load_auto_model\r\n    transformer_model = Transformer(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py\", line 53, in __init__\r\n    config = AutoConfig.from_pretrained(model_name_or_path, **config_args, cache_dir=cache_dir)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1004, in from_pretrained\r\n    raise ValueError(\r\nValueError: Unrecognized model in Cohere/Cohere-embed-multilingual-v3.0. Should have a `model_type` key in its config.json, ....\r\n```\r\nAs it can be seen there is no support for embedding model Cohere/Cohere-embed-multilingual-v3.0 from huggingface.\r\n\r\nSome idea would be appreciated, thanks in advance.\r\n\r\n------------------------\r\n\r\nSO: Ubuntu 22.04.4 LTS\r\nCommit: 10571004 (last at the moment this script was executed)",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "That's true, it's not a local model. The https://huggingface.co/Cohere/Cohere-embed-multilingual-v3.0 only contains the tokenizer.  This is a paid model for Cohere via their API, I wouldn't easily support and probably little value in it."
      }
    ]
  },
  {
    "issue_number": 1792,
    "title": "running h2ogpt on Macbook M1 error ",
    "author": "a-ml",
    "state": "closed",
    "created_at": "2024-08-09T17:01:55Z",
    "updated_at": "2024-08-12T14:49:51Z",
    "labels": [],
    "body": "Hello,\r\n\r\nI've installed H2O on Macbook M1, when installing the dependencies I'm receiving the error bellow.\r\n\r\n```\r\nIgnoring torch: markers 'sys_platform != \"darwin\" and platform_machine != \"arm64\"' don't match your environment\r\nCollecting InstructorEmbedding@ https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl (from -r reqs_optional/requirements_optional_langchain.txt (line 23))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl (20 kB)\r\nCollecting sentence_transformers_old@ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl (from -r reqs_optional/requirements_optional_langchain.txt (line 26))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl (125 kB)\r\nCollecting llava@ https://h2o-release.s3.amazonaws.com/h2ogpt/llava-1.7.0.dev0-py3-none-any.whl (from -r reqs_optional/requirements_optional_langchain.txt (line 41))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/llava-1.7.0.dev0-py3-none-any.whl (87 kB)\r\nIgnoring jq: markers 'platform_machine == \"x86_64\"' don't match your environment\r\nERROR: Cannot install torch==2.2.1 because these package versions have conflicting dependencies.\r\n\r\nThe conflict is caused by:\r\n    The user requested torch==2.2.1\r\n    The user requested (constraint) torch==2.3.1\r\n\r\nTo fix this you could try to:\r\n1. loosen the range of package versions you've specified\r\n2. remove package versions to allow pip to attempt to solve the dependency conflict\r\n\r\nERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\r\n```\r\nI don't know if this error is impediment to run the application which I'm trying with the command bellow:\r\n\r\n`python generate.py --base_model=llama --model_path_llama=https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf?download=true --tokenizer_base_model=meta-llama/Meta-Llama-3.1-8B-Instruct --max_seq_len=8192`\r\n\r\nWhen trying to run I've this error:\r\n\r\n```\r\nsoundfile, librosa, and wavio not installed, disabling STT\r\nsoundfile, librosa, and wavio not installed, disabling TTS\r\nUsing Model llama\r\nTraceback (most recent call last):\r\n  File \"/Users/kapenge/Laboratories/_ai/frameworks/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/Users/kapenge/Laboratories/_ai/frameworks/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/Users/kapenge/Laboratories/_ai/frameworks/h2ogpt/src/utils.py\", line 77, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/Users/kapenge/Laboratories/_ai/frameworks/h2ogpt/src/gen.py\", line 2013, in main\r\n    from gpt_langchain import get_embedding\r\n  File \"/Users/kapenge/Laboratories/_ai/frameworks/h2ogpt/src/gpt_langchain.py\", line 48, in <module>\r\n    from langchain_core.language_models.llms import aget_prompts, aupdate_cache\r\nImportError: cannot import name 'aget_prompts' from 'langchain_core.language_models.llms' (/Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/language_models/llms.py)\r\n```\r\n\r\n\r\nAny hint?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I have these constraints:\r\n\r\n![image](https://github.com/user-attachments/assets/da17c6c1-d5f4-4f6d-9a09-de86aa9669a7)\r\n\r\n\r\nI'm unclear why the constraint works (says has to be 2.3.1 for MAC) but the requirements.txt (that uses same constraint) thinks it needs to use 2.2.1.\r\n\r\n\r\nCan you give more complete logs of the install?"
      },
      {
        "user": "a-ml",
        "body": "I've deleted the env and I'm trying recreate it again... I'll update"
      },
      {
        "user": "a-ml",
        "body": "I've created a brand-new conda environment, tried to follow the instructions and I got the same error.\r\n\r\nHere complete install logs\r\n\r\n```\r\n❯ conda activate h2ogpt\r\n❯ pip uninstall -y pandoc pypandoc pypandoc-binary\r\nWARNING: Skipping pandoc as it is not installed.\r\nWARNING: Skipping pypandoc as it is not installed.\r\nWARNING: Skipping pypandoc-binary as it is not installed.\r\n❯ pip install --upgrade pip\r\nRequirement already satisfied: pip in /Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages (24.0)\r\nCollecting pip\r\n  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\r\nUsing cached pip-24.2-py3-none-any.whl (1.8 MB)\r\nInstalling collected packages: pip\r\n  Attempting uninstall: pip\r\n    Found existing installation: pip 24.0\r\n    Uninstalling pip-24.0:\r\n      Successfully uninstalled pip-24.0\r\nSuccessfully installed pip-24.2\r\n❯ python -m pip install --upgrade setuptools\r\nRequirement already satisfied: setuptools in /Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages (72.1.0)\r\n❯ pip install -r requirements.txt --extra-index https://download.pytorch.org/whl/cpu -c reqs_optional/reqs_constraints.txt\r\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\r\nCollecting gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl (from -r requirements.txt (line 8))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl (17.3 MB)\r\nCollecting gradio_client@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio_client-0.15.1-py3-none-any.whl (from -r requirements.txt (line 9))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/gradio_client-0.15.1-py3-none-any.whl (313 kB)\r\nIgnoring torch: markers 'sys_platform != \"darwin\" and platform_machine != \"arm64\"' don't match your environment\r\nIgnoring bitsandbytes: markers 'sys_platform != \"darwin\" and platform_machine != \"arm64\"' don't match your environment\r\nIgnoring pypandoc_binary: markers 'platform_machine == \"x86_64\"' don't match your environment\r\nIgnoring pypandoc_binary: markers 'platform_system == \"Windows\"' don't match your environment\r\nIgnoring python-magic-bin: markers 'platform_system == \"Windows\"' don't match your environment\r\nCollecting InstructorEmbedding@ https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl (from -r requirements.txt (line 74))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl (20 kB)\r\nCollecting sentence_transformers_old@ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl (from -r requirements.txt (line 75))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl (125 kB)\r\nCollecting gunicorn (from -r requirements.txt (line 12))\r\n  Using cached gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\r\nCollecting fastapi-utils (from -r requirements.txt (line 13))\r\n  Using cached fastapi_utils-0.7.0-py3-none-any.whl.metadata (5.4 kB)\r\nCollecting sse_starlette>=1.8.2 (from -r requirements.txt (line 14))\r\n  Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\r\nCollecting huggingface_hub>=0.23.3 (from -r requirements.txt (line 16))\r\n  Using cached huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\r\nCollecting appdirs>=1.4.4 (from -r requirements.txt (line 17))\r\n  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\r\nCollecting fire>=0.5.0 (from -r requirements.txt (line 18))\r\n  Downloading fire-0.6.0.tar.gz (88 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting docutils>=0.20.1 (from -r requirements.txt (line 19))\r\n  Downloading docutils-0.21.2-py3-none-any.whl.metadata (2.8 kB)\r\nCollecting torch==2.3.1 (from -r requirements.txt (line 21))\r\n  Using cached https://download.pytorch.org/whl/cpu/torch-2.3.1-cp310-none-macosx_11_0_arm64.whl (61.0 MB)\r\nCollecting evaluate>=0.4.0 (from -r requirements.txt (line 22))\r\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\r\nCollecting rouge_score>=0.1.2 (from -r requirements.txt (line 23))\r\n  Using cached rouge_score-0.1.2-py3-none-any.whl\r\nCollecting sacrebleu>=2.3.1 (from -r requirements.txt (line 24))\r\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\r\nCollecting scikit-learn>=1.2.2 (from -r requirements.txt (line 25))\r\n  Downloading scikit_learn-1.5.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (12 kB)\r\nCollecting numpy<2.0,>=1.23.4 (from -r requirements.txt (line 29))\r\n  Downloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\r\nCollecting pandas>=2.0.2 (from -r requirements.txt (line 30))\r\n  Downloading pandas-2.2.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (19 kB)\r\nCollecting matplotlib>=3.7.1 (from -r requirements.txt (line 31))\r\n  Downloading matplotlib-3.9.1.post1-cp310-cp310-macosx_11_0_arm64.whl.metadata (11 kB)\r\nCollecting loralib>=0.1.2 (from -r requirements.txt (line 34))\r\n  Using cached loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\r\nCollecting bitsandbytes==0.42.0 (from -r requirements.txt (line 37))\r\n  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\r\nCollecting accelerate>=0.30.1 (from -r requirements.txt (line 38))\r\n  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\r\nCollecting peft>=0.7.0 (from -r requirements.txt (line 39))\r\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\r\nCollecting transformers>=4.43.2 (from -r requirements.txt (line 40))\r\n  Using cached transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\r\nCollecting jinja2>=3.1.0 (from -r requirements.txt (line 41))\r\n  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\r\nCollecting tokenizers>=0.19.0 (from -r requirements.txt (line 42))\r\n  Using cached tokenizers-0.20.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\nCollecting hf_transfer>=0.1.6 (from -r requirements.txt (line 43))\r\n  Downloading hf_transfer-0.1.8-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.7 kB)\r\nCollecting optimum>=1.17.1 (from -r requirements.txt (line 44))\r\n  Using cached optimum-1.21.3-py3-none-any.whl.metadata (19 kB)\r\nCollecting datasets>=2.18.0 (from -r requirements.txt (line 45))\r\n  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\r\nCollecting sentencepiece>=0.2.0 (from -r requirements.txt (line 46))\r\n  Using cached sentencepiece-0.2.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\r\nCollecting APScheduler>=3.10.1 (from -r requirements.txt (line 48))\r\n  Downloading APScheduler-3.10.4-py3-none-any.whl.metadata (5.7 kB)\r\nCollecting pynvml>=11.5.0 (from -r requirements.txt (line 51))\r\n  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\r\nCollecting psutil>=5.9.5 (from -r requirements.txt (line 52))\r\n  Using cached psutil-6.0.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (21 kB)\r\nCollecting boto3>=1.26.101 (from -r requirements.txt (line 53))\r\n  Downloading boto3-1.34.158-py3-none-any.whl.metadata (6.6 kB)\r\nCollecting botocore>=1.29.101 (from -r requirements.txt (line 54))\r\n  Downloading botocore-1.34.158-py3-none-any.whl.metadata (5.7 kB)\r\nCollecting beautifulsoup4>=4.12.2 (from -r requirements.txt (line 55))\r\n  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\r\nCollecting markdown>=3.4.3 (from -r requirements.txt (line 56))\r\n  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\r\nCollecting pytest>=7.2.2 (from -r requirements.txt (line 59))\r\n  Downloading pytest-8.3.2-py3-none-any.whl.metadata (7.5 kB)\r\nCollecting pytest-xdist>=3.2.1 (from -r requirements.txt (line 60))\r\n  Downloading pytest_xdist-3.6.1-py3-none-any.whl.metadata (4.3 kB)\r\nCollecting nltk>=3.8.1 (from -r requirements.txt (line 61))\r\n  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\r\nCollecting textstat>=0.7.3 (from -r requirements.txt (line 62))\r\n  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\r\nCollecting pypandoc>=1.11 (from -r requirements.txt (line 64))\r\n  Using cached pypandoc-1.13-py3-none-any.whl.metadata (16 kB)\r\nCollecting openpyxl>=3.1.2 (from -r requirements.txt (line 68))\r\n  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\r\nCollecting lm_dataformat>=0.0.20 (from -r requirements.txt (line 69))\r\n  Downloading lm_dataformat-0.0.20-py3-none-any.whl.metadata (1.2 kB)\r\nCollecting bioc>=2.0 (from -r requirements.txt (line 70))\r\n  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\r\nCollecting sentence_transformers>=3.0.1 (from -r requirements.txt (line 73))\r\n  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\r\nCollecting einops>=0.6.1 (from -r requirements.txt (line 78))\r\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\r\nCollecting python-dotenv>=1.0.0 (from -r requirements.txt (line 81))\r\n  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\r\nCollecting json_repair>=0.21.0 (from -r requirements.txt (line 83))\r\n  Using cached json_repair-0.27.0-py3-none-any.whl.metadata (7.9 kB)\r\nCollecting text-generation>=0.7.0 (from -r requirements.txt (line 85))\r\n  Using cached text_generation-0.7.0-py3-none-any.whl.metadata (8.5 kB)\r\nCollecting tiktoken>=0.5.2 (from -r requirements.txt (line 88))\r\n  Using cached tiktoken-0.7.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\r\nCollecting openai>=1.40.1 (from -r requirements.txt (line 91))\r\n  Using cached openai-1.40.2-py3-none-any.whl.metadata (22 kB)\r\nCollecting requests>=2.31.0 (from -r requirements.txt (line 93))\r\n  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\r\nCollecting httpx>=0.24.1 (from -r requirements.txt (line 94))\r\n  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\r\nCollecting urllib3>=1.26.16 (from -r requirements.txt (line 95))\r\n  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\r\nCollecting filelock>=3.12.2 (from -r requirements.txt (line 96))\r\n  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\r\nCollecting joblib>=1.3.1 (from -r requirements.txt (line 97))\r\n  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\r\nCollecting tqdm>=4.65.0 (from -r requirements.txt (line 98))\r\n  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\r\nCollecting tabulate>=0.9.0 (from -r requirements.txt (line 99))\r\n  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\r\nCollecting packaging>=23.1 (from -r requirements.txt (line 100))\r\n  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\r\nCollecting uvicorn[standard] (from -r requirements.txt (line 11))\r\n  Downloading uvicorn-0.30.5-py3-none-any.whl.metadata (6.6 kB)\r\nCollecting typing-extensions>=4.8.0 (from torch==2.3.1->-r requirements.txt (line 21))\r\n  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting sympy (from torch==2.3.1->-r requirements.txt (line 21))\r\n  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\r\nCollecting networkx (from torch==2.3.1->-r requirements.txt (line 21))\r\n  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\r\nCollecting fsspec (from torch==2.3.1->-r requirements.txt (line 21))\r\n  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\r\nCollecting scipy (from bitsandbytes==0.42.0->-r requirements.txt (line 37))\r\n  Downloading scipy-1.14.0-cp310-cp310-macosx_14_0_arm64.whl.metadata (60 kB)\r\nCollecting aiofiles<24.0,>=22.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\r\nCollecting altair<6.0,>=4.2.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\r\nCollecting fastapi (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached fastapi-0.112.0-py3-none-any.whl.metadata (27 kB)\r\nCollecting ffmpy (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\r\nCollecting importlib-resources<7.0,>=1.3 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\r\nCollecting markupsafe~=2.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl (18 kB)\r\nCollecting orjson~=3.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached orjson-3.10.6-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (50 kB)\r\nCollecting pillow<11.0,>=8.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached pillow-10.4.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.2 kB)\r\nCollecting pydantic>=2.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached pydantic-2.7.0-py3-none-any.whl.metadata (103 kB)\r\nCollecting pydub (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\r\nCollecting python-multipart>=0.0.9 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\r\nCollecting pyyaml<7.0,>=5.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.1 kB)\r\nCollecting ruff>=0.2.2 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading ruff-0.5.7-py3-none-macosx_11_0_arm64.whl.metadata (25 kB)\r\nCollecting semantic-version~=2.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\r\nCollecting tomlkit==0.12.0 (from gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\r\nCollecting typer<1.0,>=0.9 (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\r\nCollecting websockets<12.0,>=10.0 (from gradio_client@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio_client-0.15.1-py3-none-any.whl->-r requirements.txt (line 9))\r\n  Downloading websockets-11.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\r\nCollecting click>=7.0 (from uvicorn[standard]->-r requirements.txt (line 11))\r\n  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting h11>=0.8 (from uvicorn[standard]->-r requirements.txt (line 11))\r\n  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\r\nCollecting httptools>=0.5.0 (from uvicorn[standard]->-r requirements.txt (line 11))\r\n  Using cached httptools-0.6.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (3.6 kB)\r\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->-r requirements.txt (line 11))\r\n  Using cached uvloop-0.19.0-cp310-cp310-macosx_10_9_universal2.whl.metadata (4.9 kB)\r\nCollecting watchfiles>=0.13 (from uvicorn[standard]->-r requirements.txt (line 11))\r\n  Downloading watchfiles-0.23.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.9 kB)\r\nCollecting psutil>=5.9.5 (from -r requirements.txt (line 52))\r\n  Using cached psutil-5.9.8-cp38-abi3-macosx_11_0_arm64.whl.metadata (21 kB)\r\nCollecting starlette (from sse_starlette>=1.8.2->-r requirements.txt (line 14))\r\n  Downloading starlette-0.38.2-py3-none-any.whl.metadata (5.9 kB)\r\nCollecting anyio (from sse_starlette>=1.8.2->-r requirements.txt (line 14))\r\n  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\r\nCollecting six (from fire>=0.5.0->-r requirements.txt (line 18))\r\n  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\r\nCollecting termcolor (from fire>=0.5.0->-r requirements.txt (line 18))\r\n  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\r\nCollecting dill (from evaluate>=0.4.0->-r requirements.txt (line 22))\r\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\nCollecting xxhash (from evaluate>=0.4.0->-r requirements.txt (line 22))\r\n  Using cached xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\r\nCollecting multiprocess (from evaluate>=0.4.0->-r requirements.txt (line 22))\r\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\r\nCollecting absl-py (from rouge_score>=0.1.2->-r requirements.txt (line 23))\r\n  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\r\nCollecting portalocker (from sacrebleu>=2.3.1->-r requirements.txt (line 24))\r\n  Using cached portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\r\nCollecting regex (from sacrebleu>=2.3.1->-r requirements.txt (line 24))\r\n  Downloading regex-2024.7.24-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\r\nCollecting colorama (from sacrebleu>=2.3.1->-r requirements.txt (line 24))\r\n  Using cached https://download.pytorch.org/whl/colorama-0.4.6-py2.py3-none-any.whl (25 kB)\r\nCollecting lxml (from sacrebleu>=2.3.1->-r requirements.txt (line 24))\r\n  Downloading lxml-5.2.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (3.4 kB)\r\nCollecting threadpoolctl>=3.1.0 (from scikit-learn>=1.2.2->-r requirements.txt (line 25))\r\n  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\r\nCollecting python-dateutil>=2.8.2 (from pandas>=2.0.2->-r requirements.txt (line 30))\r\n  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\r\nCollecting pytz>=2020.1 (from pandas>=2.0.2->-r requirements.txt (line 30))\r\n  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\r\nCollecting tzdata>=2022.7 (from pandas>=2.0.2->-r requirements.txt (line 30))\r\n  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\r\nCollecting contourpy>=1.0.1 (from matplotlib>=3.7.1->-r requirements.txt (line 31))\r\n  Downloading contourpy-1.2.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.8 kB)\r\nCollecting cycler>=0.10 (from matplotlib>=3.7.1->-r requirements.txt (line 31))\r\n  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\r\nCollecting fonttools>=4.22.0 (from matplotlib>=3.7.1->-r requirements.txt (line 31))\r\n  Downloading fonttools-4.53.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (162 kB)\r\nCollecting kiwisolver>=1.3.1 (from matplotlib>=3.7.1->-r requirements.txt (line 31))\r\n  Using cached kiwisolver-1.4.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.4 kB)\r\nCollecting pyparsing>=2.3.1 (from matplotlib>=3.7.1->-r requirements.txt (line 31))\r\n  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\r\nCollecting safetensors>=0.3.1 (from accelerate>=0.30.1->-r requirements.txt (line 38))\r\n  Downloading safetensors-0.4.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.8 kB)\r\nCollecting tokenizers>=0.19.0 (from -r requirements.txt (line 42))\r\n  Using cached tokenizers-0.19.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\nCollecting coloredlogs (from optimum>=1.17.1->-r requirements.txt (line 44))\r\n  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\nCollecting transformers>=4.43.2 (from -r requirements.txt (line 40))\r\n  Using cached transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\r\nCollecting pyarrow>=15.0.0 (from datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached pyarrow-17.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.3 kB)\r\nCollecting pyarrow-hotfix (from datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\r\nCollecting fsspec (from torch==2.3.1->-r requirements.txt (line 21))\r\n  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\r\nCollecting aiohttp (from datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached aiohttp-3.10.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.5 kB)\r\nCollecting tzlocal!=3.*,>=2.0 (from APScheduler>=3.10.1->-r requirements.txt (line 48))\r\n  Using cached tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\r\nCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.26.101->-r requirements.txt (line 53))\r\n  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\r\nCollecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.26.101->-r requirements.txt (line 53))\r\n  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\r\nCollecting soupsieve>1.2 (from beautifulsoup4>=4.12.2->-r requirements.txt (line 55))\r\n  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\r\nCollecting iniconfig (from pytest>=7.2.2->-r requirements.txt (line 59))\r\n  Using cached iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\r\nCollecting pluggy<2,>=1.5 (from pytest>=7.2.2->-r requirements.txt (line 59))\r\n  Using cached pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\r\nCollecting exceptiongroup>=1.0.0rc8 (from pytest>=7.2.2->-r requirements.txt (line 59))\r\n  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\r\nCollecting tomli>=1 (from pytest>=7.2.2->-r requirements.txt (line 59))\r\n  Using cached tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\r\nCollecting execnet>=2.1 (from pytest-xdist>=3.2.1->-r requirements.txt (line 60))\r\n  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\r\nCollecting pyphen (from textstat>=0.7.3->-r requirements.txt (line 62))\r\n  Downloading pyphen-0.16.0-py3-none-any.whl.metadata (3.2 kB)\r\nRequirement already satisfied: setuptools in /Users/kapenge/miniconda3/envs/h2ogpt/lib/python3.10/site-packages (from textstat>=0.7.3->-r requirements.txt (line 62)) (72.1.0)\r\nCollecting et-xmlfile (from openpyxl>=3.1.2->-r requirements.txt (line 68))\r\n  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\r\nCollecting jsonlines (from lm_dataformat>=0.0.20->-r requirements.txt (line 69))\r\n  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\r\nCollecting ujson (from lm_dataformat>=0.0.20->-r requirements.txt (line 69))\r\n  Downloading ujson-5.10.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.3 kB)\r\nCollecting zstandard (from lm_dataformat>=0.0.20->-r requirements.txt (line 69))\r\n  Downloading zstandard-0.23.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.0 kB)\r\nCollecting intervaltree (from bioc>=2.0->-r requirements.txt (line 70))\r\n  Using cached intervaltree-3.1.0-py2.py3-none-any.whl\r\nCollecting docopt (from bioc>=2.0->-r requirements.txt (line 70))\r\n  Downloading docopt-0.6.2.tar.gz (25 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting torchvision (from sentence_transformers_old@ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl->-r requirements.txt (line 75))\r\n  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.19.0-cp310-cp310-macosx_11_0_arm64.whl (1.7 MB)\r\nCollecting distro<2,>=1.7.0 (from openai>=1.40.1->-r requirements.txt (line 91))\r\n  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\r\nCollecting jiter<1,>=0.4.0 (from openai>=1.40.1->-r requirements.txt (line 91))\r\n  Using cached jiter-0.5.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.6 kB)\r\nCollecting sniffio (from openai>=1.40.1->-r requirements.txt (line 91))\r\n  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\r\nCollecting charset-normalizer<4,>=2 (from requests>=2.31.0->-r requirements.txt (line 93))\r\n  Using cached charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (33 kB)\r\nCollecting idna<4,>=2.5 (from requests>=2.31.0->-r requirements.txt (line 93))\r\n  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\r\nCollecting certifi>=2017.4.17 (from requests>=2.31.0->-r requirements.txt (line 93))\r\n  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\r\nCollecting httpcore==1.* (from httpx>=0.24.1->-r requirements.txt (line 94))\r\n  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\r\nCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached aiohappyeyeballs-2.3.5-py3-none-any.whl.metadata (5.8 kB)\r\nCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\r\nCollecting attrs>=17.3.0 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\r\nCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\r\nCollecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Downloading multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.2 kB)\r\nCollecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (31 kB)\r\nCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 45))\r\n  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\r\nCollecting jsonschema>=3.0 (from altair<6.0,>=4.2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\r\nCollecting toolz (from altair<6.0,>=4.2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\r\nCollecting starlette (from sse_starlette>=1.8.2->-r requirements.txt (line 14))\r\n  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\r\nCollecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\r\nCollecting pydantic-core==2.18.1 (from pydantic>=2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached pydantic_core-2.18.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.5 kB)\r\nCollecting protobuf (from transformers[sentencepiece]<4.44.0,>=4.29.0->optimum>=1.17.1->-r requirements.txt (line 44))\r\n  Downloading protobuf-5.27.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\r\nCollecting shellingham>=1.3.0 (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\r\nCollecting rich>=10.11.0 (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\r\nWARNING: typer 0.12.3 does not provide the extra 'all'\r\nCollecting humanfriendly>=9.1 (from coloredlogs->optimum>=1.17.1->-r requirements.txt (line 44))\r\n  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\nCollecting sortedcontainers<3.0,>=2.0 (from intervaltree->bioc>=2.0->-r requirements.txt (line 70))\r\n  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\r\nCollecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.3.1->-r requirements.txt (line 21))\r\n  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\r\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\r\nCollecting torchvision (from sentence_transformers_old@ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl->-r requirements.txt (line 75))\r\n  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.18.1-cp310-cp310-macosx_11_0_arm64.whl (1.6 MB)\r\nCollecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\r\nCollecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\r\nCollecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Downloading rpds_py-0.20.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.2 kB)\r\nCollecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\r\nCollecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\r\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.26.0-py3-none-any.whl->-r requirements.txt (line 8))\r\n  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\r\nUsing cached bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\r\nUsing cached tomlkit-0.12.0-py3-none-any.whl (37 kB)\r\nUsing cached gunicorn-22.0.0-py3-none-any.whl (84 kB)\r\nUsing cached fastapi_utils-0.7.0-py3-none-any.whl (18 kB)\r\nDownloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\r\nUsing cached huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\r\nUsing cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\nDownloading docutils-0.21.2-py3-none-any.whl (587 kB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 587.4/587.4 kB 3.5 MB/s eta 0:00:00\r\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\r\nDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\r\nDownloading scikit_learn-1.5.1-cp310-cp310-macosx_12_0_arm64.whl (11.0 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/11.0 MB 2.9 MB/s eta 0:00:00\r\nDownloading numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.0/14.0 MB 3.0 MB/s eta 0:00:00\r\nDownloading pandas-2.2.2-cp310-cp310-macosx_11_0_arm64.whl (11.3 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 3.1 MB/s eta 0:00:00\r\nDownloading matplotlib-3.9.1.post1-cp310-cp310-macosx_11_0_arm64.whl (7.8 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 2.3 MB/s eta 0:00:00\r\nUsing cached loralib-0.1.2-py3-none-any.whl (10 kB)\r\nUsing cached accelerate-0.33.0-py3-none-any.whl (315 kB)\r\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\r\nUsing cached jinja2-3.1.4-py3-none-any.whl (133 kB)\r\nUsing cached tokenizers-0.19.1-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\r\nDownloading hf_transfer-0.1.8-cp310-cp310-macosx_11_0_arm64.whl (1.4 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 2.2 MB/s eta 0:00:00\r\nUsing cached optimum-1.21.3-py3-none-any.whl (421 kB)\r\nUsing cached transformers-4.43.4-py3-none-any.whl (9.4 MB)\r\nUsing cached datasets-2.20.0-py3-none-any.whl (547 kB)\r\nUsing cached sentencepiece-0.2.0-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\r\nDownloading APScheduler-3.10.4-py3-none-any.whl (59 kB)\r\nDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\r\nUsing cached psutil-5.9.8-cp38-abi3-macosx_11_0_arm64.whl (249 kB)\r\nDownloading boto3-1.34.158-py3-none-any.whl (139 kB)\r\nDownloading botocore-1.34.158-py3-none-any.whl (12.5 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/12.5 MB 2.5 MB/s eta 0:00:00\r\nUsing cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\r\nDownloading Markdown-3.6-py3-none-any.whl (105 kB)\r\nDownloading pytest-8.3.2-py3-none-any.whl (341 kB)\r\nDownloading pytest_xdist-3.6.1-py3-none-any.whl (46 kB)\r\nUsing cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\nDownloading textstat-0.7.4-py3-none-any.whl (105 kB)\r\nUsing cached pypandoc-1.13-py3-none-any.whl (21 kB)\r\nDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\r\nUsing cached lm_dataformat-0.0.20-py3-none-any.whl (5.8 kB)\r\nDownloading bioc-2.1-py3-none-any.whl (33 kB)\r\nUsing cached sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\r\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\r\nUsing cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\r\nUsing cached json_repair-0.27.0-py3-none-any.whl (12 kB)\r\nUsing cached text_generation-0.7.0-py3-none-any.whl (12 kB)\r\nUsing cached tiktoken-0.7.0-cp310-cp310-macosx_11_0_arm64.whl (906 kB)\r\nUsing cached openai-1.40.2-py3-none-any.whl (360 kB)\r\nUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\r\nUsing cached httpx-0.27.0-py3-none-any.whl (75 kB)\r\nUsing cached httpcore-1.0.5-py3-none-any.whl (77 kB)\r\nUsing cached urllib3-2.2.2-py3-none-any.whl (121 kB)\r\nUsing cached filelock-3.15.4-py3-none-any.whl (16 kB)\r\nUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\r\nUsing cached tqdm-4.66.5-py3-none-any.whl (78 kB)\r\nUsing cached tabulate-0.9.0-py3-none-any.whl (35 kB)\r\nUsing cached packaging-24.1-py3-none-any.whl (53 kB)\r\nUsing cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\r\nUsing cached aiohttp-3.10.2-cp310-cp310-macosx_11_0_arm64.whl (387 kB)\r\nUsing cached altair-5.3.0-py3-none-any.whl (857 kB)\r\nUsing cached anyio-4.4.0-py3-none-any.whl (86 kB)\r\nUsing cached certifi-2024.7.4-py3-none-any.whl (162 kB)\r\nUsing cached charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl (120 kB)\r\nUsing cached click-8.1.7-py3-none-any.whl (97 kB)\r\nDownloading contourpy-1.2.1-cp310-cp310-macosx_11_0_arm64.whl (244 kB)\r\nUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\r\nDownloading dill-0.3.8-py3-none-any.whl (116 kB)\r\nUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\r\nDownloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\r\nDownloading execnet-2.1.1-py3-none-any.whl (40 kB)\r\nUsing cached fastapi-0.112.0-py3-none-any.whl (93 kB)\r\nDownloading fonttools-4.53.1-cp310-cp310-macosx_11_0_arm64.whl (2.2 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 2.6 MB/s eta 0:00:00\r\nUsing cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\r\nUsing cached h11-0.14.0-py3-none-any.whl (58 kB)\r\nUsing cached httptools-0.6.1-cp310-cp310-macosx_10_9_universal2.whl (149 kB)\r\nUsing cached idna-3.7-py3-none-any.whl (66 kB)\r\nUsing cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\r\nUsing cached jiter-0.5.0-cp310-cp310-macosx_11_0_arm64.whl (299 kB)\r\nUsing cached jmespath-1.0.1-py3-none-any.whl (20 kB)\r\nUsing cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\r\nUsing cached kiwisolver-1.4.5-cp310-cp310-macosx_11_0_arm64.whl (66 kB)\r\nDownloading lxml-5.2.2-cp310-cp310-macosx_10_9_universal2.whl (8.1 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.1/8.1 MB 2.2 MB/s eta 0:00:00\r\nUsing cached orjson-3.10.6-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (250 kB)\r\nUsing cached pillow-10.4.0-cp310-cp310-macosx_11_0_arm64.whl (3.4 MB)\r\nUsing cached pluggy-1.5.0-py3-none-any.whl (20 kB)\r\nUsing cached pyarrow-17.0.0-cp310-cp310-macosx_11_0_arm64.whl (27.2 MB)\r\nUsing cached pydantic-2.7.0-py3-none-any.whl (407 kB)\r\nUsing cached pydantic_core-2.18.1-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\r\nUsing cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\r\nUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\r\nUsing cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\r\nUsing cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\r\nDownloading PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl (171 kB)\r\nDownloading regex-2024.7.24-cp310-cp310-macosx_11_0_arm64.whl (278 kB)\r\nDownloading ruff-0.5.7-py3-none-macosx_11_0_arm64.whl (8.3 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.3/8.3 MB 1.1 MB/s eta 0:00:00\r\nUsing cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\r\nDownloading safetensors-0.4.4-cp310-cp310-macosx_11_0_arm64.whl (381 kB)\r\nDownloading scipy-1.14.0-cp310-cp310-macosx_14_0_arm64.whl (23.1 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.1/23.1 MB 762.5 kB/s eta 0:00:00\r\nUsing cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\nUsing cached six-1.16.0-py2.py3-none-any.whl (11 kB)\r\nUsing cached sniffio-1.3.1-py3-none-any.whl (10 kB)\r\nUsing cached soupsieve-2.5-py3-none-any.whl (36 kB)\r\nUsing cached starlette-0.37.2-py3-none-any.whl (71 kB)\r\nDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\r\nUsing cached tomli-2.0.1-py3-none-any.whl (12 kB)\r\nUsing cached typer-0.12.3-py3-none-any.whl (47 kB)\r\nUsing cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\r\nUsing cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\r\nUsing cached tzlocal-5.2-py3-none-any.whl (17 kB)\r\nDownloading uvicorn-0.30.5-py3-none-any.whl (62 kB)\r\nUsing cached uvloop-0.19.0-cp310-cp310-macosx_10_9_universal2.whl (1.4 MB)\r\nDownloading watchfiles-0.23.0-cp310-cp310-macosx_11_0_arm64.whl (369 kB)\r\nUsing cached websockets-11.0.3-cp310-cp310-macosx_11_0_arm64.whl (121 kB)\r\nUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\r\nUsing cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\nUsing cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\r\nDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\r\nUsing cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\r\nDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\r\nDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 909.3 kB/s eta 0:00:00\r\nUsing cached portalocker-2.10.1-py3-none-any.whl (18 kB)\r\nUsing cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\r\nUsing cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\r\nDownloading pyphen-0.16.0-py3-none-any.whl (2.1 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 918.1 kB/s eta 0:00:00\r\nDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 921.7 kB/s eta 0:00:00\r\nUsing cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\r\nDownloading ujson-5.10.0-cp310-cp310-macosx_11_0_arm64.whl (51 kB)\r\nUsing cached xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\r\nDownloading zstandard-0.23.0-cp310-cp310-macosx_11_0_arm64.whl (633 kB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 633.7/633.7 kB 861.3 kB/s eta 0:00:00\r\nUsing cached aiohappyeyeballs-2.3.5-py3-none-any.whl (12 kB)\r\nUsing cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\r\nUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\r\nUsing cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\r\nDownloading attrs-24.2.0-py3-none-any.whl (63 kB)\r\nUsing cached frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\r\nUsing cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\nUsing cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\r\nDownloading multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\r\nUsing cached rich-13.7.1-py3-none-any.whl (240 kB)\r\nUsing cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\r\nUsing cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\r\nUsing cached yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl (79 kB)\r\nDownloading protobuf-5.27.3-cp38-abi3-macosx_10_9_universal2.whl (412 kB)\r\nUsing cached toolz-0.12.1-py3-none-any.whl (56 kB)\r\nUsing cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\r\nUsing cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\nUsing cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\r\nUsing cached referencing-0.35.1-py3-none-any.whl (26 kB)\r\nDownloading rpds_py-0.20.0-cp310-cp310-macosx_11_0_arm64.whl (311 kB)\r\nUsing cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\nBuilding wheels for collected packages: fire, docopt\r\n  Building wheel for fire (setup.py) ... done\r\n  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117030 sha256=7185a126050d4222c7b567a05444f5ee59c01d506a821f71e871b44a7d189e18\r\n  Stored in directory: /Users/kapenge/Library/Caches/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\r\n  Building wheel for docopt (setup.py) ... done\r\n  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=be75b6164fc47568d338658282162b0ee172034cd5ecb69285c9e7a743845da3\r\n  Stored in directory: /Users/kapenge/Library/Caches/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\r\nSuccessfully built fire docopt\r\nInstalling collected packages: sortedcontainers, sentencepiece, pytz, pydub, mpmath, InstructorEmbedding, docopt, appdirs, zstandard, xxhash, websockets, uvloop, urllib3, ujson, tzlocal, tzdata, typing-extensions, tqdm, toolz, tomlkit, tomli, threadpoolctl, termcolor, tabulate, sympy, soupsieve, sniffio, six, shellingham, semantic-version, safetensors, ruff, rpds-py, regex, pyyaml, python-multipart, python-dotenv, pyphen, pyparsing, pypandoc, pynvml, pygments, pyarrow-hotfix, psutil, protobuf, portalocker, pluggy, pillow, packaging, orjson, numpy, networkx, multidict, mdurl, markupsafe, markdown, lxml, loralib, kiwisolver, json_repair, joblib, jmespath, jiter, intervaltree, iniconfig, importlib-resources, idna, humanfriendly, httptools, hf_transfer, h11, fsspec, frozenlist, fonttools, filelock, ffmpy, execnet, exceptiongroup, et-xmlfile, einops, docutils, distro, dill, cycler, colorama, click, charset-normalizer, certifi, attrs, async-timeout, annotated-types, aiohappyeyeballs, aiofiles, absl-py, yarl, uvicorn, textstat, scipy, sacrebleu, requests, referencing, python-dateutil, pytest, pydantic-core, pyarrow, openpyxl, nltk, multiprocess, markdown-it-py, jsonlines, jinja2, httpcore, gunicorn, fire, contourpy, coloredlogs, beautifulsoup4, APScheduler, anyio, aiosignal, watchfiles, torch, tiktoken, starlette, scikit-learn, rouge_score, rich, pytest-xdist, pydantic, pandas, matplotlib, lm_dataformat, jsonschema-specifications, huggingface_hub, httpx, botocore, bitsandbytes, bioc, aiohttp, typer, torchvision, tokenizers, text-generation, sse_starlette, s3transfer, openai, jsonschema, gradio_client, fastapi, accelerate, transformers, fastapi-utils, datasets, boto3, altair, sentence_transformers_old, sentence_transformers, peft, gradio, evaluate, optimum\r\nSuccessfully installed APScheduler-3.10.4 InstructorEmbedding-1.0.1 absl-py-2.1.0 accelerate-0.33.0 aiofiles-23.2.1 aiohappyeyeballs-2.3.5 aiohttp-3.10.2 aiosignal-1.3.1 altair-5.3.0 annotated-types-0.7.0 anyio-4.4.0 appdirs-1.4.4 async-timeout-4.0.3 attrs-24.2.0 beautifulsoup4-4.12.3 bioc-2.1 bitsandbytes-0.42.0 boto3-1.34.158 botocore-1.34.158 certifi-2024.7.4 charset-normalizer-3.3.2 click-8.1.7 colorama-0.4.6 coloredlogs-15.0.1 contourpy-1.2.1 cycler-0.12.1 datasets-2.20.0 dill-0.3.8 distro-1.9.0 docopt-0.6.2 docutils-0.21.2 einops-0.8.0 et-xmlfile-1.1.0 evaluate-0.4.2 exceptiongroup-1.2.2 execnet-2.1.1 fastapi-0.112.0 fastapi-utils-0.7.0 ffmpy-0.4.0 filelock-3.15.4 fire-0.6.0 fonttools-4.53.1 frozenlist-1.4.1 fsspec-2024.5.0 gradio-4.26.0 gradio_client-0.15.1 gunicorn-22.0.0 h11-0.14.0 hf_transfer-0.1.8 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface_hub-0.24.5 humanfriendly-10.0 idna-3.7 importlib-resources-6.4.0 iniconfig-2.0.0 intervaltree-3.1.0 jinja2-3.1.4 jiter-0.5.0 jmespath-1.0.1 joblib-1.4.2 json_repair-0.27.0 jsonlines-4.0.0 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 kiwisolver-1.4.5 lm_dataformat-0.0.20 loralib-0.1.2 lxml-5.2.2 markdown-3.6 markdown-it-py-3.0.0 markupsafe-2.1.5 matplotlib-3.9.1.post1 mdurl-0.1.2 mpmath-1.3.0 multidict-6.0.5 multiprocess-0.70.16 networkx-3.3 nltk-3.8.1 numpy-1.26.4 openai-1.40.2 openpyxl-3.1.5 optimum-1.21.3 orjson-3.10.6 packaging-24.1 pandas-2.2.2 peft-0.12.0 pillow-10.4.0 pluggy-1.5.0 portalocker-2.10.1 protobuf-5.27.3 psutil-5.9.8 pyarrow-17.0.0 pyarrow-hotfix-0.6 pydantic-2.7.0 pydantic-core-2.18.1 pydub-0.25.1 pygments-2.18.0 pynvml-11.5.3 pypandoc-1.13 pyparsing-3.1.2 pyphen-0.16.0 pytest-8.3.2 pytest-xdist-3.6.1 python-dateutil-2.9.0.post0 python-dotenv-1.0.1 python-multipart-0.0.9 pytz-2024.1 pyyaml-6.0.2 referencing-0.35.1 regex-2024.7.24 requests-2.32.3 rich-13.7.1 rouge_score-0.1.2 rpds-py-0.20.0 ruff-0.5.7 s3transfer-0.10.2 sacrebleu-2.4.2 safetensors-0.4.4 scikit-learn-1.5.1 scipy-1.14.0 semantic-version-2.10.0 sentence_transformers-3.0.1 sentence_transformers_old-2.2.2 sentencepiece-0.2.0 shellingham-1.5.4 six-1.16.0 sniffio-1.3.1 sortedcontainers-2.4.0 soupsieve-2.5 sse_starlette-2.1.3 starlette-0.37.2 sympy-1.13.1 tabulate-0.9.0 termcolor-2.4.0 text-generation-0.7.0 textstat-0.7.4 threadpoolctl-3.5.0 tiktoken-0.7.0 tokenizers-0.19.1 tomli-2.0.1 tomlkit-0.12.0 toolz-0.12.1 torch-2.3.1 torchvision-0.18.1 tqdm-4.66.5 transformers-4.43.4 typer-0.12.3 typing-extensions-4.12.2 tzdata-2024.1 tzlocal-5.2 ujson-5.10.0 urllib3-2.2.2 uvicorn-0.30.5 uvloop-0.19.0 watchfiles-0.23.0 websockets-11.0.3 xxhash-3.4.1 yarl-1.9.4 zstandard-0.23.0\r\n❯\r\n❯\r\n❯ pip install -r reqs_optional/requirements_optional_langchain.txt -c reqs_optional/reqs_constraints.txt\r\nIgnoring torch: markers 'sys_platform != \"darwin\" and platform_machine != \"arm64\"' don't match your environment\r\nCollecting InstructorEmbedding@ https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl (from -r reqs_optional/requirements_optional_langchain.txt (line 23))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl (20 kB)\r\nCollecting sentence_transformers_old@ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl (from -r reqs_optional/requirements_optional_langchain.txt (line 26))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl (125 kB)\r\nCollecting llava@ https://h2o-release.s3.amazonaws.com/h2ogpt/llava-1.7.0.dev0-py3-none-any.whl (from -r reqs_optional/requirements_optional_langchain.txt (line 41))\r\n  Using cached https://h2o-release.s3.amazonaws.com/h2ogpt/llava-1.7.0.dev0-py3-none-any.whl (87 kB)\r\nIgnoring jq: markers 'platform_machine == \"x86_64\"' don't match your environment\r\nERROR: Cannot install torch==2.2.1 because these package versions have conflicting dependencies.\r\n\r\nThe conflict is caused by:\r\n    The user requested torch==2.2.1\r\n    The user requested (constraint) torch==2.3.1\r\n\r\nTo fix this you could try to:\r\n1. loosen the range of package versions you've specified\r\n2. remove package versions to allow pip to attempt to solve the dependency conflict\r\n\r\nERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\r\n```"
      },
      {
        "user": "a-ml",
        "body": "Tried the container option with no luck at all, at this moment.\r\n\r\n\r\nchanged this part of the build as the document suggest.\r\n\r\n```\r\n# if building for CPU, would remove CMAKE_ARGS and avoid GPU image as base image\r\n# Choose llama_cpp_python ARGS for your system according to [llama_cpp_python backend documentation](https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#supported-backends), e.g. for CUDA:\r\n# export GGML_CUDA=1\r\n# export CMAKE_ARGS=\"-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=all\"\r\n# for Metal MAC M1/M2 comment out above two lines and uncomment out the below line\r\nexport CMAKE_ARGS=\"-DLLAMA_METAL=on\"\r\nexport FORCE_CMAKE=1\r\n```\r\n\r\nBuild Error\r\n\r\n```\r\n❯ docker build -t h2ogpt .\r\n[+] Building 2.4s (9/13)                                                                                                                             docker:desktop-linux\r\n => [internal] load build definition from Dockerfile                                                                                                                 0.0s\r\n => => transferring dockerfile: 1.15kB                                                                                                                               0.0s\r\n => [internal] load metadata for docker.io/nvidia/cuda:12.1.1-cudnn8-devel-ubuntu20.04                                                                               2.3s\r\n => [internal] load .dockerignore                                                                                                                                    0.0s\r\n => => transferring context: 236B                                                                                                                                    0.0s\r\n => CANCELED [1/9] FROM docker.io/nvidia/cuda:12.1.1-cudnn8-devel-ubuntu20.04@sha256:db71d4ff90d59028b6d364df7400ea65601d673b96a49cf81e7cda85cf7cbc31                0.0s\r\n => => resolve docker.io/nvidia/cuda:12.1.1-cudnn8-devel-ubuntu20.04@sha256:db71d4ff90d59028b6d364df7400ea65601d673b96a49cf81e7cda85cf7cbc31                         0.0s\r\n => => sha256:f68369b047ebfe2343393aae2a6996206e0bacf574c543685bebd92f55f59267 16.34kB / 16.34kB                                                                     0.0s\r\n => => sha256:db71d4ff90d59028b6d364df7400ea65601d673b96a49cf81e7cda85cf7cbc31 743B / 743B                                                                           0.0s\r\n => => sha256:9f8fb265f5a9d3188a0d3ad13288b06edaf7e89a7dc2aaa89b732a1e57071627 2.84kB / 2.84kB                                                                       0.0s\r\n => [internal] load build context                                                                                                                                    0.0s\r\n => => transferring context: 38.15kB                                                                                                                                 0.0s\r\n => CACHED [2/9] WORKDIR /workspace                                                                                                                                  0.0s\r\n => CACHED [3/9] COPY . /workspace/                                                                                                                                  0.0s\r\n => CACHED [4/9] COPY build_info.txt /workspace/                                                                                                                     0.0s\r\n => ERROR [5/9] COPY git_hash.txt /workspace/                                                                                                                        0.0s\r\n------\r\n > [5/9] COPY git_hash.txt /workspace/:\r\n------\r\nDockerfile:21\r\n--------------------\r\n  19 |     COPY build_info.txt /workspace/\r\n  20 |\r\n  21 | >>> COPY git_hash.txt /workspace/\r\n  22 |\r\n  23 |     RUN cd /workspace && ./docker_build_script_ubuntu.sh\r\n--------------------\r\nERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref 238fd63f-a388-4d60-8789-00a308c0d7e5::wla783xpjl7r6j5p8rnu2xnmi: \"/git_hash.txt\": not found\r\n\r\nView build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/v6r8wk5ry197rqvzd39h4hh4y\r\n\r\n```\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "`git_hash.txt` comes from the Makefile step.\r\n\r\n```\r\nmake git_hash.txt build_info.txt\r\n```\r\n\r\ni.e. one would do:\r\n\r\n```\r\nmake docker_build\r\n```\r\n\r\nBut I agree needing Makefile is bad.  @achraf-mer can we avoid so Dockerfile builds on its own?"
      },
      {
        "user": "pseudotensor",
        "body": "I removed it's need here: 282aec97e9a8f3dc4b513f33afb00e187ae857ec"
      },
      {
        "user": "a-ml",
        "body": "The Docker Build ends with this error. Should I change the Miniconda image?\r\n\r\n```\r\n266.6 2024-08-10 14:26:56 (2.84 MB/s) - 'Miniconda3-latest-Linux-x86_64.sh' saved [146836934/146836934]\r\n266.6\r\n266.6 + mkdir -p /h2ogpt_conda\r\n266.6 + bash ./Miniconda3-latest-Linux-x86_64.sh -b -u -p /h2ogpt_conda\r\n266.6 PREFIX=/h2ogpt_conda\r\n267.0 Unpacking payload ...\r\n267.0 rosetta error: failed to open elf at /lib64/ld-linux-x86-64.so.2\r\n267.0  ./Miniconda3-latest-Linux-x86_64.sh: line 442:  8661 Exit 141                extract_range $boundary1 $boundary2\r\n267.0       8662 Trace/breakpoint trap   | CONDA_QUIET=\"$BATCH\" \"$CONDA_EXEC\" constructor --extract-tarball --prefix \"$PREFIX\"\r\n267.0 + export CMAKE_ARGS=-DLLAMA_METAL=on\r\n267.0 + CMAKE_ARGS=-DLLAMA_METAL=on\r\n267.0 + export FORCE_CMAKE=1\r\n267.0 + FORCE_CMAKE=1\r\n267.0 + bash docs/linux_install.sh\r\n267.0 + shopt -s expand_aliases\r\n267.0 + test -f /usr/bin/sudo\r\n267.0 + echo 'No sudo'\r\n267.0 + alias 'sudo= '\r\n267.0 No sudo\r\n267.0 + [[ -z '' ]]\r\n267.0 + conda install weasyprint pygobject -c conda-forge -y\r\n267.0 docs/linux_install.sh: line 15: conda: command not found\r\n------\r\nDockerfile:21\r\n--------------------\r\n  19 |     COPY build_info.txt /workspace/\r\n  20 |\r\n  21 | >>> RUN cd /workspace && ./docker_build_script_ubuntu.sh\r\n  22 |\r\n  23 |     RUN chmod -R a+rwx /workspace\r\n--------------------\r\nERROR: failed to solve: process \"/bin/sh -c cd /workspace && ./docker_build_script_ubuntu.sh\" did not complete successfully: exit code: 127\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "To build the docker image for MAC, I expect some changes may be required, I'm unsure.  Thanks for the feedback."
      },
      {
        "user": "a-ml",
        "body": "I was abe to get it successfully installed. Instead of running the line bellow:\r\n\r\n`pip install -r reqs_optional/requirements_optional_langchain.txt -c reqs_optional/reqs_constraints.txt`\r\n\r\nI installed the langchain requirements by running the `reqs_optional/requirements_optional_langchain.txt`"
      }
    ]
  },
  {
    "issue_number": 1788,
    "title": "Chat history silently fails to download",
    "author": "antoninadert",
    "state": "closed",
    "created_at": "2024-08-08T09:21:21Z",
    "updated_at": "2024-08-09T18:02:16Z",
    "labels": [],
    "body": "No matter the chat history count, when I click \"Exports chats to Download\", it gives an empty file of 2.0b\r\n\r\nSee screenshot\r\n\r\n<img width=\"1108\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0489c5bf-2086-43a2-a35d-1f1941f455ff\">\r\n\r\n\r\nThere is no error in the command line. \r\n\r\nIs this a known issue ?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It only saves the \"chat saved\" items after clicking save.  The current chat view isn't saved except in database for the user."
      },
      {
        "user": "pseudotensor",
        "body": "![image](https://github.com/user-attachments/assets/91820c73-a27d-4d87-916e-1d818f1a105f)\r\n\r\n\r\n\r\nMake sure to click \"save\" among buttons to save it so it would be part of download.\r\n\r\n![image](https://github.com/user-attachments/assets/d875a7f1-82a8-473f-81cf-a8c204f5ce99)\r\n\r\n\r\nI would prefer it saved all things, but the database takes care of saving things for that general purpose."
      }
    ]
  },
  {
    "issue_number": 1787,
    "title": "TypeError: INSTRUCTOR._load_sbert_model() got an unexpected keyword argument 'token'",
    "author": "juerware",
    "state": "closed",
    "created_at": "2024-08-08T08:16:29Z",
    "updated_at": "2024-08-09T07:24:09Z",
    "labels": [],
    "body": "This is the command line lauched:\r\n```shell\r\npython generate.py \\\r\n    --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --prompt_type=llama2 \\\r\n    --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192 \\\r\n    --user_path=/opt/myDocuments/arsys.es/all/ --langchain_mode='UserData' --max_quality=True \\\r\n    --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True \\\r\n    --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True \\\r\n    --hf_embedding_model=\"hkunlp/instructor-base\" \\\r\n    --memory_restriction_level=0 --score_model=None --verbose=True --debug=True\r\n```\r\n---\r\nThis is the error found:\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/src/utils.py\", line 75, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/src/gen.py\", line 2015, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"/root/REPOSITORIES/aramirez/void_h2ogtp/src/gpt_langchain.py\", line 556, in get_embedding\r\n    embedding = HuggingFaceInstructEmbeddings(model_name=hf_embedding_model,\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 167, in __init__\r\n    self.client = INSTRUCTOR(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 287, in __init__\r\n    modules = self._load_sbert_model(\r\nTypeError: INSTRUCTOR._load_sbert_model() got an unexpected keyword argument 'token'\r\n```\r\n---\r\nSystem:\r\n- SO: Ubuntu 22.04 LTS utterly updated.\r\n- Commit: 7435b4bc (the last one at the moment this issue is written)\r\n- Environment utterly updated and clean after execution of script ```bash docs/linux_install_full.sh```\r\n---\r\nObservations:\r\n- This only happens with embedding models based on BERT\r\n- Embedding models as \"intfloat/multilingual-e5-small\" does not report any problem.\r\n---\r\nThanks for everything.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It seems the deps are not installed that should be.\r\n\r\nIn requierments_optional_langchain.txt it has:\r\n```\r\nsentence_transformers>=3.0.1\r\nInstructorEmbedding @ https://h2o-release.s3.amazonaws.com/h2ogpt/InstructorEmbedding-1.0.1-py3-none-any.whl\r\nsentence_transformers_old @ https://h2o-release.s3.amazonaws.com/h2ogpt/sentence_transformers_old-2.2.2-py3-none-any.whl\r\n```\r\n\r\nand for me I see no failure because inside `$HOME/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/InstructorEmbedding/instructor.py` it references only:\r\n```\r\nfrom sentence_transformers_old import SentenceTransformer\r\nfrom sentence_transformers_old.models import Transformer\r\n```\r\n\r\nCan you check your file and see what it shows?  I presume not the \"old\" ones but normal.  So explains failure.\r\n\r\nHowever, doesn't explain why you have wrong packages.  I just redid install and see these good packages used.\r\n\r\n\r\nYou should be able to do just:\r\n```\r\nfrom InstructorEmbedding import INSTRUCTOR\r\n```\r\nin python and it shouldn't fail.  So not related to h2oGPT itself, just those two packages.\r\n\r\nIn clean docker I also see this works fine."
      },
      {
        "user": "pseudotensor",
        "body": "677bf0817d3e342ffc32a31a94cf63cc05096660\r\n794ec254460a0c38a2e3ae3e4437f5dc0f695a09"
      },
      {
        "user": "pseudotensor",
        "body": "Building new image with above fix to see if order helps.  I saw in jenkins that earlier requirements.txt file triggered instructorembedding install, so needs to be early and not as late as langchain one."
      },
      {
        "user": "pseudotensor",
        "body": "Seems to work.  Thanks for reporting!"
      },
      {
        "user": "juerware",
        "body": "It´s working right now, thanks."
      }
    ]
  },
  {
    "issue_number": 1772,
    "title": "Implement custom function call.",
    "author": "anushaharish538",
    "state": "closed",
    "created_at": "2024-07-26T13:32:01Z",
    "updated_at": "2024-08-09T05:34:02Z",
    "labels": [],
    "body": "Hi,\r\n\r\nCan you please tell me how to implement custom function calling like. https://platform.openai.com/docs/guides/function-calling\r\n\r\n\r\nThank You\r\nAnusha",
    "comments": [
      {
        "user": "anushaharish538",
        "body": "hi @pseudotensor Can you please tell me . Please I'm using open_ai"
      },
      {
        "user": "pseudotensor",
        "body": "Hi, function calling is now part of h2oGPT OpenAI API, with 'auto' tool_choice, but it's basic at moment."
      }
    ]
  },
  {
    "issue_number": 1784,
    "title": "Running Llama 3.1 on Mac OS with m2 chip has errors",
    "author": "antoninadert",
    "state": "open",
    "created_at": "2024-08-05T08:22:16Z",
    "updated_at": "2024-08-08T10:51:14Z",
    "labels": [],
    "body": "I tried to run h2ogpt with this command : \r\n\r\n`python generate.py --base_model=meta-llama/Meta-Llama-3.1-8B-Instruct --use_auth_token=...`\r\n\r\nand it triggered errors\r\n\r\n```The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\r\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nthread exception: Traceback (most recent call last):\r\n  File \"/Users/.../h2ogpt/src/utils.py\", line 524, in run\r\n    self._return = self._target(*self._args, **self._kwargs)\r\n  File \"/Users/.../h2ogpt/src/gen.py\", line 4288, in generate_with_exceptions\r\n    func(*args, **kwargs)\r\n  File \"/Users/.../miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/Users/.../miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1727, in generate\r\n    model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\r\n  File \"/Users/.../miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/generation/utils.py\", line 493, in _prepare_attention_mask_for_generation\r\n    raise ValueError(\r\nValueError: Can't infer missing attention mask on `mps` device. Please provide an `attention_mask` or use a different device.```\r\n\r\nIt worked fine when I tried to run other older models like llama2 for example.\r\n\r\nDo you know what could be the source of this issue ?\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Looks like transformers bug: https://github.com/huggingface/transformers/issues/31744\r\n\r\nBut need new transformers for llama 3.1.\r\n\r\nMaybe stick to GGUF?"
      },
      {
        "user": "antoninadert",
        "body": "It worked fine with GGUF, I had to install different package version than what is recommended though. I will propose a pull request to be able to start with llama 3.1"
      },
      {
        "user": "antoninadert",
        "body": "See https://github.com/h2oai/h2ogpt/pull/1789 which is how I made llama 3.1 gguf work on Mac M2"
      }
    ]
  },
  {
    "issue_number": 1774,
    "title": "intfloat_multilingual-e5-large-instruct and file sentence_xlnet_config.json",
    "author": "juerware",
    "state": "closed",
    "created_at": "2024-07-30T06:58:29Z",
    "updated_at": "2024-08-02T08:09:44Z",
    "labels": [],
    "body": "I have executed the following command line:\r\n```shell\r\npython generate.py \\\r\n    --base_model=mistralai/Mistral-Nemo-Instruct-2407 --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192 \\\r\n    --user_path=/opt/myDocuments/all/ --langchain_mode='UserData' --max_quality=True \\\r\n    --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True \\\r\n    --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True \\\r\n    --hf_embedding_model=\"intfloat/multilingual-e5-large-instruct\" \\\r\n    --memory_restriction_level=0 --score_model=None --verbose=True --debug=True \\\r\n    --show_examples=True --compile_model=True \\\r\n    --share=True\r\n``` \r\nand I get the following error:\r\n```shell\r\nFileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/torch/sentence_transformers/intfloat_multilingual-e5-large-instruct/sentence_xlnet_config.json'\r\n```\r\n\r\nI have used the repository with the last commit at the time I write this issue:\r\n- Commit: 47e14672\r\n- SO: Ubuntu 20.04\r\n\r\nAs you can see the problem point out that problem is from the embedding model \"intfloat/multilingual-e5-large-instruct\", with the embedding model \"\"intfloat/multilingual-e5-large\" I did not get that kind of error.\r\n\r\nThanks in advance for help.",
    "comments": [
      {
        "user": "llmwesee",
        "body": "I'm also facing the same thing when running docker offline"
      },
      {
        "user": "pseudotensor",
        "body": "If I'm online, I see the same error.\r\n\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ python generate.py     --base_model=llama --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192     --user_path=user_path --langchain_mode='UserData' --max_quality=True     --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True     --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True     --hf_embedding_model=\"intfloat/multilingual-e5-large-instruct\"     --memory_restriction_level=0 --score_model=None --verbose=True --debug=True     --show_examples=True --compile_model=True     --share=True\r\n```\r\ngives:\r\n```\r\nCommand: generate.py --base_model=llama --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192 --user_path=user_path --langchain_mode=UserData --max_quality=True --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True --hf_embedding_model=intfloat/multilingual-e5-large-instruct --memory_restriction_level=0 --score_model=None --verbose=True --debug=True --show_examples=True --compile_model=True --share=True\r\nHash: 47e14672da4fc926aef7eeccd09d1119f112db3c\r\nWARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\r\n  warn_deprecated(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\r\n  warnings.warn(warning_message, FutureWarning)\r\n.gitattributes: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.57k/1.57k [00:00<00:00, 4.43MB/s]\r\n1_Pooling/config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 271/271 [00:00<00:00, 3.06MB/s]\r\nREADME.md: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 140k/140k [00:00<00:00, 40.4MB/s]\r\nconfig.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 690/690 [00:00<00:00, 1.72MB/s]\r\nconfig_sentence_transformers.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:00<00:00, 368kB/s]\r\nmodel.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1.12G/1.12G [00:11<00:00, 93.5MB/s]\r\nsentencepiece.bpe.model: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.07M/5.07M [00:00<00:00, 27.0MB/s]\r\nspecial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 964/964 [00:00<00:00, 2.79MB/s]\r\ntokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17.1M/17.1M [00:00<00:00, 37.0MB/s]\r\ntokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 3.37MB/s]\r\nmodules.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 349/349 [00:00<00:00, 886kB/s]\r\nWARNING:sentence_transformers.SentenceTransformer:You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.2.2. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\r\n\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jon/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/jon/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/jon/h2ogpt/src/utils.py\", line 75, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/home/jon/h2ogpt/src/gen.py\", line 1996, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 558, in get_embedding\r\n    embedding = HuggingFaceEmbeddings(model_name=hf_embedding_model, model_kwargs=model_kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 203, in warn_if_direct_instance\r\n    return wrapped(self, *args, **kwargs)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 79, in __init__\r\n    self.client = sentence_transformers.SentenceTransformer(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 95, in __init__\r\n    modules = self._load_sbert_model(model_path)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 840, in _load_sbert_model\r\n    module = module_class.load(os.path.join(model_path, module_config['path']))\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py\", line 135, in load\r\n    with open(sbert_config_path) as fIn:\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/jon/.cache/torch/sentence_transformers/intfloat_multilingual-e5-large-instruct/sentence_xlnet_config.json'\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ \r\n```\r\n\r\nI'm guessing sentence-transforemrs==2.2.2 is too old for this model.  However, that version is required for langchain's HF embedding wrapper code to work.\r\n\r\nOtherwise I get an error from sentence transformers complaining about how Langchain is misusing it:\r\n\r\n```\r\nCommand: generate.py --base_model=llama --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192 --user_path=user_path --langchain_mode=UserData --max_quality=True --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True --hf_embedding_model=intfloat/multilingual-e5-large-instruct --memory_restriction_level=0 --score_model=None --verbose=True --debug=True --show_examples=True --compile_model=True --share=True\r\nHash: 47e14672da4fc926aef7eeccd09d1119f112db3c\r\nWARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\r\n  warn_deprecated(\r\nmodules.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 349/349 [00:00<00:00, 603kB/s]\r\nconfig_sentence_transformers.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:00<00:00, 387kB/s]\r\nREADME.md: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 140k/140k [00:00<00:00, 6.15MB/s]\r\nconfig.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 690/690 [00:00<00:00, 2.11MB/s]\r\nmodel.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1.12G/1.12G [00:10<00:00, 103MB/s]\r\ntokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 3.64MB/s]\r\nsentencepiece.bpe.model: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.07M/5.07M [00:00<00:00, 5.64MB/s]\r\ntokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17.1M/17.1M [00:01<00:00, 14.5MB/s]\r\nspecial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 964/964 [00:00<00:00, 2.83MB/s]\r\n1_Pooling/config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 271/271 [00:00<00:00, 2.50MB/s]\r\nPrep: persist_directory=db_dir_UserData exists, user_path=user_path passed, adding any changed or new documents\r\nDO Loading db: UserData\r\nTraceback (most recent call last):\r\n  File \"/home/jon/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/jon/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/jon/h2ogpt/src/utils.py\", line 75, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/home/jon/h2ogpt/src/gen.py\", line 2121, in main\r\n    db = prep_langchain(persist_directory1,\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 5701, in prep_langchain\r\n    db, num_new_sources, new_sources_metadata = make_db(**langchain_kwargs)\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 5962, in make_db\r\n    return _make_db(**langchain_kwargs)\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 6187, in _make_db\r\n    get_existing_db(db, persist_directory, load_db_if_exists, db_type,\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 5858, in get_existing_db\r\n    embedding = get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"/home/jon/h2ogpt/src/gpt_langchain.py\", line 553, in get_embedding\r\n    embedding = HuggingFaceInstructEmbeddings(model_name=hf_embedding_model,\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 167, in __init__\r\n    self.client = INSTRUCTOR(\r\n  File \"/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 287, in __init__\r\n    modules = self._load_sbert_model(\r\nTypeError: INSTRUCTOR._load_sbert_model() got an unexpected keyword argument 'token'\r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "However, if you have no databases with instructor models, you can do:\r\n```\r\npip install sentence-transformers --upgrade\r\n```\r\n\r\nAnd it'll work fine.\r\n\r\nThen the above commands work.\r\n\r\ni.e. somehow langchain instructor class is broken for new sentence-transformers."
      },
      {
        "user": "pseudotensor",
        "body": "To be clear, all other RAG platforms have the same issues with sentence_transformers being broken still.\r\n\r\n\r\nhttps://github.com/UKPLab/sentence-transformers/issues/2474\r\n\r\nLooks like instructor-large is no longer a good default choice."
      },
      {
        "user": "juerware",
        "body": "Ok, thanks"
      }
    ]
  },
  {
    "issue_number": 1777,
    "title": "Doc file splitting into multiple image files",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-07-31T05:14:26Z",
    "updated_at": "2024-07-31T16:01:08Z",
    "labels": [],
    "body": "When uploading .doc or .docx files, the following warnings are displayed:\r\n `No acceptable contours found `\r\n `Contour is not a quadrilateral `\r\n`lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\r\n  warn_deprecated(`\r\n\r\nAfter uploading, a single .doc file splits into around 59 documents in .png or .jpeg formats. These are shown in the Doc Counts sidebar and also form the metadata as illustrated in the attached screenshot.\r\n\r\n![Screenshot from 2024-07-31 10-02-10](https://github.com/user-attachments/assets/df2b99eb-cc89-48be-96a8-92bbad5161d5)\r\n\r\nThe main issue is the inability of the .doc file to be parsed as a single document, unlike .pdf files. Instead, it splits into multiple .png or .jpeg files, leading to hallucination during querying.\r\n\r\nPlease address the parsing issue and provide a solution to handle .doc or .docx files correctly without splitting into multiple image files.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "We extract images so that they can be processed separately for image question-answer, but I understand if there are many images it might get messy.\r\n\r\nI added an ENV so you can control with `H2OGPT_DOCX_EXTRACT_IMAGES`.  set it to \"0\" to avoid this step.\r\n\r\nDocker with this feature will be in new build in few hours.\r\n\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 1770,
    "title": "Llama 3.1 support",
    "author": "bw-Deejee",
    "state": "closed",
    "created_at": "2024-07-25T10:19:49Z",
    "updated_at": "2024-07-25T21:15:14Z",
    "labels": [],
    "body": "Are llama3.1 models currently supported?\r\n\r\nI pulled recent repo however, didn't go through the full installation requirements and the following doesn't work:\r\n\r\n`python generate.py --base_model=meta-llama/Meta-Llama-3.1-8B-Instruct --use_auth_token=...`\r\n\r\nError message:\r\n`ValueError: 'rope_scaling' must be a dictionary with two fields, 'type' and 'factor', got {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}`",
    "comments": [
      {
        "user": "bw-Deejee",
        "body": "Nevermind. I looked deeper in to repo changes and it looks like i just had to rerun:\r\n`pip install -r requirements.txt -c reqs_optional/reqs_constraints.txt`\r\nin the newest repo version and now it works."
      },
      {
        "user": "pseudotensor",
        "body": "Ya new transformers was required. Thanks."
      }
    ]
  },
  {
    "issue_number": 1767,
    "title": "Docker Environment Variables / Arguments & Llama3.1 Support",
    "author": "plitc",
    "state": "closed",
    "created_at": "2024-07-24T07:58:28Z",
    "updated_at": "2024-07-24T22:04:29Z",
    "labels": [],
    "body": "Hello H2OGPT Team,\r\n\r\ncan you please list a Docker example that exactly reflects the functionality of https://gpt.h2o.ai/?\r\n\r\nWe currently cannot load the new Llama3.1 and with the option: prompt_type=llama the model only spits out confusing stuff.\r\n\r\n`root@ai-ubuntu22gpu-gpt:/opt# cat run-latest4`\r\n`#!/bin/sh`\r\n`export HUGGING_FACE_HUB_TOKEN=...PRIVATE...`\r\n`export CUDA_VISIBLE_DEVICES=\"0,1,2,3\"`\r\n`docker run \\`\r\n`       --gpus='\"device=0,1,2,3\"' \\`\r\n`       --runtime=nvidia \\`\r\n`       --shm-size=16g \\`\r\n`       -p 7860:7860 \\`\r\n`       --rm --init \\`\r\n`       -v /etc/passwd:/etc/passwd:ro \\`\r\n`       -v /etc/group:/etc/group:ro \\`\r\n`       -u 1000:1000 \\`\r\n`       -v /opt/h2o_gpt_data/.cache:/workspace/.cache \\`\r\n`       -v /opt/h2o_gpt_data/save:/workspace/save \\`\r\n`       -v /opt/h2o_gpt_data/db_dir_UserData:/workspace/db_dir_UserData \\`\r\n`       -v /opt/h2o_gpt_data/tmp:/tmp \\`\r\n`       -v /opt/CEPHFS_DATA:/workspace/user_path/CACHE \\`\r\n`       -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\`\r\n`       -e TOKENIZERS_PARALLELISM=false \\`\r\n`       -e ADMIN_PASS=...PRIVATE... \\`\r\n`       -e CONCURRENCY_COUNT=1 \\`\r\n`       -e API_OPEN=1 \\`\r\n`       -e ALLOW_API=1 \\`\r\n`       gcr.io/vorvan/h2oai/h2ogpt-runtime:latest /workspace/generate.py \\`\r\n`          --score_model=None \\`\r\n`          --max_output_seq_len=8192 \\`\r\n`          --max_seq_len=8192 \\`\r\n`          --prompt_type=llama3 \\`\r\n`          --use_safetensors=True \\`\r\n`          --base_model=meta-llama/Meta-Llama-3.1-8B \\`\r\n`          --tokenizer-path=meta-llama/Meta-Llama-3.1-8B \\`\r\n`          --save_dir='/workspace/save/' \\`\r\n`          --use_auth_token=$HUGGING_FACE_HUB_TOKEN \\`\r\n`          --use_gpu_id=False \\`\r\n`          --allow_upload_to_user_data=False \\`\r\n`          --allow_upload_to_my_data=True \\`\r\n`          --enable_ocr='off' \\`\r\n`          --enable_pdf_ocr='off' \\`\r\n`          --langchain_mode='UserData' \\`\r\n`          --langchain_modes=\"['LLM', 'UserData', 'MyData']\" \\`\r\n`          --user_path=/workspace/user_path \\`\r\n`          --db_type=chroma \\`\r\n`          --visible_h2ogpt_header=False \\`\r\n`          --visible_doc_selection_tab=False \\`\r\n`          --visible_doc_view_tab=False \\`\r\n`          --visible_chat_history_tab=False \\`\r\n`          --visible_expert_tab=False \\`\r\n`          --visible_models_tab=False \\`\r\n`          --visible_system_tab=False \\`\r\n`          --visible_tos_tab=False \\`\r\n`          --visible_hosts_tab=False \\`\r\n`          --answer_with_sources=False \\`\r\n`        --visible_side_bar=True \\`\r\n`          --visible_login_tab=False \\`\r\n`          --visible_visible_models=False \\`\r\n`        --visible_submit_buttons=True \\`\r\n`        --visible_chat_tab=True \\`\r\n`          --visible_h2ogpt_links=False \\`\r\n`          --visible_h2ogpt_qrcode=False \\`\r\n`          --visible_h2ogpt_logo=False \\`\r\n`          --visible_chatbot_label=False`\r\n` # EOF`\r\n`root@ai-ubuntu22gpu-gpt:/opt#`\r\n\r\nbeste regards\r\nDaniel Plominski",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Try not passing prompt_type. It is not required."
      },
      {
        "user": "plitc",
        "body": "without prompt_type =  the model only spits out confusing stuff\r\n\r\nLogoutput\r\n\r\n`INFO:     172.17.0.1:40054 - \"GET /queue/data?session_hash=e0apa8hwqu HTTP/1.1\" 200 OK\r\nNo chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\r\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\r\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\r\nINFO:     172.17.0.1:43008 - \"GET / HTTP/1.1\" 200 OK\r\n`"
      },
      {
        "user": "plitc",
        "body": "[pseudotensor](https://github.com/pseudotensor)\r\n\r\ncan you please list a Docker example that exactly reflects the functionality of https://gpt.h2o.ai/?\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "You are using the base model, you need to use the instruct version of the model.\r\n\r\ne.g. https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
      },
      {
        "user": "plitc",
        "body": "Does not work:\r\n\r\n`          --prompt_type=instruct \\`\r\n`          --use_safetensors=True \\`\r\n`          --base_model=meta-llama/Meta-Llama-3.1-8B-instruct \\`\r\n`          --tokenizer-path=meta-llama/Meta-Llama-3.1-8B-instruct \\`\r\n\r\nConfusing output:\r\n\r\n![h2ogpt](https://github.com/user-attachments/assets/4d3f6ddc-8f3e-4df7-b221-e47916f72bb6)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "You are using --prompt_type=instruct.  Please do not pass prompt_type, like we discussed."
      },
      {
        "user": "pseudotensor",
        "body": "Also there is no such parameter as `tokenizer-path` perhaps you meant tokenizer_base_model, but that's not required."
      },
      {
        "user": "plitc",
        "body": "Okey, without:\r\n\r\n`--prompt_type=instruct \\`\r\n`--tokenizer-path=meta-llama/Meta-Llama-3.1-8B-instruct \\`\r\n\r\nit now seems to deliver better results"
      },
      {
        "user": "pseudotensor",
        "body": "I'll share later how I launch gpt.h2o.ai.  The previous version (which is similar) is here already: https://github.com/h2oai/h2ogpt/blob/main/docs/FAQ.md#deploying-like-gpth2oai"
      }
    ]
  },
  {
    "issue_number": 1743,
    "title": "Support private CA",
    "author": "tomkraljevic",
    "state": "closed",
    "created_at": "2024-07-16T17:22:43Z",
    "updated_at": "2024-07-23T17:16:43Z",
    "labels": [],
    "body": "# What happens currently\r\n\r\nWhen the models that h2ogpt are pointing to are https and signed with a private CA the connection attempt errors out with an untrusted SSL certificate error.\r\n\r\n# What I want to happen\r\n\r\n1.  the helm chart should support a caCertificates section like other components from h2o.ai\r\n2. the deployment user supplies one or more PEM-format certificates in caCertificates\r\n3. the user-supplied caCertificates should be unioned with the set of root certificates that come by default with the pod\r\n4. this unioned list of certificates should be put in a place where the underlying software will find it\r\n5. the h2ogpt client honors the private CA, and the remote server is considered trusted, and the connection succeeds\r\n\r\n## Some implementation details\r\n\r\n- currently, the python code in h2ogpt uses httpx to make connections to models in the model_lock list\r\n- httpx documentation says that it uses certifi.  however by trial and error, i discovered that in this pod https uses `/etc/ssl/cert.pem`\r\n- certifi seems like it's ignored.  certifi.where() does not point to /etc/ssl/cert.pem\r\n",
    "comments": [
      {
        "user": "tomkraljevic",
        "body": "Methodogy I used to experimentally jam in certificates by hand, to see which file the current code/pod was really picking up the certs from.\r\n\r\n```\r\n\r\n\r\n1.  create the configmap:\r\n\r\nroot@ip-10-0-1-175:/home/ubuntu/tomk-1.5.1-07-15# head tomk-cacert-config \r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  namespace: h2ogpt\r\n  name: tomk\r\ndata:\r\n  cert.pem: |\r\n    # Local box\r\n    -----BEGIN CERTIFICATE-----\r\n    MIIDDjCCAfagAwIBAgIRANerbMOq4u7UvTHYe6Phnw0wDQYJKoZIhvcNAQELBQAw\r\n....\r\n\r\n\r\n2.  hack the h2ogpt deployment to add a volume and volumeMount:\r\n\r\n\r\n\r\n        volumeMounts:\r\n        - mountPath: /etc/ssl/cert.pem\r\n          name: tomk\r\n          subPath: cert.pem\r\n\r\n\r\n\r\n      volumes:\r\n      - configMap:\r\n          name: tomk\r\n        name: tomk\r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "To clarify, h2oGPT just uses OpenAI API pypi package for connecting to vllm etc.  Nothing related to these issues would be involving any other part of h2oGPT."
      },
      {
        "user": "tomkraljevic",
        "body": "a suggestion:\r\n\r\nhttpx has env vars.  maybe an init container could cat /etc/ssl/cert.pem with the provided caCertificates, write them to a new location, and set the SSL_CERT_FILE env var so they get picked up.\r\n\r\nthis would prevent the need for any code changes in the image.\r\n\r\nhttps://www.python-httpx.org/environment_variables/\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Not sure relevant, but just googled for moment:\r\n\r\nhttps://community.openai.com/t/ssl-certificate-verify-failed/32442/68?page=4\r\n\r\n```\r\nimport os \r\nos.environ['REQUESTS_CA_BUNDLE'] = <path_to_pem_certificate>\r\n```"
      },
      {
        "user": "tomkraljevic",
        "body": "so i can confirm this env var `SSL_CERT_FILE` does make a difference.\r\n"
      },
      {
        "user": "achraf-mer",
        "body": "- [x] https://github.com/h2oai/h2ogpte/pull/2895\r\n- [x] https://github.com/h2oai/h2ogpt/pull/1754\r\n\r\nboth PRs above are merged. Please re-open if more changes are required. Thanks"
      },
      {
        "user": "tomkraljevic",
        "body": "so for consistency, it would be better if it used the caCertificates-style of passing in the private CA stuff.\r\n(from the \"what i want to happen\" section at the top of the ticket.)\r\n"
      },
      {
        "user": "achraf-mer",
        "body": "> so for consistency, it would be better if it used the caCertificates-style of passing in the private CA stuff. (from the \"what i want to happen\" section at the top of the ticket.)\r\n\r\ndone here: https://github.com/h2oai/h2ogpt/pull/1758, PTAL, thanks."
      }
    ]
  },
  {
    "issue_number": 1750,
    "title": "Running model on multiple GPUs? How to do it? Does h2ogpt allows it do easily?",
    "author": "martinenkoEduard",
    "state": "closed",
    "created_at": "2024-07-17T08:12:28Z",
    "updated_at": "2024-07-17T17:36:07Z",
    "labels": [
      "type/question"
    ],
    "body": "Running model on multiple GPUs? How to do it?\r\nCan you show simple example?\r\n\r\nWhat are the restrictions? Should be GPUs identical?\r\nOr is it possible for instance to have\r\none RTX 3070 and one 3080? What about memory sharing?\r\n\r\nDo mistal and llama support these features?\r\n\r\nAlso can you share a rig configuration with multiple GPUs for local LLM deployment?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I recommend using vLLM, ollama, etc. as inference servers with h2oGPT to get most benefit if isolation and concurrency.\r\n\r\nBut you can use GGUF models, which automatically spread to multiple GPUs or however many are set as visible (CUDA_VISIBLE_DEVICES).\r\n\r\nFor HF models, just set `use_gpu_id=False` to spread it over multiple GPUs.\r\n\r\nLastly, there are many auxiliary models like embedding, TTS, STT, image generation, etc.  Those are controlled with embedding_gpu_id, caption_gpu_id, doctr_gpu_id, asr_gpu_id, stt_gpu_id, tts_gpu_id, image_gpu_ids (for each generation model added)."
      }
    ]
  },
  {
    "issue_number": 1742,
    "title": "Parallelize image calls instead of multiple images, probably better results in some cases",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-16T01:19:17Z",
    "updated_at": "2024-07-16T01:19:22Z",
    "labels": [
      "type/feature"
    ],
    "body": null,
    "comments": []
  },
  {
    "issue_number": 1692,
    "title": "Can we query Qdrant directly without needing sources?",
    "author": "andrewyuau",
    "state": "closed",
    "created_at": "2024-06-18T02:50:12Z",
    "updated_at": "2024-07-16T01:10:28Z",
    "labels": [],
    "body": "I followed [instructions](https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#using-qdrant) to spin up Qdrant using Docker and passed the QDRANT_URL and QDRANT_API_KEY. \r\n\r\nI'm trying to get h2ogpt to query Qdrant directly by not having any files in /workspace/user_path and deleting the directory /workspace/db_dir_UserData. However, I am getting an error message \"Did not generate db for UserData since no sources\" which originates from line 6258 in [gpt_langchain.py](https://github.com/h2oai/h2ogpt/blob/main/src/gpt_langchain.py). \r\n\r\nIs it possible to use my own Qdrant without needing sources?",
    "comments": [
      {
        "user": "andrewyuau",
        "body": "My current workaround is to have one entry in /workspace/user_path and h2ogpt automatically adds that entry into my Qdrant DB. I then delete that entry manually from Qdrant DB and then h2ogpt can query the rest of the entries in my own Qdrant DB. "
      },
      {
        "user": "pseudotensor",
        "body": "@Anush008 may be able to help."
      },
      {
        "user": "Anush008",
        "body": "Hey @andrewyuau, from the codebase, I see it should only be a log message and not an error. You should probably be able to continue to using it."
      },
      {
        "user": "andrewyuau",
        "body": "The workaround works for me so I will close this, thank you."
      }
    ]
  },
  {
    "issue_number": 1741,
    "title": "RAG via open-webui",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-15T23:28:43Z",
    "updated_at": "2024-07-15T23:28:46Z",
    "labels": [],
    "body": "- [ ] vector db match h2oGPT\r\n- [ ] -e OPENAI_API_USER='user:password'\r\n\r\ncont of https://github.com/h2oai/h2ogpt/pull/1663",
    "comments": []
  },
  {
    "issue_number": 248,
    "title": "add continue button",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2023-06-07T22:05:55Z",
    "updated_at": "2024-07-15T07:24:01Z",
    "labels": [],
    "body": "to continue generation",
    "comments": [
      {
        "user": "DJJones66",
        "body": "Glad this was not just me, I seriously thought I was missing something or an option that needed to be selected.  \r\n"
      },
      {
        "user": "pseudotensor",
        "body": "@DJJones66 you can always choose\r\n```\r\npython generate.py --max_new_tokens=1024\r\n```\r\nOr you can choose in expert panel this.\r\n\r\nOr something large if you want.\r\n\r\nBut I agree \"continue\" is a better option, in case the model goes wild one won't have to wait.  Our \"stop\" button can't actually stop generation itself, just what appears in UI."
      },
      {
        "user": "DJJones66",
        "body": "Can't use the token increase as an answer more because then you kind of get into an arms race.. when is it too much.  The continue button is just what has become the normally acceptable answer to the problem.  (Thank ChatGPT for that, rather than teaching everyone.. Here is the limit, it is a wall)"
      },
      {
        "user": "pseudotensor",
        "body": "- [ ] Already have continue in openai part, do for rest.\r\n- [ ] properly continue for anthropic that just puts assistant response as what it was and as last part, not user."
      }
    ]
  },
  {
    "issue_number": 1626,
    "title": "Enchance h2oGPT UI to have librechat like features.",
    "author": "hemenkapadia",
    "state": "closed",
    "created_at": "2024-05-17T05:56:22Z",
    "updated_at": "2024-07-15T06:55:00Z",
    "labels": [
      "type/feature",
      "reporter/proserve"
    ],
    "body": "Enhancement request coming from a customer for h2oGPT UI\r\n\r\n1. Enhance UI to give a more OpenAI-UI-like experience\r\n2. Option to choose/change model in between a chat session without losing context\r\n3. Persistent chat history with the ability to share it using a URL\r\n4. Admin capability to monitor usage / chats etc\r\n5. Integration with OIDC for authentication\r\n\r\nThey suggested Librechat https://www.librechat.ai/ meets their requirements and want something similar natively in the platform. \r\n\r\ncc @somanathghalimath @shivam5992 @genrichards ",
    "comments": [
      {
        "user": "hemenkapadia",
        "body": "Discussion on the topic at https://h2oai.slack.com/archives/C05MUTAHKU5/p1715925510510869\r\n- Suggest using OpenWebUI over librechat - https://h2oai.slack.com/archives/C05MUTAHKU5/p1715925510510869\r\n- https://openwebui.com/\r\n- https://github.com/open-webui/open-webui"
      },
      {
        "user": "pseudotensor",
        "body": "h2oGPT backend can be used for Open Web UI:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/FAQ.md#open-web-ui\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 1740,
    "title": "report generation",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-15T05:17:36Z",
    "updated_at": "2024-07-15T05:17:36Z",
    "labels": [],
    "body": "https://github.com/mshumer/gpt-author/blob/main/Claude_Author.ipynb",
    "comments": []
  },
  {
    "issue_number": 1739,
    "title": "action model integration",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-15T05:17:15Z",
    "updated_at": "2024-07-15T05:17:15Z",
    "labels": [],
    "body": "https://github.com/lavague-ai/LaVague?tab=readme-ov-file",
    "comments": []
  },
  {
    "issue_number": 1738,
    "title": "integrate stuff like LARS for PDF citations highlighting",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-15T05:16:08Z",
    "updated_at": "2024-07-15T05:16:08Z",
    "labels": [],
    "body": "https://www.reddit.com/r/LocalLLaMA/comments/1db98el/rag_for_documents_with_advanced_source_citations/?share_id=jsvIr2Ctx8_P04IOgaOHV&utm_content=2&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\r\n\r\nhttps://github.com/abgulati/LARS/tree/v1.4\r\nhttps://www.youtube.com/watch?v=Mam1i86n8sU&t=1s",
    "comments": []
  },
  {
    "issue_number": 1736,
    "title": "add ingestion for OpenAI API",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-13T04:43:55Z",
    "updated_at": "2024-07-13T06:48:52Z",
    "labels": [
      "type/feature"
    ],
    "body": "https://docs.privategpt.dev/api-reference/api-reference/context-chunks/chunks-retrieval\r\n\r\n\r\nhttps://platform.openai.com/docs/api-reference/files/create\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "https://github.com/h2oai/h2ogpt/issues/1563#issuecomment-2081301661"
      }
    ]
  },
  {
    "issue_number": 1737,
    "title": "for image batching, do parallel jobs instead of many images per batch, some models just not good enough to handle multiple images",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-13T04:52:12Z",
    "updated_at": "2024-07-13T04:56:51Z",
    "labels": [
      "type/feature"
    ],
    "body": "```\r\nimport cv2\r\nfrom openai import OpenAI\r\n\r\nclient = OpenAI(base_url='http://<ip>:80/v1')\r\nmodel=\"OpenGVLab/InternVL2-26B\"\r\n#client = OpenAI(base_url='http://<ip>:80/v1')\r\n#model = 'OpenGVLab/InternVL-Chat-V1-5'\r\n\r\nprompt = \"\"\"<response_instructions>\r\n- Act as a keen observer with a sharp eye for detail.\r\n- Analyze the content within the images.\r\n- Provide insights based on your observations.\r\n- Avoid making up facts.\r\n- Finally, according to our chat history, above documents, above figure captions, or given images, generate a well-structured response.\r\n</response_instructions>\r\nWhat tower do you see in the image?\r\n\"\"\"\r\n\r\nfrom PIL import Image\r\nimport base64\r\nimport requests\r\nfrom io import BytesIO\r\n\r\n\r\n# The encoding function I linked previously - but we actually don't use this function in the API server\r\ndef encode_image_base64(image: Image.Image, format: str = 'JPEG') -> str:\r\n    \"\"\"encode image to base64 format.\"\"\"\r\n\r\n    buffered = BytesIO()\r\n    if format == 'JPEG':\r\n        image = image.convert('RGB')\r\n    image.save(buffered, format)\r\n    return base64.b64encode(buffered.getvalue()).decode('utf-8')\r\n\r\n\r\n# This is what we use in the API server to load the base64 string to image\r\ndef load_image_from_base64(image: str):\r\n    \"\"\"Load image from base64 format.\"\"\"\r\n    return Image.open(BytesIO(base64.b64decode(image)))\r\n\r\n\r\nimage1 = '/tmp/image_file_764ae7bd-6b02-4ffb-b9d6-83e754c30952.jpeg'\r\nimage2 = '/tmp/image_file_1bfb88ea-a545-4b1f-a31f-051dbb90a378.jpeg'\r\nimage3 = '/tmp/image_file_ac5589e7-92a3-470f-a933-40d6bad38052.jpeg'\r\n\r\n#from PIL import Image\r\n\r\n\r\ndef remove_padding(image_path, output_path, background_color=(255, 255, 255)):\r\n    # Read the image\r\n    image = cv2.imread(image_path)\r\n\r\n    # Convert the image to grayscale\r\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n\r\n    # Apply a binary threshold to get a binary image\r\n    _, binary = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)\r\n\r\n    # Invert the binary image\r\n    inverted_binary = cv2.bitwise_not(binary)\r\n\r\n    # Find contours\r\n    contours, _ = cv2.findContours(inverted_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n    # Get the bounding box of the largest contour\r\n    x, y, w, h = cv2.boundingRect(contours[0])\r\n    for contour in contours:\r\n        x1, y1, w1, h1 = cv2.boundingRect(contour)\r\n        if w1 * h1 > w * h:\r\n            x, y, w, h = x1, y1, w1, h1\r\n\r\n    # Crop the image to the bounding box\r\n    cropped_image = image[y:y+h, x:x+w]\r\n\r\n    # Save the cropped image\r\n    cv2.imwrite(output_path, cropped_image)\r\n\r\n\r\n# Example usage\r\nif False:\r\n    ext = 'b.jpg'\r\n    remove_padding(image1, image1 + ext)\r\n    remove_padding(image2, image2 + ext)\r\n    remove_padding(image3, image3 + ext)\r\nelse:\r\n    ext = ''\r\n\r\nimage1_64 = base64.b64encode(open(image1 + ext, 'rb').read()).decode('utf-8')\r\nimage2_64 = base64.b64encode(open(image2 + ext, 'rb').read()).decode('utf-8')\r\nimage3_64 = base64.b64encode(open(image3 + ext, 'rb').read()).decode('utf-8')\r\n\r\nsystem_prompt = \"You are h2oGPTe, an expert question-answering AI system created by H2O.ai that performs like GPT-4 by OpenAI.\"\r\n\r\nmessages = [\r\n    #{'role': 'system', 'content': system_prompt},\r\n    {\r\n        'role': 'user',\r\n        'content': [\r\n            {'type': 'image_url',\r\n             'image_url': {\r\n                'url': 'data:image/jpeg;base64,' + image1_64,\r\n                }\r\n             },\r\n            {'type': 'image_url',\r\n             'image_url': {\r\n                'url': 'data:image/jpeg;base64,' + image2_64,\r\n                }\r\n             },\r\n            {'type': 'image_url',\r\n             'image_url': {\r\n                'url': 'data:image/jpeg;base64,' + image3_64,\r\n                }\r\n             },\r\n            {'type': 'text', 'text': prompt},\r\n        ],\r\n    }\r\n]\r\n\r\nresponse = client.chat.completions.create(\r\n    model=model,\r\n    messages=messages,\r\n    max_tokens=300,\r\n    temperature=0.0,\r\n)\r\n\r\nprint(response.choices[0])\r\n```\r\n\r\ngives:\r\n```\r\nThe image does not show a tower. Instead, it shows two separate items:\\n\\n1. A receipt from a shopping store.\\n2. A cake with a message congratulating Kate and Duke on their upcoming arrival.\\n\\nIf you have any specific questions about these items, please let me know!\r\n```",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "[\r\n![image_file_ac5589e7-92a3-470f-a933-40d6bad38052](https://github.com/user-attachments/assets/0a16067a-a71a-41fa-8080-9bd2e39ead1b)\r\n![image_file_1bfb88ea-a545-4b1f-a31f-051dbb90a378](https://github.com/user-attachments/assets/bfd8293e-47f6-4759-b6b9-cd83d5899e3b)\r\n![image_file_764ae7bd-6b02-4ffb-b9d6-83e754c30952](https://github.com/user-attachments/assets/6a8df125-47c7-4dcd-be19-5c38aed4fd83)\r\n](url)"
      }
    ]
  },
  {
    "issue_number": 1734,
    "title": "Running RAG optimized models (nvidia/Llama3-ChatQA-1.5-70B)",
    "author": "bw-Deejee",
    "state": "closed",
    "created_at": "2024-07-12T08:52:18Z",
    "updated_at": "2024-07-13T01:38:22Z",
    "labels": [
      "area/qa"
    ],
    "body": "Two questions:\r\n\r\n1. Do you have any recommendation regarding best models for RAG applications?\r\n\r\n2. Is it possible to run docQA optimized models like nvidia/Llama3-ChatQA-1.5-8B,\r\nwhich seems to have two different promtp formats:\r\n\r\nwhen context is available\r\n```\r\nSystem: {System}\r\n\r\n{Context}\r\n\r\nUser: {Question}\r\n\r\nAssistant: {Response}\r\n\r\nUser: {Question}\r\n\r\nAssistant:\r\n```\r\n\r\nwhen context is not available\r\n```\r\nSystem: {System}\r\n\r\nUser: {Question}\r\n\r\nAssistant: {Response}\r\n\r\nUser: {Question}\r\n\r\nAssistant:\r\n```\r\n\r\nI'd be interested on how to run this properly.\r\nI tested RAG with the normal Llama3 8b and it always mentioned chunk numbers in the response and overall didnt respond very cleanly.\r\n\r\nBest regards",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "That model has a chat template in the model HF repo, so it works OOTB.  You don't need to do anything else but something like:\r\n\r\n```\r\npython generate.py --base_model=nvidia/Llama3-ChatQA-1.5-8B\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "![image](https://github.com/user-attachments/assets/b9c4d5e1-927c-4e6e-a970-776e222b6448)\r\n"
      }
    ]
  },
  {
    "issue_number": 1587,
    "title": "Question: correct prompts template for llama3-instruct",
    "author": "slavag",
    "state": "closed",
    "created_at": "2024-04-28T09:19:14Z",
    "updated_at": "2024-07-12T08:48:48Z",
    "labels": [],
    "body": "Hi,\r\nWhat is correct prompts template for llama3-instruct, that I can choose from the existing to be able to work with model ?\r\n\r\nThanks",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It works OOTB in newer h2ogpt.  Uses chat template with end of turn stopping added."
      },
      {
        "user": "slavag",
        "body": "@pseudotensor Thanks, can you please elaborate more, as I'm not fully understand :) Sorry"
      },
      {
        "user": "pseudotensor",
        "body": "e.g. this is sufficient:\r\n```\r\npython generate.py --base_model=meta-llama/Meta-Llama-3-8B-Instruct\r\n```\r\n\r\nIt uses the chat template that is in the HF model repo.  This doesn't have a system prompt, so we add that as a pre-conversation."
      },
      {
        "user": "slavag",
        "body": "@pseudotensor Thanks, but if I'm using GGUF version, this not happens."
      },
      {
        "user": "pseudotensor",
        "body": "llama-3 prompting is tricky and so to ensure it is done right, please use latest h2oGPT as of the recent merge and run:\r\n```\r\npython generate.py --base_model=llama --model_path_llama=https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf?download=true --tokenizer_base_model=meta-llama/Meta-Llama-3-8B-Instruct --max_seq_len=8192\r\n```"
      },
      {
        "user": "slavag",
        "body": "@pseudotensor Thanks, using the latest code and example you provided, it's working fine, can you just provide a bit more details on from where model prompt template is taken in that case and why need to provide  --tokenizer_base_model ? Also if using full off-line mode will the --tokenizer_base_model work ?"
      },
      {
        "user": "pseudotensor",
        "body": "Yes, once off line then do:\r\n```bash\r\nTRANSFORMERS_OFFLINE=1 python generate.py --base_model=llama --model_path_llama=Meta-Llama-3-8B-Instruct.Q5_K_M.gguf --tokenizer_base_model=meta-llama/Meta-Llama-3-8B-Instruct --max_seq_len=8192 --gradio_offline_level=2 --share=False --add_disk_models_to_ui=False\r\n```\r\nwhich assumes the model was downloaded to location set by `llamacpp_path` (default is llamacpp_path folder).  This works for offline if previously used the earlier command that got the tokenizer.\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "The model prompt template is taken from\r\n\r\nhttps://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/blob/main/tokenizer_config.json#L2053\r\n\r\nif using that as the tokenizer_base_model"
      },
      {
        "user": "slavag",
        "body": "@pseudotensor Thanks a lot !!"
      },
      {
        "user": "dimm0",
        "body": "It keeps repeating my question when I run meta-llama/Meta-Llama-3-8B-Instruct... What am I doing wrong?\r\n\r\nRunning chat as `python3 /workspace/generate.py --base-model=meta-llama/Meta-Llama-3-8B-Instruct --tokenizer_base_model=meta-llama/Meta-Llama-3-8B-Instruct --max_seq_len=8192`"
      },
      {
        "user": "pseudotensor",
        "body": "Your command looks fine, although you don't need to pass that --tokenizer_base_model unless you are using GGUF models.\r\n\r\nE.g. when I run your command, I get:\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/2ada24f0-e750-4f4b-8f57-7cf8f4dac1d0)\r\n\r\n\r\n"
      },
      {
        "user": "dimm0",
        "body": "It works, didn't need a custom tokenizer. Thanks!"
      },
      {
        "user": "bw-Deejee",
        "body": "I'm on a fresh installation on newest repo version and i get this error when running\r\n`python generate.py --base_model=meta-llama/Meta-Llama-3-8B-Instruct --hf_embedding_model=intfloat/multilingual-e5-large --auth=None`\r\n\r\n`Error\r\nPrompt was not set: ['Traceback (most recent call last):\\n File \"/home/bw/h2ogpt/src/prompter.py\", line 2405, in apply_chat_template\\n prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\\n File \"/home/bw/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1817, in apply_chat_template\\n compiled_template = self._compile_jinja_template(chat_template)\\n File \"/home/bw/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1894, in _compile_jinja_template\\n raise ImportError(\\nImportError: apply_chat_template requires jinja2>=3.1.0 to be installed. Your version is 3.0.3.\\n', 'Traceback (most recent call last):\\n File \"/home/bw/h2ogpt/src/prompter.py\", line 2405, in apply_chat_template\\n prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\\n File \"/home/bw/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1817, in apply_chat_template\\n compiled_template = self._compile_jinja_template(chat_template)\\n File \"/home/bw/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1894, in _compile_jinja_template\\n raise ImportError(\\nImportError: apply_chat_template requires jinja2>=3.1.0 to be installed. Your version is 3.0.3.\\n']`"
      },
      {
        "user": "pseudotensor",
        "body": "It's some transformers bug in their packaging.  But I noticed it and added a while back `jinja2>=3.1.0` in requirements.txt, so should work fine.\r\n\r\nWhen I check the latest 799 docker image built, I see no issue with jinja2.\r\n```\r\nbash-5.2$ python\r\nPython 3.10.14 (main, May  1 2024, 21:48:03) [GCC 13.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import jinja2\r\n>>> jinja2.__version__\r\n'3.1.4'\r\n>>> \r\n```\r\n\r\nThe docker build just uses same script as local build.\r\n\r\nA work-around is to do `pip install jinja2==3.1.4`"
      },
      {
        "user": "bw-Deejee",
        "body": "Works. Ty"
      }
    ]
  },
  {
    "issue_number": 1718,
    "title": "system_prompt ignored for response_format JSON -- use text_context_list instead?",
    "author": "vanboom",
    "state": "closed",
    "created_at": "2024-07-01T21:35:00Z",
    "updated_at": "2024-07-08T21:37:41Z",
    "labels": [
      "type/question"
    ],
    "body": "I am not 100% sure how to express this issue but I have found a strange behavior after some testing attempts to get a JSON output using `response_format='json_object'` or `json_code`.   I am querying the model using the API.   I set up this simple example during testing to help explore the issue.\r\n\r\nThe goal:  give the model some context in the `system` message and then ask some questions. \r\n**The issue:**  The model does not consider the given context if `response_format=json_object` is configured as an ARG\r\n\r\n```\r\npython generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --langchain_mode=LLM --response_format=json_object\r\n```\r\n\r\n### Test 1 - working context\r\n```\r\npython generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --langchain_mode=LLM\r\n```\r\n\r\n```\r\n[\r\n    [0] {\r\n           :role => \"system\",\r\n        :content => \"      Reference the following context to answer questions:\\n      *****\\n      Kitty likes BBQ and golf.\\n      *****\\n\"\r\n    },\r\n    [1] {\r\n           :role => \"user\",\r\n        :content => \"      What does Kitty like?  Explain your answer.\\n\"\r\n    }\r\n]\r\n\r\n\r\n\"Kitty likes BBQ and golf, as stated in the given context. BBQ refers to barbecue, which is a method of cooking food slowly over low heat, usually involving smoking. Golf is a sport that involves hitting a small ball into a series of holes using a variety of clubs. Therefore, Kitty has a preference for both BBQ and golf.\"\r\n```\r\n\r\n### Test 2 - hallucinated answer with no consideration of the context\r\n\r\n```\r\npython generate.py --base_model=TheBloke/zephyr-7B-beta-GGUF --langchain_mode=LLM --response_format=json_object\r\n```\r\n\r\n```\r\n[\r\n    [0] {\r\n           :role => \"system\",\r\n        :content => \"      Reference the following context to answer questions:\\n      *****\\n      Kitty likes BBQ and golf.\\n      *****\\n\"\r\n    },\r\n    [1] {\r\n           :role => \"user\",\r\n        :content => \"      What does Kitty like?  Explain your answer.\\n\"\r\n    }\r\n]\r\n\r\nRESPONSE:  \"{\\\"Kitty\\\": {\\\"likes\\\": [\\\"catnip\\\", \\\"laser pointers\\\", \\\"playtime\\\", \\\"treats\\\", \\\"cuddles\\\"]}}\"\r\n\r\n```\r\n\r\nIn the web app, it works perfectly even with `--response_format='json_code'` enabled on the command line. \r\n![image](https://github.com/h2oai/h2ogpt/assets/251706/88a7de7a-633b-4b1a-a244-b6112bc338bf)\r\n\r\n\r\nAny ideas why the API would behave this way differently from the Gradio app?  I am not sure how to set `response_format` to JSON in the API calls so possibly there is a method that would work at that level.  Thanks!!",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, we purposefully nuke the system_prompt when doing json mode.\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/6d514a1b9d498f714fd3f06570a816ac169ad962/src/gen.py#L2930\r\n\r\nAs the comment says, we have seen issues with it messing up even normal models, if the system_prompt contains conflicting persona relative to JSON making.\r\n\r\nBut you can comment-out that line."
      },
      {
        "user": "pseudotensor",
        "body": "I think even better than commenting out that line is to pass your lines of information as `text_context_list` list that would be considered same as if it were a document being referenced."
      },
      {
        "user": "pseudotensor",
        "body": "As yet another alternative, if you need to input instructions, just change these prompts via API:\r\n\r\n```\r\n            \"json_object_prompt\",\r\n            \"json_object_prompt_simpler\",\r\n            \"json_code_prompt\",\r\n            \"json_code_prompt_if_no_schema\",\r\n            \"json_schema_instruction\",\r\n\r\n```"
      },
      {
        "user": "vanboom",
        "body": "> Hi, we purposefully nuke the system_prompt when doing json mode.\r\n> \r\n> https://github.com/h2oai/h2ogpt/blob/6d514a1b9d498f714fd3f06570a816ac169ad962/src/gen.py#L2930\r\n> \r\n> As the comment says, we have seen issues with it messing up even normal models, if the system_prompt contains conflicting persona relative to JSON making.\r\n> \r\n> But you can comment-out that line.\r\n\r\nThanks!  Will explore further per your suggestions.  Interestingly, setting the response_format to 'text' and requesting JSON yields good results, now curious why the system_prompt would mess things up when `response_format` is set to `json_code` or `json_object`. \r\n\r\nI am trying to control the context that the model will use to inform the Q&A similar to #1668. "
      },
      {
        "user": "vanboom",
        "body": "Yes setting `text_context_list` instead does appear to work to provide context to the model, however I am getting other failures when the `system_prompt` is cleared such as blank \"\" answers.   Also, setting `context` or `text_context_list` in the API payload results in \"Input should be a valid list\"...trying to figure out why on that. \r\n\r\nWhy not let the user control the `system_prompt` regardless of the `response_format` setting?"
      },
      {
        "user": "pseudotensor",
        "body": "Just trying to lower the number of things one has to change for default case."
      }
    ]
  },
  {
    "issue_number": 1722,
    "title": "Metrics for the generated answer",
    "author": "vitalyshalumov",
    "state": "closed",
    "created_at": "2024-07-07T07:19:55Z",
    "updated_at": "2024-07-08T21:34:09Z",
    "labels": [
      "resolution/wontfix"
    ],
    "body": "Are there any metrics that I can get on the generated answer such as faithfulness?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "We considered adding to h2oGPT this kind of stuff, but stopped.  So the answer is not at moment."
      }
    ]
  },
  {
    "issue_number": 1713,
    "title": "Getting \"an unexpected keyword argument 'cache_folder'\" during import",
    "author": "inmanityus",
    "state": "closed",
    "created_at": "2024-06-25T20:04:57Z",
    "updated_at": "2024-07-03T21:42:54Z",
    "labels": [],
    "body": "I have the following code into which I am passing in a JSON document.  It keeps throwing the same error.  I checked the JSOn and it is valid - what am I doing wrong?\r\n\r\nError:  TypeError: SentenceTransformer.__init__() got an unexpected keyword argument 'cache_folder'\r\n\r\n\r\nI am using Chroma through LangChain.\r\n\r\n    ```\r\n    db_directory = os.path.join(user_directory, database_name + \".db\")\r\n    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\r\n    # Cosine will keep the similarity scores between zero and one\r\n    chroma_db = Chroma(persist_directory=db_directory, collection_name=collection_name, \r\n    embedding_function=embedding_function,\r\n                           collection_metadata={\"hnsw:space\": \"cosine\"}, relevance_score_fn=lambda distance: 1.0 - distance / 2)\r\n    json_splitter = RecursiveJsonSplitter(max_chunk_size=2000)\r\n    docs = json_splitter.create_documents(json_splitter.split_json(json_object))\r\n    if doc_ids is None:\r\n        doc_ids = [str(uuid.uuid4()) for i in range(1, len(docs) + 1)]\r\n    else:\r\n        # We look to see if the document exists:\r\n        result = chroma_db.get(doc_ids)\r\n        if result is not None and len(result) > 0:\r\n            # This is an update:\r\n            chroma_db.update_documents(doc_ids, docs)\r\n            return doc_ids\r\n    chroma_db.from_documents(docs, embedding_function, ids=doc_ids)\r\n    return doc_ids\r\n```",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "You are probably using the wrong/unsupported version of that package.  It should be `sentence-transformers==2.2.2\r\n`"
      }
    ]
  },
  {
    "issue_number": 1683,
    "title": "Chunk should open on the same page from it has been taken",
    "author": "glenbhermon",
    "state": "open",
    "created_at": "2024-06-13T06:38:27Z",
    "updated_at": "2024-07-03T21:42:04Z",
    "labels": [
      "resolution/wontfix"
    ],
    "body": "when click on the link in the Source then the file is open in the same tab and the pdf by default goes on the first page. is there any possible way when we click on the link from the chunk has been taken from should open in the same page and the same paragraph in the source file.",
    "comments": [
      {
        "user": "srimanreddy4",
        "body": "In utils.py, check for the function \"get_url \", which is line 826, there you can make the following changes\r\n\r\n if not from_str:\r\n    source = x.metadata['source']+f\"#page={x.metadata['page']}\" //Line  828\r\n \r\nelse(of short name condition) : \r\n  source_name = os.path.basename(source.split('#')[0]) // Line 834\r\n\r\n\r\nDo these changes, then you are good to go with what you want\r\nand regarding pdf opening on same page, you can add onclick=\"\" in the final link"
      },
      {
        "user": "llmwesee",
        "body": "Modified the get_url function:\r\n\r\n```\r\ndef get_url(x, from_str=False, short_name=False, font_size=2):\r\n    if not from_str:\r\n        source = x.metadata['source']\r\n        if 'page' in x.metadata:\r\n            source += f\"#page={x.metadata['page']}\"\r\n    else:\r\n        source = x\r\n    if short_name:\r\n        source_name = get_short_name(source)\r\n    else:\r\n        #source_name = source\r\n        source_name = os.path.basename(source.split('#')[0])\r\n    if source.startswith('http://') or source.startswith('https://'):\r\n        # return \"\"\"<font size=\"%s\"><a href=\"%s\" target=\"_blank\"  rel=\"noopener noreferrer\">%s</a></font>\"\"\" % (\r\n        #     font_size, source, source_name)\r\n        return \"\"\"<font size=\"%s\"><a href=\"%s\" target=\"_blank\" rel=\"noopener noreferrer\" onclick=\"\">%s</a></font>\"\"\" % (\r\n            font_size, source, source_name)\r\n    elif '<a href=' not in source:\r\n        # return \"\"\"<font size=\"%s\"><a href=\"file/%s\" target=\"_blank\"  rel=\"noopener noreferrer\">%s</a></font>\"\"\" % (\r\n        #     font_size, source, source_name)\r\n        return \"\"\"<font size=\"%s\"><a href=\"file/%s\" target=\"_blank\" rel=\"noopener noreferrer\" onclick=\"\">%s</a></font>\"\"\" % (\r\n            font_size, source, source_name)\r\n    else:\r\n        # already filled\r\n        return source\r\n```\r\n\r\nBut the problem is that all chunks not opened in the desired page location when clicking on the link. sometimes chunks open on the irrelevant page location from where it haven't parse. \r\n\r\nAnd the this kind of metadata it always start with first page.\r\nBegin Document: Metadata: chunk_id = 13\r\nBegin Document:\r\nMetadata:\r\nchunk_id = 13\r\ndate = 2024-06-27 12:17:36.255558\r\ninput_type = .pdf\r\nsource = /tmp/gradio/611348b8d08a3f166d04ca265917a4ee6bc1af52/ GEAR AND EQUIPMENTS.pdf\r\n\r\nDocument Contents:\r\n\"\"\"\r\nI also don't understand why do every chunks not parsed along with page number. \r\nFor reference I also attaching the metadata screenshot:-\r\n\r\n![Screenshot from 2024-06-27 12-47-24](https://github.com/h2oai/h2ogpt/assets/137979399/e8675806-ce72-4db7-b241-610d0527e42f)\r\n"
      }
    ]
  },
  {
    "issue_number": 1702,
    "title": "Offline mode is still attempting to fetch from HF",
    "author": "machinelearning2014",
    "state": "closed",
    "created_at": "2024-06-19T11:00:32Z",
    "updated_at": "2024-07-03T21:34:24Z",
    "labels": [],
    "body": "I am running in Offline mode with this command:\r\n\r\nHF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 python generate.py --base_model=mistral --model_path_llama=mistral-7b-instruct-v0.2.Q2_K.gguf --prompt_type=mistral --cli=True\r\n\r\nI have already downloaded the following models manually from HF:\r\n\r\n/home/letro/.cache/huggingface/hub/models--hkunlp--instructor-large\r\n/home/letro/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2\r\n/home/letro/h2ogpt/llamacpp_path/mistral-7b-instruct-v0.2.Q2_K.gguf\r\n\r\nWhen I run the command above I get following:\r\n\r\n(h2ogpt) letro@jupyterhub-gpu-alpha-bw:~/h2ogpt$ TRANSFORMERS_OFFLINE=1 python generate.py --base_model=mistral --model_path_llama=mistral-7b-instruct-v0.2.Q2_K.gguf --prompt_type=mistral --cli=True\r\n/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nUsing Model mistral\r\nfatal: ambiguous argument 'HEAD': unknown revision or path not in the working tree.\r\nUse '--' to separate paths from revisions, like this:\r\n'git <command> [<revision>...] -- [<file>...]'\r\nWARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\r\nTraceback (most recent call last):\r\n  File \"/home/letro/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/letro/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/letro/h2ogpt/src/utils.py\", line 75, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/home/letro/h2ogpt/src/gen.py\", line 2044, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"/home/letro/h2ogpt/src/gpt_langchain.py\", line 551, in get_embedding\r\n    embedding = HuggingFaceInstructEmbeddings(model_name=hf_embedding_model,\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 164, in __init__\r\n    self.client = INSTRUCTOR(\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 87, in __init__\r\n    snapshot_download(model_name_or_path,\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/util.py\", line 442, in snapshot_download\r\n    model_info = _api.model_info(repo_id=repo_id, revision=revision, token=token)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2300, in model_info\r\n    r = get_session().get(path, headers=headers, timeout=timeout, params=params)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 602, in get\r\n    return self.request(\"GET\", url, **kwargs)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/letro/.conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 77, in send\r\n    raise OfflineModeIsEnabled(\r\nhuggingface_hub.errors.OfflineModeIsEnabled: Cannot reach https://huggingface.co/api/models/hkunlp/instructor-large: offline mode is enabled. To disable it, please unset the `HF_HUB_OFFLINE` environment variable.\r\n\r\nQuestion: why is the command still trying to fetch from HF? and why is it not seeing these already in local:\r\n/home/letro/.cache/huggingface/hub/models--hkunlp--instructor-large\r\n/home/letro/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2\r\n",
    "comments": [
      {
        "user": "machinelearning2014",
        "body": "I have now downloaded the embedding models at:\r\n\r\n/home/letro/.cache/torch/hkunlp/hkunlp_instructor-large\r\n/home/letro/.cache/torch/sentence_transformers/sentence-transformers_all-MiniLM-L6-v2\r\n\r\nand running: \r\nTRANSFORMERS_OFFLINE=1 python generate.py --base_model=mistral-7b-instruct-v0.2.Q2_K.gguf --prompt_type=mistral --max_seq_len=4096 --cli=True\r\n\r\nBut still it is still does not recognise the models in .cache so attempts to download from https://huggingface.co/api/models/hkunlp/instructor-large, which I want to avoid, as I am in corporate firewall"
      },
      {
        "user": "llmwesee",
        "body": "I'm also facing the same issue. "
      },
      {
        "user": "pseudotensor",
        "body": "I see same thing but only for the instructor model and only if using `HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1`.  If I don't set those it's fine.  This is a known bug in the instructor code.\r\n\r\nIf one unsets those, then \"mistral\" model is trying to be obtained from HF, but this is just an error in the command line.  If one is offline one needs to ensure the HF smart attempts are not used by using:\r\n\r\n```\r\npython generate.py --base_model=llama --model_path_llama=mistral-7b-instruct-v0.2.Q5_K_M.gguf --prompt_type=mistral --cli=True\r\n```\r\n\r\nthis works just fine.\r\n\r\nThis is what is documented here:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md#tldr"
      },
      {
        "user": "pseudotensor",
        "body": "As long as you already have the model int he cache, this can still be run:\r\n\r\n```\r\nTRANSFORMERS_OFFLINE=1 CONCURRENCY_COUNT=1 python generate.py --base_model=llama --model_path_llama=mistral-7b-instruct-v0.2.Q5_K_M.gguf --prompt_type=mistral --cli=True\r\n```\r\n\r\nI only see an issue with instructor and this:\r\n```\r\nTRANSFORMERS_OFFLINE=1 python\r\nPython 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from InstructorEmbedding import INSTRUCTOR\r\nclient = INSTRUCTOR(\"hkunlp/instructor-large\")\r\n```\r\n\r\nwhen I didn't run online first recently."
      }
    ]
  },
  {
    "issue_number": 1707,
    "title": "Error installing through full linux bash script",
    "author": "juerware",
    "state": "closed",
    "created_at": "2024-06-24T11:00:45Z",
    "updated_at": "2024-07-03T21:15:34Z",
    "labels": [],
    "body": "SO: Ubuntu 22.04\r\nCommit: 9a7c07b5 (last at the moment of this issue)\r\nCuda: 12.1\r\nNvidia: 535.183.01\r\n\r\nI got the following error trying to install through:\r\n\r\n```shell\r\nbash docs/linux_install_full.sh\r\n```\r\nif someone can help with the error:\r\n\r\n```shell\r\nBuilding wheels for collected packages: llama-cpp-python\r\n  Building wheel for llama-cpp-python (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [93 lines of output]\r\n      *** scikit-build-core 0.9.6 using CMake 3.27.1 (wheel)\r\n      *** Configuring CMake...\r\n      2024-06-24 12:54:28,896 - scikit_build_core - WARNING - Can't find a Python library, got libdir=/root/miniconda3/envs/h2ogpt/lib, ldlibrary=libpython3.10.a, multiarch=x86_64-linux-gnu, masd=None\r\n      loading initial cache file /tmp/tmpcjgjaztp/build/CMakeInit.txt\r\n      -- The C compiler identification is GNU 11.4.0\r\n      -- The CXX compiler identification is GNU 11.4.0\r\n      -- Detecting C compiler ABI info\r\n      -- Detecting C compiler ABI info - done\r\n      -- Check for working C compiler: /usr/bin/cc - skipped\r\n      -- Detecting C compile features\r\n      -- Detecting C compile features - done\r\n      -- Detecting CXX compiler ABI info\r\n      -- Detecting CXX compiler ABI info - done\r\n      -- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n      -- Detecting CXX compile features\r\n      -- Detecting CXX compile features - done\r\n      -- Found Git: /usr/bin/git (found version \"2.34.1\")\r\n      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n      -- Found Threads: TRUE\r\n      CMake Warning at vendor/llama.cpp/CMakeLists.txt:387 (message):\r\n        LLAMA_CUBLAS is deprecated and will be removed in the future.\r\n\r\n        Use LLAMA_CUDA instead\r\n\r\n\r\n      -- Unable to find cublas_v2.h in either \"/usr/local/cuda/include\" or \"/usr/math_libs/include\"\r\n      -- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.1.66\")\r\n      -- CUDA found\r\n      -- The CUDA compiler identification is NVIDIA 12.1.66\r\n      -- Detecting CUDA compiler ABI info\r\n      -- Detecting CUDA compiler ABI info - done\r\n      -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\r\n      -- Detecting CUDA compile features\r\n      -- Detecting CUDA compile features - done\r\n      -- Using CUDA architectures: all\r\n      -- CUDA host compiler is GNU 11.4.0\r\n\r\n      -- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.\r\n      -- CMAKE_SYSTEM_PROCESSOR: x86_64\r\n      -- x86 detected\r\n      CMake Warning (dev) at CMakeLists.txt:26 (install):\r\n        Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\r\n      This warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n      CMake Warning (dev) at CMakeLists.txt:35 (install):\r\n        Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\r\n      This warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n      -- Configuring done (5.7s)\r\n      CMake Error at vendor/llama.cpp/CMakeLists.txt:1225 (target_link_libraries):\r\n        Target \"ggml\" links to:\r\n\r\n          CUDA::cublas\r\n\r\n        but the target was not found.  Possible reasons include:\r\n\r\n          * There is a typo in the target name.\r\n          * A find_package call is missing for an IMPORTED target.\r\n          * An ALIAS target is missing.\r\n\r\n\r\n\r\n      CMake Error at vendor/llama.cpp/CMakeLists.txt:1232 (target_link_libraries):\r\n        Target \"ggml_shared\" links to:\r\n\r\n          CUDA::cublas\r\n\r\n        but the target was not found.  Possible reasons include:\r\n\r\n          * There is a typo in the target name.\r\n          * A find_package call is missing for an IMPORTED target.\r\n          * An ALIAS target is missing.\r\n\r\n\r\n\r\n      CMake Error at vendor/llama.cpp/CMakeLists.txt:1249 (target_link_libraries):\r\n        Target \"llama\" links to:\r\n\r\n          CUDA::cublas\r\n\r\n        but the target was not found.  Possible reasons include:\r\n\r\n          * There is a typo in the target name.\r\n          * A find_package call is missing for an IMPORTED target.\r\n          * An ALIAS target is missing.\r\n\r\n\r\n\r\n      -- Generating done (0.0s)\r\n      CMake Generate step failed.  Build files cannot be regenerated correctly.\r\n\r\n      *** CMake configuration failed\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for llama-cpp-python\r\nFailed to build llama-cpp-python\r\nERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\r\n``` ",
    "comments": [
      {
        "user": "juerware",
        "body": "I think I have resolved the problem following the steps:\r\n\r\n1. Follow this url documentation: https://developer.nvidia.com/cuda-12-1-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_local\r\n2. ```apt -y install nvidia-cuda-toolkit``` # be careful because this is version 11.5 not 12.1 because 11.5 is default package in the ubuntu distribution mentioned before. This package is necessary for getting installed header files for compilation through nvcc command.\r\n3. Execute command to view version of installed packages:\r\n```\r\n# dpkg -l | grep -iP '(cuda|nvidia)' | grep -i toolkit\r\nii  cuda-toolkit-12-1-config-common             12.1.55-1                               all          Common config package for CUDA Toolkit 12.1.\r\nii  cuda-toolkit-12-config-common               12.1.55-1                               all          Common config package for CUDA Toolkit 12.\r\nii  cuda-toolkit-config-common                  12.1.55-1                               all          Common config package for CUDA Toolkit.\r\nii  nvidia-cuda-toolkit                         11.5.1-1ubuntu1                         amd64        NVIDIA CUDA development toolkit\r\nii  nvidia-cuda-toolkit-doc                     11.5.1-1ubuntu1                         all          NVIDIA CUDA and OpenCL documentation\r\n```\r\n4. Executing final repository command ```bash docs/linux_install_full.sh```\r\n\r\nAs you can see with these steps it is getting up but .... I think it has to be reviewed in order to automate the right final process and coordinate cuda versions"
      },
      {
        "user": "pseudotensor",
        "body": "The steps say to install cuda 12.1 toolkit first.  Yes, depending upon your drivers etc. this may be more involved as old drivers would need to be updated."
      }
    ]
  },
  {
    "issue_number": 1536,
    "title": "One Click Installers for MacOS not working on MacMini M2",
    "author": "VillaesterModerneMedien",
    "state": "closed",
    "created_at": "2024-04-08T09:14:01Z",
    "updated_at": "2024-07-03T21:13:33Z",
    "labels": [],
    "body": "Hello,\r\n\r\nI am currently trying to install H2O GPT on my MacMini and am not getting anywhere with the One Click Installers.\r\n\r\n---\r\nMar 07, 2024\r\n\r\n[h2ogpt-osx-m1-cpu](https://h2o-release.s3.amazonaws.com/h2ogpt/Mar2024/h2ogpt-osx-m1-cpu)\r\n[h2ogpt-osx-m1-gpu](https://h2o-release.s3.amazonaws.com/h2ogpt/Mar2024/h2ogpt-osx-m1-gpu)\r\nNov 08, 2023\r\n\r\n[h2ogpt-osx-m1-cpu](https://h2o-release.s3.amazonaws.com/h2ogpt/Nov2023/h2ogpt-osx-m1-cpu)\r\n[h2ogpt-osx-m1-gpu](https://h2o-release.s3.amazonaws.com/h2ogpt/Nov2023/h2ogpt-osx-m1-gpu)\r\n---\r\n\r\n\r\nI have started the file for installation, but after a short time (it looks as if it is being installed), the file (e.g. h2ogpt-osx-m1-gpu) is simply opened in a text editor and then it doesn't go any further.\r\nI have enabled the installation of the extension in the system settings before...\r\n\r\nIs it because the install is actually for an M1 Mac?\r\nDoes anyone have a similar problem?\r\nI can't install H2O GPT at the moment...\r\n\r\nSystem Specs:\r\nMac mini 2023\r\nApple M2 Pro\r\n32 GB\r\nmacOS: Sonoma 14.4.1\r\n\r\nAny help would be appreciated, thank you very much.\r\n\r\n",
    "comments": [
      {
        "user": "verbiate",
        "body": "Same issue. The file has no extension, and attempts to rename with an appended \".dmg\" or \".app\" have been fruitless."
      },
      {
        "user": "pseudotensor",
        "body": "I've asked @Mathanraj-Sharma for help."
      },
      {
        "user": "USMCM1A1",
        "body": "Same--would love for this to work."
      },
      {
        "user": "pseudotensor",
        "body": "I'll probably need to remove the mac installer and just document use of docker for mac."
      }
    ]
  },
  {
    "issue_number": 1686,
    "title": "h2ogpt tries to download model from hugging face when using local inference server",
    "author": "hapatrick",
    "state": "closed",
    "created_at": "2024-06-14T20:42:12Z",
    "updated_at": "2024-07-03T21:13:05Z",
    "labels": [],
    "body": "I am trying to follow the instructions at https://github.com/h2oai/h2ogpt/blob/main/docs/README_InferenceServers.md to run h2ogpt with a local inference server -- specifically, vLLM serving Mixtral8x22B using the OpenAI API.\r\n\r\nWhen I run generate.py, it tries to download the mixtral model from HuggingFace. My understanding is it should not need to do this because it's going to be using the model served by my inference server!\r\n\r\nHere is the command I'm trying to use:\r\npython3.10 /workspace/generate.py --inference_server=\"vllm:http://vllm-mixtral8x22b:8000/v1/\" --prompt_type=openai --base_model=mixtral\r\n\r\nWhat am I doing wrong here? \r\n",
    "comments": [
      {
        "user": "srimanreddy4",
        "body": "I have done it recently with mixtral, it worked for me, compared to my command, in the base command I mentioned the whole model name that I was using, I have looked up the documentation, they did the same too, why don't you try that out once"
      },
      {
        "user": "pseudotensor",
        "body": "@hapatrick It should only be downloading the tokenizer, not the whole model.  You should use \"vllm_chat\" instead of \"vllm\" for the inference_server prefix and then avoid passing --prompt_type.  prompt_type openai will use no LLM prompting with vllm's text generation API and not work right."
      }
    ]
  },
  {
    "issue_number": 1705,
    "title": "Use auto ingest with openai Api ",
    "author": "rmisire",
    "state": "closed",
    "created_at": "2024-06-20T09:14:25Z",
    "updated_at": "2024-07-03T21:10:00Z",
    "labels": [],
    "body": "I would like to know if it is possible to automatically ingest files in a specific collection without going through the ui. After several searches I found nothing in the documentation. I looked at http://localhost:5001/ but nothing conclusive\r\n\r\nThank you for your feedback",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "The gradio API is extensive and it can be done that way yes.  An example test is the `test_client_chat_stream_langchain_steps3` test.\r\n\r\nFor openai I need to to this issue:\r\n\r\n\r\nhttps://github.com/h2oai/h2ogpt/issues/1563#issuecomment-2081301661"
      }
    ]
  },
  {
    "issue_number": 1691,
    "title": "Gradio Client - Repeated WebSocket Connection Rejection (HTTP 403)",
    "author": "bibo7086",
    "state": "closed",
    "created_at": "2024-06-17T19:25:18Z",
    "updated_at": "2024-07-03T21:05:47Z",
    "labels": [],
    "body": "I encountered an issue while using the Gradio client to connect to a Gradio application running at `http://localhost:7860/`. The client initially reports successful loading of the API, but attempts to establish a WebSocket connection are rejected by the server with an HTTP 403 (Forbidden) status code.\r\n\r\n**Error Message:**\r\n\r\n```\r\nLoaded as API: http://localhost:7860/ ✔\r\nGR job failed: server rejected WebSocket connection: HTTP 403\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/gradio_client/client.py\", line 798, in _inner\r\n    predictions = _predict(*data)\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/gradio_client/client.py\", line 827, in _predict\r\n    result = utils.synchronize_async(self._ws_fn, data, hash_data, helper)\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/gradio_client/utils.py\", line 540, in synchronize_async\r\n    return fsspec.asyn.sync(fsspec.asyn.get_loop(), func, *args, **kwargs)  # type: ignore\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/fsspec/asyn.py\", line 103, in sync\r\n    raise return_result\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/fsspec/asyn.py\", line 56, in _runner\r\n    result[0] = await coro\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/gradio_client/client.py\", line 1055, in _ws_fn\r\n    async with websockets.connect(  # type: ignore\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/websockets/legacy/client.py\", line 637, in __aenter__\r\n    return await self\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/websockets/legacy/client.py\", line 655, in __await_impl_timeout__\r\n    return await self.__await_impl__()\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/websockets/legacy/client.py\", line 662, in __await_impl__\r\n    await protocol.handshake(\r\nFile \"/path/to/miniconda3/envs/gradioclient/lib/python3.10/site-packages/websockets/legacy/client.py\", line 329, in handshake\r\n    raise InvalidStatusCode(status_code, response_headers)\r\n\r\nLoaded as API: http://localhost:7860/ ✔\r\nGR job failed again: server rejected WebSocket connection: HTTP 403\r\n    ... (same stack trace as above)\r\n```\r\n\r\n**Steps to Reproduce:**\r\n\r\n1. python generate.py --base_model=TheBloke/Mistral-7B-Instruct-v0.2-GGUF --prompt_type=mistral --max_seq_len=4096\r\n2. Run the Gradio client code. \r\n```\r\nimport time\r\nimport os\r\nimport sys\r\n\r\nfrom gradio_utils.grclient import GradioClient\r\n\r\n# self-contained example used for readme, to be copied to README_CLIENT.md if changed, setting local_server = True at first\r\n# The grclient.py file can be copied from h2ogpt repo and used with local gradio_client for example use\r\n\r\nif local_server:\r\n    client = GradioClient(\"http://0.0.0.0:7860\")\r\nelse:\r\n    h2ogpt_key = os.getenv('H2OGPT_KEY') or os.getenv('H2OGPT_H2OGPT_KEY')\r\n    if h2ogpt_key is None:\r\n        sys.exit(\"API key not found. Exiting.\")\r\n    # if you have API key for public instance:\r\n    client = GradioClient(\"https://gpt.h2o.ai\", h2ogpt_key=h2ogpt_key)\r\n\r\n\r\nprint(client.question(\"Who are you?\"))\r\n```\r\n\r\n* **Environment:**\r\n    * Operating System: Ubuntu 22.04.4 LTS\r\n    * gradio: 4.26.0\r\n    * gradio_client:  0.6.1\r\n    \r\n    \r\n * **Note:**\r\nI suspect this is due to internal network restrictions and using 127.0.0.1 should ideally work. Unfortunately it doesn't. \r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "You need to use gradio 4.26.0 and gradio_client 0.15.1 as is our default.\r\n\r\nAlso, in some cases, 0.0.0.0 is required as 127.0.0.1 is isolated to a local network."
      }
    ]
  },
  {
    "issue_number": 1720,
    "title": "databricks",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-07-02T22:57:01Z",
    "updated_at": "2024-07-02T22:57:01Z",
    "labels": [],
    "body": "https://python.langchain.com/v0.2/docs/integrations/llms/databricks/#wrapping-a-serving-endpoint-custom-model\r\nhttps://seattledataguy.substack.com/p/building-llms-on-databricks",
    "comments": []
  },
  {
    "issue_number": 1719,
    "title": "TypeError: 'ResponseFormat' object is not subscriptable",
    "author": "vanboom",
    "state": "closed",
    "created_at": "2024-07-01T22:25:59Z",
    "updated_at": "2024-07-01T22:28:02Z",
    "labels": [],
    "body": "In  the OpenAI server API, setting the `response_format` per the API docs causes a TypeError exception.   It appears that the code is attempting to treat the response_format as a hash instead of struct. \r\n\r\n```\r\n  File \"/data/h2ogpt/openai_server/backend.py\", line 156, in get_response\r\n    gen_kwargs['response_format'] = gen_kwargs.get('response_format')['type']\r\n```\r\n\r\nWhen the options are parsed, the response_format is a `ResponseFormat` struct, so accessing via `.type` works where `['type']` causes the TypeError.   \r\n\r\nHere is the diff working vs. non-working.\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/251c72bd70dd5b1ebd76046d9187e13f343597cb/openai_server/backend.py#L195\r\n```\r\ndiff --git a/openai_server/backend.py b/openai_server/backend.py\r\nindex fc77019a..7b41a18f 100644\r\n--- a/openai_server/backend.py\r\n+++ b/openai_server/backend.py\r\n@@ -152,7 +152,9 @@ def get_response(instruction, gen_kwargs, verbose=False, chunk_response=True, st\r\n     if gen_kwargs.get('response_format'):\r\n         # pydantic ensures type and key\r\n         # transcribe to h2oGPT way of just value\r\n-        gen_kwargs['response_format'] = gen_kwargs.get('response_format')['type']\r\n+        print(gen_kwargs)\r\n+        #gen_kwargs['response_format'] = gen_kwargs.get('response_format')['type']\r\n+        gen_kwargs['response_format'] = gen_kwargs.get('response_format').type\r\n \r\n     kwargs.update(**gen_kwargs)\r\n```\r\n\r\n",
    "comments": [
      {
        "user": "vanboom",
        "body": "This appears to be fixed in a newer version than what I was using when I reported this.  "
      }
    ]
  },
  {
    "issue_number": 1529,
    "title": "Can you add langsmith/wandb for tracing and ragas for evaluation metrics?",
    "author": "vitalyshalumov",
    "state": "open",
    "created_at": "2024-04-04T06:49:36Z",
    "updated_at": "2024-06-30T15:03:40Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I have some code in WIP for verifiers that include such things, but not done.  RAGAS is ok, but it's a bit loose compared to specific checking of actual specific faqs like done here: https://github.com/h2oai/enterprise-h2ogpte/tree/main/rag_benchmark"
      },
      {
        "user": "vitalyshalumov",
        "body": "Thank you,\r\nMy problem is not on a test set - it is on a per query metric:\r\nI want to let the user know the quality of the answer by showing him the  metrics:\r\n\r\n[Faithfulness]\r\n[Answer relevancy]\r\n[Context recall]\r\n[Context precision]\r\n[Context relevancy]\r\n[Context entity recall]\r\n\r\nhttps://docs.ragas.io/en/stable/concepts/metrics/index.html"
      }
    ]
  },
  {
    "issue_number": 1684,
    "title": "can't add personal data db/collection to auth.json",
    "author": "rxng",
    "state": "closed",
    "created_at": "2024-06-13T07:55:51Z",
    "updated_at": "2024-06-28T04:10:54Z",
    "labels": [],
    "body": "According to the instructions, we can add a make_db.py database to auth.json , but does not specify exactly how to do this. \r\n```\r\nTo make a new one for the user, fill `user_path_jon` with documents (can be soft or hard linked to avoid dups across multiple users), do:\r\n```bash\r\npython src/make_db.py --user_path=gptdocsdb/jon--collection_name=JonData --langchain_type=personal --hf_embedding_model=hkunlp/instructor-large --persist_directory=users/jon/db_dir_JonData\r\n```\r\n\r\nThen you'll have:\r\n```text\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ ls -alrt users/jon/db_dir_JonData/\r\ntotal 264\r\ndrwx------ 13 jon jon   4096 Apr 16 12:28 ../\r\ndrwx------  2 jon jon   4096 Apr 16 12:28 d7ccacb6-93fe-4380-9340-b7f5edffb655/\r\n-rw-------  1 jon jon 249856 Apr 16 12:28 chroma.sqlite3\r\n-rw-------  1 jon jon     41 Apr 16 12:28 embed_info\r\ndrwx------  3 jon jon   4096 Apr 16 12:28 ./\r\n```\r\n\r\nYou can add that database to the `auth.json` for their entry if using `auth.json` type file, and they will see when they login.\r\n```\r\n\r\nh2ogpt is being run like so and everything works well except it does not load the correct collection for the user \r\n`python generate.py --base_model=mistral-7b-instruct-v0.2.Q8_0.gguf --score_model=None --prompt_type=instruct --auth_access=closed --auth=auth.json --guest_name='' --auth_freeze`\r\n\r\nI have tried the following by adding db parameters but it does not work. \r\n```\r\n{\r\n  \"jon\": {\r\n    \"password\": \"jon1306\",\r\n    \"userid\": \"acb8fef1a77d122b5e12b261202ada7a\",\r\n    \"selection_docs_state\": {\r\n      \"langchain_modes\": [\r\n        \"JonData\",\r\n        \"LLM\",\r\n        \"Disabled\"\r\n      ],\r\n      \"langchain_mode_types\": {\r\n        \"JonData\": \"personal\"\r\n      }\r\n    },\r\n    \"dbs\": \"users/jon/db_dir_JonData\",\r\n    \"load_db_if_exists\": \"users/jon/db_dir_JonData\"\r\n  }\r\n}\r\n```\r\n\r\nHow do we make it such that when user logs in, their  collection JonData is automatically added? \r\nOr, Any way to simply specify a per user user_path? that would be easiest.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "If you are trying this for shared collection, did you try the CLI options?\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#multiple-embeddings-and-sources\r\n\r\ni.e.\r\n\r\n```\r\npython generate.py --model_lock=\"[{'base_model': 'llama', 'model_path_llama': 'Phi-3-mini-4k-instruct-q4.gguf', 'tokenizer_base_model': 'microsoft/Phi-3-mini-4k-instruct'}]\" --use_auth_token=$HUGGING_FACE_HUB_TOKEN --langchain_modes=\"['UserData', 'MyData', 'UserData2']\"\r\n```\r\n\r\nWould show all users those 2 by default.\r\n\r\nEven if a user logs in that already had a db entry, they will be forced to see those CLI ones.\r\n\r\nIf the system is online, without restarting, there's currently no way to add to all users at once with e.g. some kind of global user added settings.  Is that what you are trying to achieve?"
      },
      {
        "user": "pseudotensor",
        "body": "For personal collections, there's no CLI options for that, it's only in the db/json file.  By default sqlite3 db is used in newer h2oGPT to address speed issues with json, so one would have to edit the db using operations like in the src/db_utils.py.\r\n\r\nI'll think about how to handle this better, probably adding an option to add things via the admin page is best.  Would that work for you?"
      },
      {
        "user": "rxng",
        "body": "thanks for your quick response! Maybe I was confusing in my explanation. I was trying to achieve having a user logging in and then their own collection would be automatically loaded for them.\r\n\r\nHowever, I tried every single parameter and just found a way to do it via the auth.json file, by adding the line \r\n`\"langchain_mode\": \"JonData\",` above the selection_docs_state entry, like so\r\n```\r\n\"langchain_mode\": \"JonData\",\r\n    \"selection_docs_state\": {\r\n```\r\n\r\nThe only question I have is, if we wanted to then add more documents to the collection via make_db.py , would we then have to restart the entire instance of h2ogpt to automatically use the updated collection?\r\n\r\nIt would definitely be great if there was an admin page where these things could easily be managed :)"
      },
      {
        "user": "pseudotensor",
        "body": "![image](https://github.com/h2oai/h2ogpt/assets/2249614/3d01dd56-6096-4912-81df-4958be59cfb6)\r\n\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/eb697291-b422-4df7-858c-95823892e4d7)\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/99dd9371-2758-434c-a671-d97d64b8926a)\r\n\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/815ecc52-c030-4236-a2aa-c4d2ccd05eca)\r\n\r\n"
      },
      {
        "user": "rxng",
        "body": "> ![image](https://private-user-images.githubusercontent.com/2249614/344014499-3d01dd56-6096-4912-81df-4958be59cfb6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk1NDY4NjMsIm5iZiI6MTcxOTU0NjU2MywicGF0aCI6Ii8yMjQ5NjE0LzM0NDAxNDQ5OS0zZDAxZGQ1Ni02MDk2LTQ5MTItODFkZi00OTU4YmU1OWNmYjYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDYyOCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA2MjhUMDM0OTIzWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9N2E4YWI2NmY1NmJmNTllMzZiMGIxNzM5YTlkZjNhNzg5ODkzZjFiYmY2NWJjZTczZjU4MTNhMDRiNDg1OTNmNCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.VovhZ0UFqrbLX5V641O87RZjzXpbp0Xpx5ZOmvjgwLI)\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/2249614/344014591-eb697291-b422-4df7-858c-95823892e4d7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk1NDY4NjMsIm5iZiI6MTcxOTU0NjU2MywicGF0aCI6Ii8yMjQ5NjE0LzM0NDAxNDU5MS1lYjY5NzI5MS1iNDIyLTRkZjctODU4Yy05NTgyMzg5MmU0ZDcucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDYyOCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA2MjhUMDM0OTIzWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZmY0MjhkODJkYTllMDBmMDljMWExNjI0YmNiNjFlZGQ3NjE3YmRhODM0ODBlNmQwZmE4YjM2NjBlMjA1NDMwYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.xhcqegxl8d6SrdBCi0jmNuOwXU0IHoE8Um7gEL7g7kA)\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/2249614/344014659-99dd9371-2758-434c-a671-d97d64b8926a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk1NDY4NjMsIm5iZiI6MTcxOTU0NjU2MywicGF0aCI6Ii8yMjQ5NjE0LzM0NDAxNDY1OS05OWRkOTM3MS0yNzU4LTQzNGMtYTY3MS1kOTdkNjRiODkyNmEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDYyOCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA2MjhUMDM0OTIzWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MmQyYmU4NjQ4MWVhYWUzMjYxYzM2MDcyN2FkZjhkMGU2ZjgyMTAzZWIxMDdmNzk5OTJmNzM4MWJlODRjNmUyMCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.0jIwwX_hfgUqEdeVGKvpExcZiELLeG008tTtox_gAW0)\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/2249614/344014739-815ecc52-c030-4236-a2aa-c4d2ccd05eca.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTk1NDY4NjMsIm5iZiI6MTcxOTU0NjU2MywicGF0aCI6Ii8yMjQ5NjE0LzM0NDAxNDczOS04MTVlY2M1Mi1jMDMwLTQyMzYtYTJhYS1jNGQyY2NkMDVlY2EucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDYyOCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA2MjhUMDM0OTIzWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NGUzMmVmMDczYTRmNTc5NTczNmYzN2Y3MjdjNmJmYjljODc1ZDk1Nzg1NDI2ZmFjZmNjZmQ1YWJjOTVkZTBlMCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.Lye91otTrAkFdHY5OqMrLP1tDbpyk3xzAIdDBj1k_xs)\r\n\r\nthat's so amazing @pseudotensor !!"
      },
      {
        "user": "pseudotensor",
        "body": "Note that if you have an auth file that is .json, just pass to CLI that it is now .db and we'll migrate it to .db format that is required for this control\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/3498b03fcd814458cea7e319039c42df48b1231a/src/db_utils.py#L80-L101"
      }
    ]
  },
  {
    "issue_number": 1671,
    "title": "Unable to Programmatically Receive Sources with Prompts & Responses",
    "author": "devinrouthuzh",
    "state": "closed",
    "created_at": "2024-06-06T15:23:51Z",
    "updated_at": "2024-06-28T03:59:10Z",
    "labels": [],
    "body": "Firstly, thanks for the aid with my previous question! (#1023) Moreover, I found #685 to be very helpful for my needs—much appreciated for referencing it in my issue.\r\n\r\nOn the note of #685: I'm successfully able to programmatically evaluate a set of prompts in a JSON using:\r\n\r\n```\r\nfrom tests.utils import make_user_path_test\r\n\r\ndef run_eval(cpu=False, bits=None, base_model='h2oai/h2ogpt-oig-oasst1-512-6_9b',eval_filename=None,eval_prompts_only_num=1):\r\n    \r\n    from src.gen import main\r\n    \r\n    user_path = make_user_path_test()\r\n    \r\n    kwargs = dict(\r\n            stream_output=False,\r\n            langchain_mode='UserData',\r\n            langchain_modes=['UserData']\r\n    )\r\n    \r\n    eval_out_filename = main(base_model=base_model,\r\n                             eval=True, gradio=False,\r\n                             eval_filename=eval_filename,\r\n                             eval_prompts_only_num=eval_prompts_only_num,\r\n                             eval_as_output=False,\r\n                             eval_prompts_only_seed=123456,\r\n                             answer_with_sources=True,append_sources_to_answer=True,append_sources_to_chat=False, # !! Added so sources are appended\r\n                             user_path='src/user_path',show_link_in_sources=True, # !! Added so sources are appended\r\n                             **kwargs)\r\n    return eval_out_filename\r\n\r\neval_filename = 'my_prompts.json'\r\nnprompts = 2\r\nbits = 8\r\ncpu = False\r\nbase_model = 'h2oai/h2ogpt-4096-llama2-7b-chat'\r\neval_out_filename = run_eval(cpu=cpu, bits=bits, base_model=base_model,eval_filename=eval_filename,eval_prompts_only_num=nprompts)\r\n```\r\n\r\nwhich is code that was streamlined from the suggested source: [h2ogpt/tests/test_eval.py](https://github.com/h2oai/h2ogpt/blob/9e0e35286d6ae022ae41e46659b3786e95a11f11/tests/test_eval.py#L247-L299).\r\n\r\nWhen running this code, however, I noticed that the model never correctly sets `langchain_mode` to `UserData` (it always stays as `'langchain_mode': None`), and I'm never able to programmatically receive the citations/sources from the database with the prompts.\r\n\r\nNotably, the following code generates a gradio user interface through which `langchain_mode` is correctly set as `UserData` and wherein I receive all citations/sources/tokens correctly (but interactively):\r\n```\r\npython generate.py --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --answer_with_sources=True --langchain_mode=UserData\r\n```\r\n\r\nI attempted to use the following call as well, but `langchain_mode` remains set to `None` regardless of my inputs:\r\n```\r\npython generate.py --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --answer_with_sources=True --langchain_mode=UserData --eval=True --gradio=False --eval_prompts_only_seed=54321 --eval_filename=my_prompts.json --eval_prompts_only_num=2 --user_path=src/user_path\r\n```\r\n\r\nI need to be able to programmatically receive the responses from my custom prompts along with the paired sources/citations. Am I missing something simple here?\r\n\r\nThanks again!",
    "comments": [
      {
        "user": "devinrouthuzh",
        "body": "Hi there—does anyone else have trouble setting the Langchain mode and receiving source information when interacting programmatically with a model? Am I missing a flag somewhere, or is there something else happening?\r\n\r\nThanks!"
      }
    ]
  },
  {
    "issue_number": 1624,
    "title": "GPU Installation",
    "author": "jaysunl",
    "state": "closed",
    "created_at": "2024-05-16T00:52:38Z",
    "updated_at": "2024-06-28T03:11:54Z",
    "labels": [],
    "body": "May I possibly have specific instructions as for the GPU installation of the tool? I have followed the installation but it still says no GPU detected. I have the following GPU on my system:  NVIDIA Corporation GA102GL [A10G] (rev a1) and my nvcc --version output is \r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Thu_Nov_18_09:45:30_PST_2021\r\nCuda compilation tools, release 11.5, V11.5.119\r\nBuild cuda_11.5.r11.5/compiler.30672275_0\r\n\r\nAnd tried to install Pytorch CUDA. But when I run\r\nimport torch\r\nprint(torch.cuda.is_available())\r\n\r\nit still says False and when I try to run a model, it still says no GPU detected. Any guidance would be appreciated.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It's a good start that you checked that torch condition.  We only really support cuda 12.1 and above at this point, so maybe there's some issue with the installation because you have old cuda toolkit.  It's easy to follow our instructions for installing the cuda toolkit 12.1, but you'll need drivers that are also compatible."
      },
      {
        "user": "jaysunl",
        "body": "Ok I was actually able to follow the GPU version of PyTorch. But the launching of the actual interface takes forever. I am running this command:\r\npython generate.py --base_model=h2oai/h2ogpt-oig-oasst1-512-6_9b --load_8bit=True --langchain_mode=UserData --user_path=/some/path\r\n\r\nbut the output gets stuck at this:\r\nsoundfile, librosa, and wavio not installed, disabling STT\r\nsoundfile, librosa, and wavio not installed, disabling TTS\r\nUsing Model h2oai/h2ogpt-oig-oasst1-512-6_9b\r\n\r\nwith nothing after it. It has been like that for at least an hour. Any tips?"
      },
      {
        "user": "pseudotensor",
        "body": "I recommend not using h2oai/h2ogpt-oig-oasst1-512-6_9b as a model, but instead a GGUF model at first like `https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF`.  Also, ensure to -pass `--verbose=True` to get more info.\r\n\r\n```\r\npython generate.py --base_model=llama --model_path_llama=https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf?download=true --tokenizer_base_model=meta-llama/Meta-Llama-3-8B-Instruct --max_seq_len=8192 --verbose=True\r\n```"
      },
      {
        "user": "jaysunl",
        "body": "Sorry, I'm getting a Python: no match error when I enter this command and I copied it exactly as is. I'm using Python 3.10.12.\r\n\r\nI also tried this (without the ?download=true at the end of the model_llama_path) and it gets stuck at this:\r\n\r\n``` \r\nsoundfile, librosa, and wavio not installed, disabling STT\r\nsoundfile, librosa, and wavio not installed, disabling TTS\r\nUsing Model llama\r\nGenerating model with params:\r\nload_8bit: False\r\nload_4bit: False\r\nlow_bit_mode: 1\r\nload_half: True\r\nuse_flash_attention_2: False\r\nload_gptq: \r\nuse_autogptq: False\r\nload_awq: \r\nload_exllama: False\r\nuse_safetensors: False\r\nrevision: None\r\nuse_gpu_id: True\r\nbase_model: llama\r\ntokenizer_base_model: meta-llama/Meta-Llama-3-8B-Instruct\r\nlora_weights: \r\ngpu_id: 0\r\ncompile_model: None\r\nuse_cache: None\r\ninference_server: \r\nregenerate_clients: True\r\nregenerate_gradio_clients: False\r\nprompt_type: llama2\r\nprompt_dict: {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}\r\nsystem_prompt: auto\r\nallow_chat_system_prompt: True\r\nllamacpp_path: llamacpp_path\r\nllamacpp_dict: {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': 'https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}\r\nexllama_dict: {}\r\ngptq_dict: {}\r\nattention_sinks: False\r\nsink_dict: {}\r\ntruncation_generation: False\r\nhf_model_dict: {}\r\nforce_seq2seq_type: False\r\nforce_t5_type: False\r\nmodel_lock: None\r\nmodel_lock_columns: None\r\nmodel_lock_layout_based_upon_initial_visible: False\r\nfail_if_cannot_connect: False\r\ntemperature: 0.0\r\ntop_p: 1.0\r\ntop_k: 1\r\npenalty_alpha: 0\r\nnum_beams: 1\r\nrepetition_penalty: 1.07\r\nnum_return_sequences: 1\r\ndo_sample: False\r\nseed: 0\r\nmax_new_tokens: 1024\r\nmin_new_tokens: 0\r\nearly_stopping: False\r\nmax_time: 600\r\nmemory_restriction_level: 0\r\ndebug: False\r\nsave_dir: None\r\nlocal_files_only: False\r\nresume_download: True\r\nuse_auth_token: False\r\ntrust_remote_code: True\r\nrope_scaling: {}\r\nmax_seq_len: 8192\r\nmax_output_seq_len: None\r\noffload_folder: offline_folder\r\nsrc_lang: English\r\ntgt_lang: Russian\r\nprepare_offline_level: 0\r\ncli: False\r\ncli_loop: True\r\ngradio: True\r\nopenai_server: True\r\nopenai_port: 5000\r\ngradio_offline_level: 0\r\nserver_name: 0.0.0.0\r\nshare: True\r\nopen_browser: False\r\nclose_button: True\r\nshutdown_via_api: False\r\nroot_path: \r\nssl_verify: True\r\nssl_keyfile: None\r\nssl_certfile: None\r\nssl_keyfile_password: None\r\nchat: True\r\nchat_conversation: []\r\ntext_context_list: []\r\nstream_output: True\r\nasync_output: True\r\nnum_async: 3\r\nshow_examples: False\r\nverbose: True\r\nh2ocolors: True\r\ndark: False\r\nheight: 600\r\nrender_markdown: True\r\nshow_lora: True\r\nshow_llama: True\r\nshow_gpt4all: False\r\nlogin_mode_if_model0: False\r\nblock_gradio_exit: True\r\nconcurrency_count: 1\r\napi_open: False\r\nallow_api: True\r\nsystem_api_open: False\r\ninput_lines: 1\r\ngradio_size: None\r\nshow_copy_button: True\r\nlarge_file_count_mode: False\r\ngradio_ui_stream_chunk_size: 0\r\ngradio_ui_stream_chunk_min_seconds: 0.2\r\ngradio_ui_stream_chunk_seconds: 2.0\r\ngradio_api_use_same_stream_limits: True\r\ngradio_upload_to_chatbot: False\r\ngradio_upload_to_chatbot_num_max: 2\r\ngradio_errors_to_chatbot: True\r\npre_load_embedding_model: True\r\nembedding_gpu_id: auto\r\nauth: None\r\nauth_filename: auth.json\r\nauth_access: open\r\nauth_freeze: False\r\nauth_message: None\r\ngoogle_auth: False\r\nguest_name: guest\r\nenforce_h2ogpt_api_key: False\r\nenforce_h2ogpt_ui_key: False\r\nh2ogpt_api_keys: []\r\nh2ogpt_key: None\r\nextra_allowed_paths: []\r\nblocked_paths: []\r\nmax_max_time: 1200\r\nmax_max_new_tokens: 1024\r\nmax_visible_models: None\r\nvisible_ask_anything_high: True\r\nvisible_visible_models: True\r\nvisible_submit_buttons: True\r\nvisible_side_bar: True\r\nvisible_doc_track: True\r\nvisible_chat_tab: True\r\nvisible_doc_selection_tab: True\r\nvisible_doc_view_tab: True\r\nvisible_chat_history_tab: True\r\nvisible_expert_tab: True\r\nvisible_models_tab: True\r\nvisible_system_tab: True\r\nvisible_tos_tab: False\r\nvisible_login_tab: True\r\nvisible_hosts_tab: False\r\nchat_tables: False\r\nvisible_h2ogpt_links: True\r\nvisible_h2ogpt_qrcode: True\r\nvisible_h2ogpt_logo: True\r\nvisible_chatbot_label: True\r\nvisible_all_prompter_models: False\r\nvisible_curated_models: True\r\nactions_in_sidebar: False\r\ndocument_choice_in_sidebar: True\r\nenable_add_models_to_list_ui: False\r\nmax_raw_chunks: 1000000\r\npdf_height: 800\r\navatars: True\r\nadd_disk_models_to_ui: True\r\npage_title: h2oGPT\r\nmodel_label_prefix: h2oGPT\r\nfavicon_path: None\r\nvisible_ratings: False\r\nreviews_file: None\r\nsanitize_user_prompt: False\r\nsanitize_bot_response: False\r\nextra_model_options: []\r\nextra_lora_options: []\r\nextra_server_options: []\r\nscore_model: \r\nverifier_model: None\r\nverifier_tokenizer_base_model: None\r\nverifier_inference_server: None\r\neval_filename: None\r\neval_prompts_only_num: 0\r\neval_prompts_only_seed: 1234\r\neval_as_output: False\r\nlangchain_mode: UserData\r\nuser_path: None\r\nlangchain_modes: ['UserData', 'MyData', 'LLM', 'Disabled']\r\nlangchain_mode_paths: {'UserData': None}\r\nlangchain_mode_types: {'UserData': 'shared', 'github h2oGPT': 'shared', 'DriverlessAI docs': 'shared', 'wiki': 'shared', 'wiki_full': ''}\r\ndetect_user_path_changes_every_query: False\r\nupdate_selection_state_from_cli: True\r\nlangchain_action: Query\r\nlangchain_agents: []\r\nforce_langchain_evaluate: False\r\nvisible_langchain_actions: ['Query', 'Summarize', 'Extract']\r\nvisible_langchain_agents: ['Collection', 'Python', 'CSV', 'Pandas', 'JSON', 'SMART', 'AUTOGPT']\r\ndocument_subset: Relevant\r\ndocument_choice: ['All']\r\ndocument_source_substrings: []\r\ndocument_source_substrings_op: and\r\ndocument_content_substrings: []\r\ndocument_content_substrings_op: and\r\nuse_llm_if_no_docs: True\r\nload_db_if_exists: True\r\nkeep_sources_in_context: False\r\ndb_type: chroma\r\nuse_openai_embedding: False\r\nuse_openai_model: False\r\nhf_embedding_model: hkunlp/instructor-large\r\nmigrate_embedding_model: False\r\nauto_migrate_db: False\r\ncut_distance: 1.64\r\nanswer_with_sources: True\r\nappend_sources_to_answer: False\r\nappend_sources_to_chat: True\r\nshow_accordions: True\r\ntop_k_docs_max_show: 10\r\nshow_link_in_sources: True\r\nlangchain_instruct_mode: True\r\npre_prompt_query: None\r\nprompt_query: None\r\npre_prompt_summary: None\r\nprompt_summary: None\r\nhyde_llm_prompt: None\r\nadd_chat_history_to_context: True\r\nadd_search_to_context: False\r\ncontext: \r\niinput: \r\nallow_upload_to_user_data: True\r\nreload_langchain_state: True\r\nallow_upload_to_my_data: True\r\nenable_url_upload: True\r\nenable_text_upload: True\r\nenable_sources_list: True\r\nchunk: True\r\nchunk_size: 512\r\ntop_k_docs: 10\r\ndocs_ordering_type: best_near_prompt\r\nmin_max_new_tokens: 512\r\nmax_input_tokens: -1\r\nmax_total_input_tokens: -1\r\ndocs_token_handling: split_or_merge\r\ndocs_joiner: \r\n\r\n\r\nhyde_level: 0\r\nhyde_template: None\r\nhyde_show_only_final: False\r\nhyde_show_intermediate_in_accordion: True\r\ndoc_json_mode: False\r\nmetadata_in_context: auto\r\nauto_reduce_chunks: True\r\nmax_chunks: 100\r\nheadsize: 50\r\nn_jobs: 32\r\nn_gpus: 1\r\nclear_torch_cache_level: 1\r\nuse_unstructured: True\r\nuse_playwright: False\r\nuse_selenium: False\r\nuse_scrapeplaywright: False\r\nuse_scrapehttp: False\r\nuse_pymupdf: auto\r\nuse_unstructured_pdf: auto\r\nuse_pypdf: auto\r\nenable_pdf_ocr: auto\r\nenable_pdf_doctr: auto\r\ntry_pdf_as_html: auto\r\nenable_ocr: False\r\nenable_doctr: True\r\nenable_pix2struct: False\r\nenable_captions: True\r\nenable_llava: True\r\nenable_transcriptions: True\r\npre_load_image_audio_models: False\r\ncaption_gpu: True\r\ncaption_gpu_id: auto\r\ncaptions_model: Salesforce/blip-image-captioning-base\r\ndoctr_gpu: True\r\ndoctr_gpu_id: auto\r\nllava_model: None\r\nllava_prompt: auto\r\nimage_file: None\r\nimage_control: None\r\nresponse_format: text\r\nguided_json: \r\nguided_regex: \r\nguided_choice: \r\nguided_grammar: \r\nasr_model: openai/whisper-medium\r\nasr_gpu: True\r\nasr_gpu_id: auto\r\nasr_use_better: True\r\nasr_use_faster: False\r\nenable_stt: False\r\nstt_model: openai/whisper-base.en\r\nstt_gpu: True\r\nstt_gpu_id: auto\r\nstt_continue_mode: 1\r\nenable_tts: False\r\ntts_gpu: True\r\ntts_gpu_id: auto\r\ntts_model: microsoft/speecht5_tts\r\ntts_gan_model: microsoft/speecht5_hifigan\r\ntts_coquiai_deepspeed: True\r\ntts_coquiai_roles: {}\r\nchatbot_role: None\r\nspeaker: None\r\ntts_language: autodetect\r\ntts_speed: 1.0\r\ntts_action_phrases: []\r\ntts_stop_phrases: []\r\nsst_floor: 100\r\nenable_image: False\r\nvisible_image_models: []\r\nimage_gpu_ids: []\r\nenable_llava_chat: False\r\njq_schema: .[]\r\nextract_frames: 10\r\nmax_quality: False\r\nenable_heap_analytics: True\r\nheap_app_id: 1680123994\r\nroles_state0: {}\r\nbase_model0: llama\r\nenable_imagegen: False\r\nenable_imagechange: False\r\nenable_imagestyle: False\r\nis_hf: False\r\nis_gpth2oai: False\r\nis_public: False\r\nadmin_pass: None\r\nraise_generate_gpu_exceptions: True\r\nh2ogpt_pid: 13294\r\nlmode: wiki_full\r\nall_inference_server: None\r\nn_gpus1: 1\r\ngpu_ids: [0]\r\nmodel_lower: llama\r\nmodel_lower0: llama\r\nfirst_para: False\r\ntext_limit: None\r\ncaption_loader: None\r\ndoctr_loader: None\r\npix2struct_loader: None\r\nasr_loader: None\r\nimage_audio_loaders_options0: ['Caption']\r\nimage_audio_loaders_options: ['Caption', 'CaptionBlip2', 'Pix2Struct']\r\npdf_loaders_options0: ['PyPDF']\r\npdf_loaders_options: ['Unstructured', 'PyPDF', 'TryHTML']\r\nurl_loaders_options0: ['Unstructured']\r\nurl_loaders_options: ['Unstructured', 'ScrapeWithHttp']\r\njq_schema0: .[]\r\nextract_frames0: 10\r\nimage_audio_loaders: ['Caption']\r\npdf_loaders: ['PyPDF']\r\nurl_loaders: ['Unstructured']\r\nplaceholder_instruction: \r\nplaceholder_input: \r\nexamples: [['Translate English to French', 'Good morning', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['Give detailed answer for whether Einstein or Newton is smarter.', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['Explain in detailed list, all the best practices for coding in python.', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['Create a markdown table with 3 rows for the primary colors, and 2 columns, with color name and hex codes.', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['Translate to German:  My name is Arthur', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], [\"Please answer to the following question. Who is going to be the next Ballon d'or?\", '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['Please answer the following question. What is the boiling point of Nitrogen?', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['Answer the following yes/no question. Can you write a whole Haiku in a single tweet?', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['Simplify the following expression: (False or False and True). Explain your answer.', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], [\"Premise: At my age you will probably have learnt one lesson. Hypothesis:  It's not certain how many lessons you'll learn by your thirties. Does the premise entail the hypothesis?\", '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['The square root of x is the cube root of y. What is y to the power of 2, if x = 4?', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['Answer the following question by reasoning step by step.  The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['def area_of_rectangle(a: float, b: float):\\n    \"\"\"Return the area of the rectangle.\"\"\"', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['# a function in native python:\\ndef mean(a):\\n    return sum(a)/len(a)\\n\\n# the same function using numpy:\\nimport numpy as np\\ndef mean(a):', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['X = np.random.randn(100, 100)\\ny = np.random.randint(0, 1, 100)\\n\\n# fit random forest classifier with 20 estimators', '', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', ''], ['Jeff: Can I train a ? Transformers model on Amazon SageMaker?\\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container.\\nJeff: ok.\\nJeff: and how can I get started?\\nJeff: where can I find documentation?\\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face', 'Summarize', '', True, 'llama2', None, 0.0, 1.0, 1, 0, 1, 1024, 0, False, 600, 1.07, 1, False, 0, True, '', '', 'Disabled', True, 'Query', [], 10, True, 512, 'Relevant', [], [], 'and', [], 'and', None, None, None, None, None, 'auto', ['Caption'], ['PyPDF'], ['Unstructured'], '.[]', 10, 'auto', None, None, None, False, None, None, 'best_near_prompt', 512, -1, -1, 'split_or_merge', '\\n\\n', 0, None, False, False, 'auto', 'None', 'None', 'autodetect', 1.0, None, None, 'text', '', '', '', '']]\r\ntask_info: No task\r\ngit_hash: 6c284d58026992bdc9ec20f1be03fac0974d9a42\r\nvisible_models: None\r\nCommand: generate.py --base_model=llama --model_path_llama=https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf --tokenizer_base_model=meta-llama/Meta-Llama-3-8B-Instruct --langchain_mode=UserData --share=True --max_seq_len=8192 --verbose=True\r\nHash: 6c284d58026992bdc9ec20f1be03fac0974d9a42\r\n``` "
      },
      {
        "user": "pseudotensor",
        "body": "Can you find the PID of the process and when it gets stuck in a separate terminal do `kill -s SIGUSR1 PID` ?  and share the full traceback."
      },
      {
        "user": "jaysunl",
        "body": "It says SIGUSR1: Unknown signal; kill -l lists signals. I replaced PID with the actual PID and it said that\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "What platform are you using?  linux, windows, mac?"
      },
      {
        "user": "jaysunl",
        "body": "Linux. SIGUSR1 isn't actually an available signal on my machine\r\n\r\nOn Thu, May 23, 2024 at 1:08 AM pseudotensor ***@***.***>\r\nwrote:\r\n\r\n> What platform are you using? linux, windows, mac?\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://urldefense.com/v3/__https://github.com/h2oai/h2ogpt/issues/1624*issuecomment-2126489537__;Iw!!Mih3wA!G78pxIIEqntUEtnjJ4i6ZaAKwPtcB-ROl4-cL2dlLp3zq_68g9b8JeQCRJi-oDTwupLLA3-ORT2dGn_07-nUP5N3$>,\r\n> or unsubscribe\r\n> <https://urldefense.com/v3/__https://github.com/notifications/unsubscribe-auth/ARX4ZIYKB7PS2OPMGATUSMDZDWPWVAVCNFSM6AAAAABHZFNBGCVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCMRWGQ4DSNJTG4__;!!Mih3wA!G78pxIIEqntUEtnjJ4i6ZaAKwPtcB-ROl4-cL2dlLp3zq_68g9b8JeQCRJi-oDTwupLLA3-ORT2dGn_071Wolxol$>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Ok that's very odd, not sure about that.  Normal PC w/ Linux has that signal, you must have something else special.\r\n\r\nTry downloading the FILE separately and place in the llamacpp_path folder, then start h2oGPT with `--base_model=llama --llama_cpp_path=FILE` ."
      },
      {
        "user": "jaysunl",
        "body": "It still stops at the same place it does above. The thing is when I try launching it on CPU, it works but when I set the CUDA_VISIBLE_DEVICES env variable to enable GPU, it fails and just doesn't continue after it says Using Model Llama\r\n\r\nEdit: I did some debugging and found that it gets stuck at this line here: https://github.com/h2oai/h2ogpt/blob/e11d4c7fb4bffbfb66aaaab69718d97a299a1e18/src/gen.py#L1956"
      },
      {
        "user": "pseudotensor",
        "body": "So during getting or initializing the embedding model, which should be fairly trivial.  It's not clear, and really hard to debug without SIGUSR1.\r\n\r\nHave you tried the docker installation to avoid concerns about installation issues?"
      },
      {
        "user": "jaysunl",
        "body": "Sorry for the late reply. I just tried the Docker version and encountered an out of space error running docker build -t h2ogpt .\r\n\r\n\r\n--> 39936edc8bac\r\nStep 15/25 : RUN chmod -R a+rwx /workspace\r\n---> Running in 216f25a14b33\r\nwrite /workspace/llamacpp_path/zephyr-7b-beta.Q5_K_M.gguf: no space left on device\r\n\r\n\r\n\r\nHow much space is this expected to use?"
      },
      {
        "user": "pseudotensor",
        "body": "When you build a docker image, make sure the local path is clean.  E.g. I recommend a separate clone of the repo and be in there."
      },
      {
        "user": "jaysunl",
        "body": "Ok I have managed to build the container. I ran this example on the README:\r\n\r\n```\r\nmkdir -p ~/.cache\r\nmkdir -p ~/save\r\nmkdir -p ~/user_path\r\nmkdir -p ~/db_dir_UserData\r\nmkdir -p ~/users\r\nmkdir -p ~/db_nonusers\r\nmkdir -p ~/llamacpp_path\r\nmkdir -p ~/h2ogpt_auth\r\necho '[\"key1\",\"key2\"]' > ~/h2ogpt_auth/h2ogpt_api_keys.json\r\nexport GRADIO_SERVER_PORT=7860\r\nexport OPENAI_SERVER_PORT=5000\r\ndocker run \\\r\n       --gpus all \\\r\n       --runtime=nvidia \\\r\n       --shm-size=2g \\\r\n       -p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT \\\r\n       -p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT \\\r\n       --rm --init \\\r\n       --network host \\\r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u `id -u`:`id -g` \\\r\n       -v \"${HOME}\"/.cache:/workspace/.cache \\\r\n       -v \"${HOME}\"/save:/workspace/save \\\r\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\r\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\r\n       -v \"${HOME}\"/users:/workspace/users \\\r\n       -v \"${HOME}\"/db_nonusers:/workspace/db_nonusers \\\r\n       -v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path \\\r\n       -v \"${HOME}\"/h2ogpt_auth:/workspace/h2ogpt_auth \\\r\n       -e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT \\\r\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.0 /workspace/generate.py \\\r\n          --base_model=HuggingFaceH4/zephyr-7b-beta \\\r\n          --use_safetensors=True \\\r\n          --prompt_type=zephyr \\\r\n          --save_dir='/workspace/save/' \\\r\n          --auth_filename='/workspace/h2ogpt_auth/auth.json'\r\n          --h2ogpt_api_keys='/workspace/h2ogpt_auth/h2ogpt_api_keys.json'\r\n          --use_gpu_id=False \\\r\n          --user_path=/workspace/user_path \\\r\n          --langchain_mode=\"LLM\" \\\r\n          --langchain_modes=\"['UserData', 'LLM']\" \\\r\n          --score_model=None \\\r\n          --max_max_new_tokens=2048 \\\r\n          --max_new_tokens=1024 \\\r\n          --use_auth_token=\"${HUGGING_FACE_HUB_TOKEN}\" \\\r\n          --openai_port=$OPENAI_SERVER_PORT\r\n```\r\n\r\nBut seems to be hanging after this output:\r\n```\r\nWARNING: Published ports are discarded when using host network mode\r\nUsing Model huggingfaceh4/zephyr-7b-beta\r\nfatal: not a git repository (or any of the parent directories): .git\r\n```\r\n\r\nDoes it have anything to do with git_hash.txt? I am running this in the h2ogpt directory.\r\n\r\nAlso when I try to make a db in the container prior to running the bot, I am hanging after this output:\r\n```\r\n100%|██████████| 181/181 [02:31<00:00,  1.19it/s]\r\nExceptions: 0/17294 []\r\n```\r\n"
      },
      {
        "user": "jaysunl",
        "body": "Actually never mind, I resolved the issue. Thank you so much for your help!\r\n\r\nOne thing I'm curious about is that if I have multiple doc source directories, how would I launch multiple chatbots? Do I need to specify multiple generate.py commands? Would it all be connected to one container? I'm using docker-compose for reference."
      },
      {
        "user": "pseudotensor",
        "body": "Yes, if you made multiple collections, but want each to be served separately, then you can make them with make_db, then later launch separate h2oGPT for each, using CLI options like used here: https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#multiple-embeddings-and-sources\r\n\r\nYou can use TEI server to share the embeddings if you want to save on GPU and get better speed.  I share in FAQ how gpt.h2o.ai is setup using TEI server: https://github.com/h2oai/h2ogpt/blob/main/docs/FAQ.md#text-embedding-inference-server\r\n\r\n"
      },
      {
        "user": "jaysunl",
        "body": "Ok sounds good thanks! I am also receiving this error \r\n\r\n``` \r\nh2ogpt_1  | WARNING:matplotlib:Matplotlib created a temporary cache directory at /tmp/matplotlib-l50c0tvu because the default path (/workspace/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\r\nh2ogpt_1  | There was a problem when trying to write in your cache folder (/workspace/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\r\nh2ogpt_1  | Using Model huggingfaceh4/zephyr-7b-beta\r\nh2ogpt_1  | fatal: not a git repository (or any of the parent directories): .git\r\nh2ogpt_1  | Traceback (most recent call last):\r\nh2ogpt_1  |   File \"/workspace/generate.py\", line 20, in <module>\r\nh2ogpt_1  |     entrypoint_main()\r\nh2ogpt_1  |   File \"/workspace/generate.py\", line 16, in entrypoint_main\r\nh2ogpt_1  |     H2O_Fire(main)\r\nh2ogpt_1  |   File \"/workspace/src/utils.py\", line 72, in H2O_Fire\r\nh2ogpt_1  |     fire.Fire(component=component, command=args)\r\nh2ogpt_1  |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\nh2ogpt_1  |     component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\nh2ogpt_1  |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\nh2ogpt_1  |     component, remaining_args = _CallAndUpdateTrace(\r\nh2ogpt_1  |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\nh2ogpt_1  |     component = fn(*varargs, **kwargs)\r\nh2ogpt_1  |   File \"/workspace/src/gen.py\", line 1886, in main\r\nh2ogpt_1  |     model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\nh2ogpt_1  |   File \"/workspace/src/gpt_langchain.py\", line 544, in get_embedding\r\nh2ogpt_1  |     embedding = HuggingFaceEmbeddings(model_name=hf_embedding_model, model_kwargs=model_kwargs)\r\nh2ogpt_1  |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 72, in __init__\r\nh2ogpt_1  |     self.client = sentence_transformers.SentenceTransformer(\r\nh2ogpt_1  |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 87, in __init__\r\nh2ogpt_1  |     snapshot_download(model_name_or_path,\r\nh2ogpt_1  |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/util.py\", line 476, in snapshot_download\r\nh2ogpt_1  |     os.makedirs(nested_dirname, exist_ok=True)\r\nh2ogpt_1  |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/os.py\", line 215, in makedirs\r\nh2ogpt_1  |     makedirs(head, exist_ok=exist_ok)\r\nh2ogpt_1  |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/os.py\", line 215, in makedirs\r\nh2ogpt_1  |     makedirs(head, exist_ok=exist_ok)\r\nh2ogpt_1  |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/os.py\", line 225, in makedirs\r\nh2ogpt_1  |     mkdir(name, mode)\r\nh2ogpt_1  | PermissionError: [Errno 13] Permission denied: '/workspace/.cache/torch'\r\n```\r\n\r\nafter running `docker-compose up -d --build`. My docker-compose.yml looks like this\r\n```\r\nversion: '3'\r\n\r\nservices:\r\n  h2ogpt:\r\n    build:\r\n      context: .\r\n      dockerfile: Dockerfile\r\n    restart: always\r\n    shm_size: '2gb'\r\n    ports:\r\n      - '${H2OGPT_PORT}:7860'\r\n    volumes:\r\n      #- cache:/workspace/.cache\r\n      #- save:/workspace/save\r\n      - ../workspace/.cache:/workspace/.cache\r\n      - ../workspace/save:/workspace/save\r\n      - ../workspace/user_path:/workspace/user_path\r\n      - ../workspace/db_dir_UserData:/workspace/db_dir_UserData\r\n      - ../workspace/users:/workspace/users\r\n      - ../workspace/db_nonusers:/workspace/db_nonusers\r\n      - ../workspace/llamacpp_path:/workspace/llamacpp_path\r\n      - ../workspace/h2ogpt_auth:/workspace/h2ogpt_auth\r\n    command: |\r\n      /workspace/generate.py\r\n      --base_model=HuggingFaceH4/zephyr-7b-beta\r\n      --hf_embedding_model=sentence-transformers/all-MiniLM-L6-v2\r\n      --score_model=None\r\n      --use_gpu_id=0\r\n --langchain_mode='UserData'\r\n      --langchain_modes=['UserData2']\r\n      --enable_tts=False\r\n      --enable_stt=False\r\n      --enable_transcriptions=False\r\n      --max_seq_len=2048\r\n      --load_4bit=True\r\n      --share=True\r\n    deploy:\r\n      resources:\r\n        reservations:\r\n          devices:\r\n          - driver: nvidia\r\n            count: all\r\n            capabilities: [gpu]\r\n\r\nvolumes:\r\n  cache:\r\n  save:\r\n```\r\n\r\nAny workarounds?                 \r\n"
      },
      {
        "user": "pseudotensor",
        "body": "My guess is that you didn't make the directory as a user before it was created as root by the docker image. I've seen it in that case.  If you already have the dirs, please remove them.\r\n\r\nThen do as user:\r\n```\r\nmkdir -p $HOME/.cache/huggingface/hub\r\nmkdir -p $HOME/.triton/cache/\r\nmkdir -p $HOME/.config/vllm\r\n```\r\n\r\nthen run docker run so that those paths are mapped, e.g.:\r\n```\r\nport=5003\r\ntokens=8192\r\ndocker run -d --restart=always \\\r\n    --runtime=nvidia \\\r\n    --gpus '\"device=6\"' \\\r\n    --shm-size=10.24gb \\\r\n    -p $port:$port \\\r\n        -e NCCL_IGNORE_DISABLED_P2P=1 \\\r\n    -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\r\n    -e VLLM_NCCL_SO_PATH=/usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2 \\\r\n    -v /etc/passwd:/etc/passwd:ro \\\r\n    -v /etc/group:/etc/group:ro \\\r\n    -u `id -u`:`id -g` \\\r\n    -v \"${HOME}\"/.cache:$HOME/.cache/ -v \"${HOME}\"/.config:$HOME/.config/   -v \"${HOME}\"/.triton:$HOME/.triton/  \\\r\n    --network host \\\r\n    --name danube2chat \\\r\n    vllm/vllm-openai:latest \\\r\n        --port=$port \\\r\n        --host=0.0.0.0 \\\r\n        --model=h2oai/h2o-danube2-1.8b-chat \\\r\n        --seed 1234 \\\r\n        --trust-remote-code \\\r\n        --tensor-parallel-size=1 \\\r\n        --max-num-batched-tokens 163840 \\\r\n      \t--max-model-len=$tokens \\\r\n        --download-dir=/home/ubuntu/.cache/huggingface/hub &>> logs.vllm_server.danube2chat.txt\r\n```"
      },
      {
        "user": "jaysunl",
        "body": "Sorry for all the questions but I seem to not be able to get any of those working. I'm also getting some errors with the tool not being able to ingest any documents now\r\n\r\n```\r\n Download simple failed: Invalid URL '/tmp/gradio/361098a8271e9b9cbd110b0a3bc16f886470e713/16ffc_EDA_tools_certificate_report_DBUS0203020_1.pdf': No scheme supplied. Perhaps you meant https:///tmp/gradio/361098a8271e9b9cbd110b0a3bc16f886470e713/16ffc_EDA_tools_certificate_report_DBUS0203020_1.pdf?, trying other means\r\nh2ogpt_ctx | INFO:     172.31.35.37:0 - \"GET /queue/data?session_hash=oi9qxivpotp HTTP/1.1\" 200 OK\r\nh2ogpt_ctx | ERROR:langchain_community.document_loaders.url:Error fetching or processing http:///tmp/gradio/361098a8271e9b9cbd110b0a3bc16f886470e713/16ffc_EDA_tools_certificate_report_DBUS0203020_1.pdf, exception: Invalid URL 'http:///tmp/gradio/361098a8271e9b9cbd110b0a3bc16f886470e713/16ffc_EDA_tools_certificate_report_DBUS0203020_1.pdf': No host supplied\r\nh2ogpt_ctx | Failed to ingest /tmp/gradio/361098a8271e9b9cbd110b0a3bc16f886470e713/16ffc_EDA_tools_certificate_report_DBUS0203020_1.pdf due to Traceback (most recent call last):\r\nh2ogpt_ctx |   File \"/workspace/src/gpt_langchain.py\", line 5274, in path_to_doc1\r\nh2ogpt_ctx |     res = file_to_doc(file,\r\nh2ogpt_ctx |   File \"/workspace/src/gpt_langchain.py\", line 4531, in file_to_doc\r\nh2ogpt_ctx |     docs1a = asyncio.run(PlaywrightURLLoader(urls=final_urls).aload())\r\nh2ogpt_ctx |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/asyncio/runners.py\", line 44, in run\r\nh2ogpt_ctx |     return loop.run_until_complete(main)\r\nh2ogpt_ctx |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\nh2ogpt_ctx |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/document_loaders/url_playwright.py\", line 199, in aload\r\nh2ogpt_ctx |     return [doc async for doc in self.alazy_load()]\r\nh2ogpt_ctx |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/document_loaders/url_playwright.py\", line 199, in <listcomp>\r\nh2ogpt_ctx |     return [doc async for doc in self.alazy_load()]\r\nh2ogpt_ctx |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/document_loaders/url_playwright.py\", line 211, in alazy_load\r\nh2ogpt_ctx |     browser = await p.chromium.launch(headless=self.headless, proxy=self.proxy)\r\nh2ogpt_ctx |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/playwright/async_api/_generated.py\", line 13957, in launch\r\nh2ogpt_ctx |     await self._impl_obj.launch(\r\nh2ogpt_ctx |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/playwright/_impl/_browser_type.py\", line 94, in launch\r\nh2ogpt_ctx |     Browser, from_channel(await self._channel.send(\"launch\", params))\r\nh2ogpt_ctx |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/playwright/_impl/_connection.py\", line 59, in send\r\nh2ogpt_ctx |     return await self._connection.wrap_api_call(\r\nh2ogpt_ctx |   File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/playwright/_impl/_connection.py\", line 514, in wrap_api_call\r\nh2ogpt_ctx |     raise rewrite_error(error, f\"{parsed_st['apiName']}: {error}\") from None\r\nh2ogpt_ctx | playwright._impl._errors.Error: BrowserType.launch: Executable doesn't exist at /workspace/.cache/ms-playwright/chromium-1117/chrome-linux/chrome\r\nh2ogpt_ctx | ╔════════════════════════════════════════════════════════════╗\r\nh2ogpt_ctx | ║ Looks like Playwright was just installed or updated.       ║\r\nh2ogpt_ctx | ║ Please run the following command to download new browsers: ║\r\nh2ogpt_ctx | ║                                                            ║\r\nh2ogpt_ctx | ║     playwright install                                     ║\r\nh2ogpt_ctx | ║                                                            ║\r\nh2ogpt_ctx | ║ <3 Playwright Team                                         ║\r\nh2ogpt_ctx | ╚════════════════════════════════════════════════════════════╝\r\n```\r\n\r\nCan I get a step by step example from you if possible that uses Docker setup with document ingestion? I followed the ones on the README docs given but everytime I am met with an error. Thank you."
      },
      {
        "user": "pseudotensor",
        "body": "Hi, can you give me your startup command and an example URL you are trying to provide (or one that fails in same way)?"
      },
      {
        "user": "pseudotensor",
        "body": "That playwright line should already have been done in docker or local install here:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/5637fe44588d31912837eb36fc14ee571ce055e6/docs/linux_install.sh#L63\r\n\r\n\r\nBut, can you try in expert settings disabling playwright and only forcing unstructured to be used?\r\n\r\ni.e. related code:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/5b48852cda43a9b5fdafc6720c360faea7e721bb/src/gpt_langchain.py#L4526-L4542"
      },
      {
        "user": "pseudotensor",
        "body": "It's also possible I broke something very recently w.r.t. name handling.  I'll check soon."
      },
      {
        "user": "pseudotensor",
        "body": "A few things I just tried worked:\r\n\r\nIn UI for \"Ask or Ingest\" I put in these urls and they all worked, after clicking ingest:\r\n\r\nwww.cnn.com\r\nhttps://cdn.openai.com/papers/whisper.pdf\r\n\r\nAre are you not giving a URL, but trying to upload a PDF?\r\n\r\nPlease provide some details about what you are doing.\r\n"
      },
      {
        "user": "jaysunl",
        "body": "I am trying to upload PDFs. Actually, I just resolved the document loading issues. But I'm having a few other issues related to the ones I've been getting. First making the database with Docker command. I am running the following \r\n\r\n```\r\nmkdir -p ~/.cache\r\nmkdir -p ~/save\r\nmkdir -p ~/user_path\r\nmkdir -p ~/db_dir_UserData\r\ndocker run \\\r\n       --gpus all \\\r\n       --runtime=nvidia \\\r\n       --shm-size=2g \\\r\n       --rm --init \\\r\n       --network host \\\r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u `id -u`:`id -g` \\\r\n       -v \"${HOME}\"/.cache:/workspace/.cache \\\r\n       -v \"${HOME}\"/save:/workspace/save \\\r\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\r\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\r\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.0 /workspace/src/make_db.py --verbose\r\n```\r\n\r\nwhich is straight from the documentation and I made sure the directories I've made were writable. I put my documents under ~/user_path (181 PDFs). It manages to ingest but this is all that prints\r\n```\r\n100%|██████████| 181/181 [02:25<00:00,  1.24it/s]\r\nExceptions: 0/17294 []\r\nLoading and creating db\r\n```\r\nBut nothing else happens after that. So I'm assuming no db has been made since the command did not terminate.\r\n\r\nThe second issue I'm getting is the permission issue still. Using this one from the documentation about running the tool using a generated db (after running src/make_db.py)\r\n```\r\nmkdir -p ~/.cache\r\nmkdir -p ~/save\r\nmkdir -p ~/user_path\r\nmkdir -p ~/db_dir_UserData\r\nmkdir -p ~/users\r\nmkdir -p ~/db_nonusers\r\nmkdir -p ~/llamacpp_path\r\ndocker run \\\r\n       --gpus '\"device=0\"' \\\r\n       --runtime=nvidia \\\r\n       --shm-size=2g \\\r\n       -p 7860:7860 \\\r\n       --rm --init \\\r\n       --network host \\\r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u `id -u`:`id -g` \\\r\n       -v \"${HOME}\"/.cache:/workspace/.cache \\\r\n       -v \"${HOME}\"/save:/workspace/save \\\r\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\r\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\r\n       -v \"${HOME}\"/users:/workspace/users \\\r\n       -v \"${HOME}\"/db_nonusers:/workspace/db_nonusers \\\r\n       -v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path \\\r\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.0 /workspace/generate.py \\\r\n          --base_model=h2oai/h2ogpt-4096-llama2-7b-chat \\\r\n          --use_safetensors=True \\\r\n          --prompt_type=llama2 \\\r\n          --save_dir='/workspace/save/' \\\r\n          --use_gpu_id=False \\\r\n          --score_model=None \\\r\n          --max_max_new_tokens=2048 \\\r\n          --max_new_tokens=1024 \\\r\n          --langchain_mode=LLM\r\n```\r\nI get the following output\r\n```\r\nWARNING: Published ports are discarded when using host network mode\r\nUsing Model h2oai/h2ogpt-4096-llama2-7b-chat\r\nfatal: not a git repository (or any of the parent directories): .git\r\n```\r\nbut nothing else after. I will see if I can reproduce the permission error because I saw it quite a few times the past few days."
      },
      {
        "user": "pseudotensor",
        "body": "After make_db.py does the ingestion of PDFs, it needs to embed the data into the database.  I assume you have GPUs, and so then it would use a GPU to do the embedding.  The speed depends upon your GPUs, the embedding model, etc.  This is what is meant by the line `Loading and creating db`.\r\n\r\nYou should see intense activity on the GPU used for embedding."
      },
      {
        "user": "pseudotensor",
        "body": "These:\r\n```\r\nWARNING: Published ports are discarded when using host network mode\r\nfatal: not a git repository (or any of the parent directories): .git\r\n```\r\nare ignorable.\r\n\r\nBut if it's hanging at:\r\n```\r\nUsing Model h2oai/h2ogpt-4096-llama2-7b-chat\r\n```\r\nsome issue with the model.  It could be downloading the model (it should show that though) or a network issue in talking to HF, etc."
      },
      {
        "user": "pseudotensor",
        "body": "I ran your command on about 290 PDFs in the ~/user_path.\r\n\r\nDuring ingestion, you'll see stuff like this, using all cores efficiently since pymupdf is default PDF parser and only backup is used if it totally fails and pymupdf uses only CPU.\r\n\r\nAfter it becomes less busy, it means it's working on the last remaining files that are left in some batch, so not as parallel if the PDFs are kinda different from each other.\r\n\r\nIf some PDF fails with pymupdf, it will go to unstructured loader using tesserract, which can be very slow.  You can run with `--use_unstructured_pdf=False --enable_pdf_ocr=False` to avoid that.\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/dc2c899d-6e97-4880-af60-42cbafa604d9)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Once the GPU kicks in it'll look like the below.\r\n\r\nIt takes a while to use instructor-large that is default for GPU case.  It would be faster if one used a smaller embedding model, like `BAAI/bge-small-en-v1.5`, e.g.:\r\n\r\n```\r\nmkdir -p ~/.cache\r\nmkdir -p ~/save\r\nmkdir -p ~/user_path\r\nmkdir -p ~/db_dir_UserData\r\ndocker run \\\r\n       --gpus all \\\r\n       --runtime=nvidia \\\r\n       --shm-size=2g \\\r\n       --rm --init \\\r\n       --network host \\\r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u `id -u`:`id -g` \\\r\n       -v \"${HOME}\"/.cache:/workspace/.cache \\\r\n       -v \"${HOME}\"/save:/workspace/save \\\r\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\r\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\r\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 /workspace/src/make_db.py --verbose --use_unstructured_pdf=False --enable_pdf_ocr=False --hf_embedding_model=BAAI/bge-small-en-v1.5\r\n```\r\nwhen you run ensure to use generate.py with ` --cut_distance=10000`.\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/1ab265bf-6af3-4ee8-aba2-d081bc7164b8)\r\n"
      },
      {
        "user": "jaysunl",
        "body": "Is there a rough time estimate I should estimate for when the command should complete after seeing Loading and updating db\r\n? What should it say after that?"
      },
      {
        "user": "jaysunl",
        "body": "I resolved my issues. Thank you so much for your help."
      }
    ]
  },
  {
    "issue_number": 1670,
    "title": "Running H2ogpt with Ollama inference Server",
    "author": "rohitnanda1443",
    "state": "closed",
    "created_at": "2024-06-06T12:38:40Z",
    "updated_at": "2024-06-27T22:40:24Z",
    "labels": [],
    "body": "Hi All,\r\n\r\nI am trying to run an inference server on Ollama using the below script:\r\n\r\nollama run mistral:v0.3\r\n\r\nThen running h2o-gpt using the below script:\r\n\r\npython generate.py --guest_name='' --base_model=mistralai/Mistral-7B-Instruct-v0.3   --max_seq_len=8094  --enable_tts=False --enable_stt=False --enable_transcriptions=False --use_gpu_id=False  --inference_server=vllm_chat:http://localhost:11434/v1/ --prompt_type=openai_chat &\r\n\r\nIssue I am facing is that model not found in the H2o UI. What is the correct model name for the Ollama mistral-0.3 model to be passed in the CLI for H20? \r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Did you follow this?\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/FAQ.md#running-ollama-vs-h2ogpt-as-inference-server"
      },
      {
        "user": "pseudotensor",
        "body": "As in the instructions, the \"base_model\" has to be the same name as ollama was set to.\r\n\r\ni.e.\r\n```\r\npython generate.py --guest_name='' --base_model=mistral:v0.3 --max_seq_len=8094 --enable_tts=False --enable_stt=False --enable_transcriptions=False --use_gpu_id=False --inference_server=vllm_chat:http://localhost:11434/v1/ --prompt_type=openai_chat\r\n```\r\nand ignore errors about not finding the tokenizer etc.\r\n\r\nFor more accurate tokenization specify the tokenizer and hf token (because mistralai is gated on HF):\r\n```\r\npython generate.py --guest_name='' --base_model=mistral:v0.3 --tokenizer_base_model=mistralai/Mistral-7B-Instruct-v0.3 --max_seq_len=8094 --enable_tts=False --enable_stt=False --enable_transcriptions=False --use_gpu_id=False --inference_server=vllm_chat:http://localhost:11434/v1/ --prompt_type=openai_chat --use_auth_token=<token>\r\n```\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/792691e0-8bb5-4a72-b1f0-72a50b7f5bed)\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 1668,
    "title": "Option to place relavent documents chunks in system prompt instead of user prompt",
    "author": "ChiNoel-osu",
    "state": "open",
    "created_at": "2024-06-06T03:23:32Z",
    "updated_at": "2024-06-27T22:08:12Z",
    "labels": [
      "type/feature"
    ],
    "body": "I found out that for models that can handle system prompt, placing the document chunks in the system prompt would give better results. It also makes chat history work well.\r\nFor example: (llama3 format)\r\n```\r\n<|start_header_id|>system<|end_header_id|>\r\nYou are a helpful artificial intelligence assistant.\r\n<|eot_id|>\r\n<|start_header_id|>user<|end_header_id|>\r\nHello!\r\n<|eot_id|>\r\n<|start_header_id|>assistant<|end_header_id|>\r\nHello, how can I help you today?\r\n<|eot_id|>\r\n<|start_header_id|>user<|end_header_id|>\r\nRelevent context is below:\r\n\"\"\"\r\n1+1=3\r\n\"\"\"\r\nPrepare a concise answer to the question:\r\nWhat is 1+1?\r\n<|eot_id|>\r\n<|start_header_id|>assistant<|end_header_id|>\r\n......\r\n```\r\nwill have the option to change into this:\r\n```\r\n<|start_header_id|>system<|end_header_id|>\r\nYou are a helpful artificial intelligence assistant.\r\nRelevent context is below:\r\n\"\"\"\r\n1+1=3\r\n\"\"\"\r\nYou job is to prepare a concise answer to user's question.\r\n<|eot_id|>\r\n<|start_header_id|>user<|end_header_id|>\r\nHello!\r\n<|eot_id|>\r\n<|start_header_id|>assistant<|end_header_id|>\r\nHello, how can I help you today?\r\n<|eot_id|>\r\n<|start_header_id|>user<|end_header_id|>\r\nWhat is 1+1?\r\n<|eot_id|>\r\n<|start_header_id|>assistant<|end_header_id|>\r\n......\r\n```\r\n\r\nI actually don't know how prompt template works in h2oGPT, I'm having a hard time getting the system prompt thing working as I don't know how it interacts with the custom prompt template. Right now I have to resort to `--context` to handle my system prompt:\r\n```\r\n --prompt_type=custom --prompt_dict=\"{'PreInstruct': '<|start_header_id|>user<|end_header_id|>\\n', 'PreInput': None, 'PreResponse': '<|start_header_id|>assistant<|end_header_id|>\\n', 'terminate_response': ['<|eot_id|>'], 'chat_sep': '\\n<|eot_id|>\\n', 'chat_turn_sep': '\\n<|eot_id|>\\n', 'humanstr': '<|start_header_id|>user<|end_header_id|>\\n', 'botstr': '<|start_header_id|>assistant<|end_header_id|>\\n', 'generates_leading_space': False, 'can_handle_system_prompt': True}\" --context=\"<|start_header_id|>system<|end_header_id|>\r\nYou are a helpful artificial intelligence assistant.\r\n<|eot_id|>\r\n\"\r\n```\r\n\r\nAny help would be appreciated.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi sorry for long wait.  It's a good idea as an option, but within langchain framework there is no easy automatic way to do this.  For inference classes I have overridden, I could do it, but for rest it's not some option that I can control."
      }
    ]
  },
  {
    "issue_number": 1669,
    "title": "AutoGPT issue running on Local LLM ",
    "author": "rohitnanda1443",
    "state": "closed",
    "created_at": "2024-06-06T10:14:48Z",
    "updated_at": "2024-06-27T08:57:18Z",
    "labels": [],
    "body": "Hi, trying to use the AutoGPT agent. The config is as under:\r\n\r\n1. H2o-GPT running locally \r\n2. The LLM model is running locally / via an vLLM inference server.\r\n3. Correct parameters being passsed to h20-gpt: \r\n\r\npython generate.py --guest_name='' --base_model=mistralai/Mistral-7B-Instruct-v0.2   --max_seq_len=8094  --enable_tts=False --enable_stt=False --enable_transcriptions=False --use_gpu_id=False  --inference_server=\"vllm:0.0.0.0:5002\" &\r\n\r\n\r\n\r\n**Issue**: AutoGPT unable to complete tasks as it goes into an enless loop. The reason is that the response it is getting from the local llm is not is the Json format it expected and hence gives an error and restarts the process. \r\n\r\nHow does one resolve this issue (get the respone in a correct Json format). \r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I agree, it used to work.  Langchain must have changed some things to break it.\r\n\r\nFor me it just fails with handling the first step."
      },
      {
        "user": "rohitnanda1443",
        "body": "Yes that is correct and the reason it fails in the first step for me also is due to the response not received in the Json format expected (which is OpenAI format)"
      },
      {
        "user": "pseudotensor",
        "body": "It probably works now if using vllm, but else only with openai.\r\n\r\nd24272f9121eb4cfb5b0c89e3e1cdade49667796"
      }
    ]
  },
  {
    "issue_number": 1714,
    "title": ".clone() fix both parent and clone for session hash",
    "author": "pseudotensor",
    "state": "closed",
    "created_at": "2024-06-27T01:08:47Z",
    "updated_at": "2024-06-27T02:53:32Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "pseudotensor",
        "body": "with threading.Lock():\r\n\r\nwhen clone"
      },
      {
        "user": "pseudotensor",
        "body": "client = self.clone()"
      }
    ]
  },
  {
    "issue_number": 1688,
    "title": "ModuleNotFoundError: No module named 'tenacity.asyncio'",
    "author": "machinelearning2014",
    "state": "closed",
    "created_at": "2024-06-17T08:53:03Z",
    "updated_at": "2024-06-19T11:11:29Z",
    "labels": [],
    "body": "I followed all of the installation and running below\r\n\r\nHF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 python generate.py --base_model=mistral-7b-instruct-v0.2.Q2_K.gguf --prompt_type=mistral --max_seq_len=512 --cli=True\r\n\r\nget an error:\r\n\r\nFile \"/home/letro/.conda/envs/h2ogpt_env/lib/python3.10/site-packages/tenacity/__init__.py\", line 653, in <module>\r\n    from tenacity.asyncio import AsyncRetrying  # noqa:E402,I100\r\nModuleNotFoundError: No module named 'tenacity.asyncio'\r\n\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "What does:\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ pip freeze | grep tenacity\r\ntenacity==8.3.0\r\n```\r\n\r\nshow for you?\r\n\r\nAlso what is the more complete traceback?\r\n\r\nI presume you have an incomplete installation."
      },
      {
        "user": "pseudotensor",
        "body": "Seeing same thing in docker image:\r\n```\r\nh2ogpt-1  | ModuleNotFoundError: No module named 'tenacity.asyncio'\r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "https://github.com/langchain-ai/langchain/issues/22972"
      },
      {
        "user": "pseudotensor",
        "body": "```\r\npip install tenacity==8.3.0 --upgrade\r\n```"
      },
      {
        "user": "machinelearning2014",
        "body": "Thanks for looking into, seems the workaround is to install tenacity==8.3.0 , the bug is in tenacity>=8.4.0\r\nhttps://github.com/langchain-ai/langchain/issues/22972#issuecomment-2172865393"
      }
    ]
  },
  {
    "issue_number": 1690,
    "title": "Traceback: 'NoneType' object is not iterable (gen.py, line 2435)",
    "author": "bibo7086",
    "state": "closed",
    "created_at": "2024-06-17T18:59:39Z",
    "updated_at": "2024-06-17T19:07:56Z",
    "labels": [],
    "body": "The script `generate.py` encountered an error during execution. The error originates from line 2435 within the file `gen.py`. Here's the exact error message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/path/to/development/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/path/to/development/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/path/to/development/h2ogpt/src/utils.py\", line 75, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/path/to/miniconda3/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/path/to/miniconda3/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/path/to/miniconda3/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/path/to/development/h2ogpt/src/gen.py\", line 2435, in main\r\n    all_visible_models = [x.get('visible_models') or x.get('base_model') for x in model_lock]\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\n\r\n**Steps to Reproduce:**\r\n\r\n1. Run the following command:\r\n\r\n```\r\npython generate.py --base_model=TheBloke/Mistral-7B-Instruct-v0.2-GGUF --prompt_type=mistral --max_seq_len=4096\r\n```\r\n* **Environment:**\r\n    * Operating System: Ubuntu 22.04.4 LTS\r\n\r\n\r\n**Note:**\r\nRunning without the base model option works fine, i.e.\r\n```\r\npython generate.py --prompt_type=mistral --model_path_llama=https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf --share=False\r\n```\r\n\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, it was fixed"
      }
    ]
  },
  {
    "issue_number": 1602,
    "title": "Run docker image on any machine which haven't internet connection",
    "author": "glenbhermon",
    "state": "closed",
    "created_at": "2024-05-07T06:35:47Z",
    "updated_at": "2024-06-17T17:35:22Z",
    "labels": [],
    "body": "I built the docker image from the repo and running it using the following commands :\r\n```\r\nmkdir -p ~/.cache\r\nmkdir -p ~/save\r\nmkdir -p ~/user_path\r\nmkdir -p ~/db_dir_UserData\r\nmkdir -p ~/users\r\nmkdir -p ~/db_nonusers\r\nmkdir -p ~/llamacpp_path\r\nexport GRADIO_SERVER_PORT=7860\r\nexport OPENAI_SERVER_PORT=5000                             \r\ndocker run \\\r\n       --gpus '\"device=0\"' \\\r\n       --runtime=nvidia \\\r\n       --shm-size=2g \\\r\n       -p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT \\\r\n       -p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT \\\r\n       --rm --init \\                               \r\n       --network host \\                               \r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u `id -u`:`id -g` \\\r\n       -v \"${HOME}\"/.cache:/workspace/.cache \\\r\n       -v \"${HOME}\"/save:/workspace/save \\\r\n       -v \"${HOME}\"/user_path:/workspace/user_path \\\r\n       -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\r\n       -v \"${HOME}\"/users:/workspace/users \\\r\n       -v \"${HOME}\"/db_nonusers:/workspace/db_nonusers \\\r\n       -v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path \\\r\n       -e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT \\\r\n       h2ogpt_1 /workspace/generate.py \\\r\n          --base_model=mistralai/Mistral-7B-Instruct-v0.2  \\      \r\n          --use_safetensors=True \\\r\n          --prompt_type=mistral \\\r\n          --save_dir='/workspace/save/' \\\r\n          --use_gpu_id=False \\\r\n          --user_path=/workspace/user_path \\\r\n          --langchain_mode=\"LLM\" \\\r\n          --langchain_modes=\"['UserData', 'LLM', 'MyData']\" \\                              \r\n          --score_model=None \\                                           \r\n          --max_max_new_tokens=2048 \\\r\n          --max_new_tokens=1024 \\\r\n          --openai_port=$OPENAI_SERVER_PORT\r\n```\r\nWhen the container run first time it downloads models, ocr model and some configs from the internet then run successfully. But I want to run this docker image on another machine which haven't internet connection. How can i do that?\r\nCan you provide the steps for creating the docker image that have all the necessary configs, ocr model and required language model inside the images that you can run on any machine without the internet connection? And also share the list of commands for running this new image which have all the required things.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, can you review this first? Then see what else you need.  Thanks.\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_offline.md"
      },
      {
        "user": "glenbhermon",
        "body": "I gone through the link you mention. It has all the necessary steps about offline but without docker. I'm asking about to make only one image that have one base model, config, parser etc in it that can run h2ogpt offline successfully."
      },
      {
        "user": "glenbhermon",
        "body": "The command i ran after creating the docker image:\r\n`sudo docker run        --gpus '\"device=0\"'        --runtime=nvidia        --shm-size=2g        -p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT        -p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT        --rm --init        --network host        -v /etc/passwd:/etc/passwd:ro        -v /etc/group:/etc/group:ro        -u `id -u`:`id -g`        -v \"${HOME}\"/.cache:/workspace/.cache        -v \"${HOME}\"/save:/workspace/save        -v \"${HOME}\"/user_path:/workspace/user_path        -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData        -v \"${HOME}\"/users:/workspace/users        -v \"${HOME}\"/db_nonusers:/workspace/db_nonusers        -v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path        -e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT        h2ogpt_3 /workspace/generate.py           --base_model=mistralai/Mistral-7B-Instruct-v0.2           --use_safetensors=True           --prompt_type=mistral           --save_dir='/workspace/save/'           --use_gpu_id=False           --user_path=/workspace/user_path           --langchain_mode=\"LLM\"           --langchain_modes=\"['UserData', 'LLM', 'MyData']\"           --score_model=None           --max_max_new_tokens=4096           --max_new_tokens=2048           --openai_port=$OPENAI_SERVER_PORT \\ `\r\n\r\nAnd after running it trying to make connection with the internet for downloading the embedding model and thrown the following error:\r\n\r\n```\r\nWARNING: Published ports are discarded when using host network mode\r\nUsing Model mistralai/mistral-7b-instruct-v0.2\r\nfatal: not a git repository (or any of the parent directories): .git\r\ngit_hash.txt failed to be found: [Errno 2] No such file or directory: 'git_hash.txt'\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connection.py\", line 198, in _new_conn\r\n    sock = connection.create_connection(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/util/connection.py\", line 60, in create_connection\r\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/socket.py\", line 955, in getaddrinfo\r\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\r\nsocket.gaierror: [Errno -3] Temporary failure in name resolution\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 793, in urlopen\r\n    response = self._make_request(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 491, in _make_request\r\n    raise new_e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 467, in _make_request\r\n    self._validate_conn(conn)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1099, in _validate_conn\r\n    conn.connect()\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connection.py\", line 616, in connect\r\n    self.sock = sock = self._new_conn()\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connection.py\", line 205, in _new_conn\r\n    raise NameResolutionError(self.host, self, e) from e\r\nurllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x71188cb4d780>: Failed to resolve '[huggingface.co](http://huggingface.co/)' ([Errno -3] Temporary failure in name resolution)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\r\n    resp = conn.urlopen(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 847, in urlopen\r\n    retries = retries.increment(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/util/retry.py\", line 515, in increment\r\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='[huggingface.co](http://huggingface.co/)', port=443): Max retries exceeded with url: /api/models/hkunlp/instructor-large (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x71188cb4d780>: Failed to resolve '[huggingface.co](http://huggingface.co/)' ([Errno -3] Temporary failure in name resolution)\"))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/workspace/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/workspace/src/utils.py\", line 72, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/workspace/src/gen.py\", line 1882, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"/workspace/src/gpt_langchain.py\", line 528, in get_embedding\r\n    embedding = HuggingFaceInstructEmbeddings(model_name=hf_embedding_model,\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 158, in __init__\r\n    self.client = INSTRUCTOR(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 87, in __init__\r\n    snapshot_download(model_name_or_path,\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/util.py\", line 442, in snapshot_download\r\n    model_info = _api.model_info(repo_id=repo_id, revision=revision, token=token)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2300, in model_info\r\n    r = get_session().get(path, headers=headers, timeout=timeout, params=params)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 602, in get\r\n    return self.request(\"GET\", url, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 66, in send\r\n    return super().send(request, *args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/adapters.py\", line 519, in send\r\n    raise ConnectionError(e, request=request)\r\nrequests.exceptions.ConnectionError: (MaxRetryError('HTTPSConnectionPool(host=\\'[huggingface.co](http://huggingface.co/)\\', port=443): Max retries exceeded with url: /api/models/hkunlp/instructor-large (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x71188cb4d780>: Failed to resolve \\'[huggingface.co](http://huggingface.co/)\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 395f6b0b-82cc-4691-a4ce-1473ce23ebab)')\r\n```\r\nI also change the embedding model by `--hf_embedding_model=sentence-transformers/all-MiniLM-L6-v2` and it already have in my .cache folder inside both the container and local system but then still trying to connect and thrown the error.\r\n![Screenshot from 2024-05-08 17-22-35](https://github.com/h2oai/h2ogpt/assets/38610046/c0ae3cd1-3d7d-4eff-9894-054ff125e7fa)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Did you follow these kinds of instructions?\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/93ed01b5d704f668094ede6dffcd6d64f81d6aee/docs/README_offline.md\r\n\r\ni.e. this env should be set:\r\n```\r\nTRANSFORMERS_OFFLINE=1\r\n```\r\n\r\nand useful if pass to h2oGPT: ` --gradio_offline_level=2 --share=False`"
      },
      {
        "user": "glenbhermon",
        "body": "I have been followed all the steps that you mention but it didn't work. This is my docker command with including `TRANSFORMERS_OFFLINE=1\r\n`\r\n and  `--gradio_offline_level=2 --share=False` :\r\n \r\n```\r\nexport HF_DATASETS_OFFLINE=1\r\nexport TRANSFORMERS_OFFLINE=1\r\nexport GRADIO_SERVER_PORT=7860\r\n$export OPENAI_SERVER_PORT=5000\r\nsudo docker run --gpus all --runtime=nvidia --shm-size=2g -p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT -p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT --rm --init --network host -v /etc/passwd:/etc/passwd:ro -v /etc/group:/etc/group:ro -u `id -u`:`id -g` -v \"${HOME}\"/.cache:/workspace/.cache -v \"${HOME}\"/save:/workspace/save -v \"${HOME}\"/user_path:/workspace/user_path -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData -v \"${HOME}\"/users:/workspace/users -v \"${HOME}\"/db_nonusers:/workspace/db_nonusers -v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path -e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT h2ogpt_1 /workspace/generate.py --base_model=mistralai/Mistral-7B-Instruct-v0.2 --use_safetensors=True --prompt_type=mistral --save_dir='/workspace/save/' --use_gpu_id=False --user_path=/workspace/user_path --langchain_mode=\"LLM\" --langchain_modes=\"['UserData', 'MyData', 'LLM']\" --score_model=None --max_max_new_tokens=2048 --max_new_tokens=1024  --visible_visible_models=False openai_port=$OPENAI_SERVER_PORT\r\n```\r\n\r\nAfter running, it thrown the same error. I did also change the embedding model all-MiniLM-L6-v2 but it didn't work also.\r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "You need to pass the envs as a docker env like you have for the gradio port.\r\n```\r\n-e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT \\\r\n-e TRANSFORMERS_OFFLINE=$TRANSFORMERS_OFFLINE \\\r\n```"
      },
      {
        "user": "glenbhermon",
        "body": "Modified command:\r\n`sudo docker run --gpus all --runtime=nvidia --shm-size=2g -e TRANSFORMERS_OFFLINE=$TRANSFORMERS_OFFLINE -e HF_DATASETS_OFFLINE=$HF_DATASETS_OFFLINE -p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT -p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT --rm --init --network host -v /etc/passwd:/etc/passwd:ro -v /etc/group:/etc/group:ro -u `id -u`:`id -g` -v \"${HOME}\"/.cache:/workspace/.cache -v \"${HOME}\"/save:/workspace/save -v \"${HOME}\"/user_path:/workspace/user_path -v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData -v \"${HOME}\"/users:/workspace/users -v \"${HOME}\"/db_nonusers:/workspace/db_nonusers -v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path -e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT narad_3 /workspace/generate.py --base_model=mistralai/Mistral-7B-Instruct-v0.2 --use_safetensors=True --prompt_type=mistral --save_dir='/workspace/save/' --use_gpu_id=False --user_path=/workspace/user_path --langchain_mode=\"LLM\" --langchain_modes=\"['UserData', 'MyData', 'LLM']\" --score_model=None --max_max_new_tokens=2048 --max_new_tokens=1024  --visible_visible_models=False openai_port=$OPENAI_SERVER_PORT`\r\n\r\nThe error i got this time:\r\n```\r\nWARNING: Published ports are discarded when using host network mode\r\nUsing Model mistralai/mistral-7b-instruct-v0.2\r\nfatal: not a git repository (or any of the parent directories): .git\r\ngit_hash.txt failed to be found: [Errno 2] No such file or directory: 'git_hash.txt'\r\nTraceback (most recent call last):\r\n  File \"/workspace/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/workspace/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/workspace/src/utils.py\", line 72, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/workspace/src/gen.py\", line 1882, in main\r\n    model=get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model,\r\n  File \"/workspace/src/gpt_langchain.py\", line 528, in get_embedding\r\n    embedding = HuggingFaceInstructEmbeddings(model_name=hf_embedding_model,\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py\", line 158, in __init__\r\n    self.client = INSTRUCTOR(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 87, in __init__\r\n    snapshot_download(model_name_or_path,\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/sentence_transformers/util.py\", line 442, in snapshot_download\r\n    model_info = _api.model_info(repo_id=repo_id, revision=revision, token=token)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2300, in model_info\r\n    r = get_session().get(path, headers=headers, timeout=timeout, params=params)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 602, in get\r\n    return self.request(\"GET\", url, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 77, in send\r\n    raise OfflineModeIsEnabled(\r\nhuggingface_hub.errors.OfflineModeIsEnabled: Cannot reach https://huggingface.co/api/models/hkunlp/instructor-large: offline mode is enabled. To disable it, please unset the `HF_HUB_OFFLINE` environment variable.\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "Good.  Now that things function, the issue is that if you are offline, you need the models in place in the cache.  Because you do not, it fails.  So you need to follow the offline docs for one of the ways to get those models into the cache locations.  This can range from running online first and using the product, to running the smart download way."
      },
      {
        "user": "glenbhermon",
        "body": "1. I already have **all-MiniLM-L6-v2** in my container's  .cache folder and when try pass the `--hf_embedding_model=sentence-transformers--all-MiniLM-L6-v2` in the command then it also shown the same error just like before. Now it trying to download the all-MiniLM-L6-v2 model instead of hkunlp/instructor-large.\r\n2. My machine is connected with captive network so that i can't connect with internet."
      },
      {
        "user": "pseudotensor",
        "body": "It's not related to h2oGPT at this point.  Just do in python:\r\n\r\n```\r\nfrom InstructorEmbedding import INSTRUCTOR\r\nmodel = INSTRUCTOR('hkunlp/instructor-large')\r\n```\r\nThat is what the h2oGPT -> langchain code is doing.  I presume this fails same way, and somehow you have to get that model in the right place.\r\n\r\nIf you place it somewhere manually because you have no internet, that probably won't be right.\r\n\r\nIt seems you can set `SENTENCE_TRANSFORMERS_HOME` env to specify the location.  Else it should be in the torch cache etc.  See the sentence_transformer package.\r\ne.g. it would be located here by default if nothing is set:\r\n```\r\nfrom torch.hub import _get_torch_home\r\ntorch_cache_home = _get_torch_home()\r\nprint(torch_cache_home)\r\n```\r\ne.g. `/home/jon/.cache/torch`\r\n\r\nwhich looks like:\r\n```\r\nl(h2ogpt) jon@pseudotensor:~/h2ogpt$ ls -alrt /home/jon/.cache/torch\r\ntotal 24\r\ndrwx------  6 jon docker 4096 Feb 24 18:17 pyannote/\r\ndrwx------  3 jon jon    4096 Feb 26 08:07 hub/\r\ndrwxrwxr-x  6 jon jon    4096 Feb 26 08:07 ./\r\ndrwxrwxr-x  2 jon jon    4096 Mar 21 13:09 kernels/\r\ndrwxrwxr-x  6 jon jon    4096 Apr 30 15:35 sentence_transformers/\r\ndrwx------ 47 jon jon    4096 May  9 23:40 ../\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ \r\n```"
      },
      {
        "user": "glenbhermon",
        "body": "Yes! It already set in the right palace. \r\n![Screenshot from 2024-05-10 15-01-37](https://github.com/h2oai/h2ogpt/assets/38610046/575b5456-f071-48ec-81a8-97688295f9a4)\r\n\r\n`~/.cache/torch/sentence_transformers$ ls -alrt\r\ntotal 12\r\ndrwxrwxr-x 4 sourav sourav 4096 May  6 11:39 ..\r\ndrwxr-xr-x 4 sourav sourav 4096 May  6 11:41 hkunlp_instructor-large\r\ndrwxr-xr-x 3 sourav sourav 4096 May  6 11:41 .\r\n`"
      },
      {
        "user": "glenbhermon",
        "body": "@pseudotensor please guide me to implementing this."
      },
      {
        "user": "pseudotensor",
        "body": "Hi @glenbhermon I'm really not sure what is wrong if you have the files in the expected place.\r\n\r\n\r\nhttps://github.com/UKPLab/sentence-transformers/issues/1725\r\nhttps://github.com/UKPLab/sentence-transformers/pull/2345\r\n\r\n\r\nSeems should be no particular issue."
      },
      {
        "user": "pseudotensor",
        "body": "You have mistakes in your run line, like missing -- before `openai_port` and missing ` around the id for user group.  Also, that model doesn't use safe tensors, so need to remove that line.  \r\n\r\nThis works for me:\r\n\r\n```\r\n# 1) ensure $HOME/users and $HOME/db_nonusers are writeable by user running docker \r\n\r\nexport TRANSFORMERS_OFFLINE=1\r\nexport GRADIO_SERVER_PORT=7860\r\nexport OPENAI_SERVER_PORT=5000\r\nexport HF_HUB_OFFLINE=1\r\ndocker run --gpus all \\\r\n--runtime=nvidia \\\r\n--shm-size=2g \\\r\n-e TRANSFORMERS_OFFLINE=$TRANSFORMERS_OFFLINE \\\r\n-e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\r\n-e HF_HUB_OFFLINE=$HF_HUB_OFFLINE \\\r\n-e HF_HOME=\"/workspace/.cache/huggingface/\" \\\r\n-p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT \\\r\n-p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT \\\r\n--rm --init \\\r\n--network host \\\r\n-v /etc/passwd:/etc/passwd:ro \\\r\n-v /etc/group:/etc/group:ro \\\r\n-u `id -u`:`id -g` \\\r\n-v \"${HOME}\"/.cache/huggingface/:/workspace/.cache/huggingface \\\r\n-v \"${HOME}\"/.cache/torch/:/workspace/.cache/torch \\\r\n-v \"${HOME}\"/.cache/transformers/:/workspace/.cache/transformers \\\r\n-v \"${HOME}\"/save:/workspace/save \\\r\n-v \"${HOME}\"/user_path:/workspace/user_path \\\r\n-v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\r\n-v \"${HOME}\"/users:/workspace/users \\\r\n-v \"${HOME}\"/db_nonusers:/workspace/db_nonusers \\\r\n-v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path \\\r\n-e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT \\\r\n gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.0 \\\r\n /workspace/generate.py \\\r\n --base_model=mistralai/Mistral-7B-Instruct-v0.2 \\\r\n --use_safetensors=False \\\r\n --prompt_type=mistral \\\r\n --save_dir='/workspace/save/' \\\r\n --use_gpu_id=False \\\r\n --user_path=/workspace/user_path \\\r\n --langchain_mode=\"LLM\" \\\r\n --langchain_modes=\"['UserData', 'MyData', 'LLM']\" \\\r\n --score_model=None \\\r\n --max_max_new_tokens=2048 \\\r\n --max_new_tokens=1024 \\\r\n --visible_visible_models=False \\\r\n --openai_port=$OPENAI_SERVER_PORT\r\n```\r\n\r\nDepending upon if use links, may require more specific mappings to direct location not linked location that cannot be used\r\n```\r\n-v \"${HOME}\"/.cache/huggingface/hub:/workspace/.cache/huggingface/hub \\\r\n-v \"${HOME}\"/.cache:/workspace/.cache \\\r\n -e TRANSFORMERS_CACHE=\"/workspace/.cache/\" \\\r\n```\r\n\r\n"
      },
      {
        "user": "glenbhermon",
        "body": "I ran exactly the same command that you mention:\r\n`mkdir -p ~/.cache\r\nmkdir -p ~/save\r\nmkdir -p ~/user_path\r\nmkdir -p ~/db_dir_UserData\r\nmkdir -p ~/users\r\nmkdir -p ~/db_nonusers\r\nmkdir -p ~/llamacpp_path\r\nexport TRANSFORMERS_OFFLINE=1\r\nexport GRADIO_SERVER_PORT=7860\r\nexport OPENAI_SERVER_PORT=5000\r\nexport HF_HUB_OFFLINE=1\r\ndocker run \\\r\n\t--gpus all \\\r\n\t--runtime=nvidia \\\r\n\t--shm-size=2g \\\r\n\t-e TRANSFORMERS_OFFLINE=$TRANSFORMERS_OFFLINE \\\r\n\t-e HF_HUB_OFFLINE=$HF_HUB_OFFLINE \\\r\n\t-e HF_HOME=\"/workspace/.cache/huggingface/\" \\\r\n\t-p $GRADIO_SERVER_PORT:$GRADIO_SERVER_PORT \\\r\n\t-p $OPENAI_SERVER_PORT:$OPENAI_SERVER_PORT \\\r\n\t--rm --init \\\r\n\t--network host \\\r\n\t-v /etc/passwd:/etc/passwd:ro \\\r\n\t-v /etc/group:/etc/group:ro \\\r\n\t-u `id -u`:`id -g` \\\r\n\t-v \"${HOME}\"/.cache/huggingface/:/workspace/.cache/huggingface \\\r\n\t-v \"${HOME}\"/.cache/torch/:/workspace/.cache/torch \\\r\n\t-v \"${HOME}\"/.cache/transformers/:/workspace/.cache/transformers \\\r\n\t-v \"${HOME}\"/save:/workspace/save \\\r\n\t-v \"${HOME}\"/user_path:/workspace/user_path \\\r\n\t-v \"${HOME}\"/db_dir_UserData:/workspace/db_dir_UserData \\\r\n\t-v \"${HOME}\"/users:/workspace/users \\\r\n\t-v \"${HOME}\"/db_nonusers:/workspace/db_nonusers \\\r\n\t-v \"${HOME}\"/llamacpp_path:/workspace/llamacpp_path \\\r\n\t-e GRADIO_SERVER_PORT=$GRADIO_SERVER_PORT \\\r\n\th2ogpt_3 \\\r\n \t   /workspace/generate.py \\\r\n           --base_model=mistralai/Mistral-7B-Instruct-v0.2 \\\r\n \t   --use_safetensors=False \\\r\n \t   --prompt_type=mistral \\\r\n \t   --save_dir='/workspace/save/' \\\r\n\t   --use_gpu_id=False \\\r\n \t   --user_path=/workspace/user_path \\\r\n           --langchain_mode=\"LLM\" \\\r\n \t   --langchain_modes=\"['UserData', 'MyData', 'LLM']\" \\\r\n\t   --score_model=None \\\r\n\t   --max_max_new_tokens=2048 \\\r\n\t   --max_new_tokens=1024 \\\r\n\t   --visible_visible_models=False \\\r\n \t   --openai_port=$OPENAI_SERVER_PORT`\r\n \t\r\nAnd now getting the following error:\r\n\r\n`WARNING: Published ports are discarded when using host network mode\r\nUsing Model mistralai/mistral-7b-instruct-v0.2\r\nfatal: not a git repository (or any of the parent directories): .git\r\ngit_hash.txt failed to be found: [Errno 2] No such file or directory: 'git_hash.txt'\r\nload INSTRUCTOR_Transformer\r\nmax_seq_length  512\r\nStarting get_model: mistralai/Mistral-7B-Instruct-v0.2 \r\n/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nStarting get_model: mistralai/Mistral-7B-Instruct-v0.2 \r\nStarting get_model: mistralai/Mistral-7B-Instruct-v0.2 \r\nStarting get_model: mistralai/Mistral-7B-Instruct-v0.2 \r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connection.py\", line 198, in _new_conn\r\n    sock = connection.create_connection(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/util/connection.py\", line 60, in create_connection\r\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/socket.py\", line 955, in getaddrinfo\r\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\r\nsocket.gaierror: [Errno -3] Temporary failure in name resolution\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 793, in urlopen\r\n    response = self._make_request(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 491, in _make_request\r\n    raise new_e\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 467, in _make_request\r\n    self._validate_conn(conn)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1099, in _validate_conn\r\n    conn.connect()\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connection.py\", line 616, in connect\r\n    self.sock = sock = self._new_conn()\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connection.py\", line 205, in _new_conn\r\n    raise NameResolutionError(self.host, self, e) from e\r\nurllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x70cc207fb5b0>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\r\n    resp = conn.urlopen(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 847, in urlopen\r\n    retries = retries.increment(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/urllib3/util/retry.py\", line 515, in increment\r\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /mistralai/Mistral-7B-Instruct-v0.2/resolve/main/tf_model.h5 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x70cc207fb5b0>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)\"))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/workspace/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/workspace/src/utils.py\", line 72, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/workspace/src/gen.py\", line 2212, in main\r\n    model0, tokenizer0, device = get_model_retry(reward_type=False,\r\n  File \"/workspace/src/gen.py\", line 2564, in get_model_retry\r\n    model1, tokenizer1, device1 = get_model(**kwargs)\r\n  File \"/workspace/src/gen.py\", line 3191, in get_model\r\n    return get_hf_model(load_8bit=load_8bit,\r\n  File \"/workspace/src/gen.py\", line 3408, in get_hf_model\r\n    model = model_loader(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\r\n    return model_class.from_pretrained(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3239, in from_pretrained\r\n    if has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, **has_file_kwargs):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 627, in has_file\r\n    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=10)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/api.py\", line 100, in head\r\n    return request(\"head\", url, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/api.py\", line 59, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/requests/adapters.py\", line 519, in send\r\n    raise ConnectionError(e, request=request)\r\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /mistralai/Mistral-7B-Instruct-v0.2/resolve/main/tf_model.h5 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x70cc207fb5b0>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)\"))\r\n`\r\nI don't know how it is adding the resolve/main/tf_model.h5 in the model name. \r\n\r\n\r\n "
      },
      {
        "user": "glenbhermon",
        "body": "And when i adding the following arguments:\r\n\r\n`-v \"${HOME}\"/.cache/huggingface/hub:/workspace/.cache/huggingface/hub \\\r\n-v \"${HOME}\"/.cache:/workspace/.cache \\\r\n -e TRANSFORMERS_CACHE=\"/workspace/.cache/\" \\`\r\n \r\n Then it shows the given error:\r\n WARNING: Published ports are discarded when using host network mode\r\n/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\r\n  warnings.warn(\r\nYou are offline and the cache for model files in Transformers v4.22.0 has been updated while your local cache seems to be the one of a previous version. It is very likely that all your calls to any `from_pretrained()` method will fail. Remove the offline mode and enable internet connection to have your cache be updated automatically, then you can go back to offline mode.\r\n0it [00:00, ?it/s]\r\nUsing Model mistralai/mistral-7b-instruct-v0.2\r\nfatal: not a git repository (or any of the parent directories): .git\r\ngit_hash.txt failed to be found: [Errno 2] No such file or directory: 'git_hash.txt'\r\nload INSTRUCTOR_Transformer\r\nmax_seq_length  512\r\nStarting get_model: mistralai/Mistral-7B-Instruct-v0.2 \r\n/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nNot using tokenizer from HuggingFace:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\r\n    resolved_file = hf_hub_download(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\r\n    return _hf_hub_download_to_cache_dir(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1325, in _hf_hub_download_to_cache_dir\r\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1817, in _raise_on_head_call_error\r\n    raise LocalEntryNotFoundError(\r\nhuggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/src/gen.py\", line 2300, in get_config\r\n    config = AutoConfig.from_pretrained(base_model, token=use_auth_token,\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1138, in from_pretrained\r\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 631, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 686, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 441, in cached_file\r\n    raise EnvironmentError(\r\nOSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like mistralai/Mistral-7B-Instruct-v0.2 is not the path to a directory containing a file named config.json.\r\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\r\nStarting get_model: mistralai/Mistral-7B-Instruct-v0.2 \r\nNot using tokenizer from HuggingFace:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\r\n    resolved_file = hf_hub_download(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\r\n    return _hf_hub_download_to_cache_dir(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1325, in _hf_hub_download_to_cache_dir\r\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1817, in _raise_on_head_call_error\r\n    raise LocalEntryNotFoundError(\r\nhuggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/src/gen.py\", line 2300, in get_config\r\n    config = AutoConfig.from_pretrained(base_model, token=use_auth_token,\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1138, in from_pretrained\r\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 631, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 686, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 441, in cached_file\r\n    raise EnvironmentError(\r\nOSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like mistralai/Mistral-7B-Instruct-v0.2 is not the path to a directory containing a file named config.json.\r\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\r\nStarting get_model: mistralai/Mistral-7B-Instruct-v0.2 \r\nNot using tokenizer from HuggingFace:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\r\n    resolved_file = hf_hub_download(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\r\n    return _hf_hub_download_to_cache_dir(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1325, in _hf_hub_download_to_cache_dir\r\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1817, in _raise_on_head_call_error\r\n    raise LocalEntryNotFoundError(\r\nhuggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/src/gen.py\", line 2300, in get_config\r\n    config = AutoConfig.from_pretrained(base_model, token=use_auth_token,\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1138, in from_pretrained\r\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 631, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 686, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 441, in cached_file\r\n    raise EnvironmentError(\r\nOSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like mistralai/Mistral-7B-Instruct-v0.2 is not the path to a directory containing a file named config.json.\r\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\r\nStarting get_model: mistralai/Mistral-7B-Instruct-v0.2 \r\nNot using tokenizer from HuggingFace:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\r\n    resolved_file = hf_hub_download(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\r\n    return _hf_hub_download_to_cache_dir(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1325, in _hf_hub_download_to_cache_dir\r\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1817, in _raise_on_head_call_error\r\n    raise LocalEntryNotFoundError(\r\nhuggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/src/gen.py\", line 2300, in get_config\r\n    config = AutoConfig.from_pretrained(base_model, token=use_auth_token,\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1138, in from_pretrained\r\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 631, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 686, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 441, in cached_file\r\n    raise EnvironmentError(\r\nOSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like mistralai/Mistral-7B-Instruct-v0.2 is not the path to a directory containing a file named config.json.\r\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\r\n    resolved_file = hf_hub_download(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\r\n    return _hf_hub_download_to_cache_dir(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1325, in _hf_hub_download_to_cache_dir\r\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1817, in _raise_on_head_call_error\r\n    raise LocalEntryNotFoundError(\r\nhuggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/workspace/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/workspace/src/utils.py\", line 72, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/workspace/src/gen.py\", line 2212, in main\r\n    model0, tokenizer0, device = get_model_retry(reward_type=False,\r\n  File \"/workspace/src/gen.py\", line 2564, in get_model_retry\r\n    model1, tokenizer1, device1 = get_model(**kwargs)\r\n  File \"/workspace/src/gen.py\", line 3191, in get_model\r\n    return get_hf_model(load_8bit=load_8bit,\r\n  File \"/workspace/src/gen.py\", line 3286, in get_hf_model\r\n    config, _, max_seq_len = get_config(base_model, return_model=False, raise_exception=True, **config_kwargs)\r\n  File \"/workspace/src/gen.py\", line 2300, in get_config\r\n    config = AutoConfig.from_pretrained(base_model, token=use_auth_token,\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1138, in from_pretrained\r\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 631, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 686, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/hub.py\", line 441, in cached_file\r\n    raise EnvironmentError(\r\nOSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like mistralai/Mistral-7B-Instruct-v0.2 is not the path to a directory containing a file named config.json.\r\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\r\n "
      },
      {
        "user": "llmwesee",
        "body": "I followed all the instructions that are mentioned in the **README_offline.md**"
      },
      {
        "user": "pseudotensor",
        "body": "There's new bug in transformers where they auto convert non-safe tensors to safe, but they assume internet exists at that point.  I patched transformers in the new linux_install.sh.\r\n\r\nYou can run:\r\n```\r\nsp=`python3.10 -c 'import site; print(site.getsitepackages()[0])'`\r\npatch $sp/transformers/modeling_utils.py docs/trans.patch\r\n```\r\n\r\nTo fix the non-docker case, I'm building new docker at moment that would be ready in a few hours.\r\n\r\nOr you can delete your unsafe tensors and pass --use_safetensors=True while online to use those."
      },
      {
        "user": "pseudotensor",
        "body": "https://github.com/huggingface/transformers/issues/30920"
      },
      {
        "user": "llmwesee",
        "body": "I still got the same error when running running docker image on the machine which hasn't internet connection. "
      },
      {
        "user": "pseudotensor",
        "body": "The patch doesn't work on newer transformers.  Try sticking to transformers==4.41.0"
      },
      {
        "user": "pseudotensor",
        "body": "I updated the patch and requirements to be for `transformers>=4.41.2` and bumped them on fixing the issue."
      }
    ]
  },
  {
    "issue_number": 1676,
    "title": "h2o Windows installer \"Web Search\" and \"Q/A\"",
    "author": "UUSR",
    "state": "closed",
    "created_at": "2024-06-09T19:51:17Z",
    "updated_at": "2024-06-17T17:24:17Z",
    "labels": [
      "type/question"
    ],
    "body": "I installed the installer for Windows but I don’t have it in the application \"Web-Search integration with Chat and Document Q/A\"\r\nIt is indicated here: [https://github.com/h2oai/h2ogpt/tree/main#windows-1011-64-bit-with-full-document-qa-capability](url) Windows 10/11 64-bit with full document Q/A capability.\r\nI don't have a function in my interface.\r\n![2024-06-09_213258](https://github.com/h2oai/h2ogpt/assets/17599033/46ad4451-7687-4ffd-aebd-6cb7764eb329)\r\nIn the example on the website https://gpt.h2o.ai/ it looks like this:\r\n![2024-06-09_213316](https://github.com/h2oai/h2ogpt/assets/17599033/b05201d5-3bb3-4c69-9d32-4eb621fcff95)\r\nI looked at the file ./src/utils.py\r\n```\r\nhave_chromamigdb = False\r\ntry:\r\n    assert distribution('chromamigdb') is not None\r\n    have_chromamigdb = True\r\nexcept (PackageNotFoundError, AssertionError):\r\n    pass\r\n\r\nhave_serpapi = False\r\ntry:\r\n    assert distribution('google-search-results') is not None\r\n    have_serpapi = True\r\nexcept (PackageNotFoundError, AssertionError):\r\n    pass\r\n```\r\nHow can I enable these additional features?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, you need to have the SERP API key in your env.\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/README_SerpAPI.md"
      }
    ]
  },
  {
    "issue_number": 1689,
    "title": "Failed to load models",
    "author": "anagonzalez02",
    "state": "closed",
    "created_at": "2024-06-17T10:43:09Z",
    "updated_at": "2024-06-17T16:33:56Z",
    "labels": [],
    "body": "Currently in the last commit:  2e57e764c0bf2c595ff1259f477f0156a5f75890\r\nExecuting this script to load a model in a linux system:\r\n\r\n```\r\nTOKENIZERS_PARALLELISM=true python generate.py \\\r\n    --base_model=Qwen/Qwen2-7B-Instruct --prompt_type=qwen --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192 \\\r\n    --user_path=/opt/myDocuments/arsys.es/html/ --langchain_mode='UserData' --max_quality=True \\\r\n    --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True \\\r\n    --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True \\\r\n    --hf_embedding_model=\"hkunlp/instructor-xl\" \\\r\n    --memory_restriction_level=0 --score_model=None --verbose=True \\\r\n    --show_examples=True --compile_model=True \\\r\n    --src_lang='Spanish' --tgt_lang='Spanish' \\\r\n    --debug=True \\\r\n    --pre_prompt_query='Always answer as helpfully as possible and follow ALL given instructions. Do not speculate or make up any kind of information. As long as you can, give responses in spanish language.' \\\r\n    --share=True \r\n```\r\n    \r\n    I get the following error:\r\n    \r\n    \r\n![image](https://github.com/h2oai/h2ogpt/assets/93314129/486f94d6-b4b2-4617-91c1-80e0cbe39711)\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Yes, should be fixed now."
      }
    ]
  },
  {
    "issue_number": 1685,
    "title": "sidebar display control",
    "author": "chengchu88",
    "state": "closed",
    "created_at": "2024-06-14T01:11:33Z",
    "updated_at": "2024-06-14T16:21:01Z",
    "labels": [],
    "body": "Hello Sir,\r\nFor DocQ&A purpose, sidebar is needed for collection and document selection. I have found ways to hide a few items that are not needed, and I am down to the following 2 items:\r\n![image](https://github.com/h2oai/h2ogpt/assets/39104888/5853ac56-4ba3-4bf5-83e3-12c6932fff16)\r\n\r\nand\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/39104888/03f358b7-9962-4f35-80b1-5078fe8d9550)\r\n\r\nIs there any flag I can use to hide them?\r\n\r\nMany, many thanks!\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "714 build for docker will have"
      },
      {
        "user": "chengchu88",
        "body": "Thank you!\r\n\r\nOn Thu, Jun 13, 2024 at 11:57 PM pseudotensor ***@***.***>\r\nwrote:\r\n\r\n> 714 build for docker will have\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/h2oai/h2ogpt/issues/1685#issuecomment-2167359983>, or\r\n> unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AJKLC6EPLTMDTDBTYXNWRBLZHKH5XAVCNFSM6AAAAABJJNFUIWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRXGM2TSOJYGM>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      }
    ]
  },
  {
    "issue_number": 1682,
    "title": "tab visibility flag like --visible_system_tab=False not working",
    "author": "chengchu88",
    "state": "closed",
    "created_at": "2024-06-12T23:28:43Z",
    "updated_at": "2024-06-13T21:13:58Z",
    "labels": [],
    "body": "Hello, \r\nGot a question regarding gradio appearance control.. I set --visible_system_tab=False at python generate.py for model lock mode but it doesn't seem to have the desired effect, I can still see the system tab, and this is true for all other similar tab visibility control flag when I set them = False. Did I miss anything? I'd like to have an interface with only the chat tab (or the bare minimum).\r\nThank you,\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Not sure.  Every case works for me.  Maybe you had typo?\r\n\r\nFor:\r\n```\r\npython generate.py --base_model=llama\r\n```\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/4265224e-11e5-4717-8273-f3ff2a613437)\r\n\r\nFor:\r\n```\r\npython generate.py --base_model=llama --visible_system_tab=False\r\n```\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/7ba3bffa-06ac-4335-a54a-072ae78eba12)\r\n\r\nFor:\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ python generate.py --model_lock=\"[{'base_model': 'llama', 'model_path_llama': 'Phi-3-mini-4k-instruct-q4.gguf', 'tokenizer_base_model': 'microsoft/Phi-3-mini-4k-instruct'}]\"\r\n```\r\n\r\nFor model lock case:\r\n\r\n```\r\npython generate.py --model_lock=\"[{'base_model': 'llama', 'model_path_llama': 'Phi-3-mini-4k-instruct-q4.gguf', 'tokenizer_base_model': 'microsoft/Phi-3-mini-4k-instruct'}]\"\r\n```\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/03abfa88-ccda-46d4-9ca8-294988f9b578)\r\n\r\n\r\n```\r\npython generate.py --model_lock=\"[{'base_model': 'llama', 'model_path_llama': 'Phi-3-mini-4k-instruct-q4.gguf', 'tokenizer_base_model': 'microsoft/Phi-3-mini-4k-instruct'}]\" --visible_system_tab=False\r\n```\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/746c0d18-927c-4eec-a910-221152ecbd5e)\r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "From https://github.com/h2oai/h2ogpt/blob/main/docs/README_ui.md#sidebar-submit-buttons-and-tab-control\r\n\r\n\r\nOne can do:\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ python generate.py --model_lock=\"[{'base_model': 'llama', 'model_path_llama': 'Phi-3-mini-4k-instruct-q4.gguf', 'tokenizer_base_model': 'microsoft/Phi-3-mini-4k-instruct'}]\" --use_auth_token=$HUGGING_FACE_HUB_TOKEN --visible_submit_buttons=False --visible_side_bar=False --visible_submit_buttons=False --visible_side_bar=False --visible_chat_tab=False --visible_doc_selection_tab=False --visible_doc_view_tab=False --visible_chat_history_tab=False --visible_expert_tab=False --visible_models_tab=False --visible_system_tab=False --visible_tos_tab=False --visible_hosts_tab=False --chat_tabless=True  --visible_login_tab=False --visible_h2ogpt_logo=False --visible_h2ogpt_links=False --visible_h2ogpt_qrcode=False --visible_langchain_action_radio=False --allow_upload_to_user_data=False --allow_upload_to_my_data=False --langchain_mode=UserData\r\n```\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/904a9283-6c12-498a-9dca-0f42a91229b6)\r\n\r\n\r\nOr keep `--chat_tabless=False` to remove everything in UI. That removes everything, e.g. if one doesn't want any UI control, only API.\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/2565925b-2246-4508-9859-2e78ec5d264e)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "commit 0c0a5c7648c203f578a28a05b9b7761619334294 (HEAD -> main, origin/main, origin/HEAD)\r\nAuthor: Jonathan C. McKinney <pseudotensor@gmail.com>\r\nDate:   Wed Jun 12 23:16:51 2024 -0700\r\n\r\n    Control more\r\n\r\ncommit 14ae688c2e44ac1062fe7cf70b4d97a41fa45f0a\r\nAuthor: Jonathan C. McKinney <pseudotensor@gmail.com>\r\nDate:   Wed Jun 12 23:10:09 2024 -0700\r\n\r\n    Note\r\n\r\ncommit c666d21f109085f911784e218df22b56e0ac5747\r\nAuthor: Jonathan C. McKinney <pseudotensor@gmail.com>\r\nDate:   Wed Jun 12 23:08:02 2024 -0700\r\n\r\n    Fix tabless case\r\n\r\n"
      },
      {
        "user": "chengchu88",
        "body": "Hello Sir, thank you for the quick reply. \r\n\r\nHere is what i tried and results:\r\n\r\nroot@b707bdcc6014:~# HF_DATABASE_OFFLINE=1 GRADIO_SERVER_PORT=7865 python generate.py  --visible_submit_buttons=False --visible_side_bar=False --visible_submit_buttons=False --visible_side_bar=False --visible_chat_tab=True --visible_doc_selection_tab=False --visible_doc_view_tab=False --visible_chat_history_tab=False --visible_expert_tab=False --visible_models_tab=False --visible_system_tab=False --visible_tos_tab=False --visible_hosts_tab=False --chat_tables=True  --visible_login_tab=False --visible_h2ogpt_logo=False --visible_h2ogpt_links=False --visible_h2ogpt_qrcode=False\r\nfatal: not a git repository (or any parent up to mount point /)\r\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\nload INSTRUCTOR_Transformer\r\nmax_seq_length  512\r\n/workspace/gradio_utils/prompt_form.py:225: GradioUnusedKwargWarning: You have unused kwarg parameters in Chatbot, please remove them: {'likeable': True}\r\n  text_output = gr.Chatbot(label=output_label0,\r\n/workspace/gradio_utils/prompt_form.py:230: GradioUnusedKwargWarning: You have unused kwarg parameters in Chatbot, please remove them: {'likeable': True}\r\n  text_output2 = gr.Chatbot(label=output_label0_model2,\r\nRunning on local URL:  http://0.0.0.0:7865\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStarted Gradio Server and/or GUI: server_name: localhost port: 7865\r\nUse local URL: http://localhost:7865/\r\n/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_info\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\n/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_names\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nOpenAI API URL: http://0.0.0.0:5000\r\nINFO:__name__:OpenAI API URL: http://0.0.0.0:5000\r\nOpenAI API key: EMPTY\r\nINFO:__name__:OpenAI API key: EMPTY\r\n[Errno 98] error while attempting to bind on address ('0.0.0.0', 5000): address already in use\r\n\r\nAnd here is the screenshot:\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/39104888/0e2352e1-5e1d-4b8f-a7c7-0ad5e74d6346)\r\n\r\n\r\nTwo observations:\r\n1. I can remove/hide everything else (side bar, QR code..) but all the tabs still present.\r\n2. When I click on the tab, the chatboxes are not moving, but the items of the tab that clicked showed up below the chatbox (in the image above, I click on the 'hosts' tab), so is this some kind of gradio issue instead?\r\n3. I think my code is of a different version than yours. I checked the version.txt and it is 0.2.0, and it is the  \"latest version\" that I downloaded around May 10, since this is the version that works well with Llama 3.\r\n4. As you can see, I got a lot more tabs compared to your screenshot (ToS, Image Control, etc.)\r\n\r\nAny idea? I will try out 0.2.1 to see if this is can be fixed.\r\n\r\nAgain, thank you very much for your time and support on this matter.\r\n\r\nBest,,\r\n\r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "I think you may be using a gradio3 build perhaps, which didn't support tab removal.  In the past, 0.2.0 versions for docker might have come as gradio3 or gradio4, but now they are all gradio4.\r\n\r\nYes, please try 0.2.1 which will always be gradio4."
      },
      {
        "user": "chengchu88",
        "body": "I see. I just checked my version, it is gradio 3.50.2. Maybe this is the\r\nissue.\r\nThank you very much again for your help!\r\n\r\nOn Thu, Jun 13, 2024 at 9:42 AM pseudotensor ***@***.***>\r\nwrote:\r\n\r\n> I think you may be using a gradio3 build perhaps, which didn't support tab\r\n> removal. In the past, 0.2.0 versions for docker might have come as gradio3\r\n> or gradio4, but now they are all gradio4.\r\n>\r\n> Yes, please try 0.2.1 which will always be gradio4.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/h2oai/h2ogpt/issues/1682#issuecomment-2166195709>, or\r\n> unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AJKLC6D7J32OWHYDZYBSUFDZHHDV7AVCNFSM6AAAAABJHI7IWGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRWGE4TKNZQHE>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      },
      {
        "user": "chengchu88",
        "body": "I upgrade the gradio and now I can control the tab on/off. thanks!"
      }
    ]
  },
  {
    "issue_number": 1666,
    "title": "Can I use existing llama.cpp server as inference server?",
    "author": "ChiNoel-osu",
    "state": "closed",
    "created_at": "2024-06-05T09:10:15Z",
    "updated_at": "2024-06-13T17:20:20Z",
    "labels": [
      "type/question"
    ],
    "body": "I wanted to use my working local [llama.cpp server](https://github.com/ggerganov/llama.cpp/tree/master/examples/server) as the inference server.\r\nI looked [here](https://github.com/h2oai/h2ogpt/blob/main/docs/README_InferenceServers.md) and I put `--inference_server=\"http://localhost:8080/v1\"` but it doesn't work.\r\n```\r\nHF Client Begin: http://localhost:8080/v1 gpt-3.5-turbo\r\nStarting get_model: gpt-3.5-turbo http://localhost:8080/v1\r\nGR Client Begin: http://localhost:8080/v1 gpt-3.5-turbo\r\nLoaded as API: http://localhost:8080/v1/ ✔\r\nGR Client Failed http://localhost:8080/v1 gpt-3.5-turbo: Could not fetch config for http://localhost:8080/v1/\r\nHF Client Begin: http://localhost:8080/v1 gpt-3.5-turbo\r\nStarting get_model: gpt-3.5-turbo http://localhost:8080/v1\r\nGR Client Begin: http://localhost:8080/v1 gpt-3.5-turbo\r\nLoaded as API: http://localhost:8080/v1/ ✔\r\nGR Client Failed http://localhost:8080/v1 gpt-3.5-turbo: Could not fetch config for http://localhost:8080/v1/\r\nHF Client Begin: http://localhost:8080/v1 gpt-3.5-turbo\r\nStarting get_model: gpt-3.5-turbo http://localhost:8080/v1\r\nGR Client Begin: http://localhost:8080/v1 gpt-3.5-turbo\r\nLoaded as API: http://localhost:8080/v1/ ✔\r\nGR Client Failed http://localhost:8080/v1 gpt-3.5-turbo: Could not fetch config for http://localhost:8080/v1/\r\nHF Client Begin: http://localhost:8080/v1 gpt-3.5-turbo\r\nTraceback (most recent call last):\r\n  File \"/home/fae/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/fae/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/fae/h2ogpt/src/utils.py\", line 73, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/fae/h2ogpt/venv/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/fae/h2ogpt/venv/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/fae/h2ogpt/venv/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/home/fae/h2ogpt/src/gen.py\", line 2347, in main\r\n    model0, tokenizer0, device = get_model_retry(reward_type=False,\r\n  File \"/home/fae/h2ogpt/src/gen.py\", line 2718, in get_model_retry\r\n    model1, tokenizer1, device1 = get_model(**kwargs)\r\n  File \"/home/fae/h2ogpt/src/gen.py\", line 3021, in get_model\r\n    inference_server, gr_client, hf_client = get_client_from_inference_server(inference_server,\r\n  File \"/home/fae/h2ogpt/src/gen.py\", line 2699, in get_client_from_inference_server\r\n    res = hf_client.generate('What?', max_new_tokens=1)\r\n  File \"/home/fae/h2ogpt/venv/lib/python3.10/site-packages/text_generation/client.py\", line 284, in generate\r\n    raise parse_error(resp.status_code, payload)\r\ntext_generation.errors.NotFoundError: {'code': 404, 'message': 'File Not Found', 'type': 'not_found_error'}\r\n```\r\nllama.cpp only has completion and embedding routes, don't know if that's the problem.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "If llama.cpp is in OpenAI mode, you should use chat completion way and add prefix `vllm_chat:`.\r\n\r\n```\r\n--inference_server=\"vllm_chat:http://localhost:8080/v1\"\r\n```"
      }
    ]
  },
  {
    "issue_number": 1680,
    "title": "KeyError: images_num_max",
    "author": "jaysunl",
    "state": "closed",
    "created_at": "2024-06-11T23:55:52Z",
    "updated_at": "2024-06-12T01:39:35Z",
    "labels": [],
    "body": "Hello, I seemed to have pulled the latest repo version but when I go to use a model, I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/queueing.py\", line 566, in process_events\r\n    response = await route_utils.call_process_api(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/route_utils.py\", line 261, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/blocks.py\", line 1788, in process_api\r\n    result = await self.call_function(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/blocks.py\", line 1352, in call_function\r\n    prediction = await utils.async_iteration(iterator)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/utils.py\", line 595, in async_iteration\r\n    return await iterator.__anext__()\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/utils.py\", line 588, in __anext__\r\n    return await anyio.to_thread.run_sync(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\r\n    result = context.run(func, *args)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/utils.py\", line 571, in run_sync_iterator_async\r\n    return next(iterator)\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/utils.py\", line 754, in gen_wrapper\r\n    response = next(iterator)\r\n  File \"/workspace/src/gradio_funcs.py\", line 983, in bot\r\n    for res in get_response(fun1, history, chatbot_role1, speaker1, tts_language1, roles_state1,\r\n  File \"/workspace/src/gradio_funcs.py\", line 521, in get_response\r\n    yield from _get_response(fun1, history, chatbot_role1, speaker1, tts_language1, roles_state1, tts_speed1,\r\n  File \"/workspace/src/gradio_funcs.py\", line 652, in _get_response\r\n    for output_fun in fun1():\r\n  File \"/workspace/src/gen.py\", line 4293, in evaluate\r\n    images_num_max = images_num_max or chosen_model_state['images_num_max']\r\nKeyError: 'images_num_max'\r\n```\r\n\r\nDoes this have something to do with the latest change made (looking at the past PRs)? I have not received this error until now. I am running the following command:\r\n\r\n``` \r\ndocker run --runtime=nvidia -v /etc/passwd:/etc/passwd:ro -v /etc/group:/etc/group:ro -u 23764:60856 -v /lsc/scratch/automation/UserData:/workspace/UserData -v /lsc/scratch/automation/h2ogpt/user_path:/workspace/user_path h2ogpt:latest /workspace/generate.py --base_model=HuggingFaceH4/zephyr-7b-beta --pre_load_embedding_model=True --embedding_gpu_id=cpu --cut_distance=10000 --hf_embedding_model=BAAI/bge-base-en-v1.5 --score_model=None --enable_tts=False --enable_stt=False --enable_transcriptions=False --max_seq_len=2048 --chunk_size=128 --top_k_docs=3 --langchain_mode=UserData --load_4bit=True --share=True --use_gpu_id=0 --user_path=/workspace/user_path\r\n```\r\n\r\nAlso I have tried loading in a different model but it still shows this error.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, I run your command but don't see the issue if I just ask a question.\r\n```\r\nmkdir UserData ; docker run --runtime=nvidia -v /etc/passwd:/etc/passwd:ro -v /etc/group:/etc/group:ro -u 23764:60856 -v UserData:/workspace/UserData -v user_path:/workspace/user_path gcr.io/vorvan/h2oai/h2ogpt-runtime:0.2.1 /workspace/generate.py --base_model=HuggingFaceH4/zephyr-7b-beta --pre_load_embedding_model=True --embedding_gpu_id=cpu --cut_distance=10000 --hf_embedding_model=BAAI/bge-base-en-v1.5 --score_model=None --enable_tts=False --enable_stt=False --enable_transcriptions=False --max_seq_len=2048 --chunk_size=128 --top_k_docs=3 --langchain_mode=UserData --load_4bit=True --share=True --use_gpu_id=0 --user_path=/workspace/user_path\r\n```\r\n\r\nI changed minor things.\r\n\r\nI do see that error in a specific case of changing the model in load models, is that what you were doing instead?"
      },
      {
        "user": "jaysunl",
        "body": "I received this error at first when doing HuggingFaceH4/zephyr-7b-beta. So I tried TheBloke/Llama-2-7B-GGUF on the h2ogpt list of models using load model and it gave the error still. I will try again and see if I get the same error"
      },
      {
        "user": "pseudotensor",
        "body": "Got it, so yes  you are clicking \"load model\" not just asking question.  So the above will fix.  I'll rebuild the docker image.\r\n\r\nI merged a PR before exhaustive nightly testing after spot checks, but this error was caught in the nightly.\r\n\r\nThanks."
      }
    ]
  },
  {
    "issue_number": 1665,
    "title": "Consider switching to Coqui TTS from new repo",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-06-05T06:24:59Z",
    "updated_at": "2024-06-05T06:25:00Z",
    "labels": [],
    "body": "https://github.com/idiap/coqui-ai-TTS",
    "comments": []
  },
  {
    "issue_number": 1660,
    "title": "Does h2o have assistant API",
    "author": "cpgames",
    "state": "open",
    "created_at": "2024-06-03T07:02:58Z",
    "updated_at": "2024-06-03T21:38:58Z",
    "labels": [],
    "body": "Something like this:\r\nhttps://platform.openai.com/docs/assistants/overview\r\n\r\nI am specifically interested in tool/function calling.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I'm waiting for this to be merged:\r\n\r\nhttps://github.com/vllm-project/vllm/pull/5032/files\r\n\r\nat least in order to support function calling more generally in h2oGPT.\r\n\r\nLater PRs are not required but would be useful: https://github.com/vllm-project/vllm/issues/1869\r\n"
      }
    ]
  },
  {
    "issue_number": 1657,
    "title": "Question:extracting preference data of clients' response",
    "author": "chengchu88",
    "state": "closed",
    "created_at": "2024-05-31T23:42:02Z",
    "updated_at": "2024-06-03T19:09:50Z",
    "labels": [],
    "body": "Hello all,\r\nI got a question regarding the 'thumb up'/'thumb down' feedback of the LLM response. Is there a way to accumulate/collect the Q&A pairs and the response in the backend? I looked at gen.py parameters but it is not obvious to me how to set it up, if possible.\r\nThank you,\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "`--visible_ratings=True` .  Then you can specify the file `--reviews_file=reviews.csv` or it will be called that by default."
      },
      {
        "user": "pseudotensor",
        "body": "But regardless of `visible_ratings` being set, the up-down stuff goes to reviews.csv type file."
      },
      {
        "user": "chengchu88",
        "body": "Thank you sir for the quick reply, appreciate your help!"
      }
    ]
  },
  {
    "issue_number": 1440,
    "title": "Fatal Python Error when running docker compose ",
    "author": "mohammedouhibi",
    "state": "closed",
    "created_at": "2024-02-29T10:41:45Z",
    "updated_at": "2024-06-02T19:13:37Z",
    "labels": [],
    "body": "I'm a bit stuck here trying to run it on my server.\r\n\r\ncontainer successfully built, but running 'docker compose up' returns :\r\n`h2ogpt-main# docker compose up\r\n[+] Running 1/0\r\n ✔ Container h2ogpt-main-h2ogpt-1  Created                                                                                                               0.0s\r\nAttaching to h2ogpt-1\r\nh2ogpt-1  | /h2ogpt_conda/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\r\nh2ogpt-1  |   return torch._C._cuda_getDeviceCount() > 0\r\nh2ogpt-1  | No GPUs detected\r\nh2ogpt-1  | Using Model llama\r\nh2ogpt-1  | fatal: not a git repository (or any of the parent directories): .git\r\nh2ogpt-1  | load INSTRUCTOR_Transformer\r\nh2ogpt-1  | max_seq_length  512\r\nh2ogpt-1  | Starting get_model: llama\r\nh2ogpt-1  | Fatal Python error: Illegal instruction\r\nh2ogpt-1  |\r\nh2ogpt-1  | Current thread 0x00007f8781ade280 (most recent call first):\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/ctypes/__init__.py\", line 374 in __init__\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/llama_cpp/llama_cpp.py\", line 74 in _load_shared_library\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/llama_cpp/llama_cpp.py\", line 87 in <module>\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/llama_cpp/__init__.py\", line 1 in <module>\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\nh2ogpt-1  |   File \"/workspace/src/gpt4all_llm.py\", line 366 in validate_environment\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/pydantic/v1/main.py\", line 1100 in validate_model\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/pydantic/v1/main.py\", line 339 in __init__\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain_core/load/serializable.py\", line 107 in __init__\r\nh2ogpt-1  |   File \"/workspace/src/gpt4all_llm.py\", line 187 in get_llm_gpt4all\r\nh2ogpt-1  |   File \"/workspace/src/gpt4all_llm.py\", line 30 in get_model_tokenizer_gpt4all\r\nh2ogpt-1  |   File \"/workspace/src/gen.py\", line 2858 in get_model\r\nh2ogpt-1  |   File \"/workspace/src/gen.py\", line 2443 in get_model_retry\r\nh2ogpt-1  |   File \"/workspace/src/gen.py\", line 2110 in main\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/fire/core.py\", line 691 in _CallAndUpdateTrace\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/fire/core.py\", line 475 in _Fire\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/fire/core.py\", line 141 in Fire\r\nh2ogpt-1  |   File \"/workspace/src/utils.py\", line 65 in H2O_Fire\r\nh2ogpt-1  |   File \"/workspace/generate.py\", line 16 in entrypoint_main\r\nh2ogpt-1  |   File \"/workspace/generate.py\", line 20 in <module>\r\nh2ogpt-1  |\r\nh2ogpt-1  | Extension modules: _cffi_backend, simplejson._speedups, charset_normalizer.md, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, scipy._lib._ccallback_c, numpy.linalg.lapack_lite, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.linalg._flinalg, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize.__nnls, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._statlib, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pyarrow.lib, pyarrow._hdfsio, pandas._libs.ops, numexpr.interpreter, pyarrow._compute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, lz4._version, lz4.frame._frame, psutil._psutil_linux, psutil._psutil_posix, fontTools.misc.bezierTools, PIL._imaging, lxml._elementpath, lxml.etree, fontTools.varLib.iup, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, matplotlib._c_internal_utils, matplotlib._path, kiwisolver._cext, matplotlib._image, PIL._imagingft, av._core, av.logging, av.bytesource, av.buffer, av.audio.format, av.enum, av.error, av.utils, av.option, av.descriptor, av.container.pyio, av.dictionary, av.format, av.stream, av.container.streams, av.sidedata.motionvectors, av.sidedata.sidedata, av.packet, av.container.input, av.container.output, av.container.core, av.codec.context, av.video.format, av.video.reformatter, av.plane, av.video.plane, av.video.frame, av.video.stream, av.codec.codec, av.frame, av.audio.layout, av.audio.plane, av.audio.frame, av.audio.stream, av.audio.fifo, av.filter.pad, av.filter.link, av.filter.context, av.filter.graph, av.filter.filter, av.audio.resampler, yaml._yaml, google._upb._message, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, pyclipper._pyclipper, shapely.speedups._speedups, rapidfuzz._feature_detector_cpp, rapidfuzz.distance._initialize_cpp, rapidfuzz.distance.metrics_cpp, rapidfuzz.fuzz_cpp, rapidfuzz.process_cpp_impl, rapidfuzz.utils_cpp, scipy.cluster._vq, scipy.cluster._hierarchy, scipy.cluster._optimal_leaf_ordering, sentencepiece._sentencepiece, cython.cimports.libc.math, multidict._multidict, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, grpc._cython.cygrpc, greenlet._greenlet, sqlalchemy.cyextension.collections, sqlalchemy.cyextension.immutabledict, sqlalchemy.cyextension.processors, sqlalchemy.cyextension.resultproxy, sqlalchemy.cyextension.util, PIL._webp, regex._regex, sklearn.__check_build._check_build, sklearn.utils._isfinite, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.utils.sparsefuncs_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.utils._vector_sentinel, sklearn.feature_extraction._hashing_fast, pycrfsuite._pycrfsuite, sklearn.utils._random, sklearn.utils._seq_dataset, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, sklearn.linear_model._cd_fast, sklearn._loss._loss, sklearn.utils.arrayfuncs, sklearn.svm._liblinear, sklearn.svm._libsvm, sklearn.svm._libsvm_sparse, sklearn.utils._weight_vector, sklearn.linear_model._sgd_fast, sklearn.linear_model._sag_fast, sklearn.datasets._svmlight_format_fast (total: 318)\r\nh2ogpt-1 exited with code 0\r\n`\r\n\r\nAdditional context:\r\nOS: Ubuntu 22.04.3 LTS (Jammy Jellyfish)\r\nCPU: Intel(R) Xeon(R) CPU E5-2430 0 @ 2.20GHz (12 cores)\r\nGPU:NVIDIA GeForce GTX 1660\r\nGPU Driver: NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0\r\nDocker version: Docker version 24.0.7, build afdd53b\r\n\r\nFeel free to ask for additional context\r\nThank you.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "@EshamAaqib You got a similar error recently:\r\n\r\n```\r\n| /h2ogpt_conda/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.) h2ogpt-1 | return torch._C._cuda_getDeviceCount() > 0 h2ogpt-1 | No GPUs detected h2ogpt-1 |\r\n```\r\n\r\nI think for vLLM.  What was your resolution?\r\n\r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "@mohammedouhibi Please check your drivers are cuda 12.1+ compatible for this image.  E.g. cuda driver 535+ is ok.\r\n\r\n"
      },
      {
        "user": "mohammedouhibi",
        "body": "> @mohammedouhibi Please check your drivers are cuda 12.1+ compatible for this image. E.g. cuda driver 535+ is ok.\r\n\r\nI have upgraded my drivers to 'Driver Version: 545.29.06    CUDA Version: 12.3', and of course rebooted. The error indicating a probelm with cuda is gone, But i still have the same error popping up where python crashes. Here's the new output:\r\n'[+] Running 1/0\r\n ✔ Container h2ogpt-main-h2ogpt-1  Created                                                                                                            0.\r\nAttaching to h2ogpt-1\r\nh2ogpt-1  | Using Model llama\r\nh2ogpt-1  | fatal: not a git repository (or any of the parent directories): .git\r\nh2ogpt-1  | load INSTRUCTOR_Transformer\r\nh2ogpt-1  | max_seq_length  512\r\nh2ogpt-1  | Starting get_model: llama\r\n100%|██████████| 5.94G/5.94G [12:30<00:00, 7.92MB/s]\r\nh2ogpt-1  | Fatal Python error: Illegal instruction\r\nh2ogpt-1  |\r\nh2ogpt-1  | Thread 0x00007f32c7606700 (most recent call first):\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/threading.py\", line 324 in wait\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/threading.py\", line 607 in wait\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/tqdm/_monitor.py\", line 60 in run\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/threading.py\", line 1016 in _bootstrap_inner\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/threading.py\", line 973 in _bootstrap\r\nh2ogpt-1  |\r\nh2ogpt-1  | Current thread 0x00007f33d9a82280 (most recent call first):\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/ctypes/__init__.py\", line 374 in __init__\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/llama_cpp_cuda/llama_cpp.py\", line 74 in _load_shared_library\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/llama_cpp_cuda/llama_cpp.py\", line 87 in <module>\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/llama_cpp_cuda/__init__.py\", line 1 in <module>\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\nh2ogpt-1  |   File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\nh2ogpt-1  |   File \"/workspace/src/gpt4all_llm.py\", line 368 in validate_environment\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/pydantic/v1/main.py\", line 1100 in validate_model\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/pydantic/v1/main.py\", line 339 in __init__\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain_core/load/serializable.py\", line 107 in __init__\r\nh2ogpt-1  |   File \"/workspace/src/gpt4all_llm.py\", line 187 in get_llm_gpt4all\r\nh2ogpt-1  |   File \"/workspace/src/gpt4all_llm.py\", line 30 in get_model_tokenizer_gpt4all\r\nh2ogpt-1  |   File \"/workspace/src/gen.py\", line 2858 in get_model\r\nh2ogpt-1  |   File \"/workspace/src/gen.py\", line 2443 in get_model_retry\r\nh2ogpt-1  |   File \"/workspace/src/gen.py\", line 2110 in main\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/fire/core.py\", line 691 in _CallAndUpdateTrace\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/fire/core.py\", line 475 in _Fire\r\nh2ogpt-1  |   File \"/h2ogpt_conda/lib/python3.10/site-packages/fire/core.py\", line 141 in Fire\r\nh2ogpt-1  |   File \"/workspace/src/utils.py\", line 65 in H2O_Fire\r\nh2ogpt-1  |   File \"/workspace/generate.py\", line 16 in entrypoint_main\r\nh2ogpt-1  |   File \"/workspace/generate.py\", line 20 in <module>\r\nh2ogpt-1  |\r\nh2ogpt-1  | Extension modules: _cffi_backend, simplejson._speedups, charset_normalizer.md, numpy.core._multiarray_umath, numpy.core._multiarray_tests, ny.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._m937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, scipy._lib._ccallback_c, numpy.linalgpack_lite, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.lina_flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._funcs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.linalg._flinalg, scipy.sparse.linalg._dsolveuperlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, sc.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.spatial._ckdtreecipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxxcipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.ndimage._nd_ima _ni_label, scipy.ndimage._ni_label, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _modTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.opize._zeros, scipy.optimize.__nnls, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.s_highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize.rect, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_spel, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinofunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_uf, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._bt.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpola_bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.stats._biasedurncipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._statlib, scipy.stats._sobol, scipy.stats._qmc_cy, sy.stats._mvn, scipy.stats._rcont.rcont, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.natty pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedel, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pan._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandalibs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pyarrow.lib, pyarrow._hdfsio, pandas._libs.ops, numexpr.interpreter, pyarrow.mpute, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.intels, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._ligroupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, lz4._version, lz4.frame._frame, psutil._psutil_linux, psutil._psutil_posix, fontls.misc.bezierTools, PIL._imaging, lxml._elementpath, lxml.etree, fontTools.varLib.iup, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, tor_C._nn, torch._C._sparse, torch._C._special, matplotlib._c_internal_utils, matplotlib._path, kiwisolver._cext, matplotlib._image, PIL._imagingft, av._co av.logging, av.bytesource, av.buffer, av.audio.format, av.enum, av.error, av.utils, av.option, av.descriptor, av.container.pyio, av.dictionary, av.form av.stream, av.container.streams, av.sidedata.motionvectors, av.sidedata.sidedata, av.packet, av.container.input, av.container.output, av.container.corev.codec.context, av.video.format, av.video.reformatter, av.plane, av.video.plane, av.video.frame, av.video.stream, av.codec.codec, av.frame, av.audio.lat, av.audio.plane, av.audio.frame, av.audio.stream, av.audio.fifo, av.filter.pad, av.filter.link, av.filter.context, av.filter.graph, av.filter.filter, audio.resampler, yaml._yaml, google._upb._message, scipy.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils, h5py._errors, h5pyfs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, pyclipper._pyclipper, shapely.speedups._speedups, rapidfuzz._fure_detector_cpp, rapidfuzz.distance._initialize_cpp, rapidfuzz.distance.metrics_cpp, rapidfuzz.fuzz_cpp, rapidfuzz.process_cpp_impl, rapidfuzz.utils_cpscipy.cluster._vq, scipy.cluster._hierarchy, scipy.cluster._optimal_leaf_ordering, sentencepiece._sentencepiece, cython.cimports.libc.math, multidict._midict, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, grpc._cython.cygrpc, gnlet._greenlet, sqlalchemy.cyextension.collections, sqlalchemy.cyextension.immutabledict, sqlalchemy.cyextension.processors, sqlalchemy.cyextension.resuroxy, sqlalchemy.cyextension.util, PIL._webp, regex._regex, sklearn.__check_build._check_build, sklearn.utils._isfinite, sklearn.utils.murmurhash, skleautils._openmp_helpers, sklearn.utils.sparsefuncs_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, skle.utils._vector_sentinel, sklearn.feature_extraction._hashing_fast, pycrfsuite._pycrfsuite, sklearn.utils._random, sklearn.utils._seq_dataset, sklearn.mecs.cluster._expected_mutual_info_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cytholas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, skle.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.metri_pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fa sklearn.linear_model._cd_fast, sklearn._loss._loss, sklearn.utils.arrayfuncs, sklearn.svm._liblinear, sklearn.svm._libsvm, sklearn.svm._libsvm_sparse, earn.utils._weight_vector, sklearn.linear_model._sgd_fast, sklearn.linear_model._sag_fast, sklearn.datasets._svmlight_format_fast (total: 318)\r\nh2ogpt-1 exited with code 0\r\n'"
      },
      {
        "user": "pseudotensor",
        "body": "We are just using llama.cpp (llama_cpp_python) and that's what's failing.  Seen before.  Maybe you don't have AVX or something like that.\r\n\r\nhttps://github.com/ggerganov/llama.cpp/issues/537\r\n\r\nYou would need to rebuild the docker image after changing `1 -eq 0` to `1 -eq 1` in this block of code:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/linux_install.sh#L149-L157\r\n\r\n"
      },
      {
        "user": "tomerjr",
        "body": "> We are just using llama.cpp (llama_cpp_python) and that's what's failing. Seen before. Maybe you don't have AVX or something like that.\r\n> \r\n> [ggerganov/llama.cpp#537](https://github.com/ggerganov/llama.cpp/issues/537)\r\n> \r\n> You would need to rebuild the docker image after changing `1 -eq 0` to `1 -eq 1` in this block of code:\r\n> \r\n> https://github.com/h2oai/h2ogpt/blob/main/docs/linux_install.sh#L149-L157\r\n\r\nHi, i stumbled upon this issue after following instructions from the README_DOCKER.md file. how do i change this file and how do i then rebuild the docker image? @pseudotensor "
      },
      {
        "user": "pseudotensor",
        "body": "@tomerjr That instruction is old. Instead you would set the envs before installing, and for docker that means set those envs before building.\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/ab89711e65e339aeb72b72c29cb45bcb5df48e53/docker_build_script_ubuntu.sh#L41-L44\r\n\r\n\r\nChoose llama_cpp_python ARGS for your system according to [llama_cpp_python backend documentation](https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#supported-backends)\r\n"
      }
    ]
  },
  {
    "issue_number": 1589,
    "title": "httpx.ConnectError with --openai_server=True --ssl-verify=False",
    "author": "wypiki",
    "state": "closed",
    "created_at": "2024-04-29T19:31:58Z",
    "updated_at": "2024-05-31T08:58:25Z",
    "labels": [],
    "body": "Gradio UI works flawlessly but when trying to connect to the OpenAI-endpoint with HTTP, I get\r\n`httpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)`\r\non the server, which is strange because it shouldn't be handled as HTTPS in the first place, but when trying to reach it via HTTPS, I get\r\n`Invalid HTTP request received.`\r\n\r\nWhen trying to contact an openai-compatible tabbyAPI-server directly on the same host (the one powering H2oGPT), it just works, but I need RAG.\r\n\r\nDid I misconfigure something?\r\n\r\nHere are the relevant parameters:\r\n```\r\nexport GRADIO_SERVER_PORT=1000\r\npython generate.py --do_sample=True --temperature=0.1 --top_p=0.3 --base_model=turboderp/Mixtral-8x22B-Instruct-v0.1-exl2 --revision=6.0bpw --inference_server=vllm:127.0.0.1:5000 --max_seq_len=65000 --max_new_tokens=16384 --hf_embedding_model=intfloat/multilingual-e5-large --openai_server=True --openai_port=1001 --allow_upload_to_user_data=False --allow_upload_to_my_data=False --enable_text_upload=False --enable_sources_list=False --allow_api=True --ssl_verify=False --ssl_certfile=\"cert.crt\" --ssl_keyfile=\"cert.key\" --ssl_keyfile_password=\"password\"\r\n```\r\n(...)\r\n\r\nHere are the relevant parts of the log;\r\n```\r\nOpenAI API URL: http://0.0.0.0:1001\r\nINFO:__name__:OpenAI API URL: http://0.0.0.0:1001\r\nOpenAI API key: EMPTY\r\nINFO:__name__:OpenAI API key: EMPTY\r\nOpenAI user: {}\r\nGetting gradio client at https://localhost:1000\r\n```",
    "comments": [
      {
        "user": "wypiki",
        "body": "Edit: left out some relevant parameters and had formatting errors. Corrected now."
      },
      {
        "user": "pseudotensor",
        "body": "Hi, \r\n\r\n1) I'm supposing the ssl stuff you pass has no effect.  So can you remove the ssl args and have the same issue?  I don't pass those through so it should have no effect.\r\n\r\ni.e. these:\r\n```\r\n--ssl_verify=False\r\n--ssl_certfile=\"cert.crt\"\r\n--ssl_keyfile=\"cert.key\"\r\n--ssl_keyfile_password=\"password\"\r\n```\r\nshouldn't matter for the OpenAI proxy.\r\n\r\n2) As for why uvicorn is asking for ssl certificate and failed, I'm unsure.\r\n\r\nYou can try editing this line and remove the ssl stuff, just to be sure.\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/88b62f1ae657299374780be27859cf6d4b3e898f/openai_server/server.py#L374\r\n\r\n3) In my case the same message appear but then I get a permission denied:\r\n\r\n```\r\nOpenAI API URL: http://0.0.0.0:1001\r\nINFO:__name__:OpenAI API URL: http://0.0.0.0:1001\r\nOpenAI API key: EMPTY\r\nINFO:__name__:OpenAI API key: EMPTY\r\n[Errno 13] error while attempting to bind on address ('0.0.0.0', 1001): permission denied\r\n```\r\n\r\nMaybe you have something else on that port that is secure?  Or maybe the port isn't allowed to be used?\r\n\r\n4) If I change the port to 5001, then I don't get permission denied.\r\n\r\n```\r\nOpenAI API URL: http://0.0.0.0:5001\r\nINFO:__name__:OpenAI API URL: http://0.0.0.0:5001\r\nOpenAI API key: EMPTY\r\nINFO:__name__:OpenAI API key: EMPTY\r\nInvalid HTTP request received.\r\nINFO:     127.0.0.1:46322 - \"GET / HTTP/1.1\" 405 Method Not Allowed\r\nINFO:     127.0.0.1:46322 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\r\n```\r\n\r\n5) Perhaps 1000 and 1001 are too close."
      },
      {
        "user": "wypiki",
        "body": "Hi! Thanks for your detailed answer.\r\n\r\n1. I need the SSL stuff for the main UI. Because of limited VRAM I'd rather not have more H2oGPT-instances than necessary... Is that possible? (To have the main UI via HTTPS and the OpenAI-API via HTTP?\r\n2. You mean I could just remove those two parameters, like:\r\n` uvicorn.run(app, host=host, port=port)`\r\n?\r\n3. That port is allowed. But I also tried 5001 (with another service on port 5000).\r\n4. I'll try another port, too."
      },
      {
        "user": "wypiki",
        "body": "I changed\r\n`uvicorn.run(app, host=host, port=port, ssl_certfile=ssl_certfile, ssl_keyfile=ssl_keyfile)`\r\nto\r\n` uvicorn.run(app, host=host, port=port)`\r\n\r\nand the port from 1001 to 10001, but have the same error..."
      },
      {
        "user": "pseudotensor",
        "body": "Maybe there is some ENV that is affecting uvicorn and causing it to trigger SSL.  Not sure."
      },
      {
        "user": "pseudotensor",
        "body": "Also, on MAC, I've seen issues with SSL\r\n\r\nhttps://github.com/h2oai/h2ogpt/issues/530\r\nhttps://github.com/h2oai/h2ogpt/issues/1530\r\nhttps://github.com/h2oai/h2ogpt/issues/1531\r\n\r\nNeeds to be setup right.  Are you on MAC or have a proxy?"
      },
      {
        "user": "wypiki",
        "body": "It's on Ubuntu 22.04, and there is no proxy explicitly set up, nor is the internet connection filtered. But there is a proxy server available which browsers get via the http://gateway/wpad.dat when they're configured to automatically determine the proxy server. I don't know if this plays a role.\r\nAlso, I added the intermediate and the root certificate so the verification would work, but it still doesn't.\r\nAnd, only IPv4 is available, no IPv6 (if that plays a role.)"
      },
      {
        "user": "wypiki",
        "body": "I spent a few hours now trying to get this to work:\r\n- added the FQDN to /etc/hosts\r\n- set GRADIO_SERVER_HOST=...\r\n- changed the parameters for uvicorn.run, changed host to the FQDN\r\n- made changes to HTTPX\r\n- and some more things,\r\n\r\nbut I still get:\r\nhttpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'localhost'. (_ssl.c:1007)\r\n\r\nPlease help, @pseudotensor !\r\n\r\nGradio UI does work with the certificates, just the openai api doesn't."
      },
      {
        "user": "pseudotensor",
        "body": "\"localhost\" sounds wrong, it should be maybe a real IP address.\r\n\r\nIn latest h2oGPT what I see in end is:\r\n```\r\nRunning on local URL:  http://0.0.0.0:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStarted Gradio Server and/or GUI: server_name: localhost port: None\r\nUse local URL: http://localhost:7860/\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_info\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_names\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nOpenAI API URL: http://0.0.0.0:5000\r\nINFO:__name__:OpenAI API URL: http://0.0.0.0:5000\r\nOpenAI API key: EMPTY\r\nINFO:__name__:OpenAI API key: EMPTY\r\n```\r\n\r\ni.e. 0.0.0.0 that for linux means open public version of local IP AFAIK.  Maybe you have 'localhost' either using older h2oGPT or are on windows/mac.  But you say you are on Ubuntu PC?  \r\n\r\nWhen I go to chrome to that IP/Port, it works (the message is valid):\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/aedaf162-bb56-4167-83d7-a1f854284c3c)\r\n\r\n\r\nUnfortunately I'm not expert in SSL stuff.\r\n\r\nI would recommend trying to setup a fastAPI server, since that is what h2oGPT is doing to get OpenAI proxy server.  So do it in very basic form and see what one can do.\r\n\r\nE.g. from chatgpt:\r\n\r\n```\r\nfrom fastapi import FastAPI\r\n\r\napp = FastAPI()\r\n\r\n@app.get(\"/\")\r\ndef read_root():\r\n    return {\"Hello\": \"World\"}\r\n\r\n@app.get(\"/items/{item_id}\")\r\ndef read_item(item_id: int, q: str = None):\r\n    return {\"item_id\": item_id, \"q\": q}\r\n\r\nif __name__ == \"__main__\":\r\n    import uvicorn\r\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\r\n```\r\n\r\nThis runs, do you have problems with this?  We should be able to bisect the issue if no problem here and still problems with h2oGPT.  "
      },
      {
        "user": "wypiki",
        "body": "Thanks for your help. Finally managed to solve it. I focussed on the openai-api-server all the time when the problem was establishing the connection from there to Gradio.\r\nI replaced `os.environ['GRADIO_SERVER_HOST'] = gradio_host or 'localhost'` with `os.environ['GRADIO_SERVER_HOST'] = 'server.our-domain.com'` and it works now :-)"
      },
      {
        "user": "pseudotensor",
        "body": "I see, I think the equivalent would be to run h2oGPT like:\r\n```\r\npython generate.py --server_name=server.our-domain.com\r\n```\r\n\r\nWhen I do this, it ends up pasing this into run() as \"gradio_host\""
      },
      {
        "user": "wypiki",
        "body": "Oh no, it would have been this easy... Didn't realize that was what server_name meant. Thanks for the advice."
      }
    ]
  },
  {
    "issue_number": 1656,
    "title": "Change AutoGPT Agent Embeddings Model",
    "author": "rohitnanda1443",
    "state": "closed",
    "created_at": "2024-05-30T15:39:16Z",
    "updated_at": "2024-05-31T03:48:08Z",
    "labels": [],
    "body": "Hi how can we change the autogpt agent's embeddings model in /src\r\n/gpt_langchain.py file line 7456 to Hf_Embeddings ?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Ya, I should make that easier.  Will look soon.  I think just:\r\n```\r\ndiff --git a/src/gpt_langchain.py b/src/gpt_langchain.py\r\nindex 23b52fa5..b0ddabae 100644\r\n--- a/src/gpt_langchain.py\r\n+++ b/src/gpt_langchain.py\r\n@@ -7861,11 +7861,10 @@ def get_chain(query=None,\r\n             tools.extend([sympy_tool])\r\n \r\n         from langchain_community.docstore import InMemoryDocstore\r\n-        from langchain_community.embeddings import OpenAIEmbeddings\r\n         from langchain_community.vectorstores import FAISS\r\n \r\n         # Define your embedding model\r\n-        embeddings_model = OpenAIEmbeddings()\r\n+        embeddings_model = get_embedding(use_openai_embedding, hf_embedding_model=hf_embedding_model)\r\n         # Initialize the vectorstore as empty\r\n         import faiss\r\n \r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "I fixed it, but I see that AutoGPT plugin is even more unstable than usual.  Can't even do 2+2"
      },
      {
        "user": "rohitnanda1443",
        "body": " File \"/root/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/root/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/root/h2ogpt/src/utils.py\", line 73, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/root/h2ogpt/src/gen.py\", line 1947, in main\r\n    from src.gpt_langchain import get_embedding\r\n  File \"/root/h2ogpt/src/gpt_langchain.py\", line 67, in <module>\r\n    from utils import wrapped_partial, EThread, import_matplotlib, sanitize_filename, makedirs, get_url, flatten_list, \\\r\nImportError: cannot import name 'AsyncNullContext' from 'utils' (/root/h2ogpt/src/utils.py)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Not sure, maybe you are in wrong directory.  It shouldn't fail like that and doesn't for me."
      },
      {
        "user": "rohitnanda1443",
        "body": "Run Script in the root directory of H2oGPT:\r\npython generate.py --auth_access=closed --auth=auth.json --guest_name='' --base_model=mistralai/Mixtral-8x7B-Instruct-v0.1 --max_seq_len=16384  --pre_load_embedding_model=True --enable_tts=False --enable_stt=False --enable_transcriptions=False --extra_allowed_paths=[\"Ai_test_data\"] --use_gpu_id=False \r\n"
      }
    ]
  },
  {
    "issue_number": 1653,
    "title": "Document Content Presentation Difference Between Built-In UI and Custom UI using Gradio client",
    "author": "mohammedouhibi",
    "state": "closed",
    "created_at": "2024-05-29T11:45:08Z",
    "updated_at": "2024-05-30T23:53:32Z",
    "labels": [],
    "body": "When querying from the h2oGPT built-in UI, document content is presented in a clean format, for example:\r\n\r\ncontent: \"This is the document content.\"\r\n\r\nHowever, when querying using my custom UI using a Gradio client, the document content is presented with additional metadata, for example:\r\n\r\ncontent : \"Begin Document:\r\n\r\nMetadata:\r\nchunk_id = 4\r\ndate = 2024-03-13 13:01:30.138822\r\ninput_type = .docx\r\nsource = user_path/File.docx\r\n\r\nDocument Contents:\r\n\"\"\"\r\nThis is the document content.\r\n\r\n\"\"\"\r\nEnd Document\r\n\"\r\n\r\n\r\nThe presence of this additional metadata seems to be causing the language model to behave differently, potentially leading to suboptimal performance.\r\n\r\nRequested Change:\r\n\r\nProvide an option or configuration setting to control the presentation format of the document content. This would allow users to choose between the clean format (as seen in the built-in UI) or the format with metadata (as seen in the custom UI). Alternatively, provide a way to remove or exclude the metadata when presenting the document content to the language model.\r\n\r\n\r\n\r\nSteps to Reproduce:\r\n1. Run the h2oGPT built-in UI and query with a document.\r\n2. Run the same query with a Gradio client set to your h2oGPT URL.\r\n3. Observe the difference in the content presentation format and the language model's behavior in your history.json file under \"sources.content\" . (make sure to include \"--save_dir=your_save_folder_name\" in your startup arguments or h2o will not generate a history.json file)\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "If you have issues with the metadata, you can pass to CLI `--metadata_in_context='None'`\r\n\r\nThis metadata_in_context can be passed via both UI and API (`metadata_in_context` parameter) as well for full control.\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/0d80b3f4-7e29-4f4e-b38a-168bb982645f)\r\n\r\n\r\nBut I fixed things to be clearer and the CLI is passed along and used by UI or API if not changed."
      }
    ]
  },
  {
    "issue_number": 1646,
    "title": "Logging/Saving Settings and Instructions for Inference Jobs",
    "author": "mohammedouhibi",
    "state": "closed",
    "created_at": "2024-05-26T19:43:45Z",
    "updated_at": "2024-05-29T10:52:08Z",
    "labels": [],
    "body": "There needs to be a way to log or save all settings and instructions provided for every inference job that the vLLM inference server receives. This would be useful for debugging purposes, as it would allow us to track and analyze the input data and configurations used for each job.\r\n\r\nProposed solution :\r\n\r\nImplement a logging mechanism that captures and stores the following information for each inference job:\r\n   - Model parameters (e.g., model name, version, quantization settings)\r\n   - Input data (e.g., prompt, context, or other input files)\r\n   - Inference settings (e.g., temperature, top-k, top-p, number of tokens, etc.)\r\n   - Timestamp and other relevant metadata",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Alot of information is stored in the save directory in history.json for every inference job.  It has everything you mentioned, except not perhaps as much detail -- e.g.no input files, but just whether there are input files.\r\n\r\nIn addition, vLLM or TGI can also change its logging."
      },
      {
        "user": "mohammedouhibi",
        "body": "> Alot of information is stored in the save directory in history.json for every inference job. It has everything you mentioned, except not perhaps as much detail -- e.g.no input files, but just whether there are input files.\r\n\r\nI cant seem to find history.json under /save, is there perhaps a run option that I'm missing? "
      },
      {
        "user": "pseudotensor",
        "body": "`--save_dir=foo` will place it in `foo` directory relative to startup in repo."
      },
      {
        "user": "mohammedouhibi",
        "body": "> `--save_dir=foo` will place it in `foo` directory relative to startup in repo.\r\n\r\nGreat! It works now.\r\n\r\nthough I'm missing the docs/chunks used in the job. I really think it needs to be added.\r\nI have noticed that the docs being loaded when using the h2oGPT interface is different than those used when requesting the same instruction through a gradio client(i inferred this from the fact that the \"num_prompt_tokens\" value is higher when handling gradio client requests), it's causing my model to behave differently (worse in my case)"
      },
      {
        "user": "pseudotensor",
        "body": "Hi, the API call and history.json does contain 'save_dict['sources']` as the list of sources used."
      },
      {
        "user": "mohammedouhibi",
        "body": "I see, thanks!"
      }
    ]
  },
  {
    "issue_number": 1639,
    "title": "question regarding model_lock",
    "author": "chengchu88",
    "state": "closed",
    "created_at": "2024-05-22T23:36:05Z",
    "updated_at": "2024-05-29T07:05:05Z",
    "labels": [],
    "body": "Hello all, \r\nI wonder if someone can tell me, when using model_lock to deploy mutliple inferencing gradio services, can I specifiy different LLM control parameters (temperature, top p, top N, etc.) at the CLI command (either the server or the client command)?\r\nthanks!",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "LLM setup things can be controlled:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/1d126ec3542965f66a151ecd56b0fad9dd0f58b9/src/gen.py#L2151-L2171\r\n\r\nas well as prompt_type, prompt_dict, visible_models, h2ogpt_key\r\n\r\nBut generic LLM usage things are only controlled by the expert settings in UI or defaults via CLI for all LLMs.\r\n\r\nHowever, I pushed changes so the CLI model_lock can control all the other things too.\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/8e75086cb726428b70a09c896800ed7265b3b76a/src/evaluate_params.py#L18-L108"
      },
      {
        "user": "chengchu88",
        "body": "Thank you for the detailed explanation, truly appreciate it."
      }
    ]
  },
  {
    "issue_number": 1652,
    "title": "Timestamps issue in Youtube Chat",
    "author": "anushaharish538",
    "state": "open",
    "created_at": "2024-05-29T06:46:47Z",
    "updated_at": "2024-05-29T06:46:47Z",
    "labels": [],
    "body": "Hi,\r\nSource url and time stamps are not correct in YouTube chat. It show 0 and the source link as YouTube url without time frame\r\n\r\nRegards,\r\nAnusha",
    "comments": []
  },
  {
    "issue_number": 1627,
    "title": "Sepparate Upload Document to Database H2O and Query-Summary",
    "author": "btriw",
    "state": "open",
    "created_at": "2024-05-17T07:25:06Z",
    "updated_at": "2024-05-28T15:30:23Z",
    "labels": [],
    "body": "Hi i just tried to learn about h2o and have script to hit h2o api, but i confuse how to sepparate between upload document to h2o database and queries-summaries answering\r\nis there any proper way to do it?\r\nthis is how i tried :\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/87292096/f42bb48e-e4c8-4ed7-b13a-9aee518c8334)\r\n\r\nquery part\r\n![image](https://github.com/h2oai/h2ogpt/assets/87292096/948da082-1afd-45f9-bd69-ba1911eac05d)\r\n\r\nupload part\r\n![image](https://github.com/h2oai/h2ogpt/assets/87292096/072a0a95-9cdd-4518-a4a7-c12396feaa68)\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Sorry for delay.  At moment there is no generic upload of files that is used generically.  The upload of file is tied to the document ingest.  However, from client it's still exposed as generic files that were uploaded."
      }
    ]
  },
  {
    "issue_number": 1630,
    "title": "failed to concatenate document_choice",
    "author": "tomerjr",
    "state": "closed",
    "created_at": "2024-05-21T06:06:16Z",
    "updated_at": "2024-05-28T15:28:27Z",
    "labels": [],
    "body": "Hi, tried to search relevant issues but did not find any.\r\nwhen trying to run --langchain_mode with my db and with --document_choice like so:\r\npython generate.py --base_model=https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q2_K.gguf --prompt_type=zephyr --langchain_mode=Duck --document_choice=[file2.pdf] --hf_embedding_model=sentence-transformers/all-MiniLM-L6-v2 --score_model=None --llamacpp_dict=\"{'n_gpu_layers':10}\" --max_seq_len=1024 --enable_tts=False --enable_stt=False --enable_transcriptions=False\r\n\r\nand like so:\r\n--document_choice=\"file2.pdf\" \r\n--document_choice=[\"file2.pdf\"] \r\n--document_choice=file2.pdf \r\n\r\ni get the following error: \r\n\r\n  File \"/home/tomershm/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/tomershm/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/tomershm/h2ogpt/src/utils.py\", line 69, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/tomershm/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/tomershm/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/tomershm/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, kwargs)\r\n  File \"/home/tomershm/h2ogpt/src/gen.py\", line 2203, in main\r\n    go_gradio(locals())\r\n  File \"/home/tomershm/h2ogpt/src/gradio_runner.py\", line 872, in go_gradio\r\n    docs_state00 = kwargs['document_choice'] + [DocumentChoice.ALL.value]\r\nTypeError: can only concatenate str (not \"list\") to str\r\n\r\nI am basically trying to select specific documents to work with after creating a db via make_db. would love some help or clarification on how to use document selection. thank you!",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It should look like:\r\n\r\n--document_choice=\"['file2.pdf']\"\r\n\r\nfor Fire to process it into list, the others cause problems in bash command line.\r\n\r\nIf you are in a different command line, like pycharm, then don't need the extra outer double quotes."
      }
    ]
  },
  {
    "issue_number": 1649,
    "title": "Youtube chat does not work",
    "author": "anushaharish538",
    "state": "open",
    "created_at": "2024-05-28T03:58:02Z",
    "updated_at": "2024-05-28T14:36:37Z",
    "labels": [],
    "body": "Hi,\r\nIf I ask anything about video the answer is coming from the internet not from the video.\r\n\r\nThank you.\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "If you mean the ASR text, verify the document viewer shows the text"
      }
    ]
  },
  {
    "issue_number": 1648,
    "title": "Youtube ingestion doesn't work",
    "author": "anushaharish538",
    "state": "closed",
    "created_at": "2024-05-28T00:48:59Z",
    "updated_at": "2024-05-28T01:54:25Z",
    "labels": [],
    "body": "Hi,\r\nYoutube ingestion doesn't work .Its converting to audio but 3).\r\n[download] Destination: /tmp/gradio/6ce6b3f2-2e2a-4a1f-81e8-a58785d61295/What Is LangChain？ - LangChain + ChatGPT Overview.m4a\r\n[download] 100% of    5.92MiB in 00:00:00 at 39.03MiB/s\r\n[FixupM4a] Correcting container of \"/tmp/gradio/6ce6b3f2-2e2a-4a1f-81e8-a58785d61295/What Is LangChain？ - LangChain + ChatGPT Overview.m4a\"\r\n[ExtractAudio] Not converting audio /tmp/gradio/6ce6b3f2-2e2a-4a1f-81e8-a58785d61295/What Is LangChain？ - LangChain + ChatGPT Overview.m4a; file is already in target format m4a\r\nTranscribing part /tmp/gradio/6ce6b3f2-2e2a-4a1f-81e8-a58785d61295/What Is LangChain？ - LangChain + ChatGPT Overview.m4a!\r\nDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\r\n\r\n\r\n\r\nTaking long run and killing automatically after sometime.\r\n\r\n@pseudotensor I think its issue with database . can you please confirm. I'm getting python src/make_db.pyExceptions: 0/0 []Traceback (most recent call last): File \"/home/anushaharish538/as/h2ogpt/src/make_db.py\", line 403, in\r\nH2O_Fire(make_db_main)\r\nFile \"/home/anushaharish538/as/h2ogpt/src/utils.py\", line 73, in H2O_Fire\r\nfire.Fire(component=component, command=args)\r\nFile \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\ncomponent_trace = _Fire(component, args, parsed_flag_args, context, name)\r\nFile \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\ncomponent, remaining_args = _CallAndUpdateTrace(\r\nFile \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\ncomponent = fn(*varargs, **kwargs)\r\nFile \"/home/anushaharish538/as/h2ogpt/src/make_db.py\", line 389, in make_db_main\r\nassert len(sources) > 0 or not fail_if_no_sources, \"No sources found\"\r\nAssertionError: No sources found\r\n\r\nIt's issue with only YouTube . Pdfs working good",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I fixed CPU mode: https://github.com/h2oai/h2ogpt/issues/1643\r\n\r\nHowever, yes, CPU ASR is very slow.  I tried same video as you used in CPU vs. GPU, and CPU ASR goes for a while on my i9 8 core system, maybe 2 minutes.\r\n\r\nSo I don't think ASR is good on CPU.\r\n\r\nHowever, after that, things actually hang.\r\n\r\nLogs are like this for me using CPU:\r\n```\r\nWARNING! Model override. Using model:  openai/whisper-medium\r\nUsing the following model:  openai/whisper-medium\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nNo optimum, not using BetterTransformer: Transformers now supports natively BetterTransformer optimizations (torch.nn.functional.scaled_dot_product_attention) for the model type whisper. As such, there is no need to use `model.to_bettertransformers()` or `BetterTransformer.transform(model)` from the Optimum library. Please upgrade to transformers>=4.36 and torch>=2.1.1 to use it. Details: https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention.\r\n[youtube] Extracting URL: https://www.youtube.com/watch?v=_v_fgW2SkkQ\r\n[youtube] _v_fgW2SkkQ: Downloading webpage\r\n[youtube] _v_fgW2SkkQ: Downloading ios player API JSON\r\n[youtube] _v_fgW2SkkQ: Downloading m3u8 information\r\n[info] _v_fgW2SkkQ: Downloading 1 format(s): 140\r\n[download] Destination: /tmp/gradio/78817303-11f7-43fc-b98e-89f67d47cff5/What Is LangChain？ - LangChain + ChatGPT Overview.m4a\r\n[download] 100% of    5.92MiB in 00:00:00 at 32.78MiB/s  \r\n[FixupM4a] Correcting container of \"/tmp/gradio/78817303-11f7-43fc-b98e-89f67d47cff5/What Is LangChain？ - LangChain + ChatGPT Overview.m4a\"\r\n[ExtractAudio] Not converting audio /tmp/gradio/78817303-11f7-43fc-b98e-89f67d47cff5/What Is LangChain？ - LangChain + ChatGPT Overview.m4a; file is already in target format m4a\r\nTranscribing part /tmp/gradio/78817303-11f7-43fc-b98e-89f67d47cff5/What Is LangChain？ - LangChain + ChatGPT Overview.m4a!\r\nDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\r\n\r\nINFO:eta.core.utils: 100% |████████████| 1/1 [2.8s elapsed, 0s remaining, 0.4 videos/s] \r\n 100% |████████████| 1/1 [2.8s elapsed, 0s remaining, 0.4 videos/s] \r\nINFO:eta.core.utils: 100% |███████████| 1/1 [29.2ms elapsed, 0s remaining, 34.5 samples/s] \r\n 100% |███████████| 1/1 [29.2ms elapsed, 0s remaining, 34.5 samples/s] \r\nINFO:fiftyone.core.metadata:Computing metadata...\r\nComputing metadata...\r\n 100% |███████████| 1/1 [51.5ms elapsed, 0s remaining, 19.4 samples/s] \r\n 100% |███████| 384/384 [5.3ms elapsed, 0s remaining, 72.3K samples/s]       \r\nINFO:eta.core.utils: 100% |███████████| 1/1 [51.5ms elapsed, 0s remaining, 19.4 samples/s] \r\nINFO:eta.core.utils: 100% |███████| 384/384 [5.3ms elapsed, 0s remaining, 72.3K samples/s]       \r\n 100% |███████| 384/384 [5.7ms elapsed, 0s remaining, 67.2K samples/s]       \r\nSetting 384 frame filepaths on the input collection that exist on disk but are not recorded on the dataset\r\nINFO:eta.core.utils: 100% |███████| 384/384 [5.7ms elapsed, 0s remaining, 67.2K samples/s]       \r\nINFO:fiftyone.core.video:Setting 384 frame filepaths on the input collection that exist on disk but are not recorded on the dataset\r\nSampling video frames...\r\n   0% ||----------| 0/1 [2.0ms elapsed, ? remaining, ? samples/s] INFO:fiftyone.core.video:Sampling video frames...\r\n 100% |███████████| 1/1 [4.5s elapsed, 0s remaining, 0.2 samples/s] \r\nINFO:eta.core.utils: 100% |███████████| 1/1 [4.5s elapsed, 0s remaining, 0.2 samples/s] \r\nComputing embeddings...\r\nINFO:fiftyone.brain.internal.core.utils:Computing embeddings...\r\n 100% |███████| 384/384 [1.4m elapsed, 0s remaining, 5.0 samples/s]      \r\nINFO:eta.core.utils: 100% |███████| 384/384 [1.4m elapsed, 0s remaining, 5.0 samples/s]      \r\nINFO:fiftyone.brain.similarity:Computing unique samples...\r\nINFO:fiftyone.brain.internal.core.sklearn:Generating index for 384 embeddings...\r\nINFO:fiftyone.brain.internal.core.sklearn:Index complete\r\nINFO:fiftyone.brain.similarity:threshold: 1.000000, kept: 3, target: 10\r\nINFO:fiftyone.brain.similarity:threshold: 0.500000, kept: 6, target: 10\r\nComputing unique samples...\r\nGenerating index for 384 embeddings...\r\nIndex complete\r\nthreshold: 1.000000, kept: 3, target: 10\r\nthreshold: 0.500000, kept: 6, target: 10\r\nthreshold: 0.250000, kept: 12, target: 10\r\nthreshold: 0.375000, kept: 7, target: 10\r\nthreshold: 0.312500, kept: 9, target: 10\r\nthreshold: 0.281250, kept: 11, target: 10\r\nINFO:fiftyone.brain.similarity:threshold: 0.250000, kept: 12, target: 10\r\nINFO:fiftyone.brain.similarity:threshold: 0.375000, kept: 7, target: 10\r\nINFO:fiftyone.brain.similarity:threshold: 0.312500, kept: 9, target: 10\r\nINFO:fiftyone.brain.similarity:threshold: 0.281250, kept: 11, target: 10\r\nINFO:fiftyone.brain.similarity:threshold: 0.296875, kept: 9, target: 10\r\nINFO:fiftyone.brain.similarity:threshold: 0.289062, kept: 9, target: 10\r\nINFO:fiftyone.brain.similarity:threshold: 0.285156, kept: 10, target: 10\r\nINFO:fiftyone.brain.similarity:Uniqueness computation complete\r\nWARNING:fiftyone.core.collections:Directory '/tmp/gradio/extraction_61cafcb3-c183-4557-b943-f27e15cf5982' already exists; export will be merged with existing files\r\nthreshold: 0.296875, kept: 9, target: 10\r\nthreshold: 0.289062, kept: 9, target: 10\r\nthreshold: 0.285156, kept: 10, target: 10\r\nUniqueness computation complete\r\nDirectory '/tmp/gradio/extraction_61cafcb3-c183-4557-b943-f27e15cf5982' already exists; export will be merged with existing files\r\n 100% |█████████| 10/10 [9.5ms elapsed, 0s remaining, 1.1K samples/s] \r\nINFO:eta.core.utils: 100% |█████████| 10/10 [9.5ms elapsed, 0s remaining, 1.1K samples/s] \r\n0it [00:00, ?it/s]No acceptable contours found.\r\nNo acceptable contours found.\r\nNo acceptable contours found.\r\nNo acceptable contours found.\r\nNo acceptable contours found.\r\nNo acceptable contours found.\r\nNo acceptable contours found.\r\nNo acceptable contours found.\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\r\n  warnings.warn(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\r\n  warnings.warn(msg)\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\r\n  warnings.warn(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\r\n  warnings.warn(msg)\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\r\n  warnings.warn(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\r\n  warnings.warn(msg)\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\r\n  warnings.warn(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\r\n  warnings.warn(msg)\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\r\n  warnings.warn(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\r\n  warnings.warn(msg)\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\r\n  warnings.warn(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\r\n  warnings.warn(msg)\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\r\n  warnings.warn(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\r\n  warnings.warn(msg)\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\r\n  warnings.warn(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\r\n  warnings.warn(msg)\r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "Seems to be stuck in DocTR for me.\r\n\r\nI should disable DocTR if have CPU mode."
      },
      {
        "user": "pseudotensor",
        "body": "![Uploading image.png…]()\r\n\r\n\r\nshould work now, not hang at least."
      }
    ]
  },
  {
    "issue_number": 1643,
    "title": "Q and A not working for Youtube ",
    "author": "anushaharish538",
    "state": "closed",
    "created_at": "2024-05-23T17:56:54Z",
    "updated_at": "2024-05-28T01:31:55Z",
    "labels": [
      "type/question"
    ],
    "body": "Hi,\r\n\r\nwhen I add YouTube video url and click on ingest it says docs:1 chunks:3, but when I ask any question about the video its connecting to internet and giving answer. It's working fine for pdf.\r\n\r\nRegards,\r\nAnusha",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Review the document viewer and what is in the database.  Nominally for youtube video, will get both 10 frames + audio transcription.\r\n\r\nWhat does the console say when you upload a video?  It should show lots of details if you ran in --verbose mode."
      },
      {
        "user": "anushaharish538",
        "body": "@pseudotensor  Thank you for the reply. I'm using CPU. will  the YouTube chat works with CPU?\r\n\r\nI'm getting the following error.\r\nTraceback (most recent call last):\r\n  File \"/home/anushaharish538/as/h2ogpt/src/gpt_langchain.py\", line 4034, in file_to_doc\r\n    docs1c = model_loaders['asr'].load(from_youtube=True)\r\n  File \"/home/anushaharish538/as/h2ogpt/src/audio_langchain.py\", line 387, in load\r\n    self.load_model()\r\n  File \"/home/anushaharish538/as/h2ogpt/src/audio_langchain.py\", line 368, in load_model\r\n    self.model = OpenAIWhisperParserLocal(device=self.device,\r\n  File \"/home/anushaharish538/as/h2ogpt/src/audio_langchain.py\", line 183, in __init__\r\n    self.pipe = pipeline(\r\n  File \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/__init__.py\", line 906, in pipeline\r\n    framework, model = infer_framework_load_model(\r\n  File \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\r\n    model = model_class.from_pretrained(model, **kwargs)\r\n  File \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\r\n    return model_class.from_pretrained(\r\n  File \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3709, in from_pretrained\r\n    check_tied_parameters_on_same_device(tied_params, device_map)\r\n  File \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 598, in check_tied_parameters_on_same_device\r\n    tie_param_devices[param] = _get_param_device(param, device_map)\r\n  File \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 580, in _get_param_device\r\n    return _get_param_device(parent_param, device_map)\r\n  File \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 580, in _get_param_device\r\n    return _get_param_device(parent_param, device_map)\r\n  File \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 580, in _get_param_device\r\n    return _get_param_device(parent_param, device_map)\r\n  [Previous line repeated 1 more time]\r\n  File \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 575, in _get_param_device\r\n    return device_map[param]\r\nTypeError: 'set' object is not subscriptable\r\nASR: 'set' object is not subscriptable: None\r\nFailed to ingest https://www.youtube.com/watch?v=U1JMy1LTSu8 due to Traceback (most recent call last):\r\n  File \"/home/anushaharish538/as/h2ogpt/src/gpt_langchain.py\", line 4853, in path_to_doc1\r\n    res = file_to_doc(file,\r\n  File \"/home/anushaharish538/as/h2ogpt/src/gpt_langchain.py\", line 4081, in file_to_doc\r\n    raise ValueError(\"%s had no valid text and no meta data was parsed: %s\" % (file, str(e)))\r\nValueError: https://www.youtube.com/watch?v=U1JMy1LTSu8 had no valid text and no meta data was parsed: 'set' object is not subscriptable"
      },
      {
        "user": "pseudotensor",
        "body": "Try setting `--asr_gpu=False`"
      },
      {
        "user": "anushaharish538",
        "body": "Thank You for the help. I tried but still same issue. \r\nI'm using open ai\r\nOPENAI_API_KEY=Key python generate.py --base_model=gpt-4o --prompt_type=mixtral --inference_server=openai_chat --score_model=None --append_sources_to_answer=True --langchain_mode=UserData --asr_gpu=False\r\n\r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "anushaharish538",
        "body": "@pseudotensor I think its issue with database . can you please confirm. I'm getting  python src/make_db.pyExceptions: 0/0 []Traceback (most recent call last):  File \"/home/anushaharish538/as/h2ogpt/src/make_db.py\", line 403, in <module>\r\n    H2O_Fire(make_db_main)\r\n  File \"/home/anushaharish538/as/h2ogpt/src/utils.py\", line 73, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/anushaharish538/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/home/anushaharish538/as/h2ogpt/src/make_db.py\", line 389, in make_db_main\r\n    assert len(sources) > 0 or not fail_if_no_sources, \"No sources found\"\r\nAssertionError: No sources found\r\n\r\nIt's issue with only YouTube . Pdfs working good"
      },
      {
        "user": "anushaharish538",
        "body": "@pseudotensor  can you please tell me , will the YouTube chat works with CPU?\r\n\r\nThank you.\r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "asr_gpu -> False already if no GPUs, so it's not that."
      }
    ]
  },
  {
    "issue_number": 1567,
    "title": "No way to save prompt/response pairs in a database?",
    "author": "mohammedouhibi",
    "state": "closed",
    "created_at": "2024-04-17T16:05:20Z",
    "updated_at": "2024-05-26T19:49:08Z",
    "labels": [],
    "body": "HI,\r\nI'm currently trying to fine tune an LLM that's interacting with users, and i am able to log what comes in and out of the TCP port, however that's not sufficient since i'm not logging things like settings, system prompt, relevant information from text embeddings database present in a given prompt...\r",
    "comments": []
  },
  {
    "issue_number": 1644,
    "title": "sentence transformer version",
    "author": "chengchu88",
    "state": "closed",
    "created_at": "2024-05-25T02:21:01Z",
    "updated_at": "2024-05-25T14:59:29Z",
    "labels": [
      "type/question"
    ],
    "body": "Hello all,\r\nJust wondering, is newer sentence transformer (2.3.0 or newer) compatible with the current h2ogpt? I upgraded it since I need to try out a new embedding model, \"Salesforce/SFR-Embedding-Mistral\". But afterwards gen.py and make_db.py got issue with hkunlp embedding model.\r\nthanks,\r\nC",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "There's some things in langchain etc. that required <2.3.0 so we  have:\r\n```\r\nreqs_optional/requirements_optional_langchain.txt:sentence_transformers>=2.2.2,<2.3.0\r\n```"
      },
      {
        "user": "chengchu88",
        "body": "Thank you! "
      }
    ]
  },
  {
    "issue_number": 1642,
    "title": "Executing small model but missing config.json error with microsoft/Phi-3-mini-4k-instruct-gguf",
    "author": "harnalashok",
    "state": "open",
    "created_at": "2024-05-23T05:55:57Z",
    "updated_at": "2024-05-23T06:40:21Z",
    "labels": [],
    "body": "I have installed h2ogpt on ubuntu22.04 as per procedure. But when I run the following command I get an error of missing **config.json** file. Please let me know how to overcome this error.\r\n\r\nThe command executed is this:\r\n`python generate.py --base_model=microsoft/Phi-3-mini-4k-instruct-gguf  --max_seq_len=4096`\r\n\r\nComplete error trace is as below: \r\n\r\n`python generate.py --base_model=microsoft/Phi-3-mini-4k-instruct-gguf  --max_seq_len=4096\r\n/home/ashok/anaconda3/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nsoundfile, librosa, and wavio not installed, disabling STT\r\nsoundfile, librosa, and wavio not installed, disabling TTS\r\nUsing Model microsoft/phi-3-mini-4k-instruct-gguf\r\nload INSTRUCTOR_Transformer\r\nmax_seq_length  512\r\nMust install DocTR and LangChain installed if enabled DocTR, disabling\r\nStarting get_model: microsoft/Phi-3-mini-4k-instruct-gguf \r\n/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nNot using tokenizer from HuggingFace:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\r\n    response.raise_for_status()\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/requests/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/config.json\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py\", line 399, in cached_file\r\n    resolved_file = hf_hub_download(\r\n                    ^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\r\n    return _hf_hub_download_to_cache_dir(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1282, in _hf_hub_download_to_cache_dir\r\n    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(\r\n                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1722, in _get_metadata_or_catch_error\r\n    metadata = get_hf_file_metadata(url=url, proxies=proxies, timeout=etag_timeout, headers=headers)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1645, in get_hf_file_metadata\r\n    r = _request_wrapper(\r\n        ^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 372, in _request_wrapper\r\n    response = _request_wrapper(\r\n               ^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 396, in _request_wrapper\r\n    hf_raise_for_status(response)\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 315, in hf_raise_for_status\r\n    raise EntryNotFoundError(message, response) from e\r\nhuggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-664ed9aa-1e37434a62ea5f780ef151e5;bfa58ffc-1dbc-47db-a17b-697e66705b9e)\r\n\r\nEntry Not Found for url: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/config.json.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/h2ogpt/src/gen.py\", line 2387, in get_config\r\n    config = AutoConfig.from_pretrained(base_model, token=use_auth_token,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 934, in from_pretrained\r\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n                           ^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py\", line 453, in cached_file\r\n    raise EnvironmentError(\r\nOSError: microsoft/Phi-3-mini-4k-instruct-gguf does not appear to have a file named config.json. Checkout 'https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/tree/main' for available files.\r\nStarting get_model: microsoft/Phi-3-mini-4k-instruct-gguf \r\nNot using tokenizer from HuggingFace:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\r\n    response.raise_for_status()\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/requests/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/config.json\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py\", line 399, in cached_file\r\n    resolved_file = hf_hub_download(\r\n                    ^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\r\n    return _hf_hub_download_to_cache_dir(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1282, in _hf_hub_download_to_cache_dir\r\n    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(\r\n                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1722, in _get_metadata_or_catch_error\r\n    metadata = get_hf_file_metadata(url=url, proxies=proxies, timeout=etag_timeout, headers=headers)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1645, in get_hf_file_metadata\r\n    r = _request_wrapper(\r\n        ^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 372, in _request_wrapper\r\n    response = _request_wrapper(\r\n               ^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 396, in _request_wrapper\r\n    hf_raise_for_status(response)\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 315, in hf_raise_for_status\r\n    raise EntryNotFoundError(message, response) from e\r\nhuggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-664ed9ab-6f335f376f51ddff618ec59f;6faf03e5-aebf-4d97-93b5-fca456c09f8e)\r\n\r\nEntry Not Found for url: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/config.json.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/h2ogpt/src/gen.py\", line 2387, in get_config\r\n    config = AutoConfig.from_pretrained(base_model, token=use_auth_token,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 934, in from_pretrained\r\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n                           ^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py\", line 453, in cached_file\r\n    raise EnvironmentError(\r\nOSError: microsoft/Phi-3-mini-4k-instruct-gguf does not appear to have a file named config.json. Checkout 'https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/tree/main' for available files.\r\nStarting get_model: microsoft/Phi-3-mini-4k-instruct-gguf \r\nNot using tokenizer from HuggingFace:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\r\n    response.raise_for_status()\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/requests/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/config.json\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py\", line 399, in cached_file\r\n    resolved_file = hf_hub_download(\r\n                    ^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\r\n    return _hf_hub_download_to_cache_dir(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1282, in _hf_hub_download_to_cache_dir\r\n    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(\r\n                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1722, in _get_metadata_or_catch_error\r\n    metadata = get_hf_file_metadata(url=url, proxies=proxies, timeout=etag_timeout, headers=headers)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1645, in get_hf_file_metadata\r\n    r = _request_wrapper(\r\n        ^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 372, in _request_wrapper\r\n    response = _request_wrapper(\r\n               ^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 396, in _request_wrapper\r\n    hf_raise_for_status(response)\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 315, in hf_raise_for_status\r\n    raise EntryNotFoundError(message, response) from e\r\nhuggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-664ed9ac-443700867f5ca8797a99822d;f76d7a7d-72e2-4a21-957f-24192665affc)\r\n\r\nEntry Not Found for url: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/config.json.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/h2ogpt/src/gen.py\", line 2387, in get_config\r\n    config = AutoConfig.from_pretrained(base_model, token=use_auth_token,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 934, in from_pretrained\r\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n                           ^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py\", line 453, in cached_file\r\n    raise EnvironmentError(\r\nOSError: microsoft/Phi-3-mini-4k-instruct-gguf does not appear to have a file named config.json. Checkout 'https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/tree/main' for available files.\r\nStarting get_model: microsoft/Phi-3-mini-4k-instruct-gguf \r\nNot using tokenizer from HuggingFace:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\r\n    response.raise_for_status()\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/requests/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/config.json\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py\", line 399, in cached_file\r\n    resolved_file = hf_hub_download(\r\n                    ^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\r\n    return _hf_hub_download_to_cache_dir(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1282, in _hf_hub_download_to_cache_dir\r\n    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(\r\n                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1722, in _get_metadata_or_catch_error\r\n    metadata = get_hf_file_metadata(url=url, proxies=proxies, timeout=etag_timeout, headers=headers)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1645, in get_hf_file_metadata\r\n    r = _request_wrapper(\r\n        ^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 372, in _request_wrapper\r\n    response = _request_wrapper(\r\n               ^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 396, in _request_wrapper\r\n    hf_raise_for_status(response)\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 315, in hf_raise_for_status\r\n    raise EntryNotFoundError(message, response) from e\r\nhuggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-664ed9ad-059285741225bb7a6fc4dfd0;0d3f6eea-c030-4c14-bcd3-301719c35433)\r\n\r\nEntry Not Found for url: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/config.json.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/h2ogpt/src/gen.py\", line 2387, in get_config\r\n    config = AutoConfig.from_pretrained(base_model, token=use_auth_token,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 934, in from_pretrained\r\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n                           ^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py\", line 453, in cached_file\r\n    raise EnvironmentError(\r\nOSError: microsoft/Phi-3-mini-4k-instruct-gguf does not appear to have a file named config.json. Checkout 'https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/tree/main' for available files.\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\r\n    response.raise_for_status()\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/requests/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/config.json\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py\", line 399, in cached_file\r\n    resolved_file = hf_hub_download(\r\n                    ^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\r\n    return _hf_hub_download_to_cache_dir(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1282, in _hf_hub_download_to_cache_dir\r\n    (url_to_download, etag, commit_hash, expected_size, head_call_error) = _get_metadata_or_catch_error(\r\n                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1722, in _get_metadata_or_catch_error\r\n    metadata = get_hf_file_metadata(url=url, proxies=proxies, timeout=etag_timeout, headers=headers)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 1645, in get_hf_file_metadata\r\n    r = _request_wrapper(\r\n        ^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 372, in _request_wrapper\r\n    response = _request_wrapper(\r\n               ^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py\", line 396, in _request_wrapper\r\n    hf_raise_for_status(response)\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 315, in hf_raise_for_status\r\n    raise EntryNotFoundError(message, response) from e\r\nhuggingface_hub.utils._errors.EntryNotFoundError: 404 Client Error. (Request ID: Root=1-664ed9ad-2faeddca4207a77937c6b8ba;d77ab411-ccbd-42e2-94d6-f0317b2acd66)\r\n\r\nEntry Not Found for url: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/config.json.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/ashok/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/ashok/h2ogpt/src/utils.py\", line 73, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/h2ogpt/src/gen.py\", line 2293, in main\r\n    model0, tokenizer0, device = get_model_retry(reward_type=False,\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/h2ogpt/src/gen.py\", line 2652, in get_model_retry\r\n    model1, tokenizer1, device1 = get_model(**kwargs)\r\n                                  ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/h2ogpt/src/gen.py\", line 3318, in get_model\r\n    return get_hf_model(load_8bit=load_8bit,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/h2ogpt/src/gen.py\", line 3413, in get_hf_model\r\n    config, _, max_seq_len = get_config(base_model, return_model=False, raise_exception=True, **config_kwargs)\r\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/h2ogpt/src/gen.py\", line 2387, in get_config\r\n    config = AutoConfig.from_pretrained(base_model, token=use_auth_token,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 934, in from_pretrained\r\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\r\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\r\n    resolved_config_file = cached_file(\r\n                           ^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py\", line 453, in cached_file\r\n    raise EnvironmentError(\r\nOSError: microsoft/Phi-3-mini-4k-instruct-gguf does not appear to have a file named config.json. Checkout 'https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/tree/main' for available files.\r\n`\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, I'll try to clarify the docs, but as in readme.md and in FAQ.md, for new chat template models when using GGUF, you need to specify the `tokenizer_base_model`.\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/main/docs/FAQ.md#llama-3-or-other-chat-template-based-models\r\n\r\nAlso, `base_model` accepts a few forms of passing urls, TheBloke, etc. for gguf, but not just -gguf, so until I add that, you'll need to use more explicit way of invoking llama.cpp:\r\n\r\ne.g. for Phi it would be to use the direct download link like:\r\n\r\n```\r\npython generate.py  --tokenizer_base_model=microsoft/Phi-3-mini-4k-instruct --base_model=llama --llama_cpp_model=https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf  --max_seq_len=4096 \r\n```\r\n\r\nor you can download it yourself and place it into llamacpp_path folder."
      }
    ]
  },
  {
    "issue_number": 1628,
    "title": "Linux install of h2ogpt--Require corrections in install Instructions",
    "author": "harnalashok",
    "state": "open",
    "created_at": "2024-05-18T10:54:31Z",
    "updated_at": "2024-05-23T05:58:21Z",
    "labels": [],
    "body": "The `h2ogpt`  linux installation method as [given here](https://github.com/h2oai/h2ogpt?tab=readme-ov-file#get-started) is as follows:\r\n### A. Variable export instructions:\r\n\r\n`export PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu118 https://huggingface.github.io/autogptq-index/whl/cu118\"`\r\n`export LLAMA_CUBLAS=1`\r\n`export CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all\"`\r\n`export FORCE_CMAKE=1`\r\n\r\n### B. Then, one is required to run the following seven instructions\r\n#### (numbers are given by me)\r\n\r\n`1. git clone https://github.com/h2oai/h2ogpt.git ` \r\n`2. cd h2ogpt`    \r\n`3. pip install -r requirements.txt`\r\n`4. pip install -r reqs_optional/requirements_optional_langchain.txt`\r\n\r\n`5. pip uninstall llama_cpp_python llama_cpp_python_cuda -y`\r\n`6. pip install -r reqs_optional/requirements_optional_llamacpp_gpt4all.txt --no-cache-dir`\r\n`7. pip install -r reqs_optional/requirements_optional_langchain.urls.txt`\r\n\r\nExecuting instruction 6 and 7 results in the following error:\r\n\r\n` -- Configuring incomplete, errors occurred!`\r\n      \r\n`      *** CMake configuration failed`\r\n`      [end of output]`\r\n  \r\n  `note: This error originates from a subprocess, and is likely not a problem with pip.`\r\n  `ERROR: Failed building wheel for llama-cpp-python`\r\n`Failed to build llama-cpp-python`\r\n`ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects`\r\n\r\nHowever, this error, is avoided if after executing instructions 1 to 4, the terminal is closed and then re-opened again. And then instructions 6 and 7 are executed. Meaning thereby, variable export instructions issued earlier result in generation of error in the execution of instructions 6 and 7.\r\n\r\nThis may please be rechecked at your end and  installation document corrected accordingly.\r\nAshok Kumar Harnal\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "The terminal open-close can't matter.  Probably it's not compiling the cuda version and you are only getting the CPU version.  Can you give an expanded full version of your error from 6-7?"
      },
      {
        "user": "harnalashok",
        "body": "I have repeated the experiment three times. The behavior is the same as narrated by me before that there is a need to `unexport` one of the variables which have been made in **paragraph A.** above _before I execute instruction numbered as 6_ . Here is the complete trace of execution if I do not close the terminal but continue to work in the same terminal. (_But if I close and open the terminal again, the process exceeds successfully. Please those results also below_)\r\n\r\n`(base) ashok@ashok:~/h2ogpt$ pip install -r reqs_optional/requirements_optional_llamacpp_gpt4all.txt --no-cache-dir\r\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu121, https://huggingface.github.io/autogptq-index/whl/cu121\r\nCollecting gpt4all==1.0.5 (from -r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1))\r\n  Downloading gpt4all-1.0.5-py3-none-manylinux1_x86_64.whl.metadata (912 bytes)\r\nCollecting llama-cpp-python==0.2.56 (from -r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 4))\r\n  Downloading llama_cpp_python-0.2.56.tar.gz (36.9 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36.9/36.9 MB 28.8 MB/s eta 0:00:00\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Installing backend dependencies ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nRequirement already satisfied: requests in /home/ashok/anaconda3/lib/python3.11/site-packages (from gpt4all==1.0.5->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1)) (2.31.0)\r\nRequirement already satisfied: tqdm in /home/ashok/anaconda3/lib/python3.11/site-packages (from gpt4all==1.0.5->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1)) (4.66.4)\r\nRequirement already satisfied: typing-extensions>=4.5.0 in /home/ashok/anaconda3/lib/python3.11/site-packages (from llama-cpp-python==0.2.56->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 4)) (4.9.0)\r\nRequirement already satisfied: numpy>=1.20.0 in /home/ashok/anaconda3/lib/python3.11/site-packages (from llama-cpp-python==0.2.56->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 4)) (1.26.4)\r\nCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.56->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 4))\r\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\r\nRequirement already satisfied: jinja2>=2.11.3 in /home/ashok/anaconda3/lib/python3.11/site-packages (from llama-cpp-python==0.2.56->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 4)) (3.1.3)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /home/ashok/anaconda3/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.56->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 4)) (2.1.3)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/ashok/anaconda3/lib/python3.11/site-packages (from requests->gpt4all==1.0.5->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1)) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in /home/ashok/anaconda3/lib/python3.11/site-packages (from requests->gpt4all==1.0.5->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1)) (3.4)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/ashok/anaconda3/lib/python3.11/site-packages (from requests->gpt4all==1.0.5->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1)) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /home/ashok/anaconda3/lib/python3.11/site-packages (from requests->gpt4all==1.0.5->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1)) (2024.2.2)\r\nDownloading gpt4all-1.0.5-py3-none-manylinux1_x86_64.whl (3.6 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 9.1 MB/s eta 0:00:00\r\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.5/45.5 kB 34.3 MB/s eta 0:00:00\r\nBuilding wheels for collected packages: llama-cpp-python\r\n  Building wheel for llama-cpp-python (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\r\n  │ exit code: 1\r\n  ╰─> [45 lines of output]\r\n      *** scikit-build-core 0.9.4 using CMake 3.29.3 (wheel)\r\n      *** Configuring CMake...\r\n      2024-05-22 09:52:26,643 - scikit_build_core - WARNING - Can't find a Python library, got libdir=/home/ashok/anaconda3/lib, ldlibrary=libpython3.11.a, multiarch=x86_64-linux-gnu, masd=None\r\n      loading initial cache file /tmp/tmpnyigg61a/build/CMakeInit.txt\r\n      -- The C compiler identification is GNU 11.4.0\r\n      -- The CXX compiler identification is GNU 11.4.0\r\n      -- Detecting C compiler ABI info\r\n      -- Detecting C compiler ABI info - done\r\n      -- Check for working C compiler: /usr/bin/cc - skipped\r\n      -- Detecting C compile features\r\n      -- Detecting C compile features - done\r\n      -- Detecting CXX compiler ABI info\r\n      -- Detecting CXX compiler ABI info - done\r\n      -- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n      -- Detecting CXX compile features\r\n      -- Detecting CXX compile features - done\r\n      -- Found Git: /usr/bin/git (found version \"2.34.1\")\r\n      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n      -- Found Threads: TRUE\r\n      -- Could not find nvcc, please set CUDAToolkit_ROOT.\r\n      CMake Warning at vendor/llama.cpp/CMakeLists.txt:407 (message):\r\n        cuBLAS not found\r\n      \r\n      \r\n      -- CUDA host compiler is GNU\r\n      CMake Error at vendor/llama.cpp/CMakeLists.txt:835 (get_flags):\r\n        get_flags Function invoked with incorrect arguments for function named:\r\n        get_flags\r\n      \r\n      \r\n      -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\r\n      -- CMAKE_SYSTEM_PROCESSOR: x86_64\r\n      -- x86 detected\r\n      CMake Warning (dev) at CMakeLists.txt:21 (install):\r\n        Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\r\n      This warning is for project developers.  Use -Wno-dev to suppress it.\r\n      \r\n      CMake Warning (dev) at CMakeLists.txt:30 (install):\r\n        Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\r\n      This warning is for project developers.  Use -Wno-dev to suppress it.\r\n      \r\n      -- Configuring incomplete, errors occurred!\r\n      \r\n      *** CMake configuration failed\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for llama-cpp-python\r\nFailed to build llama-cpp-python\r\nERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\r\n`\r\n**Here is what happens if I execute instruction numbered as 6 after I close and open the terminal. No error:**\r\n\r\n`pip install -r reqs_optional/requirements_optional_llamacpp_gpt4all.txt --no-cache-dir\r\nCollecting gpt4all==1.0.5 (from -r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1))\r\n  Downloading gpt4all-1.0.5-py3-none-manylinux1_x86_64.whl.metadata (912 bytes)\r\nCollecting llama-cpp-python==0.2.56 (from -r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 4))\r\n  Downloading llama_cpp_python-0.2.56.tar.gz (36.9 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36.9/36.9 MB 23.2 MB/s eta 0:00:00\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Installing backend dependencies ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\nRequirement already satisfied: requests in /home/ashok/anaconda3/lib/python3.11/site-packages (from gpt4all==1.0.5->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1)) (2.31.0)\r\nRequirement already satisfied: tqdm in /home/ashok/anaconda3/lib/python3.11/site-packages (from gpt4all==1.0.5->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1)) (4.66.4)\r\nRequirement already satisfied: typing-extensions>=4.5.0 in /home/ashok/anaconda3/lib/python3.11/site-packages (from llama-cpp-python==0.2.56->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 4)) (4.9.0)\r\nRequirement already satisfied: numpy>=1.20.0 in /home/ashok/anaconda3/lib/python3.11/site-packages (from llama-cpp-python==0.2.56->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 4)) (1.26.4)\r\nCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.56->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 4))\r\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\r\nRequirement already satisfied: jinja2>=2.11.3 in /home/ashok/anaconda3/lib/python3.11/site-packages (from llama-cpp-python==0.2.56->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 4)) (3.1.3)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /home/ashok/anaconda3/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.56->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 4)) (2.1.3)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/ashok/anaconda3/lib/python3.11/site-packages (from requests->gpt4all==1.0.5->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1)) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in /home/ashok/anaconda3/lib/python3.11/site-packages (from requests->gpt4all==1.0.5->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1)) (3.4)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/ashok/anaconda3/lib/python3.11/site-packages (from requests->gpt4all==1.0.5->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1)) (2.0.7)\r\nRequirement already satisfied: certifi>=2017.4.17 in /home/ashok/anaconda3/lib/python3.11/site-packages (from requests->gpt4all==1.0.5->-r reqs_optional/requirements_optional_llamacpp_gpt4all.txt (line 1)) (2024.2.2)\r\nDownloading gpt4all-1.0.5-py3-none-manylinux1_x86_64.whl (3.6 MB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 25.7 MB/s eta 0:00:00\r\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.5/45.5 kB 31.4 MB/s eta 0:00:00\r\nBuilding wheels for collected packages: llama-cpp-python\r\n  Building wheel for llama-cpp-python (pyproject.toml) ... done\r\n  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.56-cp311-cp311-linux_x86_64.whl size=2827201 sha256=07293d75ff82ed6104572cae4fae96fc4fbb0f896b05211463ffd296aab81204\r\n  Stored in directory: /tmp/pip-ephem-wheel-cache-dxt7ajop/wheels/f5/48/62/014b1a3c38f77df21219f81ed63ca4c09531d52a205b15d8e4\r\nSuccessfully built llama-cpp-python\r\nInstalling collected packages: diskcache, llama-cpp-python, gpt4all\r\nSuccessfully installed diskcache-5.6.3 gpt4all-1.0.5 llama-cpp-python-0.2.56\r\n`"
      },
      {
        "user": "pseudotensor",
        "body": "I see the `- Could not find nvcc, please set CUDAToolkit_ROOT.` and  `cuBLAS not found` that means something is wrong with the cuda installation.\r\n\r\nTry again installing cuda 12.1 and ensure CUDA_HOME is set etc."
      },
      {
        "user": "harnalashok",
        "body": "Installing cuda12.1 solaves the issue. But then I face another error when I execute _python generate.py._ Here is the complete trace. Kindly help:\r\n\r\n`  \r\npython generate.py --base_model=TheBloke/Mistral-7B-Instruct-v0.2-GGUF --prompt_type=mistral --max_seq_len=4096\r\n/home/ashok/anaconda3/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\r\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\r\nsoundfile, librosa, and wavio not installed, disabling STT\r\nsoundfile, librosa, and wavio not installed, disabling TTS\r\nUsing Model llama\r\nload INSTRUCTOR_Transformer\r\nmax_seq_length  512\r\nMust install DocTR and LangChain installed if enabled DocTR, disabling\r\nStarting get_model: llama \r\nFailed to listen to n_gpus: No module named 'llama_cpp_cuda', trying llama_cpp module\r\nggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\r\nggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\r\nggml_init_cublas: found 1 CUDA devices:\r\n  **Device 0: NVIDIA GeForce GTX 1060 with Max-Q Design, compute capability 6.1, VMM: yes**\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from llamacpp_path/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 17\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q5_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q5_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.22 MiB\r\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 4807.05 MiB on device 0: cudaMalloc failed: out of memory\r\nllama_model_load: error loading model: failed to allocate buffer\r\nllama_load_model_from_file: failed to load model\r\nStarting get_model: llama \r\nFailed to listen to n_gpus: No module named 'llama_cpp_cuda', trying llama_cpp module\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from llamacpp_path/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 17\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q5_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q5_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.22 MiB\r\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 4807.05 MiB on device 0: cudaMalloc failed: out of memory\r\nllama_model_load: error loading model: failed to allocate buffer\r\nllama_load_model_from_file: failed to load model\r\nStarting get_model: llama \r\nFailed to listen to n_gpus: No module named 'llama_cpp_cuda', trying llama_cpp module\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from llamacpp_path/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 17\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q5_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q5_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.22 MiB\r\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 4807.05 MiB on device 0: cudaMalloc failed: out of memory\r\nllama_model_load: error loading model: failed to allocate buffer\r\nllama_load_model_from_file: failed to load model\r\nStarting get_model: llama \r\nFailed to listen to n_gpus: No module named 'llama_cpp_cuda', trying llama_cpp module\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from llamacpp_path/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 17\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q5_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = Q5_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.22 MiB\r\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 4807.05 MiB on device 0: cudaMalloc failed: out of memory\r\nllama_model_load: error loading model: failed to allocate buffer\r\nllama_load_model_from_file: failed to load model\r\nTraceback (most recent call last):\r\n  File \"/home/ashok/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/ashok/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/ashok/h2ogpt/src/utils.py\", line 73, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n                                ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/h2ogpt/src/gen.py\", line 2293, in main\r\n    model0, tokenizer0, device = get_model_retry(reward_type=False,\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/h2ogpt/src/gen.py\", line 2652, in get_model_retry\r\n    model1, tokenizer1, device1 = get_model(**kwargs)\r\n                                  ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/h2ogpt/src/gen.py\", line 3303, in get_model\r\n    model, tokenizer_llamacpp, device = get_model_tokenizer_gpt4all(base_model,\r\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/h2ogpt/src/gpt4all_llm.py\", line 34, in get_model_tokenizer_gpt4all\r\n    model, tokenizer, redo, max_seq_len = get_llm_gpt4all(**llama_kwargs)\r\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/h2ogpt/src/gpt4all_llm.py\", line 203, in get_llm_gpt4all\r\n    llm = cls(**model_kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ashok/anaconda3/lib/python3.11/site-packages/pydantic/v1/main.py\", line 341, in __init__\r\n    raise validation_error\r\npydantic.v1.error_wrappers.ValidationError: 1 validation error for H2OLlamaCpp\r\n__root__\r\n  Could not load Llama model from path: llamacpp_path/mistral-7b-instruct-v0.2.Q5_K_M.gguf. Received error Failed to load model from file: llamacpp_path/mistral-7b-instruct-v0.2.Q5_K_M.gguf (type=value_error)\r\n\r\n  `\r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "It means it can't find the file or the file is corrupt.\r\n\r\nThis command works for me:\r\n```\r\npython generate.py --base_model=TheBloke/Mistral-7B-Instruct-v0.2-GGUF --prompt_type=mistral --max_seq_len=4096\r\n```\r\nthat you shared.\r\n\r\nI deleted my llamacpp_path folder and tried again, and it downloads fine, and is then used correctly.\r\n\r\nMaybe at some point in past you got corrupted incomplete version of the file.\r\n\r\nPlease delete the file `llamacpp_path/mistral-7b-instruct-v0.2.Q5_K_M.gguf` and try again.  Or try to use that file with llama.cpp directly and see if that works.  If it does work with llama.cpp, then I'm confused."
      }
    ]
  },
  {
    "issue_number": 1623,
    "title": "EventListener Failure",
    "author": "jaysunl",
    "state": "closed",
    "created_at": "2024-05-16T00:43:17Z",
    "updated_at": "2024-05-20T20:13:38Z",
    "labels": [],
    "body": "Hi, every time I run the python generate.py command, it always results in this error:\r\n\r\nEnd auto-detect HF cache text generation models\r\nBegin auto-detect llama.cpp models\r\nEnd auto-detect llama.cpp models\r\nTraceback (most recent call last):\r\n  File \"/lsc/scratch/eda/jliang2/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/lsc/scratch/eda/jliang2/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/lsc/scratch/eda/jliang2/h2ogpt/src/utils.py\", line 72, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/wleong01/gpt4all/wai_gpt_pdf_file/gpt_env_gguf/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/wleong01/gpt4all/wai_gpt_pdf_file/gpt_env_gguf/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/wleong01/gpt4all/wai_gpt_pdf_file/gpt_env_gguf/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/lsc/scratch/eda/jliang2/h2ogpt/src/gen.py\", line 2277, in main\r\n    go_gradio(**locals().copy())\r\n  File \"/lsc/scratch/eda/jliang2/h2ogpt/src/gradio_runner.py\", line 1331, in go_gradio\r\n    text_output, text_output2, text_outputs = make_chatbots(output_label0, output_label0_model2,\r\n  File \"/lsc/scratch/eda/jliang2/h2ogpt/gradio_utils/prompt_form.py\", line 372, in make_chatbots\r\n    rating1.click(ratingfn1, outputs=rating_text_output, _js=review_js1)\r\nTypeError: EventListener._setup.<locals>.event_trigger() got an unexpected keyword argument '_js'\r\n\r\nAny reason what may be causing this issue?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Looks like a mix-up in your gradio version.  gradio4 should handle that fine.  We detect which gradio you should have.  What does \r\n```\r\npip freeze | grep gradio\r\n```\r\nshow?"
      },
      {
        "user": "jaysunl",
        "body": "Just resolved it. There was an issue with some of my packages not being fully installed properly."
      }
    ]
  },
  {
    "issue_number": 1614,
    "title": "doctr for scanned pdf",
    "author": "InesBenAmor99",
    "state": "open",
    "created_at": "2024-05-13T21:02:10Z",
    "updated_at": "2024-05-19T17:57:06Z",
    "labels": [],
    "body": " I have doctr installed ,  When I upload a photo, it works very well and extracts text perfectly. However, when I upload a scanned PDF, it keeps processing for a very long time without any response or error. What could be missing?  \r\n\r\nit works also with a scanned pdf but : for exemple first page is not scanned (contains title or a sentence etc) and all the next pages are scanned , it extracts text from all the pages perfectly , but when the whole pdf is scanned it keeps processing without response.\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "By default docTR isn't used if many pages for PDF.  It's possible it's using OCR (unstructured package) instead.  Can you tell from the command line?  I would disable unstructured and OCR from expert panel in UI and try again.  You can disable via CLI as well.\r\n\r\npymupdf is the default loader, unless the PDF is all (by pages) a scanned image based PDF, then it will revert progressively to other backup methods.\r\n\r\ni.e. it does in order:\r\n* pymupdf\r\n* pypdf\r\n* unstructured pdf\r\n* OCR based unstructured pdf\r\n* DocTR\r\n* As html instead of pdf in case file extension is wrong.\r\n\r\nWith CLI, you can disable everything except DocTR and see how goes.  Then narrow down which thing is taking time.  If it's many pages, DocTR does take some time, but I've seen OCR from unstructured take too long and even much longer and worse quality.\r\n\r\nAlso if you have a PDF that you can share that shows the issue, I'm happy to look.  If you need to keep it semi-private, you can email me at jon.mckinney@h2o.ai."
      },
      {
        "user": "InesBenAmor99",
        "body": "Hello, thank you for responding. Actually, it's not for a specific PDF as mentioned. In general, if the PDF contains a title or something similar that is not scanned, and all the subsequent pages are scanned, it extracts  , doctr works and extract text from the entire file. If the whole PDF is scanned, it continues processing without a response. I'm showing you an example in this video: The first document contains 7 pages. The first page contains a title that is not scanned, and the next 6 pages are all scanned. However, the tool extracts text from all pages (I can verify this through the document viewer ==> view database text). The second document is the document \"image_based_pdf_sample\" found in the test folder in h2ogpt, which is similar to my problematic documents. As you can see, it continues processing without responding. I shortened the video so I could share it here, but it's still processing, as shown in the screenshot. \r\n\r\nhttps://github.com/h2oai/h2ogpt/assets/168529190/25c21a9c-a6ef-4ff9-938f-b53d6d92d411\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/168529190/aee1f025-31d5-42a4-9cec-8e61dff7f129)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Can you do two things?\r\n\r\n1) Add --verbose to he CLI options\r\n2) do ps -auxwf |grep -A 5 -B 5 generate and share to see what is running.\r\n3) When it's stuck, do kill -s SIGUSR1 <pid> for the <pid> that is the \"generate.py\" pid from above or a deeper fork (lower in list below generate in tree) and share the full output that goes to console.  It will include many threads and that may show where it's stuck.\r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Also, can you try gpt.h2o.ai -- do you have similar issues?  This will help identify if it is an installation or computer issue."
      },
      {
        "user": "InesBenAmor99",
        "body": "Honestly, I didn't get the steps mentioned as I'm on a Windows VM, so I think the equivalent is to check running processes? But I'm sharing with you what is happening when I run with --verbose. \r\n1) for a whole scanned pdf ( only doctr option is selected ) : \r\n![image](https://github.com/h2oai/h2ogpt/assets/168529190/ba54f60f-aab7-4c45-bda4-6747b837dd86)\r\n2) for a pdf that containts a title that is not scanned and all other pages scanned : \r\n![image](https://github.com/h2oai/h2ogpt/assets/168529190/a923c867-2f3a-474c-867c-ca174f002b03)\r\n\r\n\r\nOTHER THING : that when i upolad the same whole scanned pdf without selecting doctr option i got this error  \r\n0 [main] python (3368) C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\h2ogpt\\python.exe: *** fatal error - Internal error: TP_NUM_C_BUFS too small: 50\r\nStack trace:\r\nFrame         Function      Args\r\n007D0AA07AE0  00021005FE8E (00021029B0CB, 00021026AB81, 000210314220, 007D0AA05400) msys-2.0.dll+0x1FE8E\r\n007D0AA07AE0  0002100467F9 (02254D95B100, 02254D95B150, 000000000012, 02254D95B150) msys-2.0.dll+0x67F9\r\n007D0AA07AE0  000210046832 (000000000032, 0000000010A1, 000210314220, 0010000004C0) msys-2.0.dll+0x6832\r\n007D0AA07AE0  0002100E45FF (02254D95B180, 02254D95B100, 02254D95B0F0, 000000000012) msys-2.0.dll+0xA45FF\r\n007D0AA07AE0  000210096063 (7FF84D462DF3, 000000000000, 000210314220, 001000000010) msys-2.0.dll+0x56063\r\n007D0AA07AE0  00021005813C (000000000001, 000000000030, 000000000000, 001000000010) msys-2.0.dll+0x1813C\r\n007D0AA07AE0  0002100D4B6E (000000000001, 000000000016, 0010000003E0, 022402350486) msys-2.0.dll+0x94B6E\r\n007D0AA07AE0  000210193F2B (000000000001, 000000000016, 0010000003E0, 022402350486) msys-2.0.dll+0x153F2B\r\n007D0AA07AE0  02240233215C (02243EEFEC48, 000000000006, 007D0AA07A28, 007D0AA07AF0) msys-magic-1.dll+0x215C\r\n007D0AA07AE0  022402336947 (000000000000, 000000000001, 7FF81049362B, 007D0AA07B10) msys-magic-1.dll+0x6947\r\n007D0AA07AE0  7FF825BF4541 (00000000000A, 007D0AA07D30, 7FF825BF4262, 000000000000) ffi-8.dll+0x4541\r\n007D0AA07B10  7FF825BF4332 (000000000002, 02240A489320, 007D0AA07D70, 000000000000) ffi-8.dll+0x4332\r\n007D0AA07D70  7FF825BF4212 (7FF81048DFF7, 02254DA33600, 02240A489320, 007D0AA07D10) ffi-8.dll+0x4212\r\n007D0AA07D70  7FF82511B41F (0224023319F0, 007D0AA07D20, 000000000000, 022400000000) _ctypes.pyd+0xB41F\r\n007D0AA07D70  7FF82511BE3E (02254DA38930, 02254DA38520, 000000000000, 007D00001101) _ctypes.pyd+0xBE3E\r\n000000000002  7FF825116958 (02254DA37A00, 000000000000, 022400000000, 02254DA370A0) _ctypes.pyd+0x6958\r\n000000000000  7FF8104ADEE8 (007D0AA08030, 0224D1C09630, 0224D1C09630, 000000000000) python310.dll+0x10DEE8\r\n007D0AA08030  7FF8105B6B12 (7FF81080A140, 007D0AA08030, 0224432C72B0, 000000000000) python310.dll+0x216B12\r\n007D0AA08030  7FF8105B2A18 (02254DA1F800, 02254DA1F800, 007D0AA08128, 000000000001) python310.dll+0x212A18\r\n000000000043  7FF8105B5214 (02254DA1F7F0, 02254D6F7448, 0224C504E0A0, 000000000001) python310.dll+0x215214\r\n007D0AA082B0  7FF8104AE29E (000087EB29E6, 02243F594F30, 0000000000A0, 02254DA44A00) python310.dll+0x10E29E\r\n007D0AA082B0  7FF8105AE541 (02254D6F7450, 022591C1260A, 02254D6F72E0, 000000000000) python310.dll+0x20E541\r\n007D0AA082B0  7FF8105B6B12 (02254DA408B0, 007D0AA082B0, 007D0AA082B0, 000000000000) python310.dll+0x216B12\r\n007D0AA082B0  7FF8105B2FFD (007D0AA083A0, 007D0AA083A0, 7FF8105B0219, 000000000000) python310.dll+0x212FFD\r\n000000000040  7FF8105B5214 (02254DA367C0, 02254DA367C0, 7FF81080EB90, 000000000000) python310.dll+0x215214\r\n02254DA3C5B0  7FF8105AFEE2 (02243F53BD40, 02254DA3C5B0, 000000000010, 00001B000219) python310.dll+0x20FEE2\r\n02243F53BD40  7FF8105AAEBE (02243F5350D0, 02243EEC0000, 02243EEC0000, 000000000000) python310.dll+0x20AEBE\r\n02254DA37918  7FF8105A8888 (7FF810809AC0, 02240A489320, 02243F5350D0, 02240A489320) python310.dll+0x208888\r\n02254DA37918  7FF8104F20F9 (007D0AA085F0, 02243F5350D0, 02254D6F7CE0, 0000000000A4) python310.dll+0x1520F9\r\n007D0AA085F0  7FF8105B6DEC (02254DA36640, 007D0AA08620, 02254DA37908, 02254DA36700) python310.dll+0x216DEC\r\n007D0AA08620  7FF8105B1E04 (02243F5015B0, 02243F5015B0, 02254DA323E0, 000000000003) python310.dll+0x211E04\r\n00000000004F  7FF8105B5214 (02243F5015A0, 022520D6BB88, 7FF810809AC0, 000000000003) python310.dll+0x215214\r\nEnd of stack trace (more stack frames may be present)\r\nLoaded modules:\r\n7FF7BF570000 python.exe\r\n7FF84D440000 ntdll.dll\r\n7FF84BD30000 KERNEL32.DLL\r\n7FF84AFD0000 KERNELBASE.dll\r\n7FF84A9C0000 ucrtbase.dll\r\n7FF83FB80000 VCRUNTIME140.dll\r\n7FF8103A0000 python310.dll\r\n7FF84C670000 WS2_32.dll\r\n7FF84CCA0000 RPCRT4.dll\r\n7FF84BC60000 ADVAPI32.dll\r\n7FF84D210000 msvcrt.dll\r\n7FF84D2D0000 sechost.dll\r\n7FF840230000 VERSION.dll\r\n7FF83F070000 zlib.dll\r\n7FF84AD50000 bcrypt.dll\r\n7FF84A120000 CRYPTSP.dll\r\n7FF849A30000 rsaenh.dll\r\n7FF84A140000 CRYPTBASE.dll\r\n7FF84AB70000 bcryptPrimitives.dll\r\n02243F0B0000 python3.DLL\r\n7FF8479C0000 _queue.pyd\r\n7FF845500000 _uuid.pyd\r\n7FF845460000 _hashlib.pyd\r\n7FF80E690000 libcrypto-3-x64.dll\r\n7FF84ABF0000 CRYPT32.dll\r\n7FF84B8B0000 USER32.dll\r\n7FF84B340000 win32u.dll\r\n7FF84B5C0000 GDI32.dll\r\n7FF84AD80000 gdi32full.dll\r\n7FF84AAD0000 msvcp_win.dll\r\n7FF84B430000 IMM32.DLL\r\n7FF825A00000 _socket.pyd\r\n7FF849C10000 IPHLPAPI.DLL\r\n7FF83F060000 select.pyd\r\n7FF8259D0000 _ssl.pyd\r\n7FF81E6E0000 libssl-3-x64.dll\r\n7FF82B210000 _bz2.pyd\r\n7FF8258D0000 LIBBZ2.dll\r\n7FF82B1E0000 _lzma.pyd\r\n7FF825180000 liblzma.dll\r\n7FF824AA0000 _brotli.cp310-win_amd64.pyd\r\n7FF81E5C0000 unicodedata.pyd\r\n7FF825140000 _cffi_backend.cp310-win_amd64.pyd\r\n7FF825110000 _ctypes.pyd\r\n7FF84BB20000 ole32.dll\r\n7FF84CEA0000 combase.dll\r\n7FF84CDC0000 OLEAUT32.dll\r\n7FF825BF0000 ffi-8.dll\r\n7FF8250F0000 _asyncio.pyd\r\n7FF8259C0000 _overlapped.pyd\r\n7FF849ED0000 mswsock.dll\r\n7FF84BF10000 shell32.DLL\r\n7FF824990000 _decimal.pyd\r\n7FF80BD40000 _pydantic_core.cp310-win_amd64.pyd\r\n7FF820E60000 backend_c.cp310-win_amd64.pyd\r\n7FF825960000 md.cp310-win_amd64.pyd\r\n7FF824A70000 md__mypyc.cp310-win_amd64.pyd\r\n7FF824C90000 _sqlite3.pyd\r\n7FF815CC0000 sqlite3.dll\r\n7FF825880000 _mklinit.cp310-win_amd64.pyd\r\n7FF806CE0000 mkl_rt.2.dll\r\n7FF821CF0000 _py_mkl_service.cp310-win_amd64.pyd\r\n7FF8026D0000 mkl_intel_thread.2.dll\r\n7FFFEB5F0000 mkl_core.2.dll\r\n7FF814E50000 _multiarray_umath.cp310-win_amd64.pyd\r\n7FF81E5A0000 _multiarray_tests.cp310-win_amd64.pyd\r\n7FF81E580000 _umath_linalg.cp310-win_amd64.pyd\r\n7FF81E440000 _pocketfft_internal.cp310-win_amd64.pyd\r\n7FF8166C0000 mtrand.cp310-win_amd64.pyd\r\n7FF817470000 bit_generator.cp310-win_amd64.pyd\r\n7FF8171A0000 _common.cp310-win_amd64.pyd\r\n7FF815C70000 _bounded_integers.cp310-win_amd64.pyd\r\n7FF81C0A0000 _mt19937.cp310-win_amd64.pyd\r\n7FF817020000 _philox.cp310-win_amd64.pyd\r\n7FF817000000 _pcg64.cp310-win_amd64.pyd\r\n7FF8253D0000 _sfc64.cp310-win_amd64.pyd\r\n7FF815BC0000 _generator.cp310-win_amd64.pyd\r\n7FF800170000 mkl_def.2.dll\r\n7FF80B4A0000 mkl_vml_def.2.dll\r\n7FF8250E0000 _multiprocessing.pyd\r\n7FF8153C0000 interval.cp310-win_amd64.pyd\r\n7FF814360000 hashtable.cp310-win_amd64.pyd\r\n7FF8168E0000 missing.cp310-win_amd64.pyd\r\n7FF8166A0000 dtypes.cp310-win_amd64.pyd\r\n7FF8164D0000 np_datetime.cp310-win_amd64.pyd\r\n7FF815B90000 conversion.cp310-win_amd64.pyd\r\n7FF824A60000 base.cp310-win_amd64.pyd\r\n7FF815B60000 nattype.cp310-win_amd64.pyd\r\n7FF8159D0000 tzconversion.cp310-win_amd64.pyd\r\n7FF8159A0000 timezones.cp310-win_amd64.pyd\r\n7FF821CC0000 _zoneinfo.pyd\r\n022443EA0000 tzres.dll\r\n7FF815870000 parsing.cp310-win_amd64.pyd\r\n7FF8142C0000 offsets.cp310-win_amd64.pyd\r\n7FF815360000 timedeltas.cp310-win_amd64.pyd\r\n7FF814E10000 fields.cp310-win_amd64.pyd\r\n7FF81E570000 ccalendar.cp310-win_amd64.pyd\r\n7FF814250000 timestamps.cp310-win_amd64.pyd\r\n7FF81E430000 properties.cp310-win_amd64.pyd\r\n7FF814200000 period.cp310-win_amd64.pyd\r\n7FF815840000 vectorized.cp310-win_amd64.pyd\r\n7FF817280000 ops_dispatch.cp310-win_amd64.pyd\r\n7FF80F5A0000 algos.cp310-win_amd64.pyd\r\n7FF8139C0000 lib.cp310-win_amd64.pyd\r\n7FF8141C0000 tslib.cp310-win_amd64.pyd\r\n7FF814DE0000 hashing.cp310-win_amd64.pyd\r\n7FF80B180000 lib.cp310-win_amd64.pyd\r\n7FF813330000 MSVCP140.dll\r\n7FF80DA10000 arrow_python.dll\r\n7FFFE9670000 arrow.dll\r\n7FF816580000 VCRUNTIME140_1.dll\r\n7FF849FD0000 USERENV.dll\r\n7FF822660000 WININET.dll\r\n7FF843F70000 WINHTTP.dll\r\n7FF83D650000 Secur32.dll\r\n7FF84A240000 ncrypt.dll\r\n7FF84A3C0000 SSPICLI.DLL\r\n7FF84A200000 NTASN1.dll\r\n7FF813990000 ops.cp310-win_amd64.pyd\r\n7FF813300000 interpreter.cp310-win_amd64.pyd\r\n7FF849360000 kernel.appcore.dll\r\n7FF812A40000 _compute.cp310-win_amd64.pyd\r\n7FF8164B0000 arrays.cp310-win_amd64.pyd\r\n7FF810330000 index.cp310-win_amd64.pyd\r\n7FF80D4E0000 join.cp310-win_amd64.pyd\r\n7FF80F4E0000 sparse.cp310-win_amd64.pyd\r\n7FF815350000 reduction.cp310-win_amd64.pyd\r\n7FF8141B0000 indexing.cp310-win_amd64.pyd\r\n7FF8132C0000 internals.cp310-win_amd64.pyd\r\n7FF812A10000 writers.cp310-win_amd64.pyd\r\n7FF80E640000 aggregations.cp310-win_amd64.pyd\r\n7FF80FB40000 indexers.cp310-win_amd64.pyd\r\n7FF80E570000 reshape.cp310-win_amd64.pyd\r\n7FF80E530000 strptime.cp310-win_amd64.pyd\r\n7FF80AFF0000 groupby.cp310-win_amd64.pyd\r\n7FF8129F0000 testing.cp310-win_amd64.pyd\r\n7FF80DFA0000 parsers.cp310-win_amd64.pyd\r\n7FF80F4C0000 json.cp310-win_amd64.pyd\r\n7FF813F90000 _version.cp310-win_amd64.pyd\r\n7FF80E500000 _frame.cp310-win_amd64.pyd\r\n7FF80E620000 _psutil_windows.pyd\r\n7FF84D2C0000 PSAPI.DLL\r\n7FF84A890000 POWRPROF.dll\r\n7FF83C730000 pdh.dll\r\n7FF84A870000 UMPDC.dll\r\n7FF847720000 wtsapi32.dll\r\n7FF80DCA0000 _elementtree.pyd\r\n7FF80DC60000 pyexpat.pyd\r\n7FF80DC00000 gobject-2.0-0.dll\r\n7FF80CCC0000 glib-2.0-0.dll\r\n7FF80DBC0000 intl-8.dll\r\n7FF80D3F0000 iconv.dll\r\n7FF80AF60000 pcre2-8.dll\r\n7FF80FB30000 charset.dll\r\n7FF80AF00000 pango-1.0-0.dll\r\n7FF80D9F0000 fribidi-0.dll\r\n7FF80AD70000 gio-2.0-0.dll\r\n7FF80AC60000 harfbuzz.dll\r\n7FF84B370000 SHLWAPI.dll\r\n7FF80D9D0000 USP10.dll\r\n7FF80ABB0000 freetype.dll\r\n7FF80D3C0000 graphite2.dll\r\n7FF80F4B0000 gmodule-2.0-0.dll\r\n7FF849C40000 DNSAPI.dll\r\n7FF80AB70000 libpng16.dll\r\n7FF84BA80000 NSI.dll\r\n7FF80AB20000 fontconfig-1.dll\r\n7FF80AAB0000 libexpat.dll\r\n7FF80CCA0000 pangoft2-1.0-0.dll\r\n7FF80AA40000 bezierTools.cp310-win_amd64.pyd\r\n7FF80DBB0000 shfolder.dll\r\n7FF841FF0000 windows.storage.dll\r\n7FF84B7C0000 SHCORE.dll\r\n7FF80A9D0000 _imaging.cp310-win_amd64.pyd\r\n7FF80A970000 openjp2.dll\r\n7FF80A8F0000 tiff.dll\r\n7FF80A820000 jpeg8.dll\r\n7FF80A7E0000 deflate.dll\r\n7FF80A730000 zstd.dll\r\n7FF80A6A0000 Lerc.dll\r\n7FF80A2B0000 etree.cp310-win_amd64.pyd\r\n7FF80A280000 _elementpath.cp310-win_amd64.pyd\r\n7FF80A250000 iup.cp310-win_amd64.pyd\r\n7FF80A1F0000 asmjit.dll\r\n7FF809FD0000 c10.dll\r\n7FF835FC0000 dbghelp.dll\r\n7FF809F60000 c10_cuda.dll\r\n7FF806C50000 cudart64_110.dll\r\n7FF80CC90000 caffe2_nvrtc.dll\r\n7FFFE6F00000 nvrtc64_112_0.dll\r\n7FFFE1A50000 cublas64_11.dll\r\n7FFFACE80000 cublasLt64_11.dll\r\n7FF80D670000 nvcuda.dll\r\n7FF809F10000 cudnn64_8.dll\r\n7FF848C50000 uxtheme.dll\r\n7FFFD9DA0000 cudnn_adv_infer64_8.dll\r\n7FFFA7260000 cudnn_ops_infer64_8.dll\r\n7FF84BE60000 clbcatq.dll\r\n7FFF9FE00000 cudnn_adv_train64_8.dll\r\n7FFFD0060000 cudnn_ops_train64_8.dll\r\n7FFF72CE0000 cudnn_cnn_infer64_8.dll\r\n000010000000 zlibwapi.dll\r\n7FFF9A2A0000 cudnn_cnn_train64_8.dll\r\n7FFF61F50000 cufft64_10.dll\r\n7FF806C00000 cufftw64_10.dll\r\n7FF8067D0000 cupti64_2022.3.0.dll\r\n7FFF5DAB0000 curand64_10.dll\r\n7FFF4BDF0000 cusolver64_11.dll\r\n7FFF41040000 cusolverMg64_11.dll\r\n7FFF30790000 cusparse64_11.dll\r\n7FFFD9490000 fbgemm.dll\r\n7FF806220000 libiomp5md.dll\r\n7FF809F00000 libiompstubs5md.dll\r\n7FFFD8CF0000 nvrtc-builtins64_118.dll\r\n7FF809EF0000 nvToolsExt64_1.dll\r\n7FF806760000 shm.dll\r\n7FFED9290000 torch_cpu.dll\r\n7FFFD8540000 uv.dll\r\n7FFEE86A0000 torch_cuda.dll\r\n7FF806750000 torch.dll\r\n7FF806740000 torch_global_deps.dll\r\n7FFFD7690000 torch_python.dll\r\n7FF806730000 _C.cp310-win_amd64.pyd\r\n7FF845EB0000 mrmcorer.dll\r\n7FF840270000 iertutil.dll\r\n7FF83FC80000 windows.staterepositorycore.dll\r\n7FF84A8F0000 profapi.dll\r\n7FF847E40000 bcp47mrm.dll\r\n7FF84A5D0000 CFGMGR32.dll\r\n7FF846C70000 propsys.dll\r\n7FF806720000 _c_internal_utils.cp310-win_amd64.pyd\r\n7FF8066F0000 _path.cp310-win_amd64.pyd\r\n7FF805BC0000 msvcp140-456d948669199b545d061b84c160bebc.dll\r\n7FF8000D0000 ft2font.cp310-win_amd64.pyd\r\n7FF806650000 _cext.cp310-win_amd64.pyd\r\n7FF806620000 _image.cp310-win_amd64.pyd\r\n7FFED4AA0000 cv2.pyd\r\n7FF821AF0000 MFPlat.DLL\r\n7FF8066E0000 WSOCK32.dll\r\n7FFFD7570000 MFReadWrite.dll\r\n7FF82F3C0000 MF.dll\r\n7FF8220F0000 MFCORE.DLL\r\n7FF848AE0000 RTWorkQ.DLL\r\n7FF806200000 _ccallback_c.cp310-win_amd64.pyd\r\n7FFFD7160000 _sparsetools.cp310-win_amd64.pyd\r\n7FF800030000 _csparsetools.cp310-win_amd64.pyd\r\n7FF8061B0000 _iterative.cp310-win_amd64.pyd\r\n7FFFD52A0000 libopenblas_v0.3.20-571-g3dec11c6-gcc_10_3_0-c2315440d6b6cef5037bad648efc8c59.dll\r\n7FFFD8C40000 _fblas.cp310-win_amd64.pyd\r\n7FFFD5080000 _flapack.cp310-win_amd64.pyd\r\n7FFFD5000000 _cythonized_array_utils.cp310-win_amd64.pyd\r\n7FFFD4F70000 cython_lapack.cp310-win_amd64.pyd\r\n7FF805B70000 _solve_toeplitz.cp310-win_amd64.pyd\r\n7FFFD4F30000 _decomp_lu_cython.cp310-win_amd64.pyd\r\n7FFFD4EF0000 _matfuncs_sqrtm_triu.cp310-win_amd64.pyd\r\n7FFFD4E80000 _matfuncs_expm.cp310-win_amd64.pyd\r\n7FFFD4E30000 cython_blas.cp310-win_amd64.pyd\r\n7FFFD4DD0000 _decomp_update.cp310-win_amd64.pyd\r\n7FF800010000 _flinalg.cp310-win_amd64.pyd\r\n7FFFD4D60000 _superlu.cp310-win_amd64.pyd\r\n7FFFD4C90000 _arpack.cp310-win_amd64.pyd\r\n7FFFD4C20000 _shortest_path.cp310-win_amd64.pyd\r\n7FFFD4BF0000 _tools.cp310-win_amd64.pyd\r\n7FFFD4B60000 _traversal.cp310-win_amd64.pyd\r\n7FFFD4B20000 _min_spanning_tree.cp310-win_amd64.pyd\r\n7FFFD4AD0000 _flow.cp310-win_amd64.pyd\r\n7FFFD4A80000 _matching.cp310-win_amd64.pyd\r\n7FFFD4A30000 _reordering.cp310-win_amd64.pyd\r\n7FFFD4A10000 _minpack2.cp310-win_amd64.pyd\r\n7FFFD4900000 _group_columns.cp310-win_amd64.pyd\r\n7FFFD48A0000 _trlib.cp310-win_amd64.pyd\r\n7FFFD4880000 messagestream.cp310-win_amd64.pyd\r\n7FFFCFFE0000 _lbfgsb.cp310-win_amd64.pyd\r\n7FFFCFFB0000 _moduleTNC.cp310-win_amd64.pyd\r\n7FFFCFF40000 _cobyla.cp310-win_amd64.pyd\r\n7FFFCFF10000 _slsqp.cp310-win_amd64.pyd\r\n7FFFCFEE0000 _minpack.cp310-win_amd64.pyd\r\n7FFFCFEA0000 givens_elimination.cp310-win_amd64.pyd\r\n7FF8066D0000 _zeros.cp310-win_amd64.pyd\r\n7FFFCFE30000 __nnls.cp310-win_amd64.pyd\r\n7FFFCF9C0000 _highs_wrapper.cp310-win_amd64.pyd\r\n7FFFCF8D0000 _highs_constants.cp310-win_amd64.pyd\r\n7FFFCF800000 _interpolative.cp310-win_amd64.pyd\r\n7FFFCF7B0000 _bglu_dense.cp310-win_amd64.pyd\r\n7FFFCF770000 _lsap.cp310-win_amd64.pyd\r\n7FFF9A100000 _ckdtree.cp310-win_amd64.pyd\r\n7FFFCF660000 _qhull.cp310-win_amd64.pyd\r\n7FFFCF620000 _voronoi.cp310-win_amd64.pyd\r\n7FFF9A0D0000 _distance_wrap.cp310-win_amd64.pyd\r\n7FFF9A090000 _hausdorff.cp310-win_amd64.pyd\r\n7FFF99EA0000 _ufuncs.cp310-win_amd64.pyd\r\n7FFF99D40000 _ufuncs_cxx.cp310-win_amd64.pyd\r\n7FFF99CB0000 _specfun.cp310-win_amd64.pyd\r\n7FFFCF600000 _comb.cp310-win_amd64.pyd\r\n7FFF99C90000 _ellip_harm_2.cp310-win_amd64.pyd\r\n7FFF99B30000 _distance_pybind.cp310-win_amd64.pyd\r\n7FFF99A80000 _rotation.cp310-win_amd64.pyd\r\n7FFF99A60000 _direct.cp310-win_amd64.pyd\r\n7FF8061A0000 _imagingft.cp310-win_amd64.pyd\r\n7FFF994C0000 _C.pyd\r\n7FFF99490000 image.pyd\r\n7FFF990D0000 nvjpeg64_11.dll\r\n7FFF99030000 _safetensors_rust.cp310-win_amd64.pyd\r\n7FFF98FF0000 _yaml.cp310-win_amd64.pyd\r\n00005F310000 _message.pyd\r\n7FFF98D30000 onnx_cpp2py_export.cp310-win_amd64.pyd\r\n7FFF97DC0000 onnxruntime_pybind11_state.pyd\r\n7FF800000000 onnxruntime_providers_shared.dll\r\n7FFF97DA0000 _mio_utils.cp310-win_amd64.pyd\r\n7FFF97D60000 _mio5_utils.cp310-win_amd64.pyd\r\n7FFF97D40000 _streams.cp310-win_amd64.pyd\r\n7FFF97D20000 _errors.cp310-win_amd64.pyd\r\n7FFF979C0000 hdf5.dll\r\n7FFF979A0000 h5.cp310-win_amd64.pyd\r\n7FFF97960000 defs.cp310-win_amd64.pyd\r\n7FFF97930000 hdf5_hl.dll\r\n7FFF97900000 _objects.cp310-win_amd64.pyd\r\n7FFF978C0000 _conv.cp310-win_amd64.pyd\r\n7FFF978A0000 h5r.cp310-win_amd64.pyd\r\n7FFF97830000 h5p.cp310-win_amd64.pyd\r\n7FFF977D0000 h5t.cp310-win_amd64.pyd\r\n7FFF977B0000 utils.cp310-win_amd64.pyd\r\n7FFF97780000 h5s.cp310-win_amd64.pyd\r\n7FFF97760000 h5ac.cp310-win_amd64.pyd\r\n7FFFD8C30000 _proxy.cp310-win_amd64.pyd\r\n7FFF97740000 h5z.cp310-win_amd64.pyd\r\n7FFF97640000 h5a.cp310-win_amd64.pyd\r\n7FFF975F0000 h5d.cp310-win_amd64.pyd\r\n7FFF97720000 h5ds.cp310-win_amd64.pyd\r\n7FFF97570000 h5f.cp310-win_amd64.pyd\r\n7FFF97540000 h5g.cp310-win_amd64.pyd\r\n7FFF975D0000 h5i.cp310-win_amd64.pyd\r\n7FFF97510000 h5fd.cp310-win_amd64.pyd\r\n7FFF97500000 h5pl.cp310-win_amd64.pyd\r\n7FFF97410000 h5o.cp310-win_amd64.pyd\r\n7FFF973F0000 h5l.cp310-win_amd64.pyd\r\n7FFF973C0000 _selector.cp310-win_amd64.pyd\r\n7FFF97120000 _pyopenvino.cp310-win_amd64.pyd\r\n7FFED3F00000 openvino.dll\r\n000180000000 tbb.dll\r\n7FFF970E0000 _pyclipper.cp310-win_amd64.pyd\r\n7FFED3EA0000 geos_c-84d02d5b10aa37e3ab60a885dc5b531b.dll\r\n7FFED3CC0000 geos-2c37bebf00da028367beef6fb37256e0.dll\r\n7FFED3CA0000 _speedups.cp310-win_amd64.pyd\r\n7FFF974F0000 _feature_detector_cpp.cp310-win_amd64.pyd\r\n7FFED3B00000 metrics_cpp_avx2.cp310-win_amd64.pyd\r\n7FFED3AB0000 _initialize_cpp.cp310-win_amd64.pyd\r\n7FFED39C0000 fuzz_cpp_avx2.cp310-win_amd64.pyd\r\n7FFED3920000 process_cpp_impl.cp310-win_amd64.pyd\r\n7FFED38F0000 utils_cpp.cp310-win_amd64.pyd\r\n7FFED38D0000 _vq.cp310-win_amd64.pyd\r\n7FFED3870000 _hierarchy.cp310-win_amd64.pyd\r\n7FFED3810000 _optimal_leaf_ordering.cp310-win_amd64.pyd\r\n7FFED37D0000 _backend_agg.cp310-win_amd64.pyd\r\n7FFED35F0000 _sentencepiece.cp310-win_amd64.pyd\r\n7FFED3020000 tokenizers.cp310-win_amd64.pyd\r\n7FF80C2D0000 nvcuda64.dll\r\n7FF84A360000 msasn1.dll\r\n7FF8401D0000 cryptnet.dll\r\n7FF847520000 drvstore.dll\r\n7FF84A5A0000 devobj.dll\r\n7FF84A1D0000 wldp.dll\r\n7FF833480000 nvapi64.dll\r\n7FF84C6F0000 SETUPAPI.dll\r\n7FFED2FE0000 orjson.cp310-win_amd64.pyd\r\n7FFED27D0000 cygrpc.cp310-win_amd64.pyd\r\n7FFED20E0000 _rust.pyd\r\n7FFED2080000 _bcrypt.pyd\r\n7FFED2040000 _nd_image.cp310-win_amd64.pyd\r\n7FFED1FE0000 _ni_label.cp310-win_amd64.pyd\r\n7FFED1F70000 _odepack.cp310-win_amd64.pyd\r\n7FFED1EF0000 _quadpack.cp310-win_amd64.pyd\r\n7FFED1E60000 _vode.cp310-win_amd64.pyd\r\n7FFED1DE0000 _dop.cp310-win_amd64.pyd\r\n7FFED1D60000 _lsoda.cp310-win_amd64.pyd\r\n7FFED1CB0000 _stats.cp310-win_amd64.pyd\r\n7FFED1A20000 cython_special.cp310-win_amd64.pyd\r\n7FFED1910000 beta_ufunc.cp310-win_amd64.pyd\r\n7FFED1800000 binom_ufunc.cp310-win_amd64.pyd\r\n7FFED16F0000 nbinom_ufunc.cp310-win_amd64.pyd\r\n7FFED15F0000 hypergeom_ufunc.cp310-win_amd64.pyd\r\n7FFED14E0000 ncf_ufunc.cp310-win_amd64.pyd\r\n7FFED13D0000 ncx2_ufunc.cp310-win_amd64.pyd\r\n7FFED12C0000 nct_ufunc.cp310-win_amd64.pyd\r\n7FFED1270000 skewnorm_ufunc.cp310-win_amd64.pyd\r\n7FFED1160000 invgauss_ufunc.cp310-win_amd64.pyd\r\n7FFED10E0000 _fitpack.cp310-win_amd64.pyd\r\n7FFED1030000 dfitpack.cp310-win_amd64.pyd\r\n7FFED0FD0000 _bspl.cp310-win_amd64.pyd\r\n7FFED0F70000 _ppoly.cp310-win_amd64.pyd\r\n7FFED0F10000 interpnd.cp310-win_amd64.pyd\r\n7FFED0DE0000 _rbfinterp_pythran.cp310-win_amd64.pyd\r\n7FFED0D90000 _rgi_cython.cp310-win_amd64.pyd\r\n7FFED0D20000 _biasedurn.cp310-win_amd64.pyd\r\n7FFED0D00000 levyst.cp310-win_amd64.pyd\r\n7FFED0BF0000 _stats_pythran.cp310-win_amd64.pyd\r\n7FFED0BB0000 _uarray.cp310-win_amd64.pyd\r\n7FFED0AA0000 pypocketfft.cp310-win_amd64.pyd\r\n7FFED0A80000 _statlib.cp310-win_amd64.pyd\r\n7FFED0A20000 _sobol.cp310-win_amd64.pyd\r\n7FFED09B0000 _qmc_cy.cp310-win_amd64.pyd\r\n7FFED0890000 _mvn.cp310-win_amd64.pyd\r\n7FFED0840000 rcont.cp310-win_amd64.pyd\r\n7FFED0830000 _multidict.cp310-win_amd64.pyd\r\n7FFED0810000 _quoting_c.cp310-win_amd64.pyd\r\n7FFED07F0000 _helpers.cp310-win_amd64.pyd\r\n7FFED07D0000 _http_writer.cp310-win_amd64.pyd\r\n7FFED0780000 _http_parser.cp310-win_amd64.pyd\r\n7FFED0770000 _websocket.cp310-win_amd64.pyd\r\n7FFED0750000 _frozenlist.cp310-win_amd64.pyd\r\n7FFED0720000 collections.cp310-win_amd64.pyd\r\n7FFED0700000 immutabledict.cp310-win_amd64.pyd\r\n7FFED06E0000 processors.cp310-win_amd64.pyd\r\n7FFED06C0000 resultproxy.cp310-win_amd64.pyd\r\n7FFED06A0000 util.cp310-win_amd64.pyd\r\n7FFED0660000 _greenlet.cp310-win_amd64.pyd\r\n7FFED0650000 _imagingmath.cp310-win_amd64.pyd\r\n7FFED0640000 _webp.cp310-win_amd64.pyd\r\n7FFED0630000 libwebpmux.dll\r\n7FFED0620000 libwebpdemux.dll\r\n7FFED05B0000 libwebp.dll\r\n7FFED05A0000 libsharpyuv.dll\r\n7FFED0580000 _tkinter.pyd\r\n7FFED0400000 tk86t.dll\r\n7FFED0230000 tcl86t.dll\r\n7FF84B470000 COMDLG32.dll\r\n7FF826D80000 COMCTL32.dll\r\n7FF83C850000 NETAPI32.dll\r\n7FFED0200000 zlib1.dll\r\n7FF83DF50000 LOGONCLI.DLL\r\n7FF849D20000 NETUTILS.DLL\r\n7FF8476B0000 SAMCLI.DLL\r\n7FFED0150000 _regex.cp310-win_amd64.pyd\r\n7FFED0140000 _check_build.cp310-win_amd64.pyd\r\n7FFED0100000 vcomp140.dll\r\n7FFED0070000 msvcp140.dll\r\n7FFED0030000 _isfinite.cp310-win_amd64.pyd\r\n7FFED0000000 murmurhash.cp310-win_amd64.pyd\r\n7FFECFFF0000 _openmp_helpers.cp310-win_amd64.pyd\r\n7FFECFF50000 sparsefuncs_fast.cp310-win_amd64.pyd\r\n7FFECFEF0000 _csr_polynomial_expansion.cp310-win_amd64.pyd\r\n7FFECFE80000 _target_encoder_fast.cp310-win_amd64.pyd\r\n7FFECFE60000 _hashing_fast.cp310-win_amd64.pyd\r\n7FFECFE40000 _vector_sentinel.cp310-win_amd64.pyd\r\n7FFECFDE0000 _pycrfsuite.cp310-win_amd64.pyd\r\n7FFECFDA0000 _seq_dataset.cp310-win_amd64.pyd\r\n7FFECFD50000 _random.cp310-win_amd64.pyd\r\n7FFECFD20000 _expected_mutual_info_fast.cp310-win_amd64.pyd\r\n7FFECFC90000 _dist_metrics.cp310-win_amd64.pyd\r\n7FFECFC50000 _argkmin.cp310-win_amd64.pyd\r\n7FFECFC10000 _base.cp310-win_amd64.pyd\r\n7FFECFBB0000 _datasets_pair.cp310-win_amd64.pyd\r\n7FFECFB50000 _cython_blas.cp310-win_amd64.pyd\r\n7FFECFAF0000 _middle_term_computer.cp310-win_amd64.pyd\r\n7FFECFAE0000 _heap.cp310-win_amd64.pyd\r\n7FFECFAD0000 _sorting.cp310-win_amd64.pyd\r\n7FFECFA90000 _argkmin_classmode.cp310-win_amd64.pyd\r\n7FFECFA40000 _radius_neighbors.cp310-win_amd64.pyd\r\n7FFECFA00000 _radius_neighbors_classmode.cp310-win_amd64.pyd\r\n7FFECF9C0000 _pairwise_fast.cp310-win_amd64.pyd\r\n7FFECF960000 _cd_fast.cp310-win_amd64.pyd\r\n7FFECF780000 _loss.cp310-win_amd64.pyd\r\n7FFECF740000 arrayfuncs.cp310-win_amd64.pyd\r\n7FFECF700000 _liblinear.cp310-win_amd64.pyd\r\n7FFECF6A0000 _libsvm.cp310-win_amd64.pyd\r\n7FFECF640000 _libsvm_sparse.cp310-win_amd64.pyd\r\n7FFECF600000 _sag_fast.cp310-win_amd64.pyd\r\n7FFECF5B0000 _sgd_fast.cp310-win_amd64.pyd\r\n7FFECF580000 _weight_vector.cp310-win_amd64.pyd\r\n7FFECF510000 _svmlight_format_fast.cp310-win_amd64.pyd\r\n7FFECF500000 speedups.cp310-win_amd64.pyd\r\n7FFECF4E0000 ujson.cp310-win_amd64.pyd\r\n7FF8454A0000 rasadhlp.dll\r\n7FFECF4D0000 _speedups.cp310-win_amd64.pyd\r\n7FF843660000 fwpuclnt.dll\r\n7FFECF4C0000 _cbson.cp310-win_amd64.pyd\r\n7FFECF4A0000 _cmessage.cp310-win_amd64.pyd\r\n7FFECF400000 _rust_notify.cp310-win_amd64.pyd\r\n7FFECF3E0000 parser.cp310-win_amd64.pyd\r\n7FFECF3D0000 url_parser.cp310-win_amd64.pyd\r\n7FFECF140000 _extra.cp310-win_amd64.pyd\r\n7FFECD160000 mupdfcpp64.dll\r\n7FFECC740000 _mupdf.pyd\r\n7FFECF130000 _wrappers.cp310-win_amd64.pyd\r\n7FFECF0D0000 hnswlib.cp310-win_amd64.pyd\r\n7FFECC200000 pdfium.dll\r\n7FF826AC0000 GDIPLUS.DLL\r\n7FF84CB80000 MSCTF.dll\r\n7FFECF350000 _pillow_heif.cp310-win_amd64.pyd\r\n7FFECF190000 libheif-50ec905e77f9da70dc4650e831a2ddfc.dll\r\n7FFECF330000 libwinpthread-1-60c9caf0f4ff0fee4fcd00b1b6bef7aa.dll\r\n7FFECB6B0000 libaom-dbb8f4de1c733a287d11cd64cd8462db.dll\r\n7FFECF300000 libgcc_s_seh-1-9b6b724824997f52ee36af4f72787aee.dll\r\n7FFECBFB0000 libstdc++-6-58186925398578d557aa2556ccbf6d2e.dll\r\n7FFECE840000 libde265-0-d2ff8eb04521754b4e12762167ebdaf9.dll\r\n7FFECA350000 libx265-c6b040649b9621ebb6c4276513643392.dll\r\n022402330000 msys-magic-1.dll\r\n000210040000 msys-2.0.dll\r\n022402300000 msys-z.dll\r\n022402360000 msys-bz2-1.dll\r\n022403920000 msys-lzma-5.dll\r\n02254DAC0000 msys-zstd-1.dll\r\nWindows fatal exception: access violation \r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "I guess DocTR has issues with windows VM."
      }
    ]
  },
  {
    "issue_number": 1616,
    "title": "umbrella podSecurityContext null values are always overwritten by sub-chart default values",
    "author": "robinliubin",
    "state": "closed",
    "created_at": "2024-05-14T03:29:18Z",
    "updated_at": "2024-05-16T14:31:27Z",
    "labels": [],
    "body": "when install on openshift using umbrella chart.\r\numbrella can NOT nullify podSecurityContext, if set to null, it will be override by h2oGPT default values.\r\n\r\ndetails in: https://github.com/h2oai/platform-charts/issues/260\r\n\r\nneed to set below values to empty, not default 1000:\r\nhttps://github.com/h2oai/h2ogpt/blob/6824d21afc8633dc678e0f4dcdd0e450827faf91/helm/h2ogpt-chart/values.yaml#L69-L73\r\nto \r\n```yaml\r\n  podSecurityContext:\r\n    runAsNonRoot: true\r\n    runAsUser:\r\n    runAsGroup:\r\n    fsGroup:\r\n\r\n```",
    "comments": []
  },
  {
    "issue_number": 1617,
    "title": "[Question] how model learn data from new document ?",
    "author": "Ninlawat-Puhu",
    "state": "closed",
    "created_at": "2024-05-14T03:49:02Z",
    "updated_at": "2024-05-14T04:24:53Z",
    "labels": [
      "type/question"
    ],
    "body": "Users can upload any document to the model via portal UI. From my understand, when we have new document, it usually uses third-party llamaindex or Langchain to make index, then, will find similarity with model.\r\n\r\nMy question:\r\nFor this process, how h2ogpt work ?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "We use chroma, langchain is just thin layer into chroma, but we have many things that langchain does that like filtering etc."
      }
    ]
  },
  {
    "issue_number": 1615,
    "title": "pytorch_model.bin 1.34G download hangs forever on Linux",
    "author": "han-sogawa",
    "state": "closed",
    "created_at": "2024-05-13T22:55:23Z",
    "updated_at": "2024-05-14T02:18:25Z",
    "labels": [],
    "body": "Hello, I've just done a fresh manual install of h2ogpt on linux. My OS is Rocky Linux 9.3, and I have CUDA 12.4 installed and available. \r\n\r\nI ran the `docs/linux_install.sh` file (replacing `apt-get` with `dnf` package manager for Rocky Linux) and I believe I installed all required components. I had issues with some packages but they were marked optional and I don't think they are relevant to my problem. \r\n\r\nWhenever I run `generate.py` with _any_ parameters, it has the following output:\r\n```\r\nsoundfile, librosa, and wavio not installed, disabling STT\r\nsoundfile, librosa, and wavio not installed, disabling TTS\r\nUsing Model [whatever I passed in as --base_model]\r\npytorch_model.bin:       0%|                                                 | 0.00/1.34G [00:00<?, ?B/s]\r\n```\r\n...and then it just hangs there forever, never downloading anything. It always has the same file name and size (1.34G) no matter what model I set as `base_model`. I even downloaded a model and set the path to `model_path` but I have the exact same output. I have `google-chrome-stable` installed.\r\n\r\nAny ideas why this is happening or how I can dig deeper to see what `pytorch_model.bin` file it is trying so hard to download? Is there some kind of permissions I need to grant for the python folder to access the endpoint it's trying to reach?\r\n\r\nThank you",
    "comments": [
      {
        "user": "han-sogawa",
        "body": "[https://huggingface.co/api/models/hkunlp/instructor-large](https://huggingface.co/api/models/hkunlp/instructor-large) is the file it cannot download, although I can access it on the browser. "
      },
      {
        "user": "pseudotensor",
        "body": "Are you using that as the base model?  What is your actual generate.py line?"
      },
      {
        "user": "han-sogawa",
        "body": "No, it looks like it is another dependency, which attempts to download regardless of which base model I am using\r\n\r\nOne example of a generate.py line that I have tried:\r\n`python generate.py --base_model=meta-llama/llama-2-7b-chat-hf --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048`"
      },
      {
        "user": "han-sogawa",
        "body": "I think this may be where it is entering to try to download the file: https://github.com/h2oai/h2ogpt/blob/e0f5ab9eeac64d60e394180b5bf3e7be9876a649/src/gpt_langchain.py#L530"
      },
      {
        "user": "pseudotensor",
        "body": "What if you try a different embedding model, e.g. add to generate.py line:\r\n```\r\n--hf_embedding_model=sentence-transformers/all-MiniLM-L12-v2\r\n```\r\n\r\nAlso, you can try disabling hf_transfer by setting this env:\r\n```\r\nexport HF_HUB_ENABLE_HF_TRANSFER=0\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "FYI this is what it looks like when running  your command you gave:\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ python generate.py --base_model=meta-llama/llama-2-7b-chat-hf --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048\r\nUsing Model meta-llama/llama-2-7b-chat-hf\r\nload INSTRUCTOR_Transformer\r\nmax_seq_length  512\r\nStarting get_model: meta-llama/llama-2-7b-chat-hf \r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nconfig.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 614/614 [00:00<00:00, 1.45MB/s]\r\nOverriding max_seq_len -> 4096\r\ntokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.62k/1.62k [00:00<00:00, 3.93MB/s]\r\ntokenizer.model: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 8.89MB/s]\r\ntokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 5.95MB/s]\r\nspecial_tokens_map.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 414/414 [00:00<00:00, 876kB/s]\r\nOverriding max_seq_len -> 4096\r\nOverriding max_seq_len -> 4096\r\ndevice_map: {'': 0}\r\npytorch_model.bin.index.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26.8k/26.8k [00:00<00:00, 85.6MB/s]\r\npytorch_model-00001-of-00002.bin: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 9.98G/9.98G [01:29<00:00, 112MB/s]\r\npytorch_model-00002-of-00002.bin: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 3.50G/3.50G [00:31<00:00, 110MB/s]\r\nDownloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:01<00:00, 60.77s/it]\r\nLoading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.30s/it]\r\ngeneration_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [00:00<00:00, 530kB/s]\r\nModel {'base_model': 'meta-llama/llama-2-7b-chat-hf', 'base_model0': 'meta-llama/llama-2-7b-chat-hf', 'tokenizer_base_model': '', 'lora_weights': '', 'inference_server': '', 'prompt_type': 'llama2', 'prompt_dict': {'promptA': '', 'promptB': '', 'PreInstruct': \"<s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n\", 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': ' </s>', 'humanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", 'can_handle_system_prompt': True}, 'display_name': 'meta-llama/llama-2-7b-chat-hf', 'visible_models': None, 'h2ogpt_key': None, 'load_8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': True, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': False, 'revision': None, 'use_gpu_id': True, 'gpu_id': 0, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa': 0, 'model_path_llama': '', 'model_name_gptj': '', 'model_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4096, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}, 'force_seq2seq_type': False, 'force_t5_type': False, 'trust_remote_code': True}\r\nBegin auto-detect HF cache text generation models\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nNo loading model philschmid/bart-large-cnn-samsum because is_encoder_decoder=True\r\n/home/jon/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-30b-instruct/68deee8b69383b30826ea2fc642ba170b89e4edd/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\r\n  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\r\n/home/jon/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-30b-instruct/68deee8b69383b30826ea2fc642ba170b89e4edd/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\r\n  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\r\nWARNING:transformers_modules.tiiuae.falcon-40b-instruct.ecb78d97ac356d098e79f0db222c9ce7c5d9ee5f.configuration_falcon:\r\nWARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\r\n\r\nNo loading model openai/whisper-large-v3 because is_encoder_decoder=True\r\nNo loading model openai/whisper-base.en because is_encoder_decoder=True\r\nNo loading model h2oai/ggml because h2oai/ggml does not appear to have a file named config.json. Checkout 'https://huggingface.co/h2oai/ggml/main' for available files.\r\nNo loading model Systran/faster-whisper-large-v3 because is_encoder_decoder=True\r\nNo loading model openai/whisper-medium because is_encoder_decoder=True\r\nNo loading model philschmid/flan-t5-base-samsum because is_encoder_decoder=True\r\nNo loading model stabilityai/stable-diffusion-xl-refiner-1.0 because stabilityai/stable-diffusion-xl-refiner-1.0 does not appear to have a file named config.json. Checkout 'https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/main' for available files.\r\nNo loading model distil-whisper/distil-large-v2 because is_encoder_decoder=True\r\nNo loading model tloen/alpaca-lora-7b because tloen/alpaca-lora-7b does not appear to have a file named config.json. Checkout 'https://huggingface.co/tloen/alpaca-lora-7b/main' for available files.\r\n/home/jon/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/039e37745f00858f0e01e988383a8c4393b1a4f5/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\r\n  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\r\n/home/jon/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/039e37745f00858f0e01e988383a8c4393b1a4f5/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\r\n  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\r\nNo loading model distil-whisper/distil-large-v3 because is_encoder_decoder=True\r\nNo loading model microsoft/speecht5_hifigan because The checkpoint you are trying to load has model type `hifigan` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\r\nNo loading model unstructuredio/detectron2_faster_rcnn_R_50_FPN_3x because unstructuredio/detectron2_faster_rcnn_R_50_FPN_3x does not appear to have a file named config.json. Checkout 'https://huggingface.co/unstructuredio/detectron2_faster_rcnn_R_50_FPN_3x/main' for available files.\r\nNo loading model stabilityai/stable-diffusion-xl-base-1.0 because stabilityai/stable-diffusion-xl-base-1.0 does not appear to have a file named config.json. Checkout 'https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/main' for available files.\r\nNo loading model Salesforce/blip2-flan-t5-xl because is_encoder_decoder=True\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\r\n  warnings.warn(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\r\n  warnings.warn(\r\nNo loading model google/pix2struct-textcaps-base because is_encoder_decoder=True\r\nNo loading model Salesforce/blip2-flan-t5-xxl because is_encoder_decoder=True\r\n/home/jon/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-30b-chat/28fc475f7b73a5631fbbc6419645c27177f275d4/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\r\n  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\r\n/home/jon/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-30b-chat/28fc475f7b73a5631fbbc6419645c27177f275d4/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\r\n  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\r\nNo loading model microsoft/speecht5_vc because is_encoder_decoder=True\r\nNo loading model microsoft/speecht5_tts because is_encoder_decoder=True\r\nEnd auto-detect HF cache text generation models\r\nBegin auto-detect llama.cpp models\r\nEnd auto-detect llama.cpp models\r\nRunning on local URL:  http://0.0.0.0:7860\r\n\r\nTo create a public link, set `share=True` in `launch()`.\r\nStarted Gradio Server and/or GUI: server_name: localhost port: 7860\r\nUse local URL: http://localhost:7860/\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_info\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\n/home/jon/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_names\" has conflict with protected namespace \"model_\".\r\n\r\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\r\n  warnings.warn(\r\nOpenAI API URL: http://0.0.0.0:5000\r\nINFO:__name__:OpenAI API URL: http://0.0.0.0:5000\r\nOpenAI API key: EMPTY\r\nINFO:__name__:OpenAI API key: EMPTY\r\n```\r\n\r\nAll fine here.\r\n\r\nIf I remove the instructor-large model and try again:\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ rm -rf ~/.cache/torch/sentence_transformers/hkunlp_instructor-large/\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ python generate.py --base_model=meta-llama/llama-2-7b-chat-hf --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048\r\nUsing Model meta-llama/llama-2-7b-chat-hf\r\n.gitattributes: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.48k/1.48k [00:00<00:00, 3.83MB/s]\r\n1_Pooling/config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 270/270 [00:00<00:00, 792kB/s]\r\n2_Dense/config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 1.52MB/s]\r\npytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.15M/3.15M [00:00<00:00, 31.9MB/s]\r\nREADME.md: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 66.3k/66.3k [00:00<00:00, 1.16MB/s]\r\nconfig.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.53k/1.53k [00:00<00:00, 3.43MB/s]\r\nconfig_sentence_transformers.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:00<00:00, 1.59MB/s]\r\npytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1.34G/1.34G [00:13<00:00, 100MB/s]\r\nsentence_bert_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53.0/53.0 [00:00<00:00, 116kB/s]\r\nspecial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.20k/2.20k [00:00<00:00, 6.28MB/s]\r\nspiece.model: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 13.6MB/s]\r\ntokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.42M/2.42M [00:00<00:00, 12.8MB/s]\r\ntokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.41k/2.41k [00:00<00:00, 7.18MB/s]\r\nmodules.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 461/461 [00:00<00:00, 5.86MB/s]\r\nload INSTRUCTOR_Transformer\r\n... same as before\r\n```\r\n\r\nIt downloads fine.  So I guess you have some network complication."
      },
      {
        "user": "han-sogawa",
        "body": "The workaround of adding `--hf_embedding_model=sentence-transformers/all-MiniLM-L12-v2` worked for me, thank you! Still don't know why the instructor-large embedding file wouldn't download. I'll update if I find out more, but for now, my issue is resolved. Thank you very much!\r\n"
      }
    ]
  },
  {
    "issue_number": 1608,
    "title": "Windows fatal exception: Access violation",
    "author": "InesBenAmor99",
    "state": "open",
    "created_at": "2024-05-10T08:28:59Z",
    "updated_at": "2024-05-13T20:08:24Z",
    "labels": [],
    "body": "Why im getting this error when i upload a scanned pdf file ?  everything was working correctly until i changed some configs to make mistral model work on gpu but i guess that is not related to the probleme i have now ''access violation\" \r\n\r\n\r\nWindows fatal exception: access violation\r\n\r\nThread 0x00001828 (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\concurrent\\futures\\thread.py\", line 81 in _worker\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 953 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nCurrent thread 0x000014fc (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\compat.py\", line 189 in load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\compat.py\", line 242 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1078 in _handle_fromlist\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\__init__.py\", line 433 in _add_compat\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\__init__.py\", line 469 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\unstructured\\file_utils\\filetype.py\", line 25 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\unstructured\\partition\\pdf.py\", line 58 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\langchain_community\\document_loaders\\pdf.py\", line 71 in _get_elements\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py\", line 88 in lazy_load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\langchain_core\\document_loaders\\base.py\", line 29 in load\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 4384 in file_to_doc\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 4636 in path_to_doc1\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 4933 in <listcomp>\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 4933 in path_to_docs\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 8938 in _update_user_db\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 8732 in update_user_db\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gradio_runner.py\", line 7187 in update_user_db_gr\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\gradio\\utils.py\", line 771 in wrapper\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x0000087c (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 324 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 607 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\tqdm\\_monitor.py\", line 60 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x00000708 (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 324 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\queue.py\", line 180 in get\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\posthog\\consumer.py\", line 107 in next\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\posthog\\consumer.py\", line 76 in upload\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\posthog\\consumer.py\", line 65 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x000019dc (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\", line 797 in _poll\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\", line 444 in select\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 1871 in _run_once\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 603 in run_forever\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\", line 321 in run_forever\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 636 in run_until_complete\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\runners.py\", line 44 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\uvicorn\\server.py\", line 65 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\uvicorn\\main.py\", line 575 in run\r\n  File \"C:\\Windows\\System32\\h2ogpt\\openai_server\\server.py\", line 374 in run_server\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 953 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x00000e7c (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\", line 797 in _poll\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\", line 444 in select\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 1871 in _run_once\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 603 in run_forever\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\", line 321 in run_forever\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 636 in run_until_complete\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\runners.py\", line 44 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\uvicorn\\server.py\", line 65 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 953 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x00000e88 (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 324 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 607 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\apscheduler\\schedulers\\blocking.py\", line 30 in _main_loop\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 953 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x00000e6c (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\selectors.py\", line 315 in _select\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\selectors.py\", line 324 in select\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 1871 in _run_once\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 603 in run_forever\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 953 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x00001aec (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\gradio\\blocks.py\", line 2507 in block_thread\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gradio_runner.py\", line 6849 in go_gradio\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gen.py\", line 2334 in main\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\fire\\core.py\", line 691 in _CallAndUpdateTrace\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\fire\\core.py\", line 475 in _Fire\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\fire\\core.py\", line 141 in Fire\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\utils.py\", line 73 in H2O_Fire\r\n  File \"C:\\Windows\\System32\\h2ogpt\\generate.py\", line 16 in entrypoint_main\r\n  File \"C:\\Windows\\System32\\h2ogpt\\generate.py\", line 20 in <module>\r\nWindows fatal exception: access violation\r\n\r\nThread 0x00001828 (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\concurrent\\futures\\thread.py\", line 81 in _worker\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 953 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nCurrent thread 0x000014fc (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\compat.py\", line 189 in load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\compat.py\", line 242 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1078 in _handle_fromlist\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\__init__.py\", line 433 in _add_compat\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\__init__.py\", line 469 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\unstructured\\file_utils\\filetype.py\", line 25 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\unstructured\\partition\\pdf.py\", line 58 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\langchain_community\\document_loaders\\pdf.py\", line 71 in _get_elements\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py\", line 88 in lazy_load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\langchain_core\\document_loaders\\base.py\", line 29 in load\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 4384 in file_to_doc\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 4636 in path_to_doc1\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 4933 in <listcomp>\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 4933 in path_to_docs\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 8938 in _update_user_db\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 8732 in update_user_db\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gradio_runner.py\", line 7187 in update_user_db_gr\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\gradio\\utils.py\", line 771 in wrapper\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x0000087c (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 324 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 607 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\tqdm\\_monitor.py\", line 60 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x00000708 (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 324 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\queue.py\", line 180 in get\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\posthog\\consumer.py\", line 107 in next\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\posthog\\consumer.py\", line 76 in upload\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\posthog\\consumer.py\", line 65 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x000019dc (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\", line 797 in _poll\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\", line 444 in select\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 1871 in _run_once\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 603 in run_forever\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\", line 321 in run_forever\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 636 in run_until_complete\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\runners.py\", line 44 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\uvicorn\\server.py\", line 65 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\uvicorn\\main.py\", line 575 in run\r\n  File \"C:\\Windows\\System32\\h2ogpt\\openai_server\\server.py\", line 374 in run_server\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 953 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x00000e7c (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\", line 797 in _poll\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\", line 444 in select\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 1871 in _run_once\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 603 in run_forever\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\", line 321 in run_forever\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 636 in run_until_complete\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\runners.py\", line 44 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\uvicorn\\server.py\", line 65 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 953 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x00000e88 (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 324 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 607 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\apscheduler\\schedulers\\blocking.py\", line 30 in _main_loop\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 953 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x00000e6c (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\selectors.py\", line 315 in _select\r\n  File \"Windows fatal exception: access violation\r\n\r\nC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\selectors.py\", line 324 in select\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 1871 in _run_once\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 603 in run_forever\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 953 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x00001aec (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\gradio\\blocks.py\", line 2507 in block_thread\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gradio_runner.py\", line 6849 in go_gradio\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gen.py\", line 2334 in main\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\fire\\core.py\", line 691 in _CallAndUpdateTrace\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\fire\\core.py\", line 475 in _Fire\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\fire\\core.py\", line 141 in Fire\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\utils.py\", line 73 in H2O_Fire\r\n  File \"C:\\Windows\\System32\\h2ogpt\\generate.py\", line 16 in entrypoint_main\r\n  File \"C:\\Windows\\System32\\h2ogpt\\generate.py\", line 20 in <module>\r\n      0 [main] python (6896) C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\python.exe: *** fatal error - Internal error: TP_NUM_C_BUFS too small: 50\r\nWindows fatal exception: access violation\r\n\r\nThread 0x00001828 (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\concurrent\\futures\\thread.py\", line 81 in _worker\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 953 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nCurrent thread 0x000014fc (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\compat.py\", line 189 in load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\compat.py\", line 242 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1078 in _handle_fromlist\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\__init__.py\", line 433 in _add_compat\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\__init__.py\", line 469 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\unstructured\\file_utils\\filetype.py\", line 25 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\unstructured\\partition\\pdf.py\", line 58 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 241 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 883 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 688 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 1027 in _find_and_load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\langchain_community\\document_loaders\\pdf.py\", line 71 in _get_elements\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py\", line 88 in lazy_load\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\langchain_core\\document_loaders\\base.py\", line 29 in load\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 4384 in file_to_doc\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 4636 in path_to_doc1\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 4933 in <listcomp>\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 4933 in path_to_docs\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 8938 in _update_user_db\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gpt_langchain.py\", line 8732 in update_user_db\r\n  File \"C:\\Windows\\System32\\h2ogpt\\src\\gradio_runner.py\", line 7187 in update_user_db_gr\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\gradio\\utils.py\", line 771 in wrapper\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x0000087c (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 324 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 607 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\tqdm\\_monitor.py\", line 60 in run\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 973 in _bootstrap\r\n\r\nThread 0x00000708 (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 324 in wait\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\queue.py\", line 180 in get\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\posthog\\consumer.py\", line 107 in next\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\posthog\\consumer.py\", line 76 in upload\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\posthog\\consumer.py\", line 65 in run\r\n  File \"Fatal Python error: Segmentation fault\r\n\r\nC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line 1016 in _bootstrap_inner\r\n  File \"\r\nExtension modules: _brotliC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py, _cffi_backend\", line 973 in , _bootstrap\r\n\r\nThread 0x000019dczstandard.backend_c (most recent call first):\r\n, charset_normalizer.md  File , \"mkl._mklinitC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py, mkl._py_mkl_service\", , line numpy.core._multiarray_umath797,  in numpy.core._multiarray_tests_poll,\r\n  File \"numpy.linalg._umath_linalgC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py, \"numpy.fft._pocketfft_internal, line 444, numpy.random._common in , selectnumpy.random.bit_generator\r\n  File \", C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\"numpy.random._bounded_integers, line , numpy.random._mt199371871, numpy.random.mtrand in , _run_oncenumpy.random._philox\r\n, numpy.random._pcg64  File , \"numpy.random._sfc64C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 603,  in numpy.random._generatorrun_forever\r\n  File \", C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\"pandas._libs.tslibs.np_datetime, line , pandas._libs.tslibs.dtypes321,  in pandas._libs.tslibs.baserun_forever,\r\n  File \"pandas._libs.tslibs.nattypeC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py, \", line 636 in run_until_complete\r\n  File \"pandas._libs.tslibs.timezonesC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\runners.py, \"pandas._libs.tslibs.tzconversion, line 44, pandas._libs.tslibs.ccalendar in , runpandas._libs.tslibs.fields\r\n  File \", C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\uvicorn\\server.py\"pandas._libs.tslibs.timedeltas, line 65, pandas._libs.tslibs.timestamps in , runpandas._libs.properties\r\n  File \", C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\uvicorn\\main.py\"pandas._libs.tslibs.offsets, line , pandas._libs.tslibs.parsing575,  in pandas._libs.tslibs.conversionrun,\r\n  File \"pandas._libs.tslibs.periodC:\\Windows\\System32\\h2ogpt\\openai_server\\server.py, \"pandas._libs.tslibs.vectorized, line , pandas._libs.ops_dispatch374,  in pandas._libs.missingrun_server,\r\n  File \"pandas._libs.hashtableC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py, \"pandas._libs.algos, line , pandas._libs.interval953, pandas._libs.tslib in , runpandas._libs.lib\r\n, pandas._libs.hashing  File , \"pyarrow.libC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py, \"pandas._libs.ops, line 1016, numexpr.interpreter in , _bootstrap_innerpyarrow._compute\r\n, pandas._libs.arrays  File , \"pandas._libs.indexC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py, \"pandas._libs.join, line 973, pandas._libs.sparse in , _bootstrappandas._libs.reduction\r\n, pandas._libs.indexing\r\n, pandas._libs.internalsThread 0x, 00000e7cpandas._libs.writers (most recent call first):\r\n  File \", C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py\"pandas._libs.window.aggregations, line , pandas._libs.window.indexers797,  in pandas._libs.reshape_poll,\r\n  File \"pandas._libs.tslibs.strptimeC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py, \"pandas._libs.groupby, line 444, pandas._libs.testing in , selectpandas._libs.parsers\r\n, pandas._libs.json  File , \"lz4._versionC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py, \"lz4.frame._frame, line 1871, psutil._psutil_windows in _run_once,\r\n  File \"fontTools.misc.bezierToolsC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py, \"PIL._imaging, line 603, lxml._elementpath in , run_foreverlxml.etree\r\n, fontTools.varLib.iup  File , \"torch._CC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\windows_events.py, \"torch._C._fft, line 321, torch._C._linalg in , run_forevertorch._C._nested\r\n, torch._C._nn  File , \"torch._C._sparseC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py, \"torch._C._special, line 636, matplotlib._c_internal_utils in , run_until_complete\r\n  File \"matplotlib._pathC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\runners.py, \"kiwisolver._cext, line , 44 in runmatplotlib._image\r\n, scipy._lib._ccallback_c  File \", C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\uvicorn\\server.py\"scipy.sparse._sparsetools, line 65, _csparsetools in , runscipy.sparse._csparsetools\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", , line 953 in scipy.sparse.linalg._isolve._iterativerun\r\n  File \", scipy.linalg._fblasC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py, \"scipy.linalg._flapack, line , scipy.linalg.cython_lapack1016,  in scipy.linalg._cythonized_array_utils_bootstrap_inner,\r\n  File \"scipy.linalg._solve_toeplitzC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py, \"scipy.linalg._decomp_lu_cython, line 973, scipy.linalg._matfuncs_sqrtm_triu in , _bootstrapscipy.linalg.cython_blas\r\n\r\nThread 0x00000e88, scipy.linalg._matfuncs_expm (most recent call first):\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", , line scipy.linalg._decomp_update324,  in scipy.linalg._flinalgwait\r\n  File \", scipy.sparse.linalg._dsolve._superluC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", , line scipy.sparse.linalg._eigen.arpack._arpack607 in , scipy.sparse.csgraph._toolswait,\r\n  File \"scipy.sparse.csgraph._shortest_pathC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\apscheduler\\schedulers\\blocking.py\", line , 30scipy.sparse.csgraph._traversal in , _main_loop\r\n  File \"C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\"scipy.sparse.csgraph._min_spanning_tree, line 953, scipy.sparse.csgraph._flow in , runscipy.sparse.csgraph._matching\r\n  File \", C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\"scipy.sparse.csgraph._reordering, line , 1016scipy.optimize._minpack2 in _bootstrap_inner, scipy.optimize._group_columns\r\n, scipy._lib.messagestream  File , \"scipy.optimize._trlib._trlibC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\", line , 973scipy.optimize._lbfgsb in , _bootstrap_moduleTNC\r\n, scipy.optimize._moduleTNC\r\n, scipy.optimize._cobylaThread 0x, 00000e6cscipy.optimize._slsqp (most recent call first):\r\n  File \", C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\selectors.py\"scipy.optimize._minpack, line , 315scipy.optimize._lsq.givens_elimination in _select,\r\n  File \"scipy.optimize._zerosC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\selectors.py, \"scipy.optimize.__nnls, line , scipy.optimize._highs.cython.src._highs_wrapper324,  in scipy.optimize._highs._highs_wrapperselect,\r\n  File \"scipy.optimize._highs.cython.src._highs_constantsC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py\", line 1871,  in scipy.optimize._highs._highs_constants_run_once,\r\n  File \"scipy.linalg._interpolativeC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\asyncio\\base_events.py, \"scipy.optimize._bglu_dense, line 603, scipy.optimize._lsap in , run_foreverscipy.spatial._ckdtree\r\n, scipy.spatial._qhull  File , \"scipy.spatial._voronoiC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py, \"scipy.spatial._distance_wrap, line 953, scipy.spatial._hausdorff in , runscipy.special._ufuncs_cxx\r\n, scipy.special._ufuncs  File , \"scipy.special._specfunC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py, \"scipy.special._comb, line 1016, scipy.special._ellip_harm_2 in , _bootstrap_innerscipy.spatial.transform._rotation\r\n, scipy.optimize._direct  File , \"PIL._imagingftC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py, \"yaml._yaml, line 973, google._upb._message in _bootstrap\r\n, scipy.io.matlab._mio_utils\r\n, scipy.io.matlab._streamsThread 0x, 00001aecscipy.io.matlab._mio5_utils (most recent call first):\r\n, h5py._errors  File , \"h5py.defsC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\gradio\\blocks.py, \"h5py._objects, line 2507, h5py.h5 in , block_thread\r\n  File \"h5py.utilsC:\\Windows\\System32\\h2ogpt\\src\\gradio_runner.py, \"h5py.h5t, line , 6849h5py.h5s in , go_gradioh5py.h5ac\r\n  File \", h5py.h5pC:\\Windows\\System32\\h2ogpt\\src\\gen.py, \"h5py.h5r, line , 2334h5py._proxy in , mainh5py._conv\r\n, h5py.h5z  File , \"h5py.h5aC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\fire\\core.py, \"h5py.h5d, line , h5py.h5ds691,  in h5py.h5g_CallAndUpdateTrace,\r\n  File \"h5py.h5iC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\fire\\core.py, \"h5py.h5f, line , 475h5py.h5fd in _Fire, h5py.h5pl\r\n, h5py.h5o  File , \"h5py.h5lC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\fire\\core.py, \"h5py._selector, line 141 in , pyclipper._pyclipperFire,\r\n  File \"shapely.speedups._speedupsC:\\Windows\\System32\\h2ogpt\\src\\utils.py, \", line rapidfuzz._feature_detector_cpp73,  in H2O_Fire\r\n  File \"rapidfuzz.distance._initialize_cppC:\\Windows\\System32\\h2ogpt\\generate.py, \"rapidfuzz.distance.metrics_cpp_avx2, line , rapidfuzz.fuzz_cpp_avx216,  in rapidfuzz.process_cpp_implentrypoint_main,\r\n  File \"rapidfuzz.utils_cppC:\\Windows\\System32\\h2ogpt\\generate.py, \"scipy.cluster._vq, line , 20scipy.cluster._hierarchy in , scipy.cluster._optimal_leaf_ordering<module>\r\n, sentencepiece._sentencepiece, cython.cimports.libc.mathWindows fatal exception: , access violation\r\n\r\nThread 0x00001828grpc._cython.cygrpc (most recent call first):\r\n, scipy.ndimage._nd_image  File , _ni_label\", scipy.ndimage._ni_labelC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\concurrent\\futures\\thread.py\", line 81,  in scipy.integrate._odepack_worker,\r\n  File \"scipy.integrate._quadpackC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py, \"scipy.integrate._vode, line 953 in run\r\n  File \", C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py\"scipy.integrate._dop, line , 1016scipy.integrate._lsoda in _bootstrap_inner\r\n  File \", scipy.special.cython_specialC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\threading.py, \"scipy.stats._stats, line 973, scipy.stats.beta_ufunc in _bootstrap, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc\r\n\r\n, scipy.stats._boost.binom_ufuncCurrent thread 0x, 000014fc (most recent call first):\r\nscipy.stats.nbinom_ufunc  File , scipy.stats._boost.nbinom_ufunc\", C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\compat.py\", line scipy.stats.hypergeom_ufunc189,  in scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufuncload\r\n  File \", C:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\compat.py\"scipy.stats._boost.ncf_ufunc, line 242 in , <module>scipy.stats.ncx2_ufunc\r\n, scipy.stats._boost.ncx2_ufunc  File , \"scipy.stats.nct_ufunc<frozen importlib._bootstrap>, \"scipy.stats._boost.nct_ufunc, line 241,  in scipy.stats.skewnorm_ufunc_call_with_frames_removed,\r\n  File \"scipy.stats._boost.skewnorm_ufunc<frozen importlib._bootstrap_external>, \"scipy.stats.invgauss_ufunc, line , 883 in exec_module\r\n  File \"scipy.stats._boost.invgauss_ufunc<frozen importlib._bootstrap>, \"scipy.interpolate._fitpack, line , 688scipy.interpolate.dfitpack in _load_unlocked, scipy.interpolate._bspl\r\n, scipy.interpolate._ppoly  File , \"<frozen importlib._bootstrap>scipy.interpolate.interpnd\", , line scipy.interpolate._rbfinterp_pythran1006 in , scipy.interpolate._rgi_cython_find_and_load_unlocked,\r\n  File \"scipy.stats._biasedurn<frozen importlib._bootstrap>, \"scipy.stats._levy_stable.levyst, line , 1027scipy.stats._stats_pythran in _find_and_load, scipy._lib._uarray._uarray\r\n, scipy.stats._statlib  File , \"scipy.stats._sobol<frozen importlib._bootstrap>, \"scipy.stats._qmc_cy, line , 241scipy.stats._mvn in _call_with_frames_removed, scipy.stats._rcont.rcont\r\n  File \"<frozen importlib._bootstrap>\", line , 1078multidict._multidict in , _handle_fromlist\r\n  File \"yarl._quoting_cC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\__init__.py, \"aiohttp._helpers, line 433, aiohttp._http_writer in , aiohttp._http_parser_add_compat, aiohttp._websocket\r\n  File \", frozenlist._frozenlistC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\magic\\__init__.py, \"sqlalchemy.cyextension.collections, line 469, sqlalchemy.cyextension.immutabledict in , <module>sqlalchemy.cyextension.processors\r\n  File \", sqlalchemy.cyextension.resultproxy<frozen importlib._bootstrap>, \"sqlalchemy.cyextension.util, line , 241greenlet._greenlet in _call_with_frames_removed\r\n  File \", PIL._imagingmath<frozen importlib._bootstrap_external>, \"PIL._webp, line , 883regex._regex in , exec_module\r\n  File \"sklearn.__check_build._check_build<frozen importlib._bootstrap>, \"sklearn.utils._isfinite, line , 688sklearn.utils.murmurhash in _load_unlocked, sklearn.utils._openmp_helpers\r\n, sklearn.utils.sparsefuncs_fast  File , \"sklearn.preprocessing._csr_polynomial_expansion<frozen importlib._bootstrap>, \", line 1006 in sklearn.preprocessing._target_encoder_fast_find_and_load_unlocked,\r\n  File \"sklearn.utils._vector_sentinel<frozen importlib._bootstrap>, \"sklearn.feature_extraction._hashing_fast, line , pycrfsuite._pycrfsuite1027 in , sklearn.utils._random_find_and_load,\r\n  File \"sklearn.utils._seq_datasetC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\unstructured\\file_utils\\filetype.py\", line 25,  in sklearn.metrics.cluster._expected_mutual_info_fast<module>,\r\n  File \"sklearn.metrics._dist_metrics<frozen importlib._bootstrap>, \"sklearn.metrics._pairwise_distances_reduction._datasets_pair, line 241, sklearn.utils._cython_blas in , _call_with_frames_removedsklearn.metrics._pairwise_distances_reduction._base\r\n  File \", sklearn.metrics._pairwise_distances_reduction._middle_term_computer<frozen importlib._bootstrap_external>, \"sklearn.utils._heap, line 883, sklearn.utils._sorting in , exec_modulesklearn.metrics._pairwise_distances_reduction._argkmin\r\n  File \", sklearn.metrics._pairwise_distances_reduction._argkmin_classmode<frozen importlib._bootstrap>, \"sklearn.metrics._pairwise_distances_reduction._radius_neighbors, line 688, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode in _load_unlocked, sklearn.metrics._pairwise_fast\r\n, sklearn.linear_model._cd_fast  File , \"sklearn._loss._loss<frozen importlib._bootstrap>, \"sklearn.utils.arrayfuncs, line 1006 in , _find_and_load_unlocked\r\n  File \"sklearn.svm._liblinear<frozen importlib._bootstrap>, \"sklearn.svm._libsvm, line , 1027sklearn.svm._libsvm_sparse in , sklearn.utils._weight_vector_find_and_load,\r\nsklearn.linear_model._sgd_fast  File , \"sklearn.linear_model._sag_fastC:\\Users\\Ines_Ben_Amor\\miniconda3\\envs\\weensi\\lib\\site-packages\\unstructured\\partition\\pdf.py\", line 58,  in sklearn.datasets._svmlight_format_fast<module>\r\n, websockets.speedups  File , \"ujson<frozen importlib._bootstrap>, \"markupsafe._speedups, line 241,  in httptools.parser.parser_call_with_frames_removed\r\n  File \", <frozen importlib._bootstrap_external>\"httptools.parser.url_parser, line , 883fitz._extra in , exec_modulefitz._mupdf\r\n  File \", _pillow_heif<frozen importlib._bootstrap> (total: \"286, line )688\r\n in _load_unlockedWindows fatal exception: access violation\r\n  File \"\r\n\r\n<frozen importlib._bootstrap>\", line",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Looks like pymupdf or scipy is failing.  I am not sure what to do about that."
      },
      {
        "user": "InesBenAmor99",
        "body": "After testing multiple times, I consistently encounter the same error: \"Windows fatal exception: access violation.\" This occurs sometimes even when attempting to import PDF (sometimes it works for pdf files and sometimes no) or DOCX files, or choosing ocr option , I am working on a Windows virtual machine deployed on Azure. Could the issue be related to memory constraints?"
      },
      {
        "user": "pseudotensor",
        "body": "Hi, sorry you are having trouble.  It could be related to using a windows virtual machine instead of native windows, but I don't know for sure."
      }
    ]
  },
  {
    "issue_number": 1611,
    "title": "TimeoutError: answer_question_using_context timed out, took more than 60s",
    "author": "Sameera2001Perera",
    "state": "open",
    "created_at": "2024-05-10T18:31:49Z",
    "updated_at": "2024-05-10T18:31:49Z",
    "labels": [],
    "body": "Any reasons why Im getting this error when running below code.\r\n\r\n```\r\nfrom h2ogpte import H2OGPTE\r\n\r\nAPI_KEY = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\r\nREMOTE_ADDRESS = \"https://h2ogpte.genai.h2o.ai\"\r\n\r\nclient = H2OGPTE(address=REMOTE_ADDRESS, api_key=API_KEY)\r\n\r\nllm = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\r\n\r\nanswer = client.answer_question(question=\"Who are you?\", llm=llm).content\r\nprint(f\"{llm}: {answer}\", flush=True)\r\n```\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"E:\\TechLabs\\SyntheticData_2\\test2.py\", line 10, in <module>\r\n    answer = client.answer_question(question=\"Who are you?\", llm=llm).content\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"E:\\TechLabs\\SyntheticData_2\\venv\\Lib\\site-packages\\h2ogpte\\h2ogpte.py\", line 335, in answer_question\r\n    ret = self._lang(\r\n          ^^^^^^^^^^^\r\n  File \"E:\\TechLabs\\SyntheticData_2\\venv\\Lib\\site-packages\\h2ogpte\\h2ogpte.py\", line 209, in _lang\r\n    res = self._post(\"/rpc/lang\", marshal(dict(method=method, params=kwargs)))\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"E:\\TechLabs\\SyntheticData_2\\venv\\Lib\\site-packages\\h2ogpte\\h2ogpte.py\", line 179, in _post\r\n    self._raise_error_if_any(res)\r\n  File \"E:\\TechLabs\\SyntheticData_2\\venv\\Lib\\site-packages\\h2ogpte\\h2ogpte.py\", line 197, in _raise_error_if_any\r\n    raise InternalServerError(error)\r\nh2ogpte.errors.InternalServerError: InternalServerError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"src/h2ogpte_core/server.py\", line 413, in __call__\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"src/h2ogpte_core/server.py\", line 409, in app\r\n  File \"src/h2ogpte_core/server.py\", line 405, in rpc\r\n  File \"src/h2ogpte_core/server.py\", line 112, in redis_async_wrapper\r\nTimeoutError: answer_question_using_context timed out, took more than 60s\r\n```",
    "comments": []
  },
  {
    "issue_number": 1609,
    "title": "Failed to load models",
    "author": "ikergar-bar",
    "state": "closed",
    "created_at": "2024-05-10T08:36:59Z",
    "updated_at": "2024-05-10T10:13:42Z",
    "labels": [],
    "body": "Currently in the last commit: https://github.com/h2oai/h2ogpt/commit/d806f1fae80500414297841fa1ebbb4c1e4321dd\r\nExecuting this script to load a model in a linux system:\r\n\r\nTOKENIZERS_PARALLELISM=true python generate.py \\\r\n    --base_model=SciPhi/SciPhi-Self-RAG-Mistral-7B-32k --prompt_type=sciphi --use_gpu_id=True --gpu_id=-1 --max_seq_len=8192 \\\r\n    --user_path=/opt/myDocuments/arsys.es/html --langchain_mode='UserData' --max_quality=True \\\r\n    --add_chat_history_to_context=True --keep_sources_in_context=True --enable_ocr=True --enable_doctr=True \\\r\n    --answer_with_sources=True --show_link_in_sources=True --append_sources_to_chat=True \\\r\n    -–hf_embedding_model=\"intfloat/multilingual-e5-small\" \\\r\n    --memory_restriction_level=0 --score_model=None --verbose=True \\\r\n    --show_examples=True --compile_model=True \\\r\n    --src_lang='Spanish' --tgt_lang='Spanish' \\\r\n    --debug=True --concurrency_count=1 \\\r\n    --pre_prompt_query='Always answer as helpfully as possible and follow ALL given instructions. Do not speculate or make up information.' \\\r\n    --share=True \\\r\n    --load_8bit=True\r\n\r\nIt has the following error:\r\n<img width=\"492\" alt=\"image\" src=\"https://github.com/h2oai/h2ogpt/assets/113558949/e7afc178-7cfd-4e7f-b584-20203832523e\">",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, the packages were updated.  You'll need to at least:\r\n```\r\npip install -r reqs_optional/requirements_optional_langchain.txt\r\n```\r\n\r\nor re-install the env from scratch is always safe."
      },
      {
        "user": "ikergar-bar",
        "body": "ok, right now just having updated those packages is working"
      }
    ]
  },
  {
    "issue_number": 1606,
    "title": "GPU offloading mistralai_mistral-7b-instruct-v0.2",
    "author": "InesBenAmor99",
    "state": "open",
    "created_at": "2024-05-08T16:38:52Z",
    "updated_at": "2024-05-08T23:48:29Z",
    "labels": [],
    "body": "Any reasons why mistralai_mistral-7b-instruct-v0.2 does not offload on gpu ? \r\n\r\n\r\n\r\n\r\nload INSTRUCTOR_Transformer\r\nmax_seq_length  512\r\nStarting get_model: llama \r\nFailed to listen to n_gpus: No module named 'llama_cpp_cuda', trying llama_cpp module\r\nllama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from llamacpp_path\\mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 17\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\r\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\r\nllama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\r\nllama_model_loader: - kv  23:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q5_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V3 (latest)\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_embd_head_k    = 128\r\nllm_load_print_meta: n_embd_head_v    = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: n_embd_k_gqa     = 1024\r\nllm_load_print_meta: n_embd_v_gqa     = 1024\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: f_logit_scale    = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: n_expert         = 0\r\nllm_load_print_meta: n_expert_used    = 0\r\nllm_load_print_meta: causal attn      = 1\r\nllm_load_print_meta: pooling type     = 0\r\nllm_load_print_meta: rope type        = 0\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 1000000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: ssm_d_conv       = 0\r\nllm_load_print_meta: ssm_d_inner      = 0\r\nllm_load_print_meta: ssm_d_state      = 0\r\nllm_load_print_meta: ssm_dt_rank      = 0\r\nllm_load_print_meta: model type       = 8B\r\nllm_load_print_meta: model ftype      = Q5_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 4.78 GiB (5.67 BPW)\r\nllm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\r\nllm_load_print_meta: BOS token        = 1 '<s>'\r\nllm_load_print_meta: EOS token        = 2 '</s>'\r\nllm_load_print_meta: UNK token        = 0 '<unk>'\r\nllm_load_print_meta: PAD token        = 0 '<unk>'\r\nllm_load_print_meta: LF token         = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.15 MiB\r\nllm_load_tensors:        CPU buffer size =  4892.99 MiB\r\n...................................................................................................\r\nllama_new_context_with_model: n_ctx      = 4096\r\nllama_new_context_with_model: n_batch    = 1024\r\nllama_new_context_with_model: n_ubatch   = 512\r\nllama_new_context_with_model: flash_attn = 0\r\nllama_new_context_with_model: freq_base  = 1000000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\r\nllama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\r\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\r\nllama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\r\nllama_new_context_with_model: graph nodes  = 1030\r\nllama_new_context_with_model: graph splits = 1\r\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 |\r\nModel metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama\r\n.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_t\r\nype': '17', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', '\r\ngeneral.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_toke\r\nn': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 =\r\n= 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + \r\n' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\r\nGuessed chat format: mistral-instruct\r\nModel {'base_model': 'llama', 'base_model0': 'llama', 'tokenizer_base_model': '', 'lora_weights': '', 'inference_server': '', 'prompt_type': 'mistral', 'prompt_dict': {'promptA': '\r\n', 'promptB': '', 'PreInstruct': '<s>[INST] ', 'PreInput': None, 'PreResponse': '[/INST]', 'terminate_response': ['[INST]', '</s>'], 'chat_sep': ' ', 'chat_turn_sep': '</s> ', 'hum\r\nanstr': '[INST]', 'botstr': '[/INST]', 'generates_leading_space': False, 'system_prompt': '', 'can_handle_system_prompt': False}, 'visible_models': None, 'h2ogpt_key': None, 'load_\r\n8bit': False, 'load_4bit': False, 'low_bit_mode': 1, 'load_half': True, 'use_flash_attention_2': False, 'load_gptq': '', 'load_awq': '', 'load_exllama': False, 'use_safetensors': F\r\nalse, 'revision': None, 'use_gpu_id': True, 'gpu_id': 0, 'compile_model': None, 'use_cache': None, 'llamacpp_dict': {'n_gpu_layers': 100, 'use_mlock': True, 'n_batch': 1024, 'n_gqa\r\n': 0, 'model_path_llama': 'https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf?download=true', 'model_name_gptj': '', 'm\r\nodel_name_gpt4all_llama': '', 'model_name_exllama_if_no_config': ''}, 'rope_scaling': {}, 'max_seq_len': 4096, 'max_output_seq_len': None, 'exllama_dict': {}, 'gptq_dict': {}, 'attention_sinks': False, 'sink_dict': {}, 'truncation_generation': False, 'hf_model_dict': {}, 'force_seq2seq_type': False, 'force_t5_type': False, 'trust_remote_code': True}\r\nBegin auto-detect HF cache text generation models\r\nEnd auto-detect HF cache text generation models\r\nBegin auto-detect llama.cpp models",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "How did you install h2oGPT?  It requires steps like this for cuda:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/5b6b7b362d64266936151feb26e983dc82f18804/docs/README_LINUX.md#L107-L111\r\n\r\n"
      },
      {
        "user": "InesBenAmor99",
        "body": "I'm on windows, i followed the exact steps in the README_Windows. Additionally, when I ran 'import torch' and 'print(torch.cuda.is_available())', I got 'True'."
      },
      {
        "user": "pseudotensor",
        "body": "So you did this step in the readme_windows?\r\n\r\n```\r\n* For non-CPU case, choose llama_cpp_python ARGS for your system according to [llama_cpp_python backend documentation](https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#supported-backends), e.g. for CUDA:\r\n  ```cmdline\r\n   set CMAKE_ARGS=-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all\r\n   set LLAMA_CUBLAS=1\r\n   set FORCE_CMAKE=1\r\n  ```\r\n  Note for some reason things will fail with llama_cpp_python if don't add all cuda arches, and building with all those arches does take some time.\r\n```\r\n\r\n"
      }
    ]
  },
  {
    "issue_number": 1603,
    "title": "h2ogpt vllm-check init-container stuck when istio injection ",
    "author": "robinliubin",
    "state": "closed",
    "created_at": "2024-05-07T17:28:14Z",
    "updated_at": "2024-05-08T20:25:09Z",
    "labels": [],
    "body": "One key customer is using istio as service-mesh to encript pod-pod communications.\r\n\r\nh2oGPT is employing init-container that detect vLLM-inference-service is reachable before starting the main container.\r\n```bash\r\nuntil wget -O- http://h2ogpt-vllm-inference:5000/v1/models >/dev/null 2>&1;\r\n  do\r\n      echo \"Waiting for inference service to become ready...\";\r\n      sleep 200;\r\n  done\r\n```\r\n\r\nhowever, due to the containers starting order:\r\n1. init-container: istio-validation: iptables created, networking traffic hijack started\r\n2. init-container: vllm-check: wget vllm, this request will be captured by iptables, it depends on istio-proxy to redirect the traffic\r\n3. container: istio-proxy: this container handle traffic redirect.  however due to vllm-check is not completed, this container can not be started\r\n4. container: h2ogpt\r\n\r\n#2 and #3 is dead lock in h2ogpt scenario.\r\n\r\nactually, best practice is avoid init-container as a service health probe:\r\nhttps://discuss.istio.io/t/k8s-istio-sidecar-injection-with-other-init-containers/845\r\n\r\n#### possible workaround:\r\n1. force vllm-check init-container to runAsUser: 1337, traffic from UID: 1337 will not be captured by iptables\r\n2. check vllm service availability in h2ogpt container chained with `python3 /workspace/generate.py`\r\n3. delete vllm-check init-container, just let h2ogpt container fail and restart after.\r\n\r\n\r\nref: \r\nhttps://istio.io/latest/docs/setup/additional-setup/cni/#compatibility-with-application-init-containers\r\nhttps://github.com/istio/istio/issues/48854",
    "comments": []
  },
  {
    "issue_number": 1599,
    "title": "Failed to initial linux full script intallation ",
    "author": "juerware",
    "state": "closed",
    "created_at": "2024-05-06T11:21:51Z",
    "updated_at": "2024-05-07T08:58:21Z",
    "labels": [],
    "body": "Currently in the last commit: 4059a2c9\r\nExecuting this script to install everything needed in linux system:\r\n```bash docs/linux_install_full.sh```\r\nIt has the following error:\r\n```\r\n+++ google-chrome --version\r\n++ echo Google Chrome 124.0.6367.118\r\n+ chromeVersion=124.0.6367.118\r\n+ sudo rm -rf chromedriver_linux64.zip chromedriver LICENSE.chromedriver\r\n+ sudo wget https://storage.googleapis.com/chrome-for-testing-public/124.0.6367.118/linux64/chromedriver-linux64.zip\r\n--2024-05-06 13:16:55--  https://storage.googleapis.com/chrome-for-testing-public/124.0.6367.118/linux64/chromedriver-linux64.zip\r\nResolving storage.googleapis.com (storage.googleapis.com)... 142.250.184.187, 142.250.185.27, 142.250.201.91, ...\r\nConnecting to storage.googleapis.com (storage.googleapis.com)|142.250.184.187|:443... connected.\r\nHTTP request sent, awaiting response... 404 Not Found\r\n2024-05-06 13:16:55 ERROR 404: Not Found.\r\n```\r\nAs you can see this URL does not exist:\r\n```https://storage.googleapis.com/chrome-for-testing-public/124.0.6367.118/linux64/chromedriver-linux64.zip```\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, it should be fixed now.  Google hasn't uploaded a matching version of chrome driver yet for some reason, so I use the last one for now if that happens to fail.\r\n\r\nIt was actually already updated in the repo, but not on s3.  And it happens that linux_install_full.sh references the s3 file."
      },
      {
        "user": "aramirezarsys",
        "body": "Ok, right now in the las version is working, thanks."
      }
    ]
  },
  {
    "issue_number": 1595,
    "title": "OCR issue",
    "author": "InesBenAmor99",
    "state": "open",
    "created_at": "2024-05-04T16:13:05Z",
    "updated_at": "2024-05-07T00:28:07Z",
    "labels": [],
    "body": "When I import a scanned PDF, I encounter an error ( first screenshot ) , and the OCR option is not available for me in the expert tab. Even if I activate pdf options from auto to on I still encounter another error (as shown in the second screenshot). What could be the problem? \r\n![image](https://github.com/h2oai/h2ogpt/assets/168529190/0f8d679c-98b5-4a16-ba6a-96a9a68a0b14)\r\n\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/168529190/24bcc244-883a-4662-b017-83d7ecd197a3)\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I recommend DocTR model instead of scanned/OCR handling.  Unstructured is vastly slower for OCR and less accurate.\r\n\r\nI can't tell much from the 2nd error you provided as it's only single line.  I can't tell where in code that is from etc."
      }
    ]
  },
  {
    "issue_number": 1594,
    "title": "Support for https://huggingface.co/lightblue/suzume-llama-3-8B-multilingual",
    "author": "plitc",
    "state": "closed",
    "created_at": "2024-05-04T09:38:18Z",
    "updated_at": "2024-05-07T00:24:56Z",
    "labels": [
      "type/question"
    ],
    "body": "Dear h2o team,\r\n\r\nplease add support for llama3 based models like: https://huggingface.co/lightblue/suzume-llama-3-8B-multilingual\r\n\r\nthanks in advance and best regards\r\n\r\nDaniel Plominski",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It already has a chat template, which means it would work OOTB.\r\n\r\nhttps://huggingface.co/lightblue/suzume-llama-3-8B-multilingual/blob/main/tokenizer_config.json#L2053"
      },
      {
        "user": "plitc",
        "body": "Documents Parsing doesnt work:\r\n\r\nStep 1: Create DB\r\n```\r\n#!/bin/sh\r\n\r\nexport HUGGING_FACE_HUB_TOKEN=XXX\r\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\r\ndocker run \\\r\n       --gpus all \\\r\n       --runtime=nvidia \\\r\n       --shm-size=64g \\\r\n       -p 7860:7860 \\\r\n       --rm --init \\\r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u 1000:1000 \\\r\n       -v /opt/h2ogpt_data/.cache:/workspace/.cache \\\r\n       -v /opt/h2ogpt_data/save:/workspace/save \\\r\n       -v /opt/h2ogpt_data/db_dir_MyData:/workspace/db_dir_MyData \\\r\n       -v /opt/h2ogpt_data/db_dir_UserData:/workspace/db_dir_UserData \\\r\n       -v /opt/h2ogpt_data/tmp:/tmp \\\r\n       -v /opt/h2ogpt_data/DATA:/workspace/user_path/DATA \\\r\n       -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\r\n       -e TOKENIZERS_PARALLELISM=false \\\r\n       -e ADMIN_PASS=XXX \\\r\n       -e CONCURRENCY_COUNT=1 \\\r\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:latest /workspace/src/make_db.py --db_type=chroma\r\n\r\n# EOF\r\n```\r\n\r\nStep 2: Run\r\n```\r\n#!/bin/sh\r\n\r\nexport HUGGING_FACE_HUB_TOKEN=XXX\r\nexport CUDA_VISIBLE_DEVICES=\"0,1\"\r\ndocker run \\\r\n       --gpus all \\\r\n       --runtime=nvidia \\\r\n       --shm-size=64g \\\r\n       -p 7860:7860 \\\r\n       --rm --init \\\r\n       -v /etc/passwd:/etc/passwd:ro \\\r\n       -v /etc/group:/etc/group:ro \\\r\n       -u 1000:1000 \\\r\n       -v /opt/h2ogpt_data/.cache:/workspace/.cache \\\r\n       -v /opt/h2ogpt_data/save:/workspace/save \\\r\n       -v /opt/h2ogpt_data/db_dir_UserData:/workspace/db_dir_UserData \\\r\n       -v /opt/h2ogpt_data/tmp:/tmp \\\r\n       -v /opt/h2ogpt_data/DATA:/workspace/user_path/DATA \\\r\n       -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\r\n       -e TOKENIZERS_PARALLELISM=false \\\r\n       -e ADMIN_PASS=XXX \\\r\n       -e CONCURRENCY_COUNT=1 \\\r\n       -e API_OPEN=1 \\\r\n       -e ALLOW_API=1 \\\r\n       gcr.io/vorvan/h2oai/h2ogpt-runtime:latest /workspace/generate.py \\\r\n          --use_safetensors=True \\\r\n          --prompt_type=unknown \\\r\n          --max_seq_len=8192 \\\r\n          --use_chat_template=True \\\r\n          --base_model=lightblue/suzume-llama-3-8B-multilingual \\\r\n          --save_dir='/workspace/save/' \\\r\n          --use_auth_token=$HUGGING_FACE_HUB_TOKEN \\\r\n          --use_gpu_id=False \\\r\n          --allow_upload_to_user_data=False \\\r\n          --allow_upload_to_my_data=True \\\r\n          --enable_ocr='off' \\\r\n          --enable_pdf_ocr='off' \\\r\n          --langchain_mode='UserData' \\\r\n          --langchain_modes=\"['LLM', 'UserData', 'MyData']\" \\\r\n          --user_path=/workspace/user_path \\\r\n          --db_type=chroma \\\r\n          --visible_h2ogpt_header=False \\\r\n          --visible_doc_selection_tab=False \\\r\n          --visible_doc_view_tab=False \\\r\n          --visible_chat_history_tab=False \\\r\n          --visible_expert_tab=False \\\r\n          --visible_models_tab=False \\\r\n          --visible_system_tab=False \\\r\n          --visible_tos_tab=False \\\r\n          --visible_hosts_tab=False\r\n\r\n# EOF\r\n```\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/h2ogpt_conda/envs/h2ogpt/lib/python3.10/site-packages/gradio/queueing.py\", line 566, in process_events\r\n    response = await route_utils.call_process_api(\r\n... ... ...\r\n  File \"/workspace/src/gpt_langchain.py\", line 7985, in get_chain\r\n    assert hasattr(llm, 'chat_conversation')\r\nAssertionError\r\n```\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "There is no `--use_chat_template` parameter and if using chat template, it's automatically default behavior if the model has chat template and you don't pass `--prompt_type` so I don't recommend passing that.\r\n\r\nI fixed the particular issue, thanks.\r\n\r\n```\r\npython generate.py --use_safetensors=True\r\n--max_seq_len=8192\r\n--base_model=lightblue/suzume-llama-3-8B-multilingual\r\n--use_auth_token=$HUGGING_FACE_HUB_TOKEN\r\n--add_disk_models_to_ui=False\r\n```\r\nworks now"
      }
    ]
  },
  {
    "issue_number": 1596,
    "title": "shared / personal collections",
    "author": "InesBenAmor99",
    "state": "closed",
    "created_at": "2024-05-05T12:15:25Z",
    "updated_at": "2024-05-05T19:38:46Z",
    "labels": [],
    "body": "Hello, can someone answer my question? For collection types, shared or personal, does that mean if I'm signed in as user1 and I create some shared collections, then sign out and log in as user2, am I supposed to find the collections that user1 created?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Yes, they are accessible, but by default the user would need to add it to their list in document selection, or add to CLI as default, or one adds to a specific user's auth.json entry.\r\n\r\nhttps://github.com/h2oai/h2ogpt/issues/1515"
      }
    ]
  },
  {
    "issue_number": 1593,
    "title": "branding capitalization",
    "author": "TallTed",
    "state": "closed",
    "created_at": "2024-05-03T15:08:33Z",
    "updated_at": "2024-05-04T07:34:10Z",
    "labels": [
      "type/question"
    ],
    "body": "Branding is often important. I want to use your intended branding when I talk about H2O.ai and h2oGPT, but I am not sure what your intensions are.\r\n\r\n[`H2O.ai`](https://h2o.ai/) appears to be (mostly) consistent for your company.\r\n\r\nBut do you intend \"H2O GPT\", \"H2o GPT\", \"h2oGPT\", \"h2ogpt\", something else?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "h2oGPT, thanks."
      }
    ]
  },
  {
    "issue_number": 1591,
    "title": "h2ogpt on ubuntu server",
    "author": "InesBenAmor99",
    "state": "open",
    "created_at": "2024-04-30T08:37:46Z",
    "updated_at": "2024-05-01T19:28:31Z",
    "labels": [],
    "body": "I'm running h2ogpt on an Ubuntu server, you'll find attached the server specifications. However, the model execution is too slow (TheBloke/Mistral-7B-Instruct-v0.2-GGUF), and sometimes it doesn't even generate a response. Any recommendations? What exactly could be the problem? \r\n\r\n![caracteristique serveur](https://github.com/h2oai/h2ogpt/assets/168529190/2605dd09-67ae-4925-b4d7-79887d2bec49)\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, can you give your exact run line?  For CPU it can be slow when inputting large context,so you can reduce top_k_docs etc. \r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/832ad2d4a6b1431105785045a6b218a8451591f9/docs/FAQ.md#controlling-quality-and-speed-of-parsing"
      },
      {
        "user": "InesBenAmor99",
        "body": "Hello , thank you for answering ! this is my exact run line : python generate.py --base_model=TheBloke/Mistral-7B-Instruct-v0.2-GGUF --prompt_type=mistral --max_seq_len=4096  (note that it's too slow even if I don't introduce context (source = llm), simple requests like hello or hi messages for example ) \r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Hi, same command line for me yields very fast results on CPU, but I added the top_k_docs limit.  I also added the other stuff mentioned, but that wouldn't matter of just LLM chat mode.\r\n\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ CUDA_VISIBLE_DEVICES= python generate.py --base_model=TheBloke/Mistral-7B-Instruct-v0.2-GGUF --prompt_type=mistral --max_seq_len=4096 --top_k_docs=3 --max_input_tokens=4096\r\n```\r\n\r\nThen I go to http://127.0.0.1:7860\r\n\r\nI see about 2-3 tokens per second, maybe 2 words per second, on my CPU system with i9."
      }
    ]
  },
  {
    "issue_number": 1563,
    "title": "Rest API for inference locally",
    "author": "mohamed-alired",
    "state": "open",
    "created_at": "2024-04-15T20:38:01Z",
    "updated_at": "2024-04-28T02:50:08Z",
    "labels": [
      "type/feature"
    ],
    "body": "hi \r\nI have installed h2ogpt locally, but I want to build a frontend app using it, so I was wondering if there's an API that I can consume, like one for ingestion and another for inference. ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "An extensive gradio API exists, see: See readme_client.md and examples via test code like `test_client_chat_stream_langchain_steps3`\r\n\r\nAnd a full chat OpenAI API that is REST capable exists, but no upload of file or other things exists yet.  Is that what you are looking for?\r\n\r\n"
      },
      {
        "user": "mohamed-alired",
        "body": "What I am looking for is a fastapi rest API for the different ingestion techniques and a rag completion API so I can use H2OGPT as a backend rag for my frontend webUI. Also, I wish you included JSON metadata for filtering in ingestion and rag completion so we can choose the files to chat with."
      },
      {
        "user": "abuyusif01",
        "body": "hi @mohamed-alired \r\n\r\nAm currently building something exactly like this. its still in development tho. U can certainly fork the repo or make PR's. the foundation is there. The project extends the official FastAPI Template so scalling and deploying wont really much of a husle. \r\n\r\ncheck it out here: https://github.com/abuyusif01/h2ogpt-fast-api/tree/main/backend/app/h2ogpt\r\n\r\nthere's still alot things need to be done. Including a proper README and support Streaming the Response (I planed to get this done in this weekend) \r\n\r\nHere is what we currently support:\r\n\r\n1. Chat with on disk files (there's an endpoint to upload docs, and retrieve whats being uploaded, so  u can select which doc to ingest)\r\n2. Chat with user Created pipelines (Currently MongoDB streamed data)\r\n3. Chat with Urls\r\n4. Chat with Publications, We use OpenDoaj API and scihub to download the papers. "
      },
      {
        "user": "mohamed-alired",
        "body": "hi @abuyusif01 \r\nhow are you?\r\ni am really busy so if i have some time i will definitely PR \r\nbit i can give you some recommendations like don't force the inference with users cause i may wanna use it on my existing project also i think you have to make it possible with local inference like llamaCpp or something else so it's completely locally "
      },
      {
        "user": "abuyusif01",
        "body": "@mohamed-alired \r\nYou're right we dont really need to enforce auth, hence its removal\r\nI also make it possible to local inference using llamaCPP.\r\n\r\nSubsequently, i restructure the repo, write a readme and containerize the app. Its now easy to setup + extend\r\ncheck it here: https://github.com/abuyusif01/h2ogpt-fast-api\r\n\r\n@pseudotensor \r\nSince gradio is relatively stable now, why not reference this in the readme. so other people can use it as a starting point. "
      }
    ]
  },
  {
    "issue_number": 1585,
    "title": "ValueError: load_in_8bit must be a boolean",
    "author": "Noctis-SC",
    "state": "closed",
    "created_at": "2024-04-25T09:45:11Z",
    "updated_at": "2024-04-26T05:16:57Z",
    "labels": [],
    "body": "Hello, I'm having trouble running a `h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3` model using h2opgt.\r\n\r\n**Here is a command that I call:**\r\n`CUDA_VISIBLE_DEVICES=1 python generate.py --base_model=h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 --score_model=None prompt_type=human_bot --cli=False load_in_4bit=True device_map=auto`\r\n\r\n**Here is an output:**\r\n`Using Model h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\r\nload INSTRUCTOR_Transformer\r\nmax_seq_length  512\r\nMust install DocTR and LangChain installed if enabled DocTR, disabling\r\nStarting get_model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nStarting get_model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nStarting get_model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nStarting get_model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nTraceback (most recent call last):\r\n  File \"/home/alexander/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/alexander/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/alexander/h2ogpt/src/utils.py\", line 73, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/alexander/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/alexander/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/alexander/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/home/alexander/h2ogpt/src/gen.py\", line 2269, in main\r\n    model0, tokenizer0, device = get_model_retry(reward_type=False,\r\n  File \"/home/alexander/h2ogpt/src/gen.py\", line 2621, in get_model_retry\r\n    model1, tokenizer1, device1 = get_model(**kwargs)\r\n  File \"/home/alexander/h2ogpt/src/gen.py\", line 3277, in get_model\r\n    return get_hf_model(load_8bit=load_8bit,\r\n  File \"/home/alexander/h2ogpt/src/gen.py\", line 3437, in get_hf_model\r\n    model_kwargs['quantization_config'] = BitsAndBytesConfig(\r\n  File \"/home/alexander/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 284, in __init__\r\n    self.post_init()\r\n  File \"/home/alexander/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 320, in post_init\r\n    raise ValueError(\"load_in_8bit must be a boolean\")\r\nValueError: load_in_8bit must be a boolean`\r\n\r\nI have tried `CUDA_VISIBLE_DEVICES=1 python generate.py --base_model=TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF --score_model=None prompt_type=human_bot --cli=False load_in_4bit=True device_map=auto` this command with TheBloke model and works perfectly fine. \r\n\r\nWhat is the problem?\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "You are missing -- in front of your `load_in_4bit`"
      },
      {
        "user": "Noctis-SC",
        "body": "> You are missing -- in front of your `load_in_4bit`\r\nThank you for your reply, I tried your solution, however the problem persists\r\n```\r\nCUDA_VISIBLE_DEVICES=1 python generate.py --base_model=h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 --score_model=None prompt_type=human_bot --cli=False --load_in_4bit=True device_map=auto\r\nUsing Model h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\r\nload INSTRUCTOR_Transformer\r\nmax_seq_length  512\r\nMust install DocTR and LangChain installed if enabled DocTR, disabling\r\nStarting get_model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nStarting get_model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nStarting get_model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nStarting get_model: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nCould not determine --max_seq_len, setting to 2048.  Pass if not correct\r\nTraceback (most recent call last):\r\n  File \"/home/alexander/h2ogpt/generate.py\", line 20, in <module>\r\n    entrypoint_main()\r\n  File \"/home/alexander/h2ogpt/generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/home/alexander/h2ogpt/src/utils.py\", line 73, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"/home/alexander/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"/home/alexander/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"/home/alexander/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"/home/alexander/h2ogpt/src/gen.py\", line 2269, in main\r\n    model0, tokenizer0, device = get_model_retry(reward_type=False,\r\n  File \"/home/alexander/h2ogpt/src/gen.py\", line 2621, in get_model_retry\r\n    model1, tokenizer1, device1 = get_model(**kwargs)\r\n  File \"/home/alexander/h2ogpt/src/gen.py\", line 3277, in get_model\r\n    return get_hf_model(load_8bit=load_8bit,\r\n  File \"/home/alexander/h2ogpt/src/gen.py\", line 3437, in get_hf_model\r\n    model_kwargs['quantization_config'] = BitsAndBytesConfig(\r\n  File \"/home/alexander/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 284, in __init__\r\n    self.post_init()\r\n  File \"/home/alexander/anaconda3/envs/h2ogpt/lib/python3.10/site-packages/transformers/utils/quantization_config.py\", line 320, in post_init\r\n    raise ValueError(\"load_in_8bit must be a boolean\")\r\n```\r\n`ValueError: load_in_8bit must be a boolean`\r\n\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "At end you have `device_map=auto` that has no meaning to the CLI.  It interprets it as the next argument."
      },
      {
        "user": "Noctis-SC",
        "body": "> At end you have `device_map=auto` that has no meaning to the CLI. It interprets it as the next argument.\r\n\r\nBefore it worked fine, now even \r\n`python generate.py --base_model=h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 --score_model=None prompt_type=human_bot --cli=False` this command doesn't work \r\n\r\nHowever, \r\n`python generate.py --base_model=TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF --score_model=None prompt_type=human_bot --cli=False` when I call base model from TheBloke works for me\r\n\r\nWhat do you think?\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "You again miss -- before \"prompt_type\"."
      }
    ]
  },
  {
    "issue_number": 1578,
    "title": "Failed to import transformers.pipelines ",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-04-23T12:00:24Z",
    "updated_at": "2024-04-25T03:38:17Z",
    "labels": [],
    "body": "I do a fresh installation of the latest h2ogpt.  When i hit the following command \r\n`python generate.py --base_model=meta-llama/Llama-2-7b-chat-hf  --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048`\r\n\r\nIt then start the application window  successfully but when make a simple query like **Hi**, generating the following error\r\n```\r\nraise RuntimeError(\r\nRuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\r\nDescriptors cannot be created directly.\r\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\r\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\r\n 1. Downgrade the protobuf package to 3.20.x or lower.\r\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\r\n```\r\n \r\n![protobuf_error](https://github.com/h2oai/h2ogpt/assets/137979399/1ded0f49-d035-4aba-b7c5-96ad450b77be)\r\n\r\nI also track the issues  [https://github.com/h2oai/h2ogpt/issues/965](url) and install `\r\npip install protobuf==3.20.0` \r\nthen it throws the following error:\r\n`File \"/home/abx/miniconda3/envs/gpt230424_test/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1512, in _get_module\r\n    raise RuntimeError(\r\nRuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\r\nNo module named 'gast'`\r\n \r\n \r\n![gast_error](https://github.com/h2oai/h2ogpt/assets/137979399/09e26e01-3e78-4968-9bb5-dad069e9ee80)\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi.  I did a completely fresh install using quick install way in readme_linux.md:\r\n```bash\r\ncurl -fsSL https://h2o-release.s3.amazonaws.com/h2ogpt/linux_install_full.sh | bash\r\n```\r\nand enter the sudo password when required. Once install done, do:\r\n```bash\r\nconda activate h2ogpt\r\n```\r\n\r\nand your exact run line:\r\n```\r\n(h2ogpt) jon@gpu:~/h2ogpt$ python generate.py --base_model=meta-llama/Llama-2-7b-chat-hf --score_model=None --langchain_mode='UserData' --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048\r\n```\r\n\r\nand I have no issues:\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/ccadc351-30f4-4abc-b1a5-f11e153ee40c)\r\n\r\n\r\nI have hit that protobuf thing in the past, but it's unclear the cause.\r\n\r\nHow did you install h2oGPT?"
      },
      {
        "user": "llmwesee",
        "body": "I installed it using the **INSTALL** instructions from the [https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md](url)   for cuda 11.8"
      },
      {
        "user": "llmwesee",
        "body": "And when I doing it with your way i.e quick install then the following things happened\r\n\r\n![proto_error](https://github.com/h2oai/h2ogpt/assets/137979399/41ef66f4-c8f5-4e52-92be-882ce5ea6744)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Looks like you are missing /usr/local/cuda-12.1.  Yes, that's default for that install, but you can download and edit the file instead of running it to switch to another cuda.\r\n\r\nI'd recommend moving to cuda 12.1, as cuda 11.8 has lost support from many packages and old packages that support cuda 11.8 are no longer compatible with newer required ones.\r\n\r\nIt's too challenging to support the numerous packages plus also various cuda."
      },
      {
        "user": "llmwesee",
        "body": "I first ensured that the correct environment variables are set before following the manual steps.\r\n`export PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cu118 https://huggingface.github.io/autogptq-index/whl/cu118\"`\r\n\r\n and then proceed with the provided manual steps.\r\n\r\nNote: Previously, the same procedure worked flawlessly on a server with 48GB VRAM. However, attempting the same steps on a laptop with 16GB VRAM results in the  error."
      },
      {
        "user": "pseudotensor",
        "body": "From what you said, I guess you are trying to use cuda 11.8 still.  But I cannot tell from your screen shot what the package is that is having issues.  You should share more."
      }
    ]
  },
  {
    "issue_number": 1583,
    "title": "Collection Selection showen multiple times",
    "author": "florianscherl",
    "state": "closed",
    "created_at": "2024-04-24T08:08:19Z",
    "updated_at": "2024-04-24T08:14:35Z",
    "labels": [],
    "body": "![image](https://github.com/h2oai/h2ogpt/assets/104893008/839af199-070c-4fa2-a892-ff5650e3ca89)\r\n\r\nAs showen in the picture, there is the selection of the collections multiple times. \r\n\r\nInstalled on Ubuntu, no Gpu, ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "There was an old build 2 weeks ago that did that.  Are you using latest?"
      }
    ]
  },
  {
    "issue_number": 1580,
    "title": "Document Storage",
    "author": "jaysunl",
    "state": "closed",
    "created_at": "2024-04-23T16:47:16Z",
    "updated_at": "2024-04-24T07:59:54Z",
    "labels": [
      "type/question"
    ],
    "body": "I want to change this repository so that when I open the chatbot, it already has the documents pre-loaded and I don't have to manually upload the documents myself everytime I start it up (even without logging in). I'm guessing the db directory is created somewhere everytime I ingest some docs so if I pre-create the db directory and call python generate.py, does that load up the chatbot with the docs knowledge already? And also where is the db directory located?",
    "comments": [
      {
        "user": "santosh-gkg",
        "body": "use make_db.py to create database as shared folders refer the documentations in docs/readme_langchain "
      },
      {
        "user": "pseudotensor",
        "body": "https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#database-creation"
      }
    ]
  },
  {
    "issue_number": 1582,
    "title": "How should I upload my personal data to the h2o website I deployed and make it persistent?",
    "author": "fengke-j",
    "state": "closed",
    "created_at": "2024-04-24T03:27:25Z",
    "updated_at": "2024-04-24T07:30:50Z",
    "labels": [
      "type/question"
    ],
    "body": null,
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Best approach is to enable auth so people can login.  If you are single user, then can do same or go to login tab and login."
      }
    ]
  },
  {
    "issue_number": 1579,
    "title": "Intel ARC GPU support",
    "author": "SergioVargasRamirez",
    "state": "open",
    "created_at": "2024-04-23T12:11:28Z",
    "updated_at": "2024-04-23T12:11:28Z",
    "labels": [],
    "body": "Dear h2o team,\r\n\r\nfirst of all, pretty cool project! Congrats!\r\n\r\nI have been looking for LLM projects with Intel ARC 770 support and was wondering if you plan to add support for intel in the future. I have the impression that intel is interested in extending support for their GPUs but don't know how easy this can be done or how much support they offer.\r\n\r\nthanks in advance and best regards\r\n\r\nSergio",
    "comments": []
  },
  {
    "issue_number": 1576,
    "title": "error intalling from linux_install_full.sh",
    "author": "juerware",
    "state": "closed",
    "created_at": "2024-04-22T10:25:37Z",
    "updated_at": "2024-04-23T07:55:41Z",
    "labels": [],
    "body": "According to documentation in order to install in linux machine it is necessary the following execution:\r\n```\r\ncurl -fsSL https://h2o-release.s3.amazonaws.com/h2ogpt/linux_install_full.sh | bash\r\n```\r\nAnd it is failed with the following error:\r\n```\r\n...\r\n...\r\n...\r\n+ sudo unzip -o chromedriver-linux64.zip\r\nArchive:  chromedriver-linux64.zip\r\n  inflating: chromedriver-linux64/LICENSE.chromedriver\r\n  inflating: chromedriver-linux64/chromedriver\r\n+ sudo mv chromedriver-linux64/chromedriver /usr/bin/chromedriver\r\n+ sudo chown root:root /usr/bin/chromedriver\r\n+ sudo chmod +x /usr/bin/chromedriver\r\n+ pip install optimum==1.18.0 -c reqs_optional/reqs_constraints.txt\r\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu121\r\nCollecting optimum==1.18.0\r\n  Using cached optimum-1.18.0-py3-none-any.whl.metadata (18 kB)\r\nRequirement already satisfied: coloredlogs in /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages (from optimum==1.18.0) (15.0.1)\r\nRequirement already satisfied: sympy in /root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages (from optimum==1.18.0) (1.12)\r\nINFO: pip is looking at multiple versions of optimum to determine which version is compatible with other requirements. This could take a while.\r\nERROR: Cannot install optimum==1.18.0 because these package versions have conflicting dependencies.\r\n\r\nThe conflict is caused by:\r\n    optimum 1.18.0 depends on transformers<4.40.0 and >=4.26.0\r\n    The user requested (constraint) transformers==4.40.0\r\n\r\nTo fix this you could try to:\r\n1. loosen the range of package versions you've specified\r\n2. remove package versions to allow pip attempt to solve the dependency conflict\r\n\r\nERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\r\n```",
    "comments": [
      {
        "user": "santosh-gkg",
        "body": "have you created a new enviornment?"
      },
      {
        "user": "juerware",
        "body": "No, it is the same environment I have been using for long time"
      },
      {
        "user": "santosh-gkg",
        "body": "You should create a new one that would work"
      },
      {
        "user": "pseudotensor",
        "body": "Please try again.  The s3 file was not yet updated with a release version, but now is."
      },
      {
        "user": "juerware",
        "body": "Ok, that´s working."
      }
    ]
  },
  {
    "issue_number": 1565,
    "title": "How to delete content in user_paste",
    "author": "abuyusif01",
    "state": "closed",
    "created_at": "2024-04-17T04:45:48Z",
    "updated_at": "2024-04-20T10:13:42Z",
    "labels": [
      "type/question"
    ],
    "body": "I have a very specific usecase where i stream data via pipelines and paste it to h2ogpt using `api_name='/add_text'`\r\n\r\nMy question here is, how can i delete the pasted resources i longer need. ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "You can delete any document via `delete_sources` api.  You need to know it's name, which one can get as part of the output of `add_text`.\r\n\r\nI have one unit test of this endpoint:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/ec70f56de8ac72fa2ff4a31ac925daeaf6ee7bae/tests/test_client_calls.py#L2509-L2511\r\n\r\nDoes this help?"
      },
      {
        "user": "abuyusif01",
        "body": "Yeah it does, closing issue. "
      }
    ]
  },
  {
    "issue_number": 1549,
    "title": "Loading a Large model on Multiples GPU system",
    "author": "rohitnanda1443",
    "state": "closed",
    "created_at": "2024-04-11T05:19:01Z",
    "updated_at": "2024-04-19T02:36:40Z",
    "labels": [
      "type/question"
    ],
    "body": "Hi, \r\n\r\nI had a few conceptual queries (these are not bugs):\r\n\r\n1) How does one estimate the vRAM requirement for a model\r\n\r\n2) How to run a model on more than one GPU? \r\n\r\n\tAdding CUDA_VISIBLE_DEVICES=0,1 and --use_gpu_id=False does not seem to help. Also trying the --load_4bit=True or --load_8bit=True does not help\r\n\r\n\tThe model being tried is the mistral-community/Mixtral-8x22B-v0.1 and mistral-community/Mixtral-8x22B-v0.1-4bit. The GPUs are 2xA6000 (ie. 96 GB vRAM). The model is not loading on the other GPU (vRAM of GPU#2 is empty). \r\n\r\n3) Is there any readme on the various prompt_type? \r\n\r\n4) Any resource which explains the Embedding Model and Inference Server and its uses with H2O-GPT? \r\n\r\nAim: To get the best RAG performance for a large number of documents with minimalistic errors. ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "1. Difficult in general for a variety of reasons.\r\n2. --use_gpu_id=False or use vLLM or TGI is best actually\r\n3. Only in the code, but most models have a chat template now and we'll just use that and you don't need to pass --prompt_type\r\n4. text embedding inference (TEI) server or TGI or vLLM are explained in the FAQ.md or readme_inferenceserver.md.\r\n\r\nI recommend TEI + vLLM for best performance.  If low on GPU memory, these are still best options.  You can use vLLM's `--enforce-eager` + AWQ model to reach lowest GPU memory required."
      },
      {
        "user": "rohitnanda1443",
        "body": "Thanks.\r\n\r\nIn the vLLM, I have encountered a strange problem: \r\n\r\nWhile loading mistralai/Mixtral-8x7B-Instruct-v0.1 without vLLM (using generate.py script) I can load it on a single 45 GB GPU with vRAM usage of less than 30GB. \r\n\r\nIf I try to load the same model in vLLM mode I get an Out of Memory Error. Even when the server has a 2x45GB GPU. The memory footprint has increased by over 3 times using vLLM. \r\n\r\nhttps://github.com/vllm-project/vllm/issues/4067"
      },
      {
        "user": "pseudotensor",
        "body": "For Mixtral, base context size is 32k which will require nearly 2*80GB without quantization.  You should use an AWQ version of Mixtral.  We found best success with https://huggingface.co/casperhansen/mixtral-instruct-awq\r\n .\r\nAll the other TheBloke or other AWQ Mixtral were bad, e.g. generate repeats etc.\r\n\r\nYou can also adjust `--max-model-len <length>` option for vLLM\r\n\r\nhttps://docs.vllm.ai/en/latest/models/engine_args.html\r\n\r\nSo e.g. overall the vLLM options (run in python or h2oGPT docker with options for just vLLM for 2 GPUs:\r\n\r\n```\r\n--port=5000 --host=0.0.0.0 --model casperhansen/mixtral-instruct-awq --seed 1234 --tensor-parallel-size=2 --max-num-batched-tokens=8192 --max-log-len=100 --trust-remote-code --worker-use-ray --enforce-eager --gpu-memory-utilization 0.98 --quantization awq\r\n```\r\n\r\nYou can try increasing `8192` up to `32768`\r\n\r\n\r\n"
      },
      {
        "user": "rohitnanda1443",
        "body": "Used the above model. Works perfectly during simple LLM chat but gives an error during RAG:\r\n\r\n![Error-H2o](https://github.com/h2oai/h2ogpt/assets/159398448/d8d4e334-970c-4328-9151-e6002f1314d2)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "What is your h2oGPT run line?\r\n\r\nE.g. perhaps you are using an older h2oGPT that doesn't account for fact that some chat templates (if not passing --prompt_type) can't handle system prompt."
      },
      {
        "user": "rohitnanda1443",
        "body": "1) Using the H2O-GPT version of about 1 month old \r\n\r\n2) Tried with sending --prompt_type and without --prompt_type. \r\n\r\n3) H2O firing script: python generate.py --auth_access=closed --auth=auth.json --guest_name='' --base_model=casperhansen/mixtral-instruct-awq --promt_type=zephyr --max_seq_len=4096  --pre_load_embedding_model=True --enable_tts=False --enable_stt=False --enable_transcriptions=False --extra_allowed_paths=[\"Ai_test_data\"] --use_gpu_id=False --inference_server=vllm_chat:http://localhost:5001/v1/\r\n\r\n5) I concur with you, all other Mistral AWQ versions tried by me are unable to give any RAG results (in inference server mode) and provide gibberish output. Even the Ollama Mistral cannot provide info and states that the information is not available in the document.  \r\n\r\n6) The best results were without using an inference server and running the mistralai/Mixtral-8x7B-v0.1 (It gave close to correct output but not the full details) \r\n\r\n7) The POC test use case is clear, there are 2 annual reports with a table having 24 line items of investee companies of a PE Fund. Compare these 2 year annual reports and tell us the new investments. "
      },
      {
        "user": "pseudotensor",
        "body": "Can you use `vllm` instead of `vllm_chat`   w.r.t. that user/assistant issue?"
      },
      {
        "user": "rohitnanda1443",
        "body": " \r\n![SmartSelect_20240417_184333_Chrome](https://github.com/h2oai/h2ogpt/assets/159398448/1cdf5223-6c8a-48e6-9a05-dc952499eb61)\r\n\r\nThe model runs fine during normal LLM operation but during RAG I get the above error. \r\n\r\n\r\nThe run scripts are: \r\nVLLM: python -m vllm.entrypoints.openai.api_server --port=5001 --host=0.0.0.0 --model casperhansen/mixtral-instruct-awq --seed 1234 --tensor-parallel-size=1 --max-num-batched-tokens=8192 --max-log-len=100 --trust-remote-code --worker-use-ray --enforce-eager --gpu-memory-utilization 0.85 –max-model-len=8192 &  (Was getting error when I added --quantization awq )\r\n\r\nGPT: python generate.py --auth_access=closed --auth=auth.json --guest_name='' --base_model=casperhansen/mixtral-instruct-awq --promt_type=zephyr --max_seq_len=8192 --pre_load_embedding_model=True --enable_tts=False --enable_stt=False --enable_transcriptions=False --extra_allowed_paths=[\"Ai_test_data\"] --use_gpu_id=False --inference_server=vllm:http://localhost:5001/v1/ &\r\n\r\n\r\n"
      },
      {
        "user": "rohitnanda1443",
        "body": "If I add the prompt type  --prompt_type=openai_chat, then the above error is gone but I get garbage during LLM chat: \r\n\r\n\r\n![SmartSelect_20240418_115404_Chrome](https://github.com/h2oai/h2ogpt/assets/159398448/f36fd61d-77a1-4f89-b94f-b5f4d27c3444)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "For --inference_server it should be `vllm:localhost:5001` in simplest case.  But you are probably also running old h2oGPT so will continue to hit that alternating issue until you upgrade."
      },
      {
        "user": "rohitnanda1443",
        "body": "Understood,\r\n\r\nAlso how does one work with gated huggingface models?  The mistral has\r\nbecome gated now and giving the access via huggingface-cli login does not\r\nhelp.\r\n\r\nOn Fri, 19 Apr 2024, 07:40 pseudotensor, ***@***.***> wrote:\r\n\r\n> For --inference_server it should be vllm:localhost:5001 in simplest case.\r\n> But you are probably also running old h2oGPT so will continue to hit that\r\n> alternating issue until you upgrade.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/h2oai/h2ogpt/issues/1549#issuecomment-2065622227>, or\r\n> unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/BGADUMEPNXLSRC4HVVKX5CDY6B4K7AVCNFSM6AAAAABGBUAFLGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDANRVGYZDEMRSG4>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Yes, I noticed same thing.\r\n\r\nYou either set the env of pass to CLI the auth token.\r\n```\r\nHUGGING_FACE_HUB_TOKEN=hf_xxxx python generate.py ...\r\n```\r\nor\r\n```\r\npython generate.py --use_auth_token=hf_xxxx\r\n```"
      }
    ]
  },
  {
    "issue_number": 1566,
    "title": "Can you make_db from documents stored on another (for example, PostgreSQL)",
    "author": "vitalyshalumov",
    "state": "open",
    "created_at": "2024-04-17T06:55:59Z",
    "updated_at": "2024-04-19T02:08:52Z",
    "labels": [],
    "body": null,
    "comments": [
      {
        "user": "pseudotensor",
        "body": "We support what some vector databases support.  We support vector dbs `['chroma', 'weaviate', 'faiss', 'qdrant']`"
      },
      {
        "user": "pseudotensor",
        "body": "I'm sure you can figure out how to convert documents from one db, export them, and then from those raw documents import using make_db."
      }
    ]
  },
  {
    "issue_number": 1550,
    "title": "Permissions in VectorDB",
    "author": "vitalyshalumov",
    "state": "closed",
    "created_at": "2024-04-11T08:32:55Z",
    "updated_at": "2024-04-19T00:38:28Z",
    "labels": [],
    "body": "Hi,\r\nI want each user with his own permissions to the VDB.\r\nHow can I set permissions for each document in the VDB?\r\n\r\nThank you",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Hi, at moment every user has their own db unless it's a shared db .  We don't currently have capability to set permissions on each document within that shared db, so you can use the file system to control personal collections and their access.\r\n\r\nE.g. you can use hard or soft links with all docs, and then for each user make a personal db using make_db using a user_path that contains only the links required."
      },
      {
        "user": "vitalyshalumov",
        "body": "Can I somehow utilize permission options in qdrant or weaviate to enable said control per user?\r\n\r\nOr using cli, for a db that contains all documents, enable different documents per user?"
      },
      {
        "user": "pseudotensor",
        "body": "Well, what I was suggesting was related to your 2nd question.  You can use file system to avoid dups via soft/hard links in linux, but that's a detail.  In general you can have different folders per user.\r\n\r\nE.g. if the user is \"jon\" then the folders end up looking like:\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ ls -alrt users/jon/\r\ntotal 84\r\ndrwx------   2 jon jon  4096 Apr  8 01:49 db_dir_yuppy/\r\ndrwx------   2 jon jon  4096 Apr  8 01:49 db_dir_xxx/\r\ndrwx------   2 jon jon  4096 Apr  8 01:49 db_dir_testsum1/\r\ndrwx------   2 jon jon  4096 Apr  8 01:49 db_dir_feefef/\r\ndrwx------   2 jon jon  4096 Apr  8 01:49 db_dir_dudedata/\r\ndrwx------   2 jon jon  4096 Apr  8 01:49 db_dir_dogdata1/\r\ndrwx------   2 jon jon  4096 Apr  8 01:49 db_dir_dogdata/\r\ndrwx------   2 jon jon  4096 Apr  8 01:49 db_dir_aaaaa/\r\ndrwx------  12 jon jon  4096 Apr  8 02:11 ./\r\ndrwx------   3 jon jon  4096 Apr  8 02:12 db_dir_asdfasdf/\r\ndrwx------   3 jon jon  4096 Apr  9 08:44 db_dir_MyData/\r\ndrwx------ 431 jon jon 36864 Apr 16 11:20 ../\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ \r\n```\r\n\r\nfor personal collections.\r\n\r\nTo make such dbs one would do:\r\n\r\n```\r\npython src/make_db.py --user_path=user_path_jon --collection_name=JonData --langchain_type=personal --hf_embedding_model=hkunlp/instructor-large --persist_directory=users/jon/db_dir_JonData\r\n```\r\nThen you'll have:\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ ls -alrt users/jon/db_dir_JonData/\r\ntotal 264\r\ndrwx------ 13 jon jon   4096 Apr 16 12:28 ../\r\ndrwx------  2 jon jon   4096 Apr 16 12:28 d7ccacb6-93fe-4380-9340-b7f5edffb655/\r\n-rw-------  1 jon jon 249856 Apr 16 12:28 chroma.sqlite3\r\n-rw-------  1 jon jon     41 Apr 16 12:28 embed_info\r\ndrwx------  3 jon jon   4096 Apr 16 12:28 ./\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ \r\n```\r\n\r\nYou can add that database to the auth.json for their entry if using auth.json type file, and they will see when they login.\r\n\r\nOr you can have the user add that collection by name (JonData).  i.e. at first user might see:\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/b3d82153-3f34-4a53-9d72-8979c4c9cc19)\r\n\r\n\r\nbut if they do:\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/b6b879bb-a82d-477e-bf96-416989495229)\r\n\r\n\r\nand hit enter, then they will see:\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/87a07f9c-73ee-4be7-9ae7-5d576e76a1a2)\r\n\r\n\r\nand you'll see the document count (3) is what I expect from what was in the original input folder."
      },
      {
        "user": "pseudotensor",
        "body": "I think this is sufficient for now."
      },
      {
        "user": "vitalyshalumov",
        "body": "Thank you. One hopefully last question. Can I approach it from document selection perspective, i.e, select only specific documents at the launch of gradio (an option that is available in the UI)?"
      },
      {
        "user": "pseudotensor",
        "body": "Yes, that is the CLI option `--document_choice` that is a list of document names -- has to be exact match to like those that appear in UI or via API."
      }
    ]
  },
  {
    "issue_number": 914,
    "title": "configure the serper api key? help help help 🐤 ",
    "author": "zephirusgit",
    "state": "closed",
    "created_at": "2023-10-01T07:49:03Z",
    "updated_at": "2024-04-17T13:29:13Z",
    "labels": [
      "type/question"
    ],
    "body": "Does anyone know how to configure the serper api key? I saw that it has the option to search the internet using Google, with the serper api key that allows you to use 2500 requests for free.\r\nI saw that I got the error message that it was not configured,\r\nI registered on the page to get an API\r\nhttps://serper.dev\r\nI generated an api, registering,\r\nI tested the api in python and it worked fine.\r\nlook in the files where the serpapi_api_key was\r\nand I found it in the file called gpt_langchain.py\r\n\r\n if LangChainAgent.SEARCH.value in langchain_agents:\r\n        output_parser = H2OMRKLOutputParser()\r\n\r\n\r\n###############################################original say ('SERPAPI_API_KEY'))\r\n        tools = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=os.environ.get('jsdjsdjdsjdsjsdjapi keyy herejhashsaishas'))\r\n\r\n\r\n\r\n        if inference_server.startswith('openai'):\r\n            agent_type = AgentType.OPENAI_FUNCTIONS\r\n            agent_executor_kwargs = {\"handle_parsing_errors\": True, 'output_parser': output_parser}\r\n        else:\r\n            agent_type = AgentType.ZERO_SHOT_REACT_DESCRIPTION\r\n            agent_executor_kwargs = {'output_parser': output_parser}\r\n        chain = initialize_agent(tools, llm, agent=agent_type,\r\n                                 agent_executor_kwargs=agent_executor_kwargs,\r\n                                 agent_kwargs=dict(output_parser=output_parser,\r\n                                                   format_instructions=output_parser.get_format_instructions()),\r\n                                 output_parser=output_parser,\r\n                                 max_iterations=10,\r\n                                 verbose=True)\r\n           \r\n                      I saved it and everything but nothing, it still gives an error with everything, it doesn't even launch the chat, because it tries to search first.\r\n                      \r\n                      In the console I can see that it is still not seen\r\n                                 \r\n     File \"pydantic\\main.py\", line 341, in pydantic.main.BaseModel.__init__\r\npydantic.error_wrappers.ValidationError: 1 validation error for H2OSerpAPIWrapper\r\n__root__\r\n  Did not find serpapi_api_key, please add an environment variable `SERPAPI_API_KEY` which contains it, or pass  `serpapi_api_key` as a named parameter. (type=value_error)                              ",
    "comments": [
      {
        "user": "zephirusgit",
        "body": "I tried the example in python and it works, but I don't see it even close to the python source code, enough to understand the error, and I don't understand much about python to adapt it, (the api key not is my api key :p)\r\n\r\nimport http.client\r\nimport json\r\n\r\nconn = http.client.HTTPSConnection(\"google.serper.dev\")\r\npayload = json.dumps({\r\n  \"q\": \"here the search\"\r\n})\r\nheaders = {\r\n  'X-API-KEY': '4323234apiapiapi424',\r\n  'Content-Type': 'application/json'\r\n}\r\nconn.request(\"POST\", \"/search\", payload, headers)\r\nres = conn.getresponse()\r\ndata = res.read()\r\nprint(data.decode(\"utf-8\"))"
      },
      {
        "user": "zephirusgit",
        "body": "I'm going to try the Windows version, although I was running it in PowerShell, the one on GitHub"
      },
      {
        "user": "zephirusgit",
        "body": "![search](https://github.com/h2oai/h2ogpt/assets/20031912/12943ab5-9912-4096-ad8c-65b4a751a80e)\r\nomg not have the web search option! 😵‍💫 can add it, and the ask from the api key?¿?\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "My documentation is behind on the new experimental features.\r\n\r\n1)\r\n```bash\r\npip install -r reqs_optional/requirements_optional_agents.txt\r\n````\r\n\r\n2) Setup account at https://serpapi.com/ (they have some number of free searches for free accounts)\r\n\r\n3) Setup ENV that defines: `SERPAPI_API_KEY`\r\n\r\nHowever, just be aware that we noticed that when web search is enabled, eventually it leads to some closing of sys.stdout and one gets these errors:\r\n```\r\nValueError: I/O operation on closed file.\r\n```\r\nI couldn't see from their package why this is happening.  So it's still experimental.\r\n\r\nNote the agent parts are even more experimental, e.g. search agent.  It's better than what is in langchain, but it needs more work to function well with llama2 etc.\r\n\r\nThe windows one-click installer is behind.  I'll update its release once the above I/O issue is resolved.\r\n"
      },
      {
        "user": "PiPlusTheta",
        "body": "![WhatsApp Image 2024-04-11 at 17 43 14_e4765d04](https://github.com/h2oai/h2ogpt/assets/68808227/035a85db-5ad1-4c70-8a2e-b57ce6951e09)\r\n\r\nI encountered an issue while attempting to perform web searches using the SerpApi package. Despite setting the SERPAPI_API_KEY environment variable with the appropriate API key value as instructed, I'm still unable to execute web searches successfully.\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/68808227/143016d8-43e3-4385-9693-bb7d1a735a0f)\r\n\r\n\r\nSteps to Reproduce:\r\n\r\n    Set the SERPAPI_API_KEY environment variable with the correct API key value.\r\n    Attempt to perform a web search using the SerpApi package.\r\n    Observe the validation error message: \"Did not find serpapi_api_key, please add an environment variable SERPAPI_API_KEY which contains it, or pass serpapi_api_key as a named parameter.\"\r\n\r\nExpected Behavior:\r\nAfter setting the SERPAPI_API_KEY environment variable, I expect to be able to perform web searches using the SerpApi package without encountering validation errors.\r\n\r\nActual Behavior:\r\nDespite setting the SERPAPI_API_KEY environment variable, the validation error persists, preventing me from executing web searches.\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Hi, I'm not sure what's wrong.  I have to believe that the envs you are setting are not being seen by h2oGPT.\r\n\r\nIt works fine for me to export the env:\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/88374d35-5f90-41b2-9e97-73131f52807e)\r\n"
      },
      {
        "user": "PiPlusTheta",
        "body": "I tried exporting the key from the environment and running search queries using the key in a different Python file, and it worked without any issues. But it's not working in H2oGPT.\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Not sure what is wrong.  It can't be just h2oGPT since it works for me.  Also, one can't even choose the web search option if gradio_runner.py doesn't see the key.\r\n\r\nI tried just all on single command line, both with and without the key, and I always get the expected behavior.\r\n\r\nAre you running via docker or directly in python?"
      },
      {
        "user": "PiPlusTheta",
        "body": "I'm directly running it in python. still no luck."
      }
    ]
  },
  {
    "issue_number": 1564,
    "title": "HuggingFaceM4/idefics2-8b as vision model",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2024-04-16T22:12:18Z",
    "updated_at": "2024-04-16T22:12:18Z",
    "labels": [],
    "body": "requires dev transformers 4.40.0\r\n\r\nThey said should be in TGI soon.\r\n\r\n```\r\nimport requests\r\nimport torch\r\nfrom PIL import Image\r\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\r\nfrom io import BytesIO\r\n\r\nimages = [\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\r\n\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\",\r\n\"https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg\",\r\n]\r\n\r\nimages = [Image.open(BytesIO(requests.get(image).content)) for image in images]\r\n\r\n# Note that passing the image urls (instead of the actual pil images) to the processor is also possible\r\n\r\nprocessor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\r\nmodel = AutoModelForVision2Seq.from_pretrained(\r\n    \"HuggingFaceM4/idefics2-8b\",\r\n    torch_dtype=torch.bfloat16,\r\n    _attn_implementation=\"flash_attention_2\",\r\n    device_map='auto',\r\n)\r\n\r\n\r\n# Create inputs\r\nmessages = [\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n            {\"type\": \"image\"},\r\n            {\"type\": \"text\", \"text\": \"What do we see in this image?\"},\r\n        ]\r\n    },\r\n    {\r\n        \"role\": \"assistant\",\r\n        \"content\": [\r\n            {\"type\": \"text\", \"text\": \"In this image, we can see the city of New York, and more specifically the Statue of Liberty.\"},\r\n        ]\r\n    },\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n            {\"type\": \"image\"},\r\n            {\"type\": \"text\", \"text\": \"And how about this image?\"},\r\n        ]\r\n    },\r\n]\r\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\r\ninputs = processor(text=prompt, images=[images[0], images[1]], return_tensors=\"pt\")\r\n\r\nwith torch.device('cuda'):\r\n    inputs = {k: v.to('cuda') for k, v in inputs.items()}\r\n    # Generate\r\n    generated_ids = model.generate(**inputs, max_new_tokens=500)\r\n    generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\r\n\r\nprint(generated_texts)\r\n\r\n```\r\n\r\ngives:\r\n```\r\n['User: What do we see in this image? \\nAssistant: In this image, we can see the city of New York, and more specifically the Statue of Liberty. \\nUser: And how about this image? \\nAssistant: In this image we can see buildings, trees, lights, water and sky.']\r\n```\r\n\r\n\r\n",
    "comments": []
  },
  {
    "issue_number": 1561,
    "title": "AWQ Model Works from UI in Windows, But Fails When Launched from .bat File",
    "author": "Yusuf-Demiryurek",
    "state": "open",
    "created_at": "2024-04-15T14:10:46Z",
    "updated_at": "2024-04-16T07:46:26Z",
    "labels": [],
    "body": "Hi,\r\n\r\nI need your help :)\r\n\r\nI'm trying to launch H2OGPT from a .bat file on Windows with an AWQ model as a parameter, but it doesn't seem to work. Below is the content of the .bat file :\r\n`cd %HOMEDRIVE%\\%HOMEPATH%\\h2ogpt_data`\r\n`C:\\Users\\yuyu\\AppData\\Local\\Programs\\h2oGPT\\Python\\pythonw.exe \"C:\\Users\\yuyu\\AppData\\Local\\Programs\\h2oGPT\\h2oGPT.launch.pyw\" --load_awq=model --base_model=TheBloke/zephyr-7B-beta-AWQ `\r\n\r\nWhen I use a GGUF model instead of AWQ, it works fine. Also, loading the AWQ model from the UI works without any issues.\r\n\r\nI've tried various parameters and searched through the documentation, but nothing has helped me to get it running from the .bat file.\r\n\r\nI'm running it on a computer with 128GB of RAM and an RTX 4090 GPU.\r\n\r\nCould someone please help me? Thanks. 😄 ",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I'm not an expert at .bat files.  I used chatgpt to help me with .bat file stuff for h2oGPT, so I recommend doing same with your errors or seeing if someone else will respond.\r\n\r\nAlso, you didn't say what the issue is."
      },
      {
        "user": "Yusuf-Demiryurek",
        "body": "I don't believe the issue lies with the .bat file itself. The .bat file functions similarly to the python generate.py command with parameters. The problem arises when attempting to run the AWQ model through the .bat file. Interestingly, the AWQ model runs perfectly fine when loaded from the UI, but H2OGPT doesn't start when using the .bat file.\r\n\r\nFurthermore, if I use --base_model=TheBloke/zephyr-7B-beta-GGUF instead of --base_model=TheBloke/zephyr-7B-beta-AWQ in the .bat file, H2OGPT starts without any issues with the GGUF model loaded.\r\n\r\nThere appears to be a discrepancy in behavior between loading the model from the UI and using the --base_model input in the .bat file."
      },
      {
        "user": "pseudotensor",
        "body": "Hi, please say what the issue actually is.  You haven't said what the error or stack trace is etc."
      },
      {
        "user": "Yusuf-Demiryurek",
        "body": "Sorry, I didn't know where to find the error.\r\nAfter consulting GPT, I found the error in the h2ogpt_exception.log file:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\win_run_app.py\", line 150, in main\r\n    _main()\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\win_run_app.py\", line 136, in _main\r\n    main_h2ogpt()\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\generate.py\", line 12, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\src\\utils.py\", line 65, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\fire\\core.py\", line 141, in Fire\r\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\fire\\core.py\", line 475, in _Fire\r\n    component, remaining_args = _CallAndUpdateTrace(\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\fire\\core.py\", line 691, in _CallAndUpdateTrace\r\n    component = fn(*varargs, **kwargs)\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\src\\gen.py\", line 1977, in main\r\n    model0, tokenizer0, device = get_model_retry(reward_type=False,\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\src\\gen.py\", line 2305, in get_model_retry\r\n    model1, tokenizer1, device1 = get_model(**kwargs)\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\src\\gen.py\", line 2455, in get_model\r\n    model_loader, tokenizer_loader, conditional_type = get_loaders(**loader_kwargs)\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\..\\src\\loaders.py\", line 100, in get_loaders\r\n    from awq import AutoAWQForCausalLM\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\awq\\__init__.py\", line 2, in <module>\r\n    from awq.models.auto import AutoAWQForCausalLM\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\awq\\models\\__init__.py\", line 1, in <module>\r\n    from .mpt import MptAWQForCausalLM\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\awq\\models\\mpt.py\", line 1, in <module>\r\n    from .base import BaseAWQForCausalLM\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\awq\\models\\base.py\", line 16, in <module>\r\n    from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\pkgs\\awq\\modules\\linear.py\", line 4, in <module>\r\n    import awq_inference_engine  # with CUDA kernels\r\nImportError: DLL load failed while importing awq_inference_engine: Le module sp cifi  est introuvable.\r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "I can import upper level thing but I don't have the other thing:\r\n\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ python\r\nPython 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import awq_inference_engine\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'awq_inference_engine'\r\n>>> from awq import AutoAWQForCausalLM\r\n>>> \r\n```\r\n\r\nProbably the awq install instructions are out of date for windows, will check on it."
      },
      {
        "user": "Yusuf-Demiryurek",
        "body": "I got this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\balas\\AppData\\Local\\Programs\\h2oGPT\\Python\\test.py\", line 1, in <module>\r\n    import awq_inference_engine\r\nImportError: DLL load failed while importing awq_inference_engine: Le module spécifié est introuvable.\r\n```\r\nI don't understand why it works when I load the model from UI and not in this way\r\n"
      }
    ]
  },
  {
    "issue_number": 1560,
    "title": "python dependency module version tweaks",
    "author": "spodzone",
    "state": "open",
    "created_at": "2024-04-15T12:20:34Z",
    "updated_at": "2024-04-15T15:03:21Z",
    "labels": [],
    "body": "If you're following the [Get Started](https://github.com/h2oai/h2ogpt?tab=readme-ov-file#get-started) instructions, you should also add these just before the final call to python generate.py:\r\n\r\n```\r\n$ pip uninstall cryptography\r\n$ pip install --upgrade cryptography==36.0.2\r\n$ pip install --upgrade jinja2==3.1.2\r\n\r\n```\r\n\r\nAlternatively maybe update one of the requirements files.\r\n\r\nHTH",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Can you say why?  I have `cryptography==42.0.5` and no issues."
      }
    ]
  },
  {
    "issue_number": 1551,
    "title": "Support for AWS Bedrock",
    "author": "slavag",
    "state": "open",
    "created_at": "2024-04-11T09:31:14Z",
    "updated_at": "2024-04-15T02:25:27Z",
    "labels": [],
    "body": "Hi, \r\nIs there any plans to add AWS Bedrock support ? \r\n\r\nThanks",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "https://python.langchain.com/docs/integrations/llms/bedrock/\r\n\r\nIt looks easy enough."
      }
    ]
  },
  {
    "issue_number": 1552,
    "title": "vLLM GROQ issue",
    "author": "chrissas",
    "state": "closed",
    "created_at": "2024-04-12T06:59:04Z",
    "updated_at": "2024-04-15T02:23:28Z",
    "labels": [
      "type/question"
    ],
    "body": "@pseudotensor\r\n\r\nGROQ url mentioned in the documentation is wrong, I had confirmation from the groq team https://github.com/h2oai/h2ogpt/blob/main/docs/FAQ.md.\r\n\r\nhttps://api.groq.com/openai:None:/v1: is deprecated and unused.\r\n\r\n<img width=\"1050\" alt=\"image\" src=\"https://github.com/h2oai/h2ogpt/assets/22572193/5a759861-2c8b-48c5-8316-edaa10fe4281\">\r\n\r\nI tried their url but I still get this error\r\nException: Error code: 404 - {'error': {'message': 'Unknown request URL: POST /openai/v1/completions:/completions. Please check the URL for typos, or see the docs at https://console.groq.com/docs/', 'type': 'invalid_request_error', 'code': 'unknown_url'}}\r\n<img width=\"1210\" alt=\"image\" src=\"https://github.com/h2oai/h2ogpt/assets/22572193/bd5d64f0-767b-4551-99f5-b319dba47d54\">\r\n\r\n command :\r\npython generate.py --model_lock=\"[{'inference_server':'vllm:https://api.groq.com/openai/v1/chat/completions:<myAPIkey>', 'base_model':'mixtral-8x7b-32768', 'max_seq_len': 31744, 'prompt_type':'plain'}]\" \r\n\r\ncommand :\r\npython generate.py --model_lock=\"[{'inference_server':'vllm:https://api.groq.com/openai:None:/v1/chat:GroqAPIkey', 'base_model':'mixtral-8x7b-32768', 'max_seq_len': 31744, 'prompt_type':'plain'}]\"\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/22572193/814f3b47-1ee5-4375-ab8e-81701f8deb08)\r\n\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Use the other way mentioned for groq a bit lower in the FAQ, i.e. \r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/07acd843-ecec-4d10-b613-827565e5b1d3)\r\n"
      }
    ]
  },
  {
    "issue_number": 1553,
    "title": "Mac OS auto installer doesn't work after manual uninstallation",
    "author": "antoninadert",
    "state": "open",
    "created_at": "2024-04-12T07:55:46Z",
    "updated_at": "2024-04-15T02:22:03Z",
    "labels": [],
    "body": "Hello, \r\n\r\nI tried to manually uninstall the MacOS installer, since it put many files all over the place in a chaotic way. \r\nLooking in the terminal in the install code, I identified this folder as the main installation folder: `/var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl/`\r\n\r\nExtract from the terminal :\r\n```\r\n**file**: /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl/mac_run_app.py\r\nPYTHONPATH:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl\r\nPath_1:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl\r\nNLTK_DATA:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl/nltk_data\r\nPATH:  /Users/aadert/CODE/miniconda3/bin:/Users/aadert/CODE/miniconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl/poppler/bin/:/var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl/poppler/lib/:/var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl/Tesseract-OCR\r\nPath_3:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl/h2ogpt/src\r\nPath_3:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl/h2ogpt/iterators\r\nPath_3:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl/h2ogpt/gradio_utils\r\nPath_3:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl/h2ogpt/metrics\r\nPath_3:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl/h2ogpt/models\r\nPath_3:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIYPi5Sl/h2ogpt/.\r\n\r\n**file**: /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIb1Gyf3/mac_run_app.py\r\nPYTHONPATH:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIb1Gyf3\r\nPath_1:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIb1Gyf3\r\nNLTK_DATA:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIb1Gyf3/nltk_data\r\n```\r\n\r\nThat being said, it seems the installer also put files all other the place, like in the main user directory `~/`, like `~/.cache` for example\r\n\r\nI tried to remove all folders related to the installation manually, but it seems I am missing something because now if I try to reinstall, I will run into errors. \r\n\r\nI see many potential solutions : \r\n- An official uninstaller ?\r\n- A manual uninstall documentation for the autoinstaller\r\n- Being able to see what the autoinstaller actually does. Today it is quite obfuscated as this is a binary format it seems. \r\n\r\nThanks for your help",
    "comments": []
  },
  {
    "issue_number": 1540,
    "title": "Size of Tensor A must match size of Tensor B",
    "author": "rohitnanda1443",
    "state": "open",
    "created_at": "2024-04-09T11:46:34Z",
    "updated_at": "2024-04-15T02:21:16Z",
    "labels": [],
    "body": "HI, \r\n\r\nI am trying to do RAG query on a large PDF file and get the below error:\r\n\r\nError: The size of tensor a (3351) must match the size of tensor b (4096) at non-singleton dimension 3.\r\n\r\nThe run script: python generate.py --base_model=mistralai/Mixtral-8x7B-Instruct-v0.1 --pre_load_embedding_model=True  --score_model=None --enable_tts=False --enable_stt=False --enable_transcriptions=False --auth=auth.json --system_prompt=\"My name is H2O-GPT and I am an intelligent AI\" --attention_sinks=True --max_new_tokens=100000 --max_max_new_tokens=100000 --top_k_docs=-1 --use_gpu_id=False --max_seq_len=4096 --sink_dict=\"{'num_sink_tokens': 4, 'window_length': 4096}\" \r\n\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Can you provide more of the stack trace?  My guess is that attention sinks in transformers is not bug free.\r\n\r\nSeparately, I recommend using Mixtral through vLLM in general.  Likely it will be hard to make Mixtral run for long sequences, and it already supports 32k total input+output."
      },
      {
        "user": "pseudotensor",
        "body": "I tried same with Mistral and didn't find any issues.\r\n\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/f9c8b972-b899-45c9-93fd-426f39d0dadd)\r\n\r\n\r\nBut the output and input aren't very long.    The input isn't very long because you set `--max_seq_len=4096`, so it tries to take 29 docs but gets reduced."
      },
      {
        "user": "rohitnanda1443",
        "body": "Noted. \r\n\r\nI will try again with removing --max_seq_len=4096\r\n\r\nAlso where are the error log files saved in the H2O-GPT folder? (so that i can send you the stack trace) Does one get the CLI output dumped on a file using \"> /home/user/dump\" after the CLI startup script? "
      },
      {
        "user": "rohitnanda1443",
        "body": "I tried using Mixtral with vLLM and did the following: \r\n\r\n1) Using this guide: https://github.com/h2oai/h2ogpt/blob/main/docs/README_InferenceServers.md Local install the Inference server\r\n\r\n2) Ran inference server: NCCL_SHM_DISABLE=1 CUDA_VISIBLE_DEVICES=0 text-generation-launcher --model-id h2oai/h2ogpt-oig-oasst1-512-6_9b --port 8080  --sharded false --trust-remote-code --max-stop-sequences=6 \r\n\r\n3) Ran Model: python generate.py --base_model=mistralai/Mixtral-8x7B-Instruct-v0.1 --prompt_type=zephyr --max_seq_len=4096 --pre_load_embedding_model=True  --score_model=None --enable_tts=False --enable_stt=False --enable_transcriptions=False --max_seq_len=4096 --auth=auth.json  --inference_server=\"http://127.0.0.1:8080\" &\r\n\r\n<img width=\"327\" alt=\"image\" src=\"https://github.com/h2oai/h2ogpt/assets/159398448/edcf1669-62b3-42fc-883a-e29fafa97d2b\">\r\n"
      },
      {
        "user": "rohitnanda1443",
        "body": "Issue: \r\n\r\nUnable to connect to the inference server: After starting the inference server if I do the curl test to connect to it I get the connection refused error at the port. \r\n\r\n\r\nThe Gradio Dump: \r\n\r\nUsing Model mistralai/mixtral-8x7b-instruct-v0.1\r\nload INSTRUCTOR_Transformer\r\nmax_seq_length  512\r\nStarting get_model: mistralai/Mixtral-8x7B-Instruct-v0.1 http:://127.0.0.1:8080\r\nGR Client Begin: http://http: mistralai/Mixtral-8x7B-Instruct-v0.1\r\nGR Client Failed http://http: mistralai/Mixtral-8x7B-Instruct-v0.1: HTTPConnectionPool(host='http', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7f7357c07bb0>: Failed to resolve 'http' ([Errno -2] Name or service not known)\"))\r\nHF Client Begin: http://http: mistralai/Mixtral-8x7B-Instruct-v0.1\r\nHF Client Failed http://http: mistralai/Mixtral-8x7B-Instruct-v0.1: Traceback (most recent call last):\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connection.py\", line 198, in _new_conn\r\n    sock = connection.create_connection(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/urllib3/util/connection.py\", line 60, in create_connection\r\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/socket.py\", line 955, in getaddrinfo\r\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\r\nsocket.gaierror: [Errno -2] Name or service not known\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 793, in urlopen\r\n    response = self._make_request(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 496, in _make_request\r\n    conn.request(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connection.py\", line 400, in request\r\n    self.endheaders()\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/http/client.py\", line 1278, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/http/client.py\", line 1038, in _send_output\r\n    self.send(msg)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/http/client.py\", line 976, in send\r\n    self.connect()\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connection.py\", line 238, in connect\r\n    self.sock = self._new_conn()\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connection.py\", line 205, in _new_conn\r\n    raise NameResolutionError(self.host, self, e) from e\r\nurllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPConnection object at 0x7f735dbfd000>: Failed to resolve 'http' ([Errno -2] Name or service not known)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\r\n    resp = conn.urlopen(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 847, in urlopen\r\n    retries = retries.increment(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/urllib3/util/retry.py\", line 515, in increment\r\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\r\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='http', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7f735dbfd000>: Failed to resolve 'http' ([Errno -2] Name or service not known)\"))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/root/h2ogpt/src/gen.py\", line 2498, in get_client_from_inference_server\r\n    res = hf_client.generate('What?', max_new_tokens=1)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/text_generation/client.py\", line 275, in generate\r\n    resp = requests.post(\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/requests/api.py\", line 115, in post\r\n    return request(\"post\", url, data=data, json=json, **kwargs)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/requests/api.py\", line 59, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/root/miniconda3/envs/h2ogpt/lib/python3.10/site-packages/requests/adapters.py\", line 519, in send\r\n    raise ConnectionError(e, request=request)\r\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='http', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7f735dbfd000>: Failed to resolve 'http' ([Errno -2] Name or service not known)\"))\r\n\r\nHF Client End: http://http: mistralai/Mixtral-8x7B-Instruct-v0.1 : None\r\nBegin auto-detect HF cache text generation models\r\nEnd auto-detect HF cache text generation models\r\nBegin auto-detect llama.cpp models\r\nEnd auto-detect llama.cpp models\r\nRunning on local URL:  http://0.0.0.0:7863\r\n\r\nTo create a public link, set share=True in launch().\r\nStarted Gradio Server and/or GUI: server_name: localhost port: None\r\nUse local URL: http://localhost:7863/"
      },
      {
        "user": "pseudotensor",
        "body": "If you look at the trace, you have an odd \"Begin: http:://127.0.0.1:8080\" with extra :.  As in the docs, with vLLM one would do something like `vllm:127.0.0.1:8080` or with HF client `http://127.0.0.1:8080` but not extra :'s"
      }
    ]
  },
  {
    "issue_number": 1455,
    "title": "How can I create a REST API for Flutter SDK?",
    "author": "mooodali",
    "state": "closed",
    "created_at": "2024-03-06T11:23:09Z",
    "updated_at": "2024-04-15T01:17:17Z",
    "labels": [],
    "body": "Hi guys\r\nHow can I create a REST API for FlutterSDK?\r\nthank you",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "OpenAI proxy server is rest API.  Maybe use that?"
      },
      {
        "user": "mooodali",
        "body": "Thank you for your reply\r\n  No, I want to install h2ogpt on the server and create a REST API\r\nDo you think it is possible?\r\nIf possible, tell me the method"
      },
      {
        "user": "mohamed-alired",
        "body": "?"
      },
      {
        "user": "pseudotensor",
        "body": "The OpenAI proxy can be accessed as REST API.  I don't have detailed documentation for flutter specifically, but any chat access and model listing is possible via OpenAI client access."
      }
    ]
  },
  {
    "issue_number": 1554,
    "title": "RuntimeError: An error occurred while downloading using `hf_transfer`.",
    "author": "llmwesee",
    "state": "closed",
    "created_at": "2024-04-12T08:54:00Z",
    "updated_at": "2024-04-12T14:56:36Z",
    "labels": [],
    "body": "I created fresh environment with latest h2ogpt repo. After doing all the necessary steps I launched the h2ogpt with the the following command: \r\n\r\n```\r\npython generate.py --base_model=meta-llama/Llama-2-13b-chat-hf --score_model=None --langchain_mode=UserData --user_path=user_path --use_auth_token=True --max_seq_le\r\nn=4096 --max_max_new_tokens=2048 --max_new_tokens=2048 --min_new_tokens=128  --prompt_type=llama2\r\n```\r\n\r\nwhen I doing it  and the model downloading is getting started and after sometime it getting stucked in between and sometimes throws the following error:\r\n`RuntimeError: An error occurred while downloading using `hf_transfer`. Consider disabling HF_HUB_ENABLE_HF_TRANSFER for better error handling.`\r\n\r\nNote:  The mistral 7b is downloading and running perfectly. \r\n\r\n![Screenshot from 2024-04-12 14-22-50](https://github.com/h2oai/h2ogpt/assets/137979399/6ba31a5e-0b54-41f7-a422-2b035e9af806)\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "You can set \r\n```\r\nexport HF_HUB_ENABLE_HF_TRANSFER=0\r\n```\r\nand we'll not use hf_transfer in latest main.  It's vastly faster for those that have high speed internet, so why it's default."
      }
    ]
  },
  {
    "issue_number": 1530,
    "title": "Mac OS manual Installation runs error",
    "author": "antoninadert",
    "state": "closed",
    "created_at": "2024-04-04T08:51:44Z",
    "updated_at": "2024-04-12T07:49:46Z",
    "labels": [],
    "body": "At this step of the install \r\n\r\n` pip install -r requirements.txt --extra-index https://download.pytorch.org/whl/cpu -c reqs_optional/reqs_constraints.txt`\r\n\r\nIt runs these errors : \r\n\r\n```Defaulting to user installation because normal site-packages is not writeable\r\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\r\nCollecting gradio@ https://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.20.1-py3-none-any.whl (from -r requirements.txt (line 7))\r\n  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)'))': /h2ogpt/gradio-4.20.1-py3-none-any.whl\r\n  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)'))': /h2ogpt/gradio-4.20.1-py3-none-any.whl\r\n  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)'))': /h2ogpt/gradio-4.20.1-py3-none-any.whl\r\n  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)'))': /h2ogpt/gradio-4.20.1-py3-none-any.whl\r\n  WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)'))': /h2ogpt/gradio-4.20.1-py3-none-any.whl\r\nERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='h2o-release.s3.amazonaws.com', port=443): Max retries exceeded with url: /h2ogpt/gradio-4.20.1-py3-none-any.whl (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)')))\r\n```\r\n\r\nIt seems the certificate doesn't work anymore.\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Very odd, no issues here.  Maybe aws had temporary issue?\r\n\r\nCan you try to just download the file, what happens?\r\n\r\nhttps://h2o-release.s3.amazonaws.com/h2ogpt/gradio-4.20.1-py3-none-any.whl"
      },
      {
        "user": "antoninadert",
        "body": "I tried to download it again manually following your link. It worked\r\nperfectly, but when I run `pip install -r requirements.txt`, I have the\r\nsame problem again.\r\n\r\nApparently it is my enterprise software which intercepts SSL connection here\r\n\r\nSo you have nothing to do, it is on my side ! We can close this issue\r\n"
      }
    ]
  },
  {
    "issue_number": 1544,
    "title": "auth related feature",
    "author": "zjudongze",
    "state": "closed",
    "created_at": "2024-04-10T02:55:25Z",
    "updated_at": "2024-04-11T21:42:49Z",
    "labels": [],
    "body": "How to implement such features:\r\n\r\n1. Anonymous use is not allowed; that is, when the webpage is opened, it prompts for login, and it can only be used after logging in.\r\n2. Allow the registration of new users.\r\n\r\ncurrently auth related feature can only fullfil the 2nd feature\r\nI don't how to disable anonymous usage.\r\nH2OGPT will silently create a new user for any new web sessions, resulting lots of uuid folders in users,\r\nsuch as this one:\r\n![image](https://github.com/h2oai/h2ogpt/assets/24449983/5100357f-f662-4e1d-ba3b-a395fc6ba650)\r\n\r\n",
    "comments": [
      {
        "user": "zjudongze",
        "body": "there are also no hints in login page. I mean when I register for a website, chrome usually will promt for new password and save it. When I login again, chrome will auto fill username and password.\r\n\r\nbut chrome can't recognize h2ogpt's login page. even if I save username and password in chrome manually, when I open the login page, there is no prompt or autofill from chrome."
      },
      {
        "user": "zjudongze",
        "body": "third: when I did login in h2ogpt, if I refresh the page, I am logout silently. \r\nI wish I am still in login state even if I refresh the page"
      },
      {
        "user": "santosh-gkg",
        "body": "for your first query of no anonymous usage not allowed you can pass --auth_access=closed in CLI, that will allow only registered users to sign in, you can also enable google auth, for me chrome detects the username and password and it also prompts to save it\r\n\r\nnew users can register themselves and save their history by logging again in the login tab \r\n\r\n"
      },
      {
        "user": "santosh-gkg",
        "body": "![image](https://github.com/h2oai/h2ogpt/assets/150844949/753a6cc2-4bf5-454d-9b35-f77802b1acbc)\r\n"
      },
      {
        "user": "zjudongze",
        "body": "> for your first query of no anonymous usage not allowed you can pass --auth_access=closed in CLI, that will allow only registered users to sign in, you can also enable google auth, for me chrome detects the username and password and it also prompts to save it\r\n> \r\n> new users can register themselves and save their history by logging again in the login tab\r\n\r\n--auth_access=closed only prohibit new register. but h2ogpt still siliently create a new user (ID is like uuid, such as 8220ae76-51a3-4bb7-aa33-1069ebf0ea95) for  sessions that does not login explicitly.\r\n\r\nI want to prohibit usage of h2ogpt if a user does not login manually and explicilty."
      },
      {
        "user": "pseudotensor",
        "body": "@zjudongze Use `--auth_access='closed'` .  It does not create a new user.  It's a full authentication block if they don't have the correct user name and password, nothing gets passed it, no gradio_runner.py etc. commands are run.\r\n\r\nIf you are (perhaps) referring to client session hash, that's set by client and not the server."
      },
      {
        "user": "zjudongze",
        "body": "> @zjudongze Use `--auth_access='closed'` . It does not create a new user. It's a full authentication block if they don't have the correct user name and password, nothing gets passed it, no gradio_runner.py etc. commands are run.\r\n> \r\n> If you are (perhaps) referring to client session hash, that's set by client and not the server.\r\n\r\nwhen I use --auth='closed', it almost fulfill my need. but there is still a bug.\r\nwhen I first open the website, it promt for login. I can use my previous registered user or create a new user.\r\nbut in both cases, after I login , the status in log-in/out page show \"Invalid password for user xxxx\"\r\n![image](https://github.com/h2oai/h2ogpt/assets/24449983/56fd6fba-6f19-4674-920d-f2a6b5039a69)\r\n\r\nwhen I refresh the main page, the chat history I saved under this user disappear under the chats tab. If I login in log-in/out tab, the chats tab will correctly show the chat history I save under that user.\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "with `--auth_access='closed'` you shouldn't be able to create a new user.\r\n\r\nIf you want to ensure that once logged in they cannot probe the login page, just set `--visible_login_tab=False`"
      },
      {
        "user": "pseudotensor",
        "body": "Hi, I did fix the bug of saying \"not successful login\"  That's in main now."
      }
    ]
  },
  {
    "issue_number": 312,
    "title": "Add vLLM support",
    "author": "pseudotensor",
    "state": "closed",
    "created_at": "2023-06-21T16:57:57Z",
    "updated_at": "2024-04-11T19:41:29Z",
    "labels": [],
    "body": "https://www.reddit.com/r/LocalLLaMA/comments/14em713/just_released_vllm_inference_library_that/?utm_source=share&utm_medium=ios_app&utm_name=ioscss&utm_content=2&utm_term=1\r\n\r\n\r\n[Just released - vLLM inference library that accelerates HF Transformers by 24x](https://www.reddit.com/r/LocalLLaMA/comments/14em713/just_released_vllm_inference_library_that/?utm_source=share&utm_medium=ios_app&utm_name=ioscss&utm_content=2&utm_term=1)\r\nPosted in r/LocalLLaMA by u/yanjb • 48 points and 8 comments\r\n\r\nLLM is an open-source LLM inference and serving library that accelerates HuggingFace Transformers by 24x and powers Vicuna and Chatbot Arena.\r\nGithub: https://github.com/vllm-project/vllmBlog post: https://vllm.ai/\r\nEdit - it wasn't \"just released\" apparently it's live for several days\r\nhttps://preview.redd.it/nzceocfbg87b1.png?width=1532&format=png&auto=webp&v=enabled&s=30bcee178214cb8c88e05c37db1b2215c0a1bf73",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Faster than TGI by 3-4x, does it have continuous batching?"
      },
      {
        "user": "rohitnanda1443",
        "body": "Please recheck the installation guide at https://github.com/h2oai/h2ogpt/blob/main/docs/README_InferenceServers.md \r\n\r\nIn the local install I think we missed out: \r\n\r\npip install vllm\r\n\r\nDue to which when one runs the inference server;  error is given that Module vllm not found. "
      },
      {
        "user": "pseudotensor",
        "body": "@rohitnanda1443 The block for installing it is there, but it's a bit old and setup for helping Mixtral work when it wasn't in vLLM yet.  I cleaned it up."
      }
    ]
  },
  {
    "issue_number": 1539,
    "title": "ImportError: libcudnn.so.8: cannot open shared object file: No such file or directory",
    "author": "glenbhermon",
    "state": "open",
    "created_at": "2024-04-09T10:12:31Z",
    "updated_at": "2024-04-10T09:49:09Z",
    "labels": [],
    "body": "My cuda toolkit version is **12.1** & nvcc -V show the following results\r\n`vcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2023 NVIDIA Corporation\r\nBuilt on Tue_Feb__7_19:32:13_PST_2023\r\nCuda compilation tools, release 12.1, V12.1.66\r\nBuild cuda_12.1.r12.1/compiler.32415258_0\r\n`\r\nwhen i ran for checking cudnn version by the following command :\r\n---------------------\r\n`cat /usr/include/x86_64-linux-gnu/cudnn_v*.h | grep CUDNN_MAJOR -A 2`\r\n\r\nthen it show the following things:\r\n------------------------------------------------------------------------------------------------------------------------------------------------\r\n#define CUDNN_MAJOR 9\r\n#define CUDNN_MINOR 0\r\n#define CUDNN_PATCHLEVEL 0\r\n--\r\n#define CUDNN_VERSION (CUDNN_MAJOR * 10000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\r\n\r\n/* cannot use constexpr here since this is a C-only file */\r\n\r\n------------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nIn .bashrc these things are added\r\n\r\n`export PATH=/usr/local/cuda-12.1/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\nexport PATH=$PATH:~/.local/bin`\r\n\r\nAfter following the instruction and running the command from README_LINUX.md\r\n`GPLOK=1 bash docs/linux_install.sh`\r\n\r\nIt install the necessary libraries but when I import the pytorch then it throw the following error:\r\n\r\n`ImportError: libcudnn.so.8: cannot open shared object file: No such file or directory`",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Did you do this step mentioned in the readme_linux.md?\r\n\r\n```\r\necho 'export CUDA_HOME=/usr/local/cuda-12.1' >> $HOME/.bashrc\r\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_HOME/lib64:$CUDA_HOME/extras/CUPTI/lib64' >> $HOME/.bashrc\r\necho 'export PATH=$PATH:$CUDA_HOME/bin' >> $HOME/.bashrc\r\n```\r\n\r\nand restart the shell?"
      },
      {
        "user": "glenbhermon",
        "body": "yes, i first do the steps that are mentioned in readme_linux.md and rebbot but it was not worked\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Can you (with that env) even import torch and check if cuda is avail like in readme?  i.e.\r\n\r\n```python\r\nimport torch\r\nprint(torch.cuda.is_available())\r\n```"
      },
      {
        "user": "glenbhermon",
        "body": "yes, it throws the \"ImportError: libcudnn.so.8: cannot open shared object file: No such file or directory\""
      },
      {
        "user": "pseudotensor",
        "body": "Ok, then your problem is very fundamental and not related to h2oGPT.  You should try to create an environment (virtualenv or conda), pip install torch, and be able to run that check."
      },
      {
        "user": "glenbhermon",
        "body": "I think the problem is related to the pytorch compatibility with cudnn 9.0.0. h2ogpt works perfectly with cuda toolkit 11.8 and cudnn 8.7.x"
      }
    ]
  },
  {
    "issue_number": 841,
    "title": "Multiple collections simultaneously ",
    "author": "slavag",
    "state": "open",
    "created_at": "2023-09-13T08:01:31Z",
    "updated_at": "2024-04-09T16:44:33Z",
    "labels": [
      "type/feature"
    ],
    "body": "Hi,\r\nIs there any option that I can use multiple embeddings DB simultaneously (checkbox instead of radio button :) ) ?\r\nAs an example, I have number of db's , usually I need to use only one of them, but there's a situations that I need to use 2 or 3 together.\r\n\r\nThanks",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Review https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#multiple-embeddings-and-sources and let me know if you  have any questions.  I updated it a bit to clarify some things."
      },
      {
        "user": "slavag",
        "body": "@pseudotensor Thanks, but I'm looking for a way how to combine different embeddings at generation time. In the current UI I have to choose embedding, and I can only choose one,  but in some case I would like to work with more than one. \r\nI know that I can create a db with all that data for each combination, but this will create too many db's."
      },
      {
        "user": "pseudotensor",
        "body": "Got you.      Changed title to reflect feature request."
      },
      {
        "user": "slavag",
        "body": "@pseudotensor Thanks a lot !!"
      },
      {
        "user": "pseudotensor",
        "body": "https://python.langchain.com/docs/use_cases/question_answering/how_to/multi_retrieval_qa_router"
      },
      {
        "user": "pseudotensor",
        "body": "https://stackoverflow.com/questions/76048941/how-to-combine-two-chroma-databases"
      },
      {
        "user": "pseudotensor",
        "body": "```\r\n(/data/conda/h2ogpt) jon@pseudotensor:~/h2ogpt$ cat issue841.txt \r\ndiff --git a/src/gen.py b/src/gen.py\r\nindex 4aa99db8..4b2ac2aa 100644\r\n--- a/src/gen.py\r\n+++ b/src/gen.py\r\n@@ -297,6 +297,7 @@ def main(\r\n         actions_in_sidebar: bool = False,\r\n         document_choice_in_sidebar: bool = False,\r\n         enable_add_models_to_list_ui: bool = False,\r\n+        allow_multiple_collections: bool = False,\r\n         max_raw_chunks: int = None,\r\n         pdf_height: int = 800,\r\n         avatars: bool = True,\r\n@@ -824,6 +825,7 @@ def main(\r\n            Useful if often changing picking specific document(s)\r\n     :param enable_add_models_to_list_ui: Whether to show add model, lora, server to dropdown list\r\n            Disabled by default since clutters Models tab in UI, and can just add custom item directly in dropdown\r\n+    :param allow_multiple_collections: Whether to allow selection of multiple collections for query/summarize/extract\r\n     :param max_raw_chunks: Maximum number of chunks to show in UI when asking for raw DB text from documents/collection\r\n     :param pdf_height: Height of PDF viewer in UI\r\n     :param avatars: Whether to show avatars in chatbot\r\ndiff --git a/src/gradio_runner.py b/src/gradio_runner.py\r\nindex 9cab625f..68522592 100644\r\n--- a/src/gradio_runner.py\r\n+++ b/src/gradio_runner.py\r\n@@ -905,11 +905,25 @@ def go_gradio(**kwargs):\r\n                     show_label=True,\r\n                     visible=kwargs['langchain_mode'] != 'Disabled',\r\n                     min_width=100)\r\n+                langchain_mode_dropdown_kwargs = dict(\r\n+                    choices=langchain_choices0,\r\n+                    value=[kwargs['langchain_mode']],\r\n+                    label=\"Query Collections\",\r\n+                    show_label=True,\r\n+                    visible=kwargs['langchain_mode'] != 'Disabled',\r\n+                    elem_id=\"multi-selection\",\r\n+                    multiselect=True,\r\n+                    allow_custom_value=False,\r\n+                    min_width=100)\r\n+\r\n                 if is_public:\r\n                     langchain_mode = gr.Radio(**langchain_mode_radio_kwargs)\r\n                 with gr.Accordion(resources_acc_label, open=False, visible=database_visible and not is_public):\r\n                     if not is_public:\r\n-                        langchain_mode = gr.Radio(**langchain_mode_radio_kwargs)\r\n+                        if kwargs['allow_multiple_collections']:\r\n+                            langchain_mode = gr.Dropdown(**langchain_mode_dropdown_kwargs)\r\n+                        else:\r\n+                            langchain_mode = gr.Radio(**langchain_mode_radio_kwargs)\r\n                     if kwargs['actions_in_sidebar']:\r\n                         add_chat_history_to_context = gr.Checkbox(label=\"Chat History\",\r\n                                                                   value=kwargs['add_chat_history_to_context'])\r\n@@ -2447,6 +2461,10 @@ def go_gradio(**kwargs):\r\n \r\n         # if change collection source, must clear doc selections from it to avoid inconsistency\r\n         def clear_doc_choice(langchain_mode1):\r\n+            # only query/summary/extract takes full list, rest will presume first if list\r\n+            if isinstance(langchain_mode1, (list, tuple)) and langchain_mode1:\r\n+                langchain_mode1 = langchain_mode1[0]\r\n+\r\n             if langchain_mode1 in langchain_modes_non_db:\r\n                 label1 = 'Choose Resources->Collections and Pick Collection' if not kwargs[\r\n                     'document_choice_in_sidebar'] else \"Document\"\r\n@@ -6140,6 +6158,10 @@ def update_user_db_gr(file, db1s, selection_docs_state1, requests_state1,\r\n                       dbs=None,\r\n                       get_userid_auth=None,\r\n                       **kwargs):\r\n+    # only query/summary/extract takes full list, rest will presume first if list\r\n+    if isinstance(langchain_mode, (list, tuple)) and langchain_mode:\r\n+        langchain_mode = langchain_mode[0]\r\n+\r\n     valid_key = is_valid_key(kwargs.pop('enforce_h2ogpt_api_key', None),\r\n                              kwargs.pop('enforce_h2ogpt_ui_key', None),\r\n                              kwargs.pop('h2ogpt_api_keys', []), h2ogpt_key,\r\n\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "It's a bit complex to do, because while I could make the query/summarize/extract work off of selected collections, getting all the source lists, document selection, etc. will take more work."
      },
      {
        "user": "santosh-gkg",
        "body": "is this feature working now? can we pass --allow_multiple_collections=True and it would work?"
      },
      {
        "user": "pseudotensor",
        "body": "No it was a bit complex to do, I can look back to see if there is a different way."
      },
      {
        "user": "santosh-gkg",
        "body": "yes please, it would be very helpful for scalability"
      }
    ]
  },
  {
    "issue_number": 1409,
    "title": "Mismatch Between Query Response and Source Document Ordering in a Captive Network with Multiple Documents",
    "author": "llmwesee",
    "state": "open",
    "created_at": "2024-02-15T12:07:35Z",
    "updated_at": "2024-04-09T14:52:42Z",
    "labels": [],
    "body": "Hello,\r\n\r\nI've successfully implemented the LLM Model on a custom dataset within a controlled network, leveraging over 1000 PDF documents. The responses are comprehensive and informative. However, I've noticed a significant issue in the way sources are presented.\r\n\r\nWhen a query is raised, the model accurately retrieves responses from relevant documents. However, the sources listed at the end do not seem to align with the provided response. The document mentioned in the response is consistently placed at the very bottom of the list of sources. Ideally, the relevant document should be listed first, enabling users to access the specific source document immediately.\r\n\r\nThis discrepancy undermines the primary goal of using the LLM. The intent is to present query details along with the corresponding source at the top of the list for quick access. Currently, users have to sift through the entire list of sources to find the document related to the response, which defeats the purpose of streamlining information retrieval from a pool of 1000 PDFs.\r\n\r\nI am seeking suggestions on how to address this issue as it poses a significant obstacle in the effective deployment of the solution. Your assistance in resolving this matter is crucial, and I appreciate any insights or recommendations you can provide.\r\n\r\n\r\nThank you\r\n\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "If you can give me an example repro of your case using some set of public documents, where it shows sources are at bottom of list, I can take a look.\r\n\r\nThe sources are scored based upon standard semantic similarity search and the distance metric 'l2'.  The LLM then decides what is most relevant from that.  As you might imagine, that's why LLMs are so useful, is because they *actually* find the relevant info, when semantic similarity cannot fully.  That's why we fill the context with various info the let the LLM decide.\r\n\r\nThere's no easy way to extract which text/sources the LLM used, that's an area of active research.  In some cases a simple ngram check may help, but it also might hurt for non-keyword related cases.  There may be a way to back-prop through the model to see why it decided, but nobody has figured that out for auto-regressive case, only classification.\r\n\r\nIf the user has to verify the answer, best is to present the sources more cleanly like in our Enterprise h2oGPT https://h2o.ai/resources/video/enterprise-h2ogpt/ that shows full page references with highlighted text regions.  But that doesn't really solve your problem when there are numerous documents."
      },
      {
        "user": "rohitnanda1443",
        "body": "how does one get the highlighted text in the source when one clicks in the opensource version? "
      },
      {
        "user": "pseudotensor",
        "body": "There's no highlighted text in OSS version."
      },
      {
        "user": "rohitnanda1443",
        "body": "How does one resolve the \"detail\": \"File not allowed\" error , when one clicks on the sources? "
      },
      {
        "user": "pseudotensor",
        "body": "^ https://github.com/h2oai/h2ogpt/issues/1512"
      },
      {
        "user": "rohitnanda1443",
        "body": "A further question: the LLM is unable to provide the information from the document which is given in a Table in the PDF. Even after asking specifically which table to see it is unable to give the results. \r\n\r\nFor example if one wants to know what all are the company's investments it is unable to list it down (See table 3B in the 2023 pdf). \r\n\r\nGoing forward, aim was to find out which new investments happened during the year which could be done by comparing annual reports of 2 years, the LLM  is unable to to that even after pointing which 2 tables to compare. \r\n\r\nAnnual Reports: https://www.infoedge.in/InvestorRelations/IR_Annual_Report\r\n "
      },
      {
        "user": "pseudotensor",
        "body": "Smarter models do better at such tasks.\r\n\r\nhttps://github.com/h2oai/enterprise-h2ogpte/blob/main/rag_benchmark/results/test_client_e2e.md"
      }
    ]
  },
  {
    "issue_number": 1001,
    "title": "consider pd.read_excel -> tabulate vs. UnstructuredExcelLoader",
    "author": "pseudotensor",
    "state": "open",
    "created_at": "2023-10-25T22:07:44Z",
    "updated_at": "2024-04-09T03:04:28Z",
    "labels": [],
    "body": "`UnstructuredExcelLoader` vs. `pd.read_excel`\r\n\r\n\r\nhttps://www.esma.europa.eu/sites/default/files/library/2016-1452_guidelines_mifid_ii_transaction_reporting.pdf\r\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32014L0065&qid=1689245800594\r\nhttps://www.esma.europa.eu/document/qa-mifir-data-reporting\r\nhttps://www.esma.europa.eu/sites/default/files/library/esma65-8-2594_annex_1_mifir_transaction_reporting_validation_rules.xlsx\r\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32017R0590\r\n\r\nQuestion:\r\n\r\n```\r\nwhat quantity type value should be reported on an equity swap?\r\n```\r\n\r\nbest answer:\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/98af4bfd-7475-4de7-8095-f6b77ab698e6)\r\n\r\n\r\nIssues for excel:\r\n* multilpe tables not in?\r\n* what was shown looks bad compared to pandas",
    "comments": [
      {
        "user": "rohitnanda1443",
        "body": "How does one prepare a Line or Pie chart from the data generated by the LLM after querying?"
      }
    ]
  },
  {
    "issue_number": 1537,
    "title": "Attention sink error with h2oai/h2ogpt-4096-llama2-13b-chat",
    "author": "chengchu88",
    "state": "open",
    "created_at": "2024-04-08T18:13:00Z",
    "updated_at": "2024-04-09T02:07:14Z",
    "labels": [],
    "body": "Hello,\r\n\r\nI tried out attention sink with the example command in the FAQ, and it works fine:\r\npython generate.py --base_model=mistralai/Mistral-7B-Instruct-v0.2 --score_model=None --attention_sinks=True --max_new_tokens=100000 --max_max_new_tokens=100000 --top_k_docs=-1 --use_gpu_id=False --max_seq_len=4096 --sink_dict=\"{'num_sink_tokens': 4, 'window_length': 4096}\"\r\n\r\nHowever, if I change the base model to h2oai/h2ogpt-4096-llama2-13b-chat, and I got the error below:\r\n\r\n File \"/h2ogpt_conda/lib/python3.10/site-packages/gradio/queueing.py\", line 501, in call_prediction\r\n    output = await route_utils.call_process_api(\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/gradio/route_utils.py\", line 252, in call_process_api\r\n    output = await app.get_blocks().process_api(\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1664, in process_api\r\n    result = await self.call_function(\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1217, in call_function\r\n    prediction = await utils.async_iteration(iterator)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/gradio/utils.py\", line 526, in async_iteration\r\n    return await iterator.__anext__()\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/gradio/utils.py\", line 519, in __anext__\r\n    return await anyio.to_thread.run_sync(\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\r\n    return await get_async_backend().run_sync_in_worker_thread(\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\r\n    return await future\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\r\n    result = context.run(func, *args)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/gradio/utils.py\", line 502, in run_sync_iterator_async\r\n    return next(iterator)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/gradio/utils.py\", line 685, in gen_wrapper\r\n    response = next(iterator)\r\n  File \"/workspace/src/gradio_runner.py\", line 4600, in bot\r\n    for res in get_response(fun1, history, chatbot_role1, speaker1, tts_language1, roles_state1,\r\n  File \"/workspace/src/gradio_runner.py\", line 4495, in get_response\r\n    for output_fun in fun1():\r\n  File \"/workspace/src/gen.py\", line 4096, in evaluate\r\n    for r in run_qa_db(\r\n  File \"/workspace/src/gpt_langchain.py\", line 5676, in _run_qa_db\r\n    answer = yield from run_target_func(query=query,\r\n  File \"/workspace/src/gpt_langchain.py\", line 5849, in run_target\r\n    raise thread.exc\r\n  File \"/workspace/src/utils.py\", line 476, in run\r\n    self._return = self._target(*self._args, **self._kwargs)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 316, in __call__\r\n    raise e\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 310, in __call__\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py\", line 136, in _call\r\n    output, extra_return_dict = self.combine_docs(\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py\", line 244, in combine_docs\r\n    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 293, in predict\r\n    return self(kwargs, callbacks=callbacks)[self.output_key]\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 316, in __call__\r\n    raise e\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 310, in __call__\r\n    self._call(inputs, run_manager=run_manager)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 103, in _call\r\n    response = self.generate([inputs], run_manager=run_manager)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 115, in generate\r\n    return self.llm.generate_prompt(\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 521, in generate_prompt\r\n    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 671, in generate\r\n    output = self._generate_helper(\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 558, in _generate_helper\r\n    raise e\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 545, in _generate_helper\r\n    self._generate(\r\n  File \"/workspace/src/gpt_langchain.py\", line 1769, in _generate\r\n    rets = super()._generate(prompts, stop=stop, run_manager=run_manager, **kwargs)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 203, in _generate\r\n    responses = self.pipeline(batch_prompts)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 241, in __call__\r\n    return super().__call__(text_inputs, **kwargs)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1177, in __call__\r\n    outputs = list(final_iterator)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\r\n    item = next(self.iterator)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\r\n    processed = self.infer(item, **self.params)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1102, in forward\r\n    model_outputs = self._forward(model_inputs, **forward_params)\r\n  File \"/workspace/src/h2oai_pipeline.py\", line 260, in _forward\r\n    return self.__forward(model_inputs, **generate_kwargs)\r\n  File \"/workspace/src/h2oai_pipeline.py\", line 296, in __forward\r\n    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1544, in generate\r\n    return self.greedy_search(\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2404, in greedy_search\r\n    outputs = self(\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1177, in forward\r\n    outputs = self.model(\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\r\n    output = module._old_forward(*args, **kwargs)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 983, in forward\r\n    past_key_values = DynamicCache.from_legacy_cache(past_key_values)\r\n  File \"/h2ogpt_conda/lib/python3.10/site-packages/transformers/cache_utils.py\", line 166, in from_legacy_cache\r\n    for layer_idx in range(len(past_key_values)):\r\n\r\nCould you help with this issue? thank you very much!\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Looks like a bug in transformers.  They don't support the older models as well for such new features.  Mistral based models are better anyways."
      }
    ]
  },
  {
    "issue_number": 1477,
    "title": "option to delay startup and have inference servers keep trying until up instead of skipping them.",
    "author": "pseudotensor",
    "state": "closed",
    "created_at": "2024-03-14T15:57:26Z",
    "updated_at": "2024-04-08T21:27:57Z",
    "labels": [
      "helium"
    ],
    "body": null,
    "comments": [
      {
        "user": "pseudotensor",
        "body": "Already doesn't skip for vLLM.  Not important for gradio server since only then would be gradio->gradio and not required to support in such detail."
      }
    ]
  },
  {
    "issue_number": 1535,
    "title": "about Add Doc to Chat",
    "author": "zjudongze",
    "state": "closed",
    "created_at": "2024-04-08T08:01:48Z",
    "updated_at": "2024-04-08T09:12:35Z",
    "labels": [],
    "body": "I don't know whats the point of option \"Add Doc to Chat\". \r\n\r\nI tried to query with it on or off. I didn't see any difference.\r\n\r\nand in the source code, it seems it does nothing except the following line\r\n\"do_show = gradio_upload_to_chatbot1 or gradio_errors_to_chatbot1\"\r\n\r\nand the default setting will set gradio_errors_to_chatbot1 to true, so gradio_upload_to_chatbot1  has no effect.\r\n\r\ncan this option be used to feed full content of a selected document into llm?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It works, it's just a bit confusing as was since CLI option overwhelmed UI option. I fixed that.\r\n\r\nWhat it looks like when upload image or audio:\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/906138e9-cb78-4ccb-8807-9c928df1ff62)\r\n"
      }
    ]
  },
  {
    "issue_number": 1531,
    "title": "Mac OS automatic installation runs errors",
    "author": "antoninadert",
    "state": "closed",
    "created_at": "2024-04-04T09:02:47Z",
    "updated_at": "2024-04-08T08:42:27Z",
    "labels": [],
    "body": "I tried to install but ran certificate errors and plenty other errors :\r\n\r\n```\r\nh2ogpt-osx-m1-gpu ; exit;\r\n__file__: /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/mac_run_app.py\r\nPYTHONPATH:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF\r\nPath_1:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF\r\nNLTK_DATA:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/nltk_data\r\nPATH:  /miniconda3/bin:/miniconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/poppler/bin/:/var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/poppler/lib/:/var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/Tesseract-OCR\r\nPath_3:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/h2ogpt/src\r\nPath_3:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/h2ogpt/iterators\r\nPath_3:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/h2ogpt/gradio_utils\r\nPath_3:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/h2ogpt/metrics\r\nPath_3:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/h2ogpt/models\r\nPath_3:  /var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/h2ogpt/.\r\n\r\n-----\r\n\r\nWeasyPrint could not import some external libraries. Please carefully follow the installation steps before reporting an issue:\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#installation\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#troubleshooting \r\n\r\n-----\r\n\r\n\r\n-----\r\n\r\nWeasyPrint could not import some external libraries. Please carefully follow the installation steps before reporting an issue:\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#installation\r\nhttps://doc.courtbouillon.org/weasyprint/stable/first_steps.html#troubleshooting \r\n\r\n-----\r\n\r\nfatal: not a git repository (or any of the parent directories): .git\r\nMust install DocTR and LangChain installed if enabled DocTR, disabling\r\nTraceback (most recent call last):\r\n  File \"urllib3/connectionpool.py\", line 468, in _make_request\r\n  File \"urllib3/connectionpool.py\", line 1097, in _validate_conn\r\n  File \"urllib3/connection.py\", line 642, in connect\r\n  File \"urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\r\n  File \"urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\r\n  File \"urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\r\n  File \"ssl.py\", line 513, in wrap_socket\r\n  File \"ssl.py\", line 1104, in _create\r\n  File \"ssl.py\", line 1375, in do_handshake\r\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"urllib3/connectionpool.py\", line 791, in urlopen\r\n  File \"urllib3/connectionpool.py\", line 492, in _make_request\r\nurllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"requests/adapters.py\", line 486, in send\r\n  File \"urllib3/connectionpool.py\", line 845, in urlopen\r\n  File \"urllib3/util/retry.py\", line 515, in increment\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-base.en/resolve/main/config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)')))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"mac_run_app.py\", line 47, in <module>\r\n  File \"mac_run_app.py\", line 34, in main\r\n  File \"/var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/h2ogpt/./generate.py\", line 16, in entrypoint_main\r\n    H2O_Fire(main)\r\n  File \"/var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/h2ogpt/./src/utils.py\", line 69, in H2O_Fire\r\n    fire.Fire(component=component, command=args)\r\n  File \"fire/core.py\", line 141, in Fire\r\n  File \"fire/core.py\", line 475, in _Fire\r\n  File \"fire/core.py\", line 691, in _CallAndUpdateTrace\r\n  File \"/var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/h2ogpt/./src/gen.py\", line 1837, in main\r\n    transcriber = get_transcriber(model=stt_model,\r\n  File \"/var/folders/lw/ygk6p9h16x1_2h3ch6f7_rmw0000gp/T/_MEIcH52BF/h2ogpt/./src/stt.py\", line 17, in get_transcriber\r\n    transcriber = pipeline(\"automatic-speech-recognition\", model=model, device_map=device_map)\r\n  File \"transformers/pipelines/__init__.py\", line 779, in pipeline\r\n    resolved_config_file = cached_file(\r\n  File \"transformers/utils/hub.py\", line 398, in cached_file\r\n    resolved_file = hf_hub_download(\r\n  File \"huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n  File \"huggingface_hub/file_download.py\", line 1247, in hf_hub_download\r\n  File \"huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n  File \"huggingface_hub/file_download.py\", line 1624, in get_hf_file_metadata\r\n  File \"huggingface_hub/file_download.py\", line 402, in _request_wrapper\r\n  File \"huggingface_hub/file_download.py\", line 425, in _request_wrapper\r\n  File \"requests/sessions.py\", line 589, in request\r\n  File \"requests/sessions.py\", line 703, in send\r\n  File \"huggingface_hub/utils/_http.py\", line 63, in send\r\n  File \"requests/adapters.py\", line 517, in send\r\nrequests.exceptions.SSLError: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/whisper-base.en/resolve/main/config.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)')))\"), '(Request ID: 36f49778-9375-4409-9eed-0b659d5d5aef)')\r\n[27174] Failed to execute script 'mac_run_app' due to unhandled exception!\r\n\r\n```\r\n\r\nOn a Mac OS with M2 pro, running on Sonoma 14.4.1",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "@Mathanraj-Sharma Please take a look at the mac specific issues.\r\n\r\nI think on MAC on has to setup SSL properly, I've seen people discuss before?"
      },
      {
        "user": "antoninadert",
        "body": "It is due to SSL interception of netskope agent as per my enterprise's security policy. Solution should not be on H2ogpt side, so we can close this issue I guess. "
      }
    ]
  },
  {
    "issue_number": 1528,
    "title": "Does this tool guarantee that all data is private?",
    "author": "jaysunl",
    "state": "closed",
    "created_at": "2024-04-03T23:32:40Z",
    "updated_at": "2024-04-07T00:27:34Z",
    "labels": [
      "type/question"
    ],
    "body": "I saw in the README that this tool completely protects all data, is that right? Meaning if I choose to upload documents for the chatbot to ingest, that information will not be collected/retained by any third party companies?",
    "comments": [
      {
        "user": "santosh-gkg",
        "body": "if you are doing on your system then obviously not, can't say about website but i hope no"
      },
      {
        "user": "pseudotensor",
        "body": "I do best to make sure h2oGPT avoids all telemetry (various packages like chroma, HF, gradio, etc. etc. all enable by default, but I disable them by default).  I also change code for other packages that actually don't listen to their options, like posthog related stuff. \r\n\r\nHowever, we have our own that only tracks IP address during login, but that can be easily disabled like:\r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/a0ba89a99ed774f37aa3de595a57f02f12c1f440/docs/README_offline.md#L186-L192"
      }
    ]
  },
  {
    "issue_number": 1356,
    "title": "NVIDIA guardrails: Remove personal identification information PII from the text",
    "author": "slavag",
    "state": "open",
    "created_at": "2024-02-04T10:12:18Z",
    "updated_at": "2024-04-04T17:57:37Z",
    "labels": [
      "type/feature"
    ],
    "body": "Hi,\r\nI'm trying to ask the LLM to remove PII from the given text, but seems that somehow is not working, when asking same question from chtgpt, it works fine. Maybe you can add such an option to the chat ?\r\n\r\nThanks",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "It's a good idea, but not a top priority for this open-source project.  We have an enterprise h2oGPT that has some of these features.\r\n\r\nBut if you give me an anonimized example, I might be able to help craft a better system prompt.  Also the model smartness matters alot here.\r\n\r\nIn general adding NVIDIA guardrails langchain package into h2oGPT may help.  Basically some rules and runs through the LLM twice with LLM-as-Judge type prompting.  Someone could open a PR and add that.\r\n\r\nhttps://github.com/NVIDIA/NeMo-Guardrails/blob/develop/docs/user_guides/langchain/langchain-integration.md\r\n\r\n\r\n"
      },
      {
        "user": "slavag",
        "body": "@pseudotensor Thanks, please have a look on this conversation between customer support and customer.\r\n```\r\nSubject : Re: Receipt for your payment to [Company] \r\nUser : I hope you are well. I want to ask if you have any programs for small businesses like us who have been severely affected by COVID19. Because we are in the sports industry selling team uniforms, our sales have completely dried up since March and our clients cancelled orders. We have pivoted and are now trying to donate face masks from whatever materials we had left and hopefully sell ones from other sources but we are not yet at the point we are able to continue with operational expenses for pretty much not being operational or profitable for our main business. We have frozen payroll until we can get our PPP hopefully soon and whatever small margins we hope to get with masks will be used for making more masks. In light of this; I wanted to ask if you have any programs for payment deferment or grants for your service or our subscription? We have been grateful that companies like Salesforce have provided grants through free subscriptions during this time and other small software companies are offering support. I thought I would reach out to see if you can offer the same, if we have already paid for the year, can you extend the life of the subscription restarting to when we sell Sports again? If so we would be really grateful. On Wed, Apr 22, 2020 at 2:16 PM <support@[Company].com> wrote: Hi [Customer1 email], Account ID: [Customer1 email] Attached please find payment receipt for [Company]'s backup service (account [Customer1 email]). This charge has been automatically deducted from your credit card per you payment instructions. If you have any questions about your account or any other matter, please feel free to contact us at support@[Company].com. Thank you, [Company] support team For any questions about your account email: support@[Company].com -- Sincerely, [Customer1 name] Marinshaw Team Mom/Marketing/Everything Else [Customer company name] Direct/Text: [Customer company phone1] Phone: [Customer company phone2] ext 101 Connect with Me on LinkedIn ([Customer linked profile1])\"\r\nSupport 1 : Hi [Name1], [Customer1 email] We got the below email regarding COVID19 and issues with payment. What we can offer here? They have been with us since 2013. Thanks \r\nSupport 2 : Please explain that they are on monthly subscription and pay every month We can offer them : - 25% discount for the next 3 months Or 15 months for yearly subscription (3 months free) payable +90 days. Thanks \r\nSupport 1 : Hi [Name1], Thank you. Could you please clarify this \"Or 15 months for yearly subscription (3 months free) payable +90 days. ? Should we offer to move to annual and charge for 12 months and 3 months for free? Please advise.\r\nSupport 1 : Hello [Customer1 name], Thank you for contacting us. We hope that you, your family and friends are keeping well. We're checking the issue with our management team and will update you once we hear from them. Please let us know if you have any questions. [Name2] Customer Support Specialist\r\nSupport 2 : Hi there not sure why we have 21 account for google we only have 6 active employees and max of 10 google licenses 4 of which are not active \r\nSupport 1 : Hi [Customer1 name], I've attached a report (under my signature) with all the users in the account. As you can see, there are 9 active accounts and 11 archived accounts. We charge for paused/archived users as we still keep their backed up data archived in our system. If you don't need to back them up, you can delete them from the backup. We advise downlo[Name1]ng the archived data prior to the deletion, as all the related data is removed permanently. Please let me know if you have any further queries. Best Regards, Ariela Customer Support Specialist \r\nSupport 2 : Correct. First they need to clean users if they want, as they have a lot of data. We can offer 40% discount for monthly \"Should we offer to move to annual and charge for 12 months and 3 months for free? \r\nSupport 1 : Hi [Customer1 name], I'm glad to inform you that we recived an approval from my manager to offer you special discounts. First, please deleted the unnecessary users from the account (as I mentioned in the previous mail). The options that we can offer you are: 1. Applying a 40% discount to your account for the next 3 months. 2. Switching to the annual subscription, but for 15 months. It means that you'll receive 2 months free (as we provide on the annual subscription) + additional 3 months for free, and it will be payable +90days. I hope it will help in these difficult times. Please let us know how you'd like to proceed. Best Regards, Ariela Customer Support Specialist\r\nUser : Yes I have been trying to delete them for a year but I cant get a resolution from about getting the back up. I had already said I cant download because of the size and asked for options for you to download it to one drive or a hard disk. Please kindly consider not charging us as we have no income due to the shutdown jmourad , casey and all of these should have been gone a long time ago if I can only get a back up. [Customer company] [Customer1 email] pmoore@[Customer company] G Suite Archived 6.1181E+10 455 days ago [Customer company] [Customer1 email] aburrichter@[Customer company] G Suite Archived 2.136E+10 229 days ago [Customer company] [Customer1 email] stephen@[Customer company] G Suite Archived 9.5423E+10 41 days ago [Customer company] [Customer1 email] ali@[Customer company] G Suite Archived 3497734398 436 days ago [Customer company] [Customer1 email] anne@[Customer company] G Suite Archived 1714282483 638 days ago [Customer company] [Customer1 email] genevive@[Customer company] G Suite Archived 1749538415 630 days ago [Customer company] [Customer1 email] lorren@[Customer company] G Suite Archived 4261660287 153 days ago [Customer company] [Customer1 email] luvie@[Customer company] G Suite Archived 27028444 50 days ago [Customer company] [Customer1 email] kheem@[Customer company] G Suite Archived 22278205 64 days ago [Customer company] [Customer1 email] lexy@[Customer company] G Suite Archived 51906199 49 days ago [Customer company] [Customer1 email] cris@[Customer company] G Suite Archived 56980272 34 days ago Production [Customer1 email] [Customer1 email] [Customer1 email] Salesforce Executing\\\" User : please do the 40% discount and help me get back up so i dont get charged for 19 accounts, we only have 6 users. If you can consider refunding what I have been charged since I have been asking to get a back up so I can delete, that would go a long way in these times. \r\nSupport 1 : Hi [Name1], 1) [Customer1 name] is asking for a 40% discount and a possible refund for paused: \"please do the 40% discount and help me get back up so i dont get charged for 19 accounts, we only have 6 users. If you can consider refunding what I have been charged since I have been asking to get a back up so I can delete, that would go a long way in these times.\" 2) At the same time, she cannot delete the paused users, because she cannot download their data. We already tried in the past, but she failed to because of the poor connection. Seems she still need their data. What other options do we have? A hard drive possible as a one-time solution? Should we discuss it with dev? \\\"I had already said I cant download because of the size and asked for options for you to download it to one drive or a hard disk. Please kindly consider not charging us as we have no income due to the shutdown jmourad , casey and all of these should have been gone a long time ago if I can only get a back up. \r\nSupport 2 : I have added 40% to all. Please add follow up task for 3 months. I'm afraid we cannot refund as for those users as we still keep about 1TB on our side I have checked with DEV before, hard drive isn't an option. \r\nSupport 1 : Dear [Customer1 name], Thank you for your email. Our management agreed to apply a 40% discount for your entire [Company] account. Unfortunately, we cannot refund for those users as there are about 1Tb of data. Hope for your understanding. Please feel free to contact us if you have any questions/issues. Stay safe! [Name 3] Customer Support Specialist\r\nUser : How do I get the data so I can delete them, considering that I can’t download with my connection as much as I try? I have 12 mb\r\nSupport 1 : Hi [Name1], Please see update from the user: \"How do I get the data so I can delete them, considering that I can’t download with my connection as much as I try? I have 12 mb \" Please advise our next steps. \r\nSupport 1 : Dear [Customer1 name], Thank you for the update. I'm checking the issue with our management team for possible solution and will update you asap. [Name2] Customer Support Specialist \r\nSupport 2 : First please explain that she have about 1TB of data and not 12MB. We should continue and explain that we are SAAS company and we allow our customers to directly download the data from our app. export data from S3 and shipping it will cost few thousand dollars. \r\nSupport 1 : Hello [Customer1 name], Please note that you have 1TB of data in your G Suite task and not 12MB. Since we are a SaaS company, we always allow our customers to download data directly from our app. Here you may check how to do it for G Suite: https://support.[Company].com/hc/en-us/articles/360017905833-Searching-Restoring-and-Downlo[Name1]ng-G-Suite-Backup-Data While exporting the data from S3 and shipping it will cost a few thousand dollars. Alice Customer Support Specialist\r\nSupport 1 : Hi [Name1], [Customer1 email] We provided 40% discount to all tasks due to COVID19 and issues with payment. Should we contact them now? Thanks, [Name2] \r\nSupport 1 : Hi [Name1], [Customer1 email] Could you please advise if we should contact them? Thank you, [Name2] \r\nSupport 2 : Let's keep it as is till end of September Thanks \r\nSupport 1 : Hi [Name1], [Customer1 email] Could you please advise if we should contact them? Thank you, [Name2] \r\nSupport 2 : Thanks [Name2], Let's leave it for now. Next follow up for the end of December. Thanks \r\nSupport 1 : Hi [Name1], [Customer1 email] We provided 40% discount to all tasks due to COVID19 and issues with payment. Should we contact them now? Thanks, [Name2] \r\nSupport 2 : Thanks, Please follow up with her and inform her we will need to update discount to 20% No need to follow up. Please add a reminder for 2 weeks from now. If no response just update the discount to 20% Thanks \r\nSupport 1 : Hello [Customer1 name], Hope you're doing great. We just wanted to follow-up with you and let you know that the temporary 40% discount that was provided earlier by our management will be decreased to 20%. Please feel free to contact us if you have any questions/issues. Stay safe! [Name2] Customer Support Specialist\r\nSupport 1 : Hi [Name1], The discount was updated to 20%. Thanks, [Name2]\r\n```"
      },
      {
        "user": "pseudotensor",
        "body": "@slavag In the above example, you have already anonymized or not yet?  What are you starting with and what is your goal?\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/a3b5cc1e-fbbd-4141-95ef-2e2a37e7c41e)\r\n"
      },
      {
        "user": "slavag",
        "body": "Hi @pseudotensor  this is anonymized example (seems to be partial :)) , this is what I'm trying to get from the text that contains company names, emails , end etc. I can send you not-anonymized example from what I did the above example, but only to private email, I don't want to expose that data. Thanks"
      },
      {
        "user": "pseudotensor",
        "body": "Sure, so you have a fully non-anonymous version and you want it to filter it.\r\n\r\nAs you might imagine, it just takes a good model to do that.  There's no real tricks.\r\n\r\nHere's an de-anonymous version from OpenAI:\r\n\r\n```\r\nSubject : Re: Receipt for your payment to MegaCloudBackup \r\nUser : I hope you are well. I want to ask if you have any programs for small businesses like us who have been severely affected by COVID19. Because we are in the sports industry selling team uniforms, our sales have completely dried up since March and our clients cancelled orders. We have pivoted and are now trying to donate face masks from whatever materials we had left and hopefully sell ones from other sources but we are not yet at the point we are able to continue with operational expenses for pretty much not being operational or profitable for our main business. We have frozen payroll until we can get our PPP hopefully soon and whatever small margins we hope to get with masks will be used for making more masks. In light of this; I wanted to ask if you have any programs for payment deferment or grants for your service or our subscription? We have been grateful that companies like Salesforce have provided grants through free subscriptions during this time and other small software companies are offering support. I thought I would reach out to see if you can offer the same, if we have already paid for the year, can you extend the life of the subscription restarting to when we sell Sports again? If so we would be really grateful. On Wed, Apr 22, 2020 at 2:16 PM <support@MegaCloudBackup.com> wrote: Hi jason.smith@example.com, Account ID: jason.smith@example.com Attached please find payment receipt for MegaCloudBackup's backup service (account jason.smith@example.com). This charge has been automatically deducted from your credit card per you payment instructions. If you have any questions about your account or any other matter, please feel free to contact us at support@MegaCloudBackup.com. Thank you, MegaCloudBackup support team For any questions about your account email: support@MegaCloudBackup.com -- Sincerely, Jason Smith Marinshaw Team Mom/Marketing/Everything Else Marinshaw Sports Direct/Text: 555-0102 Phone: 555-0103 ext 101 Connect with Me on LinkedIn (linkedin.com/in/jason-smith)\r\nSupport 1 : Hi Alex Johnson, jason.smith@example.com We got the below email regarding COVID19 and issues with payment. What we can offer here? They have been with us since 2013. Thanks \r\nSupport 2 : Please explain that they are on monthly subscription and pay every month We can offer them : - 25% discount for the next 3 months Or 15 months for yearly subscription (3 months free) payable +90 days. Thanks \r\nSupport 1 : Hi Alex Johnson, Thank you. Could you please clarify this \"Or 15 months for yearly subscription (3 months free) payable +90 days. ? Should we offer to move to annual and charge for 12 months and 3 months for free? Please advise.\r\nSupport 1 : Hello Jason Smith, Thank you for contacting us. We hope that you, your family and friends are keeping well. We're checking the issue with our management team and will update you once we hear from them. Please let us know if you have any questions. Sarah Miller Customer Support Specialist\r\nSupport 2 : Hi there not sure why we have 21 account for google we only have 6 active employees and max of 10 google licenses 4 of which are not active \r\nSupport 1 : Hi Jason Smith, I've attached a report (under my signature) with all the users in the account. As you can see, there are 9 active accounts and 11 archived accounts. We charge for paused/archived users as we still keep their backed up data archived in our system. If you don't need to back them up, you can delete them from the backup. We advise downloAlex Johnsonng the archived data prior to the deletion, as all the related data is removed permanently. Please let me know if you have any further queries. Best Regards, Ariela Customer Support Specialist \r\nSupport 2 : Correct. First they need to clean users if they want, as they have a lot of data. We can offer 40% discount for monthly \"Should we offer to move to annual and charge for 12 months and 3 months for free? \r\nSupport 1 : Hi Jason Smith, I'm glad to inform you that we recived an approval from my manager to offer you special discounts. First, please deleted the unnecessary users from the account (as I mentioned in the previous mail). The options that we can offer you are: 1. Applying a 40% discount to your account for the next 3 months. 2. Switching to the annual subscription, but for 15 months. It means that you'll receive 2 months free (as we provide on the annual subscription) + additional 3 months for free, and it will be payable +90days. I hope it will help in these difficult times. Please let us know how you'd like to proceed. Best Regards, Ariela Customer Support Specialist\r\nUser : Yes I have been trying to delete them for a year but I cant get a resolution from about getting the back up. I had already said I cant download because of the size and asked for options for you to download it to one drive or a hard disk. Please kindly consider not charging us as we have no income due to the shutdown jmourad , casey and all of these should have been gone a long time ago if I can only get a back up. Marinshaw Sports jason.smith@example.com pmoore@MarinshawSports.com G Suite Archived 61181000000 455 days ago Marinshaw Sports jason.smith@example.com aburrichter@MarinshawSports.com G Suite Archived 21360000000 229 days ago Marinshaw Sports jason.smith@example.com stephen@MarinshawSports.com G Suite Archived 95423000000 41 days ago Marinshaw Sports jason.smith@example.com ali@MarinshawSports.com G Suite Archived 3497734398 436 days ago Marinshaw Sports jason.smith@example.com anne@MarinshawSports.com G Suite Archived 1714282483 638 days ago Marinshaw Sports jason.smith@example.com genevive@MarinshawSports.com G Suite Archived 1749538415 630 days ago Marinshaw Sports jason.smith@example.com lorren@MarinshawSports.com G Suite Archived 4261660287 153 days ago Marinshaw Sports jason.smith@example.com luvie@MarinshawSports.com G Suite Archived 27028444 50 days ago Marinshaw Sports jason.smith@example.com kheem@MarinshawSports.com G Suite Archived 22278205 64 days ago Marinshaw Sports jason.smith@example.com lexy@MarinshawSports.com G Suite Archived 51906199 49 days ago Marinshaw Sports jason.smith@example.com cris@MarinshawSports.com G Suite Archived 56980272 34 days ago Production jason.smith@example.com jason.smith@example.com jason.smith@example.com Salesforce Executing\\\" User : please do the 40% discount and help me get back up so i dont get charged for 19 accounts, we only have 6 users. If you can consider refunding what I have been charged since I have been asking to get a back up so I can delete, that would go a long way in these times. \r\nSupport 1 : Hi Alex Johnson, 1) Jason Smith is asking for a 40% discount and a possible refund for paused: \"please do the 40% discount and help me get back up so i dont get charged for 19 accounts, we only have 6 users. If you can consider refunding what I have been charged since I have been asking to get a back up so I can delete, that would go a long way in these times.\" 2) At the same time, she cannot delete the paused users, because she cannot download their data. We already tried in the past, but she failed to because of the poor connection. Seems she still need their data. What other options do we have? A hard drive possible as a one-time solution? Should we discuss it with dev? \\\"I had already said I cant download because of the size and asked for options for you to download it to one drive or a hard disk. Please kindly consider not charging us as we have no income due to the shutdown jmourad , casey and all of these should have been gone a long time ago if I can only get a back up. \r\nSupport 2 : I have added 40% to all. Please add follow up task for 3 months. I'm afraid we cannot refund as for those users as we still keep about 1TB on our side I have checked with DEV before, hard drive isn't an option. \r\nSupport 1 : Dear Jason Smith, Thank you for your email. Our management agreed to apply a 40% discount for your entire MegaCloudBackup account. Unfortunately, we cannot refund for those users as there are about 1Tb of data. Hope for your understanding. Please feel free to contact us if you have any questions/issues. Stay safe! Chris Parker Customer Support Specialist\r\nUser : How do I get the data so I can delete them, considering that I can’t download with my connection as much as I try? I have 12 mb\r\nSupport 1 : Hi Alex Johnson, Please see update from the user: \"How do I get the data so I can delete them, considering that I can’t download with my connection as much as I try? I have 12 mb \" Please advise our next steps. \r\nSupport 1 : Dear Jason Smith, Thank you for the update. I'm checking the issue with our management team for possible solution and will update you asap. Sarah Miller Customer Support Specialist \r\nSupport 2 : First please explain that she have about 1TB of data and not 12MB. We should continue and explain that we are SAAS company and we allow our customers to directly download the data from our app. export data from S3 and shipping it will cost few thousand dollars. \r\nSupport 1 : Hello Jason Smith, Please note that you have 1TB of data in your G Suite task and not 12MB. Since we are a SaaS company, we always allow our customers to download data directly from our app. Here you may check how to do it for G Suite: https://support.MegaCloudBackup.com/hc/en-us/articles/360017905833-Searching-Restoring-and-DownloAlex Johnsonng-G-Suite-Backup-Data While exporting the data from S3 and shipping it will cost a few thousand dollars. Alice Customer Support Specialist\r\nSupport 1 : Hi Alex Johnson, jason.smith@example.com We provided 40% discount to all tasks due to COVID19 and issues with payment. Should we contact them now? Thanks, Sarah Miller \r\nSupport 1 : Hi Alex Johnson, jason.smith@example.com Could you please advise if we should contact them? Thank you, Sarah Miller \r\nSupport 2 : Let's keep it as is till end of September Thanks \r\nSupport 1 : Hi Alex Johnson, jason.smith@example.com Could you please advise if we should contact them? Thank you, Sarah Miller \r\nSupport 2 : Thanks Sarah Miller, Let's leave it for now. Next follow up for the end of December. Thanks \r\nSupport 1 : Hi Alex Johnson, jason.smith@example.com We provided 40% discount to all tasks due to COVID19 and issues with payment. Should we contact them now? Thanks, Sarah Miller \r\nSupport 2 : Thanks, Please follow up with her and inform her we will need to update discount to 20% No need to follow up. Please add a reminder for 2 weeks from now. If no response just update the discount to 20% Thanks \r\nSupport 1 : Hello Jason Smith, Hope you're doing great. We just wanted to follow-up with you and let you know that the temporary 40% discount that was provided earlier by our management will be decreased to 20%. Please feel free to contact us if you have any questions/issues. Stay safe! Sarah Miller Customer Support Specialist\r\nSupport 1 : Hi Alex Johnson, The discount was updated to 20%. Thanks, Sarah Miller\r\n```\r\n\r\nSo one can ask the same question of:\r\n\r\n```\r\nFrom the provided text, list the PII that appears to be not yet anonymized.\r\n```\r\n\r\nThen one can use GPT-4 to double check its work.  Feel free to use gpt.h2o.ai to try out some models there.  Or try your models."
      },
      {
        "user": "slavag",
        "body": "@pseudotensor Thanks, this prompt shows me what's needed to be anonymized, is there prompt that can do the exact anonymisation in gpt.h2o.ai ? as same prompt that I did in openAI, not working in llama2, mistral, zephyr, neural chat. Maybe the prompt need to be different ?\r\nThanks "
      },
      {
        "user": "pseudotensor",
        "body": "My query to GPT-4 was:\r\n```\r\nList any possible PII in this block of text.  Only list those PII that appear to be not yet anonymized.\r\n\"\"\"\r\n<block of text>\r\n\"\"\"\r\n```\r\n\r\nGoal is to have it say there is no PII  left.\r\n\r\nGPT-4 can do it, but maybe not other models.  Did you try Mixtral or chatgpt on gpt.h2o.ai?"
      },
      {
        "user": "slavag",
        "body": "@pseudotensor not tried mixtral or chatgpt on gpt.h2o.ai yet. If you would write prompt to anonymise text for open source models, how would it looks like ? I created this one, but just getting my text rephrased and keeping PII.\r\n```\r\nRemove any personal identification information, such as names, addresses, phone numbers, and email addresses, from the following text. \r\n```\r\nor that \r\n```\r\nRemove any personally identifiable information (PII) from the text, including but not limited to:\r\n\r\nFull names\r\nUsernames\r\nEmail addresses\r\nPhone numbers\r\nAddresses\r\nSocial security numbers\r\nCredit card numbers\r\nBank account numbers\r\nDates of birth\r\nMedical records or personal health information\r\nDriver's license numbers\r\nLicense plate numbers\r\nIP addresses\r\nGeolocation data\r\nPasswords\r\n\r\nReplace names with [NAME] and locations with [LOCATION]. Remove other personal information completely without leaving behind any placeholders. Leave all non-personal text intact.\r\n```\r\n\r\nThanks"
      },
      {
        "user": "pseudotensor",
        "body": "Looks like it's a tough challenge.\r\n\r\n* llama 13 stops too early\r\n* llama 70 completes output but doesn't do good redaction\r\n* Yi/Capybara fail to reproduce all text, it stops early\r\n* openchat does pretty good job on text, completes, but bad on emails -- some reason it shows redacted [] then original email still.\r\n* zephyr stops too early for whatever reason\r\n* Mixtral does pretty good job, but a bit incomplete in places, like very end still has Miller even if first name is redacted.\r\n* mistral medium does worse than Mixtral, full name appears at end.\r\n* gpt-3.5-turbo does as good as mistral medium, i.e. not great\r\n* gpt-4 does best, very well.  But still not perfect.\r\n\r\nHere's json dump from h2oGPT for those results.  Unfortunately no model name to match, but I shared image that shows order.\r\n\r\n[chats_dae7138c-9526-4544-99c6-08d5d7512811.json](https://github.com/h2oai/h2ogpt/files/14228701/chats_dae7138c-9526-4544-99c6-08d5d7512811.json)\r\n\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/d5952b3d-a57f-4b24-9f8b-39349d9bf821)\r\n\r\n\r\nchat.openai.com version:\r\n\r\nfrom fake de-anonymized version:\r\n```\r\nSubject : Re: Receipt for your payment to MegaCloudBackup \r\nUser : I hope you are well. I want to ask if you have any programs for small businesses like us who have been severely affected by COVID19. Because we are in the sports industry selling team uniforms, our sales have completely dried up since March and our clients cancelled orders. We have pivoted and are now trying to donate face masks from whatever materials we had left and hopefully sell ones from other sources but we are not yet at the point we are able to continue with operational expenses for pretty much not being operational or profitable for our main business. We have frozen payroll until we can get our PPP hopefully soon and whatever small margins we hope to get with masks will be used for making more masks. In light of this; I wanted to ask if you have any programs for payment deferment or grants for your service or our subscription? We have been grateful that companies like Salesforce have provided grants through free subscriptions during this time and other small software companies are offering support. I thought I would reach out to see if you can offer the same, if we have already paid for the year, can you extend the life of the subscription restarting to when we sell Sports again? If so we would be really grateful. On Wed, Apr 22, 2020 at 2:16 PM <support@MegaCloudBackup.com> wrote: Hi jason.smith@example.com, Account ID: jason.smith@example.com Attached please find payment receipt for MegaCloudBackup's backup service (account jason.smith@example.com). This charge has been automatically deducted from your credit card per you payment instructions. If you have any questions about your account or any other matter, please feel free to contact us at support@MegaCloudBackup.com. Thank you, MegaCloudBackup support team For any questions about your account email: support@MegaCloudBackup.com -- Sincerely, Jason Smith Marinshaw Team Mom/Marketing/Everything Else Marinshaw Sports Direct/Text: 555-0102 Phone: 555-0103 ext 101 Connect with Me on LinkedIn (linkedin.com/in/jason-smith)\r\nSupport 1 : Hi Alex Johnson, jason.smith@example.com We got the below email regarding COVID19 and issues with payment. What we can offer here? They have been with us since 2013. Thanks \r\nSupport 2 : Please explain that they are on monthly subscription and pay every month We can offer them : - 25% discount for the next 3 months Or 15 months for yearly subscription (3 months free) payable +90 days. Thanks \r\nSupport 1 : Hi Alex Johnson, Thank you. Could you please clarify this \"Or 15 months for yearly subscription (3 months free) payable +90 days. ? Should we offer to move to annual and charge for 12 months and 3 months for free? Please advise.\r\nSupport 1 : Hello Jason Smith, Thank you for contacting us. We hope that you, your family and friends are keeping well. We're checking the issue with our management team and will update you once we hear from them. Please let us know if you have any questions. Sarah Miller Customer Support Specialist\r\nSupport 2 : Hi there not sure why we have 21 account for google we only have 6 active employees and max of 10 google licenses 4 of which are not active \r\nSupport 1 : Hi Jason Smith, I've attached a report (under my signature) with all the users in the account. As you can see, there are 9 active accounts and 11 archived accounts. We charge for paused/archived users as we still keep their backed up data archived in our system. If you don't need to back them up, you can delete them from the backup. We advise downloAlex Johnsonng the archived data prior to the deletion, as all the related data is removed permanently. Please let me know if you have any further queries. Best Regards, Ariela Customer Support Specialist \r\nSupport 2 : Correct. First they need to clean users if they want, as they have a lot of data. We can offer 40% discount for monthly \"Should we offer to move to annual and charge for 12 months and 3 months for free? \r\nSupport 1 : Hi Jason Smith, I'm glad to inform you that we recived an approval from my manager to offer you special discounts. First, please deleted the unnecessary users from the account (as I mentioned in the previous mail). The options that we can offer you are: 1. Applying a 40% discount to your account for the next 3 months. 2. Switching to the annual subscription, but for 15 months. It means that you'll receive 2 months free (as we provide on the annual subscription) + additional 3 months for free, and it will be payable +90days. I hope it will help in these difficult times. Please let us know how you'd like to proceed. Best Regards, Ariela Customer Support Specialist\r\nUser : Yes I have been trying to delete them for a year but I cant get a resolution from about getting the back up. I had already said I cant download because of the size and asked for options for you to download it to one drive or a hard disk. Please kindly consider not charging us as we have no income due to the shutdown jmourad , casey and all of these should have been gone a long time ago if I can only get a back up. Marinshaw Sports jason.smith@example.com pmoore@MarinshawSports.com G Suite Archived 61181000000 455 days ago Marinshaw Sports jason.smith@example.com aburrichter@MarinshawSports.com G Suite Archived 21360000000 229 days ago Marinshaw Sports jason.smith@example.com stephen@MarinshawSports.com G Suite Archived 95423000000 41 days ago Marinshaw Sports jason.smith@example.com ali@MarinshawSports.com G Suite Archived 3497734398 436 days ago Marinshaw Sports jason.smith@example.com anne@MarinshawSports.com G Suite Archived 1714282483 638 days ago Marinshaw Sports jason.smith@example.com genevive@MarinshawSports.com G Suite Archived 1749538415 630 days ago Marinshaw Sports jason.smith@example.com lorren@MarinshawSports.com G Suite Archived 4261660287 153 days ago Marinshaw Sports jason.smith@example.com luvie@MarinshawSports.com G Suite Archived 27028444 50 days ago Marinshaw Sports jason.smith@example.com kheem@MarinshawSports.com G Suite Archived 22278205 64 days ago Marinshaw Sports jason.smith@example.com lexy@MarinshawSports.com G Suite Archived 51906199 49 days ago Marinshaw Sports jason.smith@example.com cris@MarinshawSports.com G Suite Archived 56980272 34 days ago Production jason.smith@example.com jason.smith@example.com jason.smith@example.com Salesforce Executing\\\" User : please do the 40% discount and help me get back up so i dont get charged for 19 accounts, we only have 6 users. If you can consider refunding what I have been charged since I have been asking to get a back up so I can delete, that would go a long way in these times. \r\nSupport 1 : Hi Alex Johnson, 1) Jason Smith is asking for a 40% discount and a possible refund for paused: \"please do the 40% discount and help me get back up so i dont get charged for 19 accounts, we only have 6 users. If you can consider refunding what I have been charged since I have been asking to get a back up so I can delete, that would go a long way in these times.\" 2) At the same time, she cannot delete the paused users, because she cannot download their data. We already tried in the past, but she failed to because of the poor connection. Seems she still need their data. What other options do we have? A hard drive possible as a one-time solution? Should we discuss it with dev? \\\"I had already said I cant download because of the size and asked for options for you to download it to one drive or a hard disk. Please kindly consider not charging us as we have no income due to the shutdown jmourad , casey and all of these should have been gone a long time ago if I can only get a back up. \r\nSupport 2 : I have added 40% to all. Please add follow up task for 3 months. I'm afraid we cannot refund as for those users as we still keep about 1TB on our side I have checked with DEV before, hard drive isn't an option. \r\nSupport 1 : Dear Jason Smith, Thank you for your email. Our management agreed to apply a 40% discount for your entire MegaCloudBackup account. Unfortunately, we cannot refund for those users as there are about 1Tb of data. Hope for your understanding. Please feel free to contact us if you have any questions/issues. Stay safe! Chris Parker Customer Support Specialist\r\nUser : How do I get the data so I can delete them, considering that I can’t download with my connection as much as I try? I have 12 mb\r\nSupport 1 : Hi Alex Johnson, Please see update from the user: \"How do I get the data so I can delete them, considering that I can’t download with my connection as much as I try? I have 12 mb \" Please advise our next steps. \r\nSupport 1 : Dear Jason Smith, Thank you for the update. I'm checking the issue with our management team for possible solution and will update you asap. Sarah Miller Customer Support Specialist \r\nSupport 2 : First please explain that she have about 1TB of data and not 12MB. We should continue and explain that we are SAAS company and we allow our customers to directly download the data from our app. export data from S3 and shipping it will cost few thousand dollars. \r\nSupport 1 : Hello Jason Smith, Please note that you have 1TB of data in your G Suite task and not 12MB. Since we are a SaaS company, we always allow our customers to download data directly from our app. Here you may check how to do it for G Suite: https://support.MegaCloudBackup.com/hc/en-us/articles/360017905833-Searching-Restoring-and-DownloAlex Johnsonng-G-Suite-Backup-Data While exporting the data from S3 and shipping it will cost a few thousand dollars. Alice Customer Support Specialist\r\nSupport 1 : Hi Alex Johnson, jason.smith@example.com We provided 40% discount to all tasks due to COVID19 and issues with payment. Should we contact them now? Thanks, Sarah Miller \r\nSupport 1 : Hi Alex Johnson, jason.smith@example.com Could you please advise if we should contact them? Thank you, Sarah Miller \r\nSupport 2 : Let's keep it as is till end of September Thanks \r\nSupport 1 : Hi Alex Johnson, jason.smith@example.com Could you please advise if we should contact them? Thank you, Sarah Miller \r\nSupport 2 : Thanks Sarah Miller, Let's leave it for now. Next follow up for the end of December. Thanks \r\nSupport 1 : Hi Alex Johnson, jason.smith@example.com We provided 40% discount to all tasks due to COVID19 and issues with payment. Should we contact them now? Thanks, Sarah Miller \r\nSupport 2 : Thanks, Please follow up with her and inform her we will need to update discount to 20% No need to follow up. Please add a reminder for 2 weeks from now. If no response just update the discount to 20% Thanks \r\nSupport 1 : Hello Jason Smith, Hope you're doing great. We just wanted to follow-up with you and let you know that the temporary 40% discount that was provided earlier by our management will be decreased to 20%. Please feel free to contact us if you have any questions/issues. Stay safe! Sarah Miller Customer Support Specialist\r\nSupport 1 : Hi Alex Johnson, The discount was updated to 20%. Thanks, Sarah Miller\r\n```\r\n\r\nto:\r\n```\r\n\"\"\"\r\nSubject : Re: Receipt for your payment to [COMPANY1]\r\nUser : I hope you are well. I want to ask if you have any programs for small businesses like us who have been severely affected by COVID19. Because we are in the sports industry selling team uniforms, our sales have completely dried up since March and our clients cancelled orders. We have pivoted and are now trying to donate face masks from whatever materials we had left and hopefully sell ones from other sources but we are not yet at the point we are able to continue with operational expenses for pretty much not being operational or profitable for our main business. We have frozen payroll until we can get our PPP hopefully soon and whatever small margins we hope to get with masks will be used for making more masks. In light of this; I wanted to ask if you have any programs for payment deferment or grants for your service or our subscription? We have been grateful that companies like [COMPANY2] have provided grants through free subscriptions during this time and other small software companies are offering support. I thought I would reach out to see if you can offer the same, if we have already paid for the year, can you extend the life of the subscription restarting to when we sell Sports again? If so we would be really grateful. On Wed, Apr 22, 2020 at 2:16 PM \\<[EMAIL1]\\> wrote: Hi [EMAIL2], Account ID: [EMAIL3] Attached please find payment receipt for [COMPANY1]'s backup service (account [EMAIL4]). This charge has been automatically deducted from your credit card per you payment instructions. If you have any questions about your account or any other matter, please feel free to contact us at [EMAIL5]. Thank you, [COMPANY1] support team For any questions about your account email: [EMAIL6] -- Sincerely, [NAME1] [COMPANY3] Team Mom/Marketing/Everything Else [COMPANY3] Sports Direct/Text: [PHONE1] Phone: [PHONE2] ext 101 Connect with Me on LinkedIn ([URL1])\r\nSupport 1 : Hi [NAME2], [EMAIL7] We got the below email regarding COVID19 and issues with payment. What we can offer here? They have been with us since 2013. Thanks \r\nSupport 2 : Please explain that they are on monthly subscription and pay every month We can offer them : - 25% discount for the next 3 months Or 15 months for yearly subscription (3 months free) payable +90 days. Thanks \r\nSupport 1 : Hi [NAME2], Thank you. Could you please clarify this \"Or 15 months for yearly subscription (3 months free) payable +90 days. ? Should we offer to move to annual and charge for 12 months and 3 months for free? Please advise.\r\nSupport 1 : Hello [NAME1], Thank you for contacting us. We hope that you, your family and friends are keeping well. We're checking the issue with our management team and will update you once we hear from them. Please let us know if you have any questions. [NAME3] Customer Support Specialist\r\nSupport 2 : Hi there not sure why we have 21 account for google we only have 6 active employees and max of 10 google licenses 4 of which are not active \r\nSupport 1 : Hi [NAME1], I've attached a report (under my signature) with all the users in the account. As you can see, there are 9 active accounts and 11 archived accounts. We charge for paused/archived users as we still keep their backed up data archived in our system. If you don't need to back them up, you can delete them from the backup. We advise downlo[NAME2]ng the archived data prior to the deletion, as all the related data is removed permanently. Please let me know if you have any further queries. Best Regards, [NAME4] Customer Support Specialist \r\nSupport 2 : Correct. First they need to clean users if they want, as they have a lot of data. We can offer 40% discount for monthly \"Should we offer to move to annual and charge for 12 months and 3 months for free? \r\nSupport 1 : Hi [NAME1], I'm glad to inform you that we recived an approval from my manager to offer you special discounts. First, please deleted the unnecessary users from the account (as I mentioned in the previous mail). The options that we can offer you are: 1. Applying a 40% discount to your account for the next 3 months. 2. Switching to the annual subscription, but for 15 months. It means that you'll receive 2 months free (as we provide on the annual subscription) + additional 3 months for free, and it will be payable +90days. I hope it will help in these difficult times. Please let us know how you'd like to proceed. Best Regards\r\n\r\n, [NAME4] Customer Support Specialist\r\nUser : Yes I have been trying to delete them for a year but I cant get a resolution from about getting the back up. I had already said I cant download because of the size and asked for options for you to download it to one drive or a hard disk. Please kindly consider not charging us as we have no income due to the shutdown [NAME5] , [NAME6] and all of these should have been gone a long time ago if I can only get a back up. [COMPANY3] Sports [EMAIL8] [EMAIL9]@[COMPANY3]Sports.com G Suite Archived 61181000000 455 days ago [COMPANY3] Sports [EMAIL10] [EMAIL11]@[COMPANY3]Sports.com G Suite Archived 21360000000 229 days ago [COMPANY3] Sports [EMAIL12] [EMAIL13]@[COMPANY3]Sports.com G Suite Archived 95423000000 41 days ago [COMPANY3] Sports [EMAIL14] [EMAIL15]@[COMPANY3]Sports.com G Suite Archived 3497734398 436 days ago [COMPANY3] Sports [EMAIL16] [EMAIL17]@[COMPANY3]Sports.com G Suite Archived 1714282483 638 days ago [COMPANY3] Sports [EMAIL18] [EMAIL19]@[COMPANY3]Sports.com G Suite Archived 1749538415 630 days ago [COMPANY3] Sports [EMAIL20] [EMAIL21]@[COMPANY3]Sports.com G Suite Archived 4261660287 153 days ago [COMPANY3] Sports [EMAIL22] [EMAIL23]@[COMPANY3]Sports.com G Suite Archived 27028444 50 days ago [COMPANY3] Sports [EMAIL24] [EMAIL25]@[COMPANY3]Sports.com G Suite Archived 22278205 64 days ago [COMPANY3] Sports [EMAIL26] [EMAIL27]@[COMPANY3]Sports.com G Suite Archived 51906199 49 days ago [COMPANY3] Sports [EMAIL28] [EMAIL29]@[COMPANY3]Sports.com G Suite Archived 56980272 34 days ago Production [EMAIL30] [EMAIL31] [EMAIL32] Salesforce Executing\\\" User : please do the 40% discount and help me get back up so i dont get charged for 19 accounts, we only have 6 users. If you can consider refunding what I have been charged since I have been asking to get a back up so I can delete, that would go a long way in these times. \r\nSupport 1 : Hi [NAME2], 1) [NAME1] is asking for a 40% discount and a possible refund for paused: \"please do the 40% discount and help me get back up so i dont get charged for 19 accounts, we only have 6 users. If you can consider refunding what I have been charged since I have been asking to get a back up so I can delete, that would go a long way in these times.\" 2) At the same time, she cannot delete the paused users, because she cannot download their data. We already tried in the past, but she failed to because of the poor connection. Seems she still need their data. What other options do we have? A hard drive possible as a one-time solution? Should we discuss it with dev? \\\"I had already said I cant download because of the size and asked for options for you to download it to one drive or a hard disk. Please kindly consider not charging us as we have no income due to the shutdown [NAME5] , [NAME6] and all of these should have been gone a long time ago if I can only get a back up. \r\nSupport 2 : I have added 40% to all. Please add follow up task for 3 months. I'm afraid we cannot refund as for those users as we still keep about 1TB on our side I have checked with DEV before, hard drive isn't an option. \r\nSupport 1 : Dear [NAME1], Thank you for your email. Our management agreed to apply a 40% discount for your entire [COMPANY1] account. Unfortunately, we cannot refund for those users as there are about 1Tb of data. Hope for your understanding. Please feel free to contact us if you have any questions/issues. Stay safe! [NAME7] Customer Support Specialist\r\nUser : How do I get the data so I can delete them, considering that I can’t download with my connection as much as I try? I have 12 mb\r\nSupport 1 : Hi [NAME2], Please see update from the user: \"How do I get the data so I can delete them, considering that I can’t download with my connection as much as I try? I have 12 mb \" Please\r\n\r\n advise our next steps. \r\nSupport 1 : Dear [NAME1], Thank you for the update. I'm checking the issue with our management team for possible solution and will update you asap. [NAME3] Customer Support Specialist \r\nSupport 2 : First please explain that she have about 1TB of data and not 12MB. We should continue and explain that we are SAAS company and we allow our customers to directly download the data from our app. export data from S3 and shipping it will cost few thousand dollars. \r\nSupport 1 : Hello [NAME1], Please note that you have 1TB of data in your G Suite task and not 12MB. Since we are a SaaS company, we always allow our customers to download data directly from our app. Here you may check how to do it for G Suite: [URL2] While exporting the data from S3 and shipping it will cost a few thousand dollars. [NAME8] Customer Support Specialist\r\nSupport 1 : Hi [NAME2], [EMAIL33] We provided 40% discount to all tasks due to COVID19 and issues with payment. Should we contact them now? Thanks, [NAME3] \r\nSupport 1 : Hi [NAME2], [EMAIL34] Could you please advise if we should contact them? Thank you, [NAME3] \r\nSupport 2 : Let's keep it as is till end of September Thanks \r\nSupport 1 : Hi [NAME2], [EMAIL35] Could you please advise if we should contact them? Thank you, [NAME3] \r\nSupport 2 : Thanks [NAME3], Let's leave it for now. Next follow up for the end of December. Thanks \r\nSupport 1 : Hi [NAME2], [EMAIL36] We provided 40% discount to all tasks due to COVID19 and issues with payment. Should we contact them now? Thanks, [NAME3] \r\nSupport 2 : Thanks, Please follow up with her and inform her we will need to update discount to 20% No need to follow up. Please add a reminder for 2 weeks from now. If no response just update the discount to 20% Thanks \r\nSupport 1 : Hello [NAME1], Hope you're doing great. We just wanted to follow-up with you and let you know that the temporary 40% discount that was provided earlier by our management will be decreased to 20%. Please feel free to contact us if you have any questions/issues. Stay safe! [NAME3] Customer Support Specialist\r\nSupport 1 : Hi [NAME2], The discount was updated to 20%. Thanks, [NAME3]\r\n\"\"\"\r\n```"
      },
      {
        "user": "slavag",
        "body": "@pseudotensor Wow, thanks a lot !!!! "
      },
      {
        "user": "pseudotensor",
        "body": "https://www.guardrailsai.com/docs/examples/check_for_pii"
      }
    ]
  },
  {
    "issue_number": 1491,
    "title": "Resolve Vulnerabilities in Runtime Image",
    "author": "codyharris-h2o-ai",
    "state": "open",
    "created_at": "2024-03-20T18:11:27Z",
    "updated_at": "2024-04-03T17:06:41Z",
    "labels": [
      "area/security"
    ],
    "body": "Hello!\r\nAs part of our ongoing to ensure the security of our products, one or more vulnerabilties requiring redmediation have been identified.\r\n\r\n| Vulnerability | Severity | Image | Package | Description |\r\n| --- | --- | --- | --- | --- |\r\n| CVE-2023-37920 | critical | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | certifi:2022.12.7 | Certifi is a curated collection of Root Certificates for validating the trustworthiness of SSL certificates while verifying the [...] |\r\n| CVE-2023-48022 | critical | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | ray:2.9.3 | Anyscale Ray 2.6.3 and 2.8.0 allows a remote attacker to execute arbitrary code via the job submission API. NOTE: the vendor's p[...] |\r\n\r\nTo resolve this, we recommend the following approach:\r\n1. Install `trivy` (https://aquasecurity.github.io/trivy)\r\n2. Scan the current version of the image using a command like `trivy image --scanners vuln --severity  CRITICAL,HIGH --timeout 60m [...image address...]`\r\n3. Validate that the CVEs are detected using `trivy`.  The provided scans were taken using a different scanner (ECR), so the first step should be to validate that `trivy` can see them as well.\r\n4. Iterate to resolve the vulnerabilities.  `trivy` enables you to scan the image without pushing them, so it should help in finding the resolution\r\n5. Test and publish the fix version, and let us know where we can find the fixed image(s) so we can validate the fixes on our side as well.\r\n\r\nNote that we disregard the severity levels assigned by various tools and operate soley on CVSS in line with NIST guidelines.  Also note that this scan was performed by ECR, so the results will likely be different.  It is in our experience that Trivy produces more results than ECR or Prisma.",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "* The certifi package is 2024.2.2 in image 0.2.0 408.  The older vulnerable version being detected is in a \"pkgs\" folder that is unused and just part of conda base installation before installing other packages.  So the notice is a false positive on the wrong version.\r\n\r\n* There's no resolution for the ray package, no new version is specified, no action can be taken as it's required part of vLLM.  Ray is not exposed directly, only the vLLM port that is not ray directly, so there's no real issue.\r\n\r\n"
      },
      {
        "user": "codyharris-h2o-ai",
        "body": "@pseudotensor Thanks!  For certifi, then can we remove it from the filesystem during the build process?\r\n"
      },
      {
        "user": "codyharris-h2o-ai",
        "body": "There are also a handful of HIGH severities, some of these may or may not be real\r\n\r\n| Vulnerability | Severity | Image | Package | Description |\r\n| --- | --- | --- | --- | --- |\r\n| CVE-2022-3996 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | cryptography:38.0.4 | If an X.509 certificate contains a malformed policy constraint and policy processing is enabled, then a write lock will be taken[...] |\r\n| CVE-2022-40898 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | wheel:0.37.1 | An issue discovered in Python Packaging Authority (PyPA) Wheel 0.37.1 and earlier allows remote attackers to cause a denial of s[...] |\r\n| CVE-2022-4450 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | cryptography:38.0.4 | The function PEM_read_bio_ex() reads a PEM file from a BIO and parses and decodes the \"name\" (e.g. \"CERTIFICATE\"), any header da[...] |\r\n| CVE-2023-0215 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | cryptography:38.0.4 | The public API function BIO_new_NDEF is a helper function used for streaming ASN.1 data via a BIO. It is primarily used internal[...] |\r\n| CVE-2023-0216 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | cryptography:38.0.4 | An invalid pointer dereference on read can be triggered when an application tries to load malformed PKCS7 data with the d2i_PKCS[...] |\r\n| CVE-2023-0217 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | cryptography:38.0.4 | An invalid pointer dereference on read can be triggered when an application tries to check a malformed DSA public key by the EVP[...] |\r\n| CVE-2023-0286 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | cryptography:38.0.4 | There is a type confusion vulnerability relating to X.400 address processing inside an X.509 GeneralName. X.400 addresses were p[...] |\r\n| CVE-2023-0401 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | cryptography:38.0.4 | A NULL pointer can be dereferenced when signatures are being verified on PKCS7 signed or signedAndEnveloped data. In case the ha[...] |\r\n| CVE-2023-38325 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | cryptography:38.0.4 | The cryptography package before 41.0.2 for Python mishandles SSH certificates that have critical options. |\r\n| CVE-2023-43804 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | urllib3:1.26.14 | urllib3 is a user-friendly HTTP client library for Python. urllib3 doesn't treat the `Cookie` HTTP header special or provide any[...] |\r\n| CVE-2023-4807 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | cryptography:38.0.4 | Issue summary: The POLY1305 MAC (message authentication code) implementation contains a bug that might corrupt the internal stat[...] |\r\n| CVE-2023-49083 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | cryptography:38.0.4 | cryptography is a package designed to expose cryptographic primitives and recipes to Python developers. Calling `load_pem_pkcs7_[...] |\r\n| CVE-2023-50782 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | cryptography:38.0.4 | A flaw was found in the python-cryptography package. This issue may allow a remote attacker to decrypt captured messages in TLS [...] |\r\n| CVE-2023-5363 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | cryptography:38.0.4 | Issue summary: A bug has been identified in the processing of key and initialisation vector (IV) lengths.  This can lead to pote[...] |\r\n| CVE-2023-6730 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | transformers:4.28.1 | Deserialization of Untrusted Data in GitHub repository huggingface/transformers prior to 4.36. |\r\n| CVE-2023-7018 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-408 | transformers:4.28.1 | Deserialization of Untrusted Data in GitHub repository huggingface/transformers prior to 4.36. |"
      },
      {
        "user": "pseudotensor",
        "body": "@achraf-mer Can you add the removal of pkgs folders for the h2ogpt/vllm installs like we have for DAI?"
      },
      {
        "user": "pseudotensor",
        "body": "Just randomly, @codyharris-h2o-ai For transformers, I only see 4.38.2 in the image, not 4.28.1.  I don't know where it is getting the versions."
      },
      {
        "user": "codyharris-h2o-ai",
        "body": "It's picking it up from `workspace/spaces/demo/requirements.txt`"
      },
      {
        "user": "codyharris-h2o-ai",
        "body": "[findings.json](https://github.com/h2oai/h2ogpt/files/14673047/findings.json)\r\nAttaching the raw report from ECR\r\n\r\nSearch for \"filePath\" in the JSON"
      },
      {
        "user": "pseudotensor",
        "body": "Ok, that's old code, could be updated, not part of image really."
      },
      {
        "user": "pseudotensor",
        "body": "@codyharris-h2o-ai I pushed those changes to remove those unnecessary files.  Try again tomorrow on 0.2.0-410"
      },
      {
        "user": "pseudotensor",
        "body": "@codyharris-h2o-ai Please check again."
      },
      {
        "user": "codyharris-h2o-ai",
        "body": "@pseudotensor thanks,\r\nI scanned 412 with the following results:\r\n\r\n| Vulnerability | Severity | Image | Package | Description |\r\n| --- | --- | --- | --- | --- |\r\n| CVE-2023-48022 | critical | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | ray:2.9.3 | Anyscale Ray 2.6.3 and 2.8.0 allows a remote attacker to execute arbitrary code via the job submission API. NOTE: the vendor's p[...] |\r\n| CVE-2024-0964 | critical | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | gradio:3.50.2 | A local file include could be remotely triggered in Gradio due to a vulnerable user-supplied JSON value in an API request. |\r\n| SNYK-PYTHON-GRADIO-6263801 | critical | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | gradio:3.50.2 | ## Overview [gradio](https://pypi.org/project/gradio) is a Python library for easily interacting with trained machine learning m[...] |\r\n| CVE-2022-3996 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | cryptography:38.0.4 | If an X.509 certificate contains a malformed policy constraint and policy processing is enabled, then a write lock will be taken[...] |\r\n| CVE-2022-40898 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | wheel:0.37.1 | An issue discovered in Python Packaging Authority (PyPA) Wheel 0.37.1 and earlier allows remote attackers to cause a denial of s[...] |\r\n| CVE-2022-4450 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | cryptography:38.0.4 | The function PEM_read_bio_ex() reads a PEM file from a BIO and parses and decodes the \"name\" (e.g. \"CERTIFICATE\"), any header da[...] |\r\n| CVE-2023-0215 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | cryptography:38.0.4 | The public API function BIO_new_NDEF is a helper function used for streaming ASN.1 data via a BIO. It is primarily used internal[...] |\r\n| CVE-2023-0216 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | cryptography:38.0.4 | An invalid pointer dereference on read can be triggered when an application tries to load malformed PKCS7 data with the d2i_PKCS[...] |\r\n| CVE-2023-0217 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | cryptography:38.0.4 | An invalid pointer dereference on read can be triggered when an application tries to check a malformed DSA public key by the EVP[...] |\r\n| CVE-2023-0286 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | cryptography:38.0.4 | There is a type confusion vulnerability relating to X.400 address processing inside an X.509 GeneralName. X.400 addresses were p[...] |\r\n| CVE-2023-0401 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | cryptography:38.0.4 | A NULL pointer can be dereferenced when signatures are being verified on PKCS7 signed or signedAndEnveloped data. In case the ha[...] |\r\n| CVE-2023-38325 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | cryptography:38.0.4 | The cryptography package before 41.0.2 for Python mishandles SSH certificates that have critical options. |\r\n| CVE-2023-4807 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | cryptography:38.0.4 | Issue summary: The POLY1305 MAC (message authentication code) implementation contains a bug that might corrupt the internal stat[...] |\r\n| CVE-2023-49083 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | cryptography:38.0.4 | cryptography is a package designed to expose cryptographic primitives and recipes to Python developers. Calling `load_pem_pkcs7_[...] |\r\n| CVE-2023-50782 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | cryptography:38.0.4 | A flaw was found in the python-cryptography package. This issue may allow a remote attacker to decrypt captured messages in TLS [...] |\r\n| CVE-2023-51449 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | gradio:3.50.2 | Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning mod[...] |\r\n| CVE-2023-5363 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | cryptography:38.0.4 | Issue summary: A bug has been identified in the processing of key and initialisation vector (IV) lengths.  This can lead to pote[...] |\r\n| CVE-2023-6572 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-412 | gradio:3.50.2 | Command Injection in GitHub repository gradio-app/gradio prior to main. |"
      },
      {
        "user": "pseudotensor",
        "body": "Sorry 512 is gradio 3 for k8 and 513 failed during push due to some network issue.  Need to avoid the gradio 3 builds we make for k8 issue."
      },
      {
        "user": "codyharris-h2o-ai",
        "body": "Ok will try 410"
      },
      {
        "user": "pseudotensor",
        "body": "I'm building new one, 414."
      },
      {
        "user": "achraf-mer",
        "body": "> @achraf-mer Can you add the removal of pkgs folders for the h2ogpt/vllm installs like we have for DAI?\r\n\r\nI see done in https://github.com/h2oai/h2ogpt/commit/98e390b94a7118916e85992c03ecfbc6f5e1545f and you are building a new image, so will wait and see how to address new findings, thanks."
      },
      {
        "user": "pseudotensor",
        "body": "@achraf-mer I already removed the items, I unassigned you thanks!"
      },
      {
        "user": "codyharris-h2o-ai",
        "body": "Latest scan of 414:\r\n\r\n| Vulnerability | Severity | Image | Package | Description |\r\n| --- | --- | --- | --- | --- |\r\n| CVE-2023-48022 | critical | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | ray:2.10.0 | Anyscale Ray 2.6.3 and 2.8.0 allows a remote attacker to execute arbitrary code via the job submission API. NOTE: the vendor's p[...] |\r\n| CVE-2022-3996 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | cryptography:38.0.4 | If an X.509 certificate contains a malformed policy constraint and policy processing is enabled, then a write lock will be taken[...] |\r\n| CVE-2022-40898 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | wheel:0.37.1 | An issue discovered in Python Packaging Authority (PyPA) Wheel 0.37.1 and earlier allows remote attackers to cause a denial of s[...] |\r\n| CVE-2022-4450 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | cryptography:38.0.4 | The function PEM_read_bio_ex() reads a PEM file from a BIO and parses and decodes the \"name\" (e.g. \"CERTIFICATE\"), any header da[...] |\r\n| CVE-2023-0215 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | cryptography:38.0.4 | The public API function BIO_new_NDEF is a helper function used for streaming ASN.1 data via a BIO. It is primarily used internal[...] |\r\n| CVE-2023-0216 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | cryptography:38.0.4 | An invalid pointer dereference on read can be triggered when an application tries to load malformed PKCS7 data with the d2i_PKCS[...] |\r\n| CVE-2023-0217 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | cryptography:38.0.4 | An invalid pointer dereference on read can be triggered when an application tries to check a malformed DSA public key by the EVP[...] |\r\n| CVE-2023-0286 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | cryptography:38.0.4 | There is a type confusion vulnerability relating to X.400 address processing inside an X.509 GeneralName. X.400 addresses were p[...] |\r\n| CVE-2023-0401 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | cryptography:38.0.4 | A NULL pointer can be dereferenced when signatures are being verified on PKCS7 signed or signedAndEnveloped data. In case the ha[...] |\r\n| CVE-2023-38325 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | cryptography:38.0.4 | The cryptography package before 41.0.2 for Python mishandles SSH certificates that have critical options. |\r\n| CVE-2023-4807 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | cryptography:38.0.4 | Issue summary: The POLY1305 MAC (message authentication code) implementation contains a bug that might corrupt the internal stat[...] |\r\n| CVE-2023-49083 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | cryptography:38.0.4 | cryptography is a package designed to expose cryptographic primitives and recipes to Python developers. Calling `load_pem_pkcs7_[...] |\r\n| CVE-2023-50782 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | cryptography:38.0.4 | A flaw was found in the python-cryptography package. This issue may allow a remote attacker to decrypt captured messages in TLS [...] |\r\n| CVE-2023-5363 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-414 | cryptography:38.0.4 | Issue summary: A bug has been identified in the processing of key and initialisation vector (IV) lengths.  This can lead to pote[...] |\r\n\r\nwrt ray, we must mitigate the functionality by removing the offending source files in the package (such as overwriting or deleting or stubbing out the appropriate functions), or remove ray altogether."
      },
      {
        "user": "pseudotensor",
        "body": "Where is cryptography==38.0.04 from?  I only see we install any latest version, unconstrained.  Should be 42.0.5."
      },
      {
        "user": "codyharris-h2o-ai",
        "body": "@pseudotensor, hey it appears to be coming from `h2ogpt_conda/lib/python3.10/site-packages/cryptography-38.0.4.dist-info/METADATA`"
      },
      {
        "user": "pseudotensor",
        "body": "I think it's because docker build was using fixed miniconda version, not latest, so should be ok tomorrow."
      },
      {
        "user": "codyharris-h2o-ai",
        "body": "| Vulnerability | Severity | Image | Package | Description |\r\n| --- | --- | --- | --- | --- |\r\n| CVE-2023-48022 | critical | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-446 | ray:2.10.0 | Anyscale Ray 2.6.3 and 2.8.0 allows a remote attacker to execute arbitrary code via the job submission API. NOTE: the vendor's p[...] |\r\n| SNYK-PYTHON-PILLOW-6514866 | high | 223008754879.dkr.ecr.us-east-1.amazonaws.com/h2ogpt-runtime:0.2.0-446 | pillow:10.2.0 | ## Overview  Affected versions of this package are vulnerable to Buffer Overflow via the `strcpy` function in `_imagingcms.c`, d[...] |"
      },
      {
        "user": "achraf-mer",
        "body": "@codyharris-h2o-ai is the `ray:2.10.0` issue a case of a bad report?\r\naccording to https://nvd.nist.gov/vuln/detail/CVE-2023-48022 and https://bishopfox.com/blog/ray-versions-2-6-3-2-8-0 the CVE only applies to 2.6.3 and 2.8.0."
      },
      {
        "user": "codyharris-h2o-ai",
        "body": "I discussed this with @YogevMaty and it sounds like it is still an issue"
      },
      {
        "user": "YogevMaty",
        "body": "Apparently this CVE is very similar to the one we had in h2o3 .\r\nThe default installation does not require authentication and is listening on 0.0.0.0\r\nThe company behind Ray is saying it is not a CVE it's by design this is the reason it is not visible  in some scanners.\r\nCurrently they are not planing of fixing this issue.\r\n\r\nwhat to do \r\nSecurity and isolation must be enforced outside of the Ray Cluster. Ray expects to run in a safe network environment and to act upon trusted code. Developers and platform providers must maintain the following invariants to ensure the safe operation of Ray Clusters.\r\n\r\nhttps://docs.ray.io/en/latest/ray-security/index.html#best-practices\r\n\r\n\r\nmore info in -https://www.oligo.security/blog/shadowray-attack-ai-workloads-actively-exploited-in-the-wild"
      }
    ]
  },
  {
    "issue_number": 1526,
    "title": "Add JSON et al. for output, e.g. guided_json if vllm, openai way, mistralai for models supported, etc.",
    "author": "pseudotensor",
    "state": "closed",
    "created_at": "2024-04-02T21:00:03Z",
    "updated_at": "2024-04-03T16:55:09Z",
    "labels": [],
    "body": "https://github.com/vllm-project/vllm/blob/a3c226e7eb19b976a937e745f3867eb05f809278/vllm/model_executor/guided_decoding.py#L91\r\nhttps://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\r\nhttps://github.com/vllm-project/vllm/blob/b0925b38789bb3b20dcc39e229fcfe12a311e487/tests/entrypoints/test_openai_server.py#L477\r\n\r\n```\r\nextra_body=dict(guided_json=json.dumps(json_template))\r\n```",
    "comments": []
  },
  {
    "issue_number": 1515,
    "title": "Shared collection problem with multiple users",
    "author": "llmwesee",
    "state": "closed",
    "created_at": "2024-04-01T08:29:09Z",
    "updated_at": "2024-04-03T15:27:51Z",
    "labels": [
      "type/question"
    ],
    "body": "I created multiple shared collection through UI like UserData1, UserData2 And UserData3. When i run the h2ogpt with enabling the login then UserData2 and UserData3 is not shown in the Collections Tab. \r\n\r\n`HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 python generate.py --inference_server=\"vllm:127.0.0.1:5000\" --base_model='/home/xyz/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-chat-hf/snapshots/c2f3ec81aac798ae26dcc57799a994dfbf521496' --score_model=None --langchain_modes=\"['UserData','UserData2','LLM','MyData']\" --user_path=user_path --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048 --max_new_tokens=2048 --min_new_tokens=128  --prompt_type=llama2 --enable_stt=False --enable_tts=False --auth_filename=$auth_filename --auth_access=open --guest_name=abc --auth=\"[(admin, admin)`\r\n\r\nHow can we make shared collection is accessible for all the users?",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "If they are existing users, you can add them to their auth.json entries and/or they can push the sync button in the UI.\r\n\r\nFor new users, one would pass additional things, e.g.\r\n\r\n```\r\n--langchain_modes=\"['UserData','UserData2','LLM','MyData']\"  --langchain_mode_paths=\"{'UserData': 'user_path', 'UserData2': 'user_path2'}\" --langchain_mode_types=\"{'UserData': 'shared', 'UserData2': 'shared'}\"\r\n```\r\ni.e.\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ python generate.py --langchain_modes=\"['UserData','UserData2','LLM','MyData']\"  --langchain_mode_paths=\"{'UserData': 'user_path', 'UserData2': 'user_path2'}\" --langchain_mode_types=\"{'UserData': 'shared', 'UserData2': 'shared'}\" \r\n```\r\n\r\ngives in UI for random new user:\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/34551899-ed61-44a2-9392-b8310b8dd8f0)\r\n\r\n\r\nFYI, if you do it this way, you don't need to pass `--user_path` separately."
      },
      {
        "user": "pseudotensor",
        "body": "Closing, if issues, let me know and I'll still respond."
      },
      {
        "user": "llmwesee",
        "body": "@pseudotensor Still Not working. Suppose h2ogpt with multiple collections is hosted on the server, now i want all the users/clients must have access those Collections.\r\n\r\n`HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 python generate.py --inference_server=\"vllm:127.0.0.1:5000\" --base_model='/home/xyz/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-chat-hf/snapshots/c2f3ec81aac798ae26dcc57799a994dfbf521496' --score_model=None --langchain_modes=\"['UserData','UserData2','LLM','MyData']\"  --langchain_mode_paths=\"{'UserData': 'user_path', 'UserData2': 'user_path2'}\" --langchain_mode_types=\"{'UserData': 'shared', 'UserData2': 'shared'}\" --use_auth_token=True --max_seq_len=4096 --max_max_new_tokens=2048 --max_new_tokens=2048 --min_new_tokens=128 --concurrency_count=64 --batch_size=16 --prompt_type=llama2 --enable_stt=False --enable_tts=False --auth_filename=$auth_filename --auth_access=open --guest_name=abc --auth=\"[(admin, admin)]\"` \r\n\r\nOne thing I noticed when i just remove the `--auth=\"[(admin, admin)]\"`  then it works perfectly and all the users have access to those Collections."
      },
      {
        "user": "pseudotensor",
        "body": "Hi, I'm still not seeing any issues.\r\n\r\nI ran like this:\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ python generate.py --langchain_modes=\"['UserData','UserData2','LLM','MyData']\"  --langchain_mode_paths=\"{'UserData': 'user_path', 'UserData2': 'user_path2'}\" --langchain_mode_types=\"{'UserData': 'shared', 'UserData2': 'shared'}\" --auth=\"[(admin, admin)]\" --auth_filename=auth_test.json --auth_access=open --guest_name=guest\r\n```\r\n\r\nNote that if you use an auth file, the ` --auth=\"[(admin, admin)]\"` is ignored and only used to indicate a non-empty --auth parameter that (as opposed to `--auth=''` will enable auth.\r\n\r\nSince access is open, I can use any new user.  I used 'jon' and see this:\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/6da7b820-4959-4159-a845-768dc4e73f5b)\r\n\r\n\r\nNote that any existing user won't see newly added collections.  E.g. if I add another collection:\r\n```\r\n(h2ogpt) jon@pseudotensor:~/h2ogpt$ python generate.py --langchain_modes=\"['UserData','UserData2','UserData3','LLM','MyData']\"  --langchain_mode_paths=\"{'UserData': 'user_path', 'UserData2': 'user_path2', 'UserData3': 'user_path3'}\" --langchain_mode_types=\"{'UserData': 'shared', 'UserData2': 'shared', 'UserData3': 'shared'}\" --auth=\"[(admin, admin)]\" --auth_filename=auth_test.json --auth_access=open --guest_name=guest\r\n\r\n```\r\n\r\nAnd go into `jon` I only see the original 2 because that was what is in the auth json file.\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/9ae47d79-bedd-48f0-b585-840926d0df53)\r\n\r\n\r\nBut a new user `jon2` will see all:\r\n\r\n![image](https://github.com/h2oai/h2ogpt/assets/2249614/c6f7ca74-70ae-4930-8a6f-352306664d61)\r\n\r\n\r\nI think maybe that is what you meant to describe, that **existing** users don't see new shared collections when you add new collections to the CLI?\r\n\r\n\r\n\r\n\r\n"
      },
      {
        "user": "llmwesee",
        "body": "Yes, the existing users and also the new user don't see newly created  shared collections"
      },
      {
        "user": "pseudotensor",
        "body": "Should be fixed now. Wasn’t bug just feature request."
      }
    ]
  },
  {
    "issue_number": 1523,
    "title": "\"configure_parser\" issue at install",
    "author": "Aloks6",
    "state": "closed",
    "created_at": "2024-04-02T09:17:24Z",
    "updated_at": "2024-04-03T07:47:53Z",
    "labels": [],
    "body": "Hello !\r\nFirst, thank you for your project, I'm looking forward to test it !\r\n\r\nI tried to install it, following the [install instruction](https://github.com/h2oai/h2ogpt/blob/main/docs/README_LINUX.md) (Debian 12)\r\nGetting to the \"install h2ogpt env\", I got an issue :\r\n`# install h2ogpt env\r\nconda remove -n h2ogpt --all -y`\r\n\r\nreturned \r\n```\r\n# >>>>>>>>>>>>>>>>>>>>>> ERROR REPORT <<<<<<<<<<<<<<<<<<<<<<\r\n\r\n    Traceback (most recent call last):\r\n      File \"/home/iaeval1/miniconda3/lib/python3.10/site-packages/conda/exceptions.py\", line 1124, in __call__\r\n        return func(*args, **kwargs)\r\n      File \"/home/iaeval1/miniconda3/lib/python3.10/site-packages/conda/cli/main.py\", line 57, in main_subshell\r\n        p = generate_parser()\r\n      File \"/home/iaeval1/miniconda3/lib/python3.10/site-packages/conda/cli/conda_argparse.py\", line 40, in generate_parser\r\n        p = ArgumentParser(\r\n      File \"/home/iaeval1/miniconda3/lib/python3.10/site-packages/conda/cli/conda_argparse.py\", line 117, in __init__\r\n        self._subcommands = context.plugin_manager.get_hook_results(\"subcommands\")\r\n      File \"/home/iaeval1/miniconda3/lib/python3.10/site-packages/conda/plugins/manager.py\", line 77, in get_hook_results\r\n        plugins = sorted(\r\n      File \"/home/iaeval1/miniconda3/lib/python3.10/site-packages/conda/plugins/manager.py\", line 78, in <genexpr>\r\n        (item for items in hook() for item in items),\r\n      File \"/home/iaeval1/miniconda3/lib/python3.10/site-packages/conda_libmamba_solver/plugin.py\", line 23, in conda_subcommands\r\n        yield plugins.CondaSubcommand(\r\n    TypeError: CondaSubcommand.__new__() got an unexpected keyword argument 'configure_parser'\r\n\r\n`$ /home/iaeval1/miniconda3/bin/conda remove -n h2ogpt --all -y`\r\n\r\n  environment variables:\r\n                 CIO_TEST=<not set>\r\n        CONDA_DEFAULT_ENV=base\r\n                CONDA_EXE=/home/iaeval1/miniconda3/bin/conda\r\n             CONDA_PREFIX=/home/iaeval1/miniconda3\r\n    CONDA_PROMPT_MODIFIER=(base)\r\n         CONDA_PYTHON_EXE=/home/iaeval1/miniconda3/bin/python\r\n               CONDA_ROOT=/home/iaeval1/miniconda3\r\n              CONDA_SHLVL=1\r\n           CURL_CA_BUNDLE=<not set>\r\n          LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:/usr/local/cuda-12.4/lib64:\r\n               LD_PRELOAD=<not set>\r\n                     PATH=/home/iaeval1/miniconda3/bin:/home/iaeval1/miniconda3/condabin:/usr/lo\r\n                          cal/cuda-12.4/bin:/usr/local/cuda-\r\n                          12.4/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\r\n       REQUESTS_CA_BUNDLE=<not set>\r\n            SSL_CERT_FILE=<not set>\r\n               WINDOWPATH=2\r\n\r\n     active environment : base\r\n    active env location : /home/iaeval1/miniconda3\r\n            shell level : 1\r\n       user config file : /home/iaeval1/.condarc\r\n populated config files : \r\n          conda version : 23.1.0\r\n    conda-build version : not installed\r\n         python version : 3.10.9.final.0\r\n       virtual packages : __archspec=1=haswell\r\n                          __cuda=12.4=0\r\n                          __glibc=2.36=0\r\n                          __linux=6.1.0=0\r\n                          __unix=0=0\r\n       base environment : /home/iaeval1/miniconda3  (writable)\r\n      conda av data dir : /home/iaeval1/miniconda3/etc/conda\r\n  conda av metadata url : None\r\n           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64\r\n                          https://repo.anaconda.com/pkgs/main/noarch\r\n                          https://repo.anaconda.com/pkgs/r/linux-64\r\n                          https://repo.anaconda.com/pkgs/r/noarch\r\n          package cache : /home/iaeval1/miniconda3/pkgs\r\n                          /home/iaeval1/.conda/pkgs\r\n       envs directories : /home/iaeval1/miniconda3/envs\r\n                          /home/iaeval1/.conda/envs\r\n               platform : linux-64\r\n             user-agent : conda/23.1.0 requests/2.28.1 CPython/3.10.9 Linux/6.1.0-18-amd64 debian/12 glibc/2.36\r\n                UID:GID : 1000:1000\r\n             netrc file : None\r\n           offline mode : False\r\n\r\n\r\nAn unexpected error has occurred. Conda has prepared the above report.\r\n\r\n\r\n```\r\n\r\nI went to the \"plugin.py\" to check : \r\n```\r\nfrom conda import plugins\r\n\r\nfrom .repoquery import configure_parser, repoquery\r\nfrom .solver import LibMambaSolver\r\n\r\n\r\n@plugins.hookimpl\r\ndef conda_solvers():\r\n    \"\"\"\r\n    The conda plugin hook implementation to load the solver into conda.\r\n    \"\"\"\r\n    yield plugins.CondaSolver(\r\n        name=\"libmamba\",\r\n        backend=LibMambaSolver,\r\n    )\r\n\r\n\r\n@plugins.hookimpl\r\ndef conda_subcommands():\r\n    yield plugins.CondaSubcommand(\r\n        name=\"repoquery\",\r\n        summary=\"Advanced search for repodata.\",\r\n        action=repoquery,\r\n        configure_parser=configure_parser,\r\n    )\r\n```\r\n\r\nWhat `configure_parser` do ? Can I replace it with `configparser` ?\r\nHow to fix that issue ? I checked but it seems no other issues of that type was reported\r\n\r\nThank you very much for your help !",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I'm not familiar with that issue or know why it happened.  But `conda remove -n h2ogpt --all -y` can be avoided if don't already have h2oGPT installed."
      },
      {
        "user": "Aloks6",
        "body": "Hello !\r\nThank you for your answer ! \r\n\r\nI knew it could be avoided, so I already ran the next command and got the same issue. So is for the `conda create -n h2ogpt -y`, and so is for the quick install... The same error keep repeating, with `configure_parser` not being recognized\r\n`TypeError: CondaSubcommand.__new__() got an unexpected keyword argument 'configure_parser'`\r\n\r\nThing was on my side, I performed a clean install of miniconda and it now works :) \r\nThank you again for checking it"
      },
      {
        "user": "pseudotensor",
        "body": "Ah ok, makes sense."
      }
    ]
  },
  {
    "issue_number": 1512,
    "title": "sources not visible in shared folders collections",
    "author": "santosh-gkg",
    "state": "closed",
    "created_at": "2024-03-29T05:04:16Z",
    "updated_at": "2024-03-29T17:47:00Z",
    "labels": [],
    "body": "![image](https://github.com/h2oai/h2ogpt/assets/150844949/4109e183-0322-44eb-a13c-6f924b1ba7cb)\r\nthey are working fine in localhost but not in a ngrok app\r\n",
    "comments": [
      {
        "user": "pseudotensor",
        "body": "I don't think it's ngrok or not.  If that is a shared doc, you'll need to ensure that is added to `extra_allowed_paths` so gradio knows it's ok to allow that to be downloaded.\r\n\r\nOr you can ensure `langchain_mode_paths` is filled so when used it'll fill the allowed_paths for gradio: \r\n\r\nhttps://github.com/h2oai/h2ogpt/blob/030fed4032529e5c3ebbeb879cbaafde8b77df2b/src/gradio_runner.py#L6344-L6348\r\n\r\nWithout doing anything, sources and download works on gpt.h2o.ai, which uses ngrok for MyData that uses a \"users\" directory.\r\n\r\nNote that from sources it'll download the thing, while in document view that's for viewing the thing."
      },
      {
        "user": "pseudotensor",
        "body": "If you still see issues, it might be good to give me an example, and I can reach out to gradio team if it's not h2ogpt issue."
      },
      {
        "user": "santosh-gkg",
        "body": "thank you very much it's working \r\ni added the collections folder name in paths\r\n![WhatsApp Image 2024-03-29 at 21 11 48_06e34013](https://github.com/h2oai/h2ogpt/assets/150844949/019957ed-774c-436a-b2c5-0981e344869c)\r\n"
      },
      {
        "user": "pseudotensor",
        "body": "Great.  It should work if you just pass as an extra_allowed_paths on CLI instead of changing code."
      },
      {
        "user": "santosh-gkg",
        "body": "Okay"
      }
    ]
  }
]